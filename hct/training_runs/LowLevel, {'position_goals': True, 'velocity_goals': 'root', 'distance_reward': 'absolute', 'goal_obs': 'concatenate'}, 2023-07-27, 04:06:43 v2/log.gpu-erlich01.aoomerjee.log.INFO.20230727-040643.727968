I0727 04:06:43.367072 140120985872192 low_level_env.py:187] Initialising environment...
I0727 04:07:15.664813 140120985872192 low_level_env.py:289] Environment initialised.
I0727 04:07:15.674405 140120985872192 train.py:118] JAX is running on GPU.
I0727 04:07:15.674498 140120985872192 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0727 04:07:20.535815 140120985872192 train.py:367] Running initial eval
I0727 04:07:36.375940 140120985872192 train.py:373] {'eval/walltime': 15.83768606185913, 'eval/episode_goal_distance': (Array(0.3475814, dtype=float32), Array(0.08717535, dtype=float32)), 'eval/episode_reward': (Array(-18958.81, dtype=float32), Array(8776.174, dtype=float32)), 'eval/avg_episode_length': (Array(899.02344, dtype=float32), Array(300.33, dtype=float32)), 'eval/epoch_eval_time': 15.83768606185913, 'eval/sps': 8081.988713506203}
I0727 04:07:36.377798 140120985872192 train.py:379] starting iteration 0 20.703452110290527
I0727 04:08:06.545908 140120985872192 train.py:394] {'eval/walltime': 19.712368726730347, 'training/sps': 9348.466177276601, 'training/walltime': 26.288804531097412, 'training/entropy_loss': Array(-0.04757516, dtype=float32), 'training/policy_loss': Array(0.01937652, dtype=float32), 'training/total_loss': Array(455.2351, dtype=float32), 'training/v_loss': Array(455.2633, dtype=float32), 'eval/episode_goal_distance': (Array(0.35264742, dtype=float32), Array(0.06593218, dtype=float32)), 'eval/episode_reward': (Array(-19453.621, dtype=float32), Array(7513.3804, dtype=float32)), 'eval/avg_episode_length': (Array(922.4297, dtype=float32), Array(266.46317, dtype=float32)), 'eval/epoch_eval_time': 3.874682664871216, 'eval/sps': 33034.963394674385}
I0727 04:08:06.610954 140120985872192 train.py:379] starting iteration 1 50.93659234046936
I0727 04:08:16.031202 140120985872192 train.py:394] {'eval/walltime': 23.606054067611694, 'training/sps': 44501.952999570174, 'training/walltime': 31.811258792877197, 'training/entropy_loss': Array(-0.0473454, dtype=float32), 'training/policy_loss': Array(0.00315835, dtype=float32), 'training/total_loss': Array(395.01752, dtype=float32), 'training/v_loss': Array(395.0617, dtype=float32), 'eval/episode_goal_distance': (Array(0.33417413, dtype=float32), Array(0.07156637, dtype=float32)), 'eval/episode_reward': (Array(-17352.928, dtype=float32), Array(8478.03, dtype=float32)), 'eval/avg_episode_length': (Array(860.1953, dtype=float32), Array(345.60715, dtype=float32)), 'eval/epoch_eval_time': 3.8936853408813477, 'eval/sps': 32873.74011866783}
I0727 04:08:16.033684 140120985872192 train.py:379] starting iteration 2 60.35933804512024
I0727 04:08:25.553517 140120985872192 train.py:394] {'eval/walltime': 27.585795879364014, 'training/sps': 44389.469445769486, 'training/walltime': 37.34770703315735, 'training/entropy_loss': Array(-0.04687062, dtype=float32), 'training/policy_loss': Array(0.00169729, dtype=float32), 'training/total_loss': Array(300.8111, dtype=float32), 'training/v_loss': Array(300.8563, dtype=float32), 'eval/episode_goal_distance': (Array(0.33318287, dtype=float32), Array(0.06955323, dtype=float32)), 'eval/episode_reward': (Array(-17632.844, dtype=float32), Array(7109.409, dtype=float32)), 'eval/avg_episode_length': (Array(906.8281, dtype=float32), Array(289.68378, dtype=float32)), 'eval/epoch_eval_time': 3.9797418117523193, 'eval/sps': 32162.89047244508}
I0727 04:08:25.555980 140120985872192 train.py:379] starting iteration 3 69.88163352012634
I0727 04:08:35.165102 140120985872192 train.py:394] {'eval/walltime': 31.642397165298462, 'training/sps': 44290.60992991177, 'training/walltime': 42.89651298522949, 'training/entropy_loss': Array(-0.04645367, dtype=float32), 'training/policy_loss': Array(0.00084653, dtype=float32), 'training/total_loss': Array(270.6408, dtype=float32), 'training/v_loss': Array(270.6864, dtype=float32), 'eval/episode_goal_distance': (Array(0.3248248, dtype=float32), Array(0.06268071, dtype=float32)), 'eval/episode_reward': (Array(-16392.15, dtype=float32), Array(7341.5825, dtype=float32)), 'eval/avg_episode_length': (Array(875.7422, dtype=float32), Array(328.7556, dtype=float32)), 'eval/epoch_eval_time': 4.056601285934448, 'eval/sps': 31553.50772180088}
I0727 04:08:35.167596 140120985872192 train.py:379] starting iteration 4 79.49324989318848
I0727 04:08:44.774745 140120985872192 train.py:394] {'eval/walltime': 35.69533681869507, 'training/sps': 44276.58121882913, 'training/walltime': 48.44707703590393, 'training/entropy_loss': Array(-0.04615324, dtype=float32), 'training/policy_loss': Array(0.0029801, dtype=float32), 'training/total_loss': Array(143.25877, dtype=float32), 'training/v_loss': Array(143.30194, dtype=float32), 'eval/episode_goal_distance': (Array(0.3121513, dtype=float32), Array(0.05835018, dtype=float32)), 'eval/episode_reward': (Array(-15198.199, dtype=float32), Array(6860.622, dtype=float32)), 'eval/avg_episode_length': (Array(860.2422, dtype=float32), Array(345.49106, dtype=float32)), 'eval/epoch_eval_time': 4.0529396533966064, 'eval/sps': 31582.014771112696}
I0727 04:08:44.777193 140120985872192 train.py:379] starting iteration 5 89.10284662246704
I0727 04:08:54.394044 140120985872192 train.py:394] {'eval/walltime': 39.757330656051636, 'training/sps': 44271.14068460334, 'training/walltime': 53.99832320213318, 'training/entropy_loss': Array(-0.0448683, dtype=float32), 'training/policy_loss': Array(0.00502782, dtype=float32), 'training/total_loss': Array(159.72482, dtype=float32), 'training/v_loss': Array(159.76465, dtype=float32), 'eval/episode_goal_distance': (Array(0.30948055, dtype=float32), Array(0.05796554, dtype=float32)), 'eval/episode_reward': (Array(-14661.25, dtype=float32), Array(6777.063, dtype=float32)), 'eval/avg_episode_length': (Array(852.4922, dtype=float32), Array(353.30682, dtype=float32)), 'eval/epoch_eval_time': 4.061993837356567, 'eval/sps': 31511.618462547653}
I0727 04:08:54.396577 140120985872192 train.py:379] starting iteration 6 98.72223091125488
I0727 04:09:04.048201 140120985872192 train.py:394] {'eval/walltime': 43.85269260406494, 'training/sps': 44260.30733667412, 'training/walltime': 59.55092811584473, 'training/entropy_loss': Array(-0.04284771, dtype=float32), 'training/policy_loss': Array(0.00625322, dtype=float32), 'training/total_loss': Array(165.1698, dtype=float32), 'training/v_loss': Array(165.20639, dtype=float32), 'eval/episode_goal_distance': (Array(0.30177742, dtype=float32), Array(0.06561258, dtype=float32)), 'eval/episode_reward': (Array(-15266.952, dtype=float32), Array(5927.63, dtype=float32)), 'eval/avg_episode_length': (Array(899.10156, dtype=float32), Array(300.09787, dtype=float32)), 'eval/epoch_eval_time': 4.095361948013306, 'eval/sps': 31254.86870875818}
I0727 04:09:04.050807 140120985872192 train.py:379] starting iteration 7 108.37646102905273
I0727 04:09:13.744680 140120985872192 train.py:394] {'eval/walltime': 47.98837375640869, 'training/sps': 44244.96267779352, 'training/walltime': 65.10545873641968, 'training/entropy_loss': Array(-0.04151171, dtype=float32), 'training/policy_loss': Array(0.00669326, dtype=float32), 'training/total_loss': Array(155.01167, dtype=float32), 'training/v_loss': Array(155.04648, dtype=float32), 'eval/episode_goal_distance': (Array(0.30914778, dtype=float32), Array(0.06129069, dtype=float32)), 'eval/episode_reward': (Array(-14932.448, dtype=float32), Array(5834.318, dtype=float32)), 'eval/avg_episode_length': (Array(899.03906, dtype=float32), Array(300.28357, dtype=float32)), 'eval/epoch_eval_time': 4.13568115234375, 'eval/sps': 30950.161602148793}
I0727 04:09:13.747326 140120985872192 train.py:379] starting iteration 8 118.07297992706299
I0727 04:09:23.451546 140120985872192 train.py:394] {'eval/walltime': 52.12653040885925, 'training/sps': 44182.10895739073, 'training/walltime': 70.66789126396179, 'training/entropy_loss': Array(-0.04075004, dtype=float32), 'training/policy_loss': Array(0.00571175, dtype=float32), 'training/total_loss': Array(255.08665, dtype=float32), 'training/v_loss': Array(255.12169, dtype=float32), 'eval/episode_goal_distance': (Array(0.30183196, dtype=float32), Array(0.05692095, dtype=float32)), 'eval/episode_reward': (Array(-14915.774, dtype=float32), Array(6208.73, dtype=float32)), 'eval/avg_episode_length': (Array(875.8281, dtype=float32), Array(328.52856, dtype=float32)), 'eval/epoch_eval_time': 4.1381566524505615, 'eval/sps': 30931.646805637505}
I0727 04:09:23.454015 140120985872192 train.py:379] starting iteration 9 127.7796688079834
I0727 04:09:33.122293 140120985872192 train.py:394] {'eval/walltime': 56.23359537124634, 'training/sps': 44220.3142085739, 'training/walltime': 76.22551798820496, 'training/entropy_loss': Array(-0.04056703, dtype=float32), 'training/policy_loss': Array(0.00902427, dtype=float32), 'training/total_loss': Array(89.83726, dtype=float32), 'training/v_loss': Array(89.868805, dtype=float32), 'eval/episode_goal_distance': (Array(0.30561152, dtype=float32), Array(0.06380647, dtype=float32)), 'eval/episode_reward': (Array(-13847.435, dtype=float32), Array(6308.583, dtype=float32)), 'eval/avg_episode_length': (Array(852.5, dtype=float32), Array(353.28848, dtype=float32)), 'eval/epoch_eval_time': 4.107064962387085, 'eval/sps': 31165.80847204437}
I0727 04:09:33.124712 140120985872192 train.py:379] starting iteration 10 137.45036578178406
I0727 04:09:42.807589 140120985872192 train.py:394] {'eval/walltime': 60.33670091629028, 'training/sps': 44075.478208041946, 'training/walltime': 81.8014075756073, 'training/entropy_loss': Array(-0.0404781, dtype=float32), 'training/policy_loss': Array(0.00857652, dtype=float32), 'training/total_loss': Array(79.700806, dtype=float32), 'training/v_loss': Array(79.732704, dtype=float32), 'eval/episode_goal_distance': (Array(0.2966358, dtype=float32), Array(0.06165045, dtype=float32)), 'eval/episode_reward': (Array(-14530.344, dtype=float32), Array(6132.8994, dtype=float32)), 'eval/avg_episode_length': (Array(883.625, dtype=float32), Array(319.41418, dtype=float32)), 'eval/epoch_eval_time': 4.103105545043945, 'eval/sps': 31195.882873305196}
I0727 04:09:42.810063 140120985872192 train.py:379] starting iteration 11 147.1357171535492
I0727 04:09:52.534281 140120985872192 train.py:394] {'eval/walltime': 64.43794107437134, 'training/sps': 43736.407095535076, 'training/walltime': 87.42052483558655, 'training/entropy_loss': Array(-0.03988268, dtype=float32), 'training/policy_loss': Array(0.00871587, dtype=float32), 'training/total_loss': Array(77.685455, dtype=float32), 'training/v_loss': Array(77.71663, dtype=float32), 'eval/episode_goal_distance': (Array(0.31241477, dtype=float32), Array(0.06156567, dtype=float32)), 'eval/episode_reward': (Array(-14458.387, dtype=float32), Array(6388.1807, dtype=float32)), 'eval/avg_episode_length': (Array(868., dtype=float32), Array(337.29663, dtype=float32)), 'eval/epoch_eval_time': 4.101240158081055, 'eval/sps': 31210.07184809446}
I0727 04:09:52.536708 140120985872192 train.py:379] starting iteration 12 156.86236190795898
I0727 04:10:02.290323 140120985872192 train.py:394] {'eval/walltime': 68.5519654750824, 'training/sps': 43607.095346404705, 'training/walltime': 93.05630493164062, 'training/entropy_loss': Array(-0.0400125, dtype=float32), 'training/policy_loss': Array(0.0088878, dtype=float32), 'training/total_loss': Array(73.979454, dtype=float32), 'training/v_loss': Array(74.010574, dtype=float32), 'eval/episode_goal_distance': (Array(0.29692245, dtype=float32), Array(0.06372344, dtype=float32)), 'eval/episode_reward': (Array(-13304.5, dtype=float32), Array(7362.6, dtype=float32)), 'eval/avg_episode_length': (Array(798.0703, dtype=float32), Array(399.95786, dtype=float32)), 'eval/epoch_eval_time': 4.11402440071106, 'eval/sps': 31113.08721889854}
I0727 04:10:02.292846 140120985872192 train.py:379] starting iteration 13 166.6184995174408
I0727 04:10:12.046023 140120985872192 train.py:394] {'eval/walltime': 72.64514923095703, 'training/sps': 43449.12060131208, 'training/walltime': 98.71257591247559, 'training/entropy_loss': Array(-0.0406909, dtype=float32), 'training/policy_loss': Array(0.01223563, dtype=float32), 'training/total_loss': Array(75.743454, dtype=float32), 'training/v_loss': Array(75.771904, dtype=float32), 'eval/episode_goal_distance': (Array(0.30474007, dtype=float32), Array(0.06835277, dtype=float32)), 'eval/episode_reward': (Array(-13807.886, dtype=float32), Array(7474.4165, dtype=float32)), 'eval/avg_episode_length': (Array(805.71875, dtype=float32), Array(394.3486, dtype=float32)), 'eval/epoch_eval_time': 4.093183755874634, 'eval/sps': 31271.50102076199}
I0727 04:10:12.048387 140120985872192 train.py:379] starting iteration 14 176.37404108047485
I0727 04:10:21.815724 140120985872192 train.py:394] {'eval/walltime': 76.73942542076111, 'training/sps': 43347.871930033274, 'training/walltime': 104.3820583820343, 'training/entropy_loss': Array(-0.04014312, dtype=float32), 'training/policy_loss': Array(0.0109112, dtype=float32), 'training/total_loss': Array(79.95064, dtype=float32), 'training/v_loss': Array(79.97987, dtype=float32), 'eval/episode_goal_distance': (Array(0.3059137, dtype=float32), Array(0.05333138, dtype=float32)), 'eval/episode_reward': (Array(-13829., dtype=float32), Array(6327.0815, dtype=float32)), 'eval/avg_episode_length': (Array(852.5078, dtype=float32), Array(353.2698, dtype=float32)), 'eval/epoch_eval_time': 4.094276189804077, 'eval/sps': 31263.157165302317}
I0727 04:10:21.818106 140120985872192 train.py:379] starting iteration 15 186.1437599658966
I0727 04:10:31.630108 140120985872192 train.py:394] {'eval/walltime': 80.83256340026855, 'training/sps': 43001.74448909759, 'training/walltime': 110.09717535972595, 'training/entropy_loss': Array(-0.03908742, dtype=float32), 'training/policy_loss': Array(0.01618492, dtype=float32), 'training/total_loss': Array(80.47384, dtype=float32), 'training/v_loss': Array(80.496735, dtype=float32), 'eval/episode_goal_distance': (Array(0.30065146, dtype=float32), Array(0.07049939, dtype=float32)), 'eval/episode_reward': (Array(-14530.082, dtype=float32), Array(5958.8223, dtype=float32)), 'eval/avg_episode_length': (Array(883.46875, dtype=float32), Array(319.843, dtype=float32)), 'eval/epoch_eval_time': 4.093137979507446, 'eval/sps': 31271.850751389295}
I0727 04:10:31.632551 140120985872192 train.py:379] starting iteration 16 195.95820450782776
I0727 04:10:41.465940 140120985872192 train.py:394] {'eval/walltime': 84.93631362915039, 'training/sps': 42919.97948752455, 'training/walltime': 115.82317996025085, 'training/entropy_loss': Array(-0.03760736, dtype=float32), 'training/policy_loss': Array(0.01618642, dtype=float32), 'training/total_loss': Array(188.11832, dtype=float32), 'training/v_loss': Array(188.13971, dtype=float32), 'eval/episode_goal_distance': (Array(0.29955247, dtype=float32), Array(0.05570812, dtype=float32)), 'eval/episode_reward': (Array(-15258.678, dtype=float32), Array(5507.7095, dtype=float32)), 'eval/avg_episode_length': (Array(914.5625, dtype=float32), Array(278.64142, dtype=float32)), 'eval/epoch_eval_time': 4.103750228881836, 'eval/sps': 31190.982116588668}
I0727 04:10:41.468328 140120985872192 train.py:379] starting iteration 17 205.79398155212402
I0727 04:10:51.284669 140120985872192 train.py:394] {'eval/walltime': 89.02362108230591, 'training/sps': 42925.17520480083, 'training/walltime': 121.54849147796631, 'training/entropy_loss': Array(-0.03610503, dtype=float32), 'training/policy_loss': Array(0.01401152, dtype=float32), 'training/total_loss': Array(89.50081, dtype=float32), 'training/v_loss': Array(89.522896, dtype=float32), 'eval/episode_goal_distance': (Array(0.30590534, dtype=float32), Array(0.05822835, dtype=float32)), 'eval/episode_reward': (Array(-14129.169, dtype=float32), Array(6421.489, dtype=float32)), 'eval/avg_episode_length': (Array(852.4922, dtype=float32), Array(353.30743, dtype=float32)), 'eval/epoch_eval_time': 4.087307453155518, 'eval/sps': 31316.459910834543}
I0727 04:10:51.287355 140120985872192 train.py:379] starting iteration 18 215.6130075454712
I0727 04:11:01.121382 140120985872192 train.py:394] {'eval/walltime': 93.1180374622345, 'training/sps': 42845.95896059102, 'training/walltime': 127.28438830375671, 'training/entropy_loss': Array(-0.03465788, dtype=float32), 'training/policy_loss': Array(0.01797373, dtype=float32), 'training/total_loss': Array(60.079163, dtype=float32), 'training/v_loss': Array(60.095844, dtype=float32), 'eval/episode_goal_distance': (Array(0.30468118, dtype=float32), Array(0.061467, dtype=float32)), 'eval/episode_reward': (Array(-14136.395, dtype=float32), Array(6486.7305, dtype=float32)), 'eval/avg_episode_length': (Array(860.21094, dtype=float32), Array(345.5686, dtype=float32)), 'eval/epoch_eval_time': 4.094416379928589, 'eval/sps': 31262.086735358473}
I0727 04:11:01.123771 140120985872192 train.py:379] starting iteration 19 225.44942450523376
I0727 04:11:10.967340 140120985872192 train.py:394] {'eval/walltime': 97.21230387687683, 'training/sps': 42773.847495382935, 'training/walltime': 133.0299551486969, 'training/entropy_loss': Array(-0.03411353, dtype=float32), 'training/policy_loss': Array(0.01741909, dtype=float32), 'training/total_loss': Array(60.96495, dtype=float32), 'training/v_loss': Array(60.98164, dtype=float32), 'eval/episode_goal_distance': (Array(0.29823244, dtype=float32), Array(0.06475509, dtype=float32)), 'eval/episode_reward': (Array(-14799.582, dtype=float32), Array(5894.9966, dtype=float32)), 'eval/avg_episode_length': (Array(898.9844, dtype=float32), Array(300.44583, dtype=float32)), 'eval/epoch_eval_time': 4.094266414642334, 'eval/sps': 31263.23180685881}
I0727 04:11:10.969627 140120985872192 train.py:379] starting iteration 20 235.29528045654297
I0727 04:11:20.844585 140120985872192 train.py:394] {'eval/walltime': 101.30316662788391, 'training/sps': 42516.76898967913, 'training/walltime': 138.8102626800537, 'training/entropy_loss': Array(-0.0327458, dtype=float32), 'training/policy_loss': Array(0.01846334, dtype=float32), 'training/total_loss': Array(57.54644, dtype=float32), 'training/v_loss': Array(57.56072, dtype=float32), 'eval/episode_goal_distance': (Array(0.29440737, dtype=float32), Array(0.06501142, dtype=float32)), 'eval/episode_reward': (Array(-14106., dtype=float32), Array(6047.097, dtype=float32)), 'eval/avg_episode_length': (Array(875.7422, dtype=float32), Array(328.7561, dtype=float32)), 'eval/epoch_eval_time': 4.09086275100708, 'eval/sps': 31289.243318781406}
I0727 04:11:20.846939 140120985872192 train.py:379] starting iteration 21 245.17259168624878
I0727 04:11:30.761316 140120985872192 train.py:394] {'eval/walltime': 105.43173336982727, 'training/sps': 42503.41718613278, 'training/walltime': 144.59238600730896, 'training/entropy_loss': Array(-0.03197724, dtype=float32), 'training/policy_loss': Array(0.0170624, dtype=float32), 'training/total_loss': Array(63.449024, dtype=float32), 'training/v_loss': Array(63.46394, dtype=float32), 'eval/episode_goal_distance': (Array(0.30273888, dtype=float32), Array(0.05820234, dtype=float32)), 'eval/episode_reward': (Array(-15198.939, dtype=float32), Array(5605.4614, dtype=float32)), 'eval/avg_episode_length': (Array(914.5625, dtype=float32), Array(278.64127, dtype=float32)), 'eval/epoch_eval_time': 4.128566741943359, 'eval/sps': 31003.495401832613}
I0727 04:11:30.763654 140120985872192 train.py:379] starting iteration 22 255.08930611610413
I0727 04:11:40.658758 140120985872192 train.py:394] {'eval/walltime': 109.52560877799988, 'training/sps': 42390.72475983855, 'training/walltime': 150.38988065719604, 'training/entropy_loss': Array(-0.03024842, dtype=float32), 'training/policy_loss': Array(0.02339227, dtype=float32), 'training/total_loss': Array(66.68439, dtype=float32), 'training/v_loss': Array(66.691246, dtype=float32), 'eval/episode_goal_distance': (Array(0.30666316, dtype=float32), Array(0.05804426, dtype=float32)), 'eval/episode_reward': (Array(-14883.066, dtype=float32), Array(5901.5493, dtype=float32)), 'eval/avg_episode_length': (Array(899.03906, dtype=float32), Array(300.28336, dtype=float32)), 'eval/epoch_eval_time': 4.093875408172607, 'eval/sps': 31266.217761408538}
I0727 04:11:40.661162 140120985872192 train.py:379] starting iteration 23 264.9868154525757
I0727 04:11:50.543326 140120985872192 train.py:394] {'eval/walltime': 113.61625671386719, 'training/sps': 42461.972437076256, 'training/walltime': 156.1776475906372, 'training/entropy_loss': Array(-0.02721304, dtype=float32), 'training/policy_loss': Array(0.02823895, dtype=float32), 'training/total_loss': Array(65.30221, dtype=float32), 'training/v_loss': Array(65.30118, dtype=float32), 'eval/episode_goal_distance': (Array(0.30697653, dtype=float32), Array(0.06895364, dtype=float32)), 'eval/episode_reward': (Array(-13847.241, dtype=float32), Array(6619.213, dtype=float32)), 'eval/avg_episode_length': (Array(844.66406, dtype=float32), Array(360.96857, dtype=float32)), 'eval/epoch_eval_time': 4.09064793586731, 'eval/sps': 31290.886433339834}
I0727 04:11:50.545587 140120985872192 train.py:379] starting iteration 24 274.87124037742615
I0727 04:12:00.428014 140120985872192 train.py:394] {'eval/walltime': 117.71694779396057, 'training/sps': 42532.50353223, 'training/walltime': 161.95581674575806, 'training/entropy_loss': Array(-0.01959459, dtype=float32), 'training/policy_loss': Array(0.02099195, dtype=float32), 'training/total_loss': Array(62.035843, dtype=float32), 'training/v_loss': Array(62.034447, dtype=float32), 'eval/episode_goal_distance': (Array(0.30292606, dtype=float32), Array(0.06326781, dtype=float32)), 'eval/episode_reward': (Array(-14557.99, dtype=float32), Array(6565.0215, dtype=float32)), 'eval/avg_episode_length': (Array(868.03906, dtype=float32), Array(337.19638, dtype=float32)), 'eval/epoch_eval_time': 4.100691080093384, 'eval/sps': 31214.250842100766}
I0727 04:12:00.430271 140120985872192 train.py:379] starting iteration 25 284.75592494010925
I0727 04:12:10.295825 140120985872192 train.py:394] {'eval/walltime': 121.80096912384033, 'training/sps': 42535.803140097116, 'training/walltime': 167.7335376739502, 'training/entropy_loss': Array(-0.01439028, dtype=float32), 'training/policy_loss': Array(0.0118541, dtype=float32), 'training/total_loss': Array(189.49408, dtype=float32), 'training/v_loss': Array(189.49661, dtype=float32), 'eval/episode_goal_distance': (Array(0.30546823, dtype=float32), Array(0.06391776, dtype=float32)), 'eval/episode_reward': (Array(-14464.616, dtype=float32), Array(5810.3774, dtype=float32)), 'eval/avg_episode_length': (Array(891.34375, dtype=float32), Array(310.0587, dtype=float32)), 'eval/epoch_eval_time': 4.084021329879761, 'eval/sps': 31341.65805244913}
I0727 04:12:10.298297 140120985872192 train.py:379] starting iteration 26 294.62395000457764
I0727 04:12:20.203815 140120985872192 train.py:394] {'eval/walltime': 125.89776802062988, 'training/sps': 42335.49430433768, 'training/walltime': 173.53859567642212, 'training/entropy_loss': Array(-0.00299722, dtype=float32), 'training/policy_loss': Array(0.01399988, dtype=float32), 'training/total_loss': Array(61.51738, dtype=float32), 'training/v_loss': Array(61.50637, dtype=float32), 'eval/episode_goal_distance': (Array(0.30181745, dtype=float32), Array(0.0598246, dtype=float32)), 'eval/episode_reward': (Array(-14961.111, dtype=float32), Array(5075.3403, dtype=float32)), 'eval/avg_episode_length': (Array(930.0547, dtype=float32), Array(254.33818, dtype=float32)), 'eval/epoch_eval_time': 4.096798896789551, 'eval/sps': 31243.906089778284}
I0727 04:12:20.206056 140120985872192 train.py:379] starting iteration 27 304.53170943260193
I0727 04:12:30.100727 140120985872192 train.py:394] {'eval/walltime': 129.99079084396362, 'training/sps': 42387.789256406504, 'training/walltime': 179.3364918231964, 'training/entropy_loss': Array(0.01220279, dtype=float32), 'training/policy_loss': Array(0.01363668, dtype=float32), 'training/total_loss': Array(43.109215, dtype=float32), 'training/v_loss': Array(43.083374, dtype=float32), 'eval/episode_goal_distance': (Array(0.29353672, dtype=float32), Array(0.07163199, dtype=float32)), 'eval/episode_reward': (Array(-14134.036, dtype=float32), Array(5830.0605, dtype=float32)), 'eval/avg_episode_length': (Array(891.27344, dtype=float32), Array(310.25952, dtype=float32)), 'eval/epoch_eval_time': 4.09302282333374, 'eval/sps': 31272.730577090904}
I0727 04:12:30.103271 140120985872192 train.py:379] starting iteration 28 314.42892479896545
I0727 04:12:40.043798 140120985872192 train.py:394] {'eval/walltime': 134.12877488136292, 'training/sps': 42381.71036172868, 'training/walltime': 185.1352195739746, 'training/entropy_loss': Array(0.02357378, dtype=float32), 'training/policy_loss': Array(0.01482936, dtype=float32), 'training/total_loss': Array(42.4713, dtype=float32), 'training/v_loss': Array(42.43289, dtype=float32), 'eval/episode_goal_distance': (Array(0.30065066, dtype=float32), Array(0.06149364, dtype=float32)), 'eval/episode_reward': (Array(-14608.316, dtype=float32), Array(5834.8354, dtype=float32)), 'eval/avg_episode_length': (Array(899.0469, dtype=float32), Array(300.2602, dtype=float32)), 'eval/epoch_eval_time': 4.137984037399292, 'eval/sps': 30932.937112161395}
I0727 04:12:40.046041 140120985872192 train.py:379] starting iteration 29 324.37169432640076
I0727 04:12:49.943546 140120985872192 train.py:394] {'eval/walltime': 138.2206106185913, 'training/sps': 42357.72551089054, 'training/walltime': 190.9372308254242, 'training/entropy_loss': Array(0.03396759, dtype=float32), 'training/policy_loss': Array(0.01545594, dtype=float32), 'training/total_loss': Array(39.43647, dtype=float32), 'training/v_loss': Array(39.387047, dtype=float32), 'eval/episode_goal_distance': (Array(0.28976804, dtype=float32), Array(0.06147805, dtype=float32)), 'eval/episode_reward': (Array(-13597.52, dtype=float32), Array(5814.02, dtype=float32)), 'eval/avg_episode_length': (Array(875.71094, dtype=float32), Array(328.83832, dtype=float32)), 'eval/epoch_eval_time': 4.0918357372283936, 'eval/sps': 31281.803136775194}
I0727 04:12:49.945796 140120985872192 train.py:379] starting iteration 30 334.27144956588745
I0727 04:12:59.874869 140120985872192 train.py:394] {'eval/walltime': 142.31273317337036, 'training/sps': 42131.4985680781, 'training/walltime': 196.77039623260498, 'training/entropy_loss': Array(0.04493117, dtype=float32), 'training/policy_loss': Array(0.0168191, dtype=float32), 'training/total_loss': Array(38.51579, dtype=float32), 'training/v_loss': Array(38.45404, dtype=float32), 'eval/episode_goal_distance': (Array(0.2978528, dtype=float32), Array(0.06134672, dtype=float32)), 'eval/episode_reward': (Array(-14369.32, dtype=float32), Array(5465.6, dtype=float32)), 'eval/avg_episode_length': (Array(906.77344, dtype=float32), Array(289.85352, dtype=float32)), 'eval/epoch_eval_time': 4.092122554779053, 'eval/sps': 31279.61058998907}
I0727 04:12:59.877279 140120985872192 train.py:379] starting iteration 31 344.2029330730438
I0727 04:13:09.803626 140120985872192 train.py:394] {'eval/walltime': 146.40892004966736, 'training/sps': 42180.24897830377, 'training/walltime': 202.5968198776245, 'training/entropy_loss': Array(0.05656767, dtype=float32), 'training/policy_loss': Array(0.0146763, dtype=float32), 'training/total_loss': Array(40.782154, dtype=float32), 'training/v_loss': Array(40.71091, dtype=float32), 'eval/episode_goal_distance': (Array(0.30036104, dtype=float32), Array(0.06127648, dtype=float32)), 'eval/episode_reward': (Array(-14118.9375, dtype=float32), Array(5844.516, dtype=float32)), 'eval/avg_episode_length': (Array(883.5156, dtype=float32), Array(319.71478, dtype=float32)), 'eval/epoch_eval_time': 4.096186876296997, 'eval/sps': 31248.574312047393}
I0727 04:13:09.805890 140120985872192 train.py:379] starting iteration 32 354.1315438747406
I0727 04:13:19.743489 140120985872192 train.py:394] {'eval/walltime': 150.50639629364014, 'training/sps': 42108.284518384055, 'training/walltime': 208.43320107460022, 'training/entropy_loss': Array(0.06954873, dtype=float32), 'training/policy_loss': Array(0.01978417, dtype=float32), 'training/total_loss': Array(39.545017, dtype=float32), 'training/v_loss': Array(39.45568, dtype=float32), 'eval/episode_goal_distance': (Array(0.3007753, dtype=float32), Array(0.05995532, dtype=float32)), 'eval/episode_reward': (Array(-15083.486, dtype=float32), Array(4650.5596, dtype=float32)), 'eval/avg_episode_length': (Array(945.7031, dtype=float32), Array(225.74579, dtype=float32)), 'eval/epoch_eval_time': 4.097476243972778, 'eval/sps': 31238.74121010044}
I0727 04:13:19.745948 140120985872192 train.py:379] starting iteration 33 364.0716016292572
I0727 04:13:29.673591 140120985872192 train.py:394] {'eval/walltime': 154.5989634990692, 'training/sps': 42143.71480213584, 'training/walltime': 214.26467561721802, 'training/entropy_loss': Array(0.07260757, dtype=float32), 'training/policy_loss': Array(0.01611142, dtype=float32), 'training/total_loss': Array(169.32144, dtype=float32), 'training/v_loss': Array(169.23273, dtype=float32), 'eval/episode_goal_distance': (Array(0.29397643, dtype=float32), Array(0.05361629, dtype=float32)), 'eval/episode_reward': (Array(-13799.295, dtype=float32), Array(5821.463, dtype=float32)), 'eval/avg_episode_length': (Array(875.71875, dtype=float32), Array(328.81757, dtype=float32)), 'eval/epoch_eval_time': 4.092567205429077, 'eval/sps': 31276.212112094097}
I0727 04:13:29.675902 140120985872192 train.py:379] starting iteration 34 374.0015552043915
I0727 04:13:39.626189 140120985872192 train.py:394] {'eval/walltime': 158.73245334625244, 'training/sps': 42277.79425371988, 'training/walltime': 220.0776562690735, 'training/entropy_loss': Array(0.084576, dtype=float32), 'training/policy_loss': Array(0.0186479, dtype=float32), 'training/total_loss': Array(51.45553, dtype=float32), 'training/v_loss': Array(51.352303, dtype=float32), 'eval/episode_goal_distance': (Array(0.2929532, dtype=float32), Array(0.06201708, dtype=float32)), 'eval/episode_reward': (Array(-13963.828, dtype=float32), Array(5315.0938, dtype=float32)), 'eval/avg_episode_length': (Array(906.8594, dtype=float32), Array(289.58682, dtype=float32)), 'eval/epoch_eval_time': 4.1334898471832275, 'eval/sps': 30966.569347503242}
I0727 04:13:39.631014 140120985872192 train.py:379] starting iteration 35 383.9566512107849
I0727 04:13:49.569178 140120985872192 train.py:394] {'eval/walltime': 162.83693265914917, 'training/sps': 42157.862234926026, 'training/walltime': 225.90717387199402, 'training/entropy_loss': Array(0.09698536, dtype=float32), 'training/policy_loss': Array(0.02734197, dtype=float32), 'training/total_loss': Array(34.84305, dtype=float32), 'training/v_loss': Array(34.71872, dtype=float32), 'eval/episode_goal_distance': (Array(0.30128038, dtype=float32), Array(0.0558881, dtype=float32)), 'eval/episode_reward': (Array(-14501.391, dtype=float32), Array(5532.842, dtype=float32)), 'eval/avg_episode_length': (Array(906.7578, dtype=float32), Array(289.9022, dtype=float32)), 'eval/epoch_eval_time': 4.1044793128967285, 'eval/sps': 31185.441621744765}
I0727 04:13:49.571593 140120985872192 train.py:379] starting iteration 36 393.8972465991974
I0727 04:13:59.501739 140120985872192 train.py:394] {'eval/walltime': 166.9399573802948, 'training/sps': 42202.020211515286, 'training/walltime': 231.73059177398682, 'training/entropy_loss': Array(0.10853112, dtype=float32), 'training/policy_loss': Array(0.02560709, dtype=float32), 'training/total_loss': Array(35.415558, dtype=float32), 'training/v_loss': Array(35.281418, dtype=float32), 'eval/episode_goal_distance': (Array(0.29175532, dtype=float32), Array(0.06329471, dtype=float32)), 'eval/episode_reward': (Array(-14557.047, dtype=float32), Array(5287.1377, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.75882, dtype=float32)), 'eval/epoch_eval_time': 4.10302472114563, 'eval/sps': 31196.49738894587}
I0727 04:13:59.504073 140120985872192 train.py:379] starting iteration 37 403.829726934433
I0727 04:14:09.458639 140120985872192 train.py:394] {'eval/walltime': 171.06235814094543, 'training/sps': 42164.83602772969, 'training/walltime': 237.55914521217346, 'training/entropy_loss': Array(0.11943054, dtype=float32), 'training/policy_loss': Array(0.02952092, dtype=float32), 'training/total_loss': Array(34.577175, dtype=float32), 'training/v_loss': Array(34.428223, dtype=float32), 'eval/episode_goal_distance': (Array(0.2884518, dtype=float32), Array(0.06339903, dtype=float32)), 'eval/episode_reward': (Array(-13579.475, dtype=float32), Array(5892.7583, dtype=float32)), 'eval/avg_episode_length': (Array(875.7969, dtype=float32), Array(328.6111, dtype=float32)), 'eval/epoch_eval_time': 4.122400760650635, 'eval/sps': 31049.868130675844}
I0727 04:14:09.461047 140120985872192 train.py:379] starting iteration 38 413.7867012023926
I0727 04:14:19.380191 140120985872192 train.py:394] {'eval/walltime': 175.1575276851654, 'training/sps': 42225.04543651026, 'training/walltime': 243.3793876171112, 'training/entropy_loss': Array(0.12885751, dtype=float32), 'training/policy_loss': Array(0.03941936, dtype=float32), 'training/total_loss': Array(29.242924, dtype=float32), 'training/v_loss': Array(29.074646, dtype=float32), 'eval/episode_goal_distance': (Array(0.29492235, dtype=float32), Array(0.0596382, dtype=float32)), 'eval/episode_reward': (Array(-14626.021, dtype=float32), Array(5383.682, dtype=float32)), 'eval/avg_episode_length': (Array(914.53906, dtype=float32), Array(278.71857, dtype=float32)), 'eval/epoch_eval_time': 4.095169544219971, 'eval/sps': 31256.337159633}
I0727 04:14:19.382605 140120985872192 train.py:379] starting iteration 39 423.70825839042664
I0727 04:14:29.294420 140120985872192 train.py:394] {'eval/walltime': 179.26376914978027, 'training/sps': 42375.03743670674, 'training/walltime': 249.17902851104736, 'training/entropy_loss': Array(0.13638926, dtype=float32), 'training/policy_loss': Array(0.04955916, dtype=float32), 'training/total_loss': Array(32.349464, dtype=float32), 'training/v_loss': Array(32.163517, dtype=float32), 'eval/episode_goal_distance': (Array(0.2927897, dtype=float32), Array(0.05664321, dtype=float32)), 'eval/episode_reward': (Array(-13914.899, dtype=float32), Array(6196.1553, dtype=float32)), 'eval/avg_episode_length': (Array(867.9844, dtype=float32), Array(337.33627, dtype=float32)), 'eval/epoch_eval_time': 4.106241464614868, 'eval/sps': 31172.058706977514}
I0727 04:14:29.296947 140120985872192 train.py:379] starting iteration 40 433.6226005554199
I0727 04:14:39.225816 140120985872192 train.py:394] {'eval/walltime': 183.4000849723816, 'training/sps': 42453.48896326497, 'training/walltime': 254.96795201301575, 'training/entropy_loss': Array(0.14055689, dtype=float32), 'training/policy_loss': Array(0.05476081, dtype=float32), 'training/total_loss': Array(31.257471, dtype=float32), 'training/v_loss': Array(31.062153, dtype=float32), 'eval/episode_goal_distance': (Array(0.29561168, dtype=float32), Array(0.06168486, dtype=float32)), 'eval/episode_reward': (Array(-15258.573, dtype=float32), Array(4777.471, dtype=float32)), 'eval/avg_episode_length': (Array(953.375, dtype=float32), Array(210.24402, dtype=float32)), 'eval/epoch_eval_time': 4.136315822601318, 'eval/sps': 30945.412654563966}
I0727 04:14:39.228305 140120985872192 train.py:379] starting iteration 41 443.55395913124084
I0727 04:14:49.175121 140120985872192 train.py:394] {'eval/walltime': 187.51031517982483, 'training/sps': 42133.751114102444, 'training/walltime': 260.80080556869507, 'training/entropy_loss': Array(0.12588447, dtype=float32), 'training/policy_loss': Array(0.05157954, dtype=float32), 'training/total_loss': Array(136.23007, dtype=float32), 'training/v_loss': Array(136.05261, dtype=float32), 'eval/episode_goal_distance': (Array(0.29656753, dtype=float32), Array(0.06456961, dtype=float32)), 'eval/episode_reward': (Array(-14831.14, dtype=float32), Array(6067.577, dtype=float32)), 'eval/avg_episode_length': (Array(891.3125, dtype=float32), Array(310.148, dtype=float32)), 'eval/epoch_eval_time': 4.110230207443237, 'eval/sps': 31141.808010705612}
I0727 04:14:49.177645 140120985872192 train.py:379] starting iteration 42 453.5032994747162
I0727 04:14:59.103684 140120985872192 train.py:394] {'eval/walltime': 191.6085798740387, 'training/sps': 42198.107090179285, 'training/walltime': 266.62476348876953, 'training/entropy_loss': Array(0.11258685, dtype=float32), 'training/policy_loss': Array(0.04522463, dtype=float32), 'training/total_loss': Array(65.31625, dtype=float32), 'training/v_loss': Array(65.15843, dtype=float32), 'eval/episode_goal_distance': (Array(0.29869893, dtype=float32), Array(0.06828535, dtype=float32)), 'eval/episode_reward': (Array(-14134.703, dtype=float32), Array(6996.363, dtype=float32)), 'eval/avg_episode_length': (Array(852.46094, dtype=float32), Array(353.38235, dtype=float32)), 'eval/epoch_eval_time': 4.098264694213867, 'eval/sps': 31232.731302279408}
I0727 04:14:59.106575 140120985872192 train.py:379] starting iteration 43 463.43222975730896
I0727 04:15:09.048943 140120985872192 train.py:394] {'eval/walltime': 195.7165985107422, 'training/sps': 42150.42882254203, 'training/walltime': 272.45530915260315, 'training/entropy_loss': Array(0.13597423, dtype=float32), 'training/policy_loss': Array(0.05049209, dtype=float32), 'training/total_loss': Array(36.335144, dtype=float32), 'training/v_loss': Array(36.148674, dtype=float32), 'eval/episode_goal_distance': (Array(0.29359552, dtype=float32), Array(0.06178183, dtype=float32)), 'eval/episode_reward': (Array(-14937.982, dtype=float32), Array(6739.8506, dtype=float32)), 'eval/avg_episode_length': (Array(891.2578, dtype=float32), Array(310.30368, dtype=float32)), 'eval/epoch_eval_time': 4.108018636703491, 'eval/sps': 31158.573346374716}
I0727 04:15:09.051565 140120985872192 train.py:379] starting iteration 44 473.3772189617157
I0727 04:15:18.955906 140120985872192 train.py:394] {'eval/walltime': 199.80695939064026, 'training/sps': 42298.36624147758, 'training/walltime': 278.26546263694763, 'training/entropy_loss': Array(0.1456608, dtype=float32), 'training/policy_loss': Array(0.04879659, dtype=float32), 'training/total_loss': Array(34.020103, dtype=float32), 'training/v_loss': Array(33.825645, dtype=float32), 'eval/episode_goal_distance': (Array(0.29379496, dtype=float32), Array(0.06471542, dtype=float32)), 'eval/episode_reward': (Array(-14237.705, dtype=float32), Array(6318.8394, dtype=float32)), 'eval/avg_episode_length': (Array(875.8125, dtype=float32), Array(328.56998, dtype=float32)), 'eval/epoch_eval_time': 4.090360879898071, 'eval/sps': 31293.08238523679}
I0727 04:15:18.958402 140120985872192 train.py:379] starting iteration 45 483.2840564250946
I0727 04:15:28.909509 140120985872192 train.py:394] {'eval/walltime': 203.91981959342957, 'training/sps': 42121.595734130286, 'training/walltime': 284.0999994277954, 'training/entropy_loss': Array(0.15710463, dtype=float32), 'training/policy_loss': Array(0.04649161, dtype=float32), 'training/total_loss': Array(35.11037, dtype=float32), 'training/v_loss': Array(34.906773, dtype=float32), 'eval/episode_goal_distance': (Array(0.2987905, dtype=float32), Array(0.06008704, dtype=float32)), 'eval/episode_reward': (Array(-14655.598, dtype=float32), Array(6080.55, dtype=float32)), 'eval/avg_episode_length': (Array(891.3125, dtype=float32), Array(310.14777, dtype=float32)), 'eval/epoch_eval_time': 4.112860202789307, 'eval/sps': 31121.89417797169}
I0727 04:15:28.911880 140120985872192 train.py:379] starting iteration 46 493.23753452301025
I0727 04:15:38.816367 140120985872192 train.py:394] {'eval/walltime': 208.01938724517822, 'training/sps': 42363.29610159772, 'training/walltime': 289.90124773979187, 'training/entropy_loss': Array(0.16812679, dtype=float32), 'training/policy_loss': Array(0.05250844, dtype=float32), 'training/total_loss': Array(36.539776, dtype=float32), 'training/v_loss': Array(36.31914, dtype=float32), 'eval/episode_goal_distance': (Array(0.2906011, dtype=float32), Array(0.06355549, dtype=float32)), 'eval/episode_reward': (Array(-15147.6875, dtype=float32), Array(5743.169, dtype=float32)), 'eval/avg_episode_length': (Array(914.625, dtype=float32), Array(278.43787, dtype=float32)), 'eval/epoch_eval_time': 4.099567651748657, 'eval/sps': 31222.804664633848}
I0727 04:15:38.819460 140120985872192 train.py:379] starting iteration 47 503.14511466026306
I0727 04:15:48.786546 140120985872192 train.py:394] {'eval/walltime': 212.1261773109436, 'training/sps': 41963.581254984434, 'training/walltime': 295.7577545642853, 'training/entropy_loss': Array(0.17615208, dtype=float32), 'training/policy_loss': Array(0.07006525, dtype=float32), 'training/total_loss': Array(32.82662, dtype=float32), 'training/v_loss': Array(32.5804, dtype=float32), 'eval/episode_goal_distance': (Array(0.30292708, dtype=float32), Array(0.05864336, dtype=float32)), 'eval/episode_reward': (Array(-16368.609, dtype=float32), Array(5079.4927, dtype=float32)), 'eval/avg_episode_length': (Array(961.1797, dtype=float32), Array(192.54274, dtype=float32)), 'eval/epoch_eval_time': 4.106790065765381, 'eval/sps': 31167.89462091598}
I0727 04:15:48.788920 140120985872192 train.py:379] starting iteration 48 513.1145741939545
I0727 04:15:58.689268 140120985872192 train.py:394] {'eval/walltime': 216.22556591033936, 'training/sps': 42391.70625696149, 'training/walltime': 301.55511498451233, 'training/entropy_loss': Array(0.18295792, dtype=float32), 'training/policy_loss': Array(0.07602084, dtype=float32), 'training/total_loss': Array(38.05867, dtype=float32), 'training/v_loss': Array(37.79969, dtype=float32), 'eval/episode_goal_distance': (Array(0.3052972, dtype=float32), Array(0.06053949, dtype=float32)), 'eval/episode_reward': (Array(-15074.689, dtype=float32), Array(6278.2505, dtype=float32)), 'eval/avg_episode_length': (Array(891.3594, dtype=float32), Array(310.01453, dtype=float32)), 'eval/epoch_eval_time': 4.099388599395752, 'eval/sps': 31224.168408641995}
I0727 04:15:58.691537 140120985872192 train.py:379] starting iteration 49 523.017190694809
I0727 04:16:08.614320 140120985872192 train.py:394] {'eval/walltime': 220.31922721862793, 'training/sps': 42187.43737278803, 'training/walltime': 307.3805458545685, 'training/entropy_loss': Array(0.19055852, dtype=float32), 'training/policy_loss': Array(0.09977942, dtype=float32), 'training/total_loss': Array(29.741926, dtype=float32), 'training/v_loss': Array(29.45159, dtype=float32), 'eval/episode_goal_distance': (Array(0.2941013, dtype=float32), Array(0.0642452, dtype=float32)), 'eval/episode_reward': (Array(-15012.617, dtype=float32), Array(5615.912, dtype=float32)), 'eval/avg_episode_length': (Array(930.08594, dtype=float32), Array(254.22455, dtype=float32)), 'eval/epoch_eval_time': 4.093661308288574, 'eval/sps': 31267.852995272002}
I0727 04:16:08.616780 140120985872192 train.py:379] starting iteration 50 532.9424338340759
I0727 04:16:18.535743 140120985872192 train.py:394] {'eval/walltime': 224.45107650756836, 'training/sps': 42493.960643729864, 'training/walltime': 313.16395592689514, 'training/entropy_loss': Array(0.16908464, dtype=float32), 'training/policy_loss': Array(0.06968157, dtype=float32), 'training/total_loss': Array(164.22919, dtype=float32), 'training/v_loss': Array(163.99042, dtype=float32), 'eval/episode_goal_distance': (Array(0.30319297, dtype=float32), Array(0.06621269, dtype=float32)), 'eval/episode_reward': (Array(-14084.846, dtype=float32), Array(6709.99, dtype=float32)), 'eval/avg_episode_length': (Array(860.3125, dtype=float32), Array(345.3179, dtype=float32)), 'eval/epoch_eval_time': 4.13184928894043, 'eval/sps': 30978.864679941966}
I0727 04:16:18.538108 140120985872192 train.py:379] starting iteration 51 542.8637626171112
I0727 04:16:28.462249 140120985872192 train.py:394] {'eval/walltime': 228.54995703697205, 'training/sps': 42215.311236472044, 'training/walltime': 318.98554039001465, 'training/entropy_loss': Array(0.19327322, dtype=float32), 'training/policy_loss': Array(0.09193283, dtype=float32), 'training/total_loss': Array(35.288803, dtype=float32), 'training/v_loss': Array(35.0036, dtype=float32), 'eval/episode_goal_distance': (Array(0.30283445, dtype=float32), Array(0.0598736, dtype=float32)), 'eval/episode_reward': (Array(-14218.778, dtype=float32), Array(7062.58, dtype=float32)), 'eval/avg_episode_length': (Array(836.90625, dtype=float32), Array(368.14606, dtype=float32)), 'eval/epoch_eval_time': 4.0988805294036865, 'eval/sps': 31228.038749063442}
I0727 04:16:28.516323 140120985872192 train.py:379] starting iteration 52 552.8419597148895
I0727 04:16:38.455329 140120985872192 train.py:394] {'eval/walltime': 232.65201687812805, 'training/sps': 42133.621947843836, 'training/walltime': 324.8184118270874, 'training/entropy_loss': Array(0.2012631, dtype=float32), 'training/policy_loss': Array(0.07183023, dtype=float32), 'training/total_loss': Array(31.55976, dtype=float32), 'training/v_loss': Array(31.286667, dtype=float32), 'eval/episode_goal_distance': (Array(0.29860616, dtype=float32), Array(0.06248645, dtype=float32)), 'eval/episode_reward': (Array(-15515.801, dtype=float32), Array(6016.2866, dtype=float32)), 'eval/avg_episode_length': (Array(906.78906, dtype=float32), Array(289.80502, dtype=float32)), 'eval/epoch_eval_time': 4.102059841156006, 'eval/sps': 31203.83537942932}
I0727 04:16:38.458004 140120985872192 train.py:379] starting iteration 53 562.7836585044861
I0727 04:16:48.389728 140120985872192 train.py:394] {'eval/walltime': 236.76424145698547, 'training/sps': 42257.95946451854, 'training/walltime': 330.6341209411621, 'training/entropy_loss': Array(0.00886575, dtype=float32), 'training/policy_loss': Array(0.08646134, dtype=float32), 'training/total_loss': Array(48.79071, dtype=float32), 'training/v_loss': Array(48.695385, dtype=float32), 'eval/episode_goal_distance': (Array(0.3008219, dtype=float32), Array(0.06526678, dtype=float32)), 'eval/episode_reward': (Array(-14998.484, dtype=float32), Array(6400.8496, dtype=float32)), 'eval/avg_episode_length': (Array(883.47656, dtype=float32), Array(319.8214, dtype=float32)), 'eval/epoch_eval_time': 4.112224578857422, 'eval/sps': 31126.70466931665}
I0727 04:16:48.392147 140120985872192 train.py:379] starting iteration 54 572.7178003787994
I0727 04:16:58.324200 140120985872192 train.py:394] {'eval/walltime': 240.86714720726013, 'training/sps': 42189.62683261052, 'training/walltime': 336.45924949645996, 'training/entropy_loss': Array(-0.05108152, dtype=float32), 'training/policy_loss': Array(0.00170716, dtype=float32), 'training/total_loss': Array(37.258476, dtype=float32), 'training/v_loss': Array(37.30785, dtype=float32), 'eval/episode_goal_distance': (Array(0.3078432, dtype=float32), Array(0.05823833, dtype=float32)), 'eval/episode_reward': (Array(-16425.557, dtype=float32), Array(5866.9507, dtype=float32)), 'eval/avg_episode_length': (Array(922.28125, dtype=float32), Array(266.97302, dtype=float32)), 'eval/epoch_eval_time': 4.102905750274658, 'eval/sps': 31197.401985515113}
I0727 04:16:58.326629 140120985872192 train.py:379] starting iteration 55 582.6522829532623
I0727 04:17:08.256804 140120985872192 train.py:394] {'eval/walltime': 244.98222088813782, 'training/sps': 42289.14471235614, 'training/walltime': 342.2706699371338, 'training/entropy_loss': Array(-0.0502359, dtype=float32), 'training/policy_loss': Array(0.00071911, dtype=float32), 'training/total_loss': Array(35.10477, dtype=float32), 'training/v_loss': Array(35.154285, dtype=float32), 'eval/episode_goal_distance': (Array(0.3152903, dtype=float32), Array(0.06352772, dtype=float32)), 'eval/episode_reward': (Array(-16939.605, dtype=float32), Array(6185.094, dtype=float32)), 'eval/avg_episode_length': (Array(930.08594, dtype=float32), Array(254.22437, dtype=float32)), 'eval/epoch_eval_time': 4.1150736808776855, 'eval/sps': 31105.15386268842}
I0727 04:17:08.259170 140120985872192 train.py:379] starting iteration 56 592.584823846817
I0727 04:17:18.166751 140120985872192 train.py:394] {'eval/walltime': 249.07883715629578, 'training/sps': 42320.184896439976, 'training/walltime': 348.07782793045044, 'training/entropy_loss': Array(-0.04828536, dtype=float32), 'training/policy_loss': Array(0.00064362, dtype=float32), 'training/total_loss': Array(34.31347, dtype=float32), 'training/v_loss': Array(34.361107, dtype=float32), 'eval/episode_goal_distance': (Array(0.31595555, dtype=float32), Array(0.06681477, dtype=float32)), 'eval/episode_reward': (Array(-16515.512, dtype=float32), Array(6757.5283, dtype=float32)), 'eval/avg_episode_length': (Array(906.78125, dtype=float32), Array(289.82947, dtype=float32)), 'eval/epoch_eval_time': 4.096616268157959, 'eval/sps': 31245.298954386843}
I0727 04:17:18.169327 140120985872192 train.py:379] starting iteration 57 602.4949817657471
I0727 04:17:28.094854 140120985872192 train.py:394] {'eval/walltime': 253.1880326271057, 'training/sps': 42281.33714885519, 'training/walltime': 353.8903214931488, 'training/entropy_loss': Array(-0.04472452, dtype=float32), 'training/policy_loss': Array(0.00062414, dtype=float32), 'training/total_loss': Array(32.39157, dtype=float32), 'training/v_loss': Array(32.43567, dtype=float32), 'eval/episode_goal_distance': (Array(0.31237283, dtype=float32), Array(0.06779623, dtype=float32)), 'eval/episode_reward': (Array(-15737.131, dtype=float32), Array(6914.5327, dtype=float32)), 'eval/avg_episode_length': (Array(875.6875, dtype=float32), Array(328.90048, dtype=float32)), 'eval/epoch_eval_time': 4.1091954708099365, 'eval/sps': 31149.649830303828}
I0727 04:17:28.097336 140120985872192 train.py:379] starting iteration 58 612.4229898452759
I0727 04:17:38.041306 140120985872192 train.py:394] {'eval/walltime': 257.2882263660431, 'training/sps': 42080.625632791, 'training/walltime': 359.73053884506226, 'training/entropy_loss': Array(-0.04151587, dtype=float32), 'training/policy_loss': Array(0.00074458, dtype=float32), 'training/total_loss': Array(131.39487, dtype=float32), 'training/v_loss': Array(131.43567, dtype=float32), 'eval/episode_goal_distance': (Array(0.32121482, dtype=float32), Array(0.07224689, dtype=float32)), 'eval/episode_reward': (Array(-17980.152, dtype=float32), Array(5992.8374, dtype=float32)), 'eval/avg_episode_length': (Array(945.6328, dtype=float32), Array(226.03813, dtype=float32)), 'eval/epoch_eval_time': 4.100193738937378, 'eval/sps': 31218.037036750604}
I0727 04:17:38.043762 140120985872192 train.py:379] starting iteration 59 622.3694162368774
I0727 04:17:47.946103 140120985872192 train.py:394] {'eval/walltime': 261.3850429058075, 'training/sps': 42359.50445906218, 'training/walltime': 365.532306432724, 'training/entropy_loss': Array(-0.03656949, dtype=float32), 'training/policy_loss': Array(0.00106267, dtype=float32), 'training/total_loss': Array(44.2035, dtype=float32), 'training/v_loss': Array(44.239006, dtype=float32), 'eval/episode_goal_distance': (Array(0.32077512, dtype=float32), Array(0.06283092, dtype=float32)), 'eval/episode_reward': (Array(-16325.037, dtype=float32), Array(7700.5483, dtype=float32)), 'eval/avg_episode_length': (Array(860.28125, dtype=float32), Array(345.39474, dtype=float32)), 'eval/epoch_eval_time': 4.096816539764404, 'eval/sps': 31243.77153763417}
I0727 04:17:47.948757 140120985872192 train.py:379] starting iteration 60 632.2744107246399
I0727 04:17:57.850383 140120985872192 train.py:394] {'eval/walltime': 265.47489833831787, 'training/sps': 42312.48571162811, 'training/walltime': 371.3405210971832, 'training/entropy_loss': Array(-0.03045971, dtype=float32), 'training/policy_loss': Array(0.00077808, dtype=float32), 'training/total_loss': Array(33.149662, dtype=float32), 'training/v_loss': Array(33.179344, dtype=float32), 'eval/episode_goal_distance': (Array(0.31371695, dtype=float32), Array(0.06760214, dtype=float32)), 'eval/episode_reward': (Array(-16989.176, dtype=float32), Array(7422.284, dtype=float32)), 'eval/avg_episode_length': (Array(899.02344, dtype=float32), Array(300.32974, dtype=float32)), 'eval/epoch_eval_time': 4.089855432510376, 'eval/sps': 31296.949760748117}
I0727 04:17:57.852835 140120985872192 train.py:379] starting iteration 61 642.1784887313843
I0727 04:18:07.773600 140120985872192 train.py:394] {'eval/walltime': 269.5721549987793, 'training/sps': 42229.07081694579, 'training/walltime': 377.1602087020874, 'training/entropy_loss': Array(-0.02466146, dtype=float32), 'training/policy_loss': Array(0.00107451, dtype=float32), 'training/total_loss': Array(41.70482, dtype=float32), 'training/v_loss': Array(41.7284, dtype=float32), 'eval/episode_goal_distance': (Array(0.313706, dtype=float32), Array(0.06738742, dtype=float32)), 'eval/episode_reward': (Array(-16987.357, dtype=float32), Array(6488.075, dtype=float32)), 'eval/avg_episode_length': (Array(914.59375, dtype=float32), Array(278.5397, dtype=float32)), 'eval/epoch_eval_time': 4.097256660461426, 'eval/sps': 31240.415382126652}
I0727 04:18:07.776052 140120985872192 train.py:379] starting iteration 62 652.1017060279846
I0727 04:18:17.707793 140120985872192 train.py:394] {'eval/walltime': 273.6949918270111, 'training/sps': 42335.48387181565, 'training/walltime': 382.9652681350708, 'training/entropy_loss': Array(-0.01950858, dtype=float32), 'training/policy_loss': Array(0.00130251, dtype=float32), 'training/total_loss': Array(44.22376, dtype=float32), 'training/v_loss': Array(44.241966, dtype=float32), 'eval/episode_goal_distance': (Array(0.3150536, dtype=float32), Array(0.06288696, dtype=float32)), 'eval/episode_reward': (Array(-16399.883, dtype=float32), Array(6851.765, dtype=float32)), 'eval/avg_episode_length': (Array(891.1875, dtype=float32), Array(310.5042, dtype=float32)), 'eval/epoch_eval_time': 4.1228368282318115, 'eval/sps': 31046.584022801653}
I0727 04:18:17.710400 140120985872192 train.py:379] starting iteration 63 662.0360538959503
I0727 04:18:27.664319 140120985872192 train.py:394] {'eval/walltime': 277.7978558540344, 'training/sps': 42029.654334041414, 'training/walltime': 388.8125681877136, 'training/entropy_loss': Array(-0.0126851, dtype=float32), 'training/policy_loss': Array(0.0015212, dtype=float32), 'training/total_loss': Array(32.649544, dtype=float32), 'training/v_loss': Array(32.66071, dtype=float32), 'eval/episode_goal_distance': (Array(0.3139851, dtype=float32), Array(0.06108069, dtype=float32)), 'eval/episode_reward': (Array(-16176.724, dtype=float32), Array(7797.658, dtype=float32)), 'eval/avg_episode_length': (Array(860.14844, dtype=float32), Array(345.72278, dtype=float32)), 'eval/epoch_eval_time': 4.102864027023315, 'eval/sps': 31197.719241226176}
I0727 04:18:27.666906 140120985872192 train.py:379] starting iteration 64 671.9925603866577
I0727 04:18:37.581960 140120985872192 train.py:394] {'eval/walltime': 281.8991229534149, 'training/sps': 42299.159473846645, 'training/walltime': 394.62261271476746, 'training/entropy_loss': Array(-0.00565366, dtype=float32), 'training/policy_loss': Array(0.00155743, dtype=float32), 'training/total_loss': Array(32.770443, dtype=float32), 'training/v_loss': Array(32.77454, dtype=float32), 'eval/episode_goal_distance': (Array(0.3236954, dtype=float32), Array(0.06278473, dtype=float32)), 'eval/episode_reward': (Array(-17314.137, dtype=float32), Array(7729.0586, dtype=float32)), 'eval/avg_episode_length': (Array(875.77344, dtype=float32), Array(328.67358, dtype=float32)), 'eval/epoch_eval_time': 4.101267099380493, 'eval/sps': 31209.866828555187}
I0727 04:18:37.584507 140120985872192 train.py:379] starting iteration 65 681.9101610183716
I0727 04:18:47.507594 140120985872192 train.py:394] {'eval/walltime': 285.99629831314087, 'training/sps': 42211.6514807852, 'training/walltime': 400.4447019100189, 'training/entropy_loss': Array(-0.00012928, dtype=float32), 'training/policy_loss': Array(0.00142655, dtype=float32), 'training/total_loss': Array(31.822613, dtype=float32), 'training/v_loss': Array(31.821316, dtype=float32), 'eval/episode_goal_distance': (Array(0.32839525, dtype=float32), Array(0.07227108, dtype=float32)), 'eval/episode_reward': (Array(-16995.145, dtype=float32), Array(7401.6353, dtype=float32)), 'eval/avg_episode_length': (Array(883.4531, dtype=float32), Array(319.8858, dtype=float32)), 'eval/epoch_eval_time': 4.097175359725952, 'eval/sps': 31241.035289385694}
I0727 04:18:47.510106 140120985872192 train.py:379] starting iteration 66 691.8357603549957
I0727 04:18:57.463224 140120985872192 train.py:394] {'eval/walltime': 290.11393332481384, 'training/sps': 42141.154525161684, 'training/walltime': 406.27653074264526, 'training/entropy_loss': Array(0.00567391, dtype=float32), 'training/policy_loss': Array(0.001612, dtype=float32), 'training/total_loss': Array(124.08168, dtype=float32), 'training/v_loss': Array(124.0744, dtype=float32), 'eval/episode_goal_distance': (Array(0.30526665, dtype=float32), Array(0.0650783, dtype=float32)), 'eval/episode_reward': (Array(-15290.195, dtype=float32), Array(7539.0757, dtype=float32)), 'eval/avg_episode_length': (Array(844.7031, dtype=float32), Array(360.8779, dtype=float32)), 'eval/epoch_eval_time': 4.117635011672974, 'eval/sps': 31085.80523459127}
I0727 04:18:57.465669 140120985872192 train.py:379] starting iteration 67 701.7913234233856
I0727 04:19:07.376814 140120985872192 train.py:394] {'eval/walltime': 294.210164308548, 'training/sps': 42290.82073946481, 'training/walltime': 412.0877208709717, 'training/entropy_loss': Array(0.01045167, dtype=float32), 'training/policy_loss': Array(0.00162075, dtype=float32), 'training/total_loss': Array(63.518166, dtype=float32), 'training/v_loss': Array(63.506096, dtype=float32), 'eval/episode_goal_distance': (Array(0.31167108, dtype=float32), Array(0.06847731, dtype=float32)), 'eval/episode_reward': (Array(-14789.125, dtype=float32), Array(8455.099, dtype=float32)), 'eval/avg_episode_length': (Array(790.4297, dtype=float32), Array(405.33066, dtype=float32)), 'eval/epoch_eval_time': 4.096230983734131, 'eval/sps': 31248.23783333502}
I0727 04:19:07.379268 140120985872192 train.py:379] starting iteration 68 711.7049221992493
I0727 04:19:17.355924 140120985872192 train.py:394] {'eval/walltime': 298.3523495197296, 'training/sps': 42149.16891902833, 'training/walltime': 417.9184408187866, 'training/entropy_loss': Array(0.01544736, dtype=float32), 'training/policy_loss': Array(0.00169395, dtype=float32), 'training/total_loss': Array(35.438637, dtype=float32), 'training/v_loss': Array(35.421494, dtype=float32), 'eval/episode_goal_distance': (Array(0.32035515, dtype=float32), Array(0.06777123, dtype=float32)), 'eval/episode_reward': (Array(-17652.2, dtype=float32), Array(6843.0713, dtype=float32)), 'eval/avg_episode_length': (Array(914.5703, dtype=float32), Array(278.61615, dtype=float32)), 'eval/epoch_eval_time': 4.142185211181641, 'eval/sps': 30901.563661245717}
I0727 04:19:17.358499 140120985872192 train.py:379] starting iteration 69 721.684152841568
I0727 04:19:27.238785 140120985872192 train.py:394] {'eval/walltime': 302.45046973228455, 'training/sps': 42530.48014298734, 'training/walltime': 423.6968848705292, 'training/entropy_loss': Array(0.01863587, dtype=float32), 'training/policy_loss': Array(0.00151697, dtype=float32), 'training/total_loss': Array(33.74711, dtype=float32), 'training/v_loss': Array(33.726955, dtype=float32), 'eval/episode_goal_distance': (Array(0.31179073, dtype=float32), Array(0.06463217, dtype=float32)), 'eval/episode_reward': (Array(-16577.05, dtype=float32), Array(7377.6133, dtype=float32)), 'eval/avg_episode_length': (Array(883.53906, dtype=float32), Array(319.64978, dtype=float32)), 'eval/epoch_eval_time': 4.098120212554932, 'eval/sps': 31233.832430747483}
I0727 04:19:27.241239 140120985872192 train.py:379] starting iteration 70 731.5668933391571
I0727 04:19:37.132236 140120985872192 train.py:394] {'eval/walltime': 306.55418586730957, 'training/sps': 42492.774711511316, 'training/walltime': 429.4804563522339, 'training/entropy_loss': Array(0.02110738, dtype=float32), 'training/policy_loss': Array(0.00187096, dtype=float32), 'training/total_loss': Array(47.84191, dtype=float32), 'training/v_loss': Array(47.818935, dtype=float32), 'eval/episode_goal_distance': (Array(0.3176172, dtype=float32), Array(0.06425036, dtype=float32)), 'eval/episode_reward': (Array(-17930.184, dtype=float32), Array(6758.53, dtype=float32)), 'eval/avg_episode_length': (Array(922.4531, dtype=float32), Array(266.3829, dtype=float32)), 'eval/epoch_eval_time': 4.103716135025024, 'eval/sps': 31191.241252660246}
I0727 04:19:37.134700 140120985872192 train.py:379] starting iteration 71 741.4603545665741
I0727 04:19:47.027856 140120985872192 train.py:394] {'eval/walltime': 310.6464409828186, 'training/sps': 42392.50299408586, 'training/walltime': 435.2777078151703, 'training/entropy_loss': Array(0.02077389, dtype=float32), 'training/policy_loss': Array(0.00170464, dtype=float32), 'training/total_loss': Array(31.836662, dtype=float32), 'training/v_loss': Array(31.814182, dtype=float32), 'eval/episode_goal_distance': (Array(0.32235217, dtype=float32), Array(0.06302102, dtype=float32)), 'eval/episode_reward': (Array(-17014.375, dtype=float32), Array(6734.6577, dtype=float32)), 'eval/avg_episode_length': (Array(906.83594, dtype=float32), Array(289.6595, dtype=float32)), 'eval/epoch_eval_time': 4.092255115509033, 'eval/sps': 31278.59734719353}
I0727 04:19:47.030307 140120985872192 train.py:379] starting iteration 72 751.3559601306915
I0727 04:19:56.942825 140120985872192 train.py:394] {'eval/walltime': 314.7371208667755, 'training/sps': 42240.270497146936, 'training/walltime': 441.0958523750305, 'training/entropy_loss': Array(0.02240832, dtype=float32), 'training/policy_loss': Array(0.00184063, dtype=float32), 'training/total_loss': Array(28.89777, dtype=float32), 'training/v_loss': Array(28.87352, dtype=float32), 'eval/episode_goal_distance': (Array(0.32628322, dtype=float32), Array(0.06559636, dtype=float32)), 'eval/episode_reward': (Array(-17410.498, dtype=float32), Array(8041.681, dtype=float32)), 'eval/avg_episode_length': (Array(883.4922, dtype=float32), Array(319.7786, dtype=float32)), 'eval/epoch_eval_time': 4.090679883956909, 'eval/sps': 31290.642052436957}
I0727 04:19:56.945407 140120985872192 train.py:379] starting iteration 73 761.2710604667664
I0727 04:20:06.890162 140120985872192 train.py:394] {'eval/walltime': 318.86692118644714, 'training/sps': 42290.53098200895, 'training/walltime': 446.90708231925964, 'training/entropy_loss': Array(0.02479822, dtype=float32), 'training/policy_loss': Array(0.001966, dtype=float32), 'training/total_loss': Array(28.696556, dtype=float32), 'training/v_loss': Array(28.669792, dtype=float32), 'eval/episode_goal_distance': (Array(0.30958652, dtype=float32), Array(0.06464758, dtype=float32)), 'eval/episode_reward': (Array(-16726.781, dtype=float32), Array(6600.0503, dtype=float32)), 'eval/avg_episode_length': (Array(906.78125, dtype=float32), Array(289.82916, dtype=float32)), 'eval/epoch_eval_time': 4.129800319671631, 'eval/sps': 30994.234609913914}
I0727 04:20:06.892796 140120985872192 train.py:379] starting iteration 74 771.2184495925903
I0727 04:20:16.825093 140120985872192 train.py:394] {'eval/walltime': 322.9703257083893, 'training/sps': 42189.60956468816, 'training/walltime': 452.7322132587433, 'training/entropy_loss': Array(0.02674866, dtype=float32), 'training/policy_loss': Array(0.00194587, dtype=float32), 'training/total_loss': Array(28.427282, dtype=float32), 'training/v_loss': Array(28.398586, dtype=float32), 'eval/episode_goal_distance': (Array(0.31601644, dtype=float32), Array(0.06803863, dtype=float32)), 'eval/episode_reward': (Array(-16367.229, dtype=float32), Array(7282.8203, dtype=float32)), 'eval/avg_episode_length': (Array(875.83594, dtype=float32), Array(328.50772, dtype=float32)), 'eval/epoch_eval_time': 4.103404521942139, 'eval/sps': 31193.6099196522}
I0727 04:20:16.827411 140120985872192 train.py:379] starting iteration 75 781.1530654430389
I0727 04:20:26.733901 140120985872192 train.py:394] {'eval/walltime': 327.09461736679077, 'training/sps': 42529.99406688037, 'training/walltime': 458.51072335243225, 'training/entropy_loss': Array(0.02528019, dtype=float32), 'training/policy_loss': Array(0.0019651, dtype=float32), 'training/total_loss': Array(150.20636, dtype=float32), 'training/v_loss': Array(150.17911, dtype=float32), 'eval/episode_goal_distance': (Array(0.32122964, dtype=float32), Array(0.06524833, dtype=float32)), 'eval/episode_reward': (Array(-17905.512, dtype=float32), Array(7496.004, dtype=float32)), 'eval/avg_episode_length': (Array(914.53125, dtype=float32), Array(278.74362, dtype=float32)), 'eval/epoch_eval_time': 4.124291658401489, 'eval/sps': 31035.63244351414}
I0727 04:20:26.736288 140120985872192 train.py:379] starting iteration 76 791.0619421005249
I0727 04:20:36.667040 140120985872192 train.py:394] {'eval/walltime': 331.19640493392944, 'training/sps': 42188.84115645291, 'training/walltime': 464.3359603881836, 'training/entropy_loss': Array(0.02362527, dtype=float32), 'training/policy_loss': Array(0.00168901, dtype=float32), 'training/total_loss': Array(36.68863, dtype=float32), 'training/v_loss': Array(36.663315, dtype=float32), 'eval/episode_goal_distance': (Array(0.3190698, dtype=float32), Array(0.07546247, dtype=float32)), 'eval/episode_reward': (Array(-16460.246, dtype=float32), Array(6861.616, dtype=float32)), 'eval/avg_episode_length': (Array(891.1797, dtype=float32), Array(310.5266, dtype=float32)), 'eval/epoch_eval_time': 4.101787567138672, 'eval/sps': 31205.906669927896}
I0727 04:20:36.669496 140120985872192 train.py:379] starting iteration 77 800.9951500892639
I0727 04:20:46.622479 140120985872192 train.py:394] {'eval/walltime': 335.3343462944031, 'training/sps': 42290.753071200605, 'training/walltime': 470.1471598148346, 'training/entropy_loss': Array(0.01927235, dtype=float32), 'training/policy_loss': Array(0.00169368, dtype=float32), 'training/total_loss': Array(29.585705, dtype=float32), 'training/v_loss': Array(29.56474, dtype=float32), 'eval/episode_goal_distance': (Array(0.32289076, dtype=float32), Array(0.06512348, dtype=float32)), 'eval/episode_reward': (Array(-18557., dtype=float32), Array(6247.3765, dtype=float32)), 'eval/avg_episode_length': (Array(968.875, dtype=float32), Array(173.2967, dtype=float32)), 'eval/epoch_eval_time': 4.137941360473633, 'eval/sps': 30933.256141007518}
I0727 04:20:46.624977 140120985872192 train.py:379] starting iteration 78 810.9506311416626
I0727 04:20:56.542419 140120985872192 train.py:394] {'eval/walltime': 339.4205901622772, 'training/sps': 42172.284875217076, 'training/walltime': 475.9746837615967, 'training/entropy_loss': Array(0.01593147, dtype=float32), 'training/policy_loss': Array(0.0017522, dtype=float32), 'training/total_loss': Array(34.431488, dtype=float32), 'training/v_loss': Array(34.413803, dtype=float32), 'eval/episode_goal_distance': (Array(0.31715053, dtype=float32), Array(0.06964134, dtype=float32)), 'eval/episode_reward': (Array(-15757.045, dtype=float32), Array(9027.068, dtype=float32)), 'eval/avg_episode_length': (Array(805.9531, dtype=float32), Array(393.8729, dtype=float32)), 'eval/epoch_eval_time': 4.0862438678741455, 'eval/sps': 31324.611094881024}
I0727 04:20:56.544811 140120985872192 train.py:379] starting iteration 79 820.8704648017883
I0727 04:21:06.486958 140120985872192 train.py:394] {'eval/walltime': 343.55372047424316, 'training/sps': 42333.22882898407, 'training/walltime': 481.7800524234772, 'training/entropy_loss': Array(0.01503987, dtype=float32), 'training/policy_loss': Array(0.00179322, dtype=float32), 'training/total_loss': Array(34.565342, dtype=float32), 'training/v_loss': Array(34.548508, dtype=float32), 'eval/episode_goal_distance': (Array(0.32792044, dtype=float32), Array(0.06930941, dtype=float32)), 'eval/episode_reward': (Array(-17441.668, dtype=float32), Array(8550.955, dtype=float32)), 'eval/avg_episode_length': (Array(868.1172, dtype=float32), Array(336.99698, dtype=float32)), 'eval/epoch_eval_time': 4.133130311965942, 'eval/sps': 30969.263086001323}
I0727 04:21:06.489341 140120985872192 train.py:379] starting iteration 80 830.814995765686
I0727 04:21:16.389028 140120985872192 train.py:394] {'eval/walltime': 347.65610122680664, 'training/sps': 42421.456886890766, 'training/walltime': 487.5733470916748, 'training/entropy_loss': Array(0.01439003, dtype=float32), 'training/policy_loss': Array(0.00148492, dtype=float32), 'training/total_loss': Array(28.317696, dtype=float32), 'training/v_loss': Array(28.301823, dtype=float32), 'eval/episode_goal_distance': (Array(0.31288478, dtype=float32), Array(0.06942058, dtype=float32)), 'eval/episode_reward': (Array(-16236.994, dtype=float32), Array(7131.885, dtype=float32)), 'eval/avg_episode_length': (Array(875.77344, dtype=float32), Array(328.67337, dtype=float32)), 'eval/epoch_eval_time': 4.102380752563477, 'eval/sps': 31201.394439074422}
I0727 04:21:16.391457 140120985872192 train.py:379] starting iteration 81 840.7171108722687
I0727 04:21:26.314322 140120985872192 train.py:394] {'eval/walltime': 351.7717299461365, 'training/sps': 42348.946101394045, 'training/walltime': 493.37656116485596, 'training/entropy_loss': Array(0.01198622, dtype=float32), 'training/policy_loss': Array(0.00124521, dtype=float32), 'training/total_loss': Array(25.725212, dtype=float32), 'training/v_loss': Array(25.711979, dtype=float32), 'eval/episode_goal_distance': (Array(0.31763858, dtype=float32), Array(0.07454594, dtype=float32)), 'eval/episode_reward': (Array(-15848.896, dtype=float32), Array(8215.4375, dtype=float32)), 'eval/avg_episode_length': (Array(844.6875, dtype=float32), Array(360.9145, dtype=float32)), 'eval/epoch_eval_time': 4.115628719329834, 'eval/sps': 31100.958985640184}
I0727 04:21:26.319185 140120985872192 train.py:379] starting iteration 82 850.6448242664337
I0727 04:21:36.214195 140120985872192 train.py:394] {'eval/walltime': 355.87097358703613, 'training/sps': 42435.49619773741, 'training/walltime': 499.1679391860962, 'training/entropy_loss': Array(0.00705361, dtype=float32), 'training/policy_loss': Array(0.00140277, dtype=float32), 'training/total_loss': Array(26.017239, dtype=float32), 'training/v_loss': Array(26.00878, dtype=float32), 'eval/episode_goal_distance': (Array(0.31737384, dtype=float32), Array(0.06276072, dtype=float32)), 'eval/episode_reward': (Array(-17323.617, dtype=float32), Array(7277.659, dtype=float32)), 'eval/avg_episode_length': (Array(906.84375, dtype=float32), Array(289.63516, dtype=float32)), 'eval/epoch_eval_time': 4.099243640899658, 'eval/sps': 31225.27256562577}
I0727 04:21:36.216745 140120985872192 train.py:379] starting iteration 83 860.542398929596
I0727 04:21:46.107472 140120985872192 train.py:394] {'eval/walltime': 359.9612011909485, 'training/sps': 42396.572573774116, 'training/walltime': 504.96463418006897, 'training/entropy_loss': Array(0.0021051, dtype=float32), 'training/policy_loss': Array(0.00118237, dtype=float32), 'training/total_loss': Array(137.68474, dtype=float32), 'training/v_loss': Array(137.68146, dtype=float32), 'eval/episode_goal_distance': (Array(0.31031156, dtype=float32), Array(0.06807167, dtype=float32)), 'eval/episode_reward': (Array(-15670.738, dtype=float32), Array(7765.226, dtype=float32)), 'eval/avg_episode_length': (Array(844.7656, dtype=float32), Array(360.73315, dtype=float32)), 'eval/epoch_eval_time': 4.0902276039123535, 'eval/sps': 31294.1020391057}
I0727 04:21:46.109985 140120985872192 train.py:379] starting iteration 84 870.4356389045715
I0727 04:21:55.992641 140120985872192 train.py:394] {'eval/walltime': 364.05687141418457, 'training/sps': 42495.4532256224, 'training/walltime': 510.74784111976624, 'training/entropy_loss': Array(-0.0039719, dtype=float32), 'training/policy_loss': Array(0.00071027, dtype=float32), 'training/total_loss': Array(48.25267, dtype=float32), 'training/v_loss': Array(48.25593, dtype=float32), 'eval/episode_goal_distance': (Array(0.3129902, dtype=float32), Array(0.06325398, dtype=float32)), 'eval/episode_reward': (Array(-17414.953, dtype=float32), Array(6604.8477, dtype=float32)), 'eval/avg_episode_length': (Array(914.6094, dtype=float32), Array(278.4888, dtype=float32)), 'eval/epoch_eval_time': 4.095670223236084, 'eval/sps': 31252.51619962318}
I0727 04:21:55.995048 140120985872192 train.py:379] starting iteration 85 880.3207020759583
I0727 04:22:05.877968 140120985872192 train.py:394] {'eval/walltime': 368.14581084251404, 'training/sps': 42444.50552927483, 'training/walltime': 516.5379898548126, 'training/entropy_loss': Array(-0.00819753, dtype=float32), 'training/policy_loss': Array(0.0009653, dtype=float32), 'training/total_loss': Array(28.849113, dtype=float32), 'training/v_loss': Array(28.85635, dtype=float32), 'eval/episode_goal_distance': (Array(0.31280988, dtype=float32), Array(0.06359245, dtype=float32)), 'eval/episode_reward': (Array(-16662.082, dtype=float32), Array(6629.456, dtype=float32)), 'eval/avg_episode_length': (Array(906.84375, dtype=float32), Array(289.63507, dtype=float32)), 'eval/epoch_eval_time': 4.088939428329468, 'eval/sps': 31303.96090320523}
I0727 04:22:05.880396 140120985872192 train.py:379] starting iteration 86 890.2060503959656
I0727 04:22:15.819618 140120985872192 train.py:394] {'eval/walltime': 372.24948239326477, 'training/sps': 42141.75407760757, 'training/walltime': 522.3697357177734, 'training/entropy_loss': Array(-0.01238933, dtype=float32), 'training/policy_loss': Array(0.00054334, dtype=float32), 'training/total_loss': Array(27.54409, dtype=float32), 'training/v_loss': Array(27.555935, dtype=float32), 'eval/episode_goal_distance': (Array(0.31675908, dtype=float32), Array(0.0641405, dtype=float32)), 'eval/episode_reward': (Array(-16446.703, dtype=float32), Array(7160.7803, dtype=float32)), 'eval/avg_episode_length': (Array(875.8906, dtype=float32), Array(328.36328, dtype=float32)), 'eval/epoch_eval_time': 4.103671550750732, 'eval/sps': 31191.58012940472}
I0727 04:22:15.822008 140120985872192 train.py:379] starting iteration 87 900.1476621627808
I0727 04:22:25.759999 140120985872192 train.py:394] {'eval/walltime': 376.34804916381836, 'training/sps': 42113.727747385, 'training/walltime': 528.2053625583649, 'training/entropy_loss': Array(-0.0166943, dtype=float32), 'training/policy_loss': Array(0.00057661, dtype=float32), 'training/total_loss': Array(37.085167, dtype=float32), 'training/v_loss': Array(37.10128, dtype=float32), 'eval/episode_goal_distance': (Array(0.31989485, dtype=float32), Array(0.06536245, dtype=float32)), 'eval/episode_reward': (Array(-16330.521, dtype=float32), Array(7497.0264, dtype=float32)), 'eval/avg_episode_length': (Array(868., dtype=float32), Array(337.29675, dtype=float32)), 'eval/epoch_eval_time': 4.098566770553589, 'eval/sps': 31230.42935877587}
I0727 04:22:25.762469 140120985872192 train.py:379] starting iteration 88 910.088122844696
I0727 04:22:35.692985 140120985872192 train.py:394] {'eval/walltime': 380.45071482658386, 'training/sps': 42199.822554699604, 'training/walltime': 534.0290837287903, 'training/entropy_loss': Array(-0.02011, dtype=float32), 'training/policy_loss': Array(0.00079606, dtype=float32), 'training/total_loss': Array(27.499687, dtype=float32), 'training/v_loss': Array(27.519001, dtype=float32), 'eval/episode_goal_distance': (Array(0.30482656, dtype=float32), Array(0.05293966, dtype=float32)), 'eval/episode_reward': (Array(-16103.918, dtype=float32), Array(6531.762, dtype=float32)), 'eval/avg_episode_length': (Array(906.84375, dtype=float32), Array(289.63498, dtype=float32)), 'eval/epoch_eval_time': 4.102665662765503, 'eval/sps': 31199.227653788013}
I0727 04:22:35.695554 140120985872192 train.py:379] starting iteration 89 920.0212075710297
I0727 04:22:45.593496 140120985872192 train.py:394] {'eval/walltime': 384.58148097991943, 'training/sps': 42643.725551034106, 'training/walltime': 539.7921824455261, 'training/entropy_loss': Array(-0.02204932, dtype=float32), 'training/policy_loss': Array(0.00063467, dtype=float32), 'training/total_loss': Array(26.422142, dtype=float32), 'training/v_loss': Array(26.443558, dtype=float32), 'eval/episode_goal_distance': (Array(0.3091411, dtype=float32), Array(0.071689, dtype=float32)), 'eval/episode_reward': (Array(-16344.631, dtype=float32), Array(6965.0503, dtype=float32)), 'eval/avg_episode_length': (Array(891.28906, dtype=float32), Array(310.21478, dtype=float32)), 'eval/epoch_eval_time': 4.130766153335571, 'eval/sps': 30986.987703634757}
I0727 04:22:45.596138 140120985872192 train.py:379] starting iteration 90 929.921792268753
I0727 04:22:55.479403 140120985872192 train.py:394] {'eval/walltime': 388.67044734954834, 'training/sps': 42444.7589501867, 'training/walltime': 545.5822966098785, 'training/entropy_loss': Array(-0.02412364, dtype=float32), 'training/policy_loss': Array(0.00043737, dtype=float32), 'training/total_loss': Array(25.74992, dtype=float32), 'training/v_loss': Array(25.773605, dtype=float32), 'eval/episode_goal_distance': (Array(0.32683367, dtype=float32), Array(0.06813524, dtype=float32)), 'eval/episode_reward': (Array(-16740.695, dtype=float32), Array(7328.2437, dtype=float32)), 'eval/avg_episode_length': (Array(891.16406, dtype=float32), Array(310.57126, dtype=float32)), 'eval/epoch_eval_time': 4.088966369628906, 'eval/sps': 31303.75464829676}
I0727 04:22:55.481926 140120985872192 train.py:379] starting iteration 91 939.8075807094574
I0727 04:23:05.423603 140120985872192 train.py:394] {'eval/walltime': 392.7805263996124, 'training/sps': 42173.17518681349, 'training/walltime': 551.4096975326538, 'training/entropy_loss': Array(-0.02598753, dtype=float32), 'training/policy_loss': Array(0.00035377, dtype=float32), 'training/total_loss': Array(109.32375, dtype=float32), 'training/v_loss': Array(109.349396, dtype=float32), 'eval/episode_goal_distance': (Array(0.30711338, dtype=float32), Array(0.06464979, dtype=float32)), 'eval/episode_reward': (Array(-14481.566, dtype=float32), Array(7011.317, dtype=float32)), 'eval/avg_episode_length': (Array(844.66406, dtype=float32), Array(360.9689, dtype=float32)), 'eval/epoch_eval_time': 4.110079050064087, 'eval/sps': 31142.953320570352}
I0727 04:23:05.426209 140120985872192 train.py:379] starting iteration 92 949.7518630027771
I0727 04:23:15.308595 140120985872192 train.py:394] {'eval/walltime': 396.8676393032074, 'training/sps': 42438.25660272335, 'training/walltime': 557.2006988525391, 'training/entropy_loss': Array(-0.02795096, dtype=float32), 'training/policy_loss': Array(0.00030578, dtype=float32), 'training/total_loss': Array(58.698982, dtype=float32), 'training/v_loss': Array(58.72663, dtype=float32), 'eval/episode_goal_distance': (Array(0.31783286, dtype=float32), Array(0.06319158, dtype=float32)), 'eval/episode_reward': (Array(-16962.074, dtype=float32), Array(6947.455, dtype=float32)), 'eval/avg_episode_length': (Array(899.0547, dtype=float32), Array(300.23685, dtype=float32)), 'eval/epoch_eval_time': 4.087112903594971, 'eval/sps': 31317.950597208335}
I0727 04:23:15.311093 140120985872192 train.py:379] starting iteration 93 959.6367473602295
I0727 04:23:25.244888 140120985872192 train.py:394] {'eval/walltime': 400.96686577796936, 'training/sps': 42151.91977694153, 'training/walltime': 563.0310382843018, 'training/entropy_loss': Array(-0.02900216, dtype=float32), 'training/policy_loss': Array(0.00056803, dtype=float32), 'training/total_loss': Array(29.037842, dtype=float32), 'training/v_loss': Array(29.066277, dtype=float32), 'eval/episode_goal_distance': (Array(0.31114113, dtype=float32), Array(0.07157473, dtype=float32)), 'eval/episode_reward': (Array(-16419.168, dtype=float32), Array(7012.806, dtype=float32)), 'eval/avg_episode_length': (Array(891.3828, dtype=float32), Array(309.9477, dtype=float32)), 'eval/epoch_eval_time': 4.099226474761963, 'eval/sps': 31225.403326229447}
I0727 04:23:25.247475 140120985872192 train.py:379] starting iteration 94 969.5731294155121
I0727 04:23:35.179550 140120985872192 train.py:394] {'eval/walltime': 405.0890655517578, 'training/sps': 42328.70382017408, 'training/walltime': 568.8370275497437, 'training/entropy_loss': Array(-0.02972565, dtype=float32), 'training/policy_loss': Array(0.00085966, dtype=float32), 'training/total_loss': Array(25.769035, dtype=float32), 'training/v_loss': Array(25.797901, dtype=float32), 'eval/episode_goal_distance': (Array(0.31192037, dtype=float32), Array(0.06416865, dtype=float32)), 'eval/episode_reward': (Array(-17033.75, dtype=float32), Array(6508.0137, dtype=float32)), 'eval/avg_episode_length': (Array(914.53906, dtype=float32), Array(278.71823, dtype=float32)), 'eval/epoch_eval_time': 4.122199773788452, 'eval/sps': 31051.382034879724}
I0727 04:23:35.181928 140120985872192 train.py:379] starting iteration 95 979.5075826644897
I0727 04:23:45.072557 140120985872192 train.py:394] {'eval/walltime': 409.1793444156647, 'training/sps': 42397.270095165484, 'training/walltime': 574.6336271762848, 'training/entropy_loss': Array(-0.03182014, dtype=float32), 'training/policy_loss': Array(0.00020928, dtype=float32), 'training/total_loss': Array(31.507221, dtype=float32), 'training/v_loss': Array(31.538834, dtype=float32), 'eval/episode_goal_distance': (Array(0.31085536, dtype=float32), Array(0.06376665, dtype=float32)), 'eval/episode_reward': (Array(-15610.383, dtype=float32), Array(7353.1123, dtype=float32)), 'eval/avg_episode_length': (Array(852.46875, dtype=float32), Array(353.36316, dtype=float32)), 'eval/epoch_eval_time': 4.09027886390686, 'eval/sps': 31293.70985667707}
I0727 04:23:45.075072 140120985872192 train.py:379] starting iteration 96 989.4007263183594
I0727 04:23:54.968225 140120985872192 train.py:394] {'eval/walltime': 413.2828896045685, 'training/sps': 42476.96452058562, 'training/walltime': 580.4193513393402, 'training/entropy_loss': Array(-0.03331445, dtype=float32), 'training/policy_loss': Array(0.00113007, dtype=float32), 'training/total_loss': Array(30.117031, dtype=float32), 'training/v_loss': Array(30.149212, dtype=float32), 'eval/episode_goal_distance': (Array(0.31482658, dtype=float32), Array(0.05485509, dtype=float32)), 'eval/episode_reward': (Array(-16354.681, dtype=float32), Array(7073.386, dtype=float32)), 'eval/avg_episode_length': (Array(875.6719, dtype=float32), Array(328.94186, dtype=float32)), 'eval/epoch_eval_time': 4.103545188903809, 'eval/sps': 31192.54062222061}
I0727 04:23:54.970686 140120985872192 train.py:379] starting iteration 97 999.2963399887085
I0727 04:24:04.872018 140120985872192 train.py:394] {'eval/walltime': 417.37350511550903, 'training/sps': 42321.93984096552, 'training/walltime': 586.226268529892, 'training/entropy_loss': Array(-0.03372524, dtype=float32), 'training/policy_loss': Array(0.00077102, dtype=float32), 'training/total_loss': Array(24.092833, dtype=float32), 'training/v_loss': Array(24.125786, dtype=float32), 'eval/episode_goal_distance': (Array(0.3129444, dtype=float32), Array(0.06847952, dtype=float32)), 'eval/episode_reward': (Array(-16104.032, dtype=float32), Array(6496.1626, dtype=float32)), 'eval/avg_episode_length': (Array(906.9375, dtype=float32), Array(289.34357, dtype=float32)), 'eval/epoch_eval_time': 4.090615510940552, 'eval/sps': 31291.13446562204}
I0727 04:24:04.874436 140120985872192 train.py:379] starting iteration 98 1009.2000896930695
I0727 04:24:14.829514 140120985872192 train.py:394] {'eval/walltime': 421.48622846603394, 'training/sps': 42093.06677400453, 'training/walltime': 592.0647597312927, 'training/entropy_loss': Array(-0.0345769, dtype=float32), 'training/policy_loss': Array(0.00066002, dtype=float32), 'training/total_loss': Array(22.22261, dtype=float32), 'training/v_loss': Array(22.256527, dtype=float32), 'eval/episode_goal_distance': (Array(0.315318, dtype=float32), Array(0.05690706, dtype=float32)), 'eval/episode_reward': (Array(-16040.959, dtype=float32), Array(7445.6416, dtype=float32)), 'eval/avg_episode_length': (Array(860.27344, dtype=float32), Array(345.41406, dtype=float32)), 'eval/epoch_eval_time': 4.112723350524902, 'eval/sps': 31122.929769556104}
I0727 04:24:14.831941 140120985872192 train.py:379] starting iteration 99 1019.1575951576233
I0727 04:24:24.724470 140120985872192 train.py:394] {'eval/walltime': 425.5779502391815, 'training/sps': 42394.35112109606, 'training/walltime': 597.8617584705353, 'training/entropy_loss': Array(-0.03449456, dtype=float32), 'training/policy_loss': Array(0.00129261, dtype=float32), 'training/total_loss': Array(21.723297, dtype=float32), 'training/v_loss': Array(21.7565, dtype=float32), 'eval/episode_goal_distance': (Array(0.3005799, dtype=float32), Array(0.06341513, dtype=float32)), 'eval/episode_reward': (Array(-15584.149, dtype=float32), Array(6440.3237, dtype=float32)), 'eval/avg_episode_length': (Array(883.5703, dtype=float32), Array(319.56433, dtype=float32)), 'eval/epoch_eval_time': 4.091721773147583, 'eval/sps': 31282.67440861092}
I0727 04:24:24.726938 140120985872192 train.py:379] starting iteration 100 1029.0525918006897
I0727 04:24:34.675550 140120985872192 train.py:394] {'eval/walltime': 429.684490442276, 'training/sps': 42094.567425205, 'training/walltime': 603.7000415325165, 'training/entropy_loss': Array(-0.03544152, dtype=float32), 'training/policy_loss': Array(0.00081152, dtype=float32), 'training/total_loss': Array(137.49036, dtype=float32), 'training/v_loss': Array(137.525, dtype=float32), 'eval/episode_goal_distance': (Array(0.31811312, dtype=float32), Array(0.06387381, dtype=float32)), 'eval/episode_reward': (Array(-15154.652, dtype=float32), Array(7287.052, dtype=float32)), 'eval/avg_episode_length': (Array(844.83594, dtype=float32), Array(360.5699, dtype=float32)), 'eval/epoch_eval_time': 4.106540203094482, 'eval/sps': 31169.79103322686}
I0727 04:24:34.677817 140120985872192 train.py:379] starting iteration 101 1039.0034708976746
I0727 04:24:44.586014 140120985872192 train.py:394] {'eval/walltime': 433.7936797142029, 'training/sps': 42407.489670603856, 'training/walltime': 609.4952442646027, 'training/entropy_loss': Array(-0.0372737, dtype=float32), 'training/policy_loss': Array(0.0008593, dtype=float32), 'training/total_loss': Array(31.130829, dtype=float32), 'training/v_loss': Array(31.167242, dtype=float32), 'eval/episode_goal_distance': (Array(0.31334686, dtype=float32), Array(0.06291905, dtype=float32)), 'eval/episode_reward': (Array(-14765.954, dtype=float32), Array(7338.022, dtype=float32)), 'eval/avg_episode_length': (Array(829.22656, dtype=float32), Array(374.85458, dtype=float32)), 'eval/epoch_eval_time': 4.10918927192688, 'eval/sps': 31149.696820847163}
I0727 04:24:44.588262 140120985872192 train.py:379] starting iteration 102 1048.913916349411
I0727 04:24:54.525676 140120985872192 train.py:394] {'eval/walltime': 437.8937714099884, 'training/sps': 42128.7658711566, 'training/walltime': 615.3287880420685, 'training/entropy_loss': Array(-0.03772463, dtype=float32), 'training/policy_loss': Array(0.00189298, dtype=float32), 'training/total_loss': Array(24.304932, dtype=float32), 'training/v_loss': Array(24.340763, dtype=float32), 'eval/episode_goal_distance': (Array(0.31167895, dtype=float32), Array(0.0658197, dtype=float32)), 'eval/episode_reward': (Array(-15249.277, dtype=float32), Array(6947.413, dtype=float32)), 'eval/avg_episode_length': (Array(860.27344, dtype=float32), Array(345.4141, dtype=float32)), 'eval/epoch_eval_time': 4.1000916957855225, 'eval/sps': 31218.813991787305}
I0727 04:24:54.622693 140120985872192 train.py:379] starting iteration 103 1058.9483294487
I0727 04:25:04.546874 140120985872192 train.py:394] {'eval/walltime': 441.99039363861084, 'training/sps': 42202.4642627401, 'training/walltime': 621.1521446704865, 'training/entropy_loss': Array(-0.03730933, dtype=float32), 'training/policy_loss': Array(0.00151958, dtype=float32), 'training/total_loss': Array(27.825676, dtype=float32), 'training/v_loss': Array(27.86147, dtype=float32), 'eval/episode_goal_distance': (Array(0.31295145, dtype=float32), Array(0.06562256, dtype=float32)), 'eval/episode_reward': (Array(-14930.267, dtype=float32), Array(7489.2446, dtype=float32)), 'eval/avg_episode_length': (Array(836.96094, dtype=float32), Array(368.02344, dtype=float32)), 'eval/epoch_eval_time': 4.0966222286224365, 'eval/sps': 31245.25349339871}
I0727 04:25:04.549604 140120985872192 train.py:379] starting iteration 104 1068.8752584457397
I0727 04:25:14.436346 140120985872192 train.py:394] {'eval/walltime': 446.10078740119934, 'training/sps': 42575.023397854726, 'training/walltime': 626.9245431423187, 'training/entropy_loss': Array(-0.03632936, dtype=float32), 'training/policy_loss': Array(0.00245837, dtype=float32), 'training/total_loss': Array(29.101784, dtype=float32), 'training/v_loss': Array(29.135654, dtype=float32), 'eval/episode_goal_distance': (Array(0.3086167, dtype=float32), Array(0.06431952, dtype=float32)), 'eval/episode_reward': (Array(-15907.145, dtype=float32), Array(6882.536, dtype=float32)), 'eval/avg_episode_length': (Array(883.4844, dtype=float32), Array(319.7997, dtype=float32)), 'eval/epoch_eval_time': 4.110393762588501, 'eval/sps': 31140.568858637183}
I0727 04:25:14.438822 140120985872192 train.py:379] starting iteration 105 1078.764476299286
I0727 04:25:24.320355 140120985872192 train.py:394] {'eval/walltime': 450.18797874450684, 'training/sps': 42442.48351357596, 'training/walltime': 632.7149677276611, 'training/entropy_loss': Array(-0.03714342, dtype=float32), 'training/policy_loss': Array(0.00094184, dtype=float32), 'training/total_loss': Array(22.666538, dtype=float32), 'training/v_loss': Array(22.70274, dtype=float32), 'eval/episode_goal_distance': (Array(0.31716353, dtype=float32), Array(0.06458849, dtype=float32)), 'eval/episode_reward': (Array(-15931.492, dtype=float32), Array(6673.1445, dtype=float32)), 'eval/avg_episode_length': (Array(891.25, dtype=float32), Array(310.3264, dtype=float32)), 'eval/epoch_eval_time': 4.087191343307495, 'eval/sps': 31317.34955584879}
I0727 04:25:24.323118 140120985872192 train.py:379] starting iteration 106 1088.648772239685
I0727 04:25:34.247099 140120985872192 train.py:394] {'eval/walltime': 454.28590083122253, 'training/sps': 42210.64891873086, 'training/walltime': 638.5371952056885, 'training/entropy_loss': Array(-0.0378457, dtype=float32), 'training/policy_loss': Array(0.00113323, dtype=float32), 'training/total_loss': Array(22.69331, dtype=float32), 'training/v_loss': Array(22.730026, dtype=float32), 'eval/episode_goal_distance': (Array(0.30218866, dtype=float32), Array(0.06101248, dtype=float32)), 'eval/episode_reward': (Array(-14287.853, dtype=float32), Array(6850.3145, dtype=float32)), 'eval/avg_episode_length': (Array(844.7656, dtype=float32), Array(360.73276, dtype=float32)), 'eval/epoch_eval_time': 4.097922086715698, 'eval/sps': 31235.342520283564}
I0727 04:25:34.249749 140120985872192 train.py:379] starting iteration 107 1098.5754034519196
I0727 04:25:44.203170 140120985872192 train.py:394] {'eval/walltime': 458.42456674575806, 'training/sps': 42292.9584740545, 'training/walltime': 644.3480916023254, 'training/entropy_loss': Array(-0.03820091, dtype=float32), 'training/policy_loss': Array(0.00128025, dtype=float32), 'training/total_loss': Array(21.171047, dtype=float32), 'training/v_loss': Array(21.207968, dtype=float32), 'eval/episode_goal_distance': (Array(0.31311572, dtype=float32), Array(0.06624308, dtype=float32)), 'eval/episode_reward': (Array(-15595.26, dtype=float32), Array(6517.4805, dtype=float32)), 'eval/avg_episode_length': (Array(883.4453, dtype=float32), Array(319.90744, dtype=float32)), 'eval/epoch_eval_time': 4.1386659145355225, 'eval/sps': 30927.840672146955}
I0727 04:25:44.205608 140120985872192 train.py:379] starting iteration 108 1108.531262397766
I0727 04:25:54.136768 140120985872192 train.py:394] {'eval/walltime': 462.52779960632324, 'training/sps': 42196.0842996418, 'training/walltime': 650.172328710556, 'training/entropy_loss': Array(-0.03724096, dtype=float32), 'training/policy_loss': Array(0.00097684, dtype=float32), 'training/total_loss': Array(129.13123, dtype=float32), 'training/v_loss': Array(129.1675, dtype=float32), 'eval/episode_goal_distance': (Array(0.3162418, dtype=float32), Array(0.06293046, dtype=float32)), 'eval/episode_reward': (Array(-15593.885, dtype=float32), Array(6939.162, dtype=float32)), 'eval/avg_episode_length': (Array(875.6719, dtype=float32), Array(328.9418, dtype=float32)), 'eval/epoch_eval_time': 4.1032328605651855, 'eval/sps': 31194.914924318746}
I0727 04:25:54.139268 140120985872192 train.py:379] starting iteration 109 1118.4649221897125
I0727 04:26:04.070147 140120985872192 train.py:394] {'eval/walltime': 466.66156339645386, 'training/sps': 42421.73796615602, 'training/walltime': 655.9655849933624, 'training/entropy_loss': Array(-0.03887153, dtype=float32), 'training/policy_loss': Array(0.00112801, dtype=float32), 'training/total_loss': Array(37.289055, dtype=float32), 'training/v_loss': Array(37.326797, dtype=float32), 'eval/episode_goal_distance': (Array(0.30562538, dtype=float32), Array(0.06074185, dtype=float32)), 'eval/episode_reward': (Array(-15989.692, dtype=float32), Array(5804.622, dtype=float32)), 'eval/avg_episode_length': (Array(914.625, dtype=float32), Array(278.43765, dtype=float32)), 'eval/epoch_eval_time': 4.133763790130615, 'eval/sps': 30964.517204780965}
I0727 04:26:04.072676 140120985872192 train.py:379] starting iteration 110 1128.398330450058
I0727 04:26:14.009098 140120985872192 train.py:394] {'eval/walltime': 470.7597379684448, 'training/sps': 42128.82785657125, 'training/walltime': 661.7991201877594, 'training/entropy_loss': Array(-0.03974753, dtype=float32), 'training/policy_loss': Array(0.00070415, dtype=float32), 'training/total_loss': Array(21.990803, dtype=float32), 'training/v_loss': Array(22.029846, dtype=float32), 'eval/episode_goal_distance': (Array(0.3091312, dtype=float32), Array(0.06834988, dtype=float32)), 'eval/episode_reward': (Array(-15223.124, dtype=float32), Array(6587.8506, dtype=float32)), 'eval/avg_episode_length': (Array(875.7031, dtype=float32), Array(328.8591, dtype=float32)), 'eval/epoch_eval_time': 4.098174571990967, 'eval/sps': 31233.418135678712}
I0727 04:26:14.011706 140120985872192 train.py:379] starting iteration 111 1138.3373591899872
I0727 04:26:23.899755 140120985872192 train.py:394] {'eval/walltime': 474.8527510166168, 'training/sps': 42436.504228059166, 'training/walltime': 667.5903606414795, 'training/entropy_loss': Array(-0.03930926, dtype=float32), 'training/policy_loss': Array(0.00353868, dtype=float32), 'training/total_loss': Array(20.765226, dtype=float32), 'training/v_loss': Array(20.800999, dtype=float32), 'eval/episode_goal_distance': (Array(0.31435242, dtype=float32), Array(0.06086566, dtype=float32)), 'eval/episode_reward': (Array(-15038.042, dtype=float32), Array(6434.5728, dtype=float32)), 'eval/avg_episode_length': (Array(875.6406, dtype=float32), Array(329.02444, dtype=float32)), 'eval/epoch_eval_time': 4.093013048171997, 'eval/sps': 31272.805264367966}
I0727 04:26:23.902308 140120985872192 train.py:379] starting iteration 112 1148.227962732315
I0727 04:26:33.847468 140120985872192 train.py:394] {'eval/walltime': 478.9477231502533, 'training/sps': 42035.98406486412, 'training/walltime': 673.4367802143097, 'training/entropy_loss': Array(-0.04057144, dtype=float32), 'training/policy_loss': Array(0.00093357, dtype=float32), 'training/total_loss': Array(26.699192, dtype=float32), 'training/v_loss': Array(26.73883, dtype=float32), 'eval/episode_goal_distance': (Array(0.30686283, dtype=float32), Array(0.07214487, dtype=float32)), 'eval/episode_reward': (Array(-14784.166, dtype=float32), Array(6772.4844, dtype=float32)), 'eval/avg_episode_length': (Array(860.14844, dtype=float32), Array(345.7228, dtype=float32)), 'eval/epoch_eval_time': 4.094972133636475, 'eval/sps': 31257.843966408545}
I0727 04:26:33.850002 140120985872192 train.py:379] starting iteration 113 1158.175656080246
I0727 04:26:43.807774 140120985872192 train.py:394] {'eval/walltime': 483.09310483932495, 'training/sps': 42309.097360057225, 'training/walltime': 679.2454600334167, 'training/entropy_loss': Array(-0.03832454, dtype=float32), 'training/policy_loss': Array(0.00136952, dtype=float32), 'training/total_loss': Array(23.784187, dtype=float32), 'training/v_loss': Array(23.821142, dtype=float32), 'eval/episode_goal_distance': (Array(0.29901192, dtype=float32), Array(0.06397372, dtype=float32)), 'eval/episode_reward': (Array(-14206.719, dtype=float32), Array(6869.9126, dtype=float32)), 'eval/avg_episode_length': (Array(844.6953, dtype=float32), Array(360.89658, dtype=float32)), 'eval/epoch_eval_time': 4.145381689071655, 'eval/sps': 30877.73565880376}
I0727 04:26:43.810135 140120985872192 train.py:379] starting iteration 114 1168.135788679123
I0727 04:26:53.706011 140120985872192 train.py:394] {'eval/walltime': 487.20177698135376, 'training/sps': 42493.114543519354, 'training/walltime': 685.0289852619171, 'training/entropy_loss': Array(-0.03848433, dtype=float32), 'training/policy_loss': Array(0.00273213, dtype=float32), 'training/total_loss': Array(18.652578, dtype=float32), 'training/v_loss': Array(18.688332, dtype=float32), 'eval/episode_goal_distance': (Array(0.31033134, dtype=float32), Array(0.06112593, dtype=float32)), 'eval/episode_reward': (Array(-14587.367, dtype=float32), Array(7211.488, dtype=float32)), 'eval/avg_episode_length': (Array(837.1094, dtype=float32), Array(367.68796, dtype=float32)), 'eval/epoch_eval_time': 4.108672142028809, 'eval/sps': 31153.61741586791}
I0727 04:26:53.708512 140120985872192 train.py:379] starting iteration 115 1178.0341663360596
I0727 04:27:03.664283 140120985872192 train.py:394] {'eval/walltime': 491.32137084007263, 'training/sps': 42135.766210283684, 'training/walltime': 690.8615598678589, 'training/entropy_loss': Array(-0.04001921, dtype=float32), 'training/policy_loss': Array(0.00099768, dtype=float32), 'training/total_loss': Array(17.765053, dtype=float32), 'training/v_loss': Array(17.804073, dtype=float32), 'eval/episode_goal_distance': (Array(0.29319412, dtype=float32), Array(0.06183702, dtype=float32)), 'eval/episode_reward': (Array(-15213.712, dtype=float32), Array(5956.2036, dtype=float32)), 'eval/avg_episode_length': (Array(906.83594, dtype=float32), Array(289.65924, dtype=float32)), 'eval/epoch_eval_time': 4.119593858718872, 'eval/sps': 31071.02408386587}
I0727 04:27:03.666539 140120985872192 train.py:379] starting iteration 116 1187.9921927452087
I0727 04:27:13.626437 140120985872192 train.py:394] {'eval/walltime': 495.4355261325836, 'training/sps': 42067.72486110873, 'training/walltime': 696.7035682201385, 'training/entropy_loss': Array(-0.03916422, dtype=float32), 'training/policy_loss': Array(0.00122874, dtype=float32), 'training/total_loss': Array(98.4981, dtype=float32), 'training/v_loss': Array(98.53603, dtype=float32), 'eval/episode_goal_distance': (Array(0.303755, dtype=float32), Array(0.05117618, dtype=float32)), 'eval/episode_reward': (Array(-14393.173, dtype=float32), Array(6604.884, dtype=float32)), 'eval/avg_episode_length': (Array(852.4531, dtype=float32), Array(353.40057, dtype=float32)), 'eval/epoch_eval_time': 4.114155292510986, 'eval/sps': 31112.097356412123}
I0727 04:27:13.628766 140120985872192 train.py:379] starting iteration 117 1197.954419851303
I0727 04:27:23.512016 140120985872192 train.py:394] {'eval/walltime': 499.5237720012665, 'training/sps': 42435.62547451237, 'training/walltime': 702.4949285984039, 'training/entropy_loss': Array(-0.04026082, dtype=float32), 'training/policy_loss': Array(0.00048228, dtype=float32), 'training/total_loss': Array(54.28176, dtype=float32), 'training/v_loss': Array(54.32154, dtype=float32), 'eval/episode_goal_distance': (Array(0.29456156, dtype=float32), Array(0.05532156, dtype=float32)), 'eval/episode_reward': (Array(-16113.497, dtype=float32), Array(5130.378, dtype=float32)), 'eval/avg_episode_length': (Array(937.9297, dtype=float32), Array(240.39766, dtype=float32)), 'eval/epoch_eval_time': 4.088245868682861, 'eval/sps': 31309.27153391551}
I0727 04:27:23.514281 140120985872192 train.py:379] starting iteration 118 1207.8399350643158
I0727 04:27:33.452812 140120985872192 train.py:394] {'eval/walltime': 503.62533617019653, 'training/sps': 42130.41887797192, 'training/walltime': 708.3282434940338, 'training/entropy_loss': Array(-0.04284816, dtype=float32), 'training/policy_loss': Array(-9.169154e-05, dtype=float32), 'training/total_loss': Array(21.340925, dtype=float32), 'training/v_loss': Array(21.383865, dtype=float32), 'eval/episode_goal_distance': (Array(0.30900925, dtype=float32), Array(0.05875684, dtype=float32)), 'eval/episode_reward': (Array(-15228.907, dtype=float32), Array(5942.0547, dtype=float32)), 'eval/avg_episode_length': (Array(899.10156, dtype=float32), Array(300.09766, dtype=float32)), 'eval/epoch_eval_time': 4.101564168930054, 'eval/sps': 31207.606349211517}
I0727 04:27:33.455116 140120985872192 train.py:379] starting iteration 119 1217.7807693481445
I0727 04:27:43.405388 140120985872192 train.py:394] {'eval/walltime': 507.7613227367401, 'training/sps': 42296.18804302935, 'training/walltime': 714.1386961936951, 'training/entropy_loss': Array(-0.04299773, dtype=float32), 'training/policy_loss': Array(-0.00020689, dtype=float32), 'training/total_loss': Array(16.857626, dtype=float32), 'training/v_loss': Array(16.90083, dtype=float32), 'eval/episode_goal_distance': (Array(0.31739283, dtype=float32), Array(0.06426498, dtype=float32)), 'eval/episode_reward': (Array(-15487.605, dtype=float32), Array(6473.9414, dtype=float32)), 'eval/avg_episode_length': (Array(883.4531, dtype=float32), Array(319.88577, dtype=float32)), 'eval/epoch_eval_time': 4.135986566543579, 'eval/sps': 30947.876145296304}
I0727 04:27:43.407623 140120985872192 train.py:379] starting iteration 120 1227.7332770824432
I0727 04:27:53.336499 140120985872192 train.py:394] {'eval/walltime': 511.859840631485, 'training/sps': 42178.48505414379, 'training/walltime': 719.9653635025024, 'training/entropy_loss': Array(-0.04275827, dtype=float32), 'training/policy_loss': Array(-0.00027508, dtype=float32), 'training/total_loss': Array(20.939045, dtype=float32), 'training/v_loss': Array(20.982079, dtype=float32), 'eval/episode_goal_distance': (Array(0.30485207, dtype=float32), Array(0.05256738, dtype=float32)), 'eval/episode_reward': (Array(-15780.042, dtype=float32), Array(5622.311, dtype=float32)), 'eval/avg_episode_length': (Array(914.5703, dtype=float32), Array(278.61584, dtype=float32)), 'eval/epoch_eval_time': 4.098517894744873, 'eval/sps': 31230.801789135003}
I0727 04:27:53.338995 140120985872192 train.py:379] starting iteration 121 1237.6646494865417
I0727 04:28:03.262334 140120985872192 train.py:394] {'eval/walltime': 515.9980945587158, 'training/sps': 42508.23030184414, 'training/walltime': 725.7468321323395, 'training/entropy_loss': Array(-0.04202498, dtype=float32), 'training/policy_loss': Array(-0.00031879, dtype=float32), 'training/total_loss': Array(22.095406, dtype=float32), 'training/v_loss': Array(22.13775, dtype=float32), 'eval/episode_goal_distance': (Array(0.2982876, dtype=float32), Array(0.06294052, dtype=float32)), 'eval/episode_reward': (Array(-13806.512, dtype=float32), Array(6354.7793, dtype=float32)), 'eval/avg_episode_length': (Array(852.41406, dtype=float32), Array(353.4942, dtype=float32)), 'eval/epoch_eval_time': 4.138253927230835, 'eval/sps': 30930.919718996756}
I0727 04:28:03.264683 140120985872192 train.py:379] starting iteration 122 1247.5903372764587
I0727 04:28:13.204595 140120985872192 train.py:394] {'eval/walltime': 520.1094331741333, 'training/sps': 42191.23108425372, 'training/walltime': 731.5717391967773, 'training/entropy_loss': Array(-0.04075272, dtype=float32), 'training/policy_loss': Array(0.00019305, dtype=float32), 'training/total_loss': Array(17.624454, dtype=float32), 'training/v_loss': Array(17.665014, dtype=float32), 'eval/episode_goal_distance': (Array(0.29612368, dtype=float32), Array(0.058266, dtype=float32)), 'eval/episode_reward': (Array(-14471.045, dtype=float32), Array(5827.902, dtype=float32)), 'eval/avg_episode_length': (Array(891.2422, dtype=float32), Array(310.34818, dtype=float32)), 'eval/epoch_eval_time': 4.1113386154174805, 'eval/sps': 31133.412246804783}
I0727 04:28:13.206956 140120985872192 train.py:379] starting iteration 123 1257.5326101779938
I0727 04:28:23.108135 140120985872192 train.py:394] {'eval/walltime': 524.213484287262, 'training/sps': 42420.52987546591, 'training/walltime': 737.3651604652405, 'training/entropy_loss': Array(-0.03934457, dtype=float32), 'training/policy_loss': Array(1.7642875e-05, dtype=float32), 'training/total_loss': Array(15.995634, dtype=float32), 'training/v_loss': Array(16.034962, dtype=float32), 'eval/episode_goal_distance': (Array(0.30654502, dtype=float32), Array(0.05654751, dtype=float32)), 'eval/episode_reward': (Array(-13910.305, dtype=float32), Array(6402.1665, dtype=float32)), 'eval/avg_episode_length': (Array(844.625, dtype=float32), Array(361.05957, dtype=float32)), 'eval/epoch_eval_time': 4.104051113128662, 'eval/sps': 31188.695382115042}
I0727 04:28:23.110455 140120985872192 train.py:379] starting iteration 124 1267.4361090660095
I0727 04:28:33.014802 140120985872192 train.py:394] {'eval/walltime': 528.3022882938385, 'training/sps': 42284.656875733235, 'training/walltime': 743.1771976947784, 'training/entropy_loss': Array(-0.03880066, dtype=float32), 'training/policy_loss': Array(0.00023339, dtype=float32), 'training/total_loss': Array(16.516869, dtype=float32), 'training/v_loss': Array(16.555435, dtype=float32), 'eval/episode_goal_distance': (Array(0.2979859, dtype=float32), Array(0.06439797, dtype=float32)), 'eval/episode_reward': (Array(-13526.627, dtype=float32), Array(6504.784, dtype=float32)), 'eval/avg_episode_length': (Array(844.6719, dtype=float32), Array(360.95062, dtype=float32)), 'eval/epoch_eval_time': 4.088804006576538, 'eval/sps': 31304.997694710113}
I0727 04:28:33.017096 140120985872192 train.py:379] starting iteration 125 1277.3427505493164
I0727 04:28:43.014408 140120985872192 train.py:394] {'eval/walltime': 532.4522225856781, 'training/sps': 42055.700197856684, 'training/walltime': 749.0208764076233, 'training/entropy_loss': Array(-0.04110846, dtype=float32), 'training/policy_loss': Array(-9.090238e-05, dtype=float32), 'training/total_loss': Array(109.7988, dtype=float32), 'training/v_loss': Array(109.84, dtype=float32), 'eval/episode_goal_distance': (Array(0.2988706, dtype=float32), Array(0.05322324, dtype=float32)), 'eval/episode_reward': (Array(-14868.103, dtype=float32), Array(5709.064, dtype=float32)), 'eval/avg_episode_length': (Array(899.02344, dtype=float32), Array(300.32962, dtype=float32)), 'eval/epoch_eval_time': 4.1499342918396, 'eval/sps': 30843.86185383664}
I0727 04:28:43.016656 140120985872192 train.py:379] starting iteration 126 1287.3423101902008
I0727 04:28:52.919308 140120985872192 train.py:394] {'eval/walltime': 536.554740190506, 'training/sps': 42398.98086355058, 'training/walltime': 754.8172421455383, 'training/entropy_loss': Array(-0.04157431, dtype=float32), 'training/policy_loss': Array(0.00018707, dtype=float32), 'training/total_loss': Array(26.512383, dtype=float32), 'training/v_loss': Array(26.553768, dtype=float32), 'eval/episode_goal_distance': (Array(0.29218078, dtype=float32), Array(0.05958986, dtype=float32)), 'eval/episode_reward': (Array(-13281.529, dtype=float32), Array(6446.2603, dtype=float32)), 'eval/avg_episode_length': (Array(836.9297, dtype=float32), Array(368.09317, dtype=float32)), 'eval/epoch_eval_time': 4.102517604827881, 'eval/sps': 31200.353619291825}
I0727 04:28:52.921710 140120985872192 train.py:379] starting iteration 127 1297.247364282608
I0727 04:29:02.836911 140120985872192 train.py:394] {'eval/walltime': 540.6477296352386, 'training/sps': 42237.36098565738, 'training/walltime': 760.63578748703, 'training/entropy_loss': Array(-0.0412201, dtype=float32), 'training/policy_loss': Array(-3.4827874e-05, dtype=float32), 'training/total_loss': Array(14.3454895, dtype=float32), 'training/v_loss': Array(14.386745, dtype=float32), 'eval/episode_goal_distance': (Array(0.3090435, dtype=float32), Array(0.06244441, dtype=float32)), 'eval/episode_reward': (Array(-14588.519, dtype=float32), Array(6311.0933, dtype=float32)), 'eval/avg_episode_length': (Array(875.7344, dtype=float32), Array(328.77658, dtype=float32)), 'eval/epoch_eval_time': 4.092989444732666, 'eval/sps': 31272.985608288156}
I0727 04:29:02.839339 140120985872192 train.py:379] starting iteration 128 1307.1649928092957
I0727 04:29:12.806910 140120985872192 train.py:394] {'eval/walltime': 544.7572193145752, 'training/sps': 41978.07297024006, 'training/walltime': 766.4902725219727, 'training/entropy_loss': Array(-0.04267208, dtype=float32), 'training/policy_loss': Array(-0.00036533, dtype=float32), 'training/total_loss': Array(15.99207, dtype=float32), 'training/v_loss': Array(16.035107, dtype=float32), 'eval/episode_goal_distance': (Array(0.3071403, dtype=float32), Array(0.05607656, dtype=float32)), 'eval/episode_reward': (Array(-15514.297, dtype=float32), Array(4797.4688, dtype=float32)), 'eval/avg_episode_length': (Array(937.8828, dtype=float32), Array(240.57939, dtype=float32)), 'eval/epoch_eval_time': 4.109489679336548, 'eval/sps': 31147.41974985683}
I0727 04:29:12.809302 140120985872192 train.py:379] starting iteration 129 1317.1349565982819
I0727 04:29:22.724365 140120985872192 train.py:394] {'eval/walltime': 548.8676488399506, 'training/sps': 42366.11677182035, 'training/walltime': 772.291134595871, 'training/entropy_loss': Array(-0.04266076, dtype=float32), 'training/policy_loss': Array(-0.00033372, dtype=float32), 'training/total_loss': Array(18.505297, dtype=float32), 'training/v_loss': Array(18.54829, dtype=float32), 'eval/episode_goal_distance': (Array(0.30422702, dtype=float32), Array(0.05301762, dtype=float32)), 'eval/episode_reward': (Array(-14369.398, dtype=float32), Array(5749.5454, dtype=float32)), 'eval/avg_episode_length': (Array(883.52344, dtype=float32), Array(319.69324, dtype=float32)), 'eval/epoch_eval_time': 4.110429525375366, 'eval/sps': 31140.29792015738}
I0727 04:29:22.726699 140120985872192 train.py:379] starting iteration 130 1327.0523529052734
I0727 04:29:32.621454 140120985872192 train.py:394] {'eval/walltime': 552.9583084583282, 'training/sps': 42369.56650934016, 'training/walltime': 778.0915243625641, 'training/entropy_loss': Array(-0.04407555, dtype=float32), 'training/policy_loss': Array(-0.00012183, dtype=float32), 'training/total_loss': Array(16.803493, dtype=float32), 'training/v_loss': Array(16.84769, dtype=float32), 'eval/episode_goal_distance': (Array(0.30478027, dtype=float32), Array(0.06367567, dtype=float32)), 'eval/episode_reward': (Array(-13395.721, dtype=float32), Array(6728.9897, dtype=float32)), 'eval/avg_episode_length': (Array(821.33594, dtype=float32), Array(381.74075, dtype=float32)), 'eval/epoch_eval_time': 4.0906596183776855, 'eval/sps': 31290.797069731143}
I0727 04:29:32.624060 140120985872192 train.py:379] starting iteration 131 1336.949714422226
I0727 04:29:42.582028 140120985872192 train.py:394] {'eval/walltime': 557.0650382041931, 'training/sps': 42027.87213807297, 'training/walltime': 783.9390723705292, 'training/entropy_loss': Array(-0.04400544, dtype=float32), 'training/policy_loss': Array(-0.000233, dtype=float32), 'training/total_loss': Array(15.874306, dtype=float32), 'training/v_loss': Array(15.918544, dtype=float32), 'eval/episode_goal_distance': (Array(0.30236873, dtype=float32), Array(0.0590983, dtype=float32)), 'eval/episode_reward': (Array(-15445.664, dtype=float32), Array(4899.292, dtype=float32)), 'eval/avg_episode_length': (Array(937.90625, dtype=float32), Array(240.48856, dtype=float32)), 'eval/epoch_eval_time': 4.106729745864868, 'eval/sps': 31168.352416879938}
I0727 04:29:42.584597 140120985872192 train.py:379] starting iteration 132 1346.9102511405945
I0727 04:29:52.502673 140120985872192 train.py:394] {'eval/walltime': 561.1574749946594, 'training/sps': 42213.78986275704, 'training/walltime': 789.7608666419983, 'training/entropy_loss': Array(-0.04332638, dtype=float32), 'training/policy_loss': Array(-0.00019981, dtype=float32), 'training/total_loss': Array(13.997932, dtype=float32), 'training/v_loss': Array(14.041458, dtype=float32), 'eval/episode_goal_distance': (Array(0.3091903, dtype=float32), Array(0.05984325, dtype=float32)), 'eval/episode_reward': (Array(-15052.049, dtype=float32), Array(5541.9395, dtype=float32)), 'eval/avg_episode_length': (Array(914.5078, dtype=float32), Array(278.8199, dtype=float32)), 'eval/epoch_eval_time': 4.092436790466309, 'eval/sps': 31277.20880092449}
I0727 04:29:52.505091 140120985872192 train.py:379] starting iteration 133 1356.8307445049286
I0727 04:30:02.440974 140120985872192 train.py:394] {'eval/walltime': 565.2519507408142, 'training/sps': 42098.3410129629, 'training/walltime': 795.5986263751984, 'training/entropy_loss': Array(-0.04378326, dtype=float32), 'training/policy_loss': Array(-0.00013322, dtype=float32), 'training/total_loss': Array(96.189606, dtype=float32), 'training/v_loss': Array(96.23352, dtype=float32), 'eval/episode_goal_distance': (Array(0.30233365, dtype=float32), Array(0.06266433, dtype=float32)), 'eval/episode_reward': (Array(-14545.4375, dtype=float32), Array(5997.339, dtype=float32)), 'eval/avg_episode_length': (Array(883.46875, dtype=float32), Array(319.84317, dtype=float32)), 'eval/epoch_eval_time': 4.094475746154785, 'eval/sps': 31261.633463138154}
I0727 04:30:02.443509 140120985872192 train.py:379] starting iteration 134 1366.7691628932953
I0727 04:30:12.387886 140120985872192 train.py:394] {'eval/walltime': 569.3577616214752, 'training/sps': 42118.22583835541, 'training/walltime': 801.433629989624, 'training/entropy_loss': Array(-0.04406126, dtype=float32), 'training/policy_loss': Array(-0.00028842, dtype=float32), 'training/total_loss': Array(30.808575, dtype=float32), 'training/v_loss': Array(30.852928, dtype=float32), 'eval/episode_goal_distance': (Array(0.29662696, dtype=float32), Array(0.06528179, dtype=float32)), 'eval/episode_reward': (Array(-14223.18, dtype=float32), Array(6029.899, dtype=float32)), 'eval/avg_episode_length': (Array(875.83594, dtype=float32), Array(328.50775, dtype=float32)), 'eval/epoch_eval_time': 4.105810880661011, 'eval/sps': 31175.327778222647}
I0727 04:30:12.390199 140120985872192 train.py:379] starting iteration 135 1376.7158534526825
I0727 04:30:22.315302 140120985872192 train.py:394] {'eval/walltime': 573.4867498874664, 'training/sps': 42429.230712427525, 'training/walltime': 807.2258632183075, 'training/entropy_loss': Array(-0.04304087, dtype=float32), 'training/policy_loss': Array(-0.00035886, dtype=float32), 'training/total_loss': Array(16.64361, dtype=float32), 'training/v_loss': Array(16.687008, dtype=float32), 'eval/episode_goal_distance': (Array(0.30590653, dtype=float32), Array(0.06155923, dtype=float32)), 'eval/episode_reward': (Array(-14255.248, dtype=float32), Array(6197.406, dtype=float32)), 'eval/avg_episode_length': (Array(868.0625, dtype=float32), Array(337.13678, dtype=float32)), 'eval/epoch_eval_time': 4.128988265991211, 'eval/sps': 31000.330287756857}
I0727 04:30:22.319658 140120985872192 train.py:379] starting iteration 136 1386.645296573639
I0727 04:30:32.254522 140120985872192 train.py:394] {'eval/walltime': 577.5916786193848, 'training/sps': 42184.28999080066, 'training/walltime': 813.0517287254333, 'training/entropy_loss': Array(-0.04209704, dtype=float32), 'training/policy_loss': Array(-0.00023425, dtype=float32), 'training/total_loss': Array(12.721957, dtype=float32), 'training/v_loss': Array(12.764288, dtype=float32), 'eval/episode_goal_distance': (Array(0.30098474, dtype=float32), Array(0.05803099, dtype=float32)), 'eval/episode_reward': (Array(-15025.08, dtype=float32), Array(5776.2993, dtype=float32)), 'eval/avg_episode_length': (Array(906.78906, dtype=float32), Array(289.80493, dtype=float32)), 'eval/epoch_eval_time': 4.104928731918335, 'eval/sps': 31182.02735280679}
I0727 04:30:32.257022 140120985872192 train.py:379] starting iteration 137 1396.5826756954193
I0727 04:30:42.177271 140120985872192 train.py:394] {'eval/walltime': 581.6845667362213, 'training/sps': 42200.067879252354, 'training/walltime': 818.8754160404205, 'training/entropy_loss': Array(-0.04036702, dtype=float32), 'training/policy_loss': Array(-0.00013107, dtype=float32), 'training/total_loss': Array(15.548305, dtype=float32), 'training/v_loss': Array(15.588803, dtype=float32), 'eval/episode_goal_distance': (Array(0.29273987, dtype=float32), Array(0.05810808, dtype=float32)), 'eval/episode_reward': (Array(-14311.3955, dtype=float32), Array(5429.7915, dtype=float32)), 'eval/avg_episode_length': (Array(906.7656, dtype=float32), Array(289.87762, dtype=float32)), 'eval/epoch_eval_time': 4.092888116836548, 'eval/sps': 31273.75983561775}
I0727 04:30:42.179611 140120985872192 train.py:379] starting iteration 138 1406.5052654743195
I0727 04:30:52.140578 140120985872192 train.py:394] {'eval/walltime': 585.8042204380035, 'training/sps': 42099.948646493496, 'training/walltime': 824.7129528522491, 'training/entropy_loss': Array(-0.03979139, dtype=float32), 'training/policy_loss': Array(-0.00011897, dtype=float32), 'training/total_loss': Array(14.010707, dtype=float32), 'training/v_loss': Array(14.050617, dtype=float32), 'eval/episode_goal_distance': (Array(0.29470706, dtype=float32), Array(0.06206228, dtype=float32)), 'eval/episode_reward': (Array(-13867.058, dtype=float32), Array(6101.5454, dtype=float32)), 'eval/avg_episode_length': (Array(868., dtype=float32), Array(337.29654, dtype=float32)), 'eval/epoch_eval_time': 4.119653701782227, 'eval/sps': 31070.572738826373}
I0727 04:30:52.143083 140120985872192 train.py:379] starting iteration 139 1416.4687366485596
I0727 04:31:02.040371 140120985872192 train.py:394] {'eval/walltime': 589.8929328918457, 'training/sps': 42336.014198197714, 'training/walltime': 830.5179395675659, 'training/entropy_loss': Array(-0.03975631, dtype=float32), 'training/policy_loss': Array(-0.00012934, dtype=float32), 'training/total_loss': Array(14.64046, dtype=float32), 'training/v_loss': Array(14.680346, dtype=float32), 'eval/episode_goal_distance': (Array(0.2926643, dtype=float32), Array(0.05771685, dtype=float32)), 'eval/episode_reward': (Array(-14449.576, dtype=float32), Array(5552.8047, dtype=float32)), 'eval/avg_episode_length': (Array(906.85156, dtype=float32), Array(289.61093, dtype=float32)), 'eval/epoch_eval_time': 4.088712453842163, 'eval/sps': 31305.698663088522}
I0727 04:31:02.042775 140120985872192 train.py:379] starting iteration 140 1426.3684282302856
I0727 04:31:11.943352 140120985872192 train.py:394] {'eval/walltime': 593.9791483879089, 'training/sps': 42295.51293383492, 'training/walltime': 836.3284850120544, 'training/entropy_loss': Array(-0.03940611, dtype=float32), 'training/policy_loss': Array(8.532191e-05, dtype=float32), 'training/total_loss': Array(17.853298, dtype=float32), 'training/v_loss': Array(17.89262, dtype=float32), 'eval/episode_goal_distance': (Array(0.29471564, dtype=float32), Array(0.06270028, dtype=float32)), 'eval/episode_reward': (Array(-14237.945, dtype=float32), Array(5338.778, dtype=float32)), 'eval/avg_episode_length': (Array(906.77344, dtype=float32), Array(289.8534, dtype=float32)), 'eval/epoch_eval_time': 4.086215496063232, 'eval/sps': 31324.828590983165}
I0727 04:31:11.945871 140120985872192 train.py:379] starting iteration 141 1436.271525144577
I0727 04:31:21.894120 140120985872192 train.py:394] {'eval/walltime': 598.1120157241821, 'training/sps': 42287.85741942169, 'training/walltime': 842.140082359314, 'training/entropy_loss': Array(-0.03952859, dtype=float32), 'training/policy_loss': Array(0.00012282, dtype=float32), 'training/total_loss': Array(77.62213, dtype=float32), 'training/v_loss': Array(77.66153, dtype=float32), 'eval/episode_goal_distance': (Array(0.2942778, dtype=float32), Array(0.05685871, dtype=float32)), 'eval/episode_reward': (Array(-14245.521, dtype=float32), Array(5488.684, dtype=float32)), 'eval/avg_episode_length': (Array(891.3047, dtype=float32), Array(310.17032, dtype=float32)), 'eval/epoch_eval_time': 4.132867336273193, 'eval/sps': 30971.23367028369}
I0727 04:31:21.896609 140120985872192 train.py:379] starting iteration 142 1446.2222628593445
I0727 04:31:31.830056 140120985872192 train.py:394] {'eval/walltime': 602.2123289108276, 'training/sps': 42158.4398477344, 'training/walltime': 847.9695200920105, 'training/entropy_loss': Array(-0.04049254, dtype=float32), 'training/policy_loss': Array(2.8660608e-05, dtype=float32), 'training/total_loss': Array(43.849808, dtype=float32), 'training/v_loss': Array(43.89027, dtype=float32), 'eval/episode_goal_distance': (Array(0.3029898, dtype=float32), Array(0.061653, dtype=float32)), 'eval/episode_reward': (Array(-14896.701, dtype=float32), Array(5670.6846, dtype=float32)), 'eval/avg_episode_length': (Array(899.09375, dtype=float32), Array(300.12076, dtype=float32)), 'eval/epoch_eval_time': 4.100313186645508, 'eval/sps': 31217.12761280989}
I0727 04:31:31.832418 140120985872192 train.py:379] starting iteration 143 1456.158071756363
I0727 04:31:41.758888 140120985872192 train.py:394] {'eval/walltime': 606.3047614097595, 'training/sps': 42152.302443519045, 'training/walltime': 853.7998065948486, 'training/entropy_loss': Array(-0.04138263, dtype=float32), 'training/policy_loss': Array(-0.00029512, dtype=float32), 'training/total_loss': Array(21.014198, dtype=float32), 'training/v_loss': Array(21.055878, dtype=float32), 'eval/episode_goal_distance': (Array(0.2963689, dtype=float32), Array(0.05276498, dtype=float32)), 'eval/episode_reward': (Array(-13681.166, dtype=float32), Array(5982.0337, dtype=float32)), 'eval/avg_episode_length': (Array(868.0078, dtype=float32), Array(337.27658, dtype=float32)), 'eval/epoch_eval_time': 4.092432498931885, 'eval/sps': 31277.241599808352}
I0727 04:31:41.761455 140120985872192 train.py:379] starting iteration 144 1466.0871088504791
I0727 04:31:51.690845 140120985872192 train.py:394] {'eval/walltime': 610.3940536975861, 'training/sps': 42108.0643415142, 'training/walltime': 859.6362183094025, 'training/entropy_loss': Array(-0.04022617, dtype=float32), 'training/policy_loss': Array(-1.3976948e-05, dtype=float32), 'training/total_loss': Array(12.051159, dtype=float32), 'training/v_loss': Array(12.091399, dtype=float32), 'eval/episode_goal_distance': (Array(0.30149326, dtype=float32), Array(0.05890267, dtype=float32)), 'eval/episode_reward': (Array(-14135.684, dtype=float32), Array(5853.2964, dtype=float32)), 'eval/avg_episode_length': (Array(875.7969, dtype=float32), Array(328.61115, dtype=float32)), 'eval/epoch_eval_time': 4.089292287826538, 'eval/sps': 31301.259726785658}
I0727 04:31:51.693338 140120985872192 train.py:379] starting iteration 145 1476.0189912319183
I0727 04:32:01.625824 140120985872192 train.py:394] {'eval/walltime': 614.4874722957611, 'training/sps': 42115.62391801593, 'training/walltime': 865.4715824127197, 'training/entropy_loss': Array(-0.03985463, dtype=float32), 'training/policy_loss': Array(0.00038451, dtype=float32), 'training/total_loss': Array(13.890219, dtype=float32), 'training/v_loss': Array(13.929689, dtype=float32), 'eval/episode_goal_distance': (Array(0.29781616, dtype=float32), Array(0.05832486, dtype=float32)), 'eval/episode_reward': (Array(-14082.049, dtype=float32), Array(5637.8154, dtype=float32)), 'eval/avg_episode_length': (Array(883.4297, dtype=float32), Array(319.94998, dtype=float32)), 'eval/epoch_eval_time': 4.093418598175049, 'eval/sps': 31269.706952781642}
I0727 04:32:01.628238 140120985872192 train.py:379] starting iteration 146 1485.9538924694061
I0727 04:32:11.571861 140120985872192 train.py:394] {'eval/walltime': 618.5895795822144, 'training/sps': 42098.189712599255, 'training/walltime': 871.3093631267548, 'training/entropy_loss': Array(-0.03969947, dtype=float32), 'training/policy_loss': Array(0.00022893, dtype=float32), 'training/total_loss': Array(14.303456, dtype=float32), 'training/v_loss': Array(14.342928, dtype=float32), 'eval/episode_goal_distance': (Array(0.28824168, dtype=float32), Array(0.06290344, dtype=float32)), 'eval/episode_reward': (Array(-13874.535, dtype=float32), Array(5729.01, dtype=float32)), 'eval/avg_episode_length': (Array(883.53906, dtype=float32), Array(319.6503, dtype=float32)), 'eval/epoch_eval_time': 4.102107286453247, 'eval/sps': 31203.47447340194}
I0727 04:32:11.574130 140120985872192 train.py:379] starting iteration 147 1495.8997843265533
I0727 04:32:21.483157 140120985872192 train.py:394] {'eval/walltime': 622.6865637302399, 'training/sps': 42311.88476499485, 'training/walltime': 877.1176602840424, 'training/entropy_loss': Array(-0.03831265, dtype=float32), 'training/policy_loss': Array(0.0005339, dtype=float32), 'training/total_loss': Array(15.305382, dtype=float32), 'training/v_loss': Array(15.343161, dtype=float32), 'eval/episode_goal_distance': (Array(0.29396975, dtype=float32), Array(0.0638693, dtype=float32)), 'eval/episode_reward': (Array(-14163.251, dtype=float32), Array(5587.841, dtype=float32)), 'eval/avg_episode_length': (Array(898.9297, dtype=float32), Array(300.60907, dtype=float32)), 'eval/epoch_eval_time': 4.096984148025513, 'eval/sps': 31242.49335006285}
I0727 04:32:21.485609 140120985872192 train.py:379] starting iteration 148 1505.8112630844116
I0727 04:32:31.421952 140120985872192 train.py:394] {'eval/walltime': 626.7866506576538, 'training/sps': 42135.28911454231, 'training/walltime': 882.9503009319305, 'training/entropy_loss': Array(-0.03694106, dtype=float32), 'training/policy_loss': Array(0.00131589, dtype=float32), 'training/total_loss': Array(13.967348, dtype=float32), 'training/v_loss': Array(14.002974, dtype=float32), 'eval/episode_goal_distance': (Array(0.29575187, dtype=float32), Array(0.05949333, dtype=float32)), 'eval/episode_reward': (Array(-14977.157, dtype=float32), Array(5250.7695, dtype=float32)), 'eval/avg_episode_length': (Array(922.41406, dtype=float32), Array(266.5167, dtype=float32)), 'eval/epoch_eval_time': 4.10008692741394, 'eval/sps': 31218.850299043246}
I0727 04:32:31.424454 140120985872192 train.py:379] starting iteration 149 1515.7501075267792
I0727 04:32:41.375598 140120985872192 train.py:394] {'eval/walltime': 630.9100058078766, 'training/sps': 42196.21903115548, 'training/walltime': 888.774519443512, 'training/entropy_loss': Array(-0.03722601, dtype=float32), 'training/policy_loss': Array(0.00089201, dtype=float32), 'training/total_loss': Array(14.489841, dtype=float32), 'training/v_loss': Array(14.526176, dtype=float32), 'eval/episode_goal_distance': (Array(0.30082345, dtype=float32), Array(0.06197422, dtype=float32)), 'eval/episode_reward': (Array(-13552.214, dtype=float32), Array(6415.066, dtype=float32)), 'eval/avg_episode_length': (Array(844.6406, dtype=float32), Array(361.02332, dtype=float32)), 'eval/epoch_eval_time': 4.123355150222778, 'eval/sps': 31042.681344847137}
I0727 04:32:41.377989 140120985872192 train.py:379] starting iteration 150 1525.7036426067352
I0727 04:32:51.275097 140120985872192 train.py:394] {'eval/walltime': 634.996949672699, 'training/sps': 42323.6949310461, 'training/walltime': 894.5811958312988, 'training/entropy_loss': Array(-0.03726229, dtype=float32), 'training/policy_loss': Array(0.00134856, dtype=float32), 'training/total_loss': Array(103.017944, dtype=float32), 'training/v_loss': Array(103.053856, dtype=float32), 'eval/episode_goal_distance': (Array(0.28550482, dtype=float32), Array(0.06519546, dtype=float32)), 'eval/episode_reward': (Array(-13707.457, dtype=float32), Array(6083.028, dtype=float32)), 'eval/avg_episode_length': (Array(883.5781, dtype=float32), Array(319.5428, dtype=float32)), 'eval/epoch_eval_time': 4.086943864822388, 'eval/sps': 31319.24592890455}
I0727 04:32:51.277464 140120985872192 train.py:379] starting iteration 151 1535.60311794281
I0727 04:33:01.223340 140120985872192 train.py:394] {'eval/walltime': 639.124543428421, 'training/sps': 42265.384064590595, 'training/walltime': 900.3958833217621, 'training/entropy_loss': Array(-0.03815759, dtype=float32), 'training/policy_loss': Array(0.00068582, dtype=float32), 'training/total_loss': Array(23.616566, dtype=float32), 'training/v_loss': Array(23.654037, dtype=float32), 'eval/episode_goal_distance': (Array(0.29457238, dtype=float32), Array(0.05991501, dtype=float32)), 'eval/episode_reward': (Array(-14291.918, dtype=float32), Array(5499.851, dtype=float32)), 'eval/avg_episode_length': (Array(898.97656, dtype=float32), Array(300.46912, dtype=float32)), 'eval/epoch_eval_time': 4.127593755722046, 'eval/sps': 31010.803769764105}
I0727 04:33:01.225734 140120985872192 train.py:379] starting iteration 152 1545.5513877868652
I0727 04:33:11.141483 140120985872192 train.py:394] {'eval/walltime': 643.222003698349, 'training/sps': 42266.500145994396, 'training/walltime': 906.2104172706604, 'training/entropy_loss': Array(-0.03862362, dtype=float32), 'training/policy_loss': Array(0.00094593, dtype=float32), 'training/total_loss': Array(14.039946, dtype=float32), 'training/v_loss': Array(14.077623, dtype=float32), 'eval/episode_goal_distance': (Array(0.2980882, dtype=float32), Array(0.06341133, dtype=float32)), 'eval/episode_reward': (Array(-14150.787, dtype=float32), Array(5640.395, dtype=float32)), 'eval/avg_episode_length': (Array(891.2344, dtype=float32), Array(310.37048, dtype=float32)), 'eval/epoch_eval_time': 4.0974602699279785, 'eval/sps': 31238.862995064468}
I0727 04:33:11.144049 140120985872192 train.py:379] starting iteration 153 1555.4697029590607
I0727 04:33:21.096409 140120985872192 train.py:394] {'eval/walltime': 647.3653795719147, 'training/sps': 42333.83559791782, 'training/walltime': 912.0157027244568, 'training/entropy_loss': Array(-0.0375247, dtype=float32), 'training/policy_loss': Array(0.00080724, dtype=float32), 'training/total_loss': Array(11.581847, dtype=float32), 'training/v_loss': Array(11.618565, dtype=float32), 'eval/episode_goal_distance': (Array(0.29553998, dtype=float32), Array(0.06375468, dtype=float32)), 'eval/episode_reward': (Array(-15258.145, dtype=float32), Array(5071.932, dtype=float32)), 'eval/avg_episode_length': (Array(930.1094, dtype=float32), Array(254.13939, dtype=float32)), 'eval/epoch_eval_time': 4.143375873565674, 'eval/sps': 30892.68362463258}
I0727 04:33:21.177585 140120985872192 train.py:379] starting iteration 154 1565.5032374858856
I0727 04:33:31.123453 140120985872192 train.py:394] {'eval/walltime': 651.4908664226532, 'training/sps': 42250.701992479255, 'training/walltime': 917.8324108123779, 'training/entropy_loss': Array(-0.0366437, dtype=float32), 'training/policy_loss': Array(0.00083126, dtype=float32), 'training/total_loss': Array(13.267878, dtype=float32), 'training/v_loss': Array(13.303689, dtype=float32), 'eval/episode_goal_distance': (Array(0.2939139, dtype=float32), Array(0.0589293, dtype=float32)), 'eval/episode_reward': (Array(-13891.189, dtype=float32), Array(5771.9883, dtype=float32)), 'eval/avg_episode_length': (Array(883.47656, dtype=float32), Array(319.82156, dtype=float32)), 'eval/epoch_eval_time': 4.125486850738525, 'eval/sps': 31026.64112893392}
I0727 04:33:31.126259 140120985872192 train.py:379] starting iteration 155 1575.4519131183624
I0727 04:33:41.051492 140120985872192 train.py:394] {'eval/walltime': 655.5811612606049, 'training/sps': 42145.43273974384, 'training/walltime': 923.6636476516724, 'training/entropy_loss': Array(-0.03558308, dtype=float32), 'training/policy_loss': Array(0.00127162, dtype=float32), 'training/total_loss': Array(13.389233, dtype=float32), 'training/v_loss': Array(13.423544, dtype=float32), 'eval/episode_goal_distance': (Array(0.30045366, dtype=float32), Array(0.05755388, dtype=float32)), 'eval/episode_reward': (Array(-14619.721, dtype=float32), Array(5659.297, dtype=float32)), 'eval/avg_episode_length': (Array(899.03906, dtype=float32), Array(300.28342, dtype=float32)), 'eval/epoch_eval_time': 4.09029483795166, 'eval/sps': 31293.587643696585}
I0727 04:33:41.053936 140120985872192 train.py:379] starting iteration 156 1585.3795900344849
I0727 04:33:50.981112 140120985872192 train.py:394] {'eval/walltime': 659.6897780895233, 'training/sps': 42263.04637703549, 'training/walltime': 929.4786567687988, 'training/entropy_loss': Array(-0.03314756, dtype=float32), 'training/policy_loss': Array(0.00126711, dtype=float32), 'training/total_loss': Array(12.317097, dtype=float32), 'training/v_loss': Array(12.348977, dtype=float32), 'eval/episode_goal_distance': (Array(0.2974561, dtype=float32), Array(0.05938378, dtype=float32)), 'eval/episode_reward': (Array(-14600.302, dtype=float32), Array(4937.0405, dtype=float32)), 'eval/avg_episode_length': (Array(922.3594, dtype=float32), Array(266.70468, dtype=float32)), 'eval/epoch_eval_time': 4.108616828918457, 'eval/sps': 31154.03682793521}
I0727 04:33:50.983541 140120985872192 train.py:379] starting iteration 157 1595.3091950416565
I0727 04:34:00.909009 140120985872192 train.py:394] {'eval/walltime': 663.7863411903381, 'training/sps': 42189.11225458974, 'training/walltime': 935.3038563728333, 'training/entropy_loss': Array(-0.02927859, dtype=float32), 'training/policy_loss': Array(0.00176179, dtype=float32), 'training/total_loss': Array(13.708, dtype=float32), 'training/v_loss': Array(13.735516, dtype=float32), 'eval/episode_goal_distance': (Array(0.2967646, dtype=float32), Array(0.06126524, dtype=float32)), 'eval/episode_reward': (Array(-14177.273, dtype=float32), Array(5521.73, dtype=float32)), 'eval/avg_episode_length': (Array(899.0078, dtype=float32), Array(300.3767, dtype=float32)), 'eval/epoch_eval_time': 4.096563100814819, 'eval/sps': 31245.704472253925}
I0727 04:34:00.911648 140120985872192 train.py:379] starting iteration 158 1605.23730134964
I0727 04:34:10.846470 140120985872192 train.py:394] {'eval/walltime': 667.9148409366608, 'training/sps': 42353.87047555278, 'training/walltime': 941.1063957214355, 'training/entropy_loss': Array(-0.02937557, dtype=float32), 'training/policy_loss': Array(0.0019487, dtype=float32), 'training/total_loss': Array(98.21542, dtype=float32), 'training/v_loss': Array(98.24285, dtype=float32), 'eval/episode_goal_distance': (Array(0.29937047, dtype=float32), Array(0.05673692, dtype=float32)), 'eval/episode_reward': (Array(-13450.937, dtype=float32), Array(6363.8604, dtype=float32)), 'eval/avg_episode_length': (Array(844.6797, dtype=float32), Array(360.93222, dtype=float32)), 'eval/epoch_eval_time': 4.128499746322632, 'eval/sps': 31003.998513991217}
I0727 04:34:10.849146 140120985872192 train.py:379] starting iteration 159 1615.1747996807098
I0727 04:34:20.766694 140120985872192 train.py:394] {'eval/walltime': 672.0072221755981, 'training/sps': 42215.83682742212, 'training/walltime': 946.927907705307, 'training/entropy_loss': Array(-0.02980206, dtype=float32), 'training/policy_loss': Array(0.0017269, dtype=float32), 'training/total_loss': Array(23.565588, dtype=float32), 'training/v_loss': Array(23.593662, dtype=float32), 'eval/episode_goal_distance': (Array(0.29622298, dtype=float32), Array(0.05757556, dtype=float32)), 'eval/episode_reward': (Array(-13352.209, dtype=float32), Array(5733.449, dtype=float32)), 'eval/avg_episode_length': (Array(868.0625, dtype=float32), Array(337.13654, dtype=float32)), 'eval/epoch_eval_time': 4.092381238937378, 'eval/sps': 31277.633369572457}
I0727 04:34:20.769169 140120985872192 train.py:379] starting iteration 160 1625.0948238372803
I0727 04:34:30.731457 140120985872192 train.py:394] {'eval/walltime': 676.1420183181763, 'training/sps': 42199.23171431509, 'training/walltime': 952.7517104148865, 'training/entropy_loss': Array(-0.02824652, dtype=float32), 'training/policy_loss': Array(0.00467186, dtype=float32), 'training/total_loss': Array(20.528904, dtype=float32), 'training/v_loss': Array(20.552479, dtype=float32), 'eval/episode_goal_distance': (Array(0.28969786, dtype=float32), Array(0.05720109, dtype=float32)), 'eval/episode_reward': (Array(-12815.564, dtype=float32), Array(5886.4043, dtype=float32)), 'eval/avg_episode_length': (Array(844.6953, dtype=float32), Array(360.8963, dtype=float32)), 'eval/epoch_eval_time': 4.134796142578125, 'eval/sps': 30956.786159762047}
I0727 04:34:30.734086 140120985872192 train.py:379] starting iteration 161 1635.0597400665283
I0727 04:34:40.710683 140120985872192 train.py:394] {'eval/walltime': 680.256135225296, 'training/sps': 41947.3719673687, 'training/walltime': 958.6104803085327, 'training/entropy_loss': Array(-0.02614818, dtype=float32), 'training/policy_loss': Array(0.00360726, dtype=float32), 'training/total_loss': Array(9.566572, dtype=float32), 'training/v_loss': Array(9.589113, dtype=float32), 'eval/episode_goal_distance': (Array(0.29892156, dtype=float32), Array(0.06746204, dtype=float32)), 'eval/episode_reward': (Array(-13340.133, dtype=float32), Array(6381.23, dtype=float32)), 'eval/avg_episode_length': (Array(844.6719, dtype=float32), Array(360.95053, dtype=float32)), 'eval/epoch_eval_time': 4.114116907119751, 'eval/sps': 31112.387637426527}
I0727 04:34:40.713031 140120985872192 train.py:379] starting iteration 162 1645.0386853218079
I0727 04:34:50.642866 140120985872192 train.py:394] {'eval/walltime': 684.350742816925, 'training/sps': 42143.127254815525, 'training/walltime': 964.442036151886, 'training/entropy_loss': Array(-0.02418553, dtype=float32), 'training/policy_loss': Array(0.00308339, dtype=float32), 'training/total_loss': Array(12.427118, dtype=float32), 'training/v_loss': Array(12.448221, dtype=float32), 'eval/episode_goal_distance': (Array(0.2956854, dtype=float32), Array(0.06527706, dtype=float32)), 'eval/episode_reward': (Array(-14138.0625, dtype=float32), Array(4941.2515, dtype=float32)), 'eval/avg_episode_length': (Array(914.5781, dtype=float32), Array(278.5907, dtype=float32)), 'eval/epoch_eval_time': 4.094607591629028, 'eval/sps': 31260.626845337225}
I0727 04:34:50.645313 140120985872192 train.py:379] starting iteration 163 1654.9709672927856
I0727 04:35:00.573504 140120985872192 train.py:394] {'eval/walltime': 688.4678184986115, 'training/sps': 42318.20945480897, 'training/walltime': 970.2494652271271, 'training/entropy_loss': Array(-0.02315318, dtype=float32), 'training/policy_loss': Array(0.00270145, dtype=float32), 'training/total_loss': Array(9.060399, dtype=float32), 'training/v_loss': Array(9.080851, dtype=float32), 'eval/episode_goal_distance': (Array(0.29374823, dtype=float32), Array(0.06260719, dtype=float32)), 'eval/episode_reward': (Array(-14260.172, dtype=float32), Array(4517.849, dtype=float32)), 'eval/avg_episode_length': (Array(937.85156, dtype=float32), Array(240.70004, dtype=float32)), 'eval/epoch_eval_time': 4.117075681686401, 'eval/sps': 31090.02843192082}
I0727 04:35:00.575948 140120985872192 train.py:379] starting iteration 164 1664.9016027450562
I0727 04:35:10.475966 140120985872192 train.py:394] {'eval/walltime': 692.5623910427094, 'training/sps': 42360.002312804696, 'training/walltime': 976.0511646270752, 'training/entropy_loss': Array(-0.02130332, dtype=float32), 'training/policy_loss': Array(0.00327912, dtype=float32), 'training/total_loss': Array(8.1555605, dtype=float32), 'training/v_loss': Array(8.173584, dtype=float32), 'eval/episode_goal_distance': (Array(0.29160428, dtype=float32), Array(0.05877112, dtype=float32)), 'eval/episode_reward': (Array(-14365.715, dtype=float32), Array(4709.627, dtype=float32)), 'eval/avg_episode_length': (Array(930.16406, dtype=float32), Array(253.94048, dtype=float32)), 'eval/epoch_eval_time': 4.0945725440979, 'eval/sps': 31260.894420958524}
I0727 04:35:10.478460 140120985872192 train.py:379] starting iteration 165 1674.8041141033173
I0727 04:35:20.459796 140120985872192 train.py:394] {'eval/walltime': 696.6844465732574, 'training/sps': 41970.3388226758, 'training/walltime': 981.9067285060883, 'training/entropy_loss': Array(-0.01972222, dtype=float32), 'training/policy_loss': Array(0.00237307, dtype=float32), 'training/total_loss': Array(8.818823, dtype=float32), 'training/v_loss': Array(8.836172, dtype=float32), 'eval/episode_goal_distance': (Array(0.2922693, dtype=float32), Array(0.0592448, dtype=float32)), 'eval/episode_reward': (Array(-14241.527, dtype=float32), Array(4854.278, dtype=float32)), 'eval/avg_episode_length': (Array(922.375, dtype=float32), Array(266.6511, dtype=float32)), 'eval/epoch_eval_time': 4.122055530548096, 'eval/sps': 31052.468617030074}
I0727 04:35:20.462211 140120985872192 train.py:379] starting iteration 166 1684.78786444664
I0727 04:35:30.402021 140120985872192 train.py:394] {'eval/walltime': 700.788533449173, 'training/sps': 42139.78664465199, 'training/walltime': 987.7387466430664, 'training/entropy_loss': Array(-0.01877745, dtype=float32), 'training/policy_loss': Array(0.00253323, dtype=float32), 'training/total_loss': Array(64.08081, dtype=float32), 'training/v_loss': Array(64.09705, dtype=float32), 'eval/episode_goal_distance': (Array(0.28916615, dtype=float32), Array(0.06129282, dtype=float32)), 'eval/episode_reward': (Array(-14138.258, dtype=float32), Array(4798.509, dtype=float32)), 'eval/avg_episode_length': (Array(922.3281, dtype=float32), Array(266.8119, dtype=float32)), 'eval/epoch_eval_time': 4.104086875915527, 'eval/sps': 31188.423605542255}
I0727 04:35:30.404506 140120985872192 train.py:379] starting iteration 167 1694.73015999794
I0727 04:35:40.354586 140120985872192 train.py:394] {'eval/walltime': 704.9030184745789, 'training/sps': 42141.025313507, 'training/walltime': 993.5705933570862, 'training/entropy_loss': Array(-0.01734749, dtype=float32), 'training/policy_loss': Array(0.00058784, dtype=float32), 'training/total_loss': Array(26.594265, dtype=float32), 'training/v_loss': Array(26.611025, dtype=float32), 'eval/episode_goal_distance': (Array(0.29387316, dtype=float32), Array(0.06639492, dtype=float32)), 'eval/episode_reward': (Array(-13468.533, dtype=float32), Array(5361.908, dtype=float32)), 'eval/avg_episode_length': (Array(891.3047, dtype=float32), Array(310.1703, dtype=float32)), 'eval/epoch_eval_time': 4.114485025405884, 'eval/sps': 31109.60404756197}
I0727 04:35:40.357013 140120985872192 train.py:379] starting iteration 168 1704.682667016983
I0727 04:35:50.324972 140120985872192 train.py:394] {'eval/walltime': 709.0203199386597, 'training/sps': 42032.105100309396, 'training/walltime': 999.4175524711609, 'training/entropy_loss': Array(-0.01667562, dtype=float32), 'training/policy_loss': Array(0.00070749, dtype=float32), 'training/total_loss': Array(12.3942795, dtype=float32), 'training/v_loss': Array(12.410248, dtype=float32), 'eval/episode_goal_distance': (Array(0.28677025, dtype=float32), Array(0.05823484, dtype=float32)), 'eval/episode_reward': (Array(-14681.253, dtype=float32), Array(3564.6436, dtype=float32)), 'eval/avg_episode_length': (Array(968.9844, dtype=float32), Array(172.68794, dtype=float32)), 'eval/epoch_eval_time': 4.1173014640808105, 'eval/sps': 31088.323533427752}
I0727 04:35:50.327485 140120985872192 train.py:379] starting iteration 169 1714.653139591217
I0727 04:36:00.226571 140120985872192 train.py:394] {'eval/walltime': 713.110404253006, 'training/sps': 42334.25287031542, 'training/walltime': 1005.2227807044983, 'training/entropy_loss': Array(-0.01826071, dtype=float32), 'training/policy_loss': Array(0.00078081, dtype=float32), 'training/total_loss': Array(7.998623, dtype=float32), 'training/v_loss': Array(8.016103, dtype=float32), 'eval/episode_goal_distance': (Array(0.29632068, dtype=float32), Array(0.05780033, dtype=float32)), 'eval/episode_reward': (Array(-13817.262, dtype=float32), Array(5070.6704, dtype=float32)), 'eval/avg_episode_length': (Array(906.84375, dtype=float32), Array(289.63516, dtype=float32)), 'eval/epoch_eval_time': 4.0900843143463135, 'eval/sps': 31295.19837794768}
I0727 04:36:00.229048 140120985872192 train.py:379] starting iteration 170 1724.554702281952
I0727 04:36:10.138169 140120985872192 train.py:394] {'eval/walltime': 717.2124440670013, 'training/sps': 42348.02051651503, 'training/walltime': 1011.0261216163635, 'training/entropy_loss': Array(-0.01813684, dtype=float32), 'training/policy_loss': Array(0.00064346, dtype=float32), 'training/total_loss': Array(7.8661847, dtype=float32), 'training/v_loss': Array(7.883678, dtype=float32), 'eval/episode_goal_distance': (Array(0.28988156, dtype=float32), Array(0.05971699, dtype=float32)), 'eval/episode_reward': (Array(-13772.689, dtype=float32), Array(4741.1436, dtype=float32)), 'eval/avg_episode_length': (Array(922.35156, dtype=float32), Array(266.73157, dtype=float32)), 'eval/epoch_eval_time': 4.102039813995361, 'eval/sps': 31203.987724177838}
I0727 04:36:10.140589 140120985872192 train.py:379] starting iteration 171 1734.4662430286407
I0727 04:36:20.111216 140120985872192 train.py:394] {'eval/walltime': 721.3457200527191, 'training/sps': 42130.43954137798, 'training/walltime': 1016.8594336509705, 'training/entropy_loss': Array(-0.01712994, dtype=float32), 'training/policy_loss': Array(0.00066879, dtype=float32), 'training/total_loss': Array(7.038064, dtype=float32), 'training/v_loss': Array(7.054525, dtype=float32), 'eval/episode_goal_distance': (Array(0.2895167, dtype=float32), Array(0.05879009, dtype=float32)), 'eval/episode_reward': (Array(-14229.02, dtype=float32), Array(3955.072, dtype=float32)), 'eval/avg_episode_length': (Array(953.3906, dtype=float32), Array(210.1736, dtype=float32)), 'eval/epoch_eval_time': 4.133275985717773, 'eval/sps': 30968.171600999896}
I0727 04:36:20.113616 140120985872192 train.py:379] starting iteration 172 1744.4392704963684
I0727 04:36:30.019273 140120985872192 train.py:394] {'eval/walltime': 725.4413921833038, 'training/sps': 42326.433857462966, 'training/walltime': 1022.6657342910767, 'training/entropy_loss': Array(-0.01589998, dtype=float32), 'training/policy_loss': Array(0.00088872, dtype=float32), 'training/total_loss': Array(7.334536, dtype=float32), 'training/v_loss': Array(7.3495474, dtype=float32), 'eval/episode_goal_distance': (Array(0.28765845, dtype=float32), Array(0.06242281, dtype=float32)), 'eval/episode_reward': (Array(-14362.479, dtype=float32), Array(4158.161, dtype=float32)), 'eval/avg_episode_length': (Array(953.3594, dtype=float32), Array(210.3145, dtype=float32)), 'eval/epoch_eval_time': 4.095672130584717, 'eval/sps': 31252.50164537124}
I0727 04:36:30.021493 140120985872192 train.py:379] starting iteration 173 1754.3471474647522
I0727 04:36:39.968116 140120985872192 train.py:394] {'eval/walltime': 729.543564081192, 'training/sps': 42077.31725261111, 'training/walltime': 1028.5064108371735, 'training/entropy_loss': Array(-0.01649442, dtype=float32), 'training/policy_loss': Array(0.00068313, dtype=float32), 'training/total_loss': Array(6.453129, dtype=float32), 'training/v_loss': Array(6.46894, dtype=float32), 'eval/episode_goal_distance': (Array(0.28987122, dtype=float32), Array(0.05917011, dtype=float32)), 'eval/episode_reward': (Array(-13931.617, dtype=float32), Array(4525.4146, dtype=float32)), 'eval/avg_episode_length': (Array(930.09375, dtype=float32), Array(254.19644, dtype=float32)), 'eval/epoch_eval_time': 4.102171897888184, 'eval/sps': 31202.983001735}
I0727 04:36:39.970550 140120985872192 train.py:379] starting iteration 174 1764.296204328537
I0727 04:36:49.942383 140120985872192 train.py:394] {'eval/walltime': 733.6574418544769, 'training/sps': 41979.59620680427, 'training/walltime': 1034.360683441162, 'training/entropy_loss': Array(-0.01689364, dtype=float32), 'training/policy_loss': Array(0.00053552, dtype=float32), 'training/total_loss': Array(7.670274, dtype=float32), 'training/v_loss': Array(7.686631, dtype=float32), 'eval/episode_goal_distance': (Array(0.29026204, dtype=float32), Array(0.0574002, dtype=float32)), 'eval/episode_reward': (Array(-14570.903, dtype=float32), Array(4314.7373, dtype=float32)), 'eval/avg_episode_length': (Array(945.60156, dtype=float32), Array(226.16754, dtype=float32)), 'eval/epoch_eval_time': 4.113877773284912, 'eval/sps': 31114.19615604977}
I0727 04:36:49.944661 140120985872192 train.py:379] starting iteration 175 1774.2703154087067
I0727 04:36:59.850895 140120985872192 train.py:394] {'eval/walltime': 737.7480010986328, 'training/sps': 42285.253579926444, 'training/walltime': 1040.1726386547089, 'training/entropy_loss': Array(-0.0199018, dtype=float32), 'training/policy_loss': Array(0.00057467, dtype=float32), 'training/total_loss': Array(72.84332, dtype=float32), 'training/v_loss': Array(72.862656, dtype=float32), 'eval/episode_goal_distance': (Array(0.27975872, dtype=float32), Array(0.05723124, dtype=float32)), 'eval/episode_reward': (Array(-14067.164, dtype=float32), Array(4057.9714, dtype=float32)), 'eval/avg_episode_length': (Array(945.6094, dtype=float32), Array(226.13507, dtype=float32)), 'eval/epoch_eval_time': 4.090559244155884, 'eval/sps': 31291.564883914478}
I0727 04:36:59.853097 140120985872192 train.py:379] starting iteration 176 1784.1787514686584
I0727 04:37:09.792673 140120985872192 train.py:394] {'eval/walltime': 741.8585212230682, 'training/sps': 42188.0365162583, 'training/walltime': 1045.997986793518, 'training/entropy_loss': Array(-0.02138091, dtype=float32), 'training/policy_loss': Array(0.00055759, dtype=float32), 'training/total_loss': Array(11.518722, dtype=float32), 'training/v_loss': Array(11.539545, dtype=float32), 'eval/episode_goal_distance': (Array(0.29405898, dtype=float32), Array(0.05576921, dtype=float32)), 'eval/episode_reward': (Array(-14411.305, dtype=float32), Array(4499.366, dtype=float32)), 'eval/avg_episode_length': (Array(937.8828, dtype=float32), Array(240.57924, dtype=float32)), 'eval/epoch_eval_time': 4.110520124435425, 'eval/sps': 31139.611563775194}
I0727 04:37:09.795064 140120985872192 train.py:379] starting iteration 177 1794.1207177639008
I0727 04:37:19.706593 140120985872192 train.py:394] {'eval/walltime': 745.9562809467316, 'training/sps': 42299.66979661164, 'training/walltime': 1051.8079612255096, 'training/entropy_loss': Array(-0.0200427, dtype=float32), 'training/policy_loss': Array(0.00087942, dtype=float32), 'training/total_loss': Array(9.017711, dtype=float32), 'training/v_loss': Array(9.036874, dtype=float32), 'eval/episode_goal_distance': (Array(0.29220116, dtype=float32), Array(0.05851069, dtype=float32)), 'eval/episode_reward': (Array(-14510.18, dtype=float32), Array(4307.3765, dtype=float32)), 'eval/avg_episode_length': (Array(945.64844, dtype=float32), Array(225.97281, dtype=float32)), 'eval/epoch_eval_time': 4.09775972366333, 'eval/sps': 31236.580139347483}
I0727 04:37:19.708910 140120985872192 train.py:379] starting iteration 178 1804.034564256668
I0727 04:37:29.647807 140120985872192 train.py:394] {'eval/walltime': 750.0682134628296, 'training/sps': 42203.88287031333, 'training/walltime': 1057.6311221122742, 'training/entropy_loss': Array(-0.02075006, dtype=float32), 'training/policy_loss': Array(0.00039742, dtype=float32), 'training/total_loss': Array(8.391385, dtype=float32), 'training/v_loss': Array(8.411737, dtype=float32), 'eval/episode_goal_distance': (Array(0.2901974, dtype=float32), Array(0.05468224, dtype=float32)), 'eval/episode_reward': (Array(-14523.233, dtype=float32), Array(4245.631, dtype=float32)), 'eval/avg_episode_length': (Array(945.625, dtype=float32), Array(226.07027, dtype=float32)), 'eval/epoch_eval_time': 4.1119325160980225, 'eval/sps': 31128.915540049846}
I0727 04:37:29.650165 140120985872192 train.py:379] starting iteration 179 1813.9758188724518
I0727 04:37:39.583664 140120985872192 train.py:394] {'eval/walltime': 754.162428855896, 'training/sps': 42129.75249399366, 'training/walltime': 1063.4645292758942, 'training/entropy_loss': Array(-0.02161489, dtype=float32), 'training/policy_loss': Array(0.00049012, dtype=float32), 'training/total_loss': Array(8.737616, dtype=float32), 'training/v_loss': Array(8.758741, dtype=float32), 'eval/episode_goal_distance': (Array(0.29473194, dtype=float32), Array(0.06146994, dtype=float32)), 'eval/episode_reward': (Array(-14020.773, dtype=float32), Array(4758.4453, dtype=float32)), 'eval/avg_episode_length': (Array(922.2969, dtype=float32), Array(266.9196, dtype=float32)), 'eval/epoch_eval_time': 4.094215393066406, 'eval/sps': 31263.62140515842}
I0727 04:37:39.585918 140120985872192 train.py:379] starting iteration 180 1823.9115719795227
I0727 04:37:49.544168 140120985872192 train.py:394] {'eval/walltime': 758.2738564014435, 'training/sps': 42060.39699637188, 'training/walltime': 1069.307555437088, 'training/entropy_loss': Array(-0.02133927, dtype=float32), 'training/policy_loss': Array(0.00022809, dtype=float32), 'training/total_loss': Array(7.2646174, dtype=float32), 'training/v_loss': Array(7.2857285, dtype=float32), 'eval/episode_goal_distance': (Array(0.28998116, dtype=float32), Array(0.05934819, dtype=float32)), 'eval/episode_reward': (Array(-14685.561, dtype=float32), Array(3580.8774, dtype=float32)), 'eval/avg_episode_length': (Array(968.9219, dtype=float32), Array(173.0358, dtype=float32)), 'eval/epoch_eval_time': 4.111427545547485, 'eval/sps': 31132.738831459883}
I0727 04:37:49.546505 140120985872192 train.py:379] starting iteration 181 1833.8721590042114
I0727 04:37:59.510305 140120985872192 train.py:394] {'eval/walltime': 762.386559009552, 'training/sps': 42035.91549542179, 'training/walltime': 1075.1539845466614, 'training/entropy_loss': Array(-0.02157436, dtype=float32), 'training/policy_loss': Array(0.00059337, dtype=float32), 'training/total_loss': Array(7.2935343, dtype=float32), 'training/v_loss': Array(7.314515, dtype=float32), 'eval/episode_goal_distance': (Array(0.28755, dtype=float32), Array(0.06839078, dtype=float32)), 'eval/episode_reward': (Array(-13738.297, dtype=float32), Array(5175.1743, dtype=float32)), 'eval/avg_episode_length': (Array(906.8125, dtype=float32), Array(289.73236, dtype=float32)), 'eval/epoch_eval_time': 4.1127026081085205, 'eval/sps': 31123.086738058282}
I0727 04:37:59.512650 140120985872192 train.py:379] starting iteration 182 1843.8383040428162
I0727 04:38:09.430908 140120985872192 train.py:394] {'eval/walltime': 766.4877717494965, 'training/sps': 42274.31089908358, 'training/walltime': 1080.9674441814423, 'training/entropy_loss': Array(-0.02148935, dtype=float32), 'training/policy_loss': Array(0.00016826, dtype=float32), 'training/total_loss': Array(6.9287, dtype=float32), 'training/v_loss': Array(6.950021, dtype=float32), 'eval/episode_goal_distance': (Array(0.28990468, dtype=float32), Array(0.06141086, dtype=float32)), 'eval/episode_reward': (Array(-13082.689, dtype=float32), Array(5262.662, dtype=float32)), 'eval/avg_episode_length': (Array(883.53125, dtype=float32), Array(319.6715, dtype=float32)), 'eval/epoch_eval_time': 4.101212739944458, 'eval/sps': 31210.28049906367}
I0727 04:38:09.433416 140120985872192 train.py:379] starting iteration 183 1853.7590703964233
I0727 04:38:19.374470 140120985872192 train.py:394] {'eval/walltime': 770.5845696926117, 'training/sps': 42078.318644045656, 'training/walltime': 1086.8079817295074, 'training/entropy_loss': Array(-0.02205732, dtype=float32), 'training/policy_loss': Array(0.00045215, dtype=float32), 'training/total_loss': Array(76.73247, dtype=float32), 'training/v_loss': Array(76.754074, dtype=float32), 'eval/episode_goal_distance': (Array(0.28808033, dtype=float32), Array(0.05977865, dtype=float32)), 'eval/episode_reward': (Array(-13778.172, dtype=float32), Array(4770.964, dtype=float32)), 'eval/avg_episode_length': (Array(914.59375, dtype=float32), Array(278.53964, dtype=float32)), 'eval/epoch_eval_time': 4.096797943115234, 'eval/sps': 31243.91336290017}
I0727 04:38:19.376971 140120985872192 train.py:379] starting iteration 184 1863.7026250362396
I0727 04:38:29.299678 140120985872192 train.py:394] {'eval/walltime': 774.6783971786499, 'training/sps': 42188.582148748625, 'training/walltime': 1092.6332545280457, 'training/entropy_loss': Array(-0.02350019, dtype=float32), 'training/policy_loss': Array(0.00059838, dtype=float32), 'training/total_loss': Array(15.977093, dtype=float32), 'training/v_loss': Array(15.999994, dtype=float32), 'eval/episode_goal_distance': (Array(0.29604614, dtype=float32), Array(0.06665137, dtype=float32)), 'eval/episode_reward': (Array(-14147.942, dtype=float32), Array(5426.275, dtype=float32)), 'eval/avg_episode_length': (Array(906.8281, dtype=float32), Array(289.68347, dtype=float32)), 'eval/epoch_eval_time': 4.093827486038208, 'eval/sps': 31266.583762148635}
I0727 04:38:29.302218 140120985872192 train.py:379] starting iteration 185 1873.6278719902039
I0727 04:38:39.288741 140120985872192 train.py:394] {'eval/walltime': 778.7902646064758, 'training/sps': 41859.69363002937, 'training/walltime': 1098.5042960643768, 'training/entropy_loss': Array(-0.02485497, dtype=float32), 'training/policy_loss': Array(0.00061906, dtype=float32), 'training/total_loss': Array(12.206759, dtype=float32), 'training/v_loss': Array(12.230995, dtype=float32), 'eval/episode_goal_distance': (Array(0.292631, dtype=float32), Array(0.06431276, dtype=float32)), 'eval/episode_reward': (Array(-13799.281, dtype=float32), Array(5604.9077, dtype=float32)), 'eval/avg_episode_length': (Array(891.22656, dtype=float32), Array(310.39328, dtype=float32)), 'eval/epoch_eval_time': 4.111867427825928, 'eval/sps': 31129.40829118063}
I0727 04:38:39.291110 140120985872192 train.py:379] starting iteration 186 1883.6167635917664
I0727 04:38:49.185455 140120985872192 train.py:394] {'eval/walltime': 782.8789677619934, 'training/sps': 42358.17284485129, 'training/walltime': 1104.3062460422516, 'training/entropy_loss': Array(-0.0252194, dtype=float32), 'training/policy_loss': Array(0.0005296, dtype=float32), 'training/total_loss': Array(7.630766, dtype=float32), 'training/v_loss': Array(7.6554556, dtype=float32), 'eval/episode_goal_distance': (Array(0.28572512, dtype=float32), Array(0.05530906, dtype=float32)), 'eval/episode_reward': (Array(-13424.225, dtype=float32), Array(5270.5376, dtype=float32)), 'eval/avg_episode_length': (Array(891.21094, dtype=float32), Array(310.43738, dtype=float32)), 'eval/epoch_eval_time': 4.088703155517578, 'eval/sps': 31305.769856945466}
I0727 04:38:49.187843 140120985872192 train.py:379] starting iteration 187 1893.513496875763
I0727 04:38:59.123777 140120985872192 train.py:394] {'eval/walltime': 786.9769375324249, 'training/sps': 42123.1621103113, 'training/walltime': 1110.1405658721924, 'training/entropy_loss': Array(-0.02440579, dtype=float32), 'training/policy_loss': Array(0.00056249, dtype=float32), 'training/total_loss': Array(7.179058, dtype=float32), 'training/v_loss': Array(7.202902, dtype=float32), 'eval/episode_goal_distance': (Array(0.28943598, dtype=float32), Array(0.06414237, dtype=float32)), 'eval/episode_reward': (Array(-13659.594, dtype=float32), Array(5100.235, dtype=float32)), 'eval/avg_episode_length': (Array(899.02344, dtype=float32), Array(300.32993, dtype=float32)), 'eval/epoch_eval_time': 4.0979697704315186, 'eval/sps': 31234.979067823024}
I0727 04:38:59.126208 140120985872192 train.py:379] starting iteration 188 1903.4518625736237
I0727 04:39:09.068172 140120985872192 train.py:394] {'eval/walltime': 791.0709178447723, 'training/sps': 42051.96513005571, 'training/walltime': 1115.984763622284, 'training/entropy_loss': Array(-0.0252804, dtype=float32), 'training/policy_loss': Array(0.0004433, dtype=float32), 'training/total_loss': Array(9.117155, dtype=float32), 'training/v_loss': Array(9.141993, dtype=float32), 'eval/episode_goal_distance': (Array(0.29525784, dtype=float32), Array(0.05283957, dtype=float32)), 'eval/episode_reward': (Array(-14628.961, dtype=float32), Array(3959.8848, dtype=float32)), 'eval/avg_episode_length': (Array(953.3906, dtype=float32), Array(210.1736, dtype=float32)), 'eval/epoch_eval_time': 4.093980312347412, 'eval/sps': 31265.41659566682}
I0727 04:39:09.070737 140120985872192 train.py:379] starting iteration 189 1913.3963911533356
I0727 04:39:19.022985 140120985872192 train.py:394] {'eval/walltime': 795.188633441925, 'training/sps': 42148.59673053937, 'training/walltime': 1121.8155627250671, 'training/entropy_loss': Array(-0.02573891, dtype=float32), 'training/policy_loss': Array(0.00035927, dtype=float32), 'training/total_loss': Array(6.992275, dtype=float32), 'training/v_loss': Array(7.0176544, dtype=float32), 'eval/episode_goal_distance': (Array(0.2838226, dtype=float32), Array(0.05706709, dtype=float32)), 'eval/episode_reward': (Array(-14184.186, dtype=float32), Array(4110.791, dtype=float32)), 'eval/avg_episode_length': (Array(945.6406, dtype=float32), Array(226.00534, dtype=float32)), 'eval/epoch_eval_time': 4.11771559715271, 'eval/sps': 31085.19687190358}
I0727 04:39:19.025370 140120985872192 train.py:379] starting iteration 190 1923.3510241508484
I0727 04:39:28.958883 140120985872192 train.py:394] {'eval/walltime': 799.2851321697235, 'training/sps': 42134.29189877049, 'training/walltime': 1127.6483414173126, 'training/entropy_loss': Array(-0.02618272, dtype=float32), 'training/policy_loss': Array(0.00045442, dtype=float32), 'training/total_loss': Array(7.705822, dtype=float32), 'training/v_loss': Array(7.73155, dtype=float32), 'eval/episode_goal_distance': (Array(0.28444082, dtype=float32), Array(0.06238971, dtype=float32)), 'eval/episode_reward': (Array(-14518.383, dtype=float32), Array(4043.4229, dtype=float32)), 'eval/avg_episode_length': (Array(961.125, dtype=float32), Array(192.81386, dtype=float32)), 'eval/epoch_eval_time': 4.096498727798462, 'eval/sps': 31246.19547210007}
I0727 04:39:28.961441 140120985872192 train.py:379] starting iteration 191 1933.2870953083038
I0727 04:39:38.916546 140120985872192 train.py:394] {'eval/walltime': 803.4151248931885, 'training/sps': 42219.15492237809, 'training/walltime': 1133.4693958759308, 'training/entropy_loss': Array(-0.02731805, dtype=float32), 'training/policy_loss': Array(0.00055696, dtype=float32), 'training/total_loss': Array(54.044357, dtype=float32), 'training/v_loss': Array(54.071114, dtype=float32), 'eval/episode_goal_distance': (Array(0.2897726, dtype=float32), Array(0.0543485, dtype=float32)), 'eval/episode_reward': (Array(-13903.448, dtype=float32), Array(5057.941, dtype=float32)), 'eval/avg_episode_length': (Array(906.8281, dtype=float32), Array(289.6837, dtype=float32)), 'eval/epoch_eval_time': 4.129992723464966, 'eval/sps': 30992.790682839517}
I0727 04:39:38.919068 140120985872192 train.py:379] starting iteration 192 1943.2447218894958
I0727 04:39:48.860768 140120985872192 train.py:394] {'eval/walltime': 807.5148792266846, 'training/sps': 42096.386227238254, 'training/walltime': 1139.3074266910553, 'training/entropy_loss': Array(-0.0302002, dtype=float32), 'training/policy_loss': Array(0.00015069, dtype=float32), 'training/total_loss': Array(27.309763, dtype=float32), 'training/v_loss': Array(27.339811, dtype=float32), 'eval/episode_goal_distance': (Array(0.2875073, dtype=float32), Array(0.0601072, dtype=float32)), 'eval/episode_reward': (Array(-13761.987, dtype=float32), Array(5100.5874, dtype=float32)), 'eval/avg_episode_length': (Array(906.78125, dtype=float32), Array(289.82913, dtype=float32)), 'eval/epoch_eval_time': 4.099754333496094, 'eval/sps': 31221.382938535033}
I0727 04:39:48.863839 140120985872192 train.py:379] starting iteration 193 1953.1894927024841
I0727 04:39:58.827324 140120985872192 train.py:394] {'eval/walltime': 811.6172547340393, 'training/sps': 41958.05038325885, 'training/walltime': 1145.1647055149078, 'training/entropy_loss': Array(-0.03086996, dtype=float32), 'training/policy_loss': Array(0.00033097, dtype=float32), 'training/total_loss': Array(16.178583, dtype=float32), 'training/v_loss': Array(16.209122, dtype=float32), 'eval/episode_goal_distance': (Array(0.29310092, dtype=float32), Array(0.05919245, dtype=float32)), 'eval/episode_reward': (Array(-13726.651, dtype=float32), Array(5035.2534, dtype=float32)), 'eval/avg_episode_length': (Array(898.9922, dtype=float32), Array(300.42297, dtype=float32)), 'eval/epoch_eval_time': 4.102375507354736, 'eval/sps': 31201.434332503613}
I0727 04:39:58.829971 140120985872192 train.py:379] starting iteration 194 1963.1556255817413
I0727 04:40:08.792141 140120985872192 train.py:394] {'eval/walltime': 815.7361829280853, 'training/sps': 42085.9259656054, 'training/walltime': 1151.0041873455048, 'training/entropy_loss': Array(-0.03156928, dtype=float32), 'training/policy_loss': Array(0.00019129, dtype=float32), 'training/total_loss': Array(11.025703, dtype=float32), 'training/v_loss': Array(11.057083, dtype=float32), 'eval/episode_goal_distance': (Array(0.29192626, dtype=float32), Array(0.06213235, dtype=float32)), 'eval/episode_reward': (Array(-13810.48, dtype=float32), Array(5282.3794, dtype=float32)), 'eval/avg_episode_length': (Array(899.16406, dtype=float32), Array(299.91174, dtype=float32)), 'eval/epoch_eval_time': 4.1189281940460205, 'eval/sps': 31076.045507427425}
I0727 04:40:08.794665 140120985872192 train.py:379] starting iteration 195 1973.1203184127808
I0727 04:40:18.711327 140120985872192 train.py:394] {'eval/walltime': 819.8306484222412, 'training/sps': 42238.05154728774, 'training/walltime': 1156.8226375579834, 'training/entropy_loss': Array(-0.03247717, dtype=float32), 'training/policy_loss': Array(-8.3067556e-05, dtype=float32), 'training/total_loss': Array(9.44429, dtype=float32), 'training/v_loss': Array(9.476851, dtype=float32), 'eval/episode_goal_distance': (Array(0.29549333, dtype=float32), Array(0.05781632, dtype=float32)), 'eval/episode_reward': (Array(-14291.188, dtype=float32), Array(4914.5635, dtype=float32)), 'eval/avg_episode_length': (Array(914.58594, dtype=float32), Array(278.5652, dtype=float32)), 'eval/epoch_eval_time': 4.094465494155884, 'eval/sps': 31261.71173812481}
I0727 04:40:18.713883 140120985872192 train.py:379] starting iteration 196 1983.039537191391
I0727 04:40:28.670441 140120985872192 train.py:394] {'eval/walltime': 823.9403891563416, 'training/sps': 42060.01599663778, 'training/walltime': 1162.6657166481018, 'training/entropy_loss': Array(-0.0312204, dtype=float32), 'training/policy_loss': Array(0.00029115, dtype=float32), 'training/total_loss': Array(8.600487, dtype=float32), 'training/v_loss': Array(8.631415, dtype=float32), 'eval/episode_goal_distance': (Array(0.2806404, dtype=float32), Array(0.06588773, dtype=float32)), 'eval/episode_reward': (Array(-13598.6, dtype=float32), Array(5216.0757, dtype=float32)), 'eval/avg_episode_length': (Array(899.10156, dtype=float32), Array(300.0977, dtype=float32)), 'eval/epoch_eval_time': 4.109740734100342, 'eval/sps': 31145.517024450528}
I0727 04:40:28.672883 140120985872192 train.py:379] starting iteration 197 1992.9985373020172
I0727 04:40:38.598764 140120985872192 train.py:394] {'eval/walltime': 828.0422592163086, 'training/sps': 42225.10943526599, 'training/walltime': 1168.4859502315521, 'training/entropy_loss': Array(-0.03147554, dtype=float32), 'training/policy_loss': Array(0.00016574, dtype=float32), 'training/total_loss': Array(8.175579, dtype=float32), 'training/v_loss': Array(8.206888, dtype=float32), 'eval/episode_goal_distance': (Array(0.2886808, dtype=float32), Array(0.06127089, dtype=float32)), 'eval/episode_reward': (Array(-13774.719, dtype=float32), Array(5028.169, dtype=float32)), 'eval/avg_episode_length': (Array(906.7969, dtype=float32), Array(289.78073, dtype=float32)), 'eval/epoch_eval_time': 4.101870059967041, 'eval/sps': 31205.279087029026}
I0727 04:40:38.601248 140120985872192 train.py:379] starting iteration 198 2002.9269018173218
I0727 04:40:48.554310 140120985872192 train.py:394] {'eval/walltime': 832.1625094413757, 'training/sps': 42161.04706129784, 'training/walltime': 1174.315027475357, 'training/entropy_loss': Array(-0.03233818, dtype=float32), 'training/policy_loss': Array(0.00012461, dtype=float32), 'training/total_loss': Array(8.483763, dtype=float32), 'training/v_loss': Array(8.515976, dtype=float32), 'eval/episode_goal_distance': (Array(0.2987194, dtype=float32), Array(0.05868356, dtype=float32)), 'eval/episode_reward': (Array(-14621.05, dtype=float32), Array(4521.551, dtype=float32)), 'eval/avg_episode_length': (Array(937.8594, dtype=float32), Array(240.67035, dtype=float32)), 'eval/epoch_eval_time': 4.120250225067139, 'eval/sps': 31066.07439064317}
I0727 04:40:48.556742 140120985872192 train.py:379] starting iteration 199 2012.8823964595795
I0727 04:40:58.532842 140120985872192 train.py:394] {'eval/walltime': 836.2819867134094, 'training/sps': 41988.956878911165, 'training/walltime': 1180.1679949760437, 'training/entropy_loss': Array(-0.03289252, dtype=float32), 'training/policy_loss': Array(-3.6056328e-05, dtype=float32), 'training/total_loss': Array(12.042805, dtype=float32), 'training/v_loss': Array(12.075735, dtype=float32), 'eval/episode_goal_distance': (Array(0.29872662, dtype=float32), Array(0.06396198, dtype=float32)), 'eval/episode_reward': (Array(-14458.13, dtype=float32), Array(5018.265, dtype=float32)), 'eval/avg_episode_length': (Array(922.28906, dtype=float32), Array(266.9462, dtype=float32)), 'eval/epoch_eval_time': 4.119477272033691, 'eval/sps': 31071.903435167962}
I0727 04:40:58.535209 140120985872192 train.py:379] starting iteration 200 2022.8608632087708
I0727 04:41:08.448063 140120985872192 train.py:394] {'eval/walltime': 840.3752915859222, 'training/sps': 42257.185099891576, 'training/walltime': 1185.9838106632233, 'training/entropy_loss': Array(-0.03474742, dtype=float32), 'training/policy_loss': Array(0.00059119, dtype=float32), 'training/total_loss': Array(69.95063, dtype=float32), 'training/v_loss': Array(69.98479, dtype=float32), 'eval/episode_goal_distance': (Array(0.29577917, dtype=float32), Array(0.06452171, dtype=float32)), 'eval/episode_reward': (Array(-13778.871, dtype=float32), Array(5761.6763, dtype=float32)), 'eval/avg_episode_length': (Array(875.7656, dtype=float32), Array(328.69385, dtype=float32)), 'eval/epoch_eval_time': 4.093304872512817, 'eval/sps': 31270.575729538257}
I0727 04:41:08.450576 140120985872192 train.py:379] starting iteration 201 2032.7762296199799
I0727 04:41:18.418550 140120985872192 train.py:394] {'eval/walltime': 844.4841570854187, 'training/sps': 41971.60343861942, 'training/walltime': 1191.8391981124878, 'training/entropy_loss': Array(-0.03522891, dtype=float32), 'training/policy_loss': Array(0.00038738, dtype=float32), 'training/total_loss': Array(14.274875, dtype=float32), 'training/v_loss': Array(14.309715, dtype=float32), 'eval/episode_goal_distance': (Array(0.28966567, dtype=float32), Array(0.06257264, dtype=float32)), 'eval/episode_reward': (Array(-14511.172, dtype=float32), Array(4273.9043, dtype=float32)), 'eval/avg_episode_length': (Array(945.6328, dtype=float32), Array(226.03784, dtype=float32)), 'eval/epoch_eval_time': 4.10886549949646, 'eval/sps': 31152.151370174164}
I0727 04:41:18.421241 140120985872192 train.py:379] starting iteration 202 2042.7468945980072
I0727 04:41:28.338566 140120985872192 train.py:394] {'eval/walltime': 848.5780844688416, 'training/sps': 42227.87540469713, 'training/walltime': 1197.6590504646301, 'training/entropy_loss': Array(-0.03462392, dtype=float32), 'training/policy_loss': Array(0.00137255, dtype=float32), 'training/total_loss': Array(14.46335, dtype=float32), 'training/v_loss': Array(14.496602, dtype=float32), 'eval/episode_goal_distance': (Array(0.29428527, dtype=float32), Array(0.06218998, dtype=float32)), 'eval/episode_reward': (Array(-13766.592, dtype=float32), Array(5733.868, dtype=float32)), 'eval/avg_episode_length': (Array(875.71875, dtype=float32), Array(328.81775, dtype=float32)), 'eval/epoch_eval_time': 4.093927383422852, 'eval/sps': 31265.82081506823}
I0727 04:41:28.340963 140120985872192 train.py:379] starting iteration 203 2052.6666173934937
I0727 04:41:38.265531 140120985872192 train.py:394] {'eval/walltime': 852.6819846630096, 'training/sps': 42248.855978504565, 'training/walltime': 1203.4760127067566, 'training/entropy_loss': Array(-0.03211232, dtype=float32), 'training/policy_loss': Array(0.00176936, dtype=float32), 'training/total_loss': Array(8.040583, dtype=float32), 'training/v_loss': Array(8.070925, dtype=float32), 'eval/episode_goal_distance': (Array(0.28890038, dtype=float32), Array(0.06134659, dtype=float32)), 'eval/episode_reward': (Array(-13892.527, dtype=float32), Array(5207.6006, dtype=float32)), 'eval/avg_episode_length': (Array(906.8203, dtype=float32), Array(289.708, dtype=float32)), 'eval/epoch_eval_time': 4.103900194168091, 'eval/sps': 31189.842331423246}
I0727 04:41:38.268105 140120985872192 train.py:379] starting iteration 204 2062.593758583069
I0727 04:41:48.174575 140120985872192 train.py:394] {'eval/walltime': 856.7751190662384, 'training/sps': 42301.33103456596, 'training/walltime': 1209.285758972168, 'training/entropy_loss': Array(-0.03098337, dtype=float32), 'training/policy_loss': Array(0.00186842, dtype=float32), 'training/total_loss': Array(10.019709, dtype=float32), 'training/v_loss': Array(10.048824, dtype=float32), 'eval/episode_goal_distance': (Array(0.28686982, dtype=float32), Array(0.06460051, dtype=float32)), 'eval/episode_reward': (Array(-13660.142, dtype=float32), Array(5428.965, dtype=float32)), 'eval/avg_episode_length': (Array(891.3828, dtype=float32), Array(309.9476, dtype=float32)), 'eval/epoch_eval_time': 4.09313440322876, 'eval/sps': 31271.878074423996}
I0727 04:41:48.275773 140120985872192 train.py:379] starting iteration 205 2072.6014244556427
I0727 04:41:58.222511 140120985872192 train.py:394] {'eval/walltime': 860.8790752887726, 'training/sps': 42089.581138105554, 'training/walltime': 1215.1247336864471, 'training/entropy_loss': Array(-0.03076433, dtype=float32), 'training/policy_loss': Array(0.00246113, dtype=float32), 'training/total_loss': Array(8.259017, dtype=float32), 'training/v_loss': Array(8.287321, dtype=float32), 'eval/episode_goal_distance': (Array(0.2867071, dtype=float32), Array(0.05631091, dtype=float32)), 'eval/episode_reward': (Array(-14180.622, dtype=float32), Array(4613.993, dtype=float32)), 'eval/avg_episode_length': (Array(930.1328, dtype=float32), Array(254.05438, dtype=float32)), 'eval/epoch_eval_time': 4.10395622253418, 'eval/sps': 31189.416518912185}
I0727 04:41:58.225134 140120985872192 train.py:379] starting iteration 206 2082.5507876873016
I0727 04:42:08.143228 140120985872192 train.py:394] {'eval/walltime': 864.976220369339, 'training/sps': 42246.404111817916, 'training/walltime': 1220.9420335292816, 'training/entropy_loss': Array(-0.03077441, dtype=float32), 'training/policy_loss': Array(0.00218526, dtype=float32), 'training/total_loss': Array(7.541209, dtype=float32), 'training/v_loss': Array(7.5697985, dtype=float32), 'eval/episode_goal_distance': (Array(0.2934687, dtype=float32), Array(0.05782276, dtype=float32)), 'eval/episode_reward': (Array(-13345.899, dtype=float32), Array(6028.3, dtype=float32)), 'eval/avg_episode_length': (Array(860.27344, dtype=float32), Array(345.41388, dtype=float32)), 'eval/epoch_eval_time': 4.097145080566406, 'eval/sps': 31241.266170224255}
I0727 04:42:08.145664 140120985872192 train.py:379] starting iteration 207 2092.4713187217712
I0727 04:42:18.099795 140120985872192 train.py:394] {'eval/walltime': 869.0714902877808, 'training/sps': 41988.496784890085, 'training/walltime': 1226.795065164566, 'training/entropy_loss': Array(-0.03136411, dtype=float32), 'training/policy_loss': Array(0.0018942, dtype=float32), 'training/total_loss': Array(6.8533535, dtype=float32), 'training/v_loss': Array(6.882823, dtype=float32), 'eval/episode_goal_distance': (Array(0.29447776, dtype=float32), Array(0.07051322, dtype=float32)), 'eval/episode_reward': (Array(-13051.122, dtype=float32), Array(6210.8223, dtype=float32)), 'eval/avg_episode_length': (Array(844.5547, dtype=float32), Array(361.22256, dtype=float32)), 'eval/epoch_eval_time': 4.0952699184417725, 'eval/sps': 31255.57107325011}
I0727 04:42:18.102196 140120985872192 train.py:379] starting iteration 208 2102.427850484848
I0727 04:42:28.021902 140120985872192 train.py:394] {'eval/walltime': 873.1759300231934, 'training/sps': 42288.01355589765, 'training/walltime': 1232.6066410541534, 'training/entropy_loss': Array(-0.03326233, dtype=float32), 'training/policy_loss': Array(0.00286572, dtype=float32), 'training/total_loss': Array(76.22578, dtype=float32), 'training/v_loss': Array(76.25617, dtype=float32), 'eval/episode_goal_distance': (Array(0.3014903, dtype=float32), Array(0.06311065, dtype=float32)), 'eval/episode_reward': (Array(-14448.104, dtype=float32), Array(4613.2383, dtype=float32)), 'eval/avg_episode_length': (Array(937.85156, dtype=float32), Array(240.70018, dtype=float32)), 'eval/epoch_eval_time': 4.104439735412598, 'eval/sps': 31185.74233058701}
I0727 04:42:28.024280 140120985872192 train.py:379] starting iteration 209 2112.349933862686
I0727 04:42:37.998687 140120985872192 train.py:394] {'eval/walltime': 877.2677080631256, 'training/sps': 41819.461924505886, 'training/walltime': 1238.4833307266235, 'training/entropy_loss': Array(-0.03264576, dtype=float32), 'training/policy_loss': Array(0.00247106, dtype=float32), 'training/total_loss': Array(18.994091, dtype=float32), 'training/v_loss': Array(19.024265, dtype=float32), 'eval/episode_goal_distance': (Array(0.29256237, dtype=float32), Array(0.05899034, dtype=float32)), 'eval/episode_reward': (Array(-14684.889, dtype=float32), Array(4562.006, dtype=float32)), 'eval/avg_episode_length': (Array(937.83594, dtype=float32), Array(240.76044, dtype=float32)), 'eval/epoch_eval_time': 4.091778039932251, 'eval/sps': 31282.244234860635}
I0727 04:42:38.001085 140120985872192 train.py:379] starting iteration 210 2122.326738834381
I0727 04:42:47.956054 140120985872192 train.py:394] {'eval/walltime': 881.3986947536469, 'training/sps': 42224.74101265269, 'training/walltime': 1244.3036150932312, 'training/entropy_loss': Array(-0.03321377, dtype=float32), 'training/policy_loss': Array(0.00370479, dtype=float32), 'training/total_loss': Array(13.55261, dtype=float32), 'training/v_loss': Array(13.58212, dtype=float32), 'eval/episode_goal_distance': (Array(0.29746017, dtype=float32), Array(0.05640299, dtype=float32)), 'eval/episode_reward': (Array(-15094.8125, dtype=float32), Array(4528.953, dtype=float32)), 'eval/avg_episode_length': (Array(945.72656, dtype=float32), Array(225.64792, dtype=float32)), 'eval/epoch_eval_time': 4.13098669052124, 'eval/sps': 30985.33342983228}
I0727 04:42:47.958393 140120985872192 train.py:379] starting iteration 211 2132.2840468883514
I0727 04:42:57.904843 140120985872192 train.py:394] {'eval/walltime': 885.5057866573334, 'training/sps': 42113.21157805101, 'training/walltime': 1250.1393134593964, 'training/entropy_loss': Array(-0.03297865, dtype=float32), 'training/policy_loss': Array(0.00331375, dtype=float32), 'training/total_loss': Array(11.756368, dtype=float32), 'training/v_loss': Array(11.786033, dtype=float32), 'eval/episode_goal_distance': (Array(0.29518032, dtype=float32), Array(0.05575335, dtype=float32)), 'eval/episode_reward': (Array(-14057.776, dtype=float32), Array(5451.4097, dtype=float32)), 'eval/avg_episode_length': (Array(891.3594, dtype=float32), Array(310.0141, dtype=float32)), 'eval/epoch_eval_time': 4.107091903686523, 'eval/sps': 31165.604033624684}
I0727 04:42:57.907154 140120985872192 train.py:379] starting iteration 212 2142.2328083515167
I0727 04:43:07.843421 140120985872192 train.py:394] {'eval/walltime': 889.644850730896, 'training/sps': 42420.009649514555, 'training/walltime': 1255.932805776596, 'training/entropy_loss': Array(-0.03018714, dtype=float32), 'training/policy_loss': Array(0.00280073, dtype=float32), 'training/total_loss': Array(9.575525, dtype=float32), 'training/v_loss': Array(9.602912, dtype=float32), 'eval/episode_goal_distance': (Array(0.29120106, dtype=float32), Array(0.05890181, dtype=float32)), 'eval/episode_reward': (Array(-14053.682, dtype=float32), Array(5185.5347, dtype=float32)), 'eval/avg_episode_length': (Array(906.90625, dtype=float32), Array(289.4409, dtype=float32)), 'eval/epoch_eval_time': 4.139064073562622, 'eval/sps': 30924.86555537334}
I0727 04:43:07.848160 140120985872192 train.py:379] starting iteration 213 2152.173798561096
I0727 04:43:17.811532 140120985872192 train.py:394] {'eval/walltime': 893.7504057884216, 'training/sps': 41982.79177264606, 'training/walltime': 1261.7866327762604, 'training/entropy_loss': Array(-0.03031623, dtype=float32), 'training/policy_loss': Array(0.00243316, dtype=float32), 'training/total_loss': Array(8.846285, dtype=float32), 'training/v_loss': Array(8.874167, dtype=float32), 'eval/episode_goal_distance': (Array(0.2956661, dtype=float32), Array(0.0569506, dtype=float32)), 'eval/episode_reward': (Array(-14546.227, dtype=float32), Array(4799.911, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.08237, dtype=float32)), 'eval/epoch_eval_time': 4.105555057525635, 'eval/sps': 31177.270358455247}
I0727 04:43:17.814014 140120985872192 train.py:379] starting iteration 214 2162.1396684646606
I0727 04:43:27.710262 140120985872192 train.py:394] {'eval/walltime': 897.8381774425507, 'training/sps': 42341.11818207509, 'training/walltime': 1267.5909197330475, 'training/entropy_loss': Array(-0.03010401, dtype=float32), 'training/policy_loss': Array(0.00379842, dtype=float32), 'training/total_loss': Array(9.391241, dtype=float32), 'training/v_loss': Array(9.417546, dtype=float32), 'eval/episode_goal_distance': (Array(0.28997922, dtype=float32), Array(0.06433533, dtype=float32)), 'eval/episode_reward': (Array(-14279.456, dtype=float32), Array(5234.565, dtype=float32)), 'eval/avg_episode_length': (Array(914.625, dtype=float32), Array(278.4377, dtype=float32)), 'eval/epoch_eval_time': 4.087771654129028, 'eval/sps': 31312.903662490084}
I0727 04:43:27.712860 140120985872192 train.py:379] starting iteration 215 2172.0385134220123
I0727 04:43:37.696870 140120985872192 train.py:394] {'eval/walltime': 901.9595186710358, 'training/sps': 41948.939069556654, 'training/walltime': 1273.449470758438, 'training/entropy_loss': Array(-0.02907632, dtype=float32), 'training/policy_loss': Array(0.00528379, dtype=float32), 'training/total_loss': Array(8.901825, dtype=float32), 'training/v_loss': Array(8.925617, dtype=float32), 'eval/episode_goal_distance': (Array(0.2905151, dtype=float32), Array(0.0593705, dtype=float32)), 'eval/episode_reward': (Array(-13442.071, dtype=float32), Array(5905.6313, dtype=float32)), 'eval/avg_episode_length': (Array(860.16406, dtype=float32), Array(345.68417, dtype=float32)), 'eval/epoch_eval_time': 4.121341228485107, 'eval/sps': 31057.85056459625}
I0727 04:43:37.699412 140120985872192 train.py:379] starting iteration 216 2182.025066137314
I0727 04:43:47.648763 140120985872192 train.py:394] {'eval/walltime': 906.0585396289825, 'training/sps': 42037.050348478304, 'training/walltime': 1279.295742034912, 'training/entropy_loss': Array(-0.03076564, dtype=float32), 'training/policy_loss': Array(0.00296531, dtype=float32), 'training/total_loss': Array(66.21012, dtype=float32), 'training/v_loss': Array(66.23792, dtype=float32), 'eval/episode_goal_distance': (Array(0.30141276, dtype=float32), Array(0.05825863, dtype=float32)), 'eval/episode_reward': (Array(-13892.026, dtype=float32), Array(5727.031, dtype=float32)), 'eval/avg_episode_length': (Array(875.78125, dtype=float32), Array(328.65256, dtype=float32)), 'eval/epoch_eval_time': 4.099020957946777, 'eval/sps': 31226.968906281447}
I0727 04:43:47.651138 140120985872192 train.py:379] starting iteration 217 2191.976791858673
I0727 04:43:57.609180 140120985872192 train.py:394] {'eval/walltime': 910.1900150775909, 'training/sps': 42209.52541439808, 'training/walltime': 1285.1181244850159, 'training/entropy_loss': Array(-0.0346139, dtype=float32), 'training/policy_loss': Array(0.00244575, dtype=float32), 'training/total_loss': Array(37.80152, dtype=float32), 'training/v_loss': Array(37.833687, dtype=float32), 'eval/episode_goal_distance': (Array(0.30631098, dtype=float32), Array(0.06449657, dtype=float32)), 'eval/episode_reward': (Array(-13272.448, dtype=float32), Array(6413.0337, dtype=float32)), 'eval/avg_episode_length': (Array(836.8125, dtype=float32), Array(368.3577, dtype=float32)), 'eval/epoch_eval_time': 4.131475448608398, 'eval/sps': 30981.667830826427}
I0727 04:43:57.611570 140120985872192 train.py:379] starting iteration 218 2201.937224149704
I0727 04:44:07.581637 140120985872192 train.py:394] {'eval/walltime': 914.3023223876953, 'training/sps': 42000.44881659534, 'training/walltime': 1290.9694905281067, 'training/entropy_loss': Array(-0.03627288, dtype=float32), 'training/policy_loss': Array(0.00209085, dtype=float32), 'training/total_loss': Array(21.547092, dtype=float32), 'training/v_loss': Array(21.581276, dtype=float32), 'eval/episode_goal_distance': (Array(0.2903508, dtype=float32), Array(0.05673817, dtype=float32)), 'eval/episode_reward': (Array(-14083.438, dtype=float32), Array(5449.987, dtype=float32)), 'eval/avg_episode_length': (Array(891.3594, dtype=float32), Array(310.01422, dtype=float32)), 'eval/epoch_eval_time': 4.11230731010437, 'eval/sps': 31126.078463418962}
I0727 04:44:07.584018 140120985872192 train.py:379] starting iteration 219 2211.9096727371216
I0727 04:44:17.513908 140120985872192 train.py:394] {'eval/walltime': 918.4001224040985, 'training/sps': 42166.42804519156, 'training/walltime': 1296.7978239059448, 'training/entropy_loss': Array(-0.03829993, dtype=float32), 'training/policy_loss': Array(0.00302657, dtype=float32), 'training/total_loss': Array(15.631973, dtype=float32), 'training/v_loss': Array(15.667246, dtype=float32), 'eval/episode_goal_distance': (Array(0.2943988, dtype=float32), Array(0.06296948, dtype=float32)), 'eval/episode_reward': (Array(-14064.043, dtype=float32), Array(6105.5933, dtype=float32)), 'eval/avg_episode_length': (Array(867.9375, dtype=float32), Array(337.4563, dtype=float32)), 'eval/epoch_eval_time': 4.097800016403198, 'eval/sps': 31236.27299712656}
I0727 04:44:17.516330 140120985872192 train.py:379] starting iteration 220 2221.8419835567474
I0727 04:44:27.469771 140120985872192 train.py:394] {'eval/walltime': 922.5207297801971, 'training/sps': 42160.264173613265, 'training/walltime': 1302.6270093917847, 'training/entropy_loss': Array(-0.03909406, dtype=float32), 'training/policy_loss': Array(0.0056703, dtype=float32), 'training/total_loss': Array(12.527268, dtype=float32), 'training/v_loss': Array(12.560692, dtype=float32), 'eval/episode_goal_distance': (Array(0.29356512, dtype=float32), Array(0.05851097, dtype=float32)), 'eval/episode_reward': (Array(-14571.125, dtype=float32), Array(5779.752, dtype=float32)), 'eval/avg_episode_length': (Array(899.0547, dtype=float32), Array(300.23727, dtype=float32)), 'eval/epoch_eval_time': 4.120607376098633, 'eval/sps': 31063.381758344}
I0727 04:44:27.472122 140120985872192 train.py:379] starting iteration 221 2231.7977764606476
I0727 04:44:37.417161 140120985872192 train.py:394] {'eval/walltime': 926.6218469142914, 'training/sps': 42093.65808312401, 'training/walltime': 1308.4654185771942, 'training/entropy_loss': Array(-0.03961303, dtype=float32), 'training/policy_loss': Array(0.0024929, dtype=float32), 'training/total_loss': Array(12.870931, dtype=float32), 'training/v_loss': Array(12.908052, dtype=float32), 'eval/episode_goal_distance': (Array(0.3081962, dtype=float32), Array(0.06224816, dtype=float32)), 'eval/episode_reward': (Array(-14375.726, dtype=float32), Array(6136.907, dtype=float32)), 'eval/avg_episode_length': (Array(867.96094, dtype=float32), Array(337.39603, dtype=float32)), 'eval/epoch_eval_time': 4.101117134094238, 'eval/sps': 31211.008077746537}
I0727 04:44:37.419542 140120985872192 train.py:379] starting iteration 222 2241.745195865631
I0727 04:44:47.359670 140120985872192 train.py:394] {'eval/walltime': 930.7174310684204, 'training/sps': 42074.94192977039, 'training/walltime': 1314.306424856186, 'training/entropy_loss': Array(-0.0414681, dtype=float32), 'training/policy_loss': Array(0.00210427, dtype=float32), 'training/total_loss': Array(14.15601, dtype=float32), 'training/v_loss': Array(14.195374, dtype=float32), 'eval/episode_goal_distance': (Array(0.30226302, dtype=float32), Array(0.06201731, dtype=float32)), 'eval/episode_reward': (Array(-14486.607, dtype=float32), Array(6168.09, dtype=float32)), 'eval/avg_episode_length': (Array(875.7656, dtype=float32), Array(328.6938, dtype=float32)), 'eval/epoch_eval_time': 4.095584154129028, 'eval/sps': 31253.172974349643}
I0727 04:44:47.362032 140120985872192 train.py:379] starting iteration 223 2251.6876859664917
I0727 04:44:57.320241 140120985872192 train.py:394] {'eval/walltime': 934.8254766464233, 'training/sps': 42034.081345864215, 'training/walltime': 1320.153109073639, 'training/entropy_loss': Array(-0.04277679, dtype=float32), 'training/policy_loss': Array(0.00280054, dtype=float32), 'training/total_loss': Array(15.282254, dtype=float32), 'training/v_loss': Array(15.322229, dtype=float32), 'eval/episode_goal_distance': (Array(0.30312926, dtype=float32), Array(0.06270306, dtype=float32)), 'eval/episode_reward': (Array(-14145.952, dtype=float32), Array(5857.929, dtype=float32)), 'eval/avg_episode_length': (Array(883.5, dtype=float32), Array(319.75732, dtype=float32)), 'eval/epoch_eval_time': 4.10804557800293, 'eval/sps': 31158.36900286424}
I0727 04:44:57.322595 140120985872192 train.py:379] starting iteration 224 2261.648248195648
I0727 04:45:07.240581 140120985872192 train.py:394] {'eval/walltime': 938.9253616333008, 'training/sps': 42267.8814649992, 'training/walltime': 1325.9674530029297, 'training/entropy_loss': Array(-0.04460726, dtype=float32), 'training/policy_loss': Array(0.00083409, dtype=float32), 'training/total_loss': Array(18.716629, dtype=float32), 'training/v_loss': Array(18.760403, dtype=float32), 'eval/episode_goal_distance': (Array(0.29481757, dtype=float32), Array(0.06354902, dtype=float32)), 'eval/episode_reward': (Array(-13229.668, dtype=float32), Array(6586.087, dtype=float32)), 'eval/avg_episode_length': (Array(829.125, dtype=float32), Array(375.07724, dtype=float32)), 'eval/epoch_eval_time': 4.099884986877441, 'eval/sps': 31220.38798885612}
I0727 04:45:07.243020 140120985872192 train.py:379] starting iteration 225 2271.5686745643616
I0727 04:45:17.142705 140120985872192 train.py:394] {'eval/walltime': 943.0152180194855, 'training/sps': 42327.48711769463, 'training/walltime': 1331.773609161377, 'training/entropy_loss': Array(-0.04505352, dtype=float32), 'training/policy_loss': Array(0.00153078, dtype=float32), 'training/total_loss': Array(78.55264, dtype=float32), 'training/v_loss': Array(78.59616, dtype=float32), 'eval/episode_goal_distance': (Array(0.3029851, dtype=float32), Array(0.06971343, dtype=float32)), 'eval/episode_reward': (Array(-14569.539, dtype=float32), Array(6545.0527, dtype=float32)), 'eval/avg_episode_length': (Array(860.2656, dtype=float32), Array(345.43307, dtype=float32)), 'eval/epoch_eval_time': 4.089856386184692, 'eval/sps': 31296.942462913095}
I0727 04:45:17.145283 140120985872192 train.py:379] starting iteration 226 2281.470937013626
I0727 04:45:27.096163 140120985872192 train.py:394] {'eval/walltime': 947.154260635376, 'training/sps': 42313.312475355015, 'training/walltime': 1337.5817103385925, 'training/entropy_loss': Array(-0.04640158, dtype=float32), 'training/policy_loss': Array(0.00056496, dtype=float32), 'training/total_loss': Array(22.185478, dtype=float32), 'training/v_loss': Array(22.231316, dtype=float32), 'eval/episode_goal_distance': (Array(0.29924792, dtype=float32), Array(0.05412943, dtype=float32)), 'eval/episode_reward': (Array(-15714.596, dtype=float32), Array(4213.715, dtype=float32)), 'eval/avg_episode_length': (Array(961.14844, dtype=float32), Array(192.69771, dtype=float32)), 'eval/epoch_eval_time': 4.139042615890503, 'eval/sps': 30925.025876415428}
I0727 04:45:27.098563 140120985872192 train.py:379] starting iteration 227 2291.424217224121
I0727 04:45:37.043312 140120985872192 train.py:394] {'eval/walltime': 951.2526564598083, 'training/sps': 42063.39718138815, 'training/walltime': 1343.42431974411, 'training/entropy_loss': Array(-0.04662067, dtype=float32), 'training/policy_loss': Array(0.00044628, dtype=float32), 'training/total_loss': Array(14.486195, dtype=float32), 'training/v_loss': Array(14.532369, dtype=float32), 'eval/episode_goal_distance': (Array(0.29490465, dtype=float32), Array(0.06226728, dtype=float32)), 'eval/episode_reward': (Array(-14477.988, dtype=float32), Array(6462.0674, dtype=float32)), 'eval/avg_episode_length': (Array(860.25, dtype=float32), Array(345.4718, dtype=float32)), 'eval/epoch_eval_time': 4.098395824432373, 'eval/sps': 31231.73199546385}
I0727 04:45:37.045530 140120985872192 train.py:379] starting iteration 228 2301.371183872223
I0727 04:45:46.955625 140120985872192 train.py:394] {'eval/walltime': 955.3477551937103, 'training/sps': 42289.52119963009, 'training/walltime': 1349.2356884479523, 'training/entropy_loss': Array(-0.04708042, dtype=float32), 'training/policy_loss': Array(0.00058393, dtype=float32), 'training/total_loss': Array(14.474247, dtype=float32), 'training/v_loss': Array(14.520743, dtype=float32), 'eval/episode_goal_distance': (Array(0.29329032, dtype=float32), Array(0.06673574, dtype=float32)), 'eval/episode_reward': (Array(-13260.086, dtype=float32), Array(6935.0063, dtype=float32)), 'eval/avg_episode_length': (Array(813.625, dtype=float32), Array(387.97125, dtype=float32)), 'eval/epoch_eval_time': 4.0950987339019775, 'eval/sps': 31256.87762796292}
I0727 04:45:46.957844 140120985872192 train.py:379] starting iteration 229 2311.2834980487823
I0727 04:45:56.904636 140120985872192 train.py:394] {'eval/walltime': 959.4558644294739, 'training/sps': 42117.021205640376, 'training/walltime': 1355.0708589553833, 'training/entropy_loss': Array(-0.04724559, dtype=float32), 'training/policy_loss': Array(0.00188145, dtype=float32), 'training/total_loss': Array(16.67342, dtype=float32), 'training/v_loss': Array(16.718784, dtype=float32), 'eval/episode_goal_distance': (Array(0.30691898, dtype=float32), Array(0.0667138, dtype=float32)), 'eval/episode_reward': (Array(-14482.593, dtype=float32), Array(6168.5786, dtype=float32)), 'eval/avg_episode_length': (Array(875.77344, dtype=float32), Array(328.6731, dtype=float32)), 'eval/epoch_eval_time': 4.10810923576355, 'eval/sps': 31157.886184155814}
I0727 04:45:56.907121 140120985872192 train.py:379] starting iteration 230 2321.2327756881714
I0727 04:46:06.835627 140120985872192 train.py:394] {'eval/walltime': 963.5665128231049, 'training/sps': 42268.80874985766, 'training/walltime': 1360.8850753307343, 'training/entropy_loss': Array(-0.047984, dtype=float32), 'training/policy_loss': Array(-0.00011065, dtype=float32), 'training/total_loss': Array(16.574295, dtype=float32), 'training/v_loss': Array(16.622387, dtype=float32), 'eval/episode_goal_distance': (Array(0.30412903, dtype=float32), Array(0.06418576, dtype=float32)), 'eval/episode_reward': (Array(-15677.834, dtype=float32), Array(6089.0654, dtype=float32)), 'eval/avg_episode_length': (Array(906.8047, dtype=float32), Array(289.7567, dtype=float32)), 'eval/epoch_eval_time': 4.1106483936309814, 'eval/sps': 31138.63987937343}
I0727 04:46:06.837940 140120985872192 train.py:379] starting iteration 231 2331.163594484329
I0727 04:46:16.760980 140120985872192 train.py:394] {'eval/walltime': 967.6670262813568, 'training/sps': 42235.43653204599, 'training/walltime': 1366.703885793686, 'training/entropy_loss': Array(-0.0474884, dtype=float32), 'training/policy_loss': Array(1.6995764e-06, dtype=float32), 'training/total_loss': Array(18.854385, dtype=float32), 'training/v_loss': Array(18.90187, dtype=float32), 'eval/episode_goal_distance': (Array(0.2991143, dtype=float32), Array(0.05787594, dtype=float32)), 'eval/episode_reward': (Array(-14862.146, dtype=float32), Array(6545.5713, dtype=float32)), 'eval/avg_episode_length': (Array(860.22656, dtype=float32), Array(345.53006, dtype=float32)), 'eval/epoch_eval_time': 4.100513458251953, 'eval/sps': 31215.602949042957}
I0727 04:46:16.763366 140120985872192 train.py:379] starting iteration 232 2341.089020729065
I0727 04:46:26.689820 140120985872192 train.py:394] {'eval/walltime': 971.7994866371155, 'training/sps': 42442.487008683194, 'training/walltime': 1372.4943099021912, 'training/entropy_loss': Array(-0.04687805, dtype=float32), 'training/policy_loss': Array(-0.00036376, dtype=float32), 'training/total_loss': Array(18.697014, dtype=float32), 'training/v_loss': Array(18.744255, dtype=float32), 'eval/episode_goal_distance': (Array(0.30291855, dtype=float32), Array(0.05885617, dtype=float32)), 'eval/episode_reward': (Array(-14974.997, dtype=float32), Array(5919.838, dtype=float32)), 'eval/avg_episode_length': (Array(899.0625, dtype=float32), Array(300.21362, dtype=float32)), 'eval/epoch_eval_time': 4.132460355758667, 'eval/sps': 30974.283835930673}
I0727 04:46:26.692157 140120985872192 train.py:379] starting iteration 233 2351.0178112983704
I0727 04:46:36.647091 140120985872192 train.py:394] {'eval/walltime': 975.9048521518707, 'training/sps': 42039.697434225294, 'training/walltime': 1378.340213060379, 'training/entropy_loss': Array(-0.0472328, dtype=float32), 'training/policy_loss': Array(-0.00066495, dtype=float32), 'training/total_loss': Array(85.30309, dtype=float32), 'training/v_loss': Array(85.35099, dtype=float32), 'eval/episode_goal_distance': (Array(0.31188196, dtype=float32), Array(0.06516369, dtype=float32)), 'eval/episode_reward': (Array(-14368.496, dtype=float32), Array(6883.7896, dtype=float32)), 'eval/avg_episode_length': (Array(844.7344, dtype=float32), Array(360.80557, dtype=float32)), 'eval/epoch_eval_time': 4.105365514755249, 'eval/sps': 31178.70979817762}
I0727 04:46:36.649525 140120985872192 train.py:379] starting iteration 234 2360.975179195404
I0727 04:46:46.564344 140120985872192 train.py:394] {'eval/walltime': 980.0125467777252, 'training/sps': 42346.62177704342, 'training/walltime': 1384.1437456607819, 'training/entropy_loss': Array(-0.04690508, dtype=float32), 'training/policy_loss': Array(-0.00037105, dtype=float32), 'training/total_loss': Array(25.583235, dtype=float32), 'training/v_loss': Array(25.63051, dtype=float32), 'eval/episode_goal_distance': (Array(0.31087, dtype=float32), Array(0.06440917, dtype=float32)), 'eval/episode_reward': (Array(-14477.867, dtype=float32), Array(6798.649, dtype=float32)), 'eval/avg_episode_length': (Array(852.4375, dtype=float32), Array(353.43835, dtype=float32)), 'eval/epoch_eval_time': 4.107694625854492, 'eval/sps': 31161.031103516645}
I0727 04:46:46.566901 140120985872192 train.py:379] starting iteration 235 2370.8925545215607
I0727 04:46:56.519860 140120985872192 train.py:394] {'eval/walltime': 984.119969367981, 'training/sps': 42069.94483767073, 'training/walltime': 1389.9854457378387, 'training/entropy_loss': Array(-0.04642293, dtype=float32), 'training/policy_loss': Array(-0.00098907, dtype=float32), 'training/total_loss': Array(20.14917, dtype=float32), 'training/v_loss': Array(20.196583, dtype=float32), 'eval/episode_goal_distance': (Array(0.30729395, dtype=float32), Array(0.06536737, dtype=float32)), 'eval/episode_reward': (Array(-15034.572, dtype=float32), Array(6212.746, dtype=float32)), 'eval/avg_episode_length': (Array(883.4375, dtype=float32), Array(319.92883, dtype=float32)), 'eval/epoch_eval_time': 4.107422590255737, 'eval/sps': 31163.094906197715}
I0727 04:46:56.522411 140120985872192 train.py:379] starting iteration 236 2380.848065137863
I0727 04:47:06.436386 140120985872192 train.py:394] {'eval/walltime': 988.2194974422455, 'training/sps': 42294.11071934099, 'training/walltime': 1395.7961838245392, 'training/entropy_loss': Array(-0.04512849, dtype=float32), 'training/policy_loss': Array(-0.00087708, dtype=float32), 'training/total_loss': Array(16.568419, dtype=float32), 'training/v_loss': Array(16.614422, dtype=float32), 'eval/episode_goal_distance': (Array(0.30094817, dtype=float32), Array(0.06166531, dtype=float32)), 'eval/episode_reward': (Array(-15619.074, dtype=float32), Array(5507.4395, dtype=float32)), 'eval/avg_episode_length': (Array(930.0469, dtype=float32), Array(254.36632, dtype=float32)), 'eval/epoch_eval_time': 4.099528074264526, 'eval/sps': 31223.10609446522}
I0727 04:47:06.438972 140120985872192 train.py:379] starting iteration 237 2390.7646255493164
I0727 04:47:16.387234 140120985872192 train.py:394] {'eval/walltime': 992.3300769329071, 'training/sps': 42125.67371921529, 'training/walltime': 1401.630155801773, 'training/entropy_loss': Array(-0.04479574, dtype=float32), 'training/policy_loss': Array(-0.00052133, dtype=float32), 'training/total_loss': Array(14.549038, dtype=float32), 'training/v_loss': Array(14.594355, dtype=float32), 'eval/episode_goal_distance': (Array(0.30500877, dtype=float32), Array(0.05672935, dtype=float32)), 'eval/episode_reward': (Array(-14976.381, dtype=float32), Array(5990.803, dtype=float32)), 'eval/avg_episode_length': (Array(891.28906, dtype=float32), Array(310.2146, dtype=float32)), 'eval/epoch_eval_time': 4.110579490661621, 'eval/sps': 31139.161836132665}
I0727 04:47:16.389662 140120985872192 train.py:379] starting iteration 238 2400.7153165340424
I0727 04:47:26.290885 140120985872192 train.py:394] {'eval/walltime': 996.4205338954926, 'training/sps': 42320.39165956771, 'training/walltime': 1407.4372854232788, 'training/entropy_loss': Array(-0.0452144, dtype=float32), 'training/policy_loss': Array(-0.00028111, dtype=float32), 'training/total_loss': Array(18.572765, dtype=float32), 'training/v_loss': Array(18.61826, dtype=float32), 'eval/episode_goal_distance': (Array(0.30648756, dtype=float32), Array(0.06132023, dtype=float32)), 'eval/episode_reward': (Array(-14410.4, dtype=float32), Array(6201.794, dtype=float32)), 'eval/avg_episode_length': (Array(867.9531, dtype=float32), Array(337.41623, dtype=float32)), 'eval/epoch_eval_time': 4.090456962585449, 'eval/sps': 31292.347327154184}
I0727 04:47:26.293288 140120985872192 train.py:379] starting iteration 239 2410.6189427375793
I0727 04:47:36.228698 140120985872192 train.py:394] {'eval/walltime': 1000.524742603302, 'training/sps': 42173.05095503172, 'training/walltime': 1413.2647035121918, 'training/entropy_loss': Array(-0.04565738, dtype=float32), 'training/policy_loss': Array(-8.5470885e-05, dtype=float32), 'training/total_loss': Array(19.44872, dtype=float32), 'training/v_loss': Array(19.494465, dtype=float32), 'eval/episode_goal_distance': (Array(0.29270464, dtype=float32), Array(0.05901759, dtype=float32)), 'eval/episode_reward': (Array(-14099.463, dtype=float32), Array(6453.8003, dtype=float32)), 'eval/avg_episode_length': (Array(860.2969, dtype=float32), Array(345.35587, dtype=float32)), 'eval/epoch_eval_time': 4.104208707809448, 'eval/sps': 31187.49778890212}
I0727 04:47:36.231337 140120985872192 train.py:379] starting iteration 240 2420.5569911003113
I0727 04:47:46.178355 140120985872192 train.py:394] {'eval/walltime': 1004.6567106246948, 'training/sps': 42289.50384983879, 'training/walltime': 1419.0760746002197, 'training/entropy_loss': Array(-0.04561029, dtype=float32), 'training/policy_loss': Array(-0.00051048, dtype=float32), 'training/total_loss': Array(20.502703, dtype=float32), 'training/v_loss': Array(20.548824, dtype=float32), 'eval/episode_goal_distance': (Array(0.29387486, dtype=float32), Array(0.06037276, dtype=float32)), 'eval/episode_reward': (Array(-14698.42, dtype=float32), Array(6005.3237, dtype=float32)), 'eval/avg_episode_length': (Array(891.28125, dtype=float32), Array(310.23676, dtype=float32)), 'eval/epoch_eval_time': 4.131968021392822, 'eval/sps': 30977.97449963158}
I0727 04:47:46.180904 140120985872192 train.py:379] starting iteration 241 2430.506558418274
I0727 04:47:56.090472 140120985872192 train.py:394] {'eval/walltime': 1008.753990650177, 'training/sps': 42309.576663357155, 'training/walltime': 1424.884688615799, 'training/entropy_loss': Array(-0.04608209, dtype=float32), 'training/policy_loss': Array(-0.00022533, dtype=float32), 'training/total_loss': Array(71.0404, dtype=float32), 'training/v_loss': Array(71.0867, dtype=float32), 'eval/episode_goal_distance': (Array(0.30586115, dtype=float32), Array(0.06347816, dtype=float32)), 'eval/episode_reward': (Array(-14668.234, dtype=float32), Array(6466.9575, dtype=float32)), 'eval/avg_episode_length': (Array(867.8828, dtype=float32), Array(337.59598, dtype=float32)), 'eval/epoch_eval_time': 4.097280025482178, 'eval/sps': 31240.237231512303}
I0727 04:47:56.092877 140120985872192 train.py:379] starting iteration 242 2440.4185314178467
I0727 04:48:06.025655 140120985872192 train.py:394] {'eval/walltime': 1012.8419127464294, 'training/sps': 42074.30306006761, 'training/walltime': 1430.725783586502, 'training/entropy_loss': Array(-0.04632293, dtype=float32), 'training/policy_loss': Array(-0.00106192, dtype=float32), 'training/total_loss': Array(42.47103, dtype=float32), 'training/v_loss': Array(42.518417, dtype=float32), 'eval/episode_goal_distance': (Array(0.3027594, dtype=float32), Array(0.06340618, dtype=float32)), 'eval/episode_reward': (Array(-14617.215, dtype=float32), Array(6134.691, dtype=float32)), 'eval/avg_episode_length': (Array(883.46094, dtype=float32), Array(319.8644, dtype=float32)), 'eval/epoch_eval_time': 4.087922096252441, 'eval/sps': 31311.751297154762}
I0727 04:48:06.028180 140120985872192 train.py:379] starting iteration 243 2450.353833913803
I0727 04:48:15.962780 140120985872192 train.py:394] {'eval/walltime': 1016.9397692680359, 'training/sps': 42132.299330930065, 'training/walltime': 1436.5588381290436, 'training/entropy_loss': Array(-0.04585915, dtype=float32), 'training/policy_loss': Array(-0.00066602, dtype=float32), 'training/total_loss': Array(20.823458, dtype=float32), 'training/v_loss': Array(20.869984, dtype=float32), 'eval/episode_goal_distance': (Array(0.30637062, dtype=float32), Array(0.06727333, dtype=float32)), 'eval/episode_reward': (Array(-14452.103, dtype=float32), Array(6915.798, dtype=float32)), 'eval/avg_episode_length': (Array(852.46094, dtype=float32), Array(353.38177, dtype=float32)), 'eval/epoch_eval_time': 4.097856521606445, 'eval/sps': 31235.842281227877}
I0727 04:48:15.965351 140120985872192 train.py:379] starting iteration 244 2460.291004896164
I0727 04:48:25.882827 140120985872192 train.py:394] {'eval/walltime': 1021.0381898880005, 'training/sps': 42260.66043485272, 'training/walltime': 1442.3741755485535, 'training/entropy_loss': Array(-0.0451206, dtype=float32), 'training/policy_loss': Array(-0.00073967, dtype=float32), 'training/total_loss': Array(18.652756, dtype=float32), 'training/v_loss': Array(18.698616, dtype=float32), 'eval/episode_goal_distance': (Array(0.31357884, dtype=float32), Array(0.06031509, dtype=float32)), 'eval/episode_reward': (Array(-14168.379, dtype=float32), Array(7059.924, dtype=float32)), 'eval/avg_episode_length': (Array(821.47656, dtype=float32), Array(381.44034, dtype=float32)), 'eval/epoch_eval_time': 4.0984206199646, 'eval/sps': 31231.543042818677}
I0727 04:48:25.885184 140120985872192 train.py:379] starting iteration 245 2470.210838317871
I0727 04:48:35.843293 140120985872192 train.py:394] {'eval/walltime': 1025.144856929779, 'training/sps': 42026.99823181923, 'training/walltime': 1448.221845149994, 'training/entropy_loss': Array(-0.04446252, dtype=float32), 'training/policy_loss': Array(-0.00064801, dtype=float32), 'training/total_loss': Array(15.221905, dtype=float32), 'training/v_loss': Array(15.267015, dtype=float32), 'eval/episode_goal_distance': (Array(0.3055226, dtype=float32), Array(0.05918865, dtype=float32)), 'eval/episode_reward': (Array(-15340.17, dtype=float32), Array(5396.1963, dtype=float32)), 'eval/avg_episode_length': (Array(914.7031, dtype=float32), Array(278.18298, dtype=float32)), 'eval/epoch_eval_time': 4.1066670417785645, 'eval/sps': 31168.82832180235}
I0727 04:48:35.845968 140120985872192 train.py:379] starting iteration 246 2480.171622276306
I0727 04:48:45.764340 140120985872192 train.py:394] {'eval/walltime': 1029.283539533615, 'training/sps': 42549.16125090863, 'training/walltime': 1453.9977521896362, 'training/entropy_loss': Array(-0.04451362, dtype=float32), 'training/policy_loss': Array(-0.00047905, dtype=float32), 'training/total_loss': Array(18.404171, dtype=float32), 'training/v_loss': Array(18.449165, dtype=float32), 'eval/episode_goal_distance': (Array(0.29856658, dtype=float32), Array(0.05897029, dtype=float32)), 'eval/episode_reward': (Array(-14507.434, dtype=float32), Array(6203.0522, dtype=float32)), 'eval/avg_episode_length': (Array(875.71875, dtype=float32), Array(328.81787, dtype=float32)), 'eval/epoch_eval_time': 4.13868260383606, 'eval/sps': 30927.71595515912}
I0727 04:48:45.766861 140120985872192 train.py:379] starting iteration 247 2490.092514514923
I0727 04:48:55.685748 140120985872192 train.py:394] {'eval/walltime': 1033.3843221664429, 'training/sps': 42267.17606245782, 'training/walltime': 1459.8121931552887, 'training/entropy_loss': Array(-0.04410879, dtype=float32), 'training/policy_loss': Array(-0.00053681, dtype=float32), 'training/total_loss': Array(17.520145, dtype=float32), 'training/v_loss': Array(17.564789, dtype=float32), 'eval/episode_goal_distance': (Array(0.2929597, dtype=float32), Array(0.06687111, dtype=float32)), 'eval/episode_reward': (Array(-14504.883, dtype=float32), Array(6036.803, dtype=float32)), 'eval/avg_episode_length': (Array(883.5156, dtype=float32), Array(319.7141, dtype=float32)), 'eval/epoch_eval_time': 4.100782632827759, 'eval/sps': 31213.553962926242}
I0727 04:48:55.688722 140120985872192 train.py:379] starting iteration 248 2500.014375925064
I0727 04:49:05.613357 140120985872192 train.py:394] {'eval/walltime': 1037.4809412956238, 'training/sps': 42195.41756404589, 'training/walltime': 1465.6365222930908, 'training/entropy_loss': Array(-0.04369322, dtype=float32), 'training/policy_loss': Array(-0.0004776, dtype=float32), 'training/total_loss': Array(18.437185, dtype=float32), 'training/v_loss': Array(18.481354, dtype=float32), 'eval/episode_goal_distance': (Array(0.29436564, dtype=float32), Array(0.06208985, dtype=float32)), 'eval/episode_reward': (Array(-14875.174, dtype=float32), Array(5831.241, dtype=float32)), 'eval/avg_episode_length': (Array(898.96875, dtype=float32), Array(300.4923, dtype=float32)), 'eval/epoch_eval_time': 4.096619129180908, 'eval/sps': 31245.277133096028}
I0727 04:49:05.615904 140120985872192 train.py:379] starting iteration 249 2509.941558122635
I0727 04:49:15.571435 140120985872192 train.py:394] {'eval/walltime': 1041.5838701725006, 'training/sps': 42017.61887993007, 'training/walltime': 1471.4854972362518, 'training/entropy_loss': Array(-0.04436756, dtype=float32), 'training/policy_loss': Array(-0.00052272, dtype=float32), 'training/total_loss': Array(18.964527, dtype=float32), 'training/v_loss': Array(19.009418, dtype=float32), 'eval/episode_goal_distance': (Array(0.30508226, dtype=float32), Array(0.06200663, dtype=float32)), 'eval/episode_reward': (Array(-14001.432, dtype=float32), Array(6768.8975, dtype=float32)), 'eval/avg_episode_length': (Array(829.1406, dtype=float32), Array(375.0433, dtype=float32)), 'eval/epoch_eval_time': 4.102928876876831, 'eval/sps': 31197.22613798614}
I0727 04:49:15.573873 140120985872192 train.py:379] starting iteration 250 2519.899527311325
I0727 04:49:25.483359 140120985872192 train.py:394] {'eval/walltime': 1045.7135326862335, 'training/sps': 42547.72636188124, 'training/walltime': 1477.2615990638733, 'training/entropy_loss': Array(-0.04569001, dtype=float32), 'training/policy_loss': Array(-0.00038836, dtype=float32), 'training/total_loss': Array(93.24392, dtype=float32), 'training/v_loss': Array(93.29, dtype=float32), 'eval/episode_goal_distance': (Array(0.30716783, dtype=float32), Array(0.05960171, dtype=float32)), 'eval/episode_reward': (Array(-15186.069, dtype=float32), Array(6289.9194, dtype=float32)), 'eval/avg_episode_length': (Array(883.5156, dtype=float32), Array(319.71436, dtype=float32)), 'eval/epoch_eval_time': 4.12966251373291, 'eval/sps': 30995.268880772885}
I0727 04:49:25.485951 140120985872192 train.py:379] starting iteration 251 2529.8116052150726
I0727 04:49:35.446518 140120985872192 train.py:394] {'eval/walltime': 1049.8299317359924, 'training/sps': 42078.17092292974, 'training/walltime': 1483.1021571159363, 'training/entropy_loss': Array(-0.04574455, dtype=float32), 'training/policy_loss': Array(-0.00094956, dtype=float32), 'training/total_loss': Array(22.854412, dtype=float32), 'training/v_loss': Array(22.901106, dtype=float32), 'eval/episode_goal_distance': (Array(0.3033265, dtype=float32), Array(0.06636836, dtype=float32)), 'eval/episode_reward': (Array(-14933.308, dtype=float32), Array(5685.716, dtype=float32)), 'eval/avg_episode_length': (Array(906.7578, dtype=float32), Array(289.90222, dtype=float32)), 'eval/epoch_eval_time': 4.116399049758911, 'eval/sps': 31095.13884653547}
I0727 04:49:35.448974 140120985872192 train.py:379] starting iteration 252 2539.774627685547
I0727 04:49:45.329949 140120985872192 train.py:394] {'eval/walltime': 1053.9186162948608, 'training/sps': 42456.16427546879, 'training/walltime': 1488.8907158374786, 'training/entropy_loss': Array(-0.04504815, dtype=float32), 'training/policy_loss': Array(-0.00051666, dtype=float32), 'training/total_loss': Array(16.278473, dtype=float32), 'training/v_loss': Array(16.324036, dtype=float32), 'eval/episode_goal_distance': (Array(0.29682267, dtype=float32), Array(0.06825095, dtype=float32)), 'eval/episode_reward': (Array(-14200.144, dtype=float32), Array(6444.2466, dtype=float32)), 'eval/avg_episode_length': (Array(860.3125, dtype=float32), Array(345.31732, dtype=float32)), 'eval/epoch_eval_time': 4.088684558868408, 'eval/sps': 31305.912245630785}
I0727 04:49:45.332366 140120985872192 train.py:379] starting iteration 253 2549.658019542694
I0727 04:49:55.300972 140120985872192 train.py:394] {'eval/walltime': 1058.0309796333313, 'training/sps': 41992.088864866324, 'training/walltime': 1494.743246793747, 'training/entropy_loss': Array(-0.04451599, dtype=float32), 'training/policy_loss': Array(-0.00048955, dtype=float32), 'training/total_loss': Array(18.103336, dtype=float32), 'training/v_loss': Array(18.148342, dtype=float32), 'eval/episode_goal_distance': (Array(0.30150893, dtype=float32), Array(0.06001193, dtype=float32)), 'eval/episode_reward': (Array(-13668.057, dtype=float32), Array(6755.908, dtype=float32)), 'eval/avg_episode_length': (Array(829.14844, dtype=float32), Array(375.02606, dtype=float32)), 'eval/epoch_eval_time': 4.112363338470459, 'eval/sps': 31125.654390160955}
I0727 04:49:55.303594 140120985872192 train.py:379] starting iteration 254 2559.629247903824
I0727 04:50:05.204065 140120985872192 train.py:394] {'eval/walltime': 1062.117789030075, 'training/sps': 42299.17336001741, 'training/walltime': 1500.5532894134521, 'training/entropy_loss': Array(-0.04485468, dtype=float32), 'training/policy_loss': Array(-0.00046474, dtype=float32), 'training/total_loss': Array(13.574146, dtype=float32), 'training/v_loss': Array(13.619467, dtype=float32), 'eval/episode_goal_distance': (Array(0.30899167, dtype=float32), Array(0.05649005, dtype=float32)), 'eval/episode_reward': (Array(-14808.053, dtype=float32), Array(5954.2837, dtype=float32)), 'eval/avg_episode_length': (Array(883.4922, dtype=float32), Array(319.77863, dtype=float32)), 'eval/epoch_eval_time': 4.086809396743774, 'eval/sps': 31320.2764244366}
I0727 04:50:05.206483 140120985872192 train.py:379] starting iteration 255 2569.532137155533
I0727 04:50:15.172143 140120985872192 train.py:394] {'eval/walltime': 1066.2287485599518, 'training/sps': 42003.80673766076, 'training/walltime': 1506.4041876792908, 'training/entropy_loss': Array(-0.04481441, dtype=float32), 'training/policy_loss': Array(-0.00059369, dtype=float32), 'training/total_loss': Array(18.213684, dtype=float32), 'training/v_loss': Array(18.25909, dtype=float32), 'eval/episode_goal_distance': (Array(0.30049258, dtype=float32), Array(0.06290433, dtype=float32)), 'eval/episode_reward': (Array(-13397.533, dtype=float32), Array(6836.906, dtype=float32)), 'eval/avg_episode_length': (Array(821.3203, dtype=float32), Array(381.77417, dtype=float32)), 'eval/epoch_eval_time': 4.110959529876709, 'eval/sps': 31136.283164490025}
I0727 04:50:15.320558 140120985872192 train.py:379] starting iteration 256 2579.6461894512177
I0727 04:50:25.231769 140120985872192 train.py:394] {'eval/walltime': 1070.3444726467133, 'training/sps': 42435.96788707369, 'training/walltime': 1512.1955013275146, 'training/entropy_loss': Array(-0.04490982, dtype=float32), 'training/policy_loss': Array(-0.00018245, dtype=float32), 'training/total_loss': Array(17.438816, dtype=float32), 'training/v_loss': Array(17.48391, dtype=float32), 'eval/episode_goal_distance': (Array(0.30684054, dtype=float32), Array(0.06246035, dtype=float32)), 'eval/episode_reward': (Array(-15593.31, dtype=float32), Array(5539.7354, dtype=float32)), 'eval/avg_episode_length': (Array(922.4375, dtype=float32), Array(266.4363, dtype=float32)), 'eval/epoch_eval_time': 4.115724086761475, 'eval/sps': 31100.238330290726}
I0727 04:50:25.234380 140120985872192 train.py:379] starting iteration 257 2589.560033559799
I0727 04:50:35.123928 140120985872192 train.py:394] {'eval/walltime': 1074.4391396045685, 'training/sps': 42437.32536267454, 'training/walltime': 1517.9866297245026, 'training/entropy_loss': Array(-0.0447633, dtype=float32), 'training/policy_loss': Array(-0.00016136, dtype=float32), 'training/total_loss': Array(17.805117, dtype=float32), 'training/v_loss': Array(17.850042, dtype=float32), 'eval/episode_goal_distance': (Array(0.30252442, dtype=float32), Array(0.06145523, dtype=float32)), 'eval/episode_reward': (Array(-14895.92, dtype=float32), Array(6268.6274, dtype=float32)), 'eval/avg_episode_length': (Array(883.47656, dtype=float32), Array(319.8212, dtype=float32)), 'eval/epoch_eval_time': 4.094666957855225, 'eval/sps': 31260.173615449803}
I0727 04:50:35.126540 140120985872192 train.py:379] starting iteration 258 2599.4521939754486
I0727 04:50:45.062252 140120985872192 train.py:394] {'eval/walltime': 1078.545564174652, 'training/sps': 42186.442866728954, 'training/walltime': 1523.8121979236603, 'training/entropy_loss': Array(-0.04493585, dtype=float32), 'training/policy_loss': Array(9.1716254e-05, dtype=float32), 'training/total_loss': Array(75.431114, dtype=float32), 'training/v_loss': Array(75.47595, dtype=float32), 'eval/episode_goal_distance': (Array(0.31612113, dtype=float32), Array(0.06678776, dtype=float32)), 'eval/episode_reward': (Array(-15323.398, dtype=float32), Array(5647.3164, dtype=float32)), 'eval/avg_episode_length': (Array(906.7031, dtype=float32), Array(290.0719, dtype=float32)), 'eval/epoch_eval_time': 4.106424570083618, 'eval/sps': 31170.668744901253}
I0727 04:50:45.064973 140120985872192 train.py:379] starting iteration 259 2609.390627861023
I0727 04:50:55.007120 140120985872192 train.py:394] {'eval/walltime': 1082.654191493988, 'training/sps': 42155.65710651312, 'training/walltime': 1529.6420204639435, 'training/entropy_loss': Array(-0.045112, dtype=float32), 'training/policy_loss': Array(-0.00060625, dtype=float32), 'training/total_loss': Array(30.997292, dtype=float32), 'training/v_loss': Array(31.043009, dtype=float32), 'eval/episode_goal_distance': (Array(0.31110287, dtype=float32), Array(0.06217042, dtype=float32)), 'eval/episode_reward': (Array(-14640.559, dtype=float32), Array(6607.1924, dtype=float32)), 'eval/avg_episode_length': (Array(860.1797, dtype=float32), Array(345.6457, dtype=float32)), 'eval/epoch_eval_time': 4.1086273193359375, 'eval/sps': 31153.9572833995}
I0727 04:50:55.009604 140120985872192 train.py:379] starting iteration 260 2619.3352584838867
I0727 04:51:04.955704 140120985872192 train.py:394] {'eval/walltime': 1086.7761476039886, 'training/sps': 42223.38499606723, 'training/walltime': 1535.4624917507172, 'training/entropy_loss': Array(-0.04453258, dtype=float32), 'training/policy_loss': Array(-0.00012465, dtype=float32), 'training/total_loss': Array(19.175344, dtype=float32), 'training/v_loss': Array(19.220001, dtype=float32), 'eval/episode_goal_distance': (Array(0.2957394, dtype=float32), Array(0.06683972, dtype=float32)), 'eval/episode_reward': (Array(-14246.783, dtype=float32), Array(5821.5327, dtype=float32)), 'eval/avg_episode_length': (Array(898.9844, dtype=float32), Array(300.44595, dtype=float32)), 'eval/epoch_eval_time': 4.12195611000061, 'eval/sps': 31053.217594784397}
I0727 04:51:04.958269 140120985872192 train.py:379] starting iteration 261 2629.2839238643646
I0727 04:51:14.853497 140120985872192 train.py:394] {'eval/walltime': 1090.8691470623016, 'training/sps': 42382.6653019794, 'training/walltime': 1541.261088848114, 'training/entropy_loss': Array(-0.04422607, dtype=float32), 'training/policy_loss': Array(-0.00058605, dtype=float32), 'training/total_loss': Array(15.143613, dtype=float32), 'training/v_loss': Array(15.188425, dtype=float32), 'eval/episode_goal_distance': (Array(0.31080005, dtype=float32), Array(0.05424329, dtype=float32)), 'eval/episode_reward': (Array(-15525.615, dtype=float32), Array(5401.7676, dtype=float32)), 'eval/avg_episode_length': (Array(914.52344, dtype=float32), Array(278.76904, dtype=float32)), 'eval/epoch_eval_time': 4.092999458312988, 'eval/sps': 31272.909098492226}
I0727 04:51:14.855901 140120985872192 train.py:379] starting iteration 262 2639.1815547943115
I0727 04:51:24.811989 140120985872192 train.py:394] {'eval/walltime': 1094.9843418598175, 'training/sps': 42110.08214224786, 'training/walltime': 1547.0972208976746, 'training/entropy_loss': Array(-0.04425388, dtype=float32), 'training/policy_loss': Array(-0.00036663, dtype=float32), 'training/total_loss': Array(14.928383, dtype=float32), 'training/v_loss': Array(14.973003, dtype=float32), 'eval/episode_goal_distance': (Array(0.30037707, dtype=float32), Array(0.0606731, dtype=float32)), 'eval/episode_reward': (Array(-13913.8125, dtype=float32), Array(5811.7686, dtype=float32)), 'eval/avg_episode_length': (Array(875.6953, dtype=float32), Array(328.87985, dtype=float32)), 'eval/epoch_eval_time': 4.115194797515869, 'eval/sps': 31104.2383892172}
I0727 04:51:24.814519 140120985872192 train.py:379] starting iteration 263 2649.140172958374
I0727 04:51:34.699661 140120985872192 train.py:394] {'eval/walltime': 1099.0714716911316, 'training/sps': 42413.188538180555, 'training/walltime': 1552.8916449546814, 'training/entropy_loss': Array(-0.04417949, dtype=float32), 'training/policy_loss': Array(-0.0006134, dtype=float32), 'training/total_loss': Array(14.874973, dtype=float32), 'training/v_loss': Array(14.919766, dtype=float32), 'eval/episode_goal_distance': (Array(0.29171222, dtype=float32), Array(0.06042203, dtype=float32)), 'eval/episode_reward': (Array(-13602.662, dtype=float32), Array(5888.3267, dtype=float32)), 'eval/avg_episode_length': (Array(868.08594, dtype=float32), Array(337.0768, dtype=float32)), 'eval/epoch_eval_time': 4.087129831314087, 'eval/sps': 31317.820887242444}
I0727 04:51:34.702100 140120985872192 train.py:379] starting iteration 264 2659.0277540683746
I0727 04:51:44.676654 140120985872192 train.py:394] {'eval/walltime': 1103.182849407196, 'training/sps': 41942.7498686736, 'training/walltime': 1558.7510604858398, 'training/entropy_loss': Array(-0.04485022, dtype=float32), 'training/policy_loss': Array(-0.00037575, dtype=float32), 'training/total_loss': Array(14.959338, dtype=float32), 'training/v_loss': Array(15.004565, dtype=float32), 'eval/episode_goal_distance': (Array(0.30536306, dtype=float32), Array(0.06222751, dtype=float32)), 'eval/episode_reward': (Array(-14565.51, dtype=float32), Array(6429.825, dtype=float32)), 'eval/avg_episode_length': (Array(860.3047, dtype=float32), Array(345.3369, dtype=float32)), 'eval/epoch_eval_time': 4.111377716064453, 'eval/sps': 31133.11615711286}
I0727 04:51:44.679303 140120985872192 train.py:379] starting iteration 265 2669.0049562454224
I0727 04:51:54.575905 140120985872192 train.py:394] {'eval/walltime': 1107.2732825279236, 'training/sps': 42353.68078763983, 'training/walltime': 1564.5536258220673, 'training/entropy_loss': Array(-0.04556827, dtype=float32), 'training/policy_loss': Array(-8.147017e-06, dtype=float32), 'training/total_loss': Array(16.625696, dtype=float32), 'training/v_loss': Array(16.671272, dtype=float32), 'eval/episode_goal_distance': (Array(0.29675537, dtype=float32), Array(0.05805293, dtype=float32)), 'eval/episode_reward': (Array(-14126.807, dtype=float32), Array(6164.206, dtype=float32)), 'eval/avg_episode_length': (Array(868.0469, dtype=float32), Array(337.17654, dtype=float32)), 'eval/epoch_eval_time': 4.090433120727539, 'eval/sps': 31292.529720479444}
I0727 04:51:54.578516 140120985872192 train.py:379] starting iteration 266 2678.904170036316
I0727 04:52:04.489505 140120985872192 train.py:394] {'eval/walltime': 1111.374784708023, 'training/sps': 42330.35690947111, 'training/walltime': 1570.3593883514404, 'training/entropy_loss': Array(-0.04612507, dtype=float32), 'training/policy_loss': Array(-0.00053146, dtype=float32), 'training/total_loss': Array(71.16725, dtype=float32), 'training/v_loss': Array(71.21391, dtype=float32), 'eval/episode_goal_distance': (Array(0.30097952, dtype=float32), Array(0.06216837, dtype=float32)), 'eval/episode_reward': (Array(-14010.711, dtype=float32), Array(6139.793, dtype=float32)), 'eval/avg_episode_length': (Array(860.2578, dtype=float32), Array(345.45248, dtype=float32)), 'eval/epoch_eval_time': 4.101502180099487, 'eval/sps': 31208.078011284928}
I0727 04:52:04.492067 140120985872192 train.py:379] starting iteration 267 2688.8177206516266
I0727 04:52:14.448884 140120985872192 train.py:394] {'eval/walltime': 1115.5158228874207, 'training/sps': 42283.26752221611, 'training/walltime': 1576.1716165542603, 'training/entropy_loss': Array(-0.04544988, dtype=float32), 'training/policy_loss': Array(-0.00061598, dtype=float32), 'training/total_loss': Array(35.84221, dtype=float32), 'training/v_loss': Array(35.888275, dtype=float32), 'eval/episode_goal_distance': (Array(0.3028077, dtype=float32), Array(0.0630574, dtype=float32)), 'eval/episode_reward': (Array(-14351.323, dtype=float32), Array(6400.7, dtype=float32)), 'eval/avg_episode_length': (Array(860.2578, dtype=float32), Array(345.45267, dtype=float32)), 'eval/epoch_eval_time': 4.141038179397583, 'eval/sps': 30910.12312729287}
I0727 04:52:14.451551 140120985872192 train.py:379] starting iteration 268 2698.7772040367126
I0727 04:52:24.383703 140120985872192 train.py:394] {'eval/walltime': 1119.6238536834717, 'training/sps': 42223.87619609625, 'training/walltime': 1581.9920201301575, 'training/entropy_loss': Array(-0.04529492, dtype=float32), 'training/policy_loss': Array(-0.00059316, dtype=float32), 'training/total_loss': Array(22.57002, dtype=float32), 'training/v_loss': Array(22.61591, dtype=float32), 'eval/episode_goal_distance': (Array(0.29596764, dtype=float32), Array(0.05503513, dtype=float32)), 'eval/episode_reward': (Array(-14010.281, dtype=float32), Array(6194.92, dtype=float32)), 'eval/avg_episode_length': (Array(860.2031, dtype=float32), Array(345.58765, dtype=float32)), 'eval/epoch_eval_time': 4.108030796051025, 'eval/sps': 31158.481120210698}
I0727 04:52:24.386590 140120985872192 train.py:379] starting iteration 269 2708.712243795395
I0727 04:52:34.316561 140120985872192 train.py:394] {'eval/walltime': 1123.720008134842, 'training/sps': 42153.31257876203, 'training/walltime': 1587.8221669197083, 'training/entropy_loss': Array(-0.04517499, dtype=float32), 'training/policy_loss': Array(-0.00037971, dtype=float32), 'training/total_loss': Array(15.628789, dtype=float32), 'training/v_loss': Array(15.674344, dtype=float32), 'eval/episode_goal_distance': (Array(0.29721498, dtype=float32), Array(0.0631664, dtype=float32)), 'eval/episode_reward': (Array(-14327.971, dtype=float32), Array(5774.7744, dtype=float32)), 'eval/avg_episode_length': (Array(883.46094, dtype=float32), Array(319.86426, dtype=float32)), 'eval/epoch_eval_time': 4.096154451370239, 'eval/sps': 31248.821673992697}
I0727 04:52:34.318963 140120985872192 train.py:379] starting iteration 270 2718.6446158885956
I0727 04:52:44.219605 140120985872192 train.py:394] {'eval/walltime': 1127.8145887851715, 'training/sps': 42356.97185286289, 'training/walltime': 1593.6242814064026, 'training/entropy_loss': Array(-0.04430586, dtype=float32), 'training/policy_loss': Array(-0.00044073, dtype=float32), 'training/total_loss': Array(14.410872, dtype=float32), 'training/v_loss': Array(14.455619, dtype=float32), 'eval/episode_goal_distance': (Array(0.3025812, dtype=float32), Array(0.06220504, dtype=float32)), 'eval/episode_reward': (Array(-14546.854, dtype=float32), Array(5914.619, dtype=float32)), 'eval/avg_episode_length': (Array(883.4219, dtype=float32), Array(319.97186, dtype=float32)), 'eval/epoch_eval_time': 4.09458065032959, 'eval/sps': 31260.832532312375}
I0727 04:52:44.222153 140120985872192 train.py:379] starting iteration 271 2728.547807455063
I0727 04:52:54.157338 140120985872192 train.py:394] {'eval/walltime': 1131.942612171173, 'training/sps': 42348.410231532696, 'training/walltime': 1599.427568912506, 'training/entropy_loss': Array(-0.0437325, dtype=float32), 'training/policy_loss': Array(1.2073469e-06, dtype=float32), 'training/total_loss': Array(13.706735, dtype=float32), 'training/v_loss': Array(13.750467, dtype=float32), 'eval/episode_goal_distance': (Array(0.30144563, dtype=float32), Array(0.06444982, dtype=float32)), 'eval/episode_reward': (Array(-12872.451, dtype=float32), Array(6660.597, dtype=float32)), 'eval/avg_episode_length': (Array(813.52344, dtype=float32), Array(388.1826, dtype=float32)), 'eval/epoch_eval_time': 4.128023386001587, 'eval/sps': 31007.576273442846}
I0727 04:52:54.159879 140120985872192 train.py:379] starting iteration 272 2738.4855332374573
I0727 04:53:04.085216 140120985872192 train.py:394] {'eval/walltime': 1136.0403671264648, 'training/sps': 42197.35046435451, 'training/walltime': 1605.2516312599182, 'training/entropy_loss': Array(-0.04272059, dtype=float32), 'training/policy_loss': Array(0.00082469, dtype=float32), 'training/total_loss': Array(14.241741, dtype=float32), 'training/v_loss': Array(14.283636, dtype=float32), 'eval/episode_goal_distance': (Array(0.29538488, dtype=float32), Array(0.06233028, dtype=float32)), 'eval/episode_reward': (Array(-14123.086, dtype=float32), Array(6376.5107, dtype=float32)), 'eval/avg_episode_length': (Array(860.2344, dtype=float32), Array(345.51053, dtype=float32)), 'eval/epoch_eval_time': 4.097754955291748, 'eval/sps': 31236.61648793901}
I0727 04:53:04.087614 140120985872192 train.py:379] starting iteration 273 2748.4132680892944
I0727 04:53:14.047031 140120985872192 train.py:394] {'eval/walltime': 1140.170107126236, 'training/sps': 42183.690953739264, 'training/walltime': 1611.077579498291, 'training/entropy_loss': Array(-0.04187503, dtype=float32), 'training/policy_loss': Array(0.00042719, dtype=float32), 'training/total_loss': Array(14.780193, dtype=float32), 'training/v_loss': Array(14.821641, dtype=float32), 'eval/episode_goal_distance': (Array(0.30688363, dtype=float32), Array(0.05842961, dtype=float32)), 'eval/episode_reward': (Array(-14093.275, dtype=float32), Array(6779.328, dtype=float32)), 'eval/avg_episode_length': (Array(836.96875, dtype=float32), Array(368.00543, dtype=float32)), 'eval/epoch_eval_time': 4.129739999771118, 'eval/sps': 30994.687318594904}
I0727 04:53:14.049626 140120985872192 train.py:379] starting iteration 274 2758.375280380249
I0727 04:53:23.971272 140120985872192 train.py:394] {'eval/walltime': 1144.2681016921997, 'training/sps': 42225.98122142079, 'training/walltime': 1616.8976929187775, 'training/entropy_loss': Array(-0.04183362, dtype=float32), 'training/policy_loss': Array(0.00142931, dtype=float32), 'training/total_loss': Array(13.448065, dtype=float32), 'training/v_loss': Array(13.48847, dtype=float32), 'eval/episode_goal_distance': (Array(0.29915303, dtype=float32), Array(0.05950348, dtype=float32)), 'eval/episode_reward': (Array(-13754.434, dtype=float32), Array(6541.722, dtype=float32)), 'eval/avg_episode_length': (Array(844.5625, dtype=float32), Array(361.20477, dtype=float32)), 'eval/epoch_eval_time': 4.097994565963745, 'eval/sps': 31234.79007588621}
I0727 04:53:23.973636 140120985872192 train.py:379] starting iteration 275 2768.2992901802063
I0727 04:53:33.908682 140120985872192 train.py:394] {'eval/walltime': 1148.36363863945, 'training/sps': 42112.747036468536, 'training/walltime': 1622.733455657959, 'training/entropy_loss': Array(-0.04252665, dtype=float32), 'training/policy_loss': Array(0.00310506, dtype=float32), 'training/total_loss': Array(99.21308, dtype=float32), 'training/v_loss': Array(99.25251, dtype=float32), 'eval/episode_goal_distance': (Array(0.29311973, dtype=float32), Array(0.06999255, dtype=float32)), 'eval/episode_reward': (Array(-13851.623, dtype=float32), Array(6258.829, dtype=float32)), 'eval/avg_episode_length': (Array(860.1875, dtype=float32), Array(345.62692, dtype=float32)), 'eval/epoch_eval_time': 4.095536947250366, 'eval/sps': 31253.53321154526}
I0727 04:53:33.910933 140120985872192 train.py:379] starting iteration 276 2778.2365868091583
I0727 04:53:43.851315 140120985872192 train.py:394] {'eval/walltime': 1152.4768373966217, 'training/sps': 42201.113132023595, 'training/walltime': 1628.5569987297058, 'training/entropy_loss': Array(-0.04149389, dtype=float32), 'training/policy_loss': Array(0.00422574, dtype=float32), 'training/total_loss': Array(19.787354, dtype=float32), 'training/v_loss': Array(19.824623, dtype=float32), 'eval/episode_goal_distance': (Array(0.29472905, dtype=float32), Array(0.06020683, dtype=float32)), 'eval/episode_reward': (Array(-13956.637, dtype=float32), Array(5878.9346, dtype=float32)), 'eval/avg_episode_length': (Array(875.72656, dtype=float32), Array(328.7972, dtype=float32)), 'eval/epoch_eval_time': 4.113198757171631, 'eval/sps': 31119.332557616777}
I0727 04:53:43.853551 140120985872192 train.py:379] starting iteration 277 2788.1792047023773
I0727 04:53:53.768148 140120985872192 train.py:394] {'eval/walltime': 1156.5951268672943, 'training/sps': 42426.98488080573, 'training/walltime': 1634.349538564682, 'training/entropy_loss': Array(-0.0414351, dtype=float32), 'training/policy_loss': Array(0.00217145, dtype=float32), 'training/total_loss': Array(18.325031, dtype=float32), 'training/v_loss': Array(18.364296, dtype=float32), 'eval/episode_goal_distance': (Array(0.29939145, dtype=float32), Array(0.06078866, dtype=float32)), 'eval/episode_reward': (Array(-13535.005, dtype=float32), Array(6336.8345, dtype=float32)), 'eval/avg_episode_length': (Array(844.59375, dtype=float32), Array(361.1319, dtype=float32)), 'eval/epoch_eval_time': 4.118289470672607, 'eval/sps': 31080.865226089798}
I0727 04:53:53.770577 140120985872192 train.py:379] starting iteration 278 2798.096230983734
I0727 04:54:03.710777 140120985872192 train.py:394] {'eval/walltime': 1160.7037410736084, 'training/sps': 42184.868328024306, 'training/walltime': 1640.1753242015839, 'training/entropy_loss': Array(-0.04140826, dtype=float32), 'training/policy_loss': Array(0.00334759, dtype=float32), 'training/total_loss': Array(14.217172, dtype=float32), 'training/v_loss': Array(14.255232, dtype=float32), 'eval/episode_goal_distance': (Array(0.29974377, dtype=float32), Array(0.05201514, dtype=float32)), 'eval/episode_reward': (Array(-14182.508, dtype=float32), Array(5836.42, dtype=float32)), 'eval/avg_episode_length': (Array(875.75, dtype=float32), Array(328.73508, dtype=float32)), 'eval/epoch_eval_time': 4.108614206314087, 'eval/sps': 31154.05671413261}
I0727 04:54:03.713213 140120985872192 train.py:379] starting iteration 279 2808.038866996765
I0727 04:54:13.633516 140120985872192 train.py:394] {'eval/walltime': 1164.804612159729, 'training/sps': 42257.86071845715, 'training/walltime': 1645.9910469055176, 'training/entropy_loss': Array(-0.04145589, dtype=float32), 'training/policy_loss': Array(0.00167869, dtype=float32), 'training/total_loss': Array(14.271816, dtype=float32), 'training/v_loss': Array(14.311595, dtype=float32), 'eval/episode_goal_distance': (Array(0.29630488, dtype=float32), Array(0.05454767, dtype=float32)), 'eval/episode_reward': (Array(-13846.199, dtype=float32), Array(6665.664, dtype=float32)), 'eval/avg_episode_length': (Array(836.9922, dtype=float32), Array(367.95245, dtype=float32)), 'eval/epoch_eval_time': 4.1008710861206055, 'eval/sps': 31212.88070556909}
I0727 04:54:13.635855 140120985872192 train.py:379] starting iteration 280 2817.96150970459
I0727 04:54:23.552158 140120985872192 train.py:394] {'eval/walltime': 1168.9042558670044, 'training/sps': 42276.93419841207, 'training/walltime': 1651.8041458129883, 'training/entropy_loss': Array(-0.04163512, dtype=float32), 'training/policy_loss': Array(0.00127384, dtype=float32), 'training/total_loss': Array(16.11409, dtype=float32), 'training/v_loss': Array(16.154453, dtype=float32), 'eval/episode_goal_distance': (Array(0.2980463, dtype=float32), Array(0.06459222, dtype=float32)), 'eval/episode_reward': (Array(-14655.645, dtype=float32), Array(5814.0015, dtype=float32)), 'eval/avg_episode_length': (Array(899.0469, dtype=float32), Array(300.26013, dtype=float32)), 'eval/epoch_eval_time': 4.099643707275391, 'eval/sps': 31222.225427259964}
I0727 04:54:23.554388 140120985872192 train.py:379] starting iteration 281 2827.8800415992737
I0727 04:54:33.491755 140120985872192 train.py:394] {'eval/walltime': 1173.0310504436493, 'training/sps': 42321.00153568762, 'training/walltime': 1657.6111917495728, 'training/entropy_loss': Array(-0.0418656, dtype=float32), 'training/policy_loss': Array(0.00045098, dtype=float32), 'training/total_loss': Array(15.223623, dtype=float32), 'training/v_loss': Array(15.265038, dtype=float32), 'eval/episode_goal_distance': (Array(0.28993574, dtype=float32), Array(0.06353173, dtype=float32)), 'eval/episode_reward': (Array(-12785.938, dtype=float32), Array(6466.4976, dtype=float32)), 'eval/avg_episode_length': (Array(821.3594, dtype=float32), Array(381.69077, dtype=float32)), 'eval/epoch_eval_time': 4.1267945766448975, 'eval/sps': 31016.80920208647}
I0727 04:54:33.494102 140120985872192 train.py:379] starting iteration 282 2837.819756746292
I0727 04:54:43.409796 140120985872192 train.py:394] {'eval/walltime': 1177.1283242702484, 'training/sps': 42264.94735312932, 'training/walltime': 1663.425939321518, 'training/entropy_loss': Array(-0.04220923, dtype=float32), 'training/policy_loss': Array(0.00086467, dtype=float32), 'training/total_loss': Array(17.143023, dtype=float32), 'training/v_loss': Array(17.184368, dtype=float32), 'eval/episode_goal_distance': (Array(0.30484825, dtype=float32), Array(0.05645786, dtype=float32)), 'eval/episode_reward': (Array(-13451.099, dtype=float32), Array(6452.345, dtype=float32)), 'eval/avg_episode_length': (Array(836.875, dtype=float32), Array(368.21692, dtype=float32)), 'eval/epoch_eval_time': 4.097273826599121, 'eval/sps': 31240.284495762986}
I0727 04:54:43.412358 140120985872192 train.py:379] starting iteration 283 2847.7380118370056
I0727 04:54:53.329371 140120985872192 train.py:394] {'eval/walltime': 1181.251799583435, 'training/sps': 42446.07678771179, 'training/walltime': 1669.2158737182617, 'training/entropy_loss': Array(-0.04270183, dtype=float32), 'training/policy_loss': Array(0.00295371, dtype=float32), 'training/total_loss': Array(83.98644, dtype=float32), 'training/v_loss': Array(84.026184, dtype=float32), 'eval/episode_goal_distance': (Array(0.29018584, dtype=float32), Array(0.06659894, dtype=float32)), 'eval/episode_reward': (Array(-14212.015, dtype=float32), Array(5536.389, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.30667, dtype=float32)), 'eval/epoch_eval_time': 4.1234753131866455, 'eval/sps': 31041.776724275052}
I0727 04:54:53.331916 140120985872192 train.py:379] starting iteration 284 2857.657569885254
I0727 04:55:03.231914 140120985872192 train.py:394] {'eval/walltime': 1185.3477854728699, 'training/sps': 42369.54038604667, 'training/walltime': 1675.0162670612335, 'training/entropy_loss': Array(-0.04271907, dtype=float32), 'training/policy_loss': Array(0.00058264, dtype=float32), 'training/total_loss': Array(33.65551, dtype=float32), 'training/v_loss': Array(33.697647, dtype=float32), 'eval/episode_goal_distance': (Array(0.3029101, dtype=float32), Array(0.06941327, dtype=float32)), 'eval/episode_reward': (Array(-13866.129, dtype=float32), Array(6300.63, dtype=float32)), 'eval/avg_episode_length': (Array(860.2969, dtype=float32), Array(345.35605, dtype=float32)), 'eval/epoch_eval_time': 4.0959858894348145, 'eval/sps': 31250.10765543973}
I0727 04:55:03.234420 140120985872192 train.py:379] starting iteration 285 2867.560074567795
I0727 04:55:13.159868 140120985872192 train.py:394] {'eval/walltime': 1189.4573283195496, 'training/sps': 42283.8242933742, 'training/walltime': 1680.8284187316895, 'training/entropy_loss': Array(-0.04252579, dtype=float32), 'training/policy_loss': Array(0.00130056, dtype=float32), 'training/total_loss': Array(16.435001, dtype=float32), 'training/v_loss': Array(16.476227, dtype=float32), 'eval/episode_goal_distance': (Array(0.2961737, dtype=float32), Array(0.05876545, dtype=float32)), 'eval/episode_reward': (Array(-13915.186, dtype=float32), Array(6218.792, dtype=float32)), 'eval/avg_episode_length': (Array(860.2031, dtype=float32), Array(345.58783, dtype=float32)), 'eval/epoch_eval_time': 4.1095428466796875, 'eval/sps': 31147.0167791091}
I0727 04:55:13.162292 140120985872192 train.py:379] starting iteration 286 2877.4879455566406
I0727 04:55:23.084452 140120985872192 train.py:394] {'eval/walltime': 1193.5497238636017, 'training/sps': 42183.214498368274, 'training/walltime': 1686.65443277359, 'training/entropy_loss': Array(-0.04140633, dtype=float32), 'training/policy_loss': Array(0.00098927, dtype=float32), 'training/total_loss': Array(14.808669, dtype=float32), 'training/v_loss': Array(14.849087, dtype=float32), 'eval/episode_goal_distance': (Array(0.30170518, dtype=float32), Array(0.05849725, dtype=float32)), 'eval/episode_reward': (Array(-14488.275, dtype=float32), Array(5516.321, dtype=float32)), 'eval/avg_episode_length': (Array(898.9297, dtype=float32), Array(300.6087, dtype=float32)), 'eval/epoch_eval_time': 4.092395544052124, 'eval/sps': 31277.524037488223}
I0727 04:55:23.086902 140120985872192 train.py:379] starting iteration 287 2887.4125561714172
I0727 04:55:33.011584 140120985872192 train.py:394] {'eval/walltime': 1197.6645815372467, 'training/sps': 42328.59779046158, 'training/walltime': 1692.4604365825653, 'training/entropy_loss': Array(-0.0411469, dtype=float32), 'training/policy_loss': Array(0.00146586, dtype=float32), 'training/total_loss': Array(11.271367, dtype=float32), 'training/v_loss': Array(11.311048, dtype=float32), 'eval/episode_goal_distance': (Array(0.2974543, dtype=float32), Array(0.06565611, dtype=float32)), 'eval/episode_reward': (Array(-14264.72, dtype=float32), Array(5933.363, dtype=float32)), 'eval/avg_episode_length': (Array(883.53906, dtype=float32), Array(319.6499, dtype=float32)), 'eval/epoch_eval_time': 4.1148576736450195, 'eval/sps': 31106.78671095206}
I0727 04:55:33.014046 140120985872192 train.py:379] starting iteration 288 2897.339699745178
I0727 04:55:42.927045 140120985872192 train.py:394] {'eval/walltime': 1201.7651615142822, 'training/sps': 42308.94454099843, 'training/walltime': 1698.2691373825073, 'training/entropy_loss': Array(-0.04207542, dtype=float32), 'training/policy_loss': Array(0.00152324, dtype=float32), 'training/total_loss': Array(18.707077, dtype=float32), 'training/v_loss': Array(18.747631, dtype=float32), 'eval/episode_goal_distance': (Array(0.29796666, dtype=float32), Array(0.06219466, dtype=float32)), 'eval/episode_reward': (Array(-14246.742, dtype=float32), Array(5753.7905, dtype=float32)), 'eval/avg_episode_length': (Array(891.3047, dtype=float32), Array(310.17038, dtype=float32)), 'eval/epoch_eval_time': 4.1005799770355225, 'eval/sps': 31215.096575810832}
I0727 04:55:42.929573 140120985872192 train.py:379] starting iteration 289 2907.255227088928
I0727 04:55:52.847294 140120985872192 train.py:394] {'eval/walltime': 1205.8593001365662, 'training/sps': 42228.29751069911, 'training/walltime': 1704.0889315605164, 'training/entropy_loss': Array(-0.04235788, dtype=float32), 'training/policy_loss': Array(-0.00027605, dtype=float32), 'training/total_loss': Array(12.211033, dtype=float32), 'training/v_loss': Array(12.253667, dtype=float32), 'eval/episode_goal_distance': (Array(0.30035862, dtype=float32), Array(0.06534345, dtype=float32)), 'eval/episode_reward': (Array(-14125.93, dtype=float32), Array(5992.0986, dtype=float32)), 'eval/avg_episode_length': (Array(875.6953, dtype=float32), Array(328.87985, dtype=float32)), 'eval/epoch_eval_time': 4.0941386222839355, 'eval/sps': 31264.20764145855}
I0727 04:55:52.849868 140120985872192 train.py:379] starting iteration 290 2917.1755225658417
I0727 04:56:02.806243 140120985872192 train.py:394] {'eval/walltime': 1210.0005071163177, 'training/sps': 42289.33555760194, 'training/walltime': 1709.9003257751465, 'training/entropy_loss': Array(-0.04199608, dtype=float32), 'training/policy_loss': Array(-7.3847106e-05, dtype=float32), 'training/total_loss': Array(13.091185, dtype=float32), 'training/v_loss': Array(13.133253, dtype=float32), 'eval/episode_goal_distance': (Array(0.29841295, dtype=float32), Array(0.06720608, dtype=float32)), 'eval/episode_reward': (Array(-13763.09, dtype=float32), Array(6001.714, dtype=float32)), 'eval/avg_episode_length': (Array(867.8828, dtype=float32), Array(337.59592, dtype=float32)), 'eval/epoch_eval_time': 4.141206979751587, 'eval/sps': 30908.863195163976}
I0727 04:56:02.808753 140120985872192 train.py:379] starting iteration 291 2927.134407520294
I0727 04:56:12.725393 140120985872192 train.py:394] {'eval/walltime': 1214.105310678482, 'training/sps': 42313.574753791865, 'training/walltime': 1715.7083909511566, 'training/entropy_loss': Array(-0.04306026, dtype=float32), 'training/policy_loss': Array(6.710414e-05, dtype=float32), 'training/total_loss': Array(73.389145, dtype=float32), 'training/v_loss': Array(73.432144, dtype=float32), 'eval/episode_goal_distance': (Array(0.30397964, dtype=float32), Array(0.06770266, dtype=float32)), 'eval/episode_reward': (Array(-14307.387, dtype=float32), Array(6122.189, dtype=float32)), 'eval/avg_episode_length': (Array(883.4844, dtype=float32), Array(319.80035, dtype=float32)), 'eval/epoch_eval_time': 4.104803562164307, 'eval/sps': 31182.978201400332}
I0727 04:56:12.727941 140120985872192 train.py:379] starting iteration 292 2937.0535945892334
I0727 04:56:22.635050 140120985872192 train.py:394] {'eval/walltime': 1218.1983850002289, 'training/sps': 42297.85595016529, 'training/walltime': 1721.5186145305634, 'training/entropy_loss': Array(-0.04279267, dtype=float32), 'training/policy_loss': Array(-0.00060024, dtype=float32), 'training/total_loss': Array(40.95722, dtype=float32), 'training/v_loss': Array(41.00061, dtype=float32), 'eval/episode_goal_distance': (Array(0.30176312, dtype=float32), Array(0.0575526, dtype=float32)), 'eval/episode_reward': (Array(-14754.385, dtype=float32), Array(5740.7046, dtype=float32)), 'eval/avg_episode_length': (Array(891.22656, dtype=float32), Array(310.393, dtype=float32)), 'eval/epoch_eval_time': 4.093074321746826, 'eval/sps': 31272.337108545995}
I0727 04:56:22.637609 140120985872192 train.py:379] starting iteration 293 2946.9632637500763
I0727 04:56:32.576303 140120985872192 train.py:394] {'eval/walltime': 1222.3051981925964, 'training/sps': 42167.9235817154, 'training/walltime': 1727.3467411994934, 'training/entropy_loss': Array(-0.04227774, dtype=float32), 'training/policy_loss': Array(-0.00027984, dtype=float32), 'training/total_loss': Array(23.205738, dtype=float32), 'training/v_loss': Array(23.248295, dtype=float32), 'eval/episode_goal_distance': (Array(0.29990506, dtype=float32), Array(0.06876829, dtype=float32)), 'eval/episode_reward': (Array(-12870.834, dtype=float32), Array(6721.085, dtype=float32)), 'eval/avg_episode_length': (Array(813.4922, dtype=float32), Array(388.24774, dtype=float32)), 'eval/epoch_eval_time': 4.106813192367554, 'eval/sps': 31167.719105871663}
I0727 04:56:32.578877 140120985872192 train.py:379] starting iteration 294 2956.904530763626
I0727 04:56:42.494365 140120985872192 train.py:394] {'eval/walltime': 1226.4195499420166, 'training/sps': 42391.21114438389, 'training/walltime': 1733.144169330597, 'training/entropy_loss': Array(-0.04121183, dtype=float32), 'training/policy_loss': Array(-0.0003189, dtype=float32), 'training/total_loss': Array(15.434032, dtype=float32), 'training/v_loss': Array(15.475563, dtype=float32), 'eval/episode_goal_distance': (Array(0.30332845, dtype=float32), Array(0.05873595, dtype=float32)), 'eval/episode_reward': (Array(-13891.861, dtype=float32), Array(6724.971, dtype=float32)), 'eval/avg_episode_length': (Array(836.8281, dtype=float32), Array(368.3223, dtype=float32)), 'eval/epoch_eval_time': 4.114351749420166, 'eval/sps': 31110.611779374234}
I0727 04:56:42.496802 140120985872192 train.py:379] starting iteration 295 2966.8224561214447
I0727 04:56:52.411690 140120985872192 train.py:394] {'eval/walltime': 1230.5218560695648, 'training/sps': 42307.933879107644, 'training/walltime': 1738.953008890152, 'training/entropy_loss': Array(-0.04060395, dtype=float32), 'training/policy_loss': Array(-0.00058488, dtype=float32), 'training/total_loss': Array(13.6925745, dtype=float32), 'training/v_loss': Array(13.733763, dtype=float32), 'eval/episode_goal_distance': (Array(0.30098987, dtype=float32), Array(0.05973947, dtype=float32)), 'eval/episode_reward': (Array(-13779.706, dtype=float32), Array(6484.6934, dtype=float32)), 'eval/avg_episode_length': (Array(844.6953, dtype=float32), Array(360.89606, dtype=float32)), 'eval/epoch_eval_time': 4.102306127548218, 'eval/sps': 31201.9620233706}
I0727 04:56:52.414229 140120985872192 train.py:379] starting iteration 296 2976.7398829460144
I0727 04:57:02.372131 140120985872192 train.py:394] {'eval/walltime': 1234.6572890281677, 'training/sps': 42236.4818070337, 'training/walltime': 1744.7716753482819, 'training/entropy_loss': Array(-0.0393492, dtype=float32), 'training/policy_loss': Array(-0.00029704, dtype=float32), 'training/total_loss': Array(12.375004, dtype=float32), 'training/v_loss': Array(12.41465, dtype=float32), 'eval/episode_goal_distance': (Array(0.29826236, dtype=float32), Array(0.05726096, dtype=float32)), 'eval/episode_reward': (Array(-14262.217, dtype=float32), Array(5988.7383, dtype=float32)), 'eval/avg_episode_length': (Array(875.6875, dtype=float32), Array(328.9004, dtype=float32)), 'eval/epoch_eval_time': 4.135432958602905, 'eval/sps': 30952.0191189952}
I0727 04:57:02.374706 140120985872192 train.py:379] starting iteration 297 2986.7003602981567
I0727 04:57:12.317766 140120985872192 train.py:394] {'eval/walltime': 1238.7651693820953, 'training/sps': 42143.45979424857, 'training/walltime': 1750.6031851768494, 'training/entropy_loss': Array(-0.03931773, dtype=float32), 'training/policy_loss': Array(0.00012643, dtype=float32), 'training/total_loss': Array(15.882189, dtype=float32), 'training/v_loss': Array(15.92138, dtype=float32), 'eval/episode_goal_distance': (Array(0.30541134, dtype=float32), Array(0.06472265, dtype=float32)), 'eval/episode_reward': (Array(-13522.873, dtype=float32), Array(6484.344, dtype=float32)), 'eval/avg_episode_length': (Array(836.78906, dtype=float32), Array(368.41074, dtype=float32)), 'eval/epoch_eval_time': 4.107880353927612, 'eval/sps': 31159.62223135761}
I0727 04:57:12.320232 140120985872192 train.py:379] starting iteration 298 2996.6458864212036
I0727 04:57:22.266512 140120985872192 train.py:394] {'eval/walltime': 1242.8673765659332, 'training/sps': 42079.94879825683, 'training/walltime': 1756.443496465683, 'training/entropy_loss': Array(-0.03915674, dtype=float32), 'training/policy_loss': Array(-0.00015773, dtype=float32), 'training/total_loss': Array(13.617408, dtype=float32), 'training/v_loss': Array(13.656721, dtype=float32), 'eval/episode_goal_distance': (Array(0.30242315, dtype=float32), Array(0.06610731, dtype=float32)), 'eval/episode_reward': (Array(-13588.197, dtype=float32), Array(6685.5576, dtype=float32)), 'eval/avg_episode_length': (Array(836.8594, dtype=float32), Array(368.25177, dtype=float32)), 'eval/epoch_eval_time': 4.102207183837891, 'eval/sps': 31202.714603080432}
I0727 04:57:22.268905 140120985872192 train.py:379] starting iteration 299 3006.5945587158203
I0727 04:57:32.177717 140120985872192 train.py:394] {'eval/walltime': 1246.9646277427673, 'training/sps': 42314.952203198045, 'training/walltime': 1762.25137257576, 'training/entropy_loss': Array(-0.03863721, dtype=float32), 'training/policy_loss': Array(9.199081e-05, dtype=float32), 'training/total_loss': Array(12.567067, dtype=float32), 'training/v_loss': Array(12.605613, dtype=float32), 'eval/episode_goal_distance': (Array(0.29789847, dtype=float32), Array(0.06985218, dtype=float32)), 'eval/episode_reward': (Array(-13579.931, dtype=float32), Array(6506.157, dtype=float32)), 'eval/avg_episode_length': (Array(836.90625, dtype=float32), Array(368.1466, dtype=float32)), 'eval/epoch_eval_time': 4.0972511768341064, 'eval/sps': 31240.45719327951}
I0727 04:57:32.180199 140120985872192 train.py:379] starting iteration 300 3016.505853176117
I0727 04:57:42.105419 140120985872192 train.py:394] {'eval/walltime': 1251.0757565498352, 'training/sps': 42296.69829409814, 'training/walltime': 1768.061755180359, 'training/entropy_loss': Array(-0.03982126, dtype=float32), 'training/policy_loss': Array(-7.502978e-05, dtype=float32), 'training/total_loss': Array(96.03014, dtype=float32), 'training/v_loss': Array(96.07004, dtype=float32), 'eval/episode_goal_distance': (Array(0.29013717, dtype=float32), Array(0.06287705, dtype=float32)), 'eval/episode_reward': (Array(-14269.293, dtype=float32), Array(5614.234, dtype=float32)), 'eval/avg_episode_length': (Array(899.1328, dtype=float32), Array(300.00516, dtype=float32)), 'eval/epoch_eval_time': 4.111128807067871, 'eval/sps': 31135.001116954016}
I0727 04:57:42.107812 140120985872192 train.py:379] starting iteration 301 3026.43346619606
I0727 04:57:52.040267 140120985872192 train.py:394] {'eval/walltime': 1255.1799855232239, 'training/sps': 42194.05479439943, 'training/walltime': 1773.88627243042, 'training/entropy_loss': Array(-0.03903808, dtype=float32), 'training/policy_loss': Array(-0.00077313, dtype=float32), 'training/total_loss': Array(21.132519, dtype=float32), 'training/v_loss': Array(21.172329, dtype=float32), 'eval/episode_goal_distance': (Array(0.2943399, dtype=float32), Array(0.0626123, dtype=float32)), 'eval/episode_reward': (Array(-13864.611, dtype=float32), Array(6091.305, dtype=float32)), 'eval/avg_episode_length': (Array(867.97656, dtype=float32), Array(337.3563, dtype=float32)), 'eval/epoch_eval_time': 4.104228973388672, 'eval/sps': 31187.343793422988}
I0727 04:57:52.042854 140120985872192 train.py:379] starting iteration 302 3036.368508577347
I0727 04:58:01.991063 140120985872192 train.py:394] {'eval/walltime': 1259.2671973705292, 'training/sps': 41957.826651067095, 'training/walltime': 1779.7435824871063, 'training/entropy_loss': Array(-0.03873832, dtype=float32), 'training/policy_loss': Array(-0.00058118, dtype=float32), 'training/total_loss': Array(16.681282, dtype=float32), 'training/v_loss': Array(16.720602, dtype=float32), 'eval/episode_goal_distance': (Array(0.29793105, dtype=float32), Array(0.06332202, dtype=float32)), 'eval/episode_reward': (Array(-13811.579, dtype=float32), Array(6051.058, dtype=float32)), 'eval/avg_episode_length': (Array(867.9453, dtype=float32), Array(337.436, dtype=float32)), 'eval/epoch_eval_time': 4.087211847305298, 'eval/sps': 31317.192448536895}
I0727 04:58:01.993488 140120985872192 train.py:379] starting iteration 303 3046.3191423416138
I0727 04:58:11.955042 140120985872192 train.py:394] {'eval/walltime': 1263.3814284801483, 'training/sps': 42055.41880078395, 'training/walltime': 1785.5873003005981, 'training/entropy_loss': Array(-0.03812166, dtype=float32), 'training/policy_loss': Array(-0.00012257, dtype=float32), 'training/total_loss': Array(13.663999, dtype=float32), 'training/v_loss': Array(13.702244, dtype=float32), 'eval/episode_goal_distance': (Array(0.29708308, dtype=float32), Array(0.06172084, dtype=float32)), 'eval/episode_reward': (Array(-13612.279, dtype=float32), Array(6017.297, dtype=float32)), 'eval/avg_episode_length': (Array(860.1094, dtype=float32), Array(345.81955, dtype=float32)), 'eval/epoch_eval_time': 4.114231109619141, 'eval/sps': 31111.52402224899}
I0727 04:58:11.957400 140120985872192 train.py:379] starting iteration 304 3056.2830543518066
I0727 04:58:21.880797 140120985872192 train.py:394] {'eval/walltime': 1267.4720499515533, 'training/sps': 42160.93669651576, 'training/walltime': 1791.4163928031921, 'training/entropy_loss': Array(-0.03797951, dtype=float32), 'training/policy_loss': Array(-0.00050322, dtype=float32), 'training/total_loss': Array(10.697053, dtype=float32), 'training/v_loss': Array(10.735535, dtype=float32), 'eval/episode_goal_distance': (Array(0.30470783, dtype=float32), Array(0.05645356, dtype=float32)), 'eval/episode_reward': (Array(-13788.37, dtype=float32), Array(6564.452, dtype=float32)), 'eval/avg_episode_length': (Array(836.9375, dtype=float32), Array(368.07574, dtype=float32)), 'eval/epoch_eval_time': 4.090621471405029, 'eval/sps': 31291.088871157543}
I0727 04:58:21.883364 140120985872192 train.py:379] starting iteration 305 3066.2090179920197
I0727 04:58:31.845040 140120985872192 train.py:394] {'eval/walltime': 1271.5921430587769, 'training/sps': 42096.65098090273, 'training/walltime': 1797.2543869018555, 'training/entropy_loss': Array(-0.03675065, dtype=float32), 'training/policy_loss': Array(-0.0001192, dtype=float32), 'training/total_loss': Array(13.909465, dtype=float32), 'training/v_loss': Array(13.946334, dtype=float32), 'eval/episode_goal_distance': (Array(0.3020963, dtype=float32), Array(0.06217893, dtype=float32)), 'eval/episode_reward': (Array(-13181.965, dtype=float32), Array(6640.535, dtype=float32)), 'eval/avg_episode_length': (Array(821.35156, dtype=float32), Array(381.7076, dtype=float32)), 'eval/epoch_eval_time': 4.120093107223511, 'eval/sps': 31067.259081010892}
I0727 04:58:31.847405 140120985872192 train.py:379] starting iteration 306 3076.173058271408
I0727 04:58:41.758330 140120985872192 train.py:394] {'eval/walltime': 1275.6856935024261, 'training/sps': 42272.83901184477, 'training/walltime': 1803.06804895401, 'training/entropy_loss': Array(-0.03692742, dtype=float32), 'training/policy_loss': Array(0.00011896, dtype=float32), 'training/total_loss': Array(12.235801, dtype=float32), 'training/v_loss': Array(12.27261, dtype=float32), 'eval/episode_goal_distance': (Array(0.30359247, dtype=float32), Array(0.06676, dtype=float32)), 'eval/episode_reward': (Array(-13797.982, dtype=float32), Array(6484.0205, dtype=float32)), 'eval/avg_episode_length': (Array(852.52344, dtype=float32), Array(353.2324, dtype=float32)), 'eval/epoch_eval_time': 4.093550443649292, 'eval/sps': 31268.699814992724}
I0727 04:58:41.896805 140120985872192 train.py:379] starting iteration 307 3086.2224476337433
I0727 04:58:51.849498 140120985872192 train.py:394] {'eval/walltime': 1279.7976024150848, 'training/sps': 42104.7275677166, 'training/walltime': 1808.9049232006073, 'training/entropy_loss': Array(-0.03733529, dtype=float32), 'training/policy_loss': Array(-0.00023075, dtype=float32), 'training/total_loss': Array(10.905598, dtype=float32), 'training/v_loss': Array(10.943163, dtype=float32), 'eval/episode_goal_distance': (Array(0.30847394, dtype=float32), Array(0.06159203, dtype=float32)), 'eval/episode_reward': (Array(-13129.073, dtype=float32), Array(7024.84, dtype=float32)), 'eval/avg_episode_length': (Array(805.85156, dtype=float32), Array(394.0795, dtype=float32)), 'eval/epoch_eval_time': 4.111908912658691, 'eval/sps': 31129.094228217557}
I0727 04:58:51.852207 140120985872192 train.py:379] starting iteration 308 3096.177860736847
I0727 04:59:01.776502 140120985872192 train.py:394] {'eval/walltime': 1283.9061560630798, 'training/sps': 42283.810417279325, 'training/walltime': 1814.7170767784119, 'training/entropy_loss': Array(-0.03803179, dtype=float32), 'training/policy_loss': Array(0.0001956, dtype=float32), 'training/total_loss': Array(94.64886, dtype=float32), 'training/v_loss': Array(94.68669, dtype=float32), 'eval/episode_goal_distance': (Array(0.2945236, dtype=float32), Array(0.06715544, dtype=float32)), 'eval/episode_reward': (Array(-13237.643, dtype=float32), Array(6602.799, dtype=float32)), 'eval/avg_episode_length': (Array(829.08594, dtype=float32), Array(375.1632, dtype=float32)), 'eval/epoch_eval_time': 4.108553647994995, 'eval/sps': 31154.515911570234}
I0727 04:59:01.778904 140120985872192 train.py:379] starting iteration 309 3106.1045575141907
I0727 04:59:11.731004 140120985872192 train.py:394] {'eval/walltime': 1288.0119216442108, 'training/sps': 42063.25643087532, 'training/walltime': 1820.559705734253, 'training/entropy_loss': Array(-0.03853487, dtype=float32), 'training/policy_loss': Array(-0.00020646, dtype=float32), 'training/total_loss': Array(29.585188, dtype=float32), 'training/v_loss': Array(29.623928, dtype=float32), 'eval/episode_goal_distance': (Array(0.3000511, dtype=float32), Array(0.05929022, dtype=float32)), 'eval/episode_reward': (Array(-13340.368, dtype=float32), Array(6879.7876, dtype=float32)), 'eval/avg_episode_length': (Array(813.5469, dtype=float32), Array(388.13367, dtype=float32)), 'eval/epoch_eval_time': 4.1057655811309814, 'eval/sps': 31175.671740309364}
I0727 04:59:11.733561 140120985872192 train.py:379] starting iteration 310 3116.059215068817
I0727 04:59:21.642979 140120985872192 train.py:394] {'eval/walltime': 1292.1012136936188, 'training/sps': 42253.306774378165, 'training/walltime': 1826.376055240631, 'training/entropy_loss': Array(-0.038672, dtype=float32), 'training/policy_loss': Array(-0.00049371, dtype=float32), 'training/total_loss': Array(18.946606, dtype=float32), 'training/v_loss': Array(18.985773, dtype=float32), 'eval/episode_goal_distance': (Array(0.28587228, dtype=float32), Array(0.06525793, dtype=float32)), 'eval/episode_reward': (Array(-13987.684, dtype=float32), Array(5692.797, dtype=float32)), 'eval/avg_episode_length': (Array(891.3125, dtype=float32), Array(310.14813, dtype=float32)), 'eval/epoch_eval_time': 4.089292049407959, 'eval/sps': 31301.26155174748}
I0727 04:59:21.645490 140120985872192 train.py:379] starting iteration 311 3125.9711434841156
I0727 04:59:31.612495 140120985872192 train.py:394] {'eval/walltime': 1296.2124049663544, 'training/sps': 41995.68327044729, 'training/walltime': 1832.2280852794647, 'training/entropy_loss': Array(-0.03819896, dtype=float32), 'training/policy_loss': Array(-0.00029422, dtype=float32), 'training/total_loss': Array(14.963442, dtype=float32), 'training/v_loss': Array(15.001934, dtype=float32), 'eval/episode_goal_distance': (Array(0.29781705, dtype=float32), Array(0.06302869, dtype=float32)), 'eval/episode_reward': (Array(-13291.416, dtype=float32), Array(6224.2734, dtype=float32)), 'eval/avg_episode_length': (Array(852.4375, dtype=float32), Array(353.43796, dtype=float32)), 'eval/epoch_eval_time': 4.111191272735596, 'eval/sps': 31134.528050023935}
I0727 04:59:31.614923 140120985872192 train.py:379] starting iteration 312 3135.9405772686005
I0727 04:59:41.506112 140120985872192 train.py:394] {'eval/walltime': 1300.3102519512177, 'training/sps': 42448.293176467276, 'training/walltime': 1838.0177173614502, 'training/entropy_loss': Array(-0.03748617, dtype=float32), 'training/policy_loss': Array(-0.00022407, dtype=float32), 'training/total_loss': Array(11.176266, dtype=float32), 'training/v_loss': Array(11.213976, dtype=float32), 'eval/episode_goal_distance': (Array(0.3019779, dtype=float32), Array(0.06093773, dtype=float32)), 'eval/episode_reward': (Array(-12827.458, dtype=float32), Array(6862.51, dtype=float32)), 'eval/avg_episode_length': (Array(798.0078, dtype=float32), Array(400.08163, dtype=float32)), 'eval/epoch_eval_time': 4.097846984863281, 'eval/sps': 31235.914975061114}
I0727 04:59:41.508613 140120985872192 train.py:379] starting iteration 313 3145.8342669010162
I0727 04:59:51.449131 140120985872192 train.py:394] {'eval/walltime': 1304.4150567054749, 'training/sps': 42139.2009295223, 'training/walltime': 1843.8498165607452, 'training/entropy_loss': Array(-0.03660867, dtype=float32), 'training/policy_loss': Array(-0.00018079, dtype=float32), 'training/total_loss': Array(10.6712475, dtype=float32), 'training/v_loss': Array(10.708037, dtype=float32), 'eval/episode_goal_distance': (Array(0.29661608, dtype=float32), Array(0.06004484, dtype=float32)), 'eval/episode_reward': (Array(-13385.668, dtype=float32), Array(6179.3027, dtype=float32)), 'eval/avg_episode_length': (Array(844.78906, dtype=float32), Array(360.6783, dtype=float32)), 'eval/epoch_eval_time': 4.104804754257202, 'eval/sps': 31182.96914542593}
I0727 04:59:51.451464 140120985872192 train.py:379] starting iteration 314 3155.7771186828613
I0727 05:00:01.391402 140120985872192 train.py:394] {'eval/walltime': 1308.5381410121918, 'training/sps': 42277.36075372725, 'training/walltime': 1849.6628568172455, 'training/entropy_loss': Array(-0.03621358, dtype=float32), 'training/policy_loss': Array(0.00011028, dtype=float32), 'training/total_loss': Array(13.497223, dtype=float32), 'training/v_loss': Array(13.533327, dtype=float32), 'eval/episode_goal_distance': (Array(0.3006522, dtype=float32), Array(0.06268857, dtype=float32)), 'eval/episode_reward': (Array(-13105.361, dtype=float32), Array(6721.8706, dtype=float32)), 'eval/avg_episode_length': (Array(813.60156, dtype=float32), Array(388.01993, dtype=float32)), 'eval/epoch_eval_time': 4.123084306716919, 'eval/sps': 31044.720524262655}
I0727 05:00:01.393752 140120985872192 train.py:379] starting iteration 315 3165.7194061279297
I0727 05:00:11.336920 140120985872192 train.py:394] {'eval/walltime': 1312.6530983448029, 'training/sps': 42194.25687255244, 'training/walltime': 1855.4873461723328, 'training/entropy_loss': Array(-0.03578146, dtype=float32), 'training/policy_loss': Array(-0.00061033, dtype=float32), 'training/total_loss': Array(10.172043, dtype=float32), 'training/v_loss': Array(10.208434, dtype=float32), 'eval/episode_goal_distance': (Array(0.31163162, dtype=float32), Array(0.05915412, dtype=float32)), 'eval/episode_reward': (Array(-13227.664, dtype=float32), Array(6735.423, dtype=float32)), 'eval/avg_episode_length': (Array(813.58594, dtype=float32), Array(388.0525, dtype=float32)), 'eval/epoch_eval_time': 4.114957332611084, 'eval/sps': 31106.03334464699}
I0727 05:00:11.339319 140120985872192 train.py:379] starting iteration 316 3175.6649730205536
I0727 05:00:21.260733 140120985872192 train.py:394] {'eval/walltime': 1316.7480566501617, 'training/sps': 42206.64951581955, 'training/walltime': 1861.3101253509521, 'training/entropy_loss': Array(-0.03554817, dtype=float32), 'training/policy_loss': Array(-7.063678e-05, dtype=float32), 'training/total_loss': Array(100.8292, dtype=float32), 'training/v_loss': Array(100.86482, dtype=float32), 'eval/episode_goal_distance': (Array(0.29823977, dtype=float32), Array(0.06381805, dtype=float32)), 'eval/episode_reward': (Array(-13968.738, dtype=float32), Array(5743.0864, dtype=float32)), 'eval/avg_episode_length': (Array(883.4844, dtype=float32), Array(319.79996, dtype=float32)), 'eval/epoch_eval_time': 4.094958305358887, 'eval/sps': 31257.9495211202}
I0727 05:00:21.263052 140120985872192 train.py:379] starting iteration 317 3185.5887064933777
I0727 05:00:31.169995 140120985872192 train.py:394] {'eval/walltime': 1320.8532104492188, 'training/sps': 42386.239739515906, 'training/walltime': 1867.1082334518433, 'training/entropy_loss': Array(-0.03622883, dtype=float32), 'training/policy_loss': Array(7.289361e-05, dtype=float32), 'training/total_loss': Array(38.706753, dtype=float32), 'training/v_loss': Array(38.74291, dtype=float32), 'eval/episode_goal_distance': (Array(0.29874358, dtype=float32), Array(0.05918353, dtype=float32)), 'eval/episode_reward': (Array(-13789.541, dtype=float32), Array(6256.8745, dtype=float32)), 'eval/avg_episode_length': (Array(860.22656, dtype=float32), Array(345.52972, dtype=float32)), 'eval/epoch_eval_time': 4.105153799057007, 'eval/sps': 31180.31778234541}
I0727 05:00:31.172371 140120985872192 train.py:379] starting iteration 318 3195.4980249404907
I0727 05:00:41.116438 140120985872192 train.py:394] {'eval/walltime': 1324.9476335048676, 'training/sps': 42039.01505769214, 'training/walltime': 1872.9542315006256, 'training/entropy_loss': Array(-0.03602853, dtype=float32), 'training/policy_loss': Array(7.7038785e-05, dtype=float32), 'training/total_loss': Array(18.040869, dtype=float32), 'training/v_loss': Array(18.07682, dtype=float32), 'eval/episode_goal_distance': (Array(0.3035828, dtype=float32), Array(0.0643524, dtype=float32)), 'eval/episode_reward': (Array(-13959.904, dtype=float32), Array(6146.557, dtype=float32)), 'eval/avg_episode_length': (Array(867.96094, dtype=float32), Array(337.39645, dtype=float32)), 'eval/epoch_eval_time': 4.094423055648804, 'eval/sps': 31262.035764332388}
I0727 05:00:41.118834 140120985872192 train.py:379] starting iteration 319 3205.444487094879
I0727 05:00:51.105500 140120985872192 train.py:394] {'eval/walltime': 1329.0722863674164, 'training/sps': 41951.30872312235, 'training/walltime': 1878.8124516010284, 'training/entropy_loss': Array(-0.0361882, dtype=float32), 'training/policy_loss': Array(-0.00035545, dtype=float32), 'training/total_loss': Array(13.99676, dtype=float32), 'training/v_loss': Array(14.033304, dtype=float32), 'eval/episode_goal_distance': (Array(0.29446107, dtype=float32), Array(0.05737753, dtype=float32)), 'eval/episode_reward': (Array(-14015.043, dtype=float32), Array(5843.75, dtype=float32)), 'eval/avg_episode_length': (Array(883.4297, dtype=float32), Array(319.94977, dtype=float32)), 'eval/epoch_eval_time': 4.124652862548828, 'eval/sps': 31032.914590757206}
I0727 05:00:51.108067 140120985872192 train.py:379] starting iteration 320 3215.4337210655212
I0727 05:01:01.051960 140120985872192 train.py:394] {'eval/walltime': 1333.1716313362122, 'training/sps': 42076.72812005723, 'training/walltime': 1884.6532099246979, 'training/entropy_loss': Array(-0.03612643, dtype=float32), 'training/policy_loss': Array(-0.00023001, dtype=float32), 'training/total_loss': Array(13.941283, dtype=float32), 'training/v_loss': Array(13.97764, dtype=float32), 'eval/episode_goal_distance': (Array(0.30358243, dtype=float32), Array(0.06733377, dtype=float32)), 'eval/episode_reward': (Array(-13968.457, dtype=float32), Array(6037.7812, dtype=float32)), 'eval/avg_episode_length': (Array(875.625, dtype=float32), Array(329.06567, dtype=float32)), 'eval/epoch_eval_time': 4.099344968795776, 'eval/sps': 31224.50073715101}
I0727 05:01:01.054503 140120985872192 train.py:379] starting iteration 321 3225.380156993866
I0727 05:01:11.030266 140120985872192 train.py:394] {'eval/walltime': 1337.2976377010345, 'training/sps': 42038.80932037026, 'training/walltime': 1890.4992365837097, 'training/entropy_loss': Array(-0.03581154, dtype=float32), 'training/policy_loss': Array(2.9462863e-05, dtype=float32), 'training/total_loss': Array(18.209393, dtype=float32), 'training/v_loss': Array(18.245174, dtype=float32), 'eval/episode_goal_distance': (Array(0.29089037, dtype=float32), Array(0.06130549, dtype=float32)), 'eval/episode_reward': (Array(-13035.167, dtype=float32), Array(6234.882, dtype=float32)), 'eval/avg_episode_length': (Array(836.9297, dtype=float32), Array(368.0935, dtype=float32)), 'eval/epoch_eval_time': 4.126006364822388, 'eval/sps': 31022.734499710357}
I0727 05:01:11.032643 140120985872192 train.py:379] starting iteration 322 3235.3582973480225
I0727 05:01:20.968281 140120985872192 train.py:394] {'eval/walltime': 1341.4014930725098, 'training/sps': 42168.07193368222, 'training/walltime': 1896.327342748642, 'training/entropy_loss': Array(-0.03559029, dtype=float32), 'training/policy_loss': Array(0.00017172, dtype=float32), 'training/total_loss': Array(11.162922, dtype=float32), 'training/v_loss': Array(11.19834, dtype=float32), 'eval/episode_goal_distance': (Array(0.2943533, dtype=float32), Array(0.06550419, dtype=float32)), 'eval/episode_reward': (Array(-12693.656, dtype=float32), Array(6559.705, dtype=float32)), 'eval/avg_episode_length': (Array(813.59375, dtype=float32), Array(388.03665, dtype=float32)), 'eval/epoch_eval_time': 4.10385537147522, 'eval/sps': 31190.18298980347}
I0727 05:01:20.970596 140120985872192 train.py:379] starting iteration 323 3245.296249628067
I0727 05:01:30.875211 140120985872192 train.py:394] {'eval/walltime': 1345.4914264678955, 'training/sps': 42292.91335729348, 'training/walltime': 1902.138245344162, 'training/entropy_loss': Array(-0.03525285, dtype=float32), 'training/policy_loss': Array(0.00041624, dtype=float32), 'training/total_loss': Array(11.231614, dtype=float32), 'training/v_loss': Array(11.266451, dtype=float32), 'eval/episode_goal_distance': (Array(0.31105602, dtype=float32), Array(0.05419424, dtype=float32)), 'eval/episode_reward': (Array(-13225.715, dtype=float32), Array(7042.869, dtype=float32)), 'eval/avg_episode_length': (Array(798.09375, dtype=float32), Array(399.91177, dtype=float32)), 'eval/epoch_eval_time': 4.089933395385742, 'eval/sps': 31296.35317396842}
I0727 05:01:30.877564 140120985872192 train.py:379] starting iteration 324 3255.203218460083
I0727 05:01:40.792632 140120985872192 train.py:394] {'eval/walltime': 1349.5874004364014, 'training/sps': 42260.866616752435, 'training/walltime': 1907.953554391861, 'training/entropy_loss': Array(-0.03666305, dtype=float32), 'training/policy_loss': Array(0.00064742, dtype=float32), 'training/total_loss': Array(10.980328, dtype=float32), 'training/v_loss': Array(11.016343, dtype=float32), 'eval/episode_goal_distance': (Array(0.2945944, dtype=float32), Array(0.06204278, dtype=float32)), 'eval/episode_reward': (Array(-13073.805, dtype=float32), Array(6435.1846, dtype=float32)), 'eval/avg_episode_length': (Array(829.0469, dtype=float32), Array(375.24884, dtype=float32)), 'eval/epoch_eval_time': 4.095973968505859, 'eval/sps': 31250.198605801244}
I0727 05:01:40.794986 140120985872192 train.py:379] starting iteration 325 3265.120640039444
I0727 05:01:50.749812 140120985872192 train.py:394] {'eval/walltime': 1353.7219841480255, 'training/sps': 42251.98702426188, 'training/walltime': 1913.7700855731964, 'training/entropy_loss': Array(-0.03792477, dtype=float32), 'training/policy_loss': Array(0.00213893, dtype=float32), 'training/total_loss': Array(126.27844, dtype=float32), 'training/v_loss': Array(126.31423, dtype=float32), 'eval/episode_goal_distance': (Array(0.30522344, dtype=float32), Array(0.0661386, dtype=float32)), 'eval/episode_reward': (Array(-11989.967, dtype=float32), Array(7223.1406, dtype=float32)), 'eval/avg_episode_length': (Array(751.46094, dtype=float32), Array(430.48343, dtype=float32)), 'eval/epoch_eval_time': 4.1345837116241455, 'eval/sps': 30958.376689806842}
I0727 05:01:50.752263 140120985872192 train.py:379] starting iteration 326 3275.077916622162
I0727 05:02:00.725199 140120985872192 train.py:394] {'eval/walltime': 1357.8304719924927, 'training/sps': 41934.84618768745, 'training/walltime': 1919.6306054592133, 'training/entropy_loss': Array(-0.03773654, dtype=float32), 'training/policy_loss': Array(0.00117451, dtype=float32), 'training/total_loss': Array(26.199957, dtype=float32), 'training/v_loss': Array(26.23652, dtype=float32), 'eval/episode_goal_distance': (Array(0.30964655, dtype=float32), Array(0.06229023, dtype=float32)), 'eval/episode_reward': (Array(-12809.796, dtype=float32), Array(7082.9526, dtype=float32)), 'eval/avg_episode_length': (Array(790.2969, dtype=float32), Array(405.58786, dtype=float32)), 'eval/epoch_eval_time': 4.108487844467163, 'eval/sps': 31155.014897360743}
I0727 05:02:00.727724 140120985872192 train.py:379] starting iteration 327 3285.0533776283264
I0727 05:02:10.653553 140120985872192 train.py:394] {'eval/walltime': 1361.927550792694, 'training/sps': 42190.84943744287, 'training/walltime': 1925.455565214157, 'training/entropy_loss': Array(-0.03753645, dtype=float32), 'training/policy_loss': Array(0.00289309, dtype=float32), 'training/total_loss': Array(16.031492, dtype=float32), 'training/v_loss': Array(16.066135, dtype=float32), 'eval/episode_goal_distance': (Array(0.30680868, dtype=float32), Array(0.06373752, dtype=float32)), 'eval/episode_reward': (Array(-13151.57, dtype=float32), Array(6914.047, dtype=float32)), 'eval/avg_episode_length': (Array(805.78125, dtype=float32), Array(394.22144, dtype=float32)), 'eval/epoch_eval_time': 4.097078800201416, 'eval/sps': 31241.77157483703}
I0727 05:02:10.656197 140120985872192 train.py:379] starting iteration 328 3294.9818506240845
I0727 05:02:20.569206 140120985872192 train.py:394] {'eval/walltime': 1366.0353622436523, 'training/sps': 42362.49349803926, 'training/walltime': 1931.2569234371185, 'training/entropy_loss': Array(-0.03677038, dtype=float32), 'training/policy_loss': Array(0.00168865, dtype=float32), 'training/total_loss': Array(14.850072, dtype=float32), 'training/v_loss': Array(14.885154, dtype=float32), 'eval/episode_goal_distance': (Array(0.30658543, dtype=float32), Array(0.06770094, dtype=float32)), 'eval/episode_reward': (Array(-12813.064, dtype=float32), Array(6783.498, dtype=float32)), 'eval/avg_episode_length': (Array(805.8047, dtype=float32), Array(394.1745, dtype=float32)), 'eval/epoch_eval_time': 4.107811450958252, 'eval/sps': 31160.144891786775}
I0727 05:02:20.571707 140120985872192 train.py:379] starting iteration 329 3304.8973610401154
I0727 05:02:30.519115 140120985872192 train.py:394] {'eval/walltime': 1370.134211063385, 'training/sps': 42047.48804076794, 'training/walltime': 1937.1017434597015, 'training/entropy_loss': Array(-0.03582658, dtype=float32), 'training/policy_loss': Array(0.01661171, dtype=float32), 'training/total_loss': Array(15.453547, dtype=float32), 'training/v_loss': Array(15.472761, dtype=float32), 'eval/episode_goal_distance': (Array(0.30580613, dtype=float32), Array(0.05558258, dtype=float32)), 'eval/episode_reward': (Array(-13195.205, dtype=float32), Array(6729.4478, dtype=float32)), 'eval/avg_episode_length': (Array(813.75, dtype=float32), Array(387.711, dtype=float32)), 'eval/epoch_eval_time': 4.098848819732666, 'eval/sps': 31228.28033661129}
I0727 05:02:30.521558 140120985872192 train.py:379] starting iteration 330 3314.8472113609314
I0727 05:02:40.457320 140120985872192 train.py:394] {'eval/walltime': 1374.2428271770477, 'training/sps': 42202.41069964539, 'training/walltime': 1942.9251074790955, 'training/entropy_loss': Array(-0.03543477, dtype=float32), 'training/policy_loss': Array(0.00244719, dtype=float32), 'training/total_loss': Array(11.052921, dtype=float32), 'training/v_loss': Array(11.085909, dtype=float32), 'eval/episode_goal_distance': (Array(0.30056548, dtype=float32), Array(0.06074756, dtype=float32)), 'eval/episode_reward': (Array(-14562.338, dtype=float32), Array(5683.684, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.3066, dtype=float32)), 'eval/epoch_eval_time': 4.10861611366272, 'eval/sps': 31154.042251441075}
I0727 05:02:40.459881 140120985872192 train.py:379] starting iteration 331 3324.7855348587036
I0727 05:02:50.383279 140120985872192 train.py:394] {'eval/walltime': 1378.3361756801605, 'training/sps': 42181.381281341644, 'training/walltime': 1948.751374721527, 'training/entropy_loss': Array(-0.03522737, dtype=float32), 'training/policy_loss': Array(0.00294382, dtype=float32), 'training/total_loss': Array(10.767329, dtype=float32), 'training/v_loss': Array(10.799612, dtype=float32), 'eval/episode_goal_distance': (Array(0.29889494, dtype=float32), Array(0.05901317, dtype=float32)), 'eval/episode_reward': (Array(-13565.187, dtype=float32), Array(6750.1777, dtype=float32)), 'eval/avg_episode_length': (Array(829.16406, dtype=float32), Array(374.99176, dtype=float32)), 'eval/epoch_eval_time': 4.093348503112793, 'eval/sps': 31270.242419540435}
I0727 05:02:50.385717 140120985872192 train.py:379] starting iteration 332 3334.7113716602325
I0727 05:03:00.317811 140120985872192 train.py:394] {'eval/walltime': 1382.444893836975, 'training/sps': 42229.24901009593, 'training/walltime': 1954.5710377693176, 'training/entropy_loss': Array(-0.03502901, dtype=float32), 'training/policy_loss': Array(0.00222242, dtype=float32), 'training/total_loss': Array(10.179314, dtype=float32), 'training/v_loss': Array(10.21212, dtype=float32), 'eval/episode_goal_distance': (Array(0.2971791, dtype=float32), Array(0.05796875, dtype=float32)), 'eval/episode_reward': (Array(-12556.292, dtype=float32), Array(6947.0815, dtype=float32)), 'eval/avg_episode_length': (Array(790.27344, dtype=float32), Array(405.63275, dtype=float32)), 'eval/epoch_eval_time': 4.108718156814575, 'eval/sps': 31153.268517020013}
I0727 05:03:00.320205 140120985872192 train.py:379] starting iteration 333 3344.645859003067
I0727 05:03:10.218611 140120985872192 train.py:394] {'eval/walltime': 1386.5425894260406, 'training/sps': 42395.30140006737, 'training/walltime': 1960.3679065704346, 'training/entropy_loss': Array(-0.0352327, dtype=float32), 'training/policy_loss': Array(0.00696476, dtype=float32), 'training/total_loss': Array(124.05768, dtype=float32), 'training/v_loss': Array(124.085945, dtype=float32), 'eval/episode_goal_distance': (Array(0.30035105, dtype=float32), Array(0.06174041, dtype=float32)), 'eval/episode_reward': (Array(-12100.577, dtype=float32), Array(6966.439, dtype=float32)), 'eval/avg_episode_length': (Array(774.7031, dtype=float32), Array(416.26962, dtype=float32)), 'eval/epoch_eval_time': 4.097695589065552, 'eval/sps': 31237.069034986424}
I0727 05:03:10.221169 140120985872192 train.py:379] starting iteration 334 3354.546822786331
I0727 05:03:20.179420 140120985872192 train.py:394] {'eval/walltime': 1390.6575989723206, 'training/sps': 42085.35377463546, 'training/walltime': 1966.2074677944183, 'training/entropy_loss': Array(-0.03568683, dtype=float32), 'training/policy_loss': Array(0.00513176, dtype=float32), 'training/total_loss': Array(29.036888, dtype=float32), 'training/v_loss': Array(29.067444, dtype=float32), 'eval/episode_goal_distance': (Array(0.29903793, dtype=float32), Array(0.06273357, dtype=float32)), 'eval/episode_reward': (Array(-14185.941, dtype=float32), Array(6093.0537, dtype=float32)), 'eval/avg_episode_length': (Array(875.83594, dtype=float32), Array(328.50775, dtype=float32)), 'eval/epoch_eval_time': 4.115009546279907, 'eval/sps': 31105.638652944526}
I0727 05:03:20.181932 140120985872192 train.py:379] starting iteration 335 3364.50758600235
I0727 05:03:30.092104 140120985872192 train.py:394] {'eval/walltime': 1394.7567324638367, 'training/sps': 42319.7557390858, 'training/walltime': 1972.014684677124, 'training/entropy_loss': Array(-0.03542135, dtype=float32), 'training/policy_loss': Array(0.00271208, dtype=float32), 'training/total_loss': Array(18.97702, dtype=float32), 'training/v_loss': Array(19.009731, dtype=float32), 'eval/episode_goal_distance': (Array(0.30121058, dtype=float32), Array(0.0664499, dtype=float32)), 'eval/episode_reward': (Array(-13547.772, dtype=float32), Array(6619.514, dtype=float32)), 'eval/avg_episode_length': (Array(836.8125, dtype=float32), Array(368.3578, dtype=float32)), 'eval/epoch_eval_time': 4.099133491516113, 'eval/sps': 31226.11163186532}
I0727 05:03:30.094586 140120985872192 train.py:379] starting iteration 336 3374.420239686966
I0727 05:03:40.047423 140120985872192 train.py:394] {'eval/walltime': 1398.866627216339, 'training/sps': 42088.27503272094, 'training/walltime': 1977.8538405895233, 'training/entropy_loss': Array(-0.03575253, dtype=float32), 'training/policy_loss': Array(0.0038812, dtype=float32), 'training/total_loss': Array(13.586069, dtype=float32), 'training/v_loss': Array(13.61794, dtype=float32), 'eval/episode_goal_distance': (Array(0.29924312, dtype=float32), Array(0.06235791, dtype=float32)), 'eval/episode_reward': (Array(-12140.115, dtype=float32), Array(7110.6494, dtype=float32)), 'eval/avg_episode_length': (Array(767.02344, dtype=float32), Array(421.08087, dtype=float32)), 'eval/epoch_eval_time': 4.109894752502441, 'eval/sps': 31144.34984547064}
I0727 05:03:40.049932 140120985872192 train.py:379] starting iteration 337 3384.375585794449
I0727 05:03:49.980926 140120985872192 train.py:394] {'eval/walltime': 1402.9675068855286, 'training/sps': 42180.76334013627, 'training/walltime': 1983.6801931858063, 'training/entropy_loss': Array(-0.03593206, dtype=float32), 'training/policy_loss': Array(0.0023391, dtype=float32), 'training/total_loss': Array(14.965981, dtype=float32), 'training/v_loss': Array(14.999574, dtype=float32), 'eval/episode_goal_distance': (Array(0.3005456, dtype=float32), Array(0.07044542, dtype=float32)), 'eval/episode_reward': (Array(-12554.82, dtype=float32), Array(7148.224, dtype=float32)), 'eval/avg_episode_length': (Array(790.3125, dtype=float32), Array(405.55737, dtype=float32)), 'eval/epoch_eval_time': 4.100879669189453, 'eval/sps': 31212.815377560066}
I0727 05:03:49.983329 140120985872192 train.py:379] starting iteration 338 3394.3089830875397
I0727 05:03:59.967758 140120985872192 train.py:394] {'eval/walltime': 1407.0990183353424, 'training/sps': 42015.83256669208, 'training/walltime': 1989.5294167995453, 'training/entropy_loss': Array(-0.03587193, dtype=float32), 'training/policy_loss': Array(0.00260621, dtype=float32), 'training/total_loss': Array(12.018755, dtype=float32), 'training/v_loss': Array(12.052021, dtype=float32), 'eval/episode_goal_distance': (Array(0.3091005, dtype=float32), Array(0.06696636, dtype=float32)), 'eval/episode_reward': (Array(-13197.569, dtype=float32), Array(6605.4834, dtype=float32)), 'eval/avg_episode_length': (Array(829.1406, dtype=float32), Array(375.04337, dtype=float32)), 'eval/epoch_eval_time': 4.131511449813843, 'eval/sps': 30981.397862462032}
I0727 05:03:59.970107 140120985872192 train.py:379] starting iteration 339 3404.29576086998
I0727 05:04:09.886042 140120985872192 train.py:394] {'eval/walltime': 1411.2041189670563, 'training/sps': 42321.168342473975, 'training/walltime': 1995.3364398479462, 'training/entropy_loss': Array(-0.03533491, dtype=float32), 'training/policy_loss': Array(0.00229901, dtype=float32), 'training/total_loss': Array(11.769677, dtype=float32), 'training/v_loss': Array(11.802713, dtype=float32), 'eval/episode_goal_distance': (Array(0.30360046, dtype=float32), Array(0.05981063, dtype=float32)), 'eval/episode_reward': (Array(-13212.281, dtype=float32), Array(6592.783, dtype=float32)), 'eval/avg_episode_length': (Array(821.35156, dtype=float32), Array(381.70743, dtype=float32)), 'eval/epoch_eval_time': 4.105100631713867, 'eval/sps': 31180.721615236114}
I0727 05:04:09.888513 140120985872192 train.py:379] starting iteration 340 3414.214166879654
I0727 05:04:19.840742 140120985872192 train.py:394] {'eval/walltime': 1415.338995218277, 'training/sps': 42273.16666691956, 'training/walltime': 2001.1500568389893, 'training/entropy_loss': Array(-0.03545022, dtype=float32), 'training/policy_loss': Array(0.00262912, dtype=float32), 'training/total_loss': Array(16.093737, dtype=float32), 'training/v_loss': Array(16.126556, dtype=float32), 'eval/episode_goal_distance': (Array(0.29516062, dtype=float32), Array(0.05857617, dtype=float32)), 'eval/episode_reward': (Array(-14003.235, dtype=float32), Array(5636.6377, dtype=float32)), 'eval/avg_episode_length': (Array(883.5625, dtype=float32), Array(319.58554, dtype=float32)), 'eval/epoch_eval_time': 4.134876251220703, 'eval/sps': 30956.186406355373}
I0727 05:04:19.843153 140120985872192 train.py:379] starting iteration 341 3424.1688075065613
I0727 05:04:29.791672 140120985872192 train.py:394] {'eval/walltime': 1419.4488303661346, 'training/sps': 42119.51831379427, 'training/walltime': 2006.984881401062, 'training/entropy_loss': Array(-0.03588035, dtype=float32), 'training/policy_loss': Array(0.00293954, dtype=float32), 'training/total_loss': Array(102.224556, dtype=float32), 'training/v_loss': Array(102.25749, dtype=float32), 'eval/episode_goal_distance': (Array(0.29631072, dtype=float32), Array(0.06025334, dtype=float32)), 'eval/episode_reward': (Array(-13597.951, dtype=float32), Array(5898.015, dtype=float32)), 'eval/avg_episode_length': (Array(867.9297, dtype=float32), Array(337.47614, dtype=float32)), 'eval/epoch_eval_time': 4.109835147857666, 'eval/sps': 31144.80152974568}
I0727 05:04:29.794256 140120985872192 train.py:379] starting iteration 342 3434.1199095249176
I0727 05:04:39.702729 140120985872192 train.py:394] {'eval/walltime': 1423.5473668575287, 'training/sps': 42326.7953666178, 'training/walltime': 2012.7911324501038, 'training/entropy_loss': Array(-0.03651451, dtype=float32), 'training/policy_loss': Array(0.00338634, dtype=float32), 'training/total_loss': Array(33.85907, dtype=float32), 'training/v_loss': Array(33.8922, dtype=float32), 'eval/episode_goal_distance': (Array(0.30036497, dtype=float32), Array(0.06286874, dtype=float32)), 'eval/episode_reward': (Array(-12889.665, dtype=float32), Array(6928.5996, dtype=float32)), 'eval/avg_episode_length': (Array(798., dtype=float32), Array(400.09695, dtype=float32)), 'eval/epoch_eval_time': 4.098536491394043, 'eval/sps': 31230.660082878294}
I0727 05:04:39.705099 140120985872192 train.py:379] starting iteration 343 3444.0307536125183
I0727 05:04:49.670408 140120985872192 train.py:394] {'eval/walltime': 1427.673635005951, 'training/sps': 42115.303862719404, 'training/walltime': 2018.6265408992767, 'training/entropy_loss': Array(-0.03718233, dtype=float32), 'training/policy_loss': Array(0.00035423, dtype=float32), 'training/total_loss': Array(24.820475, dtype=float32), 'training/v_loss': Array(24.857304, dtype=float32), 'eval/episode_goal_distance': (Array(0.30624062, dtype=float32), Array(0.06606923, dtype=float32)), 'eval/episode_reward': (Array(-13010.277, dtype=float32), Array(6503.654, dtype=float32)), 'eval/avg_episode_length': (Array(821.3828, dtype=float32), Array(381.641, dtype=float32)), 'eval/epoch_eval_time': 4.126268148422241, 'eval/sps': 31020.766318578517}
I0727 05:04:49.672794 140120985872192 train.py:379] starting iteration 344 3453.99844789505
I0727 05:04:59.892929 140120985872192 train.py:394] {'eval/walltime': 1431.7983901500702, 'training/sps': 40347.92920896437, 'training/walltime': 2024.7175598144531, 'training/entropy_loss': Array(-0.03776004, dtype=float32), 'training/policy_loss': Array(-0.00025554, dtype=float32), 'training/total_loss': Array(15.755064, dtype=float32), 'training/v_loss': Array(15.793078, dtype=float32), 'eval/episode_goal_distance': (Array(0.2973983, dtype=float32), Array(0.06284671, dtype=float32)), 'eval/episode_reward': (Array(-13740.155, dtype=float32), Array(5871.4106, dtype=float32)), 'eval/avg_episode_length': (Array(875.6406, dtype=float32), Array(329.02448, dtype=float32)), 'eval/epoch_eval_time': 4.124755144119263, 'eval/sps': 31032.14506744525}
I0727 05:04:59.895450 140120985872192 train.py:379] starting iteration 345 3464.221104860306
I0727 05:05:09.822767 140120985872192 train.py:394] {'eval/walltime': 1435.8971409797668, 'training/sps': 42192.23617812054, 'training/walltime': 2030.542328119278, 'training/entropy_loss': Array(-0.03770361, dtype=float32), 'training/policy_loss': Array(-0.00047725, dtype=float32), 'training/total_loss': Array(14.887959, dtype=float32), 'training/v_loss': Array(14.92614, dtype=float32), 'eval/episode_goal_distance': (Array(0.2969349, dtype=float32), Array(0.06189096, dtype=float32)), 'eval/episode_reward': (Array(-13138.408, dtype=float32), Array(6517.005, dtype=float32)), 'eval/avg_episode_length': (Array(829.22656, dtype=float32), Array(374.85486, dtype=float32)), 'eval/epoch_eval_time': 4.098750829696655, 'eval/sps': 31229.026920251494}
I0727 05:05:09.825317 140120985872192 train.py:379] starting iteration 346 3474.1509714126587
I0727 05:05:19.781216 140120985872192 train.py:394] {'eval/walltime': 1440.0171978473663, 'training/sps': 42139.98303513454, 'training/walltime': 2036.374319076538, 'training/entropy_loss': Array(-0.03797128, dtype=float32), 'training/policy_loss': Array(4.628671e-05, dtype=float32), 'training/total_loss': Array(13.556191, dtype=float32), 'training/v_loss': Array(13.594117, dtype=float32), 'eval/episode_goal_distance': (Array(0.29921412, dtype=float32), Array(0.05255954, dtype=float32)), 'eval/episode_reward': (Array(-13383.854, dtype=float32), Array(6808.166, dtype=float32)), 'eval/avg_episode_length': (Array(821.375, dtype=float32), Array(381.65735, dtype=float32)), 'eval/epoch_eval_time': 4.120056867599487, 'eval/sps': 31067.532345634347}
I0727 05:05:19.783783 140120985872192 train.py:379] starting iteration 347 3484.109437227249
I0727 05:05:29.706617 140120985872192 train.py:394] {'eval/walltime': 1444.1159975528717, 'training/sps': 42224.367407554026, 'training/walltime': 2042.1946549415588, 'training/entropy_loss': Array(-0.03810631, dtype=float32), 'training/policy_loss': Array(-0.00045564, dtype=float32), 'training/total_loss': Array(13.5653, dtype=float32), 'training/v_loss': Array(13.603863, dtype=float32), 'eval/episode_goal_distance': (Array(0.30729184, dtype=float32), Array(0.06217156, dtype=float32)), 'eval/episode_reward': (Array(-14364.358, dtype=float32), Array(6197.852, dtype=float32)), 'eval/avg_episode_length': (Array(867.83594, dtype=float32), Array(337.71558, dtype=float32)), 'eval/epoch_eval_time': 4.098799705505371, 'eval/sps': 31228.654532221877}
I0727 05:05:29.709222 140120985872192 train.py:379] starting iteration 348 3494.034876346588
I0727 05:05:39.618375 140120985872192 train.py:394] {'eval/walltime': 1448.2215654850006, 'training/sps': 42373.71703900504, 'training/walltime': 2047.994476556778, 'training/entropy_loss': Array(-0.03706088, dtype=float32), 'training/policy_loss': Array(-0.0002005, dtype=float32), 'training/total_loss': Array(17.124819, dtype=float32), 'training/v_loss': Array(17.162079, dtype=float32), 'eval/episode_goal_distance': (Array(0.30273196, dtype=float32), Array(0.06457005, dtype=float32)), 'eval/episode_reward': (Array(-12743.961, dtype=float32), Array(7175.9863, dtype=float32)), 'eval/avg_episode_length': (Array(782.4922, dtype=float32), Array(411.05167, dtype=float32)), 'eval/epoch_eval_time': 4.105567932128906, 'eval/sps': 31177.172590011614}
I0727 05:05:39.620857 140120985872192 train.py:379] starting iteration 349 3503.9465107917786
I0727 05:05:49.552427 140120985872192 train.py:394] {'eval/walltime': 1452.3554441928864, 'training/sps': 42419.547043195555, 'training/walltime': 2053.788032054901, 'training/entropy_loss': Array(-0.03750242, dtype=float32), 'training/policy_loss': Array(-6.830758e-05, dtype=float32), 'training/total_loss': Array(14.306129, dtype=float32), 'training/v_loss': Array(14.343702, dtype=float32), 'eval/episode_goal_distance': (Array(0.31476194, dtype=float32), Array(0.05601977, dtype=float32)), 'eval/episode_reward': (Array(-13264.156, dtype=float32), Array(7506.106, dtype=float32)), 'eval/avg_episode_length': (Array(782.5547, dtype=float32), Array(410.934, dtype=float32)), 'eval/epoch_eval_time': 4.133878707885742, 'eval/sps': 30963.656421710824}
I0727 05:05:49.554829 140120985872192 train.py:379] starting iteration 350 3513.8804829120636
I0727 05:05:59.460029 140120985872192 train.py:394] {'eval/walltime': 1456.4534854888916, 'training/sps': 42349.41934839245, 'training/walltime': 2059.5911812782288, 'training/entropy_loss': Array(-0.03875392, dtype=float32), 'training/policy_loss': Array(5.3637126e-05, dtype=float32), 'training/total_loss': Array(102.46628, dtype=float32), 'training/v_loss': Array(102.50499, dtype=float32), 'eval/episode_goal_distance': (Array(0.3033814, dtype=float32), Array(0.06624351, dtype=float32)), 'eval/episode_reward': (Array(-14688.189, dtype=float32), Array(6540.128, dtype=float32)), 'eval/avg_episode_length': (Array(867.8906, dtype=float32), Array(337.5757, dtype=float32)), 'eval/epoch_eval_time': 4.098041296005249, 'eval/sps': 31234.433904991096}
I0727 05:05:59.462422 140120985872192 train.py:379] starting iteration 351 3523.7880761623383
I0727 05:06:09.414836 140120985872192 train.py:394] {'eval/walltime': 1460.5985565185547, 'training/sps': 42349.54636133567, 'training/walltime': 2065.394313097, 'training/entropy_loss': Array(-0.03931191, dtype=float32), 'training/policy_loss': Array(-0.00070812, dtype=float32), 'training/total_loss': Array(25.325926, dtype=float32), 'training/v_loss': Array(25.365946, dtype=float32), 'eval/episode_goal_distance': (Array(0.30170715, dtype=float32), Array(0.05653863, dtype=float32)), 'eval/episode_reward': (Array(-14311.745, dtype=float32), Array(5785.739, dtype=float32)), 'eval/avg_episode_length': (Array(883.46094, dtype=float32), Array(319.86414, dtype=float32)), 'eval/epoch_eval_time': 4.145071029663086, 'eval/sps': 30880.049843296394}
I0727 05:06:09.417381 140120985872192 train.py:379] starting iteration 352 3533.7430350780487
I0727 05:06:19.372476 140120985872192 train.py:394] {'eval/walltime': 1464.7157592773438, 'training/sps': 42127.23179026527, 'training/walltime': 2071.22806930542, 'training/entropy_loss': Array(-0.03928617, dtype=float32), 'training/policy_loss': Array(-0.00012392, dtype=float32), 'training/total_loss': Array(17.693043, dtype=float32), 'training/v_loss': Array(17.732454, dtype=float32), 'eval/episode_goal_distance': (Array(0.3008159, dtype=float32), Array(0.0649899, dtype=float32)), 'eval/episode_reward': (Array(-13618.855, dtype=float32), Array(6358.7153, dtype=float32)), 'eval/avg_episode_length': (Array(852.35156, dtype=float32), Array(353.64365, dtype=float32)), 'eval/epoch_eval_time': 4.1172027587890625, 'eval/sps': 31089.068840915406}
I0727 05:06:19.374982 140120985872192 train.py:379] starting iteration 353 3543.7006363868713
I0727 05:06:29.263962 140120985872192 train.py:394] {'eval/walltime': 1468.8213341236115, 'training/sps': 42525.57251292403, 'training/walltime': 2077.007180213928, 'training/entropy_loss': Array(-0.03937436, dtype=float32), 'training/policy_loss': Array(0.00018618, dtype=float32), 'training/total_loss': Array(17.546219, dtype=float32), 'training/v_loss': Array(17.585407, dtype=float32), 'eval/episode_goal_distance': (Array(0.3039227, dtype=float32), Array(0.05990787, dtype=float32)), 'eval/episode_reward': (Array(-13350.38, dtype=float32), Array(6482.17, dtype=float32)), 'eval/avg_episode_length': (Array(829.0469, dtype=float32), Array(375.2486, dtype=float32)), 'eval/epoch_eval_time': 4.1055748462677, 'eval/sps': 31177.120084989405}
I0727 05:06:29.266474 140120985872192 train.py:379] starting iteration 354 3553.5921280384064
I0727 05:06:39.189420 140120985872192 train.py:394] {'eval/walltime': 1472.923877954483, 'training/sps': 42253.956287973975, 'training/walltime': 2082.8234403133392, 'training/entropy_loss': Array(-0.03844692, dtype=float32), 'training/policy_loss': Array(0.00025483, dtype=float32), 'training/total_loss': Array(14.233689, dtype=float32), 'training/v_loss': Array(14.271881, dtype=float32), 'eval/episode_goal_distance': (Array(0.30638868, dtype=float32), Array(0.06536357, dtype=float32)), 'eval/episode_reward': (Array(-13848.33, dtype=float32), Array(6318.5464, dtype=float32)), 'eval/avg_episode_length': (Array(852.34375, dtype=float32), Array(353.66254, dtype=float32)), 'eval/epoch_eval_time': 4.102543830871582, 'eval/sps': 31200.154166983393}
I0727 05:06:39.191928 140120985872192 train.py:379] starting iteration 355 3563.517582654953
I0727 05:06:49.138965 140120985872192 train.py:394] {'eval/walltime': 1477.0650234222412, 'training/sps': 42359.67331024382, 'training/walltime': 2088.625184774399, 'training/entropy_loss': Array(-0.03873856, dtype=float32), 'training/policy_loss': Array(0.0001552, dtype=float32), 'training/total_loss': Array(16.009287, dtype=float32), 'training/v_loss': Array(16.04787, dtype=float32), 'eval/episode_goal_distance': (Array(0.30936164, dtype=float32), Array(0.05541934, dtype=float32)), 'eval/episode_reward': (Array(-14605.281, dtype=float32), Array(6469.075, dtype=float32)), 'eval/avg_episode_length': (Array(860.15625, dtype=float32), Array(345.7042, dtype=float32)), 'eval/epoch_eval_time': 4.141145467758179, 'eval/sps': 30909.322311078624}
I0727 05:06:49.141452 140120985872192 train.py:379] starting iteration 356 3573.4671065807343
I0727 05:06:59.038464 140120985872192 train.py:394] {'eval/walltime': 1481.1727550029755, 'training/sps': 42481.05033342191, 'training/walltime': 2094.4103524684906, 'training/entropy_loss': Array(-0.03841082, dtype=float32), 'training/policy_loss': Array(-3.4154436e-06, dtype=float32), 'training/total_loss': Array(17.439396, dtype=float32), 'training/v_loss': Array(17.47781, dtype=float32), 'eval/episode_goal_distance': (Array(0.30700672, dtype=float32), Array(0.05862632, dtype=float32)), 'eval/episode_reward': (Array(-13834.664, dtype=float32), Array(6563.104, dtype=float32)), 'eval/avg_episode_length': (Array(844.66406, dtype=float32), Array(360.9692, dtype=float32)), 'eval/epoch_eval_time': 4.107731580734253, 'eval/sps': 31160.750765784003}
I0727 05:06:59.040977 140120985872192 train.py:379] starting iteration 357 3583.366631746292
I0727 05:07:08.993568 140120985872192 train.py:394] {'eval/walltime': 1485.2998750209808, 'training/sps': 42215.48066896729, 'training/walltime': 2100.2319135665894, 'training/entropy_loss': Array(-0.03860911, dtype=float32), 'training/policy_loss': Array(-0.00011714, dtype=float32), 'training/total_loss': Array(19.46901, dtype=float32), 'training/v_loss': Array(19.507736, dtype=float32), 'eval/episode_goal_distance': (Array(0.30645147, dtype=float32), Array(0.06225901, dtype=float32)), 'eval/episode_reward': (Array(-13636.821, dtype=float32), Array(7314.343, dtype=float32)), 'eval/avg_episode_length': (Array(805.8125, dtype=float32), Array(394.1584, dtype=float32)), 'eval/epoch_eval_time': 4.127120018005371, 'eval/sps': 31014.363391802242}
I0727 05:07:09.190162 140120985872192 train.py:379] starting iteration 358 3593.515795469284
I0727 05:07:19.059033 140120985872192 train.py:394] {'eval/walltime': 1489.390673160553, 'training/sps': 42567.14859999968, 'training/walltime': 2106.0053799152374, 'training/entropy_loss': Array(-0.0395194, dtype=float32), 'training/policy_loss': Array(0.00017035, dtype=float32), 'training/total_loss': Array(105.67514, dtype=float32), 'training/v_loss': Array(105.7145, dtype=float32), 'eval/episode_goal_distance': (Array(0.30772814, dtype=float32), Array(0.06036354, dtype=float32)), 'eval/episode_reward': (Array(-13795.395, dtype=float32), Array(6889.4736, dtype=float32)), 'eval/avg_episode_length': (Array(821.2656, dtype=float32), Array(381.8909, dtype=float32)), 'eval/epoch_eval_time': 4.0907981395721436, 'eval/sps': 31289.737511562358}
I0727 05:07:19.061497 140120985872192 train.py:379] starting iteration 359 3603.387151479721
I0727 05:07:28.987360 140120985872192 train.py:394] {'eval/walltime': 1493.4971907138824, 'training/sps': 42260.4040097508, 'training/walltime': 2111.820752620697, 'training/entropy_loss': Array(-0.03996846, dtype=float32), 'training/policy_loss': Array(-0.00024588, dtype=float32), 'training/total_loss': Array(28.200329, dtype=float32), 'training/v_loss': Array(28.24054, dtype=float32), 'eval/episode_goal_distance': (Array(0.30189306, dtype=float32), Array(0.05839833, dtype=float32)), 'eval/episode_reward': (Array(-13745.609, dtype=float32), Array(7368.193, dtype=float32)), 'eval/avg_episode_length': (Array(813.6172, dtype=float32), Array(387.98737, dtype=float32)), 'eval/epoch_eval_time': 4.106517553329468, 'eval/sps': 31169.96295223933}
I0727 05:07:28.989724 140120985872192 train.py:379] starting iteration 360 3613.315378665924
I0727 05:07:38.900750 140120985872192 train.py:394] {'eval/walltime': 1497.59339761734, 'training/sps': 42292.293879194185, 'training/walltime': 2117.63174033165, 'training/entropy_loss': Array(-0.03951791, dtype=float32), 'training/policy_loss': Array(0.00028989, dtype=float32), 'training/total_loss': Array(27.30957, dtype=float32), 'training/v_loss': Array(27.348799, dtype=float32), 'eval/episode_goal_distance': (Array(0.2922674, dtype=float32), Array(0.06049567, dtype=float32)), 'eval/episode_reward': (Array(-14782.838, dtype=float32), Array(5820.269, dtype=float32)), 'eval/avg_episode_length': (Array(906.7578, dtype=float32), Array(289.90234, dtype=float32)), 'eval/epoch_eval_time': 4.096206903457642, 'eval/sps': 31248.421531625798}
I0727 05:07:38.903473 140120985872192 train.py:379] starting iteration 361 3623.229127883911
I0727 05:07:48.831508 140120985872192 train.py:394] {'eval/walltime': 1501.7037591934204, 'training/sps': 42270.21448729547, 'training/walltime': 2123.445763349533, 'training/entropy_loss': Array(-0.03897528, dtype=float32), 'training/policy_loss': Array(0.00046526, dtype=float32), 'training/total_loss': Array(17.52684, dtype=float32), 'training/v_loss': Array(17.56535, dtype=float32), 'eval/episode_goal_distance': (Array(0.304438, dtype=float32), Array(0.06873705, dtype=float32)), 'eval/episode_reward': (Array(-14100.319, dtype=float32), Array(6953.9805, dtype=float32)), 'eval/avg_episode_length': (Array(852.3906, dtype=float32), Array(353.55017, dtype=float32)), 'eval/epoch_eval_time': 4.110361576080322, 'eval/sps': 31140.812707300058}
I0727 05:07:48.833683 140120985872192 train.py:379] starting iteration 362 3633.159337282181
I0727 05:07:58.775714 140120985872192 train.py:394] {'eval/walltime': 1505.8460431098938, 'training/sps': 42402.125479901886, 'training/walltime': 2129.24169921875, 'training/entropy_loss': Array(-0.03928109, dtype=float32), 'training/policy_loss': Array(0.00035347, dtype=float32), 'training/total_loss': Array(16.840141, dtype=float32), 'training/v_loss': Array(16.87907, dtype=float32), 'eval/episode_goal_distance': (Array(0.29508266, dtype=float32), Array(0.06254593, dtype=float32)), 'eval/episode_reward': (Array(-12678.549, dtype=float32), Array(6856.2905, dtype=float32)), 'eval/avg_episode_length': (Array(798.125, dtype=float32), Array(399.84985, dtype=float32)), 'eval/epoch_eval_time': 4.142283916473389, 'eval/sps': 30900.827316775332}
I0727 05:07:58.777956 140120985872192 train.py:379] starting iteration 363 3643.1036105155945
I0727 05:08:08.691650 140120985872192 train.py:394] {'eval/walltime': 1509.9483261108398, 'training/sps': 42315.066849940304, 'training/walltime': 2135.0495595932007, 'training/entropy_loss': Array(-0.04020238, dtype=float32), 'training/policy_loss': Array(0.00065862, dtype=float32), 'training/total_loss': Array(17.586712, dtype=float32), 'training/v_loss': Array(17.626255, dtype=float32), 'eval/episode_goal_distance': (Array(0.3048636, dtype=float32), Array(0.06429625, dtype=float32)), 'eval/episode_reward': (Array(-13917.43, dtype=float32), Array(6746.973, dtype=float32)), 'eval/avg_episode_length': (Array(836.8594, dtype=float32), Array(368.2518, dtype=float32)), 'eval/epoch_eval_time': 4.102283000946045, 'eval/sps': 31202.137924292736}
I0727 05:08:08.693804 140120985872192 train.py:379] starting iteration 364 3653.019458770752
I0727 05:08:18.639119 140120985872192 train.py:394] {'eval/walltime': 1514.0616824626923, 'training/sps': 42167.4509324097, 'training/walltime': 2140.8777515888214, 'training/entropy_loss': Array(-0.04012498, dtype=float32), 'training/policy_loss': Array(0.00058359, dtype=float32), 'training/total_loss': Array(19.443848, dtype=float32), 'training/v_loss': Array(19.483389, dtype=float32), 'eval/episode_goal_distance': (Array(0.29710054, dtype=float32), Array(0.06230251, dtype=float32)), 'eval/episode_reward': (Array(-14834.484, dtype=float32), Array(5573.9097, dtype=float32)), 'eval/avg_episode_length': (Array(906.72656, dtype=float32), Array(289.99924, dtype=float32)), 'eval/epoch_eval_time': 4.113356351852417, 'eval/sps': 31118.140285209236}
I0727 05:08:18.641318 140120985872192 train.py:379] starting iteration 365 3662.966972351074
I0727 05:08:28.552112 140120985872192 train.py:394] {'eval/walltime': 1518.1611633300781, 'training/sps': 42317.467625946534, 'training/walltime': 2146.685282468796, 'training/entropy_loss': Array(-0.04092129, dtype=float32), 'training/policy_loss': Array(0.00065948, dtype=float32), 'training/total_loss': Array(24.65374, dtype=float32), 'training/v_loss': Array(24.694004, dtype=float32), 'eval/episode_goal_distance': (Array(0.30913684, dtype=float32), Array(0.06572619, dtype=float32)), 'eval/episode_reward': (Array(-14882.291, dtype=float32), Array(6407.7417, dtype=float32)), 'eval/avg_episode_length': (Array(875.7422, dtype=float32), Array(328.75583, dtype=float32)), 'eval/epoch_eval_time': 4.099480867385864, 'eval/sps': 31223.465638863287}
I0727 05:08:28.554425 140120985872192 train.py:379] starting iteration 366 3672.8800795078278
I0727 05:08:38.492137 140120985872192 train.py:394] {'eval/walltime': 1522.300418138504, 'training/sps': 42410.05100518099, 'training/walltime': 2152.480135202408, 'training/entropy_loss': Array(-0.04153534, dtype=float32), 'training/policy_loss': Array(0.00230039, dtype=float32), 'training/total_loss': Array(87.99604, dtype=float32), 'training/v_loss': Array(88.03528, dtype=float32), 'eval/episode_goal_distance': (Array(0.29642254, dtype=float32), Array(0.06603874, dtype=float32)), 'eval/episode_reward': (Array(-14450.965, dtype=float32), Array(6281.494, dtype=float32)), 'eval/avg_episode_length': (Array(867.9531, dtype=float32), Array(337.4165, dtype=float32)), 'eval/epoch_eval_time': 4.139254808425903, 'eval/sps': 30923.44055249802}
I0727 05:08:38.494351 140120985872192 train.py:379] starting iteration 367 3682.819999933243
I0727 05:08:48.391033 140120985872192 train.py:394] {'eval/walltime': 1526.398829460144, 'training/sps': 42412.488747829215, 'training/walltime': 2158.274654865265, 'training/entropy_loss': Array(-0.04226153, dtype=float32), 'training/policy_loss': Array(0.00070589, dtype=float32), 'training/total_loss': Array(33.47102, dtype=float32), 'training/v_loss': Array(33.512577, dtype=float32), 'eval/episode_goal_distance': (Array(0.31620905, dtype=float32), Array(0.05693119, dtype=float32)), 'eval/episode_reward': (Array(-13914.391, dtype=float32), Array(7590.6406, dtype=float32)), 'eval/avg_episode_length': (Array(798.03906, dtype=float32), Array(400.01968, dtype=float32)), 'eval/epoch_eval_time': 4.098411321640015, 'eval/sps': 31231.613899792686}
I0727 05:08:48.393357 140120985872192 train.py:379] starting iteration 368 3692.7190115451813
I0727 05:08:58.309139 140120985872192 train.py:394] {'eval/walltime': 1530.5155222415924, 'training/sps': 42405.94918035316, 'training/walltime': 2164.0700681209564, 'training/entropy_loss': Array(-0.04261131, dtype=float32), 'training/policy_loss': Array(0.00061331, dtype=float32), 'training/total_loss': Array(29.200382, dtype=float32), 'training/v_loss': Array(29.242378, dtype=float32), 'eval/episode_goal_distance': (Array(0.31320107, dtype=float32), Array(0.0667772, dtype=float32)), 'eval/episode_reward': (Array(-14172.444, dtype=float32), Array(6632.561, dtype=float32)), 'eval/avg_episode_length': (Array(844.6719, dtype=float32), Array(360.9507, dtype=float32)), 'eval/epoch_eval_time': 4.116692781448364, 'eval/sps': 31092.92016563017}
I0727 05:08:58.311311 140120985872192 train.py:379] starting iteration 369 3702.636965751648
I0727 05:09:08.191617 140120985872192 train.py:394] {'eval/walltime': 1534.6058781147003, 'training/sps': 42472.43323365476, 'training/walltime': 2169.856409549713, 'training/entropy_loss': Array(-0.04270072, dtype=float32), 'training/policy_loss': Array(0.00082385, dtype=float32), 'training/total_loss': Array(22.837387, dtype=float32), 'training/v_loss': Array(22.879265, dtype=float32), 'eval/episode_goal_distance': (Array(0.29977784, dtype=float32), Array(0.06552365, dtype=float32)), 'eval/episode_reward': (Array(-13040.402, dtype=float32), Array(7407.567, dtype=float32)), 'eval/avg_episode_length': (Array(782.5625, dtype=float32), Array(410.91898, dtype=float32)), 'eval/epoch_eval_time': 4.09035587310791, 'eval/sps': 31293.1206894582}
I0727 05:09:08.193768 140120985872192 train.py:379] starting iteration 370 3712.5194222927094
I0727 05:09:18.121557 140120985872192 train.py:394] {'eval/walltime': 1538.7106025218964, 'training/sps': 42232.42732888229, 'training/walltime': 2175.675634622574, 'training/entropy_loss': Array(-0.04239433, dtype=float32), 'training/policy_loss': Array(0.00045055, dtype=float32), 'training/total_loss': Array(20.078354, dtype=float32), 'training/v_loss': Array(20.120296, dtype=float32), 'eval/episode_goal_distance': (Array(0.30024028, dtype=float32), Array(0.07184431, dtype=float32)), 'eval/episode_reward': (Array(-12284.243, dtype=float32), Array(7903.876, dtype=float32)), 'eval/avg_episode_length': (Array(735.9375, dtype=float32), Array(439.0682, dtype=float32)), 'eval/epoch_eval_time': 4.104724407196045, 'eval/sps': 31183.579529870887}
I0727 05:09:18.123897 140120985872192 train.py:379] starting iteration 371 3722.449551343918
I0727 05:09:28.069625 140120985872192 train.py:394] {'eval/walltime': 1542.8466835021973, 'training/sps': 42329.67375596913, 'training/walltime': 2181.4814908504486, 'training/entropy_loss': Array(-0.04234893, dtype=float32), 'training/policy_loss': Array(0.00171893, dtype=float32), 'training/total_loss': Array(17.06046, dtype=float32), 'training/v_loss': Array(17.10109, dtype=float32), 'eval/episode_goal_distance': (Array(0.3142215, dtype=float32), Array(0.0615233, dtype=float32)), 'eval/episode_reward': (Array(-14911.114, dtype=float32), Array(6738.6694, dtype=float32)), 'eval/avg_episode_length': (Array(860.2422, dtype=float32), Array(345.4911, dtype=float32)), 'eval/epoch_eval_time': 4.136080980300903, 'eval/sps': 30947.16970234173}
I0727 05:09:28.071882 140120985872192 train.py:379] starting iteration 372 3732.397535800934
I0727 05:09:37.964804 140120985872192 train.py:394] {'eval/walltime': 1546.9464464187622, 'training/sps': 42449.890937789394, 'training/walltime': 2187.270905017853, 'training/entropy_loss': Array(-0.04224587, dtype=float32), 'training/policy_loss': Array(0.00142001, dtype=float32), 'training/total_loss': Array(24.03867, dtype=float32), 'training/v_loss': Array(24.079494, dtype=float32), 'eval/episode_goal_distance': (Array(0.3083164, dtype=float32), Array(0.06014041, dtype=float32)), 'eval/episode_reward': (Array(-15008.244, dtype=float32), Array(7004.8286, dtype=float32)), 'eval/avg_episode_length': (Array(852.4844, dtype=float32), Array(353.32593, dtype=float32)), 'eval/epoch_eval_time': 4.099762916564941, 'eval/sps': 31221.317574931152}
I0727 05:09:37.967044 140120985872192 train.py:379] starting iteration 373 3742.292697906494
I0727 05:09:47.872493 140120985872192 train.py:394] {'eval/walltime': 1551.0582604408264, 'training/sps': 42446.52424187307, 'training/walltime': 2193.0607783794403, 'training/entropy_loss': Array(-0.04219572, dtype=float32), 'training/policy_loss': Array(0.00234, dtype=float32), 'training/total_loss': Array(20.791569, dtype=float32), 'training/v_loss': Array(20.831425, dtype=float32), 'eval/episode_goal_distance': (Array(0.30812424, dtype=float32), Array(0.06451768, dtype=float32)), 'eval/episode_reward': (Array(-13505.452, dtype=float32), Array(6882.9478, dtype=float32)), 'eval/avg_episode_length': (Array(821.375, dtype=float32), Array(381.65732, dtype=float32)), 'eval/epoch_eval_time': 4.111814022064209, 'eval/sps': 31129.8126114521}
I0727 05:09:47.874710 140120985872192 train.py:379] starting iteration 374 3752.2003626823425
I0727 05:09:57.788193 140120985872192 train.py:394] {'eval/walltime': 1555.1580436229706, 'training/sps': 42299.027555679066, 'training/walltime': 2198.870841026306, 'training/entropy_loss': Array(-0.04137661, dtype=float32), 'training/policy_loss': Array(0.00173601, dtype=float32), 'training/total_loss': Array(26.836922, dtype=float32), 'training/v_loss': Array(26.87656, dtype=float32), 'eval/episode_goal_distance': (Array(0.30442554, dtype=float32), Array(0.06679424, dtype=float32)), 'eval/episode_reward': (Array(-14875.273, dtype=float32), Array(6674.8047, dtype=float32)), 'eval/avg_episode_length': (Array(867.9453, dtype=float32), Array(337.4363, dtype=float32)), 'eval/epoch_eval_time': 4.099783182144165, 'eval/sps': 31221.163245285734}
I0727 05:09:57.790356 140120985872192 train.py:379] starting iteration 375 3762.1159999370575
I0727 05:10:07.686191 140120985872192 train.py:394] {'eval/walltime': 1559.2579700946808, 'training/sps': 42430.29258807368, 'training/walltime': 2204.6629292964935, 'training/entropy_loss': Array(-0.0426034, dtype=float32), 'training/policy_loss': Array(0.00206643, dtype=float32), 'training/total_loss': Array(112.07479, dtype=float32), 'training/v_loss': Array(112.115326, dtype=float32), 'eval/episode_goal_distance': (Array(0.3137896, dtype=float32), Array(0.05966288, dtype=float32)), 'eval/episode_reward': (Array(-14708.453, dtype=float32), Array(6935.0405, dtype=float32)), 'eval/avg_episode_length': (Array(852.3672, dtype=float32), Array(353.60645, dtype=float32)), 'eval/epoch_eval_time': 4.099926471710205, 'eval/sps': 31220.07208744094}
I0727 05:10:07.688527 140120985872192 train.py:379] starting iteration 376 3772.014181137085
I0727 05:10:17.638809 140120985872192 train.py:394] {'eval/walltime': 1563.4028525352478, 'training/sps': 42360.81178530483, 'training/walltime': 2210.4645178318024, 'training/entropy_loss': Array(-0.04166913, dtype=float32), 'training/policy_loss': Array(0.00047877, dtype=float32), 'training/total_loss': Array(26.647854, dtype=float32), 'training/v_loss': Array(26.689045, dtype=float32), 'eval/episode_goal_distance': (Array(0.299544, dtype=float32), Array(0.06629974, dtype=float32)), 'eval/episode_reward': (Array(-14402.494, dtype=float32), Array(6458.2026, dtype=float32)), 'eval/avg_episode_length': (Array(868., dtype=float32), Array(337.29642, dtype=float32)), 'eval/epoch_eval_time': 4.144882440567017, 'eval/sps': 30881.454862804192}
I0727 05:10:17.641053 140120985872192 train.py:379] starting iteration 377 3781.966707468033
I0727 05:10:27.561503 140120985872192 train.py:394] {'eval/walltime': 1567.5030138492584, 'training/sps': 42252.28837726121, 'training/walltime': 2216.281007528305, 'training/entropy_loss': Array(-0.0416373, dtype=float32), 'training/policy_loss': Array(0.00115832, dtype=float32), 'training/total_loss': Array(19.89705, dtype=float32), 'training/v_loss': Array(19.93753, dtype=float32), 'eval/episode_goal_distance': (Array(0.3022398, dtype=float32), Array(0.06446689, dtype=float32)), 'eval/episode_reward': (Array(-14118.346, dtype=float32), Array(6884.9976, dtype=float32)), 'eval/avg_episode_length': (Array(836.83594, dtype=float32), Array(368.30472, dtype=float32)), 'eval/epoch_eval_time': 4.10016131401062, 'eval/sps': 31218.283915467542}
I0727 05:10:27.563680 140120985872192 train.py:379] starting iteration 378 3791.889333963394
I0727 05:10:37.488179 140120985872192 train.py:394] {'eval/walltime': 1571.6372969150543, 'training/sps': 42470.566043675135, 'training/walltime': 2222.0676033496857, 'training/entropy_loss': Array(-0.04130108, dtype=float32), 'training/policy_loss': Array(0.00050314, dtype=float32), 'training/total_loss': Array(19.779903, dtype=float32), 'training/v_loss': Array(19.820702, dtype=float32), 'eval/episode_goal_distance': (Array(0.31752118, dtype=float32), Array(0.06177845, dtype=float32)), 'eval/episode_reward': (Array(-14892.896, dtype=float32), Array(7169.133, dtype=float32)), 'eval/avg_episode_length': (Array(844.6172, dtype=float32), Array(361.07797, dtype=float32)), 'eval/epoch_eval_time': 4.134283065795898, 'eval/sps': 30960.627988678487}
I0727 05:10:37.490466 140120985872192 train.py:379] starting iteration 379 3801.8161203861237
I0727 05:10:47.382208 140120985872192 train.py:394] {'eval/walltime': 1575.72865152359, 'training/sps': 42399.59300687674, 'training/walltime': 2227.8638854026794, 'training/entropy_loss': Array(-0.03983825, dtype=float32), 'training/policy_loss': Array(0.0008851, dtype=float32), 'training/total_loss': Array(16.081146, dtype=float32), 'training/v_loss': Array(16.1201, dtype=float32), 'eval/episode_goal_distance': (Array(0.31540698, dtype=float32), Array(0.07105105, dtype=float32)), 'eval/episode_reward': (Array(-14391.982, dtype=float32), Array(7427.5913, dtype=float32)), 'eval/avg_episode_length': (Array(828.97656, dtype=float32), Array(375.40295, dtype=float32)), 'eval/epoch_eval_time': 4.091354608535767, 'eval/sps': 31285.481765123568}
I0727 05:10:47.384639 140120985872192 train.py:379] starting iteration 380 3811.710293531418
I0727 05:10:57.334356 140120985872192 train.py:394] {'eval/walltime': 1579.8583359718323, 'training/sps': 42264.53317877222, 'training/walltime': 2233.678689956665, 'training/entropy_loss': Array(-0.03766824, dtype=float32), 'training/policy_loss': Array(0.00081399, dtype=float32), 'training/total_loss': Array(18.744284, dtype=float32), 'training/v_loss': Array(18.78114, dtype=float32), 'eval/episode_goal_distance': (Array(0.30964693, dtype=float32), Array(0.0686015, dtype=float32)), 'eval/episode_reward': (Array(-15295.83, dtype=float32), Array(6326.6772, dtype=float32)), 'eval/avg_episode_length': (Array(899.0078, dtype=float32), Array(300.3763, dtype=float32)), 'eval/epoch_eval_time': 4.1296844482421875, 'eval/sps': 30995.104251726443}
I0727 05:10:57.336880 140120985872192 train.py:379] starting iteration 381 3821.662534713745
I0727 05:11:07.278979 140120985872192 train.py:394] {'eval/walltime': 1583.9632413387299, 'training/sps': 42131.858477077854, 'training/walltime': 2239.511805534363, 'training/entropy_loss': Array(-0.03652886, dtype=float32), 'training/policy_loss': Array(0.00115407, dtype=float32), 'training/total_loss': Array(21.615185, dtype=float32), 'training/v_loss': Array(21.650562, dtype=float32), 'eval/episode_goal_distance': (Array(0.31597334, dtype=float32), Array(0.06084324, dtype=float32)), 'eval/episode_reward': (Array(-15129.753, dtype=float32), Array(6586.2456, dtype=float32)), 'eval/avg_episode_length': (Array(867.97656, dtype=float32), Array(337.3563, dtype=float32)), 'eval/epoch_eval_time': 4.104905366897583, 'eval/sps': 31182.20484014232}
I0727 05:11:07.281533 140120985872192 train.py:379] starting iteration 382 3831.607186317444
I0727 05:11:17.224316 140120985872192 train.py:394] {'eval/walltime': 1588.102332830429, 'training/sps': 42374.11419483205, 'training/walltime': 2245.311572790146, 'training/entropy_loss': Array(-0.03584731, dtype=float32), 'training/policy_loss': Array(0.001359, dtype=float32), 'training/total_loss': Array(20.015572, dtype=float32), 'training/v_loss': Array(20.05006, dtype=float32), 'eval/episode_goal_distance': (Array(0.30385023, dtype=float32), Array(0.05961016, dtype=float32)), 'eval/episode_reward': (Array(-14539.757, dtype=float32), Array(6819.723, dtype=float32)), 'eval/avg_episode_length': (Array(852.41406, dtype=float32), Array(353.49405, dtype=float32)), 'eval/epoch_eval_time': 4.139091491699219, 'eval/sps': 30924.660703127447}
I0727 05:11:17.226755 140120985872192 train.py:379] starting iteration 383 3841.552409172058
I0727 05:11:27.133615 140120985872192 train.py:394] {'eval/walltime': 1592.201449394226, 'training/sps': 42343.527133399024, 'training/walltime': 2251.115529537201, 'training/entropy_loss': Array(-0.03622033, dtype=float32), 'training/policy_loss': Array(0.00227026, dtype=float32), 'training/total_loss': Array(130.61885, dtype=float32), 'training/v_loss': Array(130.6528, dtype=float32), 'eval/episode_goal_distance': (Array(0.30214742, dtype=float32), Array(0.06592753, dtype=float32)), 'eval/episode_reward': (Array(-13996.154, dtype=float32), Array(7572.039, dtype=float32)), 'eval/avg_episode_length': (Array(805.8281, dtype=float32), Array(394.1265, dtype=float32)), 'eval/epoch_eval_time': 4.099116563796997, 'eval/sps': 31226.240583271938}
I0727 05:11:27.136047 140120985872192 train.py:379] starting iteration 384 3851.461701631546
I0727 05:11:37.053587 140120985872192 train.py:394] {'eval/walltime': 1596.3328688144684, 'training/sps': 42502.27979544653, 'training/walltime': 2256.897807598114, 'training/entropy_loss': Array(-0.03614092, dtype=float32), 'training/policy_loss': Array(0.00240209, dtype=float32), 'training/total_loss': Array(28.04288, dtype=float32), 'training/v_loss': Array(28.076618, dtype=float32), 'eval/episode_goal_distance': (Array(0.3111716, dtype=float32), Array(0.0681181, dtype=float32)), 'eval/episode_reward': (Array(-15005.467, dtype=float32), Array(7027.7876, dtype=float32)), 'eval/avg_episode_length': (Array(852.41406, dtype=float32), Array(353.4943, dtype=float32)), 'eval/epoch_eval_time': 4.13141942024231, 'eval/sps': 30982.087989626758}
I0727 05:11:37.056177 140120985872192 train.py:379] starting iteration 385 3861.3818311691284
I0727 05:11:47.003585 140120985872192 train.py:394] {'eval/walltime': 1600.432603597641, 'training/sps': 42061.45250465656, 'training/walltime': 2262.7406871318817, 'training/entropy_loss': Array(-0.0371572, dtype=float32), 'training/policy_loss': Array(0.00193938, dtype=float32), 'training/total_loss': Array(25.357437, dtype=float32), 'training/v_loss': Array(25.392654, dtype=float32), 'eval/episode_goal_distance': (Array(0.3093639, dtype=float32), Array(0.06932595, dtype=float32)), 'eval/episode_reward': (Array(-13207.565, dtype=float32), Array(7806.798, dtype=float32)), 'eval/avg_episode_length': (Array(767.03125, dtype=float32), Array(421.06714, dtype=float32)), 'eval/epoch_eval_time': 4.099734783172607, 'eval/sps': 31221.5318233211}
I0727 05:11:47.006003 140120985872192 train.py:379] starting iteration 386 3871.3316571712494
I0727 05:11:56.946882 140120985872192 train.py:394] {'eval/walltime': 1604.567527294159, 'training/sps': 42356.345275382446, 'training/walltime': 2268.5428874492645, 'training/entropy_loss': Array(-0.03710967, dtype=float32), 'training/policy_loss': Array(0.00187674, dtype=float32), 'training/total_loss': Array(26.317726, dtype=float32), 'training/v_loss': Array(26.352957, dtype=float32), 'eval/episode_goal_distance': (Array(0.29772305, dtype=float32), Array(0.06731701, dtype=float32)), 'eval/episode_reward': (Array(-14545.0625, dtype=float32), Array(6814.704, dtype=float32)), 'eval/avg_episode_length': (Array(852.375, dtype=float32), Array(353.5879, dtype=float32)), 'eval/epoch_eval_time': 4.134923696517944, 'eval/sps': 30955.83120621789}
I0727 05:11:56.949485 140120985872192 train.py:379] starting iteration 387 3881.275139093399
I0727 05:12:06.854157 140120985872192 train.py:394] {'eval/walltime': 1608.666740179062, 'training/sps': 42362.92526363601, 'training/walltime': 2274.3441865444183, 'training/entropy_loss': Array(-0.03753902, dtype=float32), 'training/policy_loss': Array(0.00165462, dtype=float32), 'training/total_loss': Array(21.244236, dtype=float32), 'training/v_loss': Array(21.280117, dtype=float32), 'eval/episode_goal_distance': (Array(0.30865484, dtype=float32), Array(0.07681221, dtype=float32)), 'eval/episode_reward': (Array(-14151.011, dtype=float32), Array(7078.987, dtype=float32)), 'eval/avg_episode_length': (Array(836.8203, dtype=float32), Array(368.34067, dtype=float32)), 'eval/epoch_eval_time': 4.099212884902954, 'eval/sps': 31225.506845817377}
I0727 05:12:06.856715 140120985872192 train.py:379] starting iteration 388 3891.1823692321777
I0727 05:12:16.757655 140120985872192 train.py:394] {'eval/walltime': 1612.780977010727, 'training/sps': 42500.31009794937, 'training/walltime': 2280.1267325878143, 'training/entropy_loss': Array(-0.03701941, dtype=float32), 'training/policy_loss': Array(0.00264578, dtype=float32), 'training/total_loss': Array(21.693338, dtype=float32), 'training/v_loss': Array(21.72771, dtype=float32), 'eval/episode_goal_distance': (Array(0.31514198, dtype=float32), Array(0.0614319, dtype=float32)), 'eval/episode_reward': (Array(-15699.943, dtype=float32), Array(6770.5254, dtype=float32)), 'eval/avg_episode_length': (Array(883.3906, dtype=float32), Array(320.0572, dtype=float32)), 'eval/epoch_eval_time': 4.114236831665039, 'eval/sps': 31111.48075260368}
I0727 05:12:16.760257 140120985872192 train.py:379] starting iteration 389 3901.085911512375
I0727 05:12:26.663690 140120985872192 train.py:394] {'eval/walltime': 1616.8679718971252, 'training/sps': 42281.52792363836, 'training/walltime': 2285.939199924469, 'training/entropy_loss': Array(-0.03605346, dtype=float32), 'training/policy_loss': Array(0.00371952, dtype=float32), 'training/total_loss': Array(22.201664, dtype=float32), 'training/v_loss': Array(22.233997, dtype=float32), 'eval/episode_goal_distance': (Array(0.30687612, dtype=float32), Array(0.06460351, dtype=float32)), 'eval/episode_reward': (Array(-15274.16, dtype=float32), Array(6847.061, dtype=float32)), 'eval/avg_episode_length': (Array(867.875, dtype=float32), Array(337.616, dtype=float32)), 'eval/epoch_eval_time': 4.086994886398315, 'eval/sps': 31318.854943026523}
I0727 05:12:26.666088 140120985872192 train.py:379] starting iteration 390 3910.991741657257
I0727 05:12:36.614265 140120985872192 train.py:394] {'eval/walltime': 1620.9935221672058, 'training/sps': 42237.69328317221, 'training/walltime': 2291.7576994895935, 'training/entropy_loss': Array(-0.03657278, dtype=float32), 'training/policy_loss': Array(0.00309095, dtype=float32), 'training/total_loss': Array(33.23086, dtype=float32), 'training/v_loss': Array(33.264343, dtype=float32), 'eval/episode_goal_distance': (Array(0.31445804, dtype=float32), Array(0.07109784, dtype=float32)), 'eval/episode_reward': (Array(-15372.058, dtype=float32), Array(7257.113, dtype=float32)), 'eval/avg_episode_length': (Array(852.2656, dtype=float32), Array(353.84958, dtype=float32)), 'eval/epoch_eval_time': 4.125550270080566, 'eval/sps': 31026.16417700331}
I0727 05:12:36.616685 140120985872192 train.py:379] starting iteration 391 3920.942338705063
I0727 05:12:46.521734 140120985872192 train.py:394] {'eval/walltime': 1625.0856251716614, 'training/sps': 42307.82968990563, 'training/walltime': 2297.5665533542633, 'training/entropy_loss': Array(-0.03531576, dtype=float32), 'training/policy_loss': Array(0.00667709, dtype=float32), 'training/total_loss': Array(122.13397, dtype=float32), 'training/v_loss': Array(122.16261, dtype=float32), 'eval/episode_goal_distance': (Array(0.30363333, dtype=float32), Array(0.06028923, dtype=float32)), 'eval/episode_reward': (Array(-14531.74, dtype=float32), Array(7204.519, dtype=float32)), 'eval/avg_episode_length': (Array(836.8594, dtype=float32), Array(368.25195, dtype=float32)), 'eval/epoch_eval_time': 4.092103004455566, 'eval/sps': 31279.760030632406}
I0727 05:12:46.524237 140120985872192 train.py:379] starting iteration 392 3930.849890947342
I0727 05:12:56.421942 140120985872192 train.py:394] {'eval/walltime': 1629.193663597107, 'training/sps': 42478.46291080238, 'training/walltime': 2303.352073431015, 'training/entropy_loss': Array(-0.03496321, dtype=float32), 'training/policy_loss': Array(0.00397898, dtype=float32), 'training/total_loss': Array(34.459557, dtype=float32), 'training/v_loss': Array(34.49054, dtype=float32), 'eval/episode_goal_distance': (Array(0.3107218, dtype=float32), Array(0.0649276, dtype=float32)), 'eval/episode_reward': (Array(-16128.748, dtype=float32), Array(6598.8906, dtype=float32)), 'eval/avg_episode_length': (Array(891.2578, dtype=float32), Array(310.30386, dtype=float32)), 'eval/epoch_eval_time': 4.108038425445557, 'eval/sps': 31158.42325309242}
I0727 05:12:56.424560 140120985872192 train.py:379] starting iteration 393 3940.7502138614655
I0727 05:13:06.307670 140120985872192 train.py:394] {'eval/walltime': 1633.295569896698, 'training/sps': 42540.390094385664, 'training/walltime': 2309.12917137146, 'training/entropy_loss': Array(-0.03517394, dtype=float32), 'training/policy_loss': Array(0.004703, dtype=float32), 'training/total_loss': Array(23.924309, dtype=float32), 'training/v_loss': Array(23.95478, dtype=float32), 'eval/episode_goal_distance': (Array(0.3163371, dtype=float32), Array(0.06350644, dtype=float32)), 'eval/episode_reward': (Array(-13128.526, dtype=float32), Array(8110.515, dtype=float32)), 'eval/avg_episode_length': (Array(751.4922, dtype=float32), Array(430.42908, dtype=float32)), 'eval/epoch_eval_time': 4.1019062995910645, 'eval/sps': 31205.003393851497}
I0727 05:13:06.310073 140120985872192 train.py:379] starting iteration 394 3950.6357266902924
I0727 05:13:16.194601 140120985872192 train.py:394] {'eval/walltime': 1637.3851299285889, 'training/sps': 42438.35444645192, 'training/walltime': 2314.920159339905, 'training/entropy_loss': Array(-0.03648726, dtype=float32), 'training/policy_loss': Array(0.00327065, dtype=float32), 'training/total_loss': Array(32.100628, dtype=float32), 'training/v_loss': Array(32.133846, dtype=float32), 'eval/episode_goal_distance': (Array(0.31740695, dtype=float32), Array(0.06277027, dtype=float32)), 'eval/episode_reward': (Array(-15658.201, dtype=float32), Array(7682.2583, dtype=float32)), 'eval/avg_episode_length': (Array(844.6797, dtype=float32), Array(360.93286, dtype=float32)), 'eval/epoch_eval_time': 4.089560031890869, 'eval/sps': 31299.21042895592}
I0727 05:13:16.197157 140120985872192 train.py:379] starting iteration 395 3960.5228114128113
I0727 05:13:26.187762 140120985872192 train.py:394] {'eval/walltime': 1641.499346256256, 'training/sps': 41847.755303902355, 'training/walltime': 2320.792875766754, 'training/entropy_loss': Array(-0.03613439, dtype=float32), 'training/policy_loss': Array(0.002783, dtype=float32), 'training/total_loss': Array(35.09729, dtype=float32), 'training/v_loss': Array(35.13064, dtype=float32), 'eval/episode_goal_distance': (Array(0.31043187, dtype=float32), Array(0.06643246, dtype=float32)), 'eval/episode_reward': (Array(-15973.408, dtype=float32), Array(6517.372, dtype=float32)), 'eval/avg_episode_length': (Array(899.1172, dtype=float32), Array(300.05103, dtype=float32)), 'eval/epoch_eval_time': 4.114216327667236, 'eval/sps': 31111.635802723114}
I0727 05:13:26.190073 140120985872192 train.py:379] starting iteration 396 3970.515727519989
I0727 05:13:36.112276 140120985872192 train.py:394] {'eval/walltime': 1645.6063611507416, 'training/sps': 42288.363999741705, 'training/walltime': 2326.6044034957886, 'training/entropy_loss': Array(-0.03442324, dtype=float32), 'training/policy_loss': Array(0.00093251, dtype=float32), 'training/total_loss': Array(21.677692, dtype=float32), 'training/v_loss': Array(21.711182, dtype=float32), 'eval/episode_goal_distance': (Array(0.31074488, dtype=float32), Array(0.06421774, dtype=float32)), 'eval/episode_reward': (Array(-15271.485, dtype=float32), Array(6470.1914, dtype=float32)), 'eval/avg_episode_length': (Array(875.6719, dtype=float32), Array(328.94174, dtype=float32)), 'eval/epoch_eval_time': 4.107014894485474, 'eval/sps': 31166.188408974795}
I0727 05:13:36.114451 140120985872192 train.py:379] starting iteration 397 3980.4401059150696
I0727 05:13:46.001811 140120985872192 train.py:394] {'eval/walltime': 1649.6999776363373, 'training/sps': 42444.91100418638, 'training/walltime': 2332.3944969177246, 'training/entropy_loss': Array(-0.03346822, dtype=float32), 'training/policy_loss': Array(0.00061869, dtype=float32), 'training/total_loss': Array(22.991116, dtype=float32), 'training/v_loss': Array(23.023964, dtype=float32), 'eval/episode_goal_distance': (Array(0.31658858, dtype=float32), Array(0.06578517, dtype=float32)), 'eval/episode_reward': (Array(-16290.763, dtype=float32), Array(7138.1665, dtype=float32)), 'eval/avg_episode_length': (Array(883.64844, dtype=float32), Array(319.3497, dtype=float32)), 'eval/epoch_eval_time': 4.093616485595703, 'eval/sps': 31268.19535987222}
I0727 05:13:46.004004 140120985872192 train.py:379] starting iteration 398 3990.329658508301
I0727 05:13:55.927062 140120985872192 train.py:394] {'eval/walltime': 1653.8305060863495, 'training/sps': 42454.73740033429, 'training/walltime': 2338.1832501888275, 'training/entropy_loss': Array(-0.03360363, dtype=float32), 'training/policy_loss': Array(0.00099089, dtype=float32), 'training/total_loss': Array(31.112823, dtype=float32), 'training/v_loss': Array(31.145435, dtype=float32), 'eval/episode_goal_distance': (Array(0.3238054, dtype=float32), Array(0.06082909, dtype=float32)), 'eval/episode_reward': (Array(-17278.322, dtype=float32), Array(6338.934, dtype=float32)), 'eval/avg_episode_length': (Array(930.0703, dtype=float32), Array(254.2815, dtype=float32)), 'eval/epoch_eval_time': 4.130528450012207, 'eval/sps': 30988.770940343413}
I0727 05:13:55.929669 140120985872192 train.py:379] starting iteration 399 4000.2553231716156
I0727 05:14:05.887632 140120985872192 train.py:394] {'eval/walltime': 1657.94783949852, 'training/sps': 42104.48507023554, 'training/walltime': 2344.0201580524445, 'training/entropy_loss': Array(-0.03276337, dtype=float32), 'training/policy_loss': Array(0.00059962, dtype=float32), 'training/total_loss': Array(22.224888, dtype=float32), 'training/v_loss': Array(22.257051, dtype=float32), 'eval/episode_goal_distance': (Array(0.30580035, dtype=float32), Array(0.06752792, dtype=float32)), 'eval/episode_reward': (Array(-16616.129, dtype=float32), Array(5985.4043, dtype=float32)), 'eval/avg_episode_length': (Array(937.875, dtype=float32), Array(240.6097, dtype=float32)), 'eval/epoch_eval_time': 4.11733341217041, 'eval/sps': 31088.08230629205}
I0727 05:14:05.889828 140120985872192 train.py:379] starting iteration 400 4010.2154817581177
I0727 05:14:15.814032 140120985872192 train.py:394] {'eval/walltime': 1662.0448350906372, 'training/sps': 42201.76622532705, 'training/walltime': 2349.8436110019684, 'training/entropy_loss': Array(-0.03345023, dtype=float32), 'training/policy_loss': Array(0.00060496, dtype=float32), 'training/total_loss': Array(75.87538, dtype=float32), 'training/v_loss': Array(75.908226, dtype=float32), 'eval/episode_goal_distance': (Array(0.2932796, dtype=float32), Array(0.06364718, dtype=float32)), 'eval/episode_reward': (Array(-15293.386, dtype=float32), Array(6155.6157, dtype=float32)), 'eval/avg_episode_length': (Array(899.0469, dtype=float32), Array(300.26038, dtype=float32)), 'eval/epoch_eval_time': 4.09699559211731, 'eval/sps': 31242.40608075689}
I0727 05:14:15.816288 140120985872192 train.py:379] starting iteration 401 4020.1419417858124
I0727 05:14:25.717831 140120985872192 train.py:394] {'eval/walltime': 1666.156102180481, 'training/sps': 42470.424304936016, 'training/walltime': 2355.630226135254, 'training/entropy_loss': Array(-0.0329605, dtype=float32), 'training/policy_loss': Array(0.00056773, dtype=float32), 'training/total_loss': Array(24.353811, dtype=float32), 'training/v_loss': Array(24.386204, dtype=float32), 'eval/episode_goal_distance': (Array(0.30673265, dtype=float32), Array(0.06216657, dtype=float32)), 'eval/episode_reward': (Array(-16088.779, dtype=float32), Array(6273.1035, dtype=float32)), 'eval/avg_episode_length': (Array(906.83594, dtype=float32), Array(289.65955, dtype=float32)), 'eval/epoch_eval_time': 4.11126708984375, 'eval/sps': 31133.953888864147}
I0727 05:14:25.720078 140120985872192 train.py:379] starting iteration 402 4030.045732498169
I0727 05:14:35.658202 140120985872192 train.py:394] {'eval/walltime': 1670.2897469997406, 'training/sps': 42367.01528639691, 'training/walltime': 2361.4309651851654, 'training/entropy_loss': Array(-0.03305898, dtype=float32), 'training/policy_loss': Array(0.0008764, dtype=float32), 'training/total_loss': Array(30.722485, dtype=float32), 'training/v_loss': Array(30.75467, dtype=float32), 'eval/episode_goal_distance': (Array(0.31035244, dtype=float32), Array(0.06497551, dtype=float32)), 'eval/episode_reward': (Array(-15788.529, dtype=float32), Array(6843.3335, dtype=float32)), 'eval/avg_episode_length': (Array(883.46875, dtype=float32), Array(319.84286, dtype=float32)), 'eval/epoch_eval_time': 4.1336448192596436, 'eval/sps': 30965.40839784233}
I0727 05:14:35.660389 140120985872192 train.py:379] starting iteration 403 4039.986043214798
I0727 05:14:45.593416 140120985872192 train.py:394] {'eval/walltime': 1674.3931720256805, 'training/sps': 42183.86876411741, 'training/walltime': 2367.2568888664246, 'training/entropy_loss': Array(-0.03324884, dtype=float32), 'training/policy_loss': Array(0.0006928, dtype=float32), 'training/total_loss': Array(30.152483, dtype=float32), 'training/v_loss': Array(30.185038, dtype=float32), 'eval/episode_goal_distance': (Array(0.32117397, dtype=float32), Array(0.06120694, dtype=float32)), 'eval/episode_reward': (Array(-16011.986, dtype=float32), Array(6556.2744, dtype=float32)), 'eval/avg_episode_length': (Array(891.3125, dtype=float32), Array(310.14816, dtype=float32)), 'eval/epoch_eval_time': 4.103425025939941, 'eval/sps': 31193.454051394536}
I0727 05:14:45.595694 140120985872192 train.py:379] starting iteration 404 4049.9213478565216
I0727 05:14:55.543653 140120985872192 train.py:394] {'eval/walltime': 1678.53098654747, 'training/sps': 42326.6754422233, 'training/walltime': 2373.0631563663483, 'training/entropy_loss': Array(-0.03310491, dtype=float32), 'training/policy_loss': Array(0.00058491, dtype=float32), 'training/total_loss': Array(22.709255, dtype=float32), 'training/v_loss': Array(22.741776, dtype=float32), 'eval/episode_goal_distance': (Array(0.31162447, dtype=float32), Array(0.06477318, dtype=float32)), 'eval/episode_reward': (Array(-15458.1875, dtype=float32), Array(6645.4463, dtype=float32)), 'eval/avg_episode_length': (Array(875.8672, dtype=float32), Array(328.42538, dtype=float32)), 'eval/epoch_eval_time': 4.137814521789551, 'eval/sps': 30934.204354969897}
I0727 05:14:55.546077 140120985872192 train.py:379] starting iteration 405 4059.871731042862
I0727 05:15:05.484709 140120985872192 train.py:394] {'eval/walltime': 1682.6396310329437, 'training/sps': 42181.77311377638, 'training/walltime': 2378.8893694877625, 'training/entropy_loss': Array(-0.03287507, dtype=float32), 'training/policy_loss': Array(0.00103483, dtype=float32), 'training/total_loss': Array(23.885485, dtype=float32), 'training/v_loss': Array(23.917324, dtype=float32), 'eval/episode_goal_distance': (Array(0.30876327, dtype=float32), Array(0.0619339, dtype=float32)), 'eval/episode_reward': (Array(-15929.216, dtype=float32), Array(6780.3926, dtype=float32)), 'eval/avg_episode_length': (Array(883.5703, dtype=float32), Array(319.56418, dtype=float32)), 'eval/epoch_eval_time': 4.108644485473633, 'eval/sps': 31153.827120489965}
I0727 05:15:05.486877 140120985872192 train.py:379] starting iteration 406 4069.8125314712524
I0727 05:15:15.401684 140120985872192 train.py:394] {'eval/walltime': 1686.7304780483246, 'training/sps': 42224.79290277262, 'training/walltime': 2384.7096467018127, 'training/entropy_loss': Array(-0.03127852, dtype=float32), 'training/policy_loss': Array(0.00205207, dtype=float32), 'training/total_loss': Array(26.521225, dtype=float32), 'training/v_loss': Array(26.55045, dtype=float32), 'eval/episode_goal_distance': (Array(0.30937058, dtype=float32), Array(0.06314823, dtype=float32)), 'eval/episode_reward': (Array(-15531.936, dtype=float32), Array(7056.5386, dtype=float32)), 'eval/avg_episode_length': (Array(867.9531, dtype=float32), Array(337.41623, dtype=float32)), 'eval/epoch_eval_time': 4.090847015380859, 'eval/sps': 31289.36367425687}
I0727 05:15:15.404093 140120985872192 train.py:379] starting iteration 407 4079.729746580124
I0727 05:15:25.338660 140120985872192 train.py:394] {'eval/walltime': 1690.8275518417358, 'training/sps': 42127.34886545257, 'training/walltime': 2390.543386697769, 'training/entropy_loss': Array(-0.03043954, dtype=float32), 'training/policy_loss': Array(0.00301817, dtype=float32), 'training/total_loss': Array(25.554739, dtype=float32), 'training/v_loss': Array(25.582159, dtype=float32), 'eval/episode_goal_distance': (Array(0.29938954, dtype=float32), Array(0.06448378, dtype=float32)), 'eval/episode_reward': (Array(-15582.045, dtype=float32), Array(5928.92, dtype=float32)), 'eval/avg_episode_length': (Array(914.52344, dtype=float32), Array(278.76895, dtype=float32)), 'eval/epoch_eval_time': 4.097073793411255, 'eval/sps': 31241.809753547597}
I0727 05:15:25.341168 140120985872192 train.py:379] starting iteration 408 4089.666822195053
I0727 05:15:35.259283 140120985872192 train.py:394] {'eval/walltime': 1694.9582779407501, 'training/sps': 42491.576584291826, 'training/walltime': 2396.327121257782, 'training/entropy_loss': Array(-0.03019362, dtype=float32), 'training/policy_loss': Array(0.00525608, dtype=float32), 'training/total_loss': Array(73.9586, dtype=float32), 'training/v_loss': Array(73.983536, dtype=float32), 'eval/episode_goal_distance': (Array(0.3112026, dtype=float32), Array(0.06108558, dtype=float32)), 'eval/episode_reward': (Array(-15498.803, dtype=float32), Array(7080.8896, dtype=float32)), 'eval/avg_episode_length': (Array(860.15625, dtype=float32), Array(345.70383, dtype=float32)), 'eval/epoch_eval_time': 4.130726099014282, 'eval/sps': 30987.288174479716}
I0727 05:15:35.440384 140120985872192 train.py:379] starting iteration 409 4099.766033172607
I0727 05:15:45.357936 140120985872192 train.py:394] {'eval/walltime': 1699.0569648742676, 'training/sps': 42262.07256126756, 'training/walltime': 2402.14226436615, 'training/entropy_loss': Array(-0.03116783, dtype=float32), 'training/policy_loss': Array(0.00054262, dtype=float32), 'training/total_loss': Array(27.02073, dtype=float32), 'training/v_loss': Array(27.051353, dtype=float32), 'eval/episode_goal_distance': (Array(0.30744463, dtype=float32), Array(0.06576955, dtype=float32)), 'eval/episode_reward': (Array(-16552.36, dtype=float32), Array(5491.8423, dtype=float32)), 'eval/avg_episode_length': (Array(945.6797, dtype=float32), Array(225.84338, dtype=float32)), 'eval/epoch_eval_time': 4.098686933517456, 'eval/sps': 31229.513762875165}
I0727 05:15:45.360244 140120985872192 train.py:379] starting iteration 410 4109.685898780823
I0727 05:15:55.296045 140120985872192 train.py:394] {'eval/walltime': 1703.180911064148, 'training/sps': 42312.685452005586, 'training/walltime': 2407.9504516124725, 'training/entropy_loss': Array(-0.02940292, dtype=float32), 'training/policy_loss': Array(0.00102183, dtype=float32), 'training/total_loss': Array(26.16021, dtype=float32), 'training/v_loss': Array(26.188591, dtype=float32), 'eval/episode_goal_distance': (Array(0.30807894, dtype=float32), Array(0.05573591, dtype=float32)), 'eval/episode_reward': (Array(-16654.9, dtype=float32), Array(6057.538, dtype=float32)), 'eval/avg_episode_length': (Array(922.46094, dtype=float32), Array(266.35645, dtype=float32)), 'eval/epoch_eval_time': 4.123946189880371, 'eval/sps': 31038.232340202547}
I0727 05:15:55.298374 140120985872192 train.py:379] starting iteration 411 4119.624028205872
I0727 05:16:05.195720 140120985872192 train.py:394] {'eval/walltime': 1707.277257680893, 'training/sps': 42391.23206439963, 'training/walltime': 2413.747876882553, 'training/entropy_loss': Array(-0.02968327, dtype=float32), 'training/policy_loss': Array(0.00184982, dtype=float32), 'training/total_loss': Array(28.195618, dtype=float32), 'training/v_loss': Array(28.22345, dtype=float32), 'eval/episode_goal_distance': (Array(0.30463374, dtype=float32), Array(0.06596471, dtype=float32)), 'eval/episode_reward': (Array(-15543.436, dtype=float32), Array(6303.76, dtype=float32)), 'eval/avg_episode_length': (Array(906.8281, dtype=float32), Array(289.6834, dtype=float32)), 'eval/epoch_eval_time': 4.096346616744995, 'eval/sps': 31247.355747866448}
I0727 05:16:05.197917 140120985872192 train.py:379] starting iteration 412 4129.523571014404
I0727 05:16:15.131450 140120985872192 train.py:394] {'eval/walltime': 1711.3879137039185, 'training/sps': 42231.795779443135, 'training/walltime': 2419.567188978195, 'training/entropy_loss': Array(-0.02995482, dtype=float32), 'training/policy_loss': Array(0.00168363, dtype=float32), 'training/total_loss': Array(25.152065, dtype=float32), 'training/v_loss': Array(25.18034, dtype=float32), 'eval/episode_goal_distance': (Array(0.3140074, dtype=float32), Array(0.06732381, dtype=float32)), 'eval/episode_reward': (Array(-15095.02, dtype=float32), Array(6960.667, dtype=float32)), 'eval/avg_episode_length': (Array(860.21875, dtype=float32), Array(345.54935, dtype=float32)), 'eval/epoch_eval_time': 4.110656023025513, 'eval/sps': 31138.582085929396}
I0727 05:16:15.133626 140120985872192 train.py:379] starting iteration 413 4139.459280490875
I0727 05:16:25.026314 140120985872192 train.py:394] {'eval/walltime': 1715.482885837555, 'training/sps': 42416.050764021435, 'training/walltime': 2425.3612220287323, 'training/entropy_loss': Array(-0.02970571, dtype=float32), 'training/policy_loss': Array(0.00188548, dtype=float32), 'training/total_loss': Array(23.540243, dtype=float32), 'training/v_loss': Array(23.568066, dtype=float32), 'eval/episode_goal_distance': (Array(0.30435407, dtype=float32), Array(0.06500094, dtype=float32)), 'eval/episode_reward': (Array(-15991.509, dtype=float32), Array(6347.2754, dtype=float32)), 'eval/avg_episode_length': (Array(899.10156, dtype=float32), Array(300.09784, dtype=float32)), 'eval/epoch_eval_time': 4.094972133636475, 'eval/sps': 31257.843966408545}
I0727 05:16:25.028703 140120985872192 train.py:379] starting iteration 414 4149.354357481003
I0727 05:16:34.937520 140120985872192 train.py:394] {'eval/walltime': 1719.5711393356323, 'training/sps': 42248.89926966803, 'training/walltime': 2431.1781783103943, 'training/entropy_loss': Array(-0.02882029, dtype=float32), 'training/policy_loss': Array(0.00192275, dtype=float32), 'training/total_loss': Array(20.89436, dtype=float32), 'training/v_loss': Array(20.921257, dtype=float32), 'eval/episode_goal_distance': (Array(0.30743378, dtype=float32), Array(0.06306894, dtype=float32)), 'eval/episode_reward': (Array(-16116.586, dtype=float32), Array(6495.644, dtype=float32)), 'eval/avg_episode_length': (Array(899.0781, dtype=float32), Array(300.16714, dtype=float32)), 'eval/epoch_eval_time': 4.088253498077393, 'eval/sps': 31309.21310535057}
I0727 05:16:34.939681 140120985872192 train.py:379] starting iteration 415 4159.265335559845
I0727 05:16:44.884277 140120985872192 train.py:394] {'eval/walltime': 1723.6989405155182, 'training/sps': 42276.83709759183, 'training/walltime': 2436.9912905693054, 'training/entropy_loss': Array(-0.02991773, dtype=float32), 'training/policy_loss': Array(0.00206458, dtype=float32), 'training/total_loss': Array(25.051582, dtype=float32), 'training/v_loss': Array(25.079433, dtype=float32), 'eval/episode_goal_distance': (Array(0.3132593, dtype=float32), Array(0.05837016, dtype=float32)), 'eval/episode_reward': (Array(-15564.08, dtype=float32), Array(6674.064, dtype=float32)), 'eval/avg_episode_length': (Array(875.6719, dtype=float32), Array(328.942, dtype=float32)), 'eval/epoch_eval_time': 4.127801179885864, 'eval/sps': 31009.24546068841}
I0727 05:16:44.886696 140120985872192 train.py:379] starting iteration 416 4169.212349176407
I0727 05:16:54.806639 140120985872192 train.py:394] {'eval/walltime': 1727.796632051468, 'training/sps': 42237.76251247925, 'training/walltime': 2442.8097805976868, 'training/entropy_loss': Array(-0.02944, dtype=float32), 'training/policy_loss': Array(0.0036613, dtype=float32), 'training/total_loss': Array(76.7876, dtype=float32), 'training/v_loss': Array(76.81338, dtype=float32), 'eval/episode_goal_distance': (Array(0.3116969, dtype=float32), Array(0.05647648, dtype=float32)), 'eval/episode_reward': (Array(-15311.211, dtype=float32), Array(6470.9565, dtype=float32)), 'eval/avg_episode_length': (Array(875.78906, dtype=float32), Array(328.6316, dtype=float32)), 'eval/epoch_eval_time': 4.097691535949707, 'eval/sps': 31237.099932250978}
I0727 05:16:54.808877 140120985872192 train.py:379] starting iteration 417 4179.134531021118
I0727 05:17:04.758649 140120985872192 train.py:394] {'eval/walltime': 1731.9225361347198, 'training/sps': 42224.88630530988, 'training/walltime': 2448.630044937134, 'training/entropy_loss': Array(-0.02933402, dtype=float32), 'training/policy_loss': Array(0.00089185, dtype=float32), 'training/total_loss': Array(29.60197, dtype=float32), 'training/v_loss': Array(29.630411, dtype=float32), 'eval/episode_goal_distance': (Array(0.30028266, dtype=float32), Array(0.05715534, dtype=float32)), 'eval/episode_reward': (Array(-15631.432, dtype=float32), Array(6389.801, dtype=float32)), 'eval/avg_episode_length': (Array(891.25, dtype=float32), Array(310.3263, dtype=float32)), 'eval/epoch_eval_time': 4.125904083251953, 'eval/sps': 31023.503556367945}
I0727 05:17:04.761024 140120985872192 train.py:379] starting iteration 418 4189.086678266525
I0727 05:17:14.726882 140120985872192 train.py:394] {'eval/walltime': 1736.0253772735596, 'training/sps': 41942.99392005063, 'training/walltime': 2454.4894263744354, 'training/entropy_loss': Array(-0.02885411, dtype=float32), 'training/policy_loss': Array(0.00116927, dtype=float32), 'training/total_loss': Array(24.620522, dtype=float32), 'training/v_loss': Array(24.648205, dtype=float32), 'eval/episode_goal_distance': (Array(0.3121286, dtype=float32), Array(0.06261735, dtype=float32)), 'eval/episode_reward': (Array(-16505.54, dtype=float32), Array(5684.4106, dtype=float32)), 'eval/avg_episode_length': (Array(930.2031, dtype=float32), Array(253.79887, dtype=float32)), 'eval/epoch_eval_time': 4.102841138839722, 'eval/sps': 31197.893281385554}
I0727 05:17:14.729271 140120985872192 train.py:379] starting iteration 419 4199.054925203323
I0727 05:17:24.634557 140120985872192 train.py:394] {'eval/walltime': 1740.120074748993, 'training/sps': 42320.92334545915, 'training/walltime': 2460.296483039856, 'training/entropy_loss': Array(-0.02888444, dtype=float32), 'training/policy_loss': Array(0.00079882, dtype=float32), 'training/total_loss': Array(28.477537, dtype=float32), 'training/v_loss': Array(28.505621, dtype=float32), 'eval/episode_goal_distance': (Array(0.31480262, dtype=float32), Array(0.06532355, dtype=float32)), 'eval/episode_reward': (Array(-15368.953, dtype=float32), Array(6563.188, dtype=float32)), 'eval/avg_episode_length': (Array(883.53906, dtype=float32), Array(319.65002, dtype=float32)), 'eval/epoch_eval_time': 4.09469747543335, 'eval/sps': 31259.940634919192}
I0727 05:17:24.636876 140120985872192 train.py:379] starting iteration 420 4208.962530374527
I0727 05:17:34.619545 140120985872192 train.py:394] {'eval/walltime': 1744.2440633773804, 'training/sps': 41973.07493228187, 'training/walltime': 2466.151665210724, 'training/entropy_loss': Array(-0.02832956, dtype=float32), 'training/policy_loss': Array(0.00032622, dtype=float32), 'training/total_loss': Array(28.947811, dtype=float32), 'training/v_loss': Array(28.975815, dtype=float32), 'eval/episode_goal_distance': (Array(0.30817354, dtype=float32), Array(0.06529616, dtype=float32)), 'eval/episode_reward': (Array(-15864.969, dtype=float32), Array(6504.785, dtype=float32)), 'eval/avg_episode_length': (Array(891.1875, dtype=float32), Array(310.50424, dtype=float32)), 'eval/epoch_eval_time': 4.123988628387451, 'eval/sps': 31037.912936741086}
I0727 05:17:34.621807 140120985872192 train.py:379] starting iteration 421 4218.947461366653
I0727 05:17:44.526911 140120985872192 train.py:394] {'eval/walltime': 1748.3364436626434, 'training/sps': 42306.18009592347, 'training/walltime': 2471.960745573044, 'training/entropy_loss': Array(-0.02898153, dtype=float32), 'training/policy_loss': Array(0.00062303, dtype=float32), 'training/total_loss': Array(20.501053, dtype=float32), 'training/v_loss': Array(20.529413, dtype=float32), 'eval/episode_goal_distance': (Array(0.32496008, dtype=float32), Array(0.06503114, dtype=float32)), 'eval/episode_reward': (Array(-16359.089, dtype=float32), Array(6985.623, dtype=float32)), 'eval/avg_episode_length': (Array(891.28906, dtype=float32), Array(310.21494, dtype=float32)), 'eval/epoch_eval_time': 4.0923802852630615, 'eval/sps': 31277.64065840525}
I0727 05:17:44.529254 140120985872192 train.py:379] starting iteration 422 4228.854908943176
I0727 05:17:54.480943 140120985872192 train.py:394] {'eval/walltime': 1752.4553294181824, 'training/sps': 42160.322802961324, 'training/walltime': 2477.789922952652, 'training/entropy_loss': Array(-0.02867963, dtype=float32), 'training/policy_loss': Array(0.00048813, dtype=float32), 'training/total_loss': Array(21.452578, dtype=float32), 'training/v_loss': Array(21.48077, dtype=float32), 'eval/episode_goal_distance': (Array(0.30903512, dtype=float32), Array(0.05557246, dtype=float32)), 'eval/episode_reward': (Array(-16204.11, dtype=float32), Array(6430.503, dtype=float32)), 'eval/avg_episode_length': (Array(906.8281, dtype=float32), Array(289.68384, dtype=float32)), 'eval/epoch_eval_time': 4.11888575553894, 'eval/sps': 31076.365696200693}
I0727 05:17:54.483510 140120985872192 train.py:379] starting iteration 423 4238.809164524078
I0727 05:18:04.407306 140120985872192 train.py:394] {'eval/walltime': 1756.565104484558, 'training/sps': 42297.09574308578, 'training/walltime': 2483.6002509593964, 'training/entropy_loss': Array(-0.02698864, dtype=float32), 'training/policy_loss': Array(0.00069992, dtype=float32), 'training/total_loss': Array(26.300817, dtype=float32), 'training/v_loss': Array(26.327106, dtype=float32), 'eval/episode_goal_distance': (Array(0.3027038, dtype=float32), Array(0.06837186, dtype=float32)), 'eval/episode_reward': (Array(-16727.1, dtype=float32), Array(5588.628, dtype=float32)), 'eval/avg_episode_length': (Array(945.6328, dtype=float32), Array(226.03789, dtype=float32)), 'eval/epoch_eval_time': 4.109775066375732, 'eval/sps': 31145.256840754242}
I0727 05:18:04.409665 140120985872192 train.py:379] starting iteration 424 4248.735318899155
I0727 05:18:14.362463 140120985872192 train.py:394] {'eval/walltime': 1760.6991198062897, 'training/sps': 42262.46762702309, 'training/walltime': 2489.4153397083282, 'training/entropy_loss': Array(-0.02682966, dtype=float32), 'training/policy_loss': Array(0.00097669, dtype=float32), 'training/total_loss': Array(20.77311, dtype=float32), 'training/v_loss': Array(20.798962, dtype=float32), 'eval/episode_goal_distance': (Array(0.29978165, dtype=float32), Array(0.06270567, dtype=float32)), 'eval/episode_reward': (Array(-14614.654, dtype=float32), Array(6426.038, dtype=float32)), 'eval/avg_episode_length': (Array(875.71094, dtype=float32), Array(328.8384, dtype=float32)), 'eval/epoch_eval_time': 4.134015321731567, 'eval/sps': 30962.633187916224}
I0727 05:18:14.364819 140120985872192 train.py:379] starting iteration 425 4258.6904733181
I0727 05:18:24.346754 140120985872192 train.py:394] {'eval/walltime': 1764.8095824718475, 'training/sps': 41883.997470362825, 'training/walltime': 2495.2829744815826, 'training/entropy_loss': Array(-0.02691243, dtype=float32), 'training/policy_loss': Array(0.00131432, dtype=float32), 'training/total_loss': Array(72.99687, dtype=float32), 'training/v_loss': Array(73.02246, dtype=float32), 'eval/episode_goal_distance': (Array(0.31438673, dtype=float32), Array(0.06669797, dtype=float32)), 'eval/episode_reward': (Array(-15185.166, dtype=float32), Array(6927.5386, dtype=float32)), 'eval/avg_episode_length': (Array(867.96094, dtype=float32), Array(337.3962, dtype=float32)), 'eval/epoch_eval_time': 4.110462665557861, 'eval/sps': 31140.046854708064}
I0727 05:18:24.349267 140120985872192 train.py:379] starting iteration 426 4268.674920797348
I0727 05:18:34.242116 140120985872192 train.py:394] {'eval/walltime': 1768.907748222351, 'training/sps': 42438.49946566525, 'training/walltime': 2501.0739426612854, 'training/entropy_loss': Array(-0.02701331, dtype=float32), 'training/policy_loss': Array(0.00069886, dtype=float32), 'training/total_loss': Array(21.607037, dtype=float32), 'training/v_loss': Array(21.633352, dtype=float32), 'eval/episode_goal_distance': (Array(0.3033812, dtype=float32), Array(0.05709062, dtype=float32)), 'eval/episode_reward': (Array(-15466.869, dtype=float32), Array(5762.4785, dtype=float32)), 'eval/avg_episode_length': (Array(906.85156, dtype=float32), Array(289.61066, dtype=float32)), 'eval/epoch_eval_time': 4.09816575050354, 'eval/sps': 31233.485367026136}
I0727 05:18:34.244589 140120985872192 train.py:379] starting iteration 427 4278.5702431201935
I0727 05:18:44.170833 140120985872192 train.py:394] {'eval/walltime': 1773.0004649162292, 'training/sps': 42155.40712606023, 'training/walltime': 2506.9037997722626, 'training/entropy_loss': Array(-0.02768002, dtype=float32), 'training/policy_loss': Array(0.00020477, dtype=float32), 'training/total_loss': Array(24.61614, dtype=float32), 'training/v_loss': Array(24.643616, dtype=float32), 'eval/episode_goal_distance': (Array(0.3065152, dtype=float32), Array(0.05912458, dtype=float32)), 'eval/episode_reward': (Array(-15734.113, dtype=float32), Array(5244.87, dtype=float32)), 'eval/avg_episode_length': (Array(930.02344, dtype=float32), Array(254.45172, dtype=float32)), 'eval/epoch_eval_time': 4.092716693878174, 'eval/sps': 31275.069733377964}
I0727 05:18:44.173228 140120985872192 train.py:379] starting iteration 428 4288.498881816864
I0727 05:18:54.118464 140120985872192 train.py:394] {'eval/walltime': 1777.1367542743683, 'training/sps': 42334.6753669988, 'training/walltime': 2512.7089700698853, 'training/entropy_loss': Array(-0.0285694, dtype=float32), 'training/policy_loss': Array(0.00014365, dtype=float32), 'training/total_loss': Array(26.699131, dtype=float32), 'training/v_loss': Array(26.727558, dtype=float32), 'eval/episode_goal_distance': (Array(0.30511373, dtype=float32), Array(0.06305404, dtype=float32)), 'eval/episode_reward': (Array(-14649.697, dtype=float32), Array(7170.93, dtype=float32)), 'eval/avg_episode_length': (Array(844.75, dtype=float32), Array(360.7697, dtype=float32)), 'eval/epoch_eval_time': 4.136289358139038, 'eval/sps': 30945.61064692742}
I0727 05:18:54.120822 140120985872192 train.py:379] starting iteration 429 4298.446475982666
I0727 05:19:04.045146 140120985872192 train.py:394] {'eval/walltime': 1781.2355818748474, 'training/sps': 42213.16578548538, 'training/walltime': 2518.5308504104614, 'training/entropy_loss': Array(-0.02972707, dtype=float32), 'training/policy_loss': Array(0.00056047, dtype=float32), 'training/total_loss': Array(17.980255, dtype=float32), 'training/v_loss': Array(18.009422, dtype=float32), 'eval/episode_goal_distance': (Array(0.3102023, dtype=float32), Array(0.05552378, dtype=float32)), 'eval/episode_reward': (Array(-16051.287, dtype=float32), Array(5837.6978, dtype=float32)), 'eval/avg_episode_length': (Array(914.5781, dtype=float32), Array(278.5909, dtype=float32)), 'eval/epoch_eval_time': 4.098827600479126, 'eval/sps': 31228.44200254669}
I0727 05:19:04.047512 140120985872192 train.py:379] starting iteration 430 4308.373166799545
I0727 05:19:13.977171 140120985872192 train.py:394] {'eval/walltime': 1785.3486440181732, 'training/sps': 42279.545684200886, 'training/walltime': 2524.343590259552, 'training/entropy_loss': Array(-0.02949747, dtype=float32), 'training/policy_loss': Array(0.0005284, dtype=float32), 'training/total_loss': Array(18.720001, dtype=float32), 'training/v_loss': Array(18.74897, dtype=float32), 'eval/episode_goal_distance': (Array(0.3054306, dtype=float32), Array(0.06429155, dtype=float32)), 'eval/episode_reward': (Array(-15955.682, dtype=float32), Array(5738.394, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.75873, dtype=float32)), 'eval/epoch_eval_time': 4.113062143325806, 'eval/sps': 31120.36617479835}
I0727 05:19:13.979679 140120985872192 train.py:379] starting iteration 431 4318.305331945419
I0727 05:19:23.918006 140120985872192 train.py:394] {'eval/walltime': 1789.4447016716003, 'training/sps': 42093.05817951773, 'training/walltime': 2530.1820826530457, 'training/entropy_loss': Array(-0.02796239, dtype=float32), 'training/policy_loss': Array(0.00094472, dtype=float32), 'training/total_loss': Array(21.490227, dtype=float32), 'training/v_loss': Array(21.517244, dtype=float32), 'eval/episode_goal_distance': (Array(0.30512536, dtype=float32), Array(0.05863757, dtype=float32)), 'eval/episode_reward': (Array(-14694.068, dtype=float32), Array(6867.489, dtype=float32)), 'eval/avg_episode_length': (Array(852.46094, dtype=float32), Array(353.38202, dtype=float32)), 'eval/epoch_eval_time': 4.096057653427124, 'eval/sps': 31249.560145449585}
I0727 05:19:23.920377 140120985872192 train.py:379] starting iteration 432 4328.246031045914
I0727 05:19:33.868224 140120985872192 train.py:394] {'eval/walltime': 1793.546421289444, 'training/sps': 42064.43910777169, 'training/walltime': 2536.0245473384857, 'training/entropy_loss': Array(-0.02863717, dtype=float32), 'training/policy_loss': Array(0.00085387, dtype=float32), 'training/total_loss': Array(20.5057, dtype=float32), 'training/v_loss': Array(20.533482, dtype=float32), 'eval/episode_goal_distance': (Array(0.31214872, dtype=float32), Array(0.05310491, dtype=float32)), 'eval/episode_reward': (Array(-15370.42, dtype=float32), Array(5932.1646, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.3065, dtype=float32)), 'eval/epoch_eval_time': 4.101719617843628, 'eval/sps': 31206.42362855915}
I0727 05:19:33.870645 140120985872192 train.py:379] starting iteration 433 4338.196298837662
I0727 05:19:43.815000 140120985872192 train.py:394] {'eval/walltime': 1797.651715517044, 'training/sps': 42116.264043202755, 'training/walltime': 2541.8598227500916, 'training/entropy_loss': Array(-0.02910044, dtype=float32), 'training/policy_loss': Array(0.00061456, dtype=float32), 'training/total_loss': Array(57.78305, dtype=float32), 'training/v_loss': Array(57.81154, dtype=float32), 'eval/episode_goal_distance': (Array(0.3149854, dtype=float32), Array(0.06382173, dtype=float32)), 'eval/episode_reward': (Array(-15242.84, dtype=float32), Array(6550.9766, dtype=float32)), 'eval/avg_episode_length': (Array(883.46094, dtype=float32), Array(319.8641, dtype=float32)), 'eval/epoch_eval_time': 4.105294227600098, 'eval/sps': 31179.25120675873}
I0727 05:19:43.817355 140120985872192 train.py:379] starting iteration 434 4348.143009185791
I0727 05:19:53.769501 140120985872192 train.py:394] {'eval/walltime': 1801.7567386627197, 'training/sps': 42058.17802732342, 'training/walltime': 2547.703157186508, 'training/entropy_loss': Array(-0.02942818, dtype=float32), 'training/policy_loss': Array(0.00090339, dtype=float32), 'training/total_loss': Array(21.9236, dtype=float32), 'training/v_loss': Array(21.952126, dtype=float32), 'eval/episode_goal_distance': (Array(0.30159754, dtype=float32), Array(0.06296306, dtype=float32)), 'eval/episode_reward': (Array(-15221.4795, dtype=float32), Array(5991.1978, dtype=float32)), 'eval/avg_episode_length': (Array(899.03906, dtype=float32), Array(300.2835, dtype=float32)), 'eval/epoch_eval_time': 4.105023145675659, 'eval/sps': 31181.31017966089}
I0727 05:19:53.771899 140120985872192 train.py:379] starting iteration 435 4358.097553014755
I0727 05:20:03.744060 140120985872192 train.py:394] {'eval/walltime': 1805.8882524967194, 'training/sps': 42104.49366938922, 'training/walltime': 2553.540063858032, 'training/entropy_loss': Array(-0.02749576, dtype=float32), 'training/policy_loss': Array(0.00041696, dtype=float32), 'training/total_loss': Array(23.071587, dtype=float32), 'training/v_loss': Array(23.098665, dtype=float32), 'eval/episode_goal_distance': (Array(0.30838567, dtype=float32), Array(0.06416862, dtype=float32)), 'eval/episode_reward': (Array(-15569.832, dtype=float32), Array(5768.062, dtype=float32)), 'eval/avg_episode_length': (Array(906.75, dtype=float32), Array(289.92636, dtype=float32)), 'eval/epoch_eval_time': 4.131513833999634, 'eval/sps': 30981.379983928513}
I0727 05:20:03.746448 140120985872192 train.py:379] starting iteration 436 4368.072101831436
I0727 05:20:13.682692 140120985872192 train.py:394] {'eval/walltime': 1809.9879534244537, 'training/sps': 42133.79244747246, 'training/walltime': 2559.3729116916656, 'training/entropy_loss': Array(-0.02876883, dtype=float32), 'training/policy_loss': Array(0.0004306, dtype=float32), 'training/total_loss': Array(23.500893, dtype=float32), 'training/v_loss': Array(23.52923, dtype=float32), 'eval/episode_goal_distance': (Array(0.30710125, dtype=float32), Array(0.05975029, dtype=float32)), 'eval/episode_reward': (Array(-15085.082, dtype=float32), Array(5741.0894, dtype=float32)), 'eval/avg_episode_length': (Array(906.8047, dtype=float32), Array(289.75656, dtype=float32)), 'eval/epoch_eval_time': 4.099700927734375, 'eval/sps': 31221.78965155316}
I0727 05:20:13.685063 140120985872192 train.py:379] starting iteration 437 4378.010717391968
I0727 05:20:23.613764 140120985872192 train.py:394] {'eval/walltime': 1814.0821089744568, 'training/sps': 42149.0637868458, 'training/walltime': 2565.203646183014, 'training/entropy_loss': Array(-0.03016918, dtype=float32), 'training/policy_loss': Array(0.00054796, dtype=float32), 'training/total_loss': Array(22.77364, dtype=float32), 'training/v_loss': Array(22.80326, dtype=float32), 'eval/episode_goal_distance': (Array(0.2938183, dtype=float32), Array(0.06390846, dtype=float32)), 'eval/episode_reward': (Array(-15451.16, dtype=float32), Array(5198.917, dtype=float32)), 'eval/avg_episode_length': (Array(930.0781, dtype=float32), Array(254.25294, dtype=float32)), 'eval/epoch_eval_time': 4.094155550003052, 'eval/sps': 31264.07837628558}
I0727 05:20:23.616225 140120985872192 train.py:379] starting iteration 438 4387.941878795624
I0727 05:20:33.582711 140120985872192 train.py:394] {'eval/walltime': 1818.1915459632874, 'training/sps': 41986.222118744845, 'training/walltime': 2571.0569949150085, 'training/entropy_loss': Array(-0.03120947, dtype=float32), 'training/policy_loss': Array(0.00059817, dtype=float32), 'training/total_loss': Array(17.48875, dtype=float32), 'training/v_loss': Array(17.519361, dtype=float32), 'eval/episode_goal_distance': (Array(0.31020737, dtype=float32), Array(0.05816423, dtype=float32)), 'eval/episode_reward': (Array(-15798.953, dtype=float32), Array(5155.89, dtype=float32)), 'eval/avg_episode_length': (Array(937.8672, dtype=float32), Array(240.63965, dtype=float32)), 'eval/epoch_eval_time': 4.109436988830566, 'eval/sps': 31147.819116804443}
I0727 05:20:33.585066 140120985872192 train.py:379] starting iteration 439 4397.910719633102
I0727 05:20:43.455734 140120985872192 train.py:394] {'eval/walltime': 1822.2861788272858, 'training/sps': 42576.03806792697, 'training/walltime': 2576.8292558193207, 'training/entropy_loss': Array(-0.03182022, dtype=float32), 'training/policy_loss': Array(0.00032133, dtype=float32), 'training/total_loss': Array(17.410252, dtype=float32), 'training/v_loss': Array(17.441753, dtype=float32), 'eval/episode_goal_distance': (Array(0.30031338, dtype=float32), Array(0.05968198, dtype=float32)), 'eval/episode_reward': (Array(-15504.465, dtype=float32), Array(5024.1562, dtype=float32)), 'eval/avg_episode_length': (Array(937.85156, dtype=float32), Array(240.70004, dtype=float32)), 'eval/epoch_eval_time': 4.094632863998413, 'eval/sps': 31260.43390249349}
I0727 05:20:43.458220 140120985872192 train.py:379] starting iteration 440 4407.783873558044
I0727 05:20:53.444760 140120985872192 train.py:394] {'eval/walltime': 1826.4053583145142, 'training/sps': 41913.49282945677, 'training/walltime': 2582.6927614212036, 'training/entropy_loss': Array(-0.03176268, dtype=float32), 'training/policy_loss': Array(0.00053309, dtype=float32), 'training/total_loss': Array(24.168484, dtype=float32), 'training/v_loss': Array(24.199713, dtype=float32), 'eval/episode_goal_distance': (Array(0.30724305, dtype=float32), Array(0.05912471, dtype=float32)), 'eval/episode_reward': (Array(-15620.461, dtype=float32), Array(5475.5063, dtype=float32)), 'eval/avg_episode_length': (Array(922.2656, dtype=float32), Array(267.02658, dtype=float32)), 'eval/epoch_eval_time': 4.1191794872283936, 'eval/sps': 31074.14969337141}
I0727 05:20:53.447374 140120985872192 train.py:379] starting iteration 441 4417.773028612137
I0727 05:21:03.362590 140120985872192 train.py:394] {'eval/walltime': 1830.5063219070435, 'training/sps': 42296.4813491121, 'training/walltime': 2588.503173828125, 'training/entropy_loss': Array(-0.03093567, dtype=float32), 'training/policy_loss': Array(0.00078063, dtype=float32), 'training/total_loss': Array(54.177177, dtype=float32), 'training/v_loss': Array(54.20733, dtype=float32), 'eval/episode_goal_distance': (Array(0.29492348, dtype=float32), Array(0.06342347, dtype=float32)), 'eval/episode_reward': (Array(-14989.096, dtype=float32), Array(5844.7812, dtype=float32)), 'eval/avg_episode_length': (Array(906.83594, dtype=float32), Array(289.65942, dtype=float32)), 'eval/epoch_eval_time': 4.100963592529297, 'eval/sps': 31212.1766292139}
I0727 05:21:03.365108 140120985872192 train.py:379] starting iteration 442 4427.690762042999
I0727 05:21:13.306755 140120985872192 train.py:394] {'eval/walltime': 1834.611834526062, 'training/sps': 42136.53440880148, 'training/walltime': 2594.3356420993805, 'training/entropy_loss': Array(-0.03023233, dtype=float32), 'training/policy_loss': Array(8.460661e-05, dtype=float32), 'training/total_loss': Array(22.047834, dtype=float32), 'training/v_loss': Array(22.077982, dtype=float32), 'eval/episode_goal_distance': (Array(0.30508843, dtype=float32), Array(0.06172539, dtype=float32)), 'eval/episode_reward': (Array(-15950.383, dtype=float32), Array(5420.778, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.08284, dtype=float32)), 'eval/epoch_eval_time': 4.105512619018555, 'eval/sps': 31177.592636555848}
I0727 05:21:13.309171 140120985872192 train.py:379] starting iteration 443 4437.634825468063
I0727 05:21:23.231703 140120985872192 train.py:394] {'eval/walltime': 1838.7151737213135, 'training/sps': 42259.686729033114, 'training/walltime': 2600.151113510132, 'training/entropy_loss': Array(-0.03056266, dtype=float32), 'training/policy_loss': Array(0.00035502, dtype=float32), 'training/total_loss': Array(18.155544, dtype=float32), 'training/v_loss': Array(18.185753, dtype=float32), 'eval/episode_goal_distance': (Array(0.3048461, dtype=float32), Array(0.05622724, dtype=float32)), 'eval/episode_reward': (Array(-15626.906, dtype=float32), Array(5146.8867, dtype=float32)), 'eval/avg_episode_length': (Array(937.8125, dtype=float32), Array(240.85152, dtype=float32)), 'eval/epoch_eval_time': 4.103339195251465, 'eval/sps': 31194.106533558403}
I0727 05:21:23.234187 140120985872192 train.py:379] starting iteration 444 4447.559841394424
I0727 05:21:33.159799 140120985872192 train.py:394] {'eval/walltime': 1842.8157873153687, 'training/sps': 42216.739352939585, 'training/walltime': 2605.972501039505, 'training/entropy_loss': Array(-0.0310448, dtype=float32), 'training/policy_loss': Array(0.00080538, dtype=float32), 'training/total_loss': Array(21.300293, dtype=float32), 'training/v_loss': Array(21.330536, dtype=float32), 'eval/episode_goal_distance': (Array(0.30147856, dtype=float32), Array(0.06035277, dtype=float32)), 'eval/episode_reward': (Array(-16016.057, dtype=float32), Array(4520.355, dtype=float32)), 'eval/avg_episode_length': (Array(961.1797, dtype=float32), Array(192.54279, dtype=float32)), 'eval/epoch_eval_time': 4.100613594055176, 'eval/sps': 31214.840673007264}
I0727 05:21:33.162156 140120985872192 train.py:379] starting iteration 445 4457.487810373306
I0727 05:21:43.083044 140120985872192 train.py:394] {'eval/walltime': 1846.9063029289246, 'training/sps': 42178.4885059061, 'training/walltime': 2611.799167871475, 'training/entropy_loss': Array(-0.03280158, dtype=float32), 'training/policy_loss': Array(0.00033861, dtype=float32), 'training/total_loss': Array(25.047237, dtype=float32), 'training/v_loss': Array(25.0797, dtype=float32), 'eval/episode_goal_distance': (Array(0.29866034, dtype=float32), Array(0.06276111, dtype=float32)), 'eval/episode_reward': (Array(-14669.687, dtype=float32), Array(5829.21, dtype=float32)), 'eval/avg_episode_length': (Array(891.2578, dtype=float32), Array(310.30396, dtype=float32)), 'eval/epoch_eval_time': 4.090515613555908, 'eval/sps': 31291.898648622657}
I0727 05:21:43.085409 140120985872192 train.py:379] starting iteration 446 4467.4110634326935
I0727 05:21:53.009394 140120985872192 train.py:394] {'eval/walltime': 1851.00337433815, 'training/sps': 42203.47853151903, 'training/walltime': 2617.6223845481873, 'training/entropy_loss': Array(-0.03261962, dtype=float32), 'training/policy_loss': Array(7.050351e-05, dtype=float32), 'training/total_loss': Array(15.563883, dtype=float32), 'training/v_loss': Array(15.596432, dtype=float32), 'eval/episode_goal_distance': (Array(0.2977978, dtype=float32), Array(0.06020898, dtype=float32)), 'eval/episode_reward': (Array(-15264.107, dtype=float32), Array(5361.095, dtype=float32)), 'eval/avg_episode_length': (Array(914.6406, dtype=float32), Array(278.38666, dtype=float32)), 'eval/epoch_eval_time': 4.097071409225464, 'eval/sps': 31241.82793391876}
I0727 05:21:53.012229 140120985872192 train.py:379] starting iteration 447 4477.337883472443
I0727 05:22:02.936332 140120985872192 train.py:394] {'eval/walltime': 1855.103410243988, 'training/sps': 42223.79144597966, 'training/walltime': 2623.442799806595, 'training/entropy_loss': Array(-0.03426188, dtype=float32), 'training/policy_loss': Array(0.00026736, dtype=float32), 'training/total_loss': Array(17.402605, dtype=float32), 'training/v_loss': Array(17.4366, dtype=float32), 'eval/episode_goal_distance': (Array(0.2953214, dtype=float32), Array(0.05590588, dtype=float32)), 'eval/episode_reward': (Array(-14687.3545, dtype=float32), Array(5789.863, dtype=float32)), 'eval/avg_episode_length': (Array(898.96875, dtype=float32), Array(300.49222, dtype=float32)), 'eval/epoch_eval_time': 4.100035905838013, 'eval/sps': 31219.238791968062}
I0727 05:22:02.938699 140120985872192 train.py:379] starting iteration 448 4487.264353752136
I0727 05:22:12.866903 140120985872192 train.py:394] {'eval/walltime': 1859.2078800201416, 'training/sps': 42226.135171525115, 'training/walltime': 2629.2628920078278, 'training/entropy_loss': Array(-0.03554102, dtype=float32), 'training/policy_loss': Array(0.00027323, dtype=float32), 'training/total_loss': Array(20.066036, dtype=float32), 'training/v_loss': Array(20.101307, dtype=float32), 'eval/episode_goal_distance': (Array(0.3105077, dtype=float32), Array(0.05745775, dtype=float32)), 'eval/episode_reward': (Array(-15378.898, dtype=float32), Array(5685.8887, dtype=float32)), 'eval/avg_episode_length': (Array(906.8125, dtype=float32), Array(289.73224, dtype=float32)), 'eval/epoch_eval_time': 4.1044697761535645, 'eval/sps': 31185.514081176418}
I0727 05:22:12.869242 140120985872192 train.py:379] starting iteration 449 4497.1948964595795
I0727 05:22:22.797283 140120985872192 train.py:394] {'eval/walltime': 1863.3023264408112, 'training/sps': 42154.941653116904, 'training/walltime': 2635.0928134918213, 'training/entropy_loss': Array(-0.03689427, dtype=float32), 'training/policy_loss': Array(0.00012518, dtype=float32), 'training/total_loss': Array(17.583696, dtype=float32), 'training/v_loss': Array(17.620466, dtype=float32), 'eval/episode_goal_distance': (Array(0.29763067, dtype=float32), Array(0.05955551, dtype=float32)), 'eval/episode_reward': (Array(-14965.256, dtype=float32), Array(5750.01, dtype=float32)), 'eval/avg_episode_length': (Array(899.0781, dtype=float32), Array(300.16785, dtype=float32)), 'eval/epoch_eval_time': 4.094446420669556, 'eval/sps': 31261.857367049986}
I0727 05:22:22.799628 140120985872192 train.py:379] starting iteration 450 4507.125282764435
I0727 05:22:32.797723 140120985872192 train.py:394] {'eval/walltime': 1867.4249622821808, 'training/sps': 41854.48921197199, 'training/walltime': 2640.9645850658417, 'training/entropy_loss': Array(-0.0376894, dtype=float32), 'training/policy_loss': Array(0.00042509, dtype=float32), 'training/total_loss': Array(58.913414, dtype=float32), 'training/v_loss': Array(58.950676, dtype=float32), 'eval/episode_goal_distance': (Array(0.29764137, dtype=float32), Array(0.05542745, dtype=float32)), 'eval/episode_reward': (Array(-14875.174, dtype=float32), Array(5666.3647, dtype=float32)), 'eval/avg_episode_length': (Array(906.85156, dtype=float32), Array(289.6109, dtype=float32)), 'eval/epoch_eval_time': 4.122635841369629, 'eval/sps': 31048.097606766943}
I0727 05:22:32.800097 140120985872192 train.py:379] starting iteration 451 4517.125750780106
I0727 05:22:42.712229 140120985872192 train.py:394] {'eval/walltime': 1871.5247025489807, 'training/sps': 42308.83687368807, 'training/walltime': 2646.7733006477356, 'training/entropy_loss': Array(-0.03735477, dtype=float32), 'training/policy_loss': Array(7.688635e-05, dtype=float32), 'training/total_loss': Array(16.42543, dtype=float32), 'training/v_loss': Array(16.462708, dtype=float32), 'eval/episode_goal_distance': (Array(0.29907465, dtype=float32), Array(0.06345694, dtype=float32)), 'eval/episode_reward': (Array(-15228.118, dtype=float32), Array(5392.98, dtype=float32)), 'eval/avg_episode_length': (Array(922.2578, dtype=float32), Array(267.05362, dtype=float32)), 'eval/epoch_eval_time': 4.099740266799927, 'eval/sps': 31221.490062810994}
I0727 05:22:42.714632 140120985872192 train.py:379] starting iteration 452 4527.040285825729
I0727 05:22:52.665237 140120985872192 train.py:394] {'eval/walltime': 1875.6394262313843, 'training/sps': 42138.930472442815, 'training/walltime': 2652.6054372787476, 'training/entropy_loss': Array(-0.03609858, dtype=float32), 'training/policy_loss': Array(3.348798e-05, dtype=float32), 'training/total_loss': Array(21.713295, dtype=float32), 'training/v_loss': Array(21.74936, dtype=float32), 'eval/episode_goal_distance': (Array(0.30862412, dtype=float32), Array(0.06505742, dtype=float32)), 'eval/episode_reward': (Array(-15052.241, dtype=float32), Array(6086.6367, dtype=float32)), 'eval/avg_episode_length': (Array(891.27344, dtype=float32), Array(310.2593, dtype=float32)), 'eval/epoch_eval_time': 4.1147236824035645, 'eval/sps': 31107.799667663323}
I0727 05:22:52.667774 140120985872192 train.py:379] starting iteration 453 4536.9934277534485
I0727 05:23:02.642761 140120985872192 train.py:394] {'eval/walltime': 1879.7451453208923, 'training/sps': 41899.713282322446, 'training/walltime': 2658.4708712100983, 'training/entropy_loss': Array(-0.0359248, dtype=float32), 'training/policy_loss': Array(0.00029502, dtype=float32), 'training/total_loss': Array(22.796516, dtype=float32), 'training/v_loss': Array(22.832146, dtype=float32), 'eval/episode_goal_distance': (Array(0.310881, dtype=float32), Array(0.06149799, dtype=float32)), 'eval/episode_reward': (Array(-14687.765, dtype=float32), Array(6566.529, dtype=float32)), 'eval/avg_episode_length': (Array(860.16406, dtype=float32), Array(345.6841, dtype=float32)), 'eval/epoch_eval_time': 4.105719089508057, 'eval/sps': 31176.02476192224}
I0727 05:23:02.645345 140120985872192 train.py:379] starting iteration 454 4546.970999479294
I0727 05:23:12.573544 140120985872192 train.py:394] {'eval/walltime': 1883.84654378891, 'training/sps': 42204.96114497674, 'training/walltime': 2664.2938833236694, 'training/entropy_loss': Array(-0.03473163, dtype=float32), 'training/policy_loss': Array(-0.00020378, dtype=float32), 'training/total_loss': Array(16.541958, dtype=float32), 'training/v_loss': Array(16.576893, dtype=float32), 'eval/episode_goal_distance': (Array(0.29445451, dtype=float32), Array(0.0669365, dtype=float32)), 'eval/episode_reward': (Array(-15030.26, dtype=float32), Array(4990.2456, dtype=float32)), 'eval/avg_episode_length': (Array(930.1094, dtype=float32), Array(254.13936, dtype=float32)), 'eval/epoch_eval_time': 4.101398468017578, 'eval/sps': 31208.86717009702}
I0727 05:23:12.575962 140120985872192 train.py:379] starting iteration 455 4556.901615858078
I0727 05:23:22.525373 140120985872192 train.py:394] {'eval/walltime': 1887.9600689411163, 'training/sps': 42138.27587732125, 'training/walltime': 2670.1261105537415, 'training/entropy_loss': Array(-0.02989351, dtype=float32), 'training/policy_loss': Array(0.00041817, dtype=float32), 'training/total_loss': Array(16.541601, dtype=float32), 'training/v_loss': Array(16.571075, dtype=float32), 'eval/episode_goal_distance': (Array(0.29381526, dtype=float32), Array(0.05995023, dtype=float32)), 'eval/episode_reward': (Array(-14564.924, dtype=float32), Array(5764.605, dtype=float32)), 'eval/avg_episode_length': (Array(899.125, dtype=float32), Array(300.02798, dtype=float32)), 'eval/epoch_eval_time': 4.113525152206421, 'eval/sps': 31116.863338332354}
I0727 05:23:22.527776 140120985872192 train.py:379] starting iteration 456 4566.8534297943115
I0727 05:23:32.437743 140120985872192 train.py:394] {'eval/walltime': 1892.0579307079315, 'training/sps': 42311.06153055397, 'training/walltime': 2675.9345207214355, 'training/entropy_loss': Array(-0.0304124, dtype=float32), 'training/policy_loss': Array(0.00073162, dtype=float32), 'training/total_loss': Array(15.214155, dtype=float32), 'training/v_loss': Array(15.243835, dtype=float32), 'eval/episode_goal_distance': (Array(0.29448932, dtype=float32), Array(0.05625711, dtype=float32)), 'eval/episode_reward': (Array(-14949.289, dtype=float32), Array(4813.4917, dtype=float32)), 'eval/avg_episode_length': (Array(937.8281, dtype=float32), Array(240.79094, dtype=float32)), 'eval/epoch_eval_time': 4.0978617668151855, 'eval/sps': 31235.802299763818}
I0727 05:23:32.440102 140120985872192 train.py:379] starting iteration 457 4576.7657561302185
I0727 05:23:42.366225 140120985872192 train.py:394] {'eval/walltime': 1896.1629824638367, 'training/sps': 42246.01627197133, 'training/walltime': 2681.7518739700317, 'training/entropy_loss': Array(-0.03292676, dtype=float32), 'training/policy_loss': Array(0.00068135, dtype=float32), 'training/total_loss': Array(17.158684, dtype=float32), 'training/v_loss': Array(17.19093, dtype=float32), 'eval/episode_goal_distance': (Array(0.29902124, dtype=float32), Array(0.06304695, dtype=float32)), 'eval/episode_reward': (Array(-15033.212, dtype=float32), Array(5176.3076, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.7584, dtype=float32)), 'eval/epoch_eval_time': 4.105051755905151, 'eval/sps': 31181.092860978166}
I0727 05:23:42.368577 140120985872192 train.py:379] starting iteration 458 4586.694230794907
I0727 05:23:52.298737 140120985872192 train.py:394] {'eval/walltime': 1900.268131494522, 'training/sps': 42217.57794092639, 'training/walltime': 2687.573145866394, 'training/entropy_loss': Array(-0.03469197, dtype=float32), 'training/policy_loss': Array(0.00050773, dtype=float32), 'training/total_loss': Array(59.23752, dtype=float32), 'training/v_loss': Array(59.271706, dtype=float32), 'eval/episode_goal_distance': (Array(0.2961327, dtype=float32), Array(0.05646075, dtype=float32)), 'eval/episode_reward': (Array(-14633.875, dtype=float32), Array(5654.9077, dtype=float32)), 'eval/avg_episode_length': (Array(899.125, dtype=float32), Array(300.0278, dtype=float32)), 'eval/epoch_eval_time': 4.105149030685425, 'eval/sps': 31180.35400011488}
I0727 05:23:52.301154 140120985872192 train.py:379] starting iteration 459 4596.6268084049225
I0727 05:24:02.285566 140120985872192 train.py:394] {'eval/walltime': 1904.3871684074402, 'training/sps': 41926.12692942796, 'training/walltime': 2693.4348845481873, 'training/entropy_loss': Array(-0.03423363, dtype=float32), 'training/policy_loss': Array(0.00028332, dtype=float32), 'training/total_loss': Array(15.77339, dtype=float32), 'training/v_loss': Array(15.807341, dtype=float32), 'eval/episode_goal_distance': (Array(0.30107868, dtype=float32), Array(0.0539507, dtype=float32)), 'eval/episode_reward': (Array(-14611.895, dtype=float32), Array(5940.293, dtype=float32)), 'eval/avg_episode_length': (Array(883.52344, dtype=float32), Array(319.69287, dtype=float32)), 'eval/epoch_eval_time': 4.119036912918091, 'eval/sps': 31075.225278648857}
I0727 05:24:02.482575 140120985872192 train.py:379] starting iteration 460 4606.808223724365
I0727 05:24:12.416871 140120985872192 train.py:394] {'eval/walltime': 1908.49360871315, 'training/sps': 42197.042984901054, 'training/walltime': 2699.2589893341064, 'training/entropy_loss': Array(-0.03474927, dtype=float32), 'training/policy_loss': Array(0.00067999, dtype=float32), 'training/total_loss': Array(19.280146, dtype=float32), 'training/v_loss': Array(19.314213, dtype=float32), 'eval/episode_goal_distance': (Array(0.29930213, dtype=float32), Array(0.06327354, dtype=float32)), 'eval/episode_reward': (Array(-14468.855, dtype=float32), Array(6014.966, dtype=float32)), 'eval/avg_episode_length': (Array(883.5781, dtype=float32), Array(319.54276, dtype=float32)), 'eval/epoch_eval_time': 4.106440305709839, 'eval/sps': 31170.549300819297}
I0727 05:24:12.419404 140120985872192 train.py:379] starting iteration 461 4616.7450585365295
I0727 05:24:22.405374 140120985872192 train.py:394] {'eval/walltime': 1912.6311249732971, 'training/sps': 42047.700723823145, 'training/walltime': 2705.1037797927856, 'training/entropy_loss': Array(-0.0337781, dtype=float32), 'training/policy_loss': Array(0.00084528, dtype=float32), 'training/total_loss': Array(19.854694, dtype=float32), 'training/v_loss': Array(19.887627, dtype=float32), 'eval/episode_goal_distance': (Array(0.29586512, dtype=float32), Array(0.06358755, dtype=float32)), 'eval/episode_reward': (Array(-14618.736, dtype=float32), Array(5055.519, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.75827, dtype=float32)), 'eval/epoch_eval_time': 4.137516260147095, 'eval/sps': 30936.43431275589}
I0727 05:24:22.407709 140120985872192 train.py:379] starting iteration 462 4626.733364105225
I0727 05:24:32.354174 140120985872192 train.py:394] {'eval/walltime': 1916.7399616241455, 'training/sps': 42125.15037093136, 'training/walltime': 2710.9378242492676, 'training/entropy_loss': Array(-0.03090653, dtype=float32), 'training/policy_loss': Array(0.00075407, dtype=float32), 'training/total_loss': Array(18.360939, dtype=float32), 'training/v_loss': Array(18.39109, dtype=float32), 'eval/episode_goal_distance': (Array(0.29433572, dtype=float32), Array(0.0595639, dtype=float32)), 'eval/episode_reward': (Array(-14381.996, dtype=float32), Array(5099.4927, dtype=float32)), 'eval/avg_episode_length': (Array(914.53125, dtype=float32), Array(278.74332, dtype=float32)), 'eval/epoch_eval_time': 4.108836650848389, 'eval/sps': 31152.370093264886}
I0727 05:24:32.356807 140120985872192 train.py:379] starting iteration 463 4636.682460784912
I0727 05:24:42.261399 140120985872192 train.py:394] {'eval/walltime': 1920.828537940979, 'training/sps': 42281.89733788555, 'training/walltime': 2716.750240802765, 'training/entropy_loss': Array(-0.0286604, dtype=float32), 'training/policy_loss': Array(0.00105926, dtype=float32), 'training/total_loss': Array(11.8314705, dtype=float32), 'training/v_loss': Array(11.859073, dtype=float32), 'eval/episode_goal_distance': (Array(0.30039883, dtype=float32), Array(0.06049375, dtype=float32)), 'eval/episode_reward': (Array(-14452.298, dtype=float32), Array(5347.7495, dtype=float32)), 'eval/avg_episode_length': (Array(906.8672, dtype=float32), Array(289.56216, dtype=float32)), 'eval/epoch_eval_time': 4.088576316833496, 'eval/sps': 31306.74104651003}
I0727 05:24:42.263784 140120985872192 train.py:379] starting iteration 464 4646.5894384384155
I0727 05:24:52.225191 140120985872192 train.py:394] {'eval/walltime': 1924.9359815120697, 'training/sps': 42008.87373058512, 'training/walltime': 2722.6004333496094, 'training/entropy_loss': Array(-0.0294881, dtype=float32), 'training/policy_loss': Array(0.00081515, dtype=float32), 'training/total_loss': Array(15.57098, dtype=float32), 'training/v_loss': Array(15.599655, dtype=float32), 'eval/episode_goal_distance': (Array(0.29345438, dtype=float32), Array(0.06077823, dtype=float32)), 'eval/episode_reward': (Array(-14660.65, dtype=float32), Array(5094.2383, dtype=float32)), 'eval/avg_episode_length': (Array(922.3125, dtype=float32), Array(266.86588, dtype=float32)), 'eval/epoch_eval_time': 4.107443571090698, 'eval/sps': 31162.93572500879}
I0727 05:24:52.227786 140120985872192 train.py:379] starting iteration 465 4656.553440093994
I0727 05:25:02.158468 140120985872192 train.py:394] {'eval/walltime': 1929.0353014469147, 'training/sps': 42171.14270879814, 'training/walltime': 2728.428115129471, 'training/entropy_loss': Array(-0.0322837, dtype=float32), 'training/policy_loss': Array(0.00050661, dtype=float32), 'training/total_loss': Array(17.00763, dtype=float32), 'training/v_loss': Array(17.039406, dtype=float32), 'eval/episode_goal_distance': (Array(0.3050773, dtype=float32), Array(0.05310192, dtype=float32)), 'eval/episode_reward': (Array(-14894.758, dtype=float32), Array(5519.643, dtype=float32)), 'eval/avg_episode_length': (Array(906.8047, dtype=float32), Array(289.75653, dtype=float32)), 'eval/epoch_eval_time': 4.099319934844971, 'eval/sps': 31224.691420637006}
I0727 05:25:02.160832 140120985872192 train.py:379] starting iteration 466 4666.486485719681
I0727 05:25:12.143729 140120985872192 train.py:394] {'eval/walltime': 1933.1657438278198, 'training/sps': 42019.523534597705, 'training/walltime': 2734.276824951172, 'training/entropy_loss': Array(-0.03228367, dtype=float32), 'training/policy_loss': Array(0.00114855, dtype=float32), 'training/total_loss': Array(46.948357, dtype=float32), 'training/v_loss': Array(46.979492, dtype=float32), 'eval/episode_goal_distance': (Array(0.30432644, dtype=float32), Array(0.05863056, dtype=float32)), 'eval/episode_reward': (Array(-14868.611, dtype=float32), Array(5813.1045, dtype=float32)), 'eval/avg_episode_length': (Array(899., dtype=float32), Array(300.39963, dtype=float32)), 'eval/epoch_eval_time': 4.130442380905151, 'eval/sps': 30989.416676465025}
I0727 05:25:12.146237 140120985872192 train.py:379] starting iteration 467 4676.471890926361
I0727 05:25:22.069665 140120985872192 train.py:394] {'eval/walltime': 1937.2616715431213, 'training/sps': 42197.58539549608, 'training/walltime': 2740.100854873657, 'training/entropy_loss': Array(-0.03182127, dtype=float32), 'training/policy_loss': Array(8.987303e-05, dtype=float32), 'training/total_loss': Array(21.485165, dtype=float32), 'training/v_loss': Array(21.516895, dtype=float32), 'eval/episode_goal_distance': (Array(0.30051085, dtype=float32), Array(0.05462956, dtype=float32)), 'eval/episode_reward': (Array(-15316.129, dtype=float32), Array(4926.7, dtype=float32)), 'eval/avg_episode_length': (Array(930.1875, dtype=float32), Array(253.85515, dtype=float32)), 'eval/epoch_eval_time': 4.095927715301514, 'eval/sps': 31250.551498215962}
I0727 05:25:22.072392 140120985872192 train.py:379] starting iteration 468 4686.398046255112
I0727 05:25:32.033398 140120985872192 train.py:394] {'eval/walltime': 1941.3964552879333, 'training/sps': 42208.43135150518, 'training/walltime': 2745.9233882427216, 'training/entropy_loss': Array(-0.03157878, dtype=float32), 'training/policy_loss': Array(0.00057241, dtype=float32), 'training/total_loss': Array(12.9878845, dtype=float32), 'training/v_loss': Array(13.01889, dtype=float32), 'eval/episode_goal_distance': (Array(0.28989977, dtype=float32), Array(0.05459841, dtype=float32)), 'eval/episode_reward': (Array(-13841.158, dtype=float32), Array(5298.96, dtype=float32)), 'eval/avg_episode_length': (Array(891.21875, dtype=float32), Array(310.41525, dtype=float32)), 'eval/epoch_eval_time': 4.134783744812012, 'eval/sps': 30956.878980818266}
I0727 05:25:32.035955 140120985872192 train.py:379] starting iteration 469 4696.361609697342
I0727 05:25:42.007047 140120985872192 train.py:394] {'eval/walltime': 1945.5180015563965, 'training/sps': 42040.97309198172, 'training/walltime': 2751.7691140174866, 'training/entropy_loss': Array(-0.03009973, dtype=float32), 'training/policy_loss': Array(0.00081151, dtype=float32), 'training/total_loss': Array(16.95959, dtype=float32), 'training/v_loss': Array(16.988878, dtype=float32), 'eval/episode_goal_distance': (Array(0.29116666, dtype=float32), Array(0.06486864, dtype=float32)), 'eval/episode_reward': (Array(-14121.443, dtype=float32), Array(5828.0947, dtype=float32)), 'eval/avg_episode_length': (Array(883.4922, dtype=float32), Array(319.77878, dtype=float32)), 'eval/epoch_eval_time': 4.121546268463135, 'eval/sps': 31056.305488893457}
I0727 05:25:42.009477 140120985872192 train.py:379] starting iteration 470 4706.335131406784
I0727 05:25:51.924481 140120985872192 train.py:394] {'eval/walltime': 1949.6132910251617, 'training/sps': 42255.88935857998, 'training/walltime': 2757.5851080417633, 'training/entropy_loss': Array(-0.0325759, dtype=float32), 'training/policy_loss': Array(0.00077774, dtype=float32), 'training/total_loss': Array(22.686867, dtype=float32), 'training/v_loss': Array(22.718666, dtype=float32), 'eval/episode_goal_distance': (Array(0.2976812, dtype=float32), Array(0.05163309, dtype=float32)), 'eval/episode_reward': (Array(-14693.068, dtype=float32), Array(5338.029, dtype=float32)), 'eval/avg_episode_length': (Array(906.8672, dtype=float32), Array(289.56232, dtype=float32)), 'eval/epoch_eval_time': 4.095289468765259, 'eval/sps': 31255.42186364481}
I0727 05:25:51.927030 140120985872192 train.py:379] starting iteration 471 4716.252684354782
I0727 05:26:01.854492 140120985872192 train.py:394] {'eval/walltime': 1953.719882965088, 'training/sps': 42246.9997368764, 'training/walltime': 2763.4023258686066, 'training/entropy_loss': Array(-0.03267091, dtype=float32), 'training/policy_loss': Array(0.00057399, dtype=float32), 'training/total_loss': Array(14.651338, dtype=float32), 'training/v_loss': Array(14.683434, dtype=float32), 'eval/episode_goal_distance': (Array(0.2940176, dtype=float32), Array(0.06371194, dtype=float32)), 'eval/episode_reward': (Array(-14769.237, dtype=float32), Array(4996.7266, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.75848, dtype=float32)), 'eval/epoch_eval_time': 4.1065919399261475, 'eval/sps': 31169.398341122236}
I0727 05:26:01.856902 140120985872192 train.py:379] starting iteration 472 4726.182556629181
I0727 05:26:11.745875 140120985872192 train.py:394] {'eval/walltime': 1957.8052973747253, 'training/sps': 42372.058819672675, 'training/walltime': 2769.202374458313, 'training/entropy_loss': Array(-0.03095784, dtype=float32), 'training/policy_loss': Array(0.00035291, dtype=float32), 'training/total_loss': Array(13.676201, dtype=float32), 'training/v_loss': Array(13.706805, dtype=float32), 'eval/episode_goal_distance': (Array(0.3028835, dtype=float32), Array(0.06272968, dtype=float32)), 'eval/episode_reward': (Array(-14382.574, dtype=float32), Array(5624.5156, dtype=float32)), 'eval/avg_episode_length': (Array(891.25, dtype=float32), Array(310.32587, dtype=float32)), 'eval/epoch_eval_time': 4.085414409637451, 'eval/sps': 31330.97090421214}
I0727 05:26:11.748257 140120985872192 train.py:379] starting iteration 473 4736.0739114284515
I0727 05:26:21.695999 140120985872192 train.py:394] {'eval/walltime': 1961.9135093688965, 'training/sps': 42112.076050053285, 'training/walltime': 2775.0382301807404, 'training/entropy_loss': Array(-0.02672027, dtype=float32), 'training/policy_loss': Array(0.00115879, dtype=float32), 'training/total_loss': Array(17.201136, dtype=float32), 'training/v_loss': Array(17.226698, dtype=float32), 'eval/episode_goal_distance': (Array(0.28926432, dtype=float32), Array(0.06440128, dtype=float32)), 'eval/episode_reward': (Array(-13776.807, dtype=float32), Array(5845.727, dtype=float32)), 'eval/avg_episode_length': (Array(883.4375, dtype=float32), Array(319.9285, dtype=float32)), 'eval/epoch_eval_time': 4.108211994171143, 'eval/sps': 31157.106834216524}
I0727 05:26:21.698538 140120985872192 train.py:379] starting iteration 474 4746.024191856384
I0727 05:26:31.637006 140120985872192 train.py:394] {'eval/walltime': 1966.002368927002, 'training/sps': 42038.972195584014, 'training/walltime': 2780.884234189987, 'training/entropy_loss': Array(-0.02662498, dtype=float32), 'training/policy_loss': Array(0.00114085, dtype=float32), 'training/total_loss': Array(12.769721, dtype=float32), 'training/v_loss': Array(12.795206, dtype=float32), 'eval/episode_goal_distance': (Array(0.29385847, dtype=float32), Array(0.05700479, dtype=float32)), 'eval/episode_reward': (Array(-14800.528, dtype=float32), Array(4592.5664, dtype=float32)), 'eval/avg_episode_length': (Array(937.8672, dtype=float32), Array(240.63976, dtype=float32)), 'eval/epoch_eval_time': 4.088859558105469, 'eval/sps': 31304.572382844934}
I0727 05:26:31.639380 140120985872192 train.py:379] starting iteration 475 4755.965034008026
I0727 05:26:41.601971 140120985872192 train.py:394] {'eval/walltime': 1970.1242804527283, 'training/sps': 42103.6062552515, 'training/walltime': 2786.721263885498, 'training/entropy_loss': Array(-0.02641252, dtype=float32), 'training/policy_loss': Array(0.00266015, dtype=float32), 'training/total_loss': Array(55.542366, dtype=float32), 'training/v_loss': Array(55.566116, dtype=float32), 'eval/episode_goal_distance': (Array(0.29922375, dtype=float32), Array(0.05758932, dtype=float32)), 'eval/episode_reward': (Array(-13562.328, dtype=float32), Array(5801.513, dtype=float32)), 'eval/avg_episode_length': (Array(868.03906, dtype=float32), Array(337.19675, dtype=float32)), 'eval/epoch_eval_time': 4.121911525726318, 'eval/sps': 31053.553479036702}
I0727 05:26:41.604529 140120985872192 train.py:379] starting iteration 476 4765.930182695389
I0727 05:26:51.497409 140120985872192 train.py:394] {'eval/walltime': 1974.213907957077, 'training/sps': 42375.460747847894, 'training/walltime': 2792.5208468437195, 'training/entropy_loss': Array(-0.02529184, dtype=float32), 'training/policy_loss': Array(0.00148381, dtype=float32), 'training/total_loss': Array(12.097799, dtype=float32), 'training/v_loss': Array(12.121607, dtype=float32), 'eval/episode_goal_distance': (Array(0.292915, dtype=float32), Array(0.05676332, dtype=float32)), 'eval/episode_reward': (Array(-13570.255, dtype=float32), Array(5582.3076, dtype=float32)), 'eval/avg_episode_length': (Array(883.52344, dtype=float32), Array(319.6931, dtype=float32)), 'eval/epoch_eval_time': 4.089627504348755, 'eval/sps': 31298.694040933962}
I0727 05:26:51.499986 140120985872192 train.py:379] starting iteration 477 4775.825640439987
I0727 05:27:01.449391 140120985872192 train.py:394] {'eval/walltime': 1978.3293132781982, 'training/sps': 42152.283482398896, 'training/walltime': 2798.351135969162, 'training/entropy_loss': Array(-0.0230553, dtype=float32), 'training/policy_loss': Array(0.00615788, dtype=float32), 'training/total_loss': Array(13.449198, dtype=float32), 'training/v_loss': Array(13.466097, dtype=float32), 'eval/episode_goal_distance': (Array(0.29843837, dtype=float32), Array(0.05828856, dtype=float32)), 'eval/episode_reward': (Array(-14740.233, dtype=float32), Array(4686.2817, dtype=float32)), 'eval/avg_episode_length': (Array(937.8906, dtype=float32), Array(240.54924, dtype=float32)), 'eval/epoch_eval_time': 4.115405321121216, 'eval/sps': 31102.647251553637}
I0727 05:27:01.451806 140120985872192 train.py:379] starting iteration 478 4785.777460098267
I0727 05:27:11.368686 140120985872192 train.py:394] {'eval/walltime': 1982.4164514541626, 'training/sps': 42182.58787236454, 'training/walltime': 2804.177236557007, 'training/entropy_loss': Array(-0.02369787, dtype=float32), 'training/policy_loss': Array(0.00178123, dtype=float32), 'training/total_loss': Array(14.161943, dtype=float32), 'training/v_loss': Array(14.18386, dtype=float32), 'eval/episode_goal_distance': (Array(0.2909146, dtype=float32), Array(0.06721694, dtype=float32)), 'eval/episode_reward': (Array(-14184.126, dtype=float32), Array(5024.708, dtype=float32)), 'eval/avg_episode_length': (Array(914.5, dtype=float32), Array(278.8458, dtype=float32)), 'eval/epoch_eval_time': 4.0871381759643555, 'eval/sps': 31317.75694610534}
I0727 05:27:11.371228 140120985872192 train.py:379] starting iteration 479 4795.696881771088
I0727 05:27:21.321254 140120985872192 train.py:394] {'eval/walltime': 1986.528202533722, 'training/sps': 42121.0656024247, 'training/walltime': 2810.011846780777, 'training/entropy_loss': Array(-0.0256041, dtype=float32), 'training/policy_loss': Array(0.0018118, dtype=float32), 'training/total_loss': Array(15.63515, dtype=float32), 'training/v_loss': Array(15.658943, dtype=float32), 'eval/episode_goal_distance': (Array(0.29379404, dtype=float32), Array(0.06185183, dtype=float32)), 'eval/episode_reward': (Array(-14357.18, dtype=float32), Array(4947.409, dtype=float32)), 'eval/avg_episode_length': (Array(922.41406, dtype=float32), Array(266.5169, dtype=float32)), 'eval/epoch_eval_time': 4.111751079559326, 'eval/sps': 31130.28914525592}
I0727 05:27:21.324080 140120985872192 train.py:379] starting iteration 480 4805.64973449707
I0727 05:27:31.243891 140120985872192 train.py:394] {'eval/walltime': 1990.6165492534637, 'training/sps': 42170.56129739039, 'training/walltime': 2815.8396089076996, 'training/entropy_loss': Array(-0.02370668, dtype=float32), 'training/policy_loss': Array(0.00204892, dtype=float32), 'training/total_loss': Array(11.907741, dtype=float32), 'training/v_loss': Array(11.929399, dtype=float32), 'eval/episode_goal_distance': (Array(0.29015812, dtype=float32), Array(0.06148962, dtype=float32)), 'eval/episode_reward': (Array(-13835.096, dtype=float32), Array(5055.955, dtype=float32)), 'eval/avg_episode_length': (Array(906.8203, dtype=float32), Array(289.70773, dtype=float32)), 'eval/epoch_eval_time': 4.088346719741821, 'eval/sps': 31308.499198933692}
I0727 05:27:31.246289 140120985872192 train.py:379] starting iteration 481 4815.5719435215
I0727 05:27:41.216918 140120985872192 train.py:394] {'eval/walltime': 1994.7295227050781, 'training/sps': 41981.5777764745, 'training/walltime': 2821.693605184555, 'training/entropy_loss': Array(-0.02124061, dtype=float32), 'training/policy_loss': Array(0.00737431, dtype=float32), 'training/total_loss': Array(9.602224, dtype=float32), 'training/v_loss': Array(9.616091, dtype=float32), 'eval/episode_goal_distance': (Array(0.3000309, dtype=float32), Array(0.05613856, dtype=float32)), 'eval/episode_reward': (Array(-14345.461, dtype=float32), Array(5166.244, dtype=float32)), 'eval/avg_episode_length': (Array(906.75, dtype=float32), Array(289.9264, dtype=float32)), 'eval/epoch_eval_time': 4.11297345161438, 'eval/sps': 31121.037250984158}
I0727 05:27:41.219551 140120985872192 train.py:379] starting iteration 482 4825.545205354691
I0727 05:27:51.133924 140120985872192 train.py:394] {'eval/walltime': 1998.819950580597, 'training/sps': 42225.33948644945, 'training/walltime': 2827.5138070583344, 'training/entropy_loss': Array(-0.02113875, dtype=float32), 'training/policy_loss': Array(0.00123051, dtype=float32), 'training/total_loss': Array(13.525836, dtype=float32), 'training/v_loss': Array(13.545744, dtype=float32), 'eval/episode_goal_distance': (Array(0.28570294, dtype=float32), Array(0.06533907, dtype=float32)), 'eval/episode_reward': (Array(-13336.704, dtype=float32), Array(5283., dtype=float32)), 'eval/avg_episode_length': (Array(899.0078, dtype=float32), Array(300.37613, dtype=float32)), 'eval/epoch_eval_time': 4.090427875518799, 'eval/sps': 31292.56984729634}
I0727 05:27:51.136344 140120985872192 train.py:379] starting iteration 483 4835.461998462677
I0727 05:28:01.068352 140120985872192 train.py:394] {'eval/walltime': 2002.9309813976288, 'training/sps': 42247.07245969099, 'training/walltime': 2833.3310148715973, 'training/entropy_loss': Array(-0.02380077, dtype=float32), 'training/policy_loss': Array(0.00114117, dtype=float32), 'training/total_loss': Array(53.162697, dtype=float32), 'training/v_loss': Array(53.185356, dtype=float32), 'eval/episode_goal_distance': (Array(0.28851154, dtype=float32), Array(0.05993529, dtype=float32)), 'eval/episode_reward': (Array(-13360.101, dtype=float32), Array(5451.441, dtype=float32)), 'eval/avg_episode_length': (Array(883.46875, dtype=float32), Array(319.84314, dtype=float32)), 'eval/epoch_eval_time': 4.11103081703186, 'eval/sps': 31135.743247095197}
I0727 05:28:01.071176 140120985872192 train.py:379] starting iteration 484 4845.396830320358
I0727 05:28:10.956156 140120985872192 train.py:394] {'eval/walltime': 2007.031385421753, 'training/sps': 42511.72250868088, 'training/walltime': 2839.1120085716248, 'training/entropy_loss': Array(-0.02571401, dtype=float32), 'training/policy_loss': Array(0.00062788, dtype=float32), 'training/total_loss': Array(13.69252, dtype=float32), 'training/v_loss': Array(13.717607, dtype=float32), 'eval/episode_goal_distance': (Array(0.28671387, dtype=float32), Array(0.05829417, dtype=float32)), 'eval/episode_reward': (Array(-13366.678, dtype=float32), Array(5876.2236, dtype=float32)), 'eval/avg_episode_length': (Array(868.08594, dtype=float32), Array(337.07657, dtype=float32)), 'eval/epoch_eval_time': 4.1004040241241455, 'eval/sps': 31216.43605043068}
I0727 05:28:10.958537 140120985872192 train.py:379] starting iteration 485 4855.284190416336
I0727 05:28:20.888214 140120985872192 train.py:394] {'eval/walltime': 2011.1350629329681, 'training/sps': 42211.05166638486, 'training/walltime': 2844.934180498123, 'training/entropy_loss': Array(-0.02833022, dtype=float32), 'training/policy_loss': Array(0.00067532, dtype=float32), 'training/total_loss': Array(14.557783, dtype=float32), 'training/v_loss': Array(14.585438, dtype=float32), 'eval/episode_goal_distance': (Array(0.29785496, dtype=float32), Array(0.05557165, dtype=float32)), 'eval/episode_reward': (Array(-14782.608, dtype=float32), Array(4435.8022, dtype=float32)), 'eval/avg_episode_length': (Array(945.6406, dtype=float32), Array(226.0055, dtype=float32)), 'eval/epoch_eval_time': 4.10367751121521, 'eval/sps': 31191.534824600712}
I0727 05:28:20.890726 140120985872192 train.py:379] starting iteration 486 4865.216380119324
I0727 05:28:30.831602 140120985872192 train.py:394] {'eval/walltime': 2015.2628335952759, 'training/sps': 42303.858727346764, 'training/walltime': 2850.7435796260834, 'training/entropy_loss': Array(-0.02903274, dtype=float32), 'training/policy_loss': Array(0.00070318, dtype=float32), 'training/total_loss': Array(16.185837, dtype=float32), 'training/v_loss': Array(16.214167, dtype=float32), 'eval/episode_goal_distance': (Array(0.2929067, dtype=float32), Array(0.0617606, dtype=float32)), 'eval/episode_reward': (Array(-14905.785, dtype=float32), Array(4670.111, dtype=float32)), 'eval/avg_episode_length': (Array(937.8281, dtype=float32), Array(240.79114, dtype=float32)), 'eval/epoch_eval_time': 4.127770662307739, 'eval/sps': 31009.47471932421}
I0727 05:28:30.834122 140120985872192 train.py:379] starting iteration 487 4875.159776926041
I0727 05:28:40.751393 140120985872192 train.py:394] {'eval/walltime': 2019.3679413795471, 'training/sps': 42311.256047193914, 'training/walltime': 2856.5519630908966, 'training/entropy_loss': Array(-0.02969348, dtype=float32), 'training/policy_loss': Array(0.001142, dtype=float32), 'training/total_loss': Array(16.911629, dtype=float32), 'training/v_loss': Array(16.94018, dtype=float32), 'eval/episode_goal_distance': (Array(0.2947965, dtype=float32), Array(0.05868271, dtype=float32)), 'eval/episode_reward': (Array(-14460.94, dtype=float32), Array(5036.7397, dtype=float32)), 'eval/avg_episode_length': (Array(922.3125, dtype=float32), Array(266.86563, dtype=float32)), 'eval/epoch_eval_time': 4.10510778427124, 'eval/sps': 31180.667287332435}
I0727 05:28:40.754177 140120985872192 train.py:379] starting iteration 488 4885.079831600189
I0727 05:28:50.676838 140120985872192 train.py:394] {'eval/walltime': 2023.4760882854462, 'training/sps': 42293.31420573409, 'training/walltime': 2862.362810611725, 'training/entropy_loss': Array(-0.02820441, dtype=float32), 'training/policy_loss': Array(0.0011664, dtype=float32), 'training/total_loss': Array(12.643478, dtype=float32), 'training/v_loss': Array(12.670515, dtype=float32), 'eval/episode_goal_distance': (Array(0.2951593, dtype=float32), Array(0.06584733, dtype=float32)), 'eval/episode_reward': (Array(-14208.635, dtype=float32), Array(4696.085, dtype=float32)), 'eval/avg_episode_length': (Array(930.08594, dtype=float32), Array(254.22452, dtype=float32)), 'eval/epoch_eval_time': 4.108146905899048, 'eval/sps': 31157.60047825939}
I0727 05:28:50.679643 140120985872192 train.py:379] starting iteration 489 4895.005297422409
I0727 05:29:00.598898 140120985872192 train.py:394] {'eval/walltime': 2027.5748164653778, 'training/sps': 42250.002358443584, 'training/walltime': 2868.179615020752, 'training/entropy_loss': Array(-0.0275301, dtype=float32), 'training/policy_loss': Array(0.00121683, dtype=float32), 'training/total_loss': Array(9.924009, dtype=float32), 'training/v_loss': Array(9.950323, dtype=float32), 'eval/episode_goal_distance': (Array(0.29494408, dtype=float32), Array(0.05989504, dtype=float32)), 'eval/episode_reward': (Array(-13721.81, dtype=float32), Array(5229.7812, dtype=float32)), 'eval/avg_episode_length': (Array(899.02344, dtype=float32), Array(300.32996, dtype=float32)), 'eval/epoch_eval_time': 4.098728179931641, 'eval/sps': 31229.199493325465}
I0727 05:29:00.601555 140120985872192 train.py:379] starting iteration 490 4904.927207946777
I0727 05:29:10.549944 140120985872192 train.py:394] {'eval/walltime': 2031.701166152954, 'training/sps': 42238.733477422, 'training/walltime': 2873.9979712963104, 'training/entropy_loss': Array(-0.02625687, dtype=float32), 'training/policy_loss': Array(0.0024586, dtype=float32), 'training/total_loss': Array(14.739277, dtype=float32), 'training/v_loss': Array(14.763075, dtype=float32), 'eval/episode_goal_distance': (Array(0.28821665, dtype=float32), Array(0.06046411, dtype=float32)), 'eval/episode_reward': (Array(-14380.127, dtype=float32), Array(4434.79, dtype=float32)), 'eval/avg_episode_length': (Array(937.9297, dtype=float32), Array(240.39754, dtype=float32)), 'eval/epoch_eval_time': 4.126349687576294, 'eval/sps': 31020.15332956033}
I0727 05:29:10.552348 140120985872192 train.py:379] starting iteration 491 4914.8780019283295
I0727 05:29:20.508985 140120985872192 train.py:394] {'eval/walltime': 2035.8083004951477, 'training/sps': 42039.87917643462, 'training/walltime': 2879.843849182129, 'training/entropy_loss': Array(-0.02778199, dtype=float32), 'training/policy_loss': Array(0.00082161, dtype=float32), 'training/total_loss': Array(54.09842, dtype=float32), 'training/v_loss': Array(54.12538, dtype=float32), 'eval/episode_goal_distance': (Array(0.2967627, dtype=float32), Array(0.0610822, dtype=float32)), 'eval/episode_reward': (Array(-14082.426, dtype=float32), Array(5149.8486, dtype=float32)), 'eval/avg_episode_length': (Array(906.8203, dtype=float32), Array(289.7077, dtype=float32)), 'eval/epoch_eval_time': 4.1071343421936035, 'eval/sps': 31165.282003323937}
I0727 05:29:20.511589 140120985872192 train.py:379] starting iteration 492 4924.837243318558
I0727 05:29:30.396552 140120985872192 train.py:394] {'eval/walltime': 2039.8902788162231, 'training/sps': 42377.338748640446, 'training/walltime': 2885.643175125122, 'training/entropy_loss': Array(-0.0292235, dtype=float32), 'training/policy_loss': Array(0.00095389, dtype=float32), 'training/total_loss': Array(18.300758, dtype=float32), 'training/v_loss': Array(18.32903, dtype=float32), 'eval/episode_goal_distance': (Array(0.28404382, dtype=float32), Array(0.05785501, dtype=float32)), 'eval/episode_reward': (Array(-13596.393, dtype=float32), Array(5021.226, dtype=float32)), 'eval/avg_episode_length': (Array(906.7969, dtype=float32), Array(289.78058, dtype=float32)), 'eval/epoch_eval_time': 4.0819783210754395, 'eval/sps': 31357.34438841338}
I0727 05:29:30.399200 140120985872192 train.py:379] starting iteration 493 4934.724855184555
I0727 05:29:40.335249 140120985872192 train.py:394] {'eval/walltime': 2043.988404750824, 'training/sps': 42123.983213374966, 'training/walltime': 2891.4773812294006, 'training/entropy_loss': Array(-0.02984038, dtype=float32), 'training/policy_loss': Array(0.00057386, dtype=float32), 'training/total_loss': Array(10.237373, dtype=float32), 'training/v_loss': Array(10.266641, dtype=float32), 'eval/episode_goal_distance': (Array(0.28492516, dtype=float32), Array(0.05664255, dtype=float32)), 'eval/episode_reward': (Array(-14566.561, dtype=float32), Array(4400.556, dtype=float32)), 'eval/avg_episode_length': (Array(945.6406, dtype=float32), Array(226.00555, dtype=float32)), 'eval/epoch_eval_time': 4.09812593460083, 'eval/sps': 31233.78882022267}
I0727 05:29:40.337773 140120985872192 train.py:379] starting iteration 494 4944.6634266376495
I0727 05:29:50.289879 140120985872192 train.py:394] {'eval/walltime': 2048.117038965225, 'training/sps': 42229.20229912473, 'training/walltime': 2897.297050714493, 'training/entropy_loss': Array(-0.03174206, dtype=float32), 'training/policy_loss': Array(0.000349, dtype=float32), 'training/total_loss': Array(16.27103, dtype=float32), 'training/v_loss': Array(16.302423, dtype=float32), 'eval/episode_goal_distance': (Array(0.2859205, dtype=float32), Array(0.05671929, dtype=float32)), 'eval/episode_reward': (Array(-14900.064, dtype=float32), Array(4094.7173, dtype=float32)), 'eval/avg_episode_length': (Array(961.2031, dtype=float32), Array(192.4264, dtype=float32)), 'eval/epoch_eval_time': 4.128634214401245, 'eval/sps': 31002.988725307358}
I0727 05:29:50.292463 140120985872192 train.py:379] starting iteration 495 4954.6181173324585
I0727 05:30:00.194359 140120985872192 train.py:394] {'eval/walltime': 2052.210087299347, 'training/sps': 42335.212628047746, 'training/walltime': 2903.1021473407745, 'training/entropy_loss': Array(-0.03331515, dtype=float32), 'training/policy_loss': Array(0.0004777, dtype=float32), 'training/total_loss': Array(15.893591, dtype=float32), 'training/v_loss': Array(15.926429, dtype=float32), 'eval/episode_goal_distance': (Array(0.30332714, dtype=float32), Array(0.06441907, dtype=float32)), 'eval/episode_reward': (Array(-13630.75, dtype=float32), Array(5595.911, dtype=float32)), 'eval/avg_episode_length': (Array(883.46875, dtype=float32), Array(319.84286, dtype=float32)), 'eval/epoch_eval_time': 4.093048334121704, 'eval/sps': 31272.535663194543}
I0727 05:30:00.196876 140120985872192 train.py:379] starting iteration 496 4964.5225303173065
I0727 05:30:10.143734 140120985872192 train.py:394] {'eval/walltime': 2056.312134742737, 'training/sps': 42073.514804026665, 'training/walltime': 2908.9433517456055, 'training/entropy_loss': Array(-0.03262087, dtype=float32), 'training/policy_loss': Array(0.0001616, dtype=float32), 'training/total_loss': Array(11.583906, dtype=float32), 'training/v_loss': Array(11.616366, dtype=float32), 'eval/episode_goal_distance': (Array(0.29652417, dtype=float32), Array(0.05870676, dtype=float32)), 'eval/episode_reward': (Array(-14920.049, dtype=float32), Array(4495.255, dtype=float32)), 'eval/avg_episode_length': (Array(945.66406, dtype=float32), Array(225.90779, dtype=float32)), 'eval/epoch_eval_time': 4.102047443389893, 'eval/sps': 31203.929687907763}
I0727 05:30:10.146260 140120985872192 train.py:379] starting iteration 497 4974.471914291382
I0727 05:30:20.122288 140120985872192 train.py:394] {'eval/walltime': 2060.430088043213, 'training/sps': 41978.68840604794, 'training/walltime': 2914.7977509498596, 'training/entropy_loss': Array(-0.03223728, dtype=float32), 'training/policy_loss': Array(0.00040328, dtype=float32), 'training/total_loss': Array(12.560371, dtype=float32), 'training/v_loss': Array(12.592205, dtype=float32), 'eval/episode_goal_distance': (Array(0.29303062, dtype=float32), Array(0.05968544, dtype=float32)), 'eval/episode_reward': (Array(-13774.519, dtype=float32), Array(5380.2847, dtype=float32)), 'eval/avg_episode_length': (Array(899.0547, dtype=float32), Array(300.23697, dtype=float32)), 'eval/epoch_eval_time': 4.117953300476074, 'eval/sps': 31083.402520665302}
I0727 05:30:20.124813 140120985872192 train.py:379] starting iteration 498 4984.450467586517
I0727 05:30:30.014696 140120985872192 train.py:394] {'eval/walltime': 2064.520990371704, 'training/sps': 42407.02035877485, 'training/walltime': 2920.5930178165436, 'training/entropy_loss': Array(-0.03164449, dtype=float32), 'training/policy_loss': Array(0.00023135, dtype=float32), 'training/total_loss': Array(14.548413, dtype=float32), 'training/v_loss': Array(14.579827, dtype=float32), 'eval/episode_goal_distance': (Array(0.29583687, dtype=float32), Array(0.06679597, dtype=float32)), 'eval/episode_reward': (Array(-14434.615, dtype=float32), Array(4626.2505, dtype=float32)), 'eval/avg_episode_length': (Array(937.7969, dtype=float32), Array(240.91214, dtype=float32)), 'eval/epoch_eval_time': 4.090902328491211, 'eval/sps': 31288.940610617905}
I0727 05:30:30.017190 140120985872192 train.py:379] starting iteration 499 4994.342844486237
I0727 05:30:39.935184 140120985872192 train.py:394] {'eval/walltime': 2068.6180515289307, 'training/sps': 42246.60669171181, 'training/walltime': 2926.4102897644043, 'training/entropy_loss': Array(-0.03129941, dtype=float32), 'training/policy_loss': Array(0.00076313, dtype=float32), 'training/total_loss': Array(12.534143, dtype=float32), 'training/v_loss': Array(12.564681, dtype=float32), 'eval/episode_goal_distance': (Array(0.28951094, dtype=float32), Array(0.0651819, dtype=float32)), 'eval/episode_reward': (Array(-13687.506, dtype=float32), Array(5607.069, dtype=float32)), 'eval/avg_episode_length': (Array(883.59375, dtype=float32), Array(319.49976, dtype=float32)), 'eval/epoch_eval_time': 4.0970611572265625, 'eval/sps': 31241.90610975587}
I0727 05:30:39.937699 140120985872192 train.py:379] starting iteration 500 5004.263353347778
I0727 05:30:49.868662 140120985872192 train.py:394] {'eval/walltime': 2072.705357313156, 'training/sps': 42082.121961025485, 'training/walltime': 2932.2502994537354, 'training/entropy_loss': Array(-0.03493107, dtype=float32), 'training/policy_loss': Array(0.00038772, dtype=float32), 'training/total_loss': Array(73.72094, dtype=float32), 'training/v_loss': Array(73.755486, dtype=float32), 'eval/episode_goal_distance': (Array(0.30177313, dtype=float32), Array(0.06104407, dtype=float32)), 'eval/episode_reward': (Array(-14794.541, dtype=float32), Array(5190.778, dtype=float32)), 'eval/avg_episode_length': (Array(922.2969, dtype=float32), Array(266.9193, dtype=float32)), 'eval/epoch_eval_time': 4.087305784225464, 'eval/sps': 31316.47269798184}
I0727 05:30:49.871213 140120985872192 train.py:379] starting iteration 501 5014.196867465973
I0727 05:30:59.791107 140120985872192 train.py:394] {'eval/walltime': 2076.807109594345, 'training/sps': 42267.3268467719, 'training/walltime': 2938.0647196769714, 'training/entropy_loss': Array(-0.03768984, dtype=float32), 'training/policy_loss': Array(0.0002203, dtype=float32), 'training/total_loss': Array(21.15359, dtype=float32), 'training/v_loss': Array(21.191061, dtype=float32), 'eval/episode_goal_distance': (Array(0.29548097, dtype=float32), Array(0.05659729, dtype=float32)), 'eval/episode_reward': (Array(-14212.322, dtype=float32), Array(5348.2544, dtype=float32)), 'eval/avg_episode_length': (Array(899.0078, dtype=float32), Array(300.3763, dtype=float32)), 'eval/epoch_eval_time': 4.101752281188965, 'eval/sps': 31206.175123500376}
I0727 05:30:59.794017 140120985872192 train.py:379] starting iteration 502 5024.119671344757
I0727 05:31:09.744170 140120985872192 train.py:394] {'eval/walltime': 2080.9351167678833, 'training/sps': 42237.19656954974, 'training/walltime': 2943.883287668228, 'training/entropy_loss': Array(-0.03887848, dtype=float32), 'training/policy_loss': Array(-1.0347361e-05, dtype=float32), 'training/total_loss': Array(18.58715, dtype=float32), 'training/v_loss': Array(18.62604, dtype=float32), 'eval/episode_goal_distance': (Array(0.29272243, dtype=float32), Array(0.06298631, dtype=float32)), 'eval/episode_reward': (Array(-14902.64, dtype=float32), Array(4522.956, dtype=float32)), 'eval/avg_episode_length': (Array(945.5703, dtype=float32), Array(226.29745, dtype=float32)), 'eval/epoch_eval_time': 4.128007173538208, 'eval/sps': 31007.698053559416}
I0727 05:31:09.746596 140120985872192 train.py:379] starting iteration 503 5034.0722506046295
I0727 05:31:19.692275 140120985872192 train.py:394] {'eval/walltime': 2085.039203643799, 'training/sps': 42097.223479108805, 'training/walltime': 2949.7212023735046, 'training/entropy_loss': Array(-0.03978993, dtype=float32), 'training/policy_loss': Array(-0.00017278, dtype=float32), 'training/total_loss': Array(19.469975, dtype=float32), 'training/v_loss': Array(19.509937, dtype=float32), 'eval/episode_goal_distance': (Array(0.30284798, dtype=float32), Array(0.06259595, dtype=float32)), 'eval/episode_reward': (Array(-13744.047, dtype=float32), Array(5938.665, dtype=float32)), 'eval/avg_episode_length': (Array(867.9219, dtype=float32), Array(337.49567, dtype=float32)), 'eval/epoch_eval_time': 4.104086875915527, 'eval/sps': 31188.423605542255}
I0727 05:31:19.694775 140120985872192 train.py:379] starting iteration 504 5044.020428419113
I0727 05:31:29.612513 140120985872192 train.py:394] {'eval/walltime': 2089.1696424484253, 'training/sps': 42492.69763696672, 'training/walltime': 2955.504784345627, 'training/entropy_loss': Array(-0.03909177, dtype=float32), 'training/policy_loss': Array(-0.00038343, dtype=float32), 'training/total_loss': Array(17.173168, dtype=float32), 'training/v_loss': Array(17.212643, dtype=float32), 'eval/episode_goal_distance': (Array(0.2999776, dtype=float32), Array(0.06612412, dtype=float32)), 'eval/episode_reward': (Array(-14370.245, dtype=float32), Array(5506.103, dtype=float32)), 'eval/avg_episode_length': (Array(906.7578, dtype=float32), Array(289.902, dtype=float32)), 'eval/epoch_eval_time': 4.130438804626465, 'eval/sps': 30989.443508188146}
I0727 05:31:29.615142 140120985872192 train.py:379] starting iteration 505 5053.940796613693
I0727 05:31:39.545817 140120985872192 train.py:394] {'eval/walltime': 2093.277372598648, 'training/sps': 42231.422049493245, 'training/walltime': 2961.324147939682, 'training/entropy_loss': Array(-0.03866323, dtype=float32), 'training/policy_loss': Array(-2.7301548e-05, dtype=float32), 'training/total_loss': Array(16.615208, dtype=float32), 'training/v_loss': Array(16.653896, dtype=float32), 'eval/episode_goal_distance': (Array(0.29792905, dtype=float32), Array(0.06304662, dtype=float32)), 'eval/episode_reward': (Array(-14914.049, dtype=float32), Array(5098.7305, dtype=float32)), 'eval/avg_episode_length': (Array(922.3203, dtype=float32), Array(266.83884, dtype=float32)), 'eval/epoch_eval_time': 4.107730150222778, 'eval/sps': 31160.761617473352}
I0727 05:31:39.548149 140120985872192 train.py:379] starting iteration 506 5063.873802900314
I0727 05:31:49.472822 140120985872192 train.py:394] {'eval/walltime': 2097.397844314575, 'training/sps': 42369.30179479012, 'training/walltime': 2967.124573945999, 'training/entropy_loss': Array(-0.03837717, dtype=float32), 'training/policy_loss': Array(-0.00022102, dtype=float32), 'training/total_loss': Array(16.904263, dtype=float32), 'training/v_loss': Array(16.942863, dtype=float32), 'eval/episode_goal_distance': (Array(0.29450387, dtype=float32), Array(0.06194942, dtype=float32)), 'eval/episode_reward': (Array(-14654.229, dtype=float32), Array(5229.5693, dtype=float32)), 'eval/avg_episode_length': (Array(914.53906, dtype=float32), Array(278.71808, dtype=float32)), 'eval/epoch_eval_time': 4.120471715927124, 'eval/sps': 31064.404472243645}
I0727 05:31:49.475328 140120985872192 train.py:379] starting iteration 507 5073.800982236862
I0727 05:31:59.445772 140120985872192 train.py:394] {'eval/walltime': 2101.516783475876, 'training/sps': 42026.39679980495, 'training/walltime': 2972.972327232361, 'training/entropy_loss': Array(-0.03844014, dtype=float32), 'training/policy_loss': Array(-0.00029061, dtype=float32), 'training/total_loss': Array(16.45773, dtype=float32), 'training/v_loss': Array(16.496462, dtype=float32), 'eval/episode_goal_distance': (Array(0.30628914, dtype=float32), Array(0.06269611, dtype=float32)), 'eval/episode_reward': (Array(-13468.604, dtype=float32), Array(6603.2397, dtype=float32)), 'eval/avg_episode_length': (Array(829.0781, dtype=float32), Array(375.18063, dtype=float32)), 'eval/epoch_eval_time': 4.118939161300659, 'eval/sps': 31075.96276308698}
I0727 05:31:59.448278 140120985872192 train.py:379] starting iteration 508 5083.773931264877
I0727 05:32:09.333087 140120985872192 train.py:394] {'eval/walltime': 2105.600976228714, 'training/sps': 42395.50192309156, 'training/walltime': 2978.769168615341, 'training/entropy_loss': Array(-0.03918979, dtype=float32), 'training/policy_loss': Array(-0.00010164, dtype=float32), 'training/total_loss': Array(70.38594, dtype=float32), 'training/v_loss': Array(70.42523, dtype=float32), 'eval/episode_goal_distance': (Array(0.30351868, dtype=float32), Array(0.0589138, dtype=float32)), 'eval/episode_reward': (Array(-15257.727, dtype=float32), Array(5261.9297, dtype=float32)), 'eval/avg_episode_length': (Array(930.0703, dtype=float32), Array(254.28133, dtype=float32)), 'eval/epoch_eval_time': 4.084192752838135, 'eval/sps': 31340.342571014036}
I0727 05:32:09.335573 140120985872192 train.py:379] starting iteration 509 5093.661226272583
I0727 05:32:19.258448 140120985872192 train.py:394] {'eval/walltime': 2109.7101628780365, 'training/sps': 42300.04473546943, 'training/walltime': 2984.5790915489197, 'training/entropy_loss': Array(-0.03917128, dtype=float32), 'training/policy_loss': Array(-0.00063001, dtype=float32), 'training/total_loss': Array(26.41238, dtype=float32), 'training/v_loss': Array(26.45218, dtype=float32), 'eval/episode_goal_distance': (Array(0.29442817, dtype=float32), Array(0.06626796, dtype=float32)), 'eval/episode_reward': (Array(-14354.566, dtype=float32), Array(5448.322, dtype=float32)), 'eval/avg_episode_length': (Array(899., dtype=float32), Array(300.39966, dtype=float32)), 'eval/epoch_eval_time': 4.10918664932251, 'eval/sps': 31149.71670150433}
I0727 05:32:19.261026 140120985872192 train.py:379] starting iteration 510 5103.586679697037
I0727 05:32:29.218088 140120985872192 train.py:394] {'eval/walltime': 2113.8417842388153, 'training/sps': 42218.08975532653, 'training/walltime': 2990.4002928733826, 'training/entropy_loss': Array(-0.03929112, dtype=float32), 'training/policy_loss': Array(0.00017558, dtype=float32), 'training/total_loss': Array(22.186531, dtype=float32), 'training/v_loss': Array(22.225645, dtype=float32), 'eval/episode_goal_distance': (Array(0.30281872, dtype=float32), Array(0.06717067, dtype=float32)), 'eval/episode_reward': (Array(-15235.912, dtype=float32), Array(5221.787, dtype=float32)), 'eval/avg_episode_length': (Array(930.0469, dtype=float32), Array(254.36661, dtype=float32)), 'eval/epoch_eval_time': 4.131621360778809, 'eval/sps': 30980.573683516843}
I0727 05:32:30.332860 140120985872192 train.py:410] total steps: 125583360
