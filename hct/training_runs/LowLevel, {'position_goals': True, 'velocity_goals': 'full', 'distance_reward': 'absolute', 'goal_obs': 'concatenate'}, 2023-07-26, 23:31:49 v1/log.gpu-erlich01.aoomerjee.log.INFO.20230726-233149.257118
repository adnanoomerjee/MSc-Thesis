I0726 23:31:49.259470 140267183036224 low_level_env.py:187] Initialising environment...
I0726 23:32:21.729497 140267183036224 low_level_env.py:289] Environment initialised.
I0726 23:32:21.734980 140267183036224 train.py:118] JAX is running on GPU.
I0726 23:32:21.735013 140267183036224 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0726 23:32:25.645266 140267183036224 train.py:367] Running initial eval
I0726 23:32:41.284643 140267183036224 train.py:373] {'eval/walltime': 15.637202978134155, 'eval/episode_goal_distance': (Array(0.30415323, dtype=float32), Array(0.04459127, dtype=float32)), 'eval/episode_reward': (Array(-47442.688, dtype=float32), Array(17767.611, dtype=float32)), 'eval/avg_episode_length': (Array(898.9922, dtype=float32), Array(300.42285, dtype=float32)), 'eval/epoch_eval_time': 15.637202978134155, 'eval/sps': 8185.6071177809235}
I0726 23:32:41.286308 140267183036224 train.py:379] starting iteration 0 19.55133867263794
I0726 23:33:08.634899 140267183036224 train.py:394] {'eval/walltime': 19.501603603363037, 'training/sps': 5233.526848913052, 'training/walltime': 23.479386568069458, 'training/entropy_loss': Array(-0.0401822, dtype=float32), 'training/policy_loss': Array(0.0307278, dtype=float32), 'training/total_loss': Array(471399.8, dtype=float32), 'training/v_loss': Array(471399.8, dtype=float32), 'eval/episode_goal_distance': (Array(0.30299944, dtype=float32), Array(0.04073156, dtype=float32)), 'eval/episode_reward': (Array(-48456.047, dtype=float32), Array(15769.067, dtype=float32)), 'eval/avg_episode_length': (Array(922.375, dtype=float32), Array(266.65097, dtype=float32)), 'eval/epoch_eval_time': 3.864400625228882, 'eval/sps': 33122.85976882088}
I0726 23:33:08.662973 140267183036224 train.py:379] starting iteration 1 46.92800259590149
I0726 23:33:15.503452 140267183036224 train.py:394] {'eval/walltime': 23.367284297943115, 'training/sps': 41358.305320504835, 'training/walltime': 26.45049476623535, 'training/entropy_loss': Array(-0.04067029, dtype=float32), 'training/policy_loss': Array(0.00335819, dtype=float32), 'training/total_loss': Array(455121., dtype=float32), 'training/v_loss': Array(455121., dtype=float32), 'eval/episode_goal_distance': (Array(0.29897082, dtype=float32), Array(0.03846277, dtype=float32)), 'eval/episode_reward': (Array(-46017.004, dtype=float32), Array(18687.066, dtype=float32)), 'eval/avg_episode_length': (Array(875.75, dtype=float32), Array(328.73523, dtype=float32)), 'eval/epoch_eval_time': 3.865680694580078, 'eval/sps': 33111.89156917794}
I0726 23:33:15.506012 140267183036224 train.py:379] starting iteration 2 53.771042346954346
I0726 23:33:22.351531 140267183036224 train.py:394] {'eval/walltime': 27.22978162765503, 'training/sps': 41246.68993753168, 'training/walltime': 29.429642915725708, 'training/entropy_loss': Array(-0.04131711, dtype=float32), 'training/policy_loss': Array(0.0010479, dtype=float32), 'training/total_loss': Array(434832.34, dtype=float32), 'training/v_loss': Array(434832.4, dtype=float32), 'eval/episode_goal_distance': (Array(0.30651835, dtype=float32), Array(0.04094209, dtype=float32)), 'eval/episode_reward': (Array(-47300.645, dtype=float32), Array(18274.322, dtype=float32)), 'eval/avg_episode_length': (Array(891.28906, dtype=float32), Array(310.2148, dtype=float32)), 'eval/epoch_eval_time': 3.862497329711914, 'eval/sps': 33139.181486384856}
I0726 23:33:22.353944 140267183036224 train.py:379] starting iteration 3 60.61897373199463
I0726 23:33:29.205833 140267183036224 train.py:394] {'eval/walltime': 31.0910587310791, 'training/sps': 41140.65657922654, 'training/walltime': 32.41646933555603, 'training/entropy_loss': Array(-0.04151886, dtype=float32), 'training/policy_loss': Array(0.00145956, dtype=float32), 'training/total_loss': Array(408796.97, dtype=float32), 'training/v_loss': Array(408797.03, dtype=float32), 'eval/episode_goal_distance': (Array(0.30395514, dtype=float32), Array(0.04473329, dtype=float32)), 'eval/episode_reward': (Array(-45626.375, dtype=float32), Array(18983.457, dtype=float32)), 'eval/avg_episode_length': (Array(867.97656, dtype=float32), Array(337.3561, dtype=float32)), 'eval/epoch_eval_time': 3.8612771034240723, 'eval/sps': 33149.65400605235}
I0726 23:33:29.208354 140267183036224 train.py:379] starting iteration 4 67.47338342666626
I0726 23:33:36.086487 140267183036224 train.py:394] {'eval/walltime': 34.95609927177429, 'training/sps': 40832.92152984074, 'training/walltime': 35.42580580711365, 'training/entropy_loss': Array(-0.04229154, dtype=float32), 'training/policy_loss': Array(0.00267626, dtype=float32), 'training/total_loss': Array(379791., dtype=float32), 'training/v_loss': Array(379791.06, dtype=float32), 'eval/episode_goal_distance': (Array(0.30175686, dtype=float32), Array(0.04056787, dtype=float32)), 'eval/episode_reward': (Array(-44498.957, dtype=float32), Array(19150.889, dtype=float32)), 'eval/avg_episode_length': (Array(860.2422, dtype=float32), Array(345.49112, dtype=float32)), 'eval/epoch_eval_time': 3.8650405406951904, 'eval/sps': 33117.37578229312}
I0726 23:33:36.088921 140267183036224 train.py:379] starting iteration 5 74.35395097732544
I0726 23:33:42.973266 140267183036224 train.py:394] {'eval/walltime': 38.81687140464783, 'training/sps': 40690.75615037883, 'training/walltime': 38.445656299591064, 'training/entropy_loss': Array(-0.04283463, dtype=float32), 'training/policy_loss': Array(0.00488253, dtype=float32), 'training/total_loss': Array(348605.62, dtype=float32), 'training/v_loss': Array(348605.7, dtype=float32), 'eval/episode_goal_distance': (Array(0.30578724, dtype=float32), Array(0.03782596, dtype=float32)), 'eval/episode_reward': (Array(-44851.008, dtype=float32), Array(20433.703, dtype=float32)), 'eval/avg_episode_length': (Array(844.75, dtype=float32), Array(360.76907, dtype=float32)), 'eval/epoch_eval_time': 3.860772132873535, 'eval/sps': 33153.98982242727}
I0726 23:33:42.975869 140267183036224 train.py:379] starting iteration 6 81.24089860916138
I0726 23:33:49.867160 140267183036224 train.py:394] {'eval/walltime': 42.67559552192688, 'training/sps': 40566.48333845338, 'training/walltime': 41.47475790977478, 'training/entropy_loss': Array(-0.0423836, dtype=float32), 'training/policy_loss': Array(0.00722636, dtype=float32), 'training/total_loss': Array(313050.7, dtype=float32), 'training/v_loss': Array(313050.75, dtype=float32), 'eval/episode_goal_distance': (Array(0.3029857, dtype=float32), Array(0.04509345, dtype=float32)), 'eval/episode_reward': (Array(-47303.082, dtype=float32), Array(16486.943, dtype=float32)), 'eval/avg_episode_length': (Array(914.66406, dtype=float32), Array(278.31052, dtype=float32)), 'eval/epoch_eval_time': 3.8587241172790527, 'eval/sps': 33171.5862833589}
I0726 23:33:49.869590 140267183036224 train.py:379] starting iteration 7 88.13462018966675
I0726 23:33:56.766214 140267183036224 train.py:394] {'eval/walltime': 46.54958176612854, 'training/sps': 40703.170031877286, 'training/walltime': 44.49368739128113, 'training/entropy_loss': Array(-0.0417081, dtype=float32), 'training/policy_loss': Array(0.00963716, dtype=float32), 'training/total_loss': Array(275946.8, dtype=float32), 'training/v_loss': Array(275946.8, dtype=float32), 'eval/episode_goal_distance': (Array(0.30476815, dtype=float32), Array(0.03741739, dtype=float32)), 'eval/episode_reward': (Array(-47826.992, dtype=float32), Array(17422.44, dtype=float32)), 'eval/avg_episode_length': (Array(899.08594, dtype=float32), Array(300.14413, dtype=float32)), 'eval/epoch_eval_time': 3.87398624420166, 'eval/sps': 33040.90204026469}
I0726 23:33:56.768677 140267183036224 train.py:379] starting iteration 8 95.0337073802948
I0726 23:34:03.693742 140267183036224 train.py:394] {'eval/walltime': 50.44311881065369, 'training/sps': 40584.096969800055, 'training/walltime': 47.52147436141968, 'training/entropy_loss': Array(-0.04090349, dtype=float32), 'training/policy_loss': Array(0.01970033, dtype=float32), 'training/total_loss': Array(237849.4, dtype=float32), 'training/v_loss': Array(237849.44, dtype=float32), 'eval/episode_goal_distance': (Array(0.3011397, dtype=float32), Array(0.03746143, dtype=float32)), 'eval/episode_reward': (Array(-45035.637, dtype=float32), Array(19003.83, dtype=float32)), 'eval/avg_episode_length': (Array(868.0703, dtype=float32), Array(337.1168, dtype=float32)), 'eval/epoch_eval_time': 3.8935370445251465, 'eval/sps': 32874.992207916905}
I0726 23:34:03.696264 140267183036224 train.py:379] starting iteration 9 101.96129298210144
I0726 23:34:10.642026 140267183036224 train.py:394] {'eval/walltime': 54.34510350227356, 'training/sps': 40419.40791939396, 'training/walltime': 50.56159806251526, 'training/entropy_loss': Array(-0.03992426, dtype=float32), 'training/policy_loss': Array(0.01429181, dtype=float32), 'training/total_loss': Array(200412.9, dtype=float32), 'training/v_loss': Array(200412.94, dtype=float32), 'eval/episode_goal_distance': (Array(0.30257696, dtype=float32), Array(0.04006572, dtype=float32)), 'eval/episode_reward': (Array(-44973.117, dtype=float32), Array(19859.098, dtype=float32)), 'eval/avg_episode_length': (Array(852.47656, dtype=float32), Array(353.34448, dtype=float32)), 'eval/epoch_eval_time': 3.901984691619873, 'eval/sps': 32803.819111566525}
I0726 23:34:10.644549 140267183036224 train.py:379] starting iteration 10 108.90957903862
I0726 23:34:17.609263 140267183036224 train.py:394] {'eval/walltime': 58.25688433647156, 'training/sps': 40297.370550748215, 'training/walltime': 53.610928535461426, 'training/entropy_loss': Array(-0.03882099, dtype=float32), 'training/policy_loss': Array(0.01953725, dtype=float32), 'training/total_loss': Array(163108.1, dtype=float32), 'training/v_loss': Array(163108.11, dtype=float32), 'eval/episode_goal_distance': (Array(0.29879844, dtype=float32), Array(0.03722284, dtype=float32)), 'eval/episode_reward': (Array(-46160.33, dtype=float32), Array(18006.496, dtype=float32)), 'eval/avg_episode_length': (Array(883.5469, dtype=float32), Array(319.62872, dtype=float32)), 'eval/epoch_eval_time': 3.911780834197998, 'eval/sps': 32721.66959891628}
I0726 23:34:17.611829 140267183036224 train.py:379] starting iteration 11 115.8768584728241
I0726 23:34:24.602886 140267183036224 train.py:394] {'eval/walltime': 62.193727254867554, 'training/sps': 40282.33139089838, 'training/walltime': 56.6613974571228, 'training/entropy_loss': Array(-0.03746898, dtype=float32), 'training/policy_loss': Array(0.02896808, dtype=float32), 'training/total_loss': Array(122050.29, dtype=float32), 'training/v_loss': Array(122050.29, dtype=float32), 'eval/episode_goal_distance': (Array(0.30722556, dtype=float32), Array(0.03911848, dtype=float32)), 'eval/episode_reward': (Array(-46084.22, dtype=float32), Array(18462.482, dtype=float32)), 'eval/avg_episode_length': (Array(875.72656, dtype=float32), Array(328.79733, dtype=float32)), 'eval/epoch_eval_time': 3.936842918395996, 'eval/sps': 32513.362268503097}
I0726 23:34:24.605330 140267183036224 train.py:379] starting iteration 12 122.87036037445068
I0726 23:34:31.619251 140267183036224 train.py:394] {'eval/walltime': 66.1529471874237, 'training/sps': 40276.381818630805, 'training/walltime': 59.71231698989868, 'training/entropy_loss': Array(-0.0356227, dtype=float32), 'training/policy_loss': Array(0.03960625, dtype=float32), 'training/total_loss': Array(89038.55, dtype=float32), 'training/v_loss': Array(89038.55, dtype=float32), 'eval/episode_goal_distance': (Array(0.29974446, dtype=float32), Array(0.04284319, dtype=float32)), 'eval/episode_reward': (Array(-41651.65, dtype=float32), Array(21432.137, dtype=float32)), 'eval/avg_episode_length': (Array(805.8594, dtype=float32), Array(394.06317, dtype=float32)), 'eval/epoch_eval_time': 3.9592199325561523, 'eval/sps': 32329.60082552439}
I0726 23:34:31.621691 140267183036224 train.py:379] starting iteration 13 129.8867208957672
I0726 23:34:38.624626 140267183036224 train.py:394] {'eval/walltime': 70.11618423461914, 'training/sps': 40474.180984104925, 'training/walltime': 62.748326539993286, 'training/entropy_loss': Array(-0.03488441, dtype=float32), 'training/policy_loss': Array(0.02947689, dtype=float32), 'training/total_loss': Array(61316.117, dtype=float32), 'training/v_loss': Array(61316.12, dtype=float32), 'eval/episode_goal_distance': (Array(0.29255074, dtype=float32), Array(0.03919837, dtype=float32)), 'eval/episode_reward': (Array(-42001.812, dtype=float32), Array(20192.691, dtype=float32)), 'eval/avg_episode_length': (Array(829.02344, dtype=float32), Array(375.30035, dtype=float32)), 'eval/epoch_eval_time': 3.9632370471954346, 'eval/sps': 32296.83172511182}
I0726 23:34:38.626996 140267183036224 train.py:379] starting iteration 14 136.89202618598938
I0726 23:34:45.636949 140267183036224 train.py:394] {'eval/walltime': 74.07664823532104, 'training/sps': 40343.228107467854, 'training/walltime': 65.79419088363647, 'training/entropy_loss': Array(-0.03249029, dtype=float32), 'training/policy_loss': Array(0.0277722, dtype=float32), 'training/total_loss': Array(40218.6, dtype=float32), 'training/v_loss': Array(40218.605, dtype=float32), 'eval/episode_goal_distance': (Array(0.29827118, dtype=float32), Array(0.04448787, dtype=float32)), 'eval/episode_reward': (Array(-44397.69, dtype=float32), Array(19373.832, dtype=float32)), 'eval/avg_episode_length': (Array(860.3047, dtype=float32), Array(345.33685, dtype=float32)), 'eval/epoch_eval_time': 3.9604640007019043, 'eval/sps': 32319.44539258906}
I0726 23:34:45.639328 140267183036224 train.py:379] starting iteration 15 143.9043574333191
I0726 23:34:52.673322 140267183036224 train.py:394] {'eval/walltime': 78.05391192436218, 'training/sps': 40245.94060843986, 'training/walltime': 68.84741806983948, 'training/entropy_loss': Array(-0.02976428, dtype=float32), 'training/policy_loss': Array(0.03446675, dtype=float32), 'training/total_loss': Array(27170.969, dtype=float32), 'training/v_loss': Array(27170.96, dtype=float32), 'eval/episode_goal_distance': (Array(0.29941797, dtype=float32), Array(0.04758167, dtype=float32)), 'eval/episode_reward': (Array(-44931.355, dtype=float32), Array(18674.9, dtype=float32)), 'eval/avg_episode_length': (Array(875.66406, dtype=float32), Array(328.96237, dtype=float32)), 'eval/epoch_eval_time': 3.9772636890411377, 'eval/sps': 32182.930277589665}
I0726 23:34:52.675714 140267183036224 train.py:379] starting iteration 16 150.94074392318726
I0726 23:34:59.730274 140267183036224 train.py:394] {'eval/walltime': 82.05703020095825, 'training/sps': 40317.60214046325, 'training/walltime': 71.89521837234497, 'training/entropy_loss': Array(-0.02622309, dtype=float32), 'training/policy_loss': Array(0.06093899, dtype=float32), 'training/total_loss': Array(26812.717, dtype=float32), 'training/v_loss': Array(26812.682, dtype=float32), 'eval/episode_goal_distance': (Array(0.29306886, dtype=float32), Array(0.03592201, dtype=float32)), 'eval/episode_reward': (Array(-47922.53, dtype=float32), Array(15021.133, dtype=float32)), 'eval/avg_episode_length': (Array(930.1172, dtype=float32), Array(254.11069, dtype=float32)), 'eval/epoch_eval_time': 4.003118276596069, 'eval/sps': 31975.0732193806}
I0726 23:34:59.732780 140267183036224 train.py:379] starting iteration 17 157.99780988693237
I0726 23:35:06.806657 140267183036224 train.py:394] {'eval/walltime': 86.07838439941406, 'training/sps': 40305.296215683935, 'training/walltime': 74.9439492225647, 'training/entropy_loss': Array(-0.02262473, dtype=float32), 'training/policy_loss': Array(0.05575314, dtype=float32), 'training/total_loss': Array(22036.041, dtype=float32), 'training/v_loss': Array(22036.008, dtype=float32), 'eval/episode_goal_distance': (Array(0.29313207, dtype=float32), Array(0.03866969, dtype=float32)), 'eval/episode_reward': (Array(-45537.594, dtype=float32), Array(17867.605, dtype=float32)), 'eval/avg_episode_length': (Array(883.53906, dtype=float32), Array(319.6505, dtype=float32)), 'eval/epoch_eval_time': 4.0213541984558105, 'eval/sps': 31830.0735730147}
I0726 23:35:06.809120 140267183036224 train.py:379] starting iteration 18 165.07414889335632
I0726 23:35:13.880990 140267183036224 train.py:394] {'eval/walltime': 90.09955263137817, 'training/sps': 40328.17995049784, 'training/walltime': 77.99095010757446, 'training/entropy_loss': Array(-0.01983638, dtype=float32), 'training/policy_loss': Array(0.04888618, dtype=float32), 'training/total_loss': Array(17103.52, dtype=float32), 'training/v_loss': Array(17103.49, dtype=float32), 'eval/episode_goal_distance': (Array(0.29530033, dtype=float32), Array(0.04133365, dtype=float32)), 'eval/episode_reward': (Array(-45981.43, dtype=float32), Array(17881.703, dtype=float32)), 'eval/avg_episode_length': (Array(891.27344, dtype=float32), Array(310.2593, dtype=float32)), 'eval/epoch_eval_time': 4.021168231964111, 'eval/sps': 31831.545614663155}
I0726 23:35:13.883481 140267183036224 train.py:379] starting iteration 19 172.1485104560852
I0726 23:35:20.978806 140267183036224 train.py:394] {'eval/walltime': 94.13789391517639, 'training/sps': 40243.21921344911, 'training/walltime': 81.04438376426697, 'training/entropy_loss': Array(-0.02383674, dtype=float32), 'training/policy_loss': Array(0.04581498, dtype=float32), 'training/total_loss': Array(15749.614, dtype=float32), 'training/v_loss': Array(15749.592, dtype=float32), 'eval/episode_goal_distance': (Array(0.30132538, dtype=float32), Array(0.04434032, dtype=float32)), 'eval/episode_reward': (Array(-46129.137, dtype=float32), Array(17681.879, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.30667, dtype=float32)), 'eval/epoch_eval_time': 4.038341283798218, 'eval/sps': 31696.181923388853}
I0726 23:35:20.981137 140267183036224 train.py:379] starting iteration 20 179.2461678981781
I0726 23:35:28.068519 140267183036224 train.py:394] {'eval/walltime': 98.17864155769348, 'training/sps': 40382.0097889119, 'training/walltime': 84.08732295036316, 'training/entropy_loss': Array(-0.02658435, dtype=float32), 'training/policy_loss': Array(0.02533773, dtype=float32), 'training/total_loss': Array(14493.073, dtype=float32), 'training/v_loss': Array(14493.074, dtype=float32), 'eval/episode_goal_distance': (Array(0.2986995, dtype=float32), Array(0.03946019, dtype=float32)), 'eval/episode_reward': (Array(-44947.277, dtype=float32), Array(17099.781, dtype=float32)), 'eval/avg_episode_length': (Array(891.25, dtype=float32), Array(310.32632, dtype=float32)), 'eval/epoch_eval_time': 4.04074764251709, 'eval/sps': 31677.3061136442}
I0726 23:35:28.071398 140267183036224 train.py:379] starting iteration 21 186.3364269733429
I0726 23:35:35.172253 140267183036224 train.py:394] {'eval/walltime': 102.22594356536865, 'training/sps': 40290.103108740484, 'training/walltime': 87.13720345497131, 'training/entropy_loss': Array(-0.02576573, dtype=float32), 'training/policy_loss': Array(0.02084557, dtype=float32), 'training/total_loss': Array(13313.205, dtype=float32), 'training/v_loss': Array(13313.21, dtype=float32), 'eval/episode_goal_distance': (Array(0.30835712, dtype=float32), Array(0.04049924, dtype=float32)), 'eval/episode_reward': (Array(-47461.35, dtype=float32), Array(16680.752, dtype=float32)), 'eval/avg_episode_length': (Array(906.78906, dtype=float32), Array(289.8049, dtype=float32)), 'eval/epoch_eval_time': 4.047302007675171, 'eval/sps': 31626.006598288193}
I0726 23:35:35.174541 140267183036224 train.py:379] starting iteration 22 193.4395706653595
I0726 23:35:42.264483 140267183036224 train.py:394] {'eval/walltime': 106.26473140716553, 'training/sps': 40321.24838164188, 'training/walltime': 90.18472814559937, 'training/entropy_loss': Array(-0.02635402, dtype=float32), 'training/policy_loss': Array(0.01831001, dtype=float32), 'training/total_loss': Array(12303.642, dtype=float32), 'training/v_loss': Array(12303.649, dtype=float32), 'eval/episode_goal_distance': (Array(0.30813318, dtype=float32), Array(0.03959766, dtype=float32)), 'eval/episode_reward': (Array(-48652.656, dtype=float32), Array(15452.097, dtype=float32)), 'eval/avg_episode_length': (Array(922.3203, dtype=float32), Array(266.83887, dtype=float32)), 'eval/epoch_eval_time': 4.038787841796875, 'eval/sps': 31692.677361099566}
I0726 23:35:42.266933 140267183036224 train.py:379] starting iteration 23 200.53196215629578
I0726 23:35:49.379798 140267183036224 train.py:394] {'eval/walltime': 110.33164978027344, 'training/sps': 40390.99751319967, 'training/walltime': 93.22699022293091, 'training/entropy_loss': Array(-0.027311, dtype=float32), 'training/policy_loss': Array(0.01829608, dtype=float32), 'training/total_loss': Array(11696.643, dtype=float32), 'training/v_loss': Array(11696.652, dtype=float32), 'eval/episode_goal_distance': (Array(0.30802083, dtype=float32), Array(0.04483942, dtype=float32)), 'eval/episode_reward': (Array(-45248.047, dtype=float32), Array(19557.434, dtype=float32)), 'eval/avg_episode_length': (Array(860.22656, dtype=float32), Array(345.52954, dtype=float32)), 'eval/epoch_eval_time': 4.06691837310791, 'eval/sps': 31473.461785313215}
I0726 23:35:49.382181 140267183036224 train.py:379] starting iteration 24 207.64721059799194
I0726 23:35:56.482084 140267183036224 train.py:394] {'eval/walltime': 114.37704038619995, 'training/sps': 40277.14351760462, 'training/walltime': 96.27785205841064, 'training/entropy_loss': Array(-0.03150967, dtype=float32), 'training/policy_loss': Array(0.01457264, dtype=float32), 'training/total_loss': Array(10499.026, dtype=float32), 'training/v_loss': Array(10499.043, dtype=float32), 'eval/episode_goal_distance': (Array(0.3061098, dtype=float32), Array(0.04060411, dtype=float32)), 'eval/episode_reward': (Array(-46171.387, dtype=float32), Array(18619.293, dtype=float32)), 'eval/avg_episode_length': (Array(875.8828, dtype=float32), Array(328.38382, dtype=float32)), 'eval/epoch_eval_time': 4.045390605926514, 'eval/sps': 31640.94953216124}
I0726 23:35:56.484577 140267183036224 train.py:379] starting iteration 25 214.74960708618164
I0726 23:36:03.585335 140267183036224 train.py:394] {'eval/walltime': 118.42188358306885, 'training/sps': 40258.26691293603, 'training/walltime': 99.33014440536499, 'training/entropy_loss': Array(-0.03457595, dtype=float32), 'training/policy_loss': Array(0.01356746, dtype=float32), 'training/total_loss': Array(9223.109, dtype=float32), 'training/v_loss': Array(9223.131, dtype=float32), 'eval/episode_goal_distance': (Array(0.31283078, dtype=float32), Array(0.03604736, dtype=float32)), 'eval/episode_reward': (Array(-47716.883, dtype=float32), Array(17960.688, dtype=float32)), 'eval/avg_episode_length': (Array(891.27344, dtype=float32), Array(310.2591, dtype=float32)), 'eval/epoch_eval_time': 4.0448431968688965, 'eval/sps': 31645.231661658603}
I0726 23:36:03.587789 140267183036224 train.py:379] starting iteration 26 221.85281801223755
I0726 23:36:10.674031 140267183036224 train.py:394] {'eval/walltime': 122.46712589263916, 'training/sps': 40456.30058459564, 'training/walltime': 102.36749577522278, 'training/entropy_loss': Array(-0.03385242, dtype=float32), 'training/policy_loss': Array(0.01197068, dtype=float32), 'training/total_loss': Array(8258.764, dtype=float32), 'training/v_loss': Array(8258.785, dtype=float32), 'eval/episode_goal_distance': (Array(0.3157361, dtype=float32), Array(0.0425552, dtype=float32)), 'eval/episode_reward': (Array(-48747.63, dtype=float32), Array(16386.754, dtype=float32)), 'eval/avg_episode_length': (Array(914.6094, dtype=float32), Array(278.48917, dtype=float32)), 'eval/epoch_eval_time': 4.0452423095703125, 'eval/sps': 31642.109471953045}
I0726 23:36:10.678782 140267183036224 train.py:379] starting iteration 27 228.94379568099976
I0726 23:36:17.787322 140267183036224 train.py:394] {'eval/walltime': 126.52160620689392, 'training/sps': 40312.34842335984, 'training/walltime': 105.41569328308105, 'training/entropy_loss': Array(-0.03336026, dtype=float32), 'training/policy_loss': Array(0.01008309, dtype=float32), 'training/total_loss': Array(7944.077, dtype=float32), 'training/v_loss': Array(7944.1006, dtype=float32), 'eval/episode_goal_distance': (Array(0.3051126, dtype=float32), Array(0.04260482, dtype=float32)), 'eval/episode_reward': (Array(-48244.973, dtype=float32), Array(17395.719, dtype=float32)), 'eval/avg_episode_length': (Array(906.8047, dtype=float32), Array(289.75674, dtype=float32)), 'eval/epoch_eval_time': 4.054480314254761, 'eval/sps': 31570.013930016383}
I0726 23:36:17.789629 140267183036224 train.py:379] starting iteration 28 236.05465865135193
I0726 23:36:24.893064 140267183036224 train.py:394] {'eval/walltime': 130.56672167778015, 'training/sps': 40226.85775272048, 'training/walltime': 108.4703688621521, 'training/entropy_loss': Array(-0.03508495, dtype=float32), 'training/policy_loss': Array(0.00883095, dtype=float32), 'training/total_loss': Array(7329.092, dtype=float32), 'training/v_loss': Array(7329.118, dtype=float32), 'eval/episode_goal_distance': (Array(0.3108858, dtype=float32), Array(0.04418336, dtype=float32)), 'eval/episode_reward': (Array(-46173.85, dtype=float32), Array(19785.787, dtype=float32)), 'eval/avg_episode_length': (Array(860.3281, dtype=float32), Array(345.27917, dtype=float32)), 'eval/epoch_eval_time': 4.0451154708862305, 'eval/sps': 31643.101642276957}
I0726 23:36:24.895312 140267183036224 train.py:379] starting iteration 29 243.16034197807312
I0726 23:36:31.984425 140267183036224 train.py:394] {'eval/walltime': 134.61370515823364, 'training/sps': 40438.27416228817, 'training/walltime': 111.5090742111206, 'training/entropy_loss': Array(-0.03639967, dtype=float32), 'training/policy_loss': Array(0.00778038, dtype=float32), 'training/total_loss': Array(6911.74, dtype=float32), 'training/v_loss': Array(6911.769, dtype=float32), 'eval/episode_goal_distance': (Array(0.31212002, dtype=float32), Array(0.04287973, dtype=float32)), 'eval/episode_reward': (Array(-46254.027, dtype=float32), Array(19043.736, dtype=float32)), 'eval/avg_episode_length': (Array(875.6797, dtype=float32), Array(328.9211, dtype=float32)), 'eval/epoch_eval_time': 4.046983480453491, 'eval/sps': 31628.495796493036}
I0726 23:36:31.986712 140267183036224 train.py:379] starting iteration 30 250.25174164772034
I0726 23:36:39.090329 140267183036224 train.py:394] {'eval/walltime': 138.66592693328857, 'training/sps': 40317.441292070405, 'training/walltime': 114.55688667297363, 'training/entropy_loss': Array(-0.04031217, dtype=float32), 'training/policy_loss': Array(0.00675265, dtype=float32), 'training/total_loss': Array(6585.404, dtype=float32), 'training/v_loss': Array(6585.4375, dtype=float32), 'eval/episode_goal_distance': (Array(0.3063056, dtype=float32), Array(0.04307222, dtype=float32)), 'eval/episode_reward': (Array(-47213.387, dtype=float32), Array(17758.062, dtype=float32)), 'eval/avg_episode_length': (Array(891.3047, dtype=float32), Array(310.1702, dtype=float32)), 'eval/epoch_eval_time': 4.052221775054932, 'eval/sps': 31587.609737442082}
I0726 23:36:39.092772 140267183036224 train.py:379] starting iteration 31 257.35780215263367
I0726 23:36:46.203938 140267183036224 train.py:394] {'eval/walltime': 142.71796298027039, 'training/sps': 40216.201222366464, 'training/walltime': 117.61237168312073, 'training/entropy_loss': Array(-0.04170254, dtype=float32), 'training/policy_loss': Array(0.0042051, dtype=float32), 'training/total_loss': Array(6509.45, dtype=float32), 'training/v_loss': Array(6509.4873, dtype=float32), 'eval/episode_goal_distance': (Array(0.30620062, dtype=float32), Array(0.0412867, dtype=float32)), 'eval/episode_reward': (Array(-46294.36, dtype=float32), Array(18044.865, dtype=float32)), 'eval/avg_episode_length': (Array(883.4453, dtype=float32), Array(319.90747, dtype=float32)), 'eval/epoch_eval_time': 4.0520360469818115, 'eval/sps': 31589.05757892794}
I0726 23:36:46.206341 140267183036224 train.py:379] starting iteration 32 264.4713706970215
I0726 23:36:53.302082 140267183036224 train.py:394] {'eval/walltime': 146.76278591156006, 'training/sps': 40324.595555466534, 'training/walltime': 120.65964341163635, 'training/entropy_loss': Array(-0.04280365, dtype=float32), 'training/policy_loss': Array(0.00480172, dtype=float32), 'training/total_loss': Array(5902.586, dtype=float32), 'training/v_loss': Array(5902.624, dtype=float32), 'eval/episode_goal_distance': (Array(0.31211117, dtype=float32), Array(0.03858151, dtype=float32)), 'eval/episode_reward': (Array(-49252.6, dtype=float32), Array(16159.544, dtype=float32)), 'eval/avg_episode_length': (Array(922.3828, dtype=float32), Array(266.62442, dtype=float32)), 'eval/epoch_eval_time': 4.044822931289673, 'eval/sps': 31645.390212220685}
I0726 23:36:53.304521 140267183036224 train.py:379] starting iteration 33 271.5695505142212
I0726 23:37:00.409648 140267183036224 train.py:394] {'eval/walltime': 150.81542229652405, 'training/sps': 40304.83288029804, 'training/walltime': 123.7084093093872, 'training/entropy_loss': Array(-0.04227335, dtype=float32), 'training/policy_loss': Array(0.00255062, dtype=float32), 'training/total_loss': Array(13678.258, dtype=float32), 'training/v_loss': Array(13678.298, dtype=float32), 'eval/episode_goal_distance': (Array(0.30004883, dtype=float32), Array(0.03922345, dtype=float32)), 'eval/episode_reward': (Array(-47109.64, dtype=float32), Array(16374.394, dtype=float32)), 'eval/avg_episode_length': (Array(906.8594, dtype=float32), Array(289.5863, dtype=float32)), 'eval/epoch_eval_time': 4.052636384963989, 'eval/sps': 31584.378128495082}
I0726 23:37:00.412030 140267183036224 train.py:379] starting iteration 34 278.6770598888397
I0726 23:37:07.516629 140267183036224 train.py:394] {'eval/walltime': 154.86288714408875, 'training/sps': 40241.53816604001, 'training/walltime': 126.76197052001953, 'training/entropy_loss': Array(-0.04051712, dtype=float32), 'training/policy_loss': Array(0.00305986, dtype=float32), 'training/total_loss': Array(8190.905, dtype=float32), 'training/v_loss': Array(8190.9424, dtype=float32), 'eval/episode_goal_distance': (Array(0.30116242, dtype=float32), Array(0.04301111, dtype=float32)), 'eval/episode_reward': (Array(-46867.72, dtype=float32), Array(16870.822, dtype=float32)), 'eval/avg_episode_length': (Array(906.9453, dtype=float32), Array(289.31995, dtype=float32)), 'eval/epoch_eval_time': 4.047464847564697, 'eval/sps': 31624.734202945776}
I0726 23:37:07.519033 140267183036224 train.py:379] starting iteration 35 285.78406286239624
I0726 23:37:14.624824 140267183036224 train.py:394] {'eval/walltime': 158.91162633895874, 'training/sps': 40243.26634764356, 'training/walltime': 129.81540060043335, 'training/entropy_loss': Array(-0.03776108, dtype=float32), 'training/policy_loss': Array(0.00166593, dtype=float32), 'training/total_loss': Array(6185.492, dtype=float32), 'training/v_loss': Array(6185.5283, dtype=float32), 'eval/episode_goal_distance': (Array(0.3073619, dtype=float32), Array(0.04031, dtype=float32)), 'eval/episode_reward': (Array(-48223.344, dtype=float32), Array(16772.299, dtype=float32)), 'eval/avg_episode_length': (Array(906.7656, dtype=float32), Array(289.8778, dtype=float32)), 'eval/epoch_eval_time': 4.048739194869995, 'eval/sps': 31614.780265961308}
I0726 23:37:14.627354 140267183036224 train.py:379] starting iteration 36 292.8923840522766
I0726 23:37:21.741622 140267183036224 train.py:394] {'eval/walltime': 162.97373366355896, 'training/sps': 40308.473659611824, 'training/walltime': 132.86389112472534, 'training/entropy_loss': Array(-0.03441365, dtype=float32), 'training/policy_loss': Array(0.00178708, dtype=float32), 'training/total_loss': Array(5942.1396, dtype=float32), 'training/v_loss': Array(5942.173, dtype=float32), 'eval/episode_goal_distance': (Array(0.29804814, dtype=float32), Array(0.03856416, dtype=float32)), 'eval/episode_reward': (Array(-46114.902, dtype=float32), Array(18167.457, dtype=float32)), 'eval/avg_episode_length': (Array(883.5, dtype=float32), Array(319.75705, dtype=float32)), 'eval/epoch_eval_time': 4.06210732460022, 'eval/sps': 31510.738090258947}
I0726 23:37:21.744008 140267183036224 train.py:379] starting iteration 37 300.00903725624084
I0726 23:37:28.853189 140267183036224 train.py:394] {'eval/walltime': 167.03019618988037, 'training/sps': 40300.35136950691, 'training/walltime': 135.91299605369568, 'training/entropy_loss': Array(-0.03074235, dtype=float32), 'training/policy_loss': Array(0.00293147, dtype=float32), 'training/total_loss': Array(5356.2744, dtype=float32), 'training/v_loss': Array(5356.3022, dtype=float32), 'eval/episode_goal_distance': (Array(0.30492437, dtype=float32), Array(0.03768711, dtype=float32)), 'eval/episode_reward': (Array(-44909.832, dtype=float32), Array(17445.484, dtype=float32)), 'eval/avg_episode_length': (Array(883.5156, dtype=float32), Array(319.71432, dtype=float32)), 'eval/epoch_eval_time': 4.056462526321411, 'eval/sps': 31554.587074190564}
I0726 23:37:28.855471 140267183036224 train.py:379] starting iteration 38 307.12050104141235
I0726 23:37:35.967350 140267183036224 train.py:394] {'eval/walltime': 171.07889795303345, 'training/sps': 40170.869780777146, 'training/walltime': 138.97192907333374, 'training/entropy_loss': Array(-0.0280929, dtype=float32), 'training/policy_loss': Array(0.00273914, dtype=float32), 'training/total_loss': Array(5418.997, dtype=float32), 'training/v_loss': Array(5419.0225, dtype=float32), 'eval/episode_goal_distance': (Array(0.30240697, dtype=float32), Array(0.03894318, dtype=float32)), 'eval/episode_reward': (Array(-46683.844, dtype=float32), Array(17575.158, dtype=float32)), 'eval/avg_episode_length': (Array(891.2344, dtype=float32), Array(310.37094, dtype=float32)), 'eval/epoch_eval_time': 4.048701763153076, 'eval/sps': 31615.07255607666}
I0726 23:37:35.969912 140267183036224 train.py:379] starting iteration 39 314.2349417209625
I0726 23:37:43.069948 140267183036224 train.py:394] {'eval/walltime': 175.12646436691284, 'training/sps': 40310.05941672918, 'training/walltime': 142.02029967308044, 'training/entropy_loss': Array(-0.02489276, dtype=float32), 'training/policy_loss': Array(0.00404399, dtype=float32), 'training/total_loss': Array(4916.7734, dtype=float32), 'training/v_loss': Array(4916.7944, dtype=float32), 'eval/episode_goal_distance': (Array(0.2924077, dtype=float32), Array(0.03797517, dtype=float32)), 'eval/episode_reward': (Array(-43544.836, dtype=float32), Array(19150.594, dtype=float32)), 'eval/avg_episode_length': (Array(852.47656, dtype=float32), Array(353.34433, dtype=float32)), 'eval/epoch_eval_time': 4.0475664138793945, 'eval/sps': 31623.94063778147}
I0726 23:37:43.072444 140267183036224 train.py:379] starting iteration 40 321.3374743461609
I0726 23:37:50.182910 140267183036224 train.py:394] {'eval/walltime': 179.17978262901306, 'training/sps': 40247.075155464685, 'training/walltime': 145.0734407901764, 'training/entropy_loss': Array(-0.02303197, dtype=float32), 'training/policy_loss': Array(0.00320303, dtype=float32), 'training/total_loss': Array(5088.849, dtype=float32), 'training/v_loss': Array(5088.869, dtype=float32), 'eval/episode_goal_distance': (Array(0.2999184, dtype=float32), Array(0.04073624, dtype=float32)), 'eval/episode_reward': (Array(-47134.98, dtype=float32), Array(15997.737, dtype=float32)), 'eval/avg_episode_length': (Array(914.5469, dtype=float32), Array(278.69244, dtype=float32)), 'eval/epoch_eval_time': 4.05331826210022, 'eval/sps': 31579.06478670565}
I0726 23:37:50.185477 140267183036224 train.py:379] starting iteration 41 328.4505069255829
I0726 23:37:57.300659 140267183036224 train.py:394] {'eval/walltime': 183.23375821113586, 'training/sps': 40194.30636530118, 'training/walltime': 148.1305902004242, 'training/entropy_loss': Array(-0.02151527, dtype=float32), 'training/policy_loss': Array(0.0029733, dtype=float32), 'training/total_loss': Array(4888.9316, dtype=float32), 'training/v_loss': Array(4888.95, dtype=float32), 'eval/episode_goal_distance': (Array(0.2936862, dtype=float32), Array(0.04263304, dtype=float32)), 'eval/episode_reward': (Array(-44875.344, dtype=float32), Array(17794.68, dtype=float32)), 'eval/avg_episode_length': (Array(883.53125, dtype=float32), Array(319.6717, dtype=float32)), 'eval/epoch_eval_time': 4.053975582122803, 'eval/sps': 31573.94449153903}
I0726 23:37:57.303119 140267183036224 train.py:379] starting iteration 42 335.56814908981323
I0726 23:38:04.413003 140267183036224 train.py:394] {'eval/walltime': 187.2812044620514, 'training/sps': 40176.40926181639, 'training/walltime': 151.18910145759583, 'training/entropy_loss': Array(-0.02115176, dtype=float32), 'training/policy_loss': Array(0.00200515, dtype=float32), 'training/total_loss': Array(4712.5366, dtype=float32), 'training/v_loss': Array(4712.5557, dtype=float32), 'eval/episode_goal_distance': (Array(0.2989815, dtype=float32), Array(0.03963694, dtype=float32)), 'eval/episode_reward': (Array(-42209.977, dtype=float32), Array(20822.81, dtype=float32)), 'eval/avg_episode_length': (Array(821.2969, dtype=float32), Array(381.82446, dtype=float32)), 'eval/epoch_eval_time': 4.047446250915527, 'eval/sps': 31624.879507923437}
I0726 23:38:04.415369 140267183036224 train.py:379] starting iteration 43 342.68039894104004
I0726 23:38:11.509144 140267183036224 train.py:394] {'eval/walltime': 191.32896518707275, 'training/sps': 40388.772362396776, 'training/walltime': 154.23153114318848, 'training/entropy_loss': Array(-0.02103176, dtype=float32), 'training/policy_loss': Array(0.00287747, dtype=float32), 'training/total_loss': Array(4671.2197, dtype=float32), 'training/v_loss': Array(4671.2383, dtype=float32), 'eval/episode_goal_distance': (Array(0.28824592, dtype=float32), Array(0.04300651, dtype=float32)), 'eval/episode_reward': (Array(-42247.758, dtype=float32), Array(19983.4, dtype=float32)), 'eval/avg_episode_length': (Array(836.91406, dtype=float32), Array(368.12885, dtype=float32)), 'eval/epoch_eval_time': 4.047760725021362, 'eval/sps': 31622.422543102388}
I0726 23:38:11.511538 140267183036224 train.py:379] starting iteration 44 349.7765681743622
I0726 23:38:18.625504 140267183036224 train.py:394] {'eval/walltime': 195.3858368396759, 'training/sps': 40242.95212176607, 'training/walltime': 157.2849850654602, 'training/entropy_loss': Array(-0.02006304, dtype=float32), 'training/policy_loss': Array(0.00206446, dtype=float32), 'training/total_loss': Array(4386.9136, dtype=float32), 'training/v_loss': Array(4386.9316, dtype=float32), 'eval/episode_goal_distance': (Array(0.28699157, dtype=float32), Array(0.03891484, dtype=float32)), 'eval/episode_reward': (Array(-42701.14, dtype=float32), Array(19117.543, dtype=float32)), 'eval/avg_episode_length': (Array(852.46094, dtype=float32), Array(353.382, dtype=float32)), 'eval/epoch_eval_time': 4.056871652603149, 'eval/sps': 31551.404865832268}
I0726 23:38:18.628017 140267183036224 train.py:379] starting iteration 45 356.8930470943451
I0726 23:38:25.740023 140267183036224 train.py:394] {'eval/walltime': 199.44057893753052, 'training/sps': 40239.94208652169, 'training/walltime': 160.3386673927307, 'training/entropy_loss': Array(-0.01872362, dtype=float32), 'training/policy_loss': Array(0.00154019, dtype=float32), 'training/total_loss': Array(4151.8833, dtype=float32), 'training/v_loss': Array(4151.9004, dtype=float32), 'eval/episode_goal_distance': (Array(0.28960228, dtype=float32), Array(0.03712726, dtype=float32)), 'eval/episode_reward': (Array(-40822.758, dtype=float32), Array(20634.514, dtype=float32)), 'eval/avg_episode_length': (Array(813.6875, dtype=float32), Array(387.8411, dtype=float32)), 'eval/epoch_eval_time': 4.054742097854614, 'eval/sps': 31567.975696339723}
I0726 23:38:25.742446 140267183036224 train.py:379] starting iteration 46 364.00747537612915
I0726 23:38:32.848646 140267183036224 train.py:394] {'eval/walltime': 203.49243474006653, 'training/sps': 40276.62417427017, 'training/walltime': 163.389568567276, 'training/entropy_loss': Array(-0.01769577, dtype=float32), 'training/policy_loss': Array(0.00237683, dtype=float32), 'training/total_loss': Array(4082.8367, dtype=float32), 'training/v_loss': Array(4082.852, dtype=float32), 'eval/episode_goal_distance': (Array(0.2895898, dtype=float32), Array(0.03442552, dtype=float32)), 'eval/episode_reward': (Array(-43835.617, dtype=float32), Array(17864.006, dtype=float32)), 'eval/avg_episode_length': (Array(875.78125, dtype=float32), Array(328.65277, dtype=float32)), 'eval/epoch_eval_time': 4.051855802536011, 'eval/sps': 31590.462799758632}
I0726 23:38:32.851176 140267183036224 train.py:379] starting iteration 47 371.11620688438416
I0726 23:38:39.960885 140267183036224 train.py:394] {'eval/walltime': 207.55740070343018, 'training/sps': 40412.39108860002, 'training/walltime': 166.4302201271057, 'training/entropy_loss': Array(-0.01654229, dtype=float32), 'training/policy_loss': Array(0.0015269, dtype=float32), 'training/total_loss': Array(4008.7441, dtype=float32), 'training/v_loss': Array(4008.759, dtype=float32), 'eval/episode_goal_distance': (Array(0.2943722, dtype=float32), Array(0.03339966, dtype=float32)), 'eval/episode_reward': (Array(-48029.984, dtype=float32), Array(14710.188, dtype=float32)), 'eval/avg_episode_length': (Array(930.10156, dtype=float32), Array(254.16788, dtype=float32)), 'eval/epoch_eval_time': 4.0649659633636475, 'eval/sps': 31488.578540048467}
I0726 23:38:39.963437 140267183036224 train.py:379] starting iteration 48 378.22846722602844
I0726 23:38:47.078874 140267183036224 train.py:394] {'eval/walltime': 211.6214427947998, 'training/sps': 40318.68080402351, 'training/walltime': 169.47793889045715, 'training/entropy_loss': Array(-0.0153198, dtype=float32), 'training/policy_loss': Array(0.00102298, dtype=float32), 'training/total_loss': Array(3934.1228, dtype=float32), 'training/v_loss': Array(3934.1372, dtype=float32), 'eval/episode_goal_distance': (Array(0.2884701, dtype=float32), Array(0.04388227, dtype=float32)), 'eval/episode_reward': (Array(-46467.797, dtype=float32), Array(18096.146, dtype=float32)), 'eval/avg_episode_length': (Array(891.3672, dtype=float32), Array(309.99197, dtype=float32)), 'eval/epoch_eval_time': 4.064042091369629, 'eval/sps': 31495.736786737492}
I0726 23:38:47.081515 140267183036224 train.py:379] starting iteration 49 385.3465452194214
I0726 23:38:54.192792 140267183036224 train.py:394] {'eval/walltime': 215.67775964736938, 'training/sps': 40270.37107312058, 'training/walltime': 172.52931380271912, 'training/entropy_loss': Array(-0.01325854, dtype=float32), 'training/policy_loss': Array(0.00077247, dtype=float32), 'training/total_loss': Array(3915.03, dtype=float32), 'training/v_loss': Array(3915.0425, dtype=float32), 'eval/episode_goal_distance': (Array(0.28931794, dtype=float32), Array(0.03789283, dtype=float32)), 'eval/episode_reward': (Array(-46031.68, dtype=float32), Array(16886.102, dtype=float32)), 'eval/avg_episode_length': (Array(899.0625, dtype=float32), Array(300.21368, dtype=float32)), 'eval/epoch_eval_time': 4.05631685256958, 'eval/sps': 31555.720288200624}
I0726 23:38:54.195240 140267183036224 train.py:379] starting iteration 50 392.4602699279785
I0726 23:39:01.292770 140267183036224 train.py:394] {'eval/walltime': 219.73576021194458, 'training/sps': 40476.021388842295, 'training/walltime': 175.56518530845642, 'training/entropy_loss': Array(-0.01477715, dtype=float32), 'training/policy_loss': Array(0.00080581, dtype=float32), 'training/total_loss': Array(13943.277, dtype=float32), 'training/v_loss': Array(13943.293, dtype=float32), 'eval/episode_goal_distance': (Array(0.2926132, dtype=float32), Array(0.03921579, dtype=float32)), 'eval/episode_reward': (Array(-41579.562, dtype=float32), Array(20533.795, dtype=float32)), 'eval/avg_episode_length': (Array(821.52344, dtype=float32), Array(381.34064, dtype=float32)), 'eval/epoch_eval_time': 4.058000564575195, 'eval/sps': 31542.627449929754}
I0726 23:39:01.295310 140267183036224 train.py:379] starting iteration 51 399.56033968925476
I0726 23:39:08.426136 140267183036224 train.py:394] {'eval/walltime': 223.80846285820007, 'training/sps': 40229.740223489855, 'training/walltime': 178.61964201927185, 'training/entropy_loss': Array(-0.01434378, dtype=float32), 'training/policy_loss': Array(0.00035099, dtype=float32), 'training/total_loss': Array(6144.876, dtype=float32), 'training/v_loss': Array(6144.8906, dtype=float32), 'eval/episode_goal_distance': (Array(0.29111987, dtype=float32), Array(0.04125572, dtype=float32)), 'eval/episode_reward': (Array(-38650.707, dtype=float32), Array(22714.576, dtype=float32)), 'eval/avg_episode_length': (Array(759.14844, dtype=float32), Array(426.04483, dtype=float32)), 'eval/epoch_eval_time': 4.072702646255493, 'eval/sps': 31428.761468182613}
I0726 23:39:08.478218 140267183036224 train.py:379] starting iteration 52 406.7432315349579
I0726 23:39:15.605656 140267183036224 train.py:394] {'eval/walltime': 227.8820514678955, 'training/sps': 40291.98350877366, 'training/walltime': 181.66938018798828, 'training/entropy_loss': Array(-0.01561643, dtype=float32), 'training/policy_loss': Array(0.00070655, dtype=float32), 'training/total_loss': Array(4849.8193, dtype=float32), 'training/v_loss': Array(4849.834, dtype=float32), 'eval/episode_goal_distance': (Array(0.28957897, dtype=float32), Array(0.03799724, dtype=float32)), 'eval/episode_reward': (Array(-44905.484, dtype=float32), Array(16496.545, dtype=float32)), 'eval/avg_episode_length': (Array(898.96875, dtype=float32), Array(300.49246, dtype=float32)), 'eval/epoch_eval_time': 4.073588609695435, 'eval/sps': 31421.926037241654}
I0726 23:39:15.608238 140267183036224 train.py:379] starting iteration 53 413.87326741218567
I0726 23:39:22.730091 140267183036224 train.py:394] {'eval/walltime': 231.95398569107056, 'training/sps': 40338.785412933306, 'training/walltime': 184.71557998657227, 'training/entropy_loss': Array(-0.01633362, dtype=float32), 'training/policy_loss': Array(-0.00014401, dtype=float32), 'training/total_loss': Array(4242.16, dtype=float32), 'training/v_loss': Array(4242.177, dtype=float32), 'eval/episode_goal_distance': (Array(0.28836006, dtype=float32), Array(0.0382827, dtype=float32)), 'eval/episode_reward': (Array(-42416.92, dtype=float32), Array(18491.377, dtype=float32)), 'eval/avg_episode_length': (Array(860.1875, dtype=float32), Array(345.62625, dtype=float32)), 'eval/epoch_eval_time': 4.071934223175049, 'eval/sps': 31434.69245438678}
I0726 23:39:22.732479 140267183036224 train.py:379] starting iteration 54 420.99750900268555
I0726 23:39:29.856405 140267183036224 train.py:394] {'eval/walltime': 236.02490997314453, 'training/sps': 40296.55767670515, 'training/walltime': 187.76497197151184, 'training/entropy_loss': Array(-0.01443238, dtype=float32), 'training/policy_loss': Array(2.1477284e-05, dtype=float32), 'training/total_loss': Array(4022.842, dtype=float32), 'training/v_loss': Array(4022.8564, dtype=float32), 'eval/episode_goal_distance': (Array(0.28805882, dtype=float32), Array(0.03757133, dtype=float32)), 'eval/episode_reward': (Array(-44342.17, dtype=float32), Array(18420.674, dtype=float32)), 'eval/avg_episode_length': (Array(867.875, dtype=float32), Array(337.61566, dtype=float32)), 'eval/epoch_eval_time': 4.070924282073975, 'eval/sps': 31442.490975240904}
I0726 23:39:29.858812 140267183036224 train.py:379] starting iteration 55 428.12384152412415
I0726 23:39:36.992566 140267183036224 train.py:394] {'eval/walltime': 240.0998466014862, 'training/sps': 40221.175659713466, 'training/walltime': 190.82007908821106, 'training/entropy_loss': Array(-0.01339489, dtype=float32), 'training/policy_loss': Array(0.00014599, dtype=float32), 'training/total_loss': Array(3635.003, dtype=float32), 'training/v_loss': Array(3635.0159, dtype=float32), 'eval/episode_goal_distance': (Array(0.29129797, dtype=float32), Array(0.04095681, dtype=float32)), 'eval/episode_reward': (Array(-46571.766, dtype=float32), Array(17049.783, dtype=float32)), 'eval/avg_episode_length': (Array(899.09375, dtype=float32), Array(300.1209, dtype=float32)), 'eval/epoch_eval_time': 4.074936628341675, 'eval/sps': 31411.531435788373}
I0726 23:39:36.995004 140267183036224 train.py:379] starting iteration 56 435.26003432273865
I0726 23:39:44.100179 140267183036224 train.py:394] {'eval/walltime': 244.1667058467865, 'training/sps': 40491.74952873184, 'training/walltime': 193.85477137565613, 'training/entropy_loss': Array(-0.01246596, dtype=float32), 'training/policy_loss': Array(-0.00027134, dtype=float32), 'training/total_loss': Array(3562.6743, dtype=float32), 'training/v_loss': Array(3562.687, dtype=float32), 'eval/episode_goal_distance': (Array(0.28453478, dtype=float32), Array(0.04016075, dtype=float32)), 'eval/episode_reward': (Array(-43622.605, dtype=float32), Array(18141.92, dtype=float32)), 'eval/avg_episode_length': (Array(875.6875, dtype=float32), Array(328.90067, dtype=float32)), 'eval/epoch_eval_time': 4.066859245300293, 'eval/sps': 31473.919375970094}
I0726 23:39:44.102636 140267183036224 train.py:379] starting iteration 57 442.3676657676697
I0726 23:39:51.232613 140267183036224 train.py:394] {'eval/walltime': 248.24461793899536, 'training/sps': 40319.406253498346, 'training/walltime': 196.90243530273438, 'training/entropy_loss': Array(-0.01170397, dtype=float32), 'training/policy_loss': Array(9.482348e-06, dtype=float32), 'training/total_loss': Array(3294.9504, dtype=float32), 'training/v_loss': Array(3294.962, dtype=float32), 'eval/episode_goal_distance': (Array(0.2894485, dtype=float32), Array(0.03617227, dtype=float32)), 'eval/episode_reward': (Array(-42813.18, dtype=float32), Array(18953.951, dtype=float32)), 'eval/avg_episode_length': (Array(852.3906, dtype=float32), Array(353.5502, dtype=float32)), 'eval/epoch_eval_time': 4.077912092208862, 'eval/sps': 31388.611893952544}
I0726 23:39:51.235075 140267183036224 train.py:379] starting iteration 58 449.50010442733765
I0726 23:39:58.370311 140267183036224 train.py:394] {'eval/walltime': 252.32135820388794, 'training/sps': 40226.26121450205, 'training/walltime': 199.95715618133545, 'training/entropy_loss': Array(-0.01313633, dtype=float32), 'training/policy_loss': Array(-0.00046033, dtype=float32), 'training/total_loss': Array(3230.2625, dtype=float32), 'training/v_loss': Array(3230.2761, dtype=float32), 'eval/episode_goal_distance': (Array(0.28482956, dtype=float32), Array(0.03490976, dtype=float32)), 'eval/episode_reward': (Array(-46486.4, dtype=float32), Array(15327.878, dtype=float32)), 'eval/avg_episode_length': (Array(922.3047, dtype=float32), Array(266.89252, dtype=float32)), 'eval/epoch_eval_time': 4.076740264892578, 'eval/sps': 31397.634306578224}
I0726 23:39:58.375185 140267183036224 train.py:379] starting iteration 59 456.6402008533478
I0726 23:40:05.516146 140267183036224 train.py:394] {'eval/walltime': 256.4018406867981, 'training/sps': 40202.85321681437, 'training/walltime': 203.01365566253662, 'training/entropy_loss': Array(-0.01107473, dtype=float32), 'training/policy_loss': Array(0.00042664, dtype=float32), 'training/total_loss': Array(3152.1545, dtype=float32), 'training/v_loss': Array(3152.165, dtype=float32), 'eval/episode_goal_distance': (Array(0.28632426, dtype=float32), Array(0.03614956, dtype=float32)), 'eval/episode_reward': (Array(-42659.53, dtype=float32), Array(19151.457, dtype=float32)), 'eval/avg_episode_length': (Array(852.4453, dtype=float32), Array(353.4197, dtype=float32)), 'eval/epoch_eval_time': 4.080482482910156, 'eval/sps': 31368.83947819616}
I0726 23:40:05.518532 140267183036224 train.py:379] starting iteration 60 463.78356170654297
I0726 23:40:12.655665 140267183036224 train.py:394] {'eval/walltime': 260.480516910553, 'training/sps': 40225.78085711224, 'training/walltime': 206.0684130191803, 'training/entropy_loss': Array(-0.00996783, dtype=float32), 'training/policy_loss': Array(0.00074687, dtype=float32), 'training/total_loss': Array(3071.4194, dtype=float32), 'training/v_loss': Array(3071.4287, dtype=float32), 'eval/episode_goal_distance': (Array(0.2853064, dtype=float32), Array(0.03793851, dtype=float32)), 'eval/episode_reward': (Array(-39128.273, dtype=float32), Array(21148.363, dtype=float32)), 'eval/avg_episode_length': (Array(790.3906, dtype=float32), Array(405.40668, dtype=float32)), 'eval/epoch_eval_time': 4.078676223754883, 'eval/sps': 31382.731302501263}
I0726 23:40:12.658078 140267183036224 train.py:379] starting iteration 61 470.92310786247253
I0726 23:40:19.796565 140267183036224 train.py:394] {'eval/walltime': 264.56031799316406, 'training/sps': 40222.30881088324, 'training/walltime': 209.12343406677246, 'training/entropy_loss': Array(-0.00811676, dtype=float32), 'training/policy_loss': Array(0.00047745, dtype=float32), 'training/total_loss': Array(3062.0469, dtype=float32), 'training/v_loss': Array(3062.0547, dtype=float32), 'eval/episode_goal_distance': (Array(0.2848214, dtype=float32), Array(0.03607632, dtype=float32)), 'eval/episode_reward': (Array(-42754.984, dtype=float32), Array(18398.623, dtype=float32)), 'eval/avg_episode_length': (Array(860.21094, dtype=float32), Array(345.56836, dtype=float32)), 'eval/epoch_eval_time': 4.079801082611084, 'eval/sps': 31374.078639657513}
I0726 23:40:19.799030 140267183036224 train.py:379] starting iteration 62 478.06405997276306
I0726 23:40:26.930449 140267183036224 train.py:394] {'eval/walltime': 268.64278507232666, 'training/sps': 40352.602971756176, 'training/walltime': 212.16859078407288, 'training/entropy_loss': Array(-0.00800981, dtype=float32), 'training/policy_loss': Array(0.00010591, dtype=float32), 'training/total_loss': Array(3002.6729, dtype=float32), 'training/v_loss': Array(3002.6807, dtype=float32), 'eval/episode_goal_distance': (Array(0.285272, dtype=float32), Array(0.04137259, dtype=float32)), 'eval/episode_reward': (Array(-41798.164, dtype=float32), Array(19682.715, dtype=float32)), 'eval/avg_episode_length': (Array(836.84375, dtype=float32), Array(368.28726, dtype=float32)), 'eval/epoch_eval_time': 4.082467079162598, 'eval/sps': 31353.590247751752}
I0726 23:40:26.932920 140267183036224 train.py:379] starting iteration 63 485.197949886322
I0726 23:40:34.085277 140267183036224 train.py:394] {'eval/walltime': 272.74751257896423, 'training/sps': 40370.05661955303, 'training/walltime': 215.2124309539795, 'training/entropy_loss': Array(-0.00882566, dtype=float32), 'training/policy_loss': Array(0.00014796, dtype=float32), 'training/total_loss': Array(2928.934, dtype=float32), 'training/v_loss': Array(2928.9429, dtype=float32), 'eval/episode_goal_distance': (Array(0.28397483, dtype=float32), Array(0.03877866, dtype=float32)), 'eval/episode_reward': (Array(-39915.336, dtype=float32), Array(21174.453, dtype=float32)), 'eval/avg_episode_length': (Array(798.1094, dtype=float32), Array(399.88138, dtype=float32)), 'eval/epoch_eval_time': 4.104727506637573, 'eval/sps': 31183.55598344028}
I0726 23:40:34.087773 140267183036224 train.py:379] starting iteration 64 492.3528027534485
I0726 23:40:41.237117 140267183036224 train.py:394] {'eval/walltime': 276.84089279174805, 'training/sps': 40258.77320413767, 'training/walltime': 218.2646849155426, 'training/entropy_loss': Array(-0.00835517, dtype=float32), 'training/policy_loss': Array(-4.9499635e-05, dtype=float32), 'training/total_loss': Array(2845.4417, dtype=float32), 'training/v_loss': Array(2845.4502, dtype=float32), 'eval/episode_goal_distance': (Array(0.29331294, dtype=float32), Array(0.03430552, dtype=float32)), 'eval/episode_reward': (Array(-41337.57, dtype=float32), Array(21457.596, dtype=float32)), 'eval/avg_episode_length': (Array(805.8203, dtype=float32), Array(394.1428, dtype=float32)), 'eval/epoch_eval_time': 4.0933802127838135, 'eval/sps': 31270.00018230658}
I0726 23:40:41.239595 140267183036224 train.py:379] starting iteration 65 499.504625082016
I0726 23:40:48.387292 140267183036224 train.py:394] {'eval/walltime': 280.93753933906555, 'training/sps': 40322.25152905349, 'training/walltime': 221.3121337890625, 'training/entropy_loss': Array(-0.00901543, dtype=float32), 'training/policy_loss': Array(-0.00024334, dtype=float32), 'training/total_loss': Array(2809.7253, dtype=float32), 'training/v_loss': Array(2809.7344, dtype=float32), 'eval/episode_goal_distance': (Array(0.2899403, dtype=float32), Array(0.03916857, dtype=float32)), 'eval/episode_reward': (Array(-41755.46, dtype=float32), Array(21572.826, dtype=float32)), 'eval/avg_episode_length': (Array(805.8125, dtype=float32), Array(394.15872, dtype=float32)), 'eval/epoch_eval_time': 4.096646547317505, 'eval/sps': 31245.068013938067}
I0726 23:40:48.389663 140267183036224 train.py:379] starting iteration 66 506.65469312667847
I0726 23:40:55.532536 140267183036224 train.py:394] {'eval/walltime': 285.0239362716675, 'training/sps': 40250.87525390303, 'training/walltime': 224.3649866580963, 'training/entropy_loss': Array(-0.01192117, dtype=float32), 'training/policy_loss': Array(-2.6120953e-05, dtype=float32), 'training/total_loss': Array(9550.307, dtype=float32), 'training/v_loss': Array(9550.318, dtype=float32), 'eval/episode_goal_distance': (Array(0.28243175, dtype=float32), Array(0.03861415, dtype=float32)), 'eval/episode_reward': (Array(-39336.04, dtype=float32), Array(20873.902, dtype=float32)), 'eval/avg_episode_length': (Array(798.0078, dtype=float32), Array(400.0817, dtype=float32)), 'eval/epoch_eval_time': 4.086396932601929, 'eval/sps': 31323.437764646776}
I0726 23:40:55.535192 140267183036224 train.py:379] starting iteration 67 513.8002219200134
I0726 23:41:02.690818 140267183036224 train.py:394] {'eval/walltime': 289.11899065971375, 'training/sps': 40198.10591072646, 'training/walltime': 227.42184710502625, 'training/entropy_loss': Array(-0.01376652, dtype=float32), 'training/policy_loss': Array(-0.00024206, dtype=float32), 'training/total_loss': Array(6604.907, dtype=float32), 'training/v_loss': Array(6604.9214, dtype=float32), 'eval/episode_goal_distance': (Array(0.289348, dtype=float32), Array(0.03411914, dtype=float32)), 'eval/episode_reward': (Array(-38634.727, dtype=float32), Array(22393.377, dtype=float32)), 'eval/avg_episode_length': (Array(767.0547, dtype=float32), Array(421.0243, dtype=float32)), 'eval/epoch_eval_time': 4.095054388046265, 'eval/sps': 31257.216112596816}
I0726 23:41:02.693352 140267183036224 train.py:379] starting iteration 68 520.958381652832
I0726 23:41:09.833975 140267183036224 train.py:394] {'eval/walltime': 293.2074694633484, 'training/sps': 40308.426372547954, 'training/walltime': 230.47034120559692, 'training/entropy_loss': Array(-0.01330014, dtype=float32), 'training/policy_loss': Array(-0.00037738, dtype=float32), 'training/total_loss': Array(4590.6997, dtype=float32), 'training/v_loss': Array(4590.7134, dtype=float32), 'eval/episode_goal_distance': (Array(0.2814417, dtype=float32), Array(0.03536411, dtype=float32)), 'eval/episode_reward': (Array(-44692.406, dtype=float32), Array(17304.525, dtype=float32)), 'eval/avg_episode_length': (Array(891.3281, dtype=float32), Array(310.1034, dtype=float32)), 'eval/epoch_eval_time': 4.0884788036346436, 'eval/sps': 31307.48773509806}
I0726 23:41:09.836643 140267183036224 train.py:379] starting iteration 69 528.101672410965
I0726 23:41:16.986823 140267183036224 train.py:394] {'eval/walltime': 297.3048701286316, 'training/sps': 40299.6045479318, 'training/walltime': 233.5195026397705, 'training/entropy_loss': Array(-0.01271733, dtype=float32), 'training/policy_loss': Array(1.572715e-05, dtype=float32), 'training/total_loss': Array(4069.1035, dtype=float32), 'training/v_loss': Array(4069.116, dtype=float32), 'eval/episode_goal_distance': (Array(0.28668505, dtype=float32), Array(0.03263545, dtype=float32)), 'eval/episode_reward': (Array(-41167.42, dtype=float32), Array(20256.592, dtype=float32)), 'eval/avg_episode_length': (Array(821.40625, dtype=float32), Array(381.59067, dtype=float32)), 'eval/epoch_eval_time': 4.097400665283203, 'eval/sps': 31239.31742495408}
I0726 23:41:16.989281 140267183036224 train.py:379] starting iteration 70 535.2543108463287
I0726 23:41:24.140391 140267183036224 train.py:394] {'eval/walltime': 301.3963689804077, 'training/sps': 40208.84382232517, 'training/walltime': 236.5755467414856, 'training/entropy_loss': Array(-0.01248542, dtype=float32), 'training/policy_loss': Array(-0.00017456, dtype=float32), 'training/total_loss': Array(3572.5088, dtype=float32), 'training/v_loss': Array(3572.5215, dtype=float32), 'eval/episode_goal_distance': (Array(0.27846372, dtype=float32), Array(0.03938219, dtype=float32)), 'eval/episode_reward': (Array(-44862.47, dtype=float32), Array(16797.396, dtype=float32)), 'eval/avg_episode_length': (Array(899.0781, dtype=float32), Array(300.16745, dtype=float32)), 'eval/epoch_eval_time': 4.091498851776123, 'eval/sps': 31284.378814975127}
I0726 23:41:24.143113 140267183036224 train.py:379] starting iteration 71 542.4081428050995
I0726 23:41:31.289135 140267183036224 train.py:394] {'eval/walltime': 305.4883668422699, 'training/sps': 40283.433356437476, 'training/walltime': 239.6259322166443, 'training/entropy_loss': Array(-0.01141891, dtype=float32), 'training/policy_loss': Array(6.844652e-05, dtype=float32), 'training/total_loss': Array(3212.0562, dtype=float32), 'training/v_loss': Array(3212.0671, dtype=float32), 'eval/episode_goal_distance': (Array(0.28252852, dtype=float32), Array(0.04153478, dtype=float32)), 'eval/episode_reward': (Array(-42740.14, dtype=float32), Array(19175.855, dtype=float32)), 'eval/avg_episode_length': (Array(852.47656, dtype=float32), Array(353.3448, dtype=float32)), 'eval/epoch_eval_time': 4.091997861862183, 'eval/sps': 31280.563754192648}
I0726 23:41:31.291782 140267183036224 train.py:379] starting iteration 72 549.556812286377
I0726 23:41:38.430255 140267183036224 train.py:394] {'eval/walltime': 309.57421469688416, 'training/sps': 40299.55728167486, 'training/walltime': 242.67509722709656, 'training/entropy_loss': Array(-0.01044524, dtype=float32), 'training/policy_loss': Array(5.726322e-05, dtype=float32), 'training/total_loss': Array(3126.4595, dtype=float32), 'training/v_loss': Array(3126.4702, dtype=float32), 'eval/episode_goal_distance': (Array(0.28850213, dtype=float32), Array(0.03516178, dtype=float32)), 'eval/episode_reward': (Array(-40722.1, dtype=float32), Array(21595.182, dtype=float32)), 'eval/avg_episode_length': (Array(798.0156, dtype=float32), Array(400.06656, dtype=float32)), 'eval/epoch_eval_time': 4.085847854614258, 'eval/sps': 31327.647174978913}
I0726 23:41:38.432739 140267183036224 train.py:379] starting iteration 73 556.6977689266205
I0726 23:41:45.571531 140267183036224 train.py:394] {'eval/walltime': 313.6625442504883, 'training/sps': 40358.57192525479, 'training/walltime': 245.71980357170105, 'training/entropy_loss': Array(-0.00910666, dtype=float32), 'training/policy_loss': Array(0.00034499, dtype=float32), 'training/total_loss': Array(2996.408, dtype=float32), 'training/v_loss': Array(2996.417, dtype=float32), 'eval/episode_goal_distance': (Array(0.27693495, dtype=float32), Array(0.03873093, dtype=float32)), 'eval/episode_reward': (Array(-40812.453, dtype=float32), Array(19063.09, dtype=float32)), 'eval/avg_episode_length': (Array(836.90625, dtype=float32), Array(368.1464, dtype=float32)), 'eval/epoch_eval_time': 4.088329553604126, 'eval/sps': 31308.63065751628}
I0726 23:41:45.574021 140267183036224 train.py:379] starting iteration 74 563.8390512466431
I0726 23:41:52.725095 140267183036224 train.py:394] {'eval/walltime': 317.7671580314636, 'training/sps': 40384.21204427159, 'training/walltime': 248.7625768184662, 'training/entropy_loss': Array(-0.00931235, dtype=float32), 'training/policy_loss': Array(0.0005199, dtype=float32), 'training/total_loss': Array(3216.9026, dtype=float32), 'training/v_loss': Array(3216.9111, dtype=float32), 'eval/episode_goal_distance': (Array(0.2839061, dtype=float32), Array(0.04256865, dtype=float32)), 'eval/episode_reward': (Array(-40870.773, dtype=float32), Array(20896.627, dtype=float32)), 'eval/avg_episode_length': (Array(813.66406, dtype=float32), Array(387.88974, dtype=float32)), 'eval/epoch_eval_time': 4.104613780975342, 'eval/sps': 31184.41997960269}
I0726 23:41:52.727762 140267183036224 train.py:379] starting iteration 75 570.9927916526794
I0726 23:41:59.882833 140267183036224 train.py:394] {'eval/walltime': 321.8732089996338, 'training/sps': 40350.46418418867, 'training/walltime': 251.80789494514465, 'training/entropy_loss': Array(-0.01149081, dtype=float32), 'training/policy_loss': Array(0.00020816, dtype=float32), 'training/total_loss': Array(2993.5054, dtype=float32), 'training/v_loss': Array(2993.5166, dtype=float32), 'eval/episode_goal_distance': (Array(0.28272527, dtype=float32), Array(0.03826616, dtype=float32)), 'eval/episode_reward': (Array(-40147.83, dtype=float32), Array(19657.375, dtype=float32)), 'eval/avg_episode_length': (Array(821.3594, dtype=float32), Array(381.69116, dtype=float32)), 'eval/epoch_eval_time': 4.106050968170166, 'eval/sps': 31173.5049058688}
I0726 23:41:59.885354 140267183036224 train.py:379] starting iteration 76 578.1503841876984
I0726 23:42:07.048129 140267183036224 train.py:394] {'eval/walltime': 325.98012018203735, 'training/sps': 40260.01540109639, 'training/walltime': 254.86005473136902, 'training/entropy_loss': Array(-0.01152816, dtype=float32), 'training/policy_loss': Array(-3.308525e-05, dtype=float32), 'training/total_loss': Array(2902.9004, dtype=float32), 'training/v_loss': Array(2902.912, dtype=float32), 'eval/episode_goal_distance': (Array(0.28457743, dtype=float32), Array(0.03568447, dtype=float32)), 'eval/episode_reward': (Array(-41713.363, dtype=float32), Array(20093.15, dtype=float32)), 'eval/avg_episode_length': (Array(829., dtype=float32), Array(375.35168, dtype=float32)), 'eval/epoch_eval_time': 4.1069111824035645, 'eval/sps': 31166.975450656853}
I0726 23:42:07.050650 140267183036224 train.py:379] starting iteration 77 585.3156790733337
I0726 23:42:14.208802 140267183036224 train.py:394] {'eval/walltime': 330.0877802371979, 'training/sps': 40331.200045449244, 'training/walltime': 257.9068274497986, 'training/entropy_loss': Array(-0.01116988, dtype=float32), 'training/policy_loss': Array(0.00017897, dtype=float32), 'training/total_loss': Array(2831.1152, dtype=float32), 'training/v_loss': Array(2831.1262, dtype=float32), 'eval/episode_goal_distance': (Array(0.28005588, dtype=float32), Array(0.03576088, dtype=float32)), 'eval/episode_reward': (Array(-43237.42, dtype=float32), Array(17999.713, dtype=float32)), 'eval/avg_episode_length': (Array(867.9531, dtype=float32), Array(337.41638, dtype=float32)), 'eval/epoch_eval_time': 4.1076600551605225, 'eval/sps': 31161.29335951047}
I0726 23:42:14.211236 140267183036224 train.py:379] starting iteration 78 592.476265668869
I0726 23:42:21.368782 140267183036224 train.py:394] {'eval/walltime': 334.1920304298401, 'training/sps': 40293.13640362491, 'training/walltime': 260.95647835731506, 'training/entropy_loss': Array(-0.01016441, dtype=float32), 'training/policy_loss': Array(0.00037277, dtype=float32), 'training/total_loss': Array(2729.954, dtype=float32), 'training/v_loss': Array(2729.9639, dtype=float32), 'eval/episode_goal_distance': (Array(0.28710467, dtype=float32), Array(0.03776276, dtype=float32)), 'eval/episode_reward': (Array(-36815.91, dtype=float32), Array(22791.176, dtype=float32)), 'eval/avg_episode_length': (Array(735.91406, dtype=float32), Array(439.10724, dtype=float32)), 'eval/epoch_eval_time': 4.104250192642212, 'eval/sps': 31187.18255272758}
I0726 23:42:21.371354 140267183036224 train.py:379] starting iteration 79 599.6363842487335
I0726 23:42:28.523097 140267183036224 train.py:394] {'eval/walltime': 338.2981734275818, 'training/sps': 40393.064622577076, 'training/walltime': 263.99858474731445, 'training/entropy_loss': Array(-0.01196981, dtype=float32), 'training/policy_loss': Array(0.00050337, dtype=float32), 'training/total_loss': Array(2751.0884, dtype=float32), 'training/v_loss': Array(2751.0999, dtype=float32), 'eval/episode_goal_distance': (Array(0.28434932, dtype=float32), Array(0.03699473, dtype=float32)), 'eval/episode_reward': (Array(-43634.83, dtype=float32), Array(19997.277, dtype=float32)), 'eval/avg_episode_length': (Array(844.6328, dtype=float32), Array(361.0414, dtype=float32)), 'eval/epoch_eval_time': 4.106142997741699, 'eval/sps': 31172.80622481915}
I0726 23:42:28.525660 140267183036224 train.py:379] starting iteration 80 606.7906901836395
I0726 23:42:35.685025 140267183036224 train.py:394] {'eval/walltime': 342.4000391960144, 'training/sps': 40236.310534873766, 'training/walltime': 267.0525426864624, 'training/entropy_loss': Array(-0.01051666, dtype=float32), 'training/policy_loss': Array(-7.579618e-05, dtype=float32), 'training/total_loss': Array(2676.9575, dtype=float32), 'training/v_loss': Array(2676.968, dtype=float32), 'eval/episode_goal_distance': (Array(0.28260726, dtype=float32), Array(0.03712507, dtype=float32)), 'eval/episode_reward': (Array(-39872.766, dtype=float32), Array(20341.9, dtype=float32)), 'eval/avg_episode_length': (Array(813.60156, dtype=float32), Array(388.02045, dtype=float32)), 'eval/epoch_eval_time': 4.101865768432617, 'eval/sps': 31205.31173522791}
I0726 23:42:35.687465 140267183036224 train.py:379] starting iteration 81 613.9524943828583
I0726 23:42:42.858302 140267183036224 train.py:394] {'eval/walltime': 346.51723408699036, 'training/sps': 40289.02912315506, 'training/walltime': 270.10250449180603, 'training/entropy_loss': Array(-0.01128895, dtype=float32), 'training/policy_loss': Array(0.00015749, dtype=float32), 'training/total_loss': Array(2865.6978, dtype=float32), 'training/v_loss': Array(2865.7087, dtype=float32), 'eval/episode_goal_distance': (Array(0.28605175, dtype=float32), Array(0.03595709, dtype=float32)), 'eval/episode_reward': (Array(-38127.477, dtype=float32), Array(22063.055, dtype=float32)), 'eval/avg_episode_length': (Array(766.9219, dtype=float32), Array(421.26447, dtype=float32)), 'eval/epoch_eval_time': 4.117194890975952, 'eval/sps': 31089.128251021048}
I0726 23:42:42.860699 140267183036224 train.py:379] starting iteration 82 621.1257283687592
I0726 23:42:50.028561 140267183036224 train.py:394] {'eval/walltime': 350.62533473968506, 'training/sps': 40208.9002867851, 'training/walltime': 273.1585443019867, 'training/entropy_loss': Array(-0.01066059, dtype=float32), 'training/policy_loss': Array(9.329202e-05, dtype=float32), 'training/total_loss': Array(2625.7795, dtype=float32), 'training/v_loss': Array(2625.79, dtype=float32), 'eval/episode_goal_distance': (Array(0.27709234, dtype=float32), Array(0.0361965, dtype=float32)), 'eval/episode_reward': (Array(-39859.656, dtype=float32), Array(20340.883, dtype=float32)), 'eval/avg_episode_length': (Array(813.60156, dtype=float32), Array(388.02032, dtype=float32)), 'eval/epoch_eval_time': 4.108100652694702, 'eval/sps': 31157.951282434766}
I0726 23:42:50.030996 140267183036224 train.py:379] starting iteration 83 628.2960262298584
I0726 23:42:57.196729 140267183036224 train.py:394] {'eval/walltime': 354.7382743358612, 'training/sps': 40301.00052804565, 'training/walltime': 276.20760011672974, 'training/entropy_loss': Array(-0.01046731, dtype=float32), 'training/policy_loss': Array(0.00027778, dtype=float32), 'training/total_loss': Array(12809.075, dtype=float32), 'training/v_loss': Array(12809.085, dtype=float32), 'eval/episode_goal_distance': (Array(0.2832815, dtype=float32), Array(0.03990745, dtype=float32)), 'eval/episode_reward': (Array(-37659.31, dtype=float32), Array(21427.03, dtype=float32)), 'eval/avg_episode_length': (Array(767.10156, dtype=float32), Array(420.94016, dtype=float32)), 'eval/epoch_eval_time': 4.1129395961761475, 'eval/sps': 31121.293422106963}
I0726 23:42:57.199362 140267183036224 train.py:379] starting iteration 84 635.4643907546997
I0726 23:43:04.369270 140267183036224 train.py:394] {'eval/walltime': 358.85123109817505, 'training/sps': 40246.25173823901, 'training/walltime': 279.2608036994934, 'training/entropy_loss': Array(-0.01303537, dtype=float32), 'training/policy_loss': Array(0.00054926, dtype=float32), 'training/total_loss': Array(4553.2397, dtype=float32), 'training/v_loss': Array(4553.2524, dtype=float32), 'eval/episode_goal_distance': (Array(0.2808024, dtype=float32), Array(0.03726974, dtype=float32)), 'eval/episode_reward': (Array(-41410.54, dtype=float32), Array(20027.932, dtype=float32)), 'eval/avg_episode_length': (Array(829.1328, dtype=float32), Array(375.06088, dtype=float32)), 'eval/epoch_eval_time': 4.112956762313843, 'eval/sps': 31121.16353199651}
I0726 23:43:04.372069 140267183036224 train.py:379] starting iteration 85 642.6370980739594
I0726 23:43:11.548335 140267183036224 train.py:394] {'eval/walltime': 362.97468876838684, 'training/sps': 40300.178054016724, 'training/walltime': 282.3099217414856, 'training/entropy_loss': Array(-0.01233518, dtype=float32), 'training/policy_loss': Array(0.00027523, dtype=float32), 'training/total_loss': Array(4368.441, dtype=float32), 'training/v_loss': Array(4368.453, dtype=float32), 'eval/episode_goal_distance': (Array(0.28237128, dtype=float32), Array(0.03772865, dtype=float32)), 'eval/episode_reward': (Array(-42256.492, dtype=float32), Array(18796.438, dtype=float32)), 'eval/avg_episode_length': (Array(852.52344, dtype=float32), Array(353.2323, dtype=float32)), 'eval/epoch_eval_time': 4.123457670211792, 'eval/sps': 31041.909542247242}
I0726 23:43:11.550892 140267183036224 train.py:379] starting iteration 86 649.8159215450287
I0726 23:43:18.712722 140267183036224 train.py:394] {'eval/walltime': 367.08666610717773, 'training/sps': 40338.457064965914, 'training/walltime': 285.3561463356018, 'training/entropy_loss': Array(-0.01315977, dtype=float32), 'training/policy_loss': Array(0.00025127, dtype=float32), 'training/total_loss': Array(3947.9485, dtype=float32), 'training/v_loss': Array(3947.9614, dtype=float32), 'eval/episode_goal_distance': (Array(0.28514147, dtype=float32), Array(0.03368711, dtype=float32)), 'eval/episode_reward': (Array(-42805.367, dtype=float32), Array(20148.703, dtype=float32)), 'eval/avg_episode_length': (Array(836.96094, dtype=float32), Array(368.02295, dtype=float32)), 'eval/epoch_eval_time': 4.1119773387908936, 'eval/sps': 31128.576218670933}
I0726 23:43:18.715247 140267183036224 train.py:379] starting iteration 87 656.9802768230438
I0726 23:43:25.886201 140267183036224 train.py:394] {'eval/walltime': 371.203195810318, 'training/sps': 40278.01226730362, 'training/walltime': 288.4069423675537, 'training/entropy_loss': Array(-0.01492017, dtype=float32), 'training/policy_loss': Array(0.00039861, dtype=float32), 'training/total_loss': Array(3270.6648, dtype=float32), 'training/v_loss': Array(3270.6792, dtype=float32), 'eval/episode_goal_distance': (Array(0.28723145, dtype=float32), Array(0.03788994, dtype=float32)), 'eval/episode_reward': (Array(-38847.84, dtype=float32), Array(22131.504, dtype=float32)), 'eval/avg_episode_length': (Array(774.8125, dtype=float32), Array(416.0676, dtype=float32)), 'eval/epoch_eval_time': 4.116529703140259, 'eval/sps': 31094.151926647417}
I0726 23:43:25.888807 140267183036224 train.py:379] starting iteration 88 664.1538367271423
I0726 23:43:33.066341 140267183036224 train.py:394] {'eval/walltime': 375.32028222084045, 'training/sps': 40199.98400412205, 'training/walltime': 291.46366000175476, 'training/entropy_loss': Array(-0.0148682, dtype=float32), 'training/policy_loss': Array(-0.00045329, dtype=float32), 'training/total_loss': Array(3027.3706, dtype=float32), 'training/v_loss': Array(3027.3857, dtype=float32), 'eval/episode_goal_distance': (Array(0.28188044, dtype=float32), Array(0.03634744, dtype=float32)), 'eval/episode_reward': (Array(-37657.184, dtype=float32), Array(21450.174, dtype=float32)), 'eval/avg_episode_length': (Array(767.0703, dtype=float32), Array(420.99625, dtype=float32)), 'eval/epoch_eval_time': 4.117086410522461, 'eval/sps': 31089.947413505153}
I0726 23:43:33.068944 140267183036224 train.py:379] starting iteration 89 671.3339741230011
I0726 23:43:40.239113 140267183036224 train.py:394] {'eval/walltime': 379.44023752212524, 'training/sps': 40334.286879447696, 'training/walltime': 294.51019954681396, 'training/entropy_loss': Array(-0.01239014, dtype=float32), 'training/policy_loss': Array(7.879415e-05, dtype=float32), 'training/total_loss': Array(2790.9766, dtype=float32), 'training/v_loss': Array(2790.989, dtype=float32), 'eval/episode_goal_distance': (Array(0.2799604, dtype=float32), Array(0.04002362, dtype=float32)), 'eval/episode_reward': (Array(-40692.375, dtype=float32), Array(20293.277, dtype=float32)), 'eval/avg_episode_length': (Array(821.2422, dtype=float32), Array(381.9409, dtype=float32)), 'eval/epoch_eval_time': 4.11995530128479, 'eval/sps': 31068.29823131425}
I0726 23:43:40.241707 140267183036224 train.py:379] starting iteration 90 678.5067374706268
I0726 23:43:47.423187 140267183036224 train.py:394] {'eval/walltime': 383.56619358062744, 'training/sps': 40265.066743332856, 'training/walltime': 297.5619764328003, 'training/entropy_loss': Array(-0.01232834, dtype=float32), 'training/policy_loss': Array(-0.00034151, dtype=float32), 'training/total_loss': Array(2734.311, dtype=float32), 'training/v_loss': Array(2734.3237, dtype=float32), 'eval/episode_goal_distance': (Array(0.28709763, dtype=float32), Array(0.03532916, dtype=float32)), 'eval/episode_reward': (Array(-41412.344, dtype=float32), Array(21132.371, dtype=float32)), 'eval/avg_episode_length': (Array(805.6875, dtype=float32), Array(394.41208, dtype=float32)), 'eval/epoch_eval_time': 4.125956058502197, 'eval/sps': 31023.11274891922}
I0726 23:43:47.425786 140267183036224 train.py:379] starting iteration 91 685.6908164024353
I0726 23:43:54.615017 140267183036224 train.py:394] {'eval/walltime': 387.70501160621643, 'training/sps': 40330.316376291135, 'training/walltime': 300.608815908432, 'training/entropy_loss': Array(-0.01205686, dtype=float32), 'training/policy_loss': Array(0.00048578, dtype=float32), 'training/total_loss': Array(2655.064, dtype=float32), 'training/v_loss': Array(2655.0757, dtype=float32), 'eval/episode_goal_distance': (Array(0.28698707, dtype=float32), Array(0.04023146, dtype=float32)), 'eval/episode_reward': (Array(-37117.477, dtype=float32), Array(22334.44, dtype=float32)), 'eval/avg_episode_length': (Array(751.3281, dtype=float32), Array(430.7128, dtype=float32)), 'eval/epoch_eval_time': 4.138818025588989, 'eval/sps': 30926.704003079358}
I0726 23:43:54.617481 140267183036224 train.py:379] starting iteration 92 692.8825113773346
I0726 23:44:01.780130 140267183036224 train.py:394] {'eval/walltime': 391.8168911933899, 'training/sps': 40328.16417271509, 'training/walltime': 303.65581798553467, 'training/entropy_loss': Array(-0.01168542, dtype=float32), 'training/policy_loss': Array(0.00031075, dtype=float32), 'training/total_loss': Array(2737.9307, dtype=float32), 'training/v_loss': Array(2737.9421, dtype=float32), 'eval/episode_goal_distance': (Array(0.28005755, dtype=float32), Array(0.0347113, dtype=float32)), 'eval/episode_reward': (Array(-40222.297, dtype=float32), Array(20694.307, dtype=float32)), 'eval/avg_episode_length': (Array(805.8906, dtype=float32), Array(394.00037, dtype=float32)), 'eval/epoch_eval_time': 4.111879587173462, 'eval/sps': 31129.316237586663}
I0726 23:44:01.782679 140267183036224 train.py:379] starting iteration 93 700.047708272934
I0726 23:44:08.965723 140267183036224 train.py:394] {'eval/walltime': 395.9504315853119, 'training/sps': 40342.08181755847, 'training/walltime': 306.70176887512207, 'training/entropy_loss': Array(-0.01159661, dtype=float32), 'training/policy_loss': Array(0.00039231, dtype=float32), 'training/total_loss': Array(2647.0874, dtype=float32), 'training/v_loss': Array(2647.0984, dtype=float32), 'eval/episode_goal_distance': (Array(0.27954686, dtype=float32), Array(0.04329129, dtype=float32)), 'eval/episode_reward': (Array(-42338.32, dtype=float32), Array(18390.842, dtype=float32)), 'eval/avg_episode_length': (Array(867.8672, dtype=float32), Array(337.63574, dtype=float32)), 'eval/epoch_eval_time': 4.133540391921997, 'eval/sps': 30966.1906897402}
I0726 23:44:08.968191 140267183036224 train.py:379] starting iteration 94 707.23322057724
I0726 23:44:16.128916 140267183036224 train.py:394] {'eval/walltime': 400.06306886672974, 'training/sps': 40363.26870096793, 'training/walltime': 309.746120929718, 'training/entropy_loss': Array(-0.01089622, dtype=float32), 'training/policy_loss': Array(7.4742056e-05, dtype=float32), 'training/total_loss': Array(2580.023, dtype=float32), 'training/v_loss': Array(2580.0337, dtype=float32), 'eval/episode_goal_distance': (Array(0.2818119, dtype=float32), Array(0.03319711, dtype=float32)), 'eval/episode_reward': (Array(-41508.934, dtype=float32), Array(20096.582, dtype=float32)), 'eval/avg_episode_length': (Array(829.1172, dtype=float32), Array(375.0948, dtype=float32)), 'eval/epoch_eval_time': 4.112637281417847, 'eval/sps': 31123.581108974322}
I0726 23:44:16.131517 140267183036224 train.py:379] starting iteration 95 714.3965466022491
I0726 23:44:23.290986 140267183036224 train.py:394] {'eval/walltime': 404.1770956516266, 'training/sps': 40398.1810741154, 'training/walltime': 312.7878420352936, 'training/entropy_loss': Array(-0.01188856, dtype=float32), 'training/policy_loss': Array(0.00041875, dtype=float32), 'training/total_loss': Array(2522.8442, dtype=float32), 'training/v_loss': Array(2522.856, dtype=float32), 'eval/episode_goal_distance': (Array(0.28034687, dtype=float32), Array(0.03415629, dtype=float32)), 'eval/episode_reward': (Array(-36210.867, dtype=float32), Array(22470.553, dtype=float32)), 'eval/avg_episode_length': (Array(735.9531, dtype=float32), Array(439.04193, dtype=float32)), 'eval/epoch_eval_time': 4.114026784896851, 'eval/sps': 31113.069188053254}
I0726 23:44:23.293575 140267183036224 train.py:379] starting iteration 96 721.5586051940918
I0726 23:44:30.457288 140267183036224 train.py:394] {'eval/walltime': 408.29343366622925, 'training/sps': 40372.88058544726, 'training/walltime': 315.83146929740906, 'training/entropy_loss': Array(-0.01301778, dtype=float32), 'training/policy_loss': Array(-0.00020153, dtype=float32), 'training/total_loss': Array(2575.3633, dtype=float32), 'training/v_loss': Array(2575.3765, dtype=float32), 'eval/episode_goal_distance': (Array(0.2836088, dtype=float32), Array(0.03480672, dtype=float32)), 'eval/episode_reward': (Array(-42261.5, dtype=float32), Array(18843.305, dtype=float32)), 'eval/avg_episode_length': (Array(852.3672, dtype=float32), Array(353.60663, dtype=float32)), 'eval/epoch_eval_time': 4.116338014602661, 'eval/sps': 31095.5999108726}
I0726 23:44:30.459931 140267183036224 train.py:379] starting iteration 97 728.7249615192413
I0726 23:44:37.618316 140267183036224 train.py:394] {'eval/walltime': 412.4117603302002, 'training/sps': 40467.73611914282, 'training/walltime': 318.8679623603821, 'training/entropy_loss': Array(-0.01236146, dtype=float32), 'training/policy_loss': Array(0.0002169, dtype=float32), 'training/total_loss': Array(2565.8044, dtype=float32), 'training/v_loss': Array(2565.8167, dtype=float32), 'eval/episode_goal_distance': (Array(0.27929837, dtype=float32), Array(0.03885124, dtype=float32)), 'eval/episode_reward': (Array(-39524.32, dtype=float32), Array(20821.025, dtype=float32)), 'eval/avg_episode_length': (Array(805.9453, dtype=float32), Array(393.88892, dtype=float32)), 'eval/epoch_eval_time': 4.118326663970947, 'eval/sps': 31080.58452958674}
I0726 23:44:37.620768 140267183036224 train.py:379] starting iteration 98 735.8857982158661
I0726 23:44:44.785365 140267183036224 train.py:394] {'eval/walltime': 416.5271677970886, 'training/sps': 40349.21639956832, 'training/walltime': 321.9133746623993, 'training/entropy_loss': Array(-0.01299359, dtype=float32), 'training/policy_loss': Array(0.00027854, dtype=float32), 'training/total_loss': Array(2541.3115, dtype=float32), 'training/v_loss': Array(2541.3242, dtype=float32), 'eval/episode_goal_distance': (Array(0.28728622, dtype=float32), Array(0.03518024, dtype=float32)), 'eval/episode_reward': (Array(-38199.43, dtype=float32), Array(22663.482, dtype=float32)), 'eval/avg_episode_length': (Array(751.4844, dtype=float32), Array(430.44278, dtype=float32)), 'eval/epoch_eval_time': 4.115407466888428, 'eval/sps': 31102.63103468053}
I0726 23:44:44.787846 140267183036224 train.py:379] starting iteration 99 743.0528755187988
I0726 23:44:51.963245 140267183036224 train.py:394] {'eval/walltime': 420.6619906425476, 'training/sps': 40462.446380962254, 'training/walltime': 324.9502646923065, 'training/entropy_loss': Array(-0.01198906, dtype=float32), 'training/policy_loss': Array(7.498469e-05, dtype=float32), 'training/total_loss': Array(2545.316, dtype=float32), 'training/v_loss': Array(2545.3281, dtype=float32), 'eval/episode_goal_distance': (Array(0.28427142, dtype=float32), Array(0.03774114, dtype=float32)), 'eval/episode_reward': (Array(-40484.297, dtype=float32), Array(20087.375, dtype=float32)), 'eval/avg_episode_length': (Array(821.4297, dtype=float32), Array(381.5407, dtype=float32)), 'eval/epoch_eval_time': 4.134822845458984, 'eval/sps': 30956.586239377666}
I0726 23:44:51.965823 140267183036224 train.py:379] starting iteration 100 750.2308526039124
I0726 23:44:59.130134 140267183036224 train.py:394] {'eval/walltime': 424.7820656299591, 'training/sps': 40413.655459437134, 'training/walltime': 327.99082112312317, 'training/entropy_loss': Array(-0.01267761, dtype=float32), 'training/policy_loss': Array(0.00026794, dtype=float32), 'training/total_loss': Array(12885.279, dtype=float32), 'training/v_loss': Array(12885.293, dtype=float32), 'eval/episode_goal_distance': (Array(0.28288728, dtype=float32), Array(0.04057993, dtype=float32)), 'eval/episode_reward': (Array(-41829.82, dtype=float32), Array(19352.094, dtype=float32)), 'eval/avg_episode_length': (Array(844.6172, dtype=float32), Array(361.0778, dtype=float32)), 'eval/epoch_eval_time': 4.120074987411499, 'eval/sps': 31067.39571272172}
I0726 23:44:59.132602 140267183036224 train.py:379] starting iteration 101 757.3976328372955
I0726 23:45:06.292548 140267183036224 train.py:394] {'eval/walltime': 428.89804673194885, 'training/sps': 40416.96411237978, 'training/walltime': 331.03112864494324, 'training/entropy_loss': Array(-0.01283509, dtype=float32), 'training/policy_loss': Array(3.337476e-05, dtype=float32), 'training/total_loss': Array(3595.4946, dtype=float32), 'training/v_loss': Array(3595.5073, dtype=float32), 'eval/episode_goal_distance': (Array(0.28563333, dtype=float32), Array(0.03358462, dtype=float32)), 'eval/episode_reward': (Array(-39889.574, dtype=float32), Array(22460.697, dtype=float32)), 'eval/avg_episode_length': (Array(774.6953, dtype=float32), Array(416.28394, dtype=float32)), 'eval/epoch_eval_time': 4.115981101989746, 'eval/sps': 31098.296330399156}
I0726 23:45:06.295005 140267183036224 train.py:379] starting iteration 102 764.5600337982178
I0726 23:45:13.457868 140267183036224 train.py:394] {'eval/walltime': 433.0147922039032, 'training/sps': 40389.73139448608, 'training/walltime': 334.0734860897064, 'training/entropy_loss': Array(-0.01325197, dtype=float32), 'training/policy_loss': Array(0.00012555, dtype=float32), 'training/total_loss': Array(3469.9277, dtype=float32), 'training/v_loss': Array(3469.941, dtype=float32), 'eval/episode_goal_distance': (Array(0.28796712, dtype=float32), Array(0.03803627, dtype=float32)), 'eval/episode_reward': (Array(-41444.008, dtype=float32), Array(21661.592, dtype=float32)), 'eval/avg_episode_length': (Array(805.8047, dtype=float32), Array(394.1741, dtype=float32)), 'eval/epoch_eval_time': 4.116745471954346, 'eval/sps': 31092.522205225006}
I0726 23:45:13.560960 140267183036224 train.py:379] starting iteration 103 771.8259627819061
I0726 23:45:20.751157 140267183036224 train.py:394] {'eval/walltime': 437.13616919517517, 'training/sps': 40101.157419715964, 'training/walltime': 337.13773679733276, 'training/entropy_loss': Array(-0.01292541, dtype=float32), 'training/policy_loss': Array(6.1203486e-05, dtype=float32), 'training/total_loss': Array(3584.0427, dtype=float32), 'training/v_loss': Array(3584.0557, dtype=float32), 'eval/episode_goal_distance': (Array(0.28458434, dtype=float32), Array(0.03540368, dtype=float32)), 'eval/episode_reward': (Array(-41015.39, dtype=float32), Array(21067.475, dtype=float32)), 'eval/avg_episode_length': (Array(805.6797, dtype=float32), Array(394.42807, dtype=float32)), 'eval/epoch_eval_time': 4.121376991271973, 'eval/sps': 31057.58106357934}
I0726 23:45:20.753824 140267183036224 train.py:379] starting iteration 104 779.0188536643982
I0726 23:45:27.938470 140267183036224 train.py:394] {'eval/walltime': 441.2617139816284, 'training/sps': 40219.223399432056, 'training/walltime': 340.1929922103882, 'training/entropy_loss': Array(-0.01250101, dtype=float32), 'training/policy_loss': Array(4.350229e-05, dtype=float32), 'training/total_loss': Array(2878.747, dtype=float32), 'training/v_loss': Array(2878.7593, dtype=float32), 'eval/episode_goal_distance': (Array(0.28273177, dtype=float32), Array(0.03858128, dtype=float32)), 'eval/episode_reward': (Array(-39459.64, dtype=float32), Array(21505.895, dtype=float32)), 'eval/avg_episode_length': (Array(790.3203, dtype=float32), Array(405.542, dtype=float32)), 'eval/epoch_eval_time': 4.125544786453247, 'eval/sps': 31026.205416628694}
I0726 23:45:27.940923 140267183036224 train.py:379] starting iteration 105 786.2059524059296
I0726 23:45:35.132685 140267183036224 train.py:394] {'eval/walltime': 445.3907268047333, 'training/sps': 40170.31247269641, 'training/walltime': 343.2519676685333, 'training/entropy_loss': Array(-0.01234621, dtype=float32), 'training/policy_loss': Array(-0.00015321, dtype=float32), 'training/total_loss': Array(2906.4282, dtype=float32), 'training/v_loss': Array(2906.441, dtype=float32), 'eval/episode_goal_distance': (Array(0.28536916, dtype=float32), Array(0.03724187, dtype=float32)), 'eval/episode_reward': (Array(-40424.176, dtype=float32), Array(20745.814, dtype=float32)), 'eval/avg_episode_length': (Array(805.77344, dtype=float32), Array(394.2376, dtype=float32)), 'eval/epoch_eval_time': 4.129012823104858, 'eval/sps': 31000.145914719862}
I0726 23:45:35.135216 140267183036224 train.py:379] starting iteration 106 793.4002451896667
I0726 23:45:42.326326 140267183036224 train.py:394] {'eval/walltime': 449.5275423526764, 'training/sps': 40281.62616459257, 'training/walltime': 346.3024899959564, 'training/entropy_loss': Array(-0.01117665, dtype=float32), 'training/policy_loss': Array(0.00021831, dtype=float32), 'training/total_loss': Array(2789.085, dtype=float32), 'training/v_loss': Array(2789.0957, dtype=float32), 'eval/episode_goal_distance': (Array(0.2854228, dtype=float32), Array(0.03949504, dtype=float32)), 'eval/episode_reward': (Array(-39081.113, dtype=float32), Array(21176.232, dtype=float32)), 'eval/avg_episode_length': (Array(790.28125, dtype=float32), Array(405.61792, dtype=float32)), 'eval/epoch_eval_time': 4.136815547943115, 'eval/sps': 30941.674463499698}
I0726 23:45:42.328717 140267183036224 train.py:379] starting iteration 107 800.5937469005585
I0726 23:45:49.531914 140267183036224 train.py:394] {'eval/walltime': 453.6827771663666, 'training/sps': 40365.475235427904, 'training/walltime': 349.34667563438416, 'training/entropy_loss': Array(-0.01031069, dtype=float32), 'training/policy_loss': Array(0.00030822, dtype=float32), 'training/total_loss': Array(2692.6406, dtype=float32), 'training/v_loss': Array(2692.6511, dtype=float32), 'eval/episode_goal_distance': (Array(0.2843022, dtype=float32), Array(0.03441913, dtype=float32)), 'eval/episode_reward': (Array(-41306.445, dtype=float32), Array(20073.936, dtype=float32)), 'eval/avg_episode_length': (Array(829., dtype=float32), Array(375.35184, dtype=float32)), 'eval/epoch_eval_time': 4.1552348136901855, 'eval/sps': 30804.51664928308}
I0726 23:45:49.534337 140267183036224 train.py:379] starting iteration 108 807.7993671894073
I0726 23:45:56.702268 140267183036224 train.py:394] {'eval/walltime': 457.7969114780426, 'training/sps': 40286.54437306656, 'training/walltime': 352.3968255519867, 'training/entropy_loss': Array(-0.00972143, dtype=float32), 'training/policy_loss': Array(-7.528784e-05, dtype=float32), 'training/total_loss': Array(2650.518, dtype=float32), 'training/v_loss': Array(2650.5278, dtype=float32), 'eval/episode_goal_distance': (Array(0.28148025, dtype=float32), Array(0.03634065, dtype=float32)), 'eval/episode_reward': (Array(-39888.812, dtype=float32), Array(20263.46, dtype=float32)), 'eval/avg_episode_length': (Array(813.4297, dtype=float32), Array(388.37747, dtype=float32)), 'eval/epoch_eval_time': 4.114134311676025, 'eval/sps': 31112.256018655615}
I0726 23:45:56.704647 140267183036224 train.py:379] starting iteration 109 814.9696776866913
I0726 23:46:03.870126 140267183036224 train.py:394] {'eval/walltime': 461.9146912097931, 'training/sps': 40368.7443846367, 'training/walltime': 355.44076466560364, 'training/entropy_loss': Array(-0.00934283, dtype=float32), 'training/policy_loss': Array(-0.00014333, dtype=float32), 'training/total_loss': Array(2575.0417, dtype=float32), 'training/v_loss': Array(2575.0513, dtype=float32), 'eval/episode_goal_distance': (Array(0.2838388, dtype=float32), Array(0.03866345, dtype=float32)), 'eval/episode_reward': (Array(-40906.734, dtype=float32), Array(20474.26, dtype=float32)), 'eval/avg_episode_length': (Array(821.52344, dtype=float32), Array(381.34027, dtype=float32)), 'eval/epoch_eval_time': 4.117779731750488, 'eval/sps': 31084.71271861513}
I0726 23:46:03.872655 140267183036224 train.py:379] starting iteration 110 822.1376843452454
I0726 23:46:11.044935 140267183036224 train.py:394] {'eval/walltime': 466.03805446624756, 'training/sps': 40352.00901687148, 'training/walltime': 358.4859662055969, 'training/entropy_loss': Array(-0.00978652, dtype=float32), 'training/policy_loss': Array(0.00015589, dtype=float32), 'training/total_loss': Array(2588.2473, dtype=float32), 'training/v_loss': Array(2588.2568, dtype=float32), 'eval/episode_goal_distance': (Array(0.28030074, dtype=float32), Array(0.0385674, dtype=float32)), 'eval/episode_reward': (Array(-41671.016, dtype=float32), Array(19584.434, dtype=float32)), 'eval/avg_episode_length': (Array(836.96094, dtype=float32), Array(368.023, dtype=float32)), 'eval/epoch_eval_time': 4.123363256454468, 'eval/sps': 31042.620317197718}
I0726 23:46:11.047311 140267183036224 train.py:379] starting iteration 111 829.3123414516449
I0726 23:46:18.216927 140267183036224 train.py:394] {'eval/walltime': 470.15743231773376, 'training/sps': 40334.87715612064, 'training/walltime': 361.53246116638184, 'training/entropy_loss': Array(-0.01088147, dtype=float32), 'training/policy_loss': Array(0.00029788, dtype=float32), 'training/total_loss': Array(2582.7126, dtype=float32), 'training/v_loss': Array(2582.7234, dtype=float32), 'eval/episode_goal_distance': (Array(0.28589183, dtype=float32), Array(0.03551077, dtype=float32)), 'eval/episode_reward': (Array(-41706.887, dtype=float32), Array(20916.045, dtype=float32)), 'eval/avg_episode_length': (Array(813.4922, dtype=float32), Array(388.2475, dtype=float32)), 'eval/epoch_eval_time': 4.119377851486206, 'eval/sps': 31072.653350752866}
I0726 23:46:18.219633 140267183036224 train.py:379] starting iteration 112 836.4846622943878
I0726 23:46:25.399875 140267183036224 train.py:394] {'eval/walltime': 474.2797176837921, 'training/sps': 40231.03715607334, 'training/walltime': 364.5868194103241, 'training/entropy_loss': Array(-0.0107059, dtype=float32), 'training/policy_loss': Array(-7.221246e-05, dtype=float32), 'training/total_loss': Array(2754.0547, dtype=float32), 'training/v_loss': Array(2754.0654, dtype=float32), 'eval/episode_goal_distance': (Array(0.28402174, dtype=float32), Array(0.0371516, dtype=float32)), 'eval/episode_reward': (Array(-37796.945, dtype=float32), Array(21730.45, dtype=float32)), 'eval/avg_episode_length': (Array(767., dtype=float32), Array(421.1233, dtype=float32)), 'eval/epoch_eval_time': 4.12228536605835, 'eval/sps': 31050.737305552222}
I0726 23:46:25.402342 140267183036224 train.py:379] starting iteration 113 843.6673712730408
I0726 23:46:32.585714 140267183036224 train.py:394] {'eval/walltime': 478.3978590965271, 'training/sps': 40138.04629959674, 'training/walltime': 367.6482539176941, 'training/entropy_loss': Array(-0.01144356, dtype=float32), 'training/policy_loss': Array(0.00022341, dtype=float32), 'training/total_loss': Array(2502.6045, dtype=float32), 'training/v_loss': Array(2502.6157, dtype=float32), 'eval/episode_goal_distance': (Array(0.27608058, dtype=float32), Array(0.03700231, dtype=float32)), 'eval/episode_reward': (Array(-38400.656, dtype=float32), Array(20791.406, dtype=float32)), 'eval/avg_episode_length': (Array(790.22656, dtype=float32), Array(405.7234, dtype=float32)), 'eval/epoch_eval_time': 4.118141412734985, 'eval/sps': 31081.982664357132}
I0726 23:46:32.588296 140267183036224 train.py:379] starting iteration 114 850.8533260822296
I0726 23:46:39.774340 140267183036224 train.py:394] {'eval/walltime': 482.5200455188751, 'training/sps': 40156.13449774025, 'training/walltime': 370.70830941200256, 'training/entropy_loss': Array(-0.01053762, dtype=float32), 'training/policy_loss': Array(-0.0002699, dtype=float32), 'training/total_loss': Array(2495.1147, dtype=float32), 'training/v_loss': Array(2495.1255, dtype=float32), 'eval/episode_goal_distance': (Array(0.2845919, dtype=float32), Array(0.03821094, dtype=float32)), 'eval/episode_reward': (Array(-42214.03, dtype=float32), Array(19765.521, dtype=float32)), 'eval/avg_episode_length': (Array(836.9531, dtype=float32), Array(368.04053, dtype=float32)), 'eval/epoch_eval_time': 4.1221864223480225, 'eval/sps': 31051.482607885166}
I0726 23:46:39.776718 140267183036224 train.py:379] starting iteration 115 858.0417482852936
I0726 23:46:46.960294 140267183036224 train.py:394] {'eval/walltime': 486.6430878639221, 'training/sps': 40199.33809577702, 'training/walltime': 373.7650761604309, 'training/entropy_loss': Array(-0.00939844, dtype=float32), 'training/policy_loss': Array(0.00026749, dtype=float32), 'training/total_loss': Array(2452.0957, dtype=float32), 'training/v_loss': Array(2452.105, dtype=float32), 'eval/episode_goal_distance': (Array(0.27757835, dtype=float32), Array(0.03638861, dtype=float32)), 'eval/episode_reward': (Array(-40171.95, dtype=float32), Array(18802.576, dtype=float32)), 'eval/avg_episode_length': (Array(844.72656, dtype=float32), Array(360.82382, dtype=float32)), 'eval/epoch_eval_time': 4.123042345046997, 'eval/sps': 31045.036477436657}
I0726 23:46:46.962928 140267183036224 train.py:379] starting iteration 116 865.2279579639435
I0726 23:46:54.146113 140267183036224 train.py:394] {'eval/walltime': 490.76858949661255, 'training/sps': 40236.80999142092, 'training/walltime': 376.8189961910248, 'training/entropy_loss': Array(-0.00890744, dtype=float32), 'training/policy_loss': Array(0.00032458, dtype=float32), 'training/total_loss': Array(11447.068, dtype=float32), 'training/v_loss': Array(11447.077, dtype=float32), 'eval/episode_goal_distance': (Array(0.28815138, dtype=float32), Array(0.03337645, dtype=float32)), 'eval/episode_reward': (Array(-42028.06, dtype=float32), Array(20075.48, dtype=float32)), 'eval/avg_episode_length': (Array(829.1172, dtype=float32), Array(375.09454, dtype=float32)), 'eval/epoch_eval_time': 4.12550163269043, 'eval/sps': 31026.52995837631}
I0726 23:46:54.148547 140267183036224 train.py:379] starting iteration 117 872.4135763645172
I0726 23:47:01.332216 140267183036224 train.py:394] {'eval/walltime': 494.89563751220703, 'training/sps': 40250.45088970262, 'training/walltime': 379.8718812465668, 'training/entropy_loss': Array(-0.0102152, dtype=float32), 'training/policy_loss': Array(0.00022415, dtype=float32), 'training/total_loss': Array(5255.8877, dtype=float32), 'training/v_loss': Array(5255.8975, dtype=float32), 'eval/episode_goal_distance': (Array(0.2820034, dtype=float32), Array(0.03651955, dtype=float32)), 'eval/episode_reward': (Array(-43784.406, dtype=float32), Array(17946.729, dtype=float32)), 'eval/avg_episode_length': (Array(875.7969, dtype=float32), Array(328.611, dtype=float32)), 'eval/epoch_eval_time': 4.127048015594482, 'eval/sps': 31014.904482898823}
I0726 23:47:01.334859 140267183036224 train.py:379] starting iteration 118 879.5998883247375
I0726 23:47:08.520885 140267183036224 train.py:394] {'eval/walltime': 499.01975750923157, 'training/sps': 40180.897706092976, 'training/walltime': 382.93005084991455, 'training/entropy_loss': Array(-0.01089055, dtype=float32), 'training/policy_loss': Array(0.00029456, dtype=float32), 'training/total_loss': Array(3688.1362, dtype=float32), 'training/v_loss': Array(3688.147, dtype=float32), 'eval/episode_goal_distance': (Array(0.28303832, dtype=float32), Array(0.03841256, dtype=float32)), 'eval/episode_reward': (Array(-42281.42, dtype=float32), Array(19675.154, dtype=float32)), 'eval/avg_episode_length': (Array(837.03125, dtype=float32), Array(367.86444, dtype=float32)), 'eval/epoch_eval_time': 4.124119997024536, 'eval/sps': 31036.92426320021}
I0726 23:47:08.523331 140267183036224 train.py:379] starting iteration 119 886.7883610725403
I0726 23:47:15.720359 140267183036224 train.py:394] {'eval/walltime': 503.1604778766632, 'training/sps': 40253.74544818365, 'training/walltime': 385.98268604278564, 'training/entropy_loss': Array(-0.01073329, dtype=float32), 'training/policy_loss': Array(0.00025773, dtype=float32), 'training/total_loss': Array(3180.1545, dtype=float32), 'training/v_loss': Array(3180.1648, dtype=float32), 'eval/episode_goal_distance': (Array(0.28539336, dtype=float32), Array(0.03801876, dtype=float32)), 'eval/episode_reward': (Array(-39635.07, dtype=float32), Array(21714.176, dtype=float32)), 'eval/avg_episode_length': (Array(790.1797, dtype=float32), Array(405.8142, dtype=float32)), 'eval/epoch_eval_time': 4.140720367431641, 'eval/sps': 30912.495566416237}
I0726 23:47:15.722847 140267183036224 train.py:379] starting iteration 120 893.9878768920898
I0726 23:47:22.897790 140267183036224 train.py:394] {'eval/walltime': 507.28905272483826, 'training/sps': 40384.64239808559, 'training/walltime': 389.025426864624, 'training/entropy_loss': Array(-0.00901035, dtype=float32), 'training/policy_loss': Array(0.00018596, dtype=float32), 'training/total_loss': Array(2808.005, dtype=float32), 'training/v_loss': Array(2808.014, dtype=float32), 'eval/episode_goal_distance': (Array(0.29002082, dtype=float32), Array(0.03793581, dtype=float32)), 'eval/episode_reward': (Array(-41432.72, dtype=float32), Array(20121.686, dtype=float32)), 'eval/avg_episode_length': (Array(829.15625, dtype=float32), Array(375.009, dtype=float32)), 'eval/epoch_eval_time': 4.128574848175049, 'eval/sps': 31003.434528159214}
I0726 23:47:22.900275 140267183036224 train.py:379] starting iteration 121 901.1653044223785
I0726 23:47:30.067054 140267183036224 train.py:394] {'eval/walltime': 511.4072313308716, 'training/sps': 40356.57786455387, 'training/walltime': 392.07028365135193, 'training/entropy_loss': Array(-0.00889316, dtype=float32), 'training/policy_loss': Array(6.3780855e-05, dtype=float32), 'training/total_loss': Array(2706.3423, dtype=float32), 'training/v_loss': Array(2706.351, dtype=float32), 'eval/episode_goal_distance': (Array(0.2850479, dtype=float32), Array(0.03584498, dtype=float32)), 'eval/episode_reward': (Array(-38424.37, dtype=float32), Array(22140.576, dtype=float32)), 'eval/avg_episode_length': (Array(766.9219, dtype=float32), Array(421.26477, dtype=float32)), 'eval/epoch_eval_time': 4.118178606033325, 'eval/sps': 31081.70194767026}
I0726 23:47:30.069511 140267183036224 train.py:379] starting iteration 122 908.3345408439636
I0726 23:47:37.233558 140267183036224 train.py:394] {'eval/walltime': 515.5238246917725, 'training/sps': 40371.07800842018, 'training/walltime': 395.1140468120575, 'training/entropy_loss': Array(-0.00770708, dtype=float32), 'training/policy_loss': Array(3.854932e-05, dtype=float32), 'training/total_loss': Array(2797.2988, dtype=float32), 'training/v_loss': Array(2797.3064, dtype=float32), 'eval/episode_goal_distance': (Array(0.28126466, dtype=float32), Array(0.0398911, dtype=float32)), 'eval/episode_reward': (Array(-39008.258, dtype=float32), Array(20801.682, dtype=float32)), 'eval/avg_episode_length': (Array(797.91406, dtype=float32), Array(400.26727, dtype=float32)), 'eval/epoch_eval_time': 4.116593360900879, 'eval/sps': 31093.6710960415}
I0726 23:47:37.236044 140267183036224 train.py:379] starting iteration 123 915.5010733604431
I0726 23:47:44.399885 140267183036224 train.py:394] {'eval/walltime': 519.642909526825, 'training/sps': 40406.38086369336, 'training/walltime': 398.15515065193176, 'training/entropy_loss': Array(-0.00751496, dtype=float32), 'training/policy_loss': Array(0.00018261, dtype=float32), 'training/total_loss': Array(2707.9482, dtype=float32), 'training/v_loss': Array(2707.9556, dtype=float32), 'eval/episode_goal_distance': (Array(0.28334153, dtype=float32), Array(0.03715841, dtype=float32)), 'eval/episode_reward': (Array(-37328.32, dtype=float32), Array(23121.125, dtype=float32)), 'eval/avg_episode_length': (Array(735.8672, dtype=float32), Array(439.1854, dtype=float32)), 'eval/epoch_eval_time': 4.11908483505249, 'eval/sps': 31074.863744186245}
I0726 23:47:44.402352 140267183036224 train.py:379] starting iteration 124 922.6673760414124
I0726 23:47:51.560562 140267183036224 train.py:394] {'eval/walltime': 523.7578566074371, 'training/sps': 40426.772823965715, 'training/walltime': 401.1947205066681, 'training/entropy_loss': Array(-0.00698135, dtype=float32), 'training/policy_loss': Array(0.00029109, dtype=float32), 'training/total_loss': Array(2569.327, dtype=float32), 'training/v_loss': Array(2569.3335, dtype=float32), 'eval/episode_goal_distance': (Array(0.28249508, dtype=float32), Array(0.03653882, dtype=float32)), 'eval/episode_reward': (Array(-38612.746, dtype=float32), Array(21860.473, dtype=float32)), 'eval/avg_episode_length': (Array(774.72656, dtype=float32), Array(416.226, dtype=float32)), 'eval/epoch_eval_time': 4.114947080612183, 'eval/sps': 31106.110842367718}
I0726 23:47:51.563622 140267183036224 train.py:379] starting iteration 125 929.8286519050598
I0726 23:47:58.737608 140267183036224 train.py:394] {'eval/walltime': 527.8791632652283, 'training/sps': 40300.62552614455, 'training/walltime': 404.24380469322205, 'training/entropy_loss': Array(-0.00688033, dtype=float32), 'training/policy_loss': Array(0.00016054, dtype=float32), 'training/total_loss': Array(2506.1924, dtype=float32), 'training/v_loss': Array(2506.199, dtype=float32), 'eval/episode_goal_distance': (Array(0.2876498, dtype=float32), Array(0.03881914, dtype=float32)), 'eval/episode_reward': (Array(-40892.918, dtype=float32), Array(21157.07, dtype=float32)), 'eval/avg_episode_length': (Array(805.71875, dtype=float32), Array(394.34842, dtype=float32)), 'eval/epoch_eval_time': 4.121306657791138, 'eval/sps': 31058.111086691883}
I0726 23:47:58.740073 140267183036224 train.py:379] starting iteration 126 937.0051023960114
I0726 23:48:05.909812 140267183036224 train.py:394] {'eval/walltime': 531.9985947608948, 'training/sps': 40341.128204518085, 'training/walltime': 407.28982758522034, 'training/entropy_loss': Array(-0.00581094, dtype=float32), 'training/policy_loss': Array(0.00028982, dtype=float32), 'training/total_loss': Array(2458.9883, dtype=float32), 'training/v_loss': Array(2458.9937, dtype=float32), 'eval/episode_goal_distance': (Array(0.28300554, dtype=float32), Array(0.03651924, dtype=float32)), 'eval/episode_reward': (Array(-36429.035, dtype=float32), Array(22492.01, dtype=float32)), 'eval/avg_episode_length': (Array(743.65625, dtype=float32), Array(434.93893, dtype=float32)), 'eval/epoch_eval_time': 4.119431495666504, 'eval/sps': 31072.248715545207}
I0726 23:48:05.912384 140267183036224 train.py:379] starting iteration 127 944.1774144172668
I0726 23:48:13.080409 140267183036224 train.py:394] {'eval/walltime': 536.1191160678864, 'training/sps': 40374.36387919278, 'training/walltime': 410.3333430290222, 'training/entropy_loss': Array(-0.00506242, dtype=float32), 'training/policy_loss': Array(9.651972e-05, dtype=float32), 'training/total_loss': Array(2465.2234, dtype=float32), 'training/v_loss': Array(2465.2283, dtype=float32), 'eval/episode_goal_distance': (Array(0.28596807, dtype=float32), Array(0.03799934, dtype=float32)), 'eval/episode_reward': (Array(-42713.098, dtype=float32), Array(19734.445, dtype=float32)), 'eval/avg_episode_length': (Array(844.59375, dtype=float32), Array(361.13214, dtype=float32)), 'eval/epoch_eval_time': 4.120521306991577, 'eval/sps': 31064.030607683893}
I0726 23:48:13.082923 140267183036224 train.py:379] starting iteration 128 951.347953081131
I0726 23:48:20.252688 140267183036224 train.py:394] {'eval/walltime': 540.2371351718903, 'training/sps': 40323.00549974307, 'training/walltime': 413.3807349205017, 'training/entropy_loss': Array(-0.0043559, dtype=float32), 'training/policy_loss': Array(-4.4523003e-05, dtype=float32), 'training/total_loss': Array(2537.5918, dtype=float32), 'training/v_loss': Array(2537.5962, dtype=float32), 'eval/episode_goal_distance': (Array(0.28586537, dtype=float32), Array(0.0376536, dtype=float32)), 'eval/episode_reward': (Array(-43833.836, dtype=float32), Array(18722.197, dtype=float32)), 'eval/avg_episode_length': (Array(867.97656, dtype=float32), Array(337.3563, dtype=float32)), 'eval/epoch_eval_time': 4.118019104003906, 'eval/sps': 31082.905826140282}
I0726 23:48:20.255095 140267183036224 train.py:379] starting iteration 129 958.5201244354248
I0726 23:48:27.434979 140267183036224 train.py:394] {'eval/walltime': 544.3654565811157, 'training/sps': 40349.12795194206, 'training/walltime': 416.42615389823914, 'training/entropy_loss': Array(-0.00568216, dtype=float32), 'training/policy_loss': Array(6.277881e-05, dtype=float32), 'training/total_loss': Array(2434.67, dtype=float32), 'training/v_loss': Array(2434.6753, dtype=float32), 'eval/episode_goal_distance': (Array(0.28780833, dtype=float32), Array(0.03391349, dtype=float32)), 'eval/episode_reward': (Array(-41392.32, dtype=float32), Array(20231.75, dtype=float32)), 'eval/avg_episode_length': (Array(821.2422, dtype=float32), Array(381.94128, dtype=float32)), 'eval/epoch_eval_time': 4.128321409225464, 'eval/sps': 31005.337838754844}
I0726 23:48:27.437405 140267183036224 train.py:379] starting iteration 130 965.7024347782135
I0726 23:48:34.603276 140267183036224 train.py:394] {'eval/walltime': 548.4822931289673, 'training/sps': 40354.20800114001, 'training/walltime': 419.47118949890137, 'training/entropy_loss': Array(-0.00395491, dtype=float32), 'training/policy_loss': Array(0.00015155, dtype=float32), 'training/total_loss': Array(2357.454, dtype=float32), 'training/v_loss': Array(2357.458, dtype=float32), 'eval/episode_goal_distance': (Array(0.28813857, dtype=float32), Array(0.04308771, dtype=float32)), 'eval/episode_reward': (Array(-36303.46, dtype=float32), Array(24206.361, dtype=float32)), 'eval/avg_episode_length': (Array(712.6094, dtype=float32), Array(450.70627, dtype=float32)), 'eval/epoch_eval_time': 4.1168365478515625, 'eval/sps': 31091.83435198535}
I0726 23:48:34.605749 140267183036224 train.py:379] starting iteration 131 972.8707792758942
I0726 23:48:41.791381 140267183036224 train.py:394] {'eval/walltime': 552.6080687046051, 'training/sps': 40211.10566168721, 'training/walltime': 422.5270617008209, 'training/entropy_loss': Array(-0.00410931, dtype=float32), 'training/policy_loss': Array(0.00020402, dtype=float32), 'training/total_loss': Array(2318.92, dtype=float32), 'training/v_loss': Array(2318.9238, dtype=float32), 'eval/episode_goal_distance': (Array(0.28353393, dtype=float32), Array(0.03397204, dtype=float32)), 'eval/episode_reward': (Array(-43645.992, dtype=float32), Array(17658.393, dtype=float32)), 'eval/avg_episode_length': (Array(875.8281, dtype=float32), Array(328.52856, dtype=float32)), 'eval/epoch_eval_time': 4.125775575637817, 'eval/sps': 31024.469861090798}
I0726 23:48:41.793909 140267183036224 train.py:379] starting iteration 132 980.0589389801025
I0726 23:48:48.973249 140267183036224 train.py:394] {'eval/walltime': 556.7253837585449, 'training/sps': 40180.14590789696, 'training/walltime': 425.5852885246277, 'training/entropy_loss': Array(-0.00388613, dtype=float32), 'training/policy_loss': Array(0.00029836, dtype=float32), 'training/total_loss': Array(2382.7788, dtype=float32), 'training/v_loss': Array(2382.7822, dtype=float32), 'eval/episode_goal_distance': (Array(0.2851996, dtype=float32), Array(0.03095567, dtype=float32)), 'eval/episode_reward': (Array(-39630.72, dtype=float32), Array(20770.344, dtype=float32)), 'eval/avg_episode_length': (Array(798.10156, dtype=float32), Array(399.89658, dtype=float32)), 'eval/epoch_eval_time': 4.117315053939819, 'eval/sps': 31088.22092142743}
I0726 23:48:48.975710 140267183036224 train.py:379] starting iteration 133 987.2407393455505
I0726 23:48:56.159215 140267183036224 train.py:394] {'eval/walltime': 560.8480134010315, 'training/sps': 40194.50698340034, 'training/walltime': 428.6424226760864, 'training/entropy_loss': Array(-0.00344441, dtype=float32), 'training/policy_loss': Array(0.00051627, dtype=float32), 'training/total_loss': Array(12628.5205, dtype=float32), 'training/v_loss': Array(12628.523, dtype=float32), 'eval/episode_goal_distance': (Array(0.27985442, dtype=float32), Array(0.03794313, dtype=float32)), 'eval/episode_reward': (Array(-39651.047, dtype=float32), Array(21128.746, dtype=float32)), 'eval/avg_episode_length': (Array(798.0078, dtype=float32), Array(400.08194, dtype=float32)), 'eval/epoch_eval_time': 4.122629642486572, 'eval/sps': 31048.144291417975}
I0726 23:48:56.161651 140267183036224 train.py:379] starting iteration 134 994.4266803264618
I0726 23:49:03.347979 140267183036224 train.py:394] {'eval/walltime': 564.9718592166901, 'training/sps': 40170.62869617024, 'training/walltime': 431.7013740539551, 'training/entropy_loss': Array(-0.00471016, dtype=float32), 'training/policy_loss': Array(0.00022801, dtype=float32), 'training/total_loss': Array(4187.599, dtype=float32), 'training/v_loss': Array(4187.6035, dtype=float32), 'eval/episode_goal_distance': (Array(0.28649026, dtype=float32), Array(0.03932974, dtype=float32)), 'eval/episode_reward': (Array(-38423.46, dtype=float32), Array(21333.303, dtype=float32)), 'eval/avg_episode_length': (Array(782.5625, dtype=float32), Array(410.91885, dtype=float32)), 'eval/epoch_eval_time': 4.123845815658569, 'eval/sps': 31038.987809382656}
I0726 23:49:03.350366 140267183036224 train.py:379] starting iteration 135 1001.6153953075409
I0726 23:49:10.529771 140267183036224 train.py:394] {'eval/walltime': 569.0929968357086, 'training/sps': 40228.094840992446, 'training/walltime': 434.75595569610596, 'training/entropy_loss': Array(-0.00509292, dtype=float32), 'training/policy_loss': Array(0.00036164, dtype=float32), 'training/total_loss': Array(3735.6482, dtype=float32), 'training/v_loss': Array(3735.6528, dtype=float32), 'eval/episode_goal_distance': (Array(0.2820194, dtype=float32), Array(0.03748683, dtype=float32)), 'eval/episode_reward': (Array(-39761.844, dtype=float32), Array(21114.037, dtype=float32)), 'eval/avg_episode_length': (Array(798.0469, dtype=float32), Array(400.00464, dtype=float32)), 'eval/epoch_eval_time': 4.121137619018555, 'eval/sps': 31059.385012840965}
I0726 23:49:10.532306 140267183036224 train.py:379] starting iteration 136 1008.7973356246948
I0726 23:49:17.710744 140267183036224 train.py:394] {'eval/walltime': 573.2116043567657, 'training/sps': 40206.04589328077, 'training/walltime': 437.8122124671936, 'training/entropy_loss': Array(-0.00513958, dtype=float32), 'training/policy_loss': Array(-8.307667e-06, dtype=float32), 'training/total_loss': Array(2640.0352, dtype=float32), 'training/v_loss': Array(2640.04, dtype=float32), 'eval/episode_goal_distance': (Array(0.28797686, dtype=float32), Array(0.03796668, dtype=float32)), 'eval/episode_reward': (Array(-40696.49, dtype=float32), Array(21319.52, dtype=float32)), 'eval/avg_episode_length': (Array(798.03906, dtype=float32), Array(400.02026, dtype=float32)), 'eval/epoch_eval_time': 4.118607521057129, 'eval/sps': 31078.46507480423}
I0726 23:49:17.713158 140267183036224 train.py:379] starting iteration 137 1015.978188753128
I0726 23:49:24.893791 140267183036224 train.py:394] {'eval/walltime': 577.3406763076782, 'training/sps': 40316.25546901914, 'training/walltime': 440.8601145744324, 'training/entropy_loss': Array(-0.00435529, dtype=float32), 'training/policy_loss': Array(0.00036985, dtype=float32), 'training/total_loss': Array(2974.8682, dtype=float32), 'training/v_loss': Array(2974.872, dtype=float32), 'eval/episode_goal_distance': (Array(0.2799905, dtype=float32), Array(0.03999461, dtype=float32)), 'eval/episode_reward': (Array(-39794.484, dtype=float32), Array(20702.254, dtype=float32)), 'eval/avg_episode_length': (Array(805.9453, dtype=float32), Array(393.88895, dtype=float32)), 'eval/epoch_eval_time': 4.129071950912476, 'eval/sps': 30999.70199640467}
I0726 23:49:24.896233 140267183036224 train.py:379] starting iteration 138 1023.161262512207
I0726 23:49:32.076315 140267183036224 train.py:394] {'eval/walltime': 581.4697201251984, 'training/sps': 40322.02755156119, 'training/walltime': 443.9075803756714, 'training/entropy_loss': Array(-0.00391531, dtype=float32), 'training/policy_loss': Array(0.00037118, dtype=float32), 'training/total_loss': Array(2684.8726, dtype=float32), 'training/v_loss': Array(2684.876, dtype=float32), 'eval/episode_goal_distance': (Array(0.28409988, dtype=float32), Array(0.04287402, dtype=float32)), 'eval/episode_reward': (Array(-36796.31, dtype=float32), Array(22229.717, dtype=float32)), 'eval/avg_episode_length': (Array(751.4922, dtype=float32), Array(430.429, dtype=float32)), 'eval/epoch_eval_time': 4.129043817520142, 'eval/sps': 30999.913214017524}
I0726 23:49:32.078720 140267183036224 train.py:379] starting iteration 139 1030.3437485694885
I0726 23:49:39.266454 140267183036224 train.py:394] {'eval/walltime': 585.6041574478149, 'training/sps': 40293.996392745146, 'training/walltime': 446.95716619491577, 'training/entropy_loss': Array(-0.00374181, dtype=float32), 'training/policy_loss': Array(0.00011945, dtype=float32), 'training/total_loss': Array(2725.3057, dtype=float32), 'training/v_loss': Array(2725.309, dtype=float32), 'eval/episode_goal_distance': (Array(0.28077853, dtype=float32), Array(0.03414112, dtype=float32)), 'eval/episode_reward': (Array(-40273.15, dtype=float32), Array(20054.281, dtype=float32)), 'eval/avg_episode_length': (Array(821.3906, dtype=float32), Array(381.6245, dtype=float32)), 'eval/epoch_eval_time': 4.134437322616577, 'eval/sps': 30959.472840428054}
I0726 23:49:39.268952 140267183036224 train.py:379] starting iteration 140 1037.533982038498
I0726 23:49:46.456180 140267183036224 train.py:394] {'eval/walltime': 589.7440330982208, 'training/sps': 40369.825786504814, 'training/walltime': 450.00102376937866, 'training/entropy_loss': Array(-0.00452418, dtype=float32), 'training/policy_loss': Array(0.00031404, dtype=float32), 'training/total_loss': Array(2629.648, dtype=float32), 'training/v_loss': Array(2629.6519, dtype=float32), 'eval/episode_goal_distance': (Array(0.28607923, dtype=float32), Array(0.03615358, dtype=float32)), 'eval/episode_reward': (Array(-40687.727, dtype=float32), Array(20927.762, dtype=float32)), 'eval/avg_episode_length': (Array(805.7422, dtype=float32), Array(394.3011, dtype=float32)), 'eval/epoch_eval_time': 4.139875650405884, 'eval/sps': 30918.80307744281}
I0726 23:49:46.458577 140267183036224 train.py:379] starting iteration 141 1044.7236070632935
I0726 23:49:53.627864 140267183036224 train.py:394] {'eval/walltime': 593.8636107444763, 'training/sps': 40341.6618433462, 'training/walltime': 453.0470063686371, 'training/entropy_loss': Array(-0.0041611, dtype=float32), 'training/policy_loss': Array(8.27076e-05, dtype=float32), 'training/total_loss': Array(2508.154, dtype=float32), 'training/v_loss': Array(2508.1582, dtype=float32), 'eval/episode_goal_distance': (Array(0.28044695, dtype=float32), Array(0.03890312, dtype=float32)), 'eval/episode_reward': (Array(-38699.258, dtype=float32), Array(21462.09, dtype=float32)), 'eval/avg_episode_length': (Array(782.47656, dtype=float32), Array(411.08185, dtype=float32)), 'eval/epoch_eval_time': 4.119577646255493, 'eval/sps': 31071.146362867104}
I0726 23:49:53.630482 140267183036224 train.py:379] starting iteration 142 1051.8955116271973
I0726 23:50:00.795350 140267183036224 train.py:394] {'eval/walltime': 597.9825406074524, 'training/sps': 40392.042120897735, 'training/walltime': 456.0891897678375, 'training/entropy_loss': Array(-0.00364941, dtype=float32), 'training/policy_loss': Array(0.00050922, dtype=float32), 'training/total_loss': Array(2365.981, dtype=float32), 'training/v_loss': Array(2365.9841, dtype=float32), 'eval/episode_goal_distance': (Array(0.2868957, dtype=float32), Array(0.03731504, dtype=float32)), 'eval/episode_reward': (Array(-37551.12, dtype=float32), Array(23232.627, dtype=float32)), 'eval/avg_episode_length': (Array(735.75, dtype=float32), Array(439.38004, dtype=float32)), 'eval/epoch_eval_time': 4.118929862976074, 'eval/sps': 31076.032915868935}
I0726 23:50:00.800159 140267183036224 train.py:379] starting iteration 143 1059.0651738643646
I0726 23:50:07.960659 140267183036224 train.py:394] {'eval/walltime': 602.0930833816528, 'training/sps': 40350.39152625931, 'training/walltime': 459.1345133781433, 'training/entropy_loss': Array(-0.00313332, dtype=float32), 'training/policy_loss': Array(0.00043137, dtype=float32), 'training/total_loss': Array(2562.44, dtype=float32), 'training/v_loss': Array(2562.4426, dtype=float32), 'eval/episode_goal_distance': (Array(0.27723342, dtype=float32), Array(0.03534575, dtype=float32)), 'eval/episode_reward': (Array(-39332.43, dtype=float32), Array(21206.393, dtype=float32)), 'eval/avg_episode_length': (Array(790.2344, dtype=float32), Array(405.70822, dtype=float32)), 'eval/epoch_eval_time': 4.1105427742004395, 'eval/sps': 31139.439979407067}
I0726 23:50:07.963319 140267183036224 train.py:379] starting iteration 144 1066.228348493576
I0726 23:50:15.146229 140267183036224 train.py:394] {'eval/walltime': 606.2162683010101, 'training/sps': 40258.34238410876, 'training/walltime': 462.18680000305176, 'training/entropy_loss': Array(-0.0033031, dtype=float32), 'training/policy_loss': Array(0.00034892, dtype=float32), 'training/total_loss': Array(2411.6743, dtype=float32), 'training/v_loss': Array(2411.6772, dtype=float32), 'eval/episode_goal_distance': (Array(0.28489152, dtype=float32), Array(0.0395188, dtype=float32)), 'eval/episode_reward': (Array(-39553.15, dtype=float32), Array(21277.645, dtype=float32)), 'eval/avg_episode_length': (Array(790.3125, dtype=float32), Array(405.5574, dtype=float32)), 'eval/epoch_eval_time': 4.1231849193573, 'eval/sps': 31043.96298091621}
I0726 23:50:15.148747 140267183036224 train.py:379] starting iteration 145 1073.4137773513794
I0726 23:50:22.319354 140267183036224 train.py:394] {'eval/walltime': 610.3382091522217, 'training/sps': 40361.751453355195, 'training/walltime': 465.2312664985657, 'training/entropy_loss': Array(-0.00332055, dtype=float32), 'training/policy_loss': Array(0.00026736, dtype=float32), 'training/total_loss': Array(2434.0986, dtype=float32), 'training/v_loss': Array(2434.1018, dtype=float32), 'eval/episode_goal_distance': (Array(0.28140044, dtype=float32), Array(0.03770116, dtype=float32)), 'eval/episode_reward': (Array(-38544.844, dtype=float32), Array(21253.172, dtype=float32)), 'eval/avg_episode_length': (Array(782.4375, dtype=float32), Array(411.15512, dtype=float32)), 'eval/epoch_eval_time': 4.121940851211548, 'eval/sps': 31053.332549004772}
I0726 23:50:22.321939 140267183036224 train.py:379] starting iteration 146 1080.5869688987732
I0726 23:50:29.485624 140267183036224 train.py:394] {'eval/walltime': 614.4526295661926, 'training/sps': 40369.63606268824, 'training/walltime': 468.2751383781433, 'training/entropy_loss': Array(-0.00376707, dtype=float32), 'training/policy_loss': Array(0.00026801, dtype=float32), 'training/total_loss': Array(2395.3086, dtype=float32), 'training/v_loss': Array(2395.312, dtype=float32), 'eval/episode_goal_distance': (Array(0.27960408, dtype=float32), Array(0.03457075, dtype=float32)), 'eval/episode_reward': (Array(-37855.293, dtype=float32), Array(21402.76, dtype=float32)), 'eval/avg_episode_length': (Array(774.65625, dtype=float32), Array(416.3562, dtype=float32)), 'eval/epoch_eval_time': 4.114420413970947, 'eval/sps': 31110.09258202262}
I0726 23:50:29.488239 140267183036224 train.py:379] starting iteration 147 1087.7532687187195
I0726 23:50:36.663363 140267183036224 train.py:394] {'eval/walltime': 618.5755598545074, 'training/sps': 40339.22111368092, 'training/walltime': 471.3213052749634, 'training/entropy_loss': Array(-0.00454133, dtype=float32), 'training/policy_loss': Array(0.00018612, dtype=float32), 'training/total_loss': Array(2413.5212, dtype=float32), 'training/v_loss': Array(2413.5256, dtype=float32), 'eval/episode_goal_distance': (Array(0.28206903, dtype=float32), Array(0.04227339, dtype=float32)), 'eval/episode_reward': (Array(-38230.363, dtype=float32), Array(20981.42, dtype=float32)), 'eval/avg_episode_length': (Array(790.2578, dtype=float32), Array(405.6636, dtype=float32)), 'eval/epoch_eval_time': 4.122930288314819, 'eval/sps': 31045.88024754547}
I0726 23:50:36.665852 140267183036224 train.py:379] starting iteration 148 1094.9308812618256
I0726 23:50:43.840793 140267183036224 train.py:394] {'eval/walltime': 622.6940412521362, 'training/sps': 40258.42100021457, 'training/walltime': 474.37358593940735, 'training/entropy_loss': Array(-0.00430718, dtype=float32), 'training/policy_loss': Array(0.00022391, dtype=float32), 'training/total_loss': Array(2393.6914, dtype=float32), 'training/v_loss': Array(2393.6953, dtype=float32), 'eval/episode_goal_distance': (Array(0.27946803, dtype=float32), Array(0.03605168, dtype=float32)), 'eval/episode_reward': (Array(-40215.285, dtype=float32), Array(19529.219, dtype=float32)), 'eval/avg_episode_length': (Array(829.33594, dtype=float32), Array(374.61484, dtype=float32)), 'eval/epoch_eval_time': 4.118481397628784, 'eval/sps': 31079.41681458025}
I0726 23:50:43.843292 140267183036224 train.py:379] starting iteration 149 1102.1083221435547
I0726 23:50:51.016116 140267183036224 train.py:394] {'eval/walltime': 626.8124279975891, 'training/sps': 40282.649380479554, 'training/walltime': 477.42403078079224, 'training/entropy_loss': Array(-0.00353944, dtype=float32), 'training/policy_loss': Array(0.0001675, dtype=float32), 'training/total_loss': Array(2314.3765, dtype=float32), 'training/v_loss': Array(2314.38, dtype=float32), 'eval/episode_goal_distance': (Array(0.2860202, dtype=float32), Array(0.03657784, dtype=float32)), 'eval/episode_reward': (Array(-36280.875, dtype=float32), Array(22584.975, dtype=float32)), 'eval/avg_episode_length': (Array(735.71094, dtype=float32), Array(439.44498, dtype=float32)), 'eval/epoch_eval_time': 4.118386745452881, 'eval/sps': 31080.13110748403}
I0726 23:50:51.018672 140267183036224 train.py:379] starting iteration 150 1109.283702135086
I0726 23:50:58.196456 140267183036224 train.py:394] {'eval/walltime': 630.9353239536285, 'training/sps': 40273.82624579442, 'training/walltime': 480.47514390945435, 'training/entropy_loss': Array(-0.0037442, dtype=float32), 'training/policy_loss': Array(-0.00033876, dtype=float32), 'training/total_loss': Array(13711.46, dtype=float32), 'training/v_loss': Array(13711.465, dtype=float32), 'eval/episode_goal_distance': (Array(0.2795421, dtype=float32), Array(0.03582052, dtype=float32)), 'eval/episode_reward': (Array(-40064.562, dtype=float32), Array(20133.912, dtype=float32)), 'eval/avg_episode_length': (Array(821.3594, dtype=float32), Array(381.69073, dtype=float32)), 'eval/epoch_eval_time': 4.122895956039429, 'eval/sps': 31046.138773523755}
I0726 23:50:58.199036 140267183036224 train.py:379] starting iteration 151 1116.4640665054321
I0726 23:51:05.384598 140267183036224 train.py:394] {'eval/walltime': 635.0601577758789, 'training/sps': 40197.07130986672, 'training/walltime': 483.5320830345154, 'training/entropy_loss': Array(-0.00504491, dtype=float32), 'training/policy_loss': Array(0.00040341, dtype=float32), 'training/total_loss': Array(3800.7224, dtype=float32), 'training/v_loss': Array(3800.7273, dtype=float32), 'eval/episode_goal_distance': (Array(0.28387752, dtype=float32), Array(0.03938747, dtype=float32)), 'eval/episode_reward': (Array(-38941.758, dtype=float32), Array(22012.031, dtype=float32)), 'eval/avg_episode_length': (Array(782.41406, dtype=float32), Array(411.19962, dtype=float32)), 'eval/epoch_eval_time': 4.124833822250366, 'eval/sps': 31031.553152404973}
I0726 23:51:05.387205 140267183036224 train.py:379] starting iteration 152 1123.652235031128
I0726 23:51:12.563668 140267183036224 train.py:394] {'eval/walltime': 639.1784536838531, 'training/sps': 40229.14987993607, 'training/walltime': 486.5865845680237, 'training/entropy_loss': Array(-0.00575001, dtype=float32), 'training/policy_loss': Array(0.00025216, dtype=float32), 'training/total_loss': Array(3090.4358, dtype=float32), 'training/v_loss': Array(3090.4412, dtype=float32), 'eval/episode_goal_distance': (Array(0.28582928, dtype=float32), Array(0.03311166, dtype=float32)), 'eval/episode_reward': (Array(-37944.27, dtype=float32), Array(22337.357, dtype=float32)), 'eval/avg_episode_length': (Array(759.1406, dtype=float32), Array(426.05872, dtype=float32)), 'eval/epoch_eval_time': 4.118295907974243, 'eval/sps': 31080.81664363991}
I0726 23:51:12.566082 140267183036224 train.py:379] starting iteration 153 1130.8311111927032
I0726 23:51:19.745962 140267183036224 train.py:394] {'eval/walltime': 643.2993342876434, 'training/sps': 40216.56837851866, 'training/walltime': 489.642041683197, 'training/entropy_loss': Array(-0.00485968, dtype=float32), 'training/policy_loss': Array(3.0645984e-05, dtype=float32), 'training/total_loss': Array(2662.118, dtype=float32), 'training/v_loss': Array(2662.1226, dtype=float32), 'eval/episode_goal_distance': (Array(0.28390598, dtype=float32), Array(0.03906472, dtype=float32)), 'eval/episode_reward': (Array(-45259.57, dtype=float32), Array(16400.842, dtype=float32)), 'eval/avg_episode_length': (Array(906.75, dtype=float32), Array(289.9267, dtype=float32)), 'eval/epoch_eval_time': 4.120880603790283, 'eval/sps': 31061.322155819995}
I0726 23:51:19.825994 140267183036224 train.py:379] starting iteration 154 1138.091022014618
I0726 23:51:27.023733 140267183036224 train.py:394] {'eval/walltime': 647.4428491592407, 'training/sps': 40283.53411058063, 'training/walltime': 492.6924195289612, 'training/entropy_loss': Array(-0.00500725, dtype=float32), 'training/policy_loss': Array(0.00035751, dtype=float32), 'training/total_loss': Array(2574.0415, dtype=float32), 'training/v_loss': Array(2574.0464, dtype=float32), 'eval/episode_goal_distance': (Array(0.27890724, dtype=float32), Array(0.03488865, dtype=float32)), 'eval/episode_reward': (Array(-38253.78, dtype=float32), Array(20540.197, dtype=float32)), 'eval/avg_episode_length': (Array(790.35156, dtype=float32), Array(405.48227, dtype=float32)), 'eval/epoch_eval_time': 4.14351487159729, 'eval/sps': 30891.647301040597}
I0726 23:51:27.026477 140267183036224 train.py:379] starting iteration 155 1145.2915070056915
I0726 23:51:34.192523 140267183036224 train.py:394] {'eval/walltime': 651.562371969223, 'training/sps': 40383.75322126352, 'training/walltime': 495.7352273464203, 'training/entropy_loss': Array(-0.00513788, dtype=float32), 'training/policy_loss': Array(-3.4940007e-05, dtype=float32), 'training/total_loss': Array(2763.4336, dtype=float32), 'training/v_loss': Array(2763.439, dtype=float32), 'eval/episode_goal_distance': (Array(0.28959364, dtype=float32), Array(0.03320614, dtype=float32)), 'eval/episode_reward': (Array(-41466.617, dtype=float32), Array(20782.67, dtype=float32)), 'eval/avg_episode_length': (Array(813.6094, dtype=float32), Array(388.00424, dtype=float32)), 'eval/epoch_eval_time': 4.1195228099823, 'eval/sps': 31071.559960739716}
I0726 23:51:34.194953 140267183036224 train.py:379] starting iteration 156 1152.4599831104279
I0726 23:51:41.354007 140267183036224 train.py:394] {'eval/walltime': 655.6751549243927, 'training/sps': 40385.8132600313, 'training/walltime': 498.7778799533844, 'training/entropy_loss': Array(-0.00422631, dtype=float32), 'training/policy_loss': Array(0.0004428, dtype=float32), 'training/total_loss': Array(2703.9055, dtype=float32), 'training/v_loss': Array(2703.9092, dtype=float32), 'eval/episode_goal_distance': (Array(0.28531888, dtype=float32), Array(0.03514119, dtype=float32)), 'eval/episode_reward': (Array(-39348.246, dtype=float32), Array(22213.295, dtype=float32)), 'eval/avg_episode_length': (Array(774.75, dtype=float32), Array(416.1827, dtype=float32)), 'eval/epoch_eval_time': 4.112782955169678, 'eval/sps': 31122.47871945365}
I0726 23:51:41.356440 140267183036224 train.py:379] starting iteration 157 1159.6214699745178
I0726 23:51:48.513229 140267183036224 train.py:394] {'eval/walltime': 659.7870056629181, 'training/sps': 40405.13595365588, 'training/walltime': 501.81907749176025, 'training/entropy_loss': Array(-0.00322627, dtype=float32), 'training/policy_loss': Array(0.00013244, dtype=float32), 'training/total_loss': Array(2516.6577, dtype=float32), 'training/v_loss': Array(2516.6611, dtype=float32), 'eval/episode_goal_distance': (Array(0.2853606, dtype=float32), Array(0.03779604, dtype=float32)), 'eval/episode_reward': (Array(-42117.367, dtype=float32), Array(19985.97, dtype=float32)), 'eval/avg_episode_length': (Array(836.7422, dtype=float32), Array(368.51645, dtype=float32)), 'eval/epoch_eval_time': 4.111850738525391, 'eval/sps': 31129.53464013723}
I0726 23:51:48.515796 140267183036224 train.py:379] starting iteration 158 1166.7808258533478
I0726 23:51:55.676797 140267183036224 train.py:394] {'eval/walltime': 663.9025311470032, 'training/sps': 40395.96779647175, 'training/walltime': 504.8609652519226, 'training/entropy_loss': Array(-0.00312062, dtype=float32), 'training/policy_loss': Array(-4.6106565e-05, dtype=float32), 'training/total_loss': Array(2381.0415, dtype=float32), 'training/v_loss': Array(2381.0447, dtype=float32), 'eval/episode_goal_distance': (Array(0.28518087, dtype=float32), Array(0.0369236, dtype=float32)), 'eval/episode_reward': (Array(-34662.18, dtype=float32), Array(23900.703, dtype=float32)), 'eval/avg_episode_length': (Array(689.2969, dtype=float32), Array(460.84836, dtype=float32)), 'eval/epoch_eval_time': 4.115525484085083, 'eval/sps': 31101.739132701667}
I0726 23:51:55.679264 140267183036224 train.py:379] starting iteration 159 1173.9442932605743
I0726 23:52:02.835991 140267183036224 train.py:394] {'eval/walltime': 668.0121686458588, 'training/sps': 40382.76916017931, 'training/walltime': 507.9038472175598, 'training/entropy_loss': Array(-0.00324118, dtype=float32), 'training/policy_loss': Array(5.8399586e-05, dtype=float32), 'training/total_loss': Array(2539.4077, dtype=float32), 'training/v_loss': Array(2539.411, dtype=float32), 'eval/episode_goal_distance': (Array(0.2866568, dtype=float32), Array(0.0418317, dtype=float32)), 'eval/episode_reward': (Array(-38736.184, dtype=float32), Array(21853.625, dtype=float32)), 'eval/avg_episode_length': (Array(774.7656, dtype=float32), Array(416.15424, dtype=float32)), 'eval/epoch_eval_time': 4.109637498855591, 'eval/sps': 31146.299408559542}
I0726 23:52:02.838475 140267183036224 train.py:379] starting iteration 160 1181.103505373001
I0726 23:52:10.028074 140267183036224 train.py:394] {'eval/walltime': 672.1529047489166, 'training/sps': 40357.519567555784, 'training/walltime': 510.94863295555115, 'training/entropy_loss': Array(-0.00386036, dtype=float32), 'training/policy_loss': Array(0.00025062, dtype=float32), 'training/total_loss': Array(2434.7024, dtype=float32), 'training/v_loss': Array(2434.706, dtype=float32), 'eval/episode_goal_distance': (Array(0.27824968, dtype=float32), Array(0.0413572, dtype=float32)), 'eval/episode_reward': (Array(-35382.953, dtype=float32), Array(22594.76, dtype=float32)), 'eval/avg_episode_length': (Array(728.14844, dtype=float32), Array(443.13953, dtype=float32)), 'eval/epoch_eval_time': 4.140736103057861, 'eval/sps': 30912.378092744002}
I0726 23:52:10.030615 140267183036224 train.py:379] starting iteration 161 1188.2956447601318
I0726 23:52:17.186683 140267183036224 train.py:394] {'eval/walltime': 676.2613034248352, 'training/sps': 40374.44927468794, 'training/walltime': 513.9921419620514, 'training/entropy_loss': Array(-0.00458618, dtype=float32), 'training/policy_loss': Array(0.0004025, dtype=float32), 'training/total_loss': Array(2319.2988, dtype=float32), 'training/v_loss': Array(2319.303, dtype=float32), 'eval/episode_goal_distance': (Array(0.28509763, dtype=float32), Array(0.03711705, dtype=float32)), 'eval/episode_reward': (Array(-38214.13, dtype=float32), Array(23156.129, dtype=float32)), 'eval/avg_episode_length': (Array(751.41406, dtype=float32), Array(430.56442, dtype=float32)), 'eval/epoch_eval_time': 4.108398675918579, 'eval/sps': 31155.691084770646}
I0726 23:52:17.189236 140267183036224 train.py:379] starting iteration 162 1195.4542660713196
I0726 23:52:24.352179 140267183036224 train.py:394] {'eval/walltime': 680.3751006126404, 'training/sps': 40382.62361180736, 'training/walltime': 517.0350348949432, 'training/entropy_loss': Array(-0.00536509, dtype=float32), 'training/policy_loss': Array(0.00029424, dtype=float32), 'training/total_loss': Array(2498.302, dtype=float32), 'training/v_loss': Array(2498.3071, dtype=float32), 'eval/episode_goal_distance': (Array(0.28514445, dtype=float32), Array(0.036027, dtype=float32)), 'eval/episode_reward': (Array(-40814.062, dtype=float32), Array(21307.387, dtype=float32)), 'eval/avg_episode_length': (Array(805.75, dtype=float32), Array(394.28506, dtype=float32)), 'eval/epoch_eval_time': 4.113797187805176, 'eval/sps': 31114.805654357388}
I0726 23:52:24.354418 140267183036224 train.py:379] starting iteration 163 1202.6194484233856
I0726 23:52:31.543451 140267183036224 train.py:394] {'eval/walltime': 684.5163781642914, 'training/sps': 40371.13492944397, 'training/walltime': 520.0787937641144, 'training/entropy_loss': Array(-0.0056367, dtype=float32), 'training/policy_loss': Array(6.0647966e-05, dtype=float32), 'training/total_loss': Array(2383.038, dtype=float32), 'training/v_loss': Array(2383.0435, dtype=float32), 'eval/episode_goal_distance': (Array(0.28385365, dtype=float32), Array(0.03660218, dtype=float32)), 'eval/episode_reward': (Array(-38534.168, dtype=float32), Array(21799.771, dtype=float32)), 'eval/avg_episode_length': (Array(774.7031, dtype=float32), Array(416.26926, dtype=float32)), 'eval/epoch_eval_time': 4.141277551651001, 'eval/sps': 30908.33647432549}
I0726 23:52:31.545737 140267183036224 train.py:379] starting iteration 164 1209.8107664585114
I0726 23:52:38.704651 140267183036224 train.py:394] {'eval/walltime': 688.6237578392029, 'training/sps': 40322.88561967296, 'training/walltime': 523.1261947154999, 'training/entropy_loss': Array(-0.00621742, dtype=float32), 'training/policy_loss': Array(0.00024853, dtype=float32), 'training/total_loss': Array(2535.0928, dtype=float32), 'training/v_loss': Array(2535.0986, dtype=float32), 'eval/episode_goal_distance': (Array(0.28203148, dtype=float32), Array(0.03474677, dtype=float32)), 'eval/episode_reward': (Array(-40496.043, dtype=float32), Array(20939.234, dtype=float32)), 'eval/avg_episode_length': (Array(806., dtype=float32), Array(393.77805, dtype=float32)), 'eval/epoch_eval_time': 4.107379674911499, 'eval/sps': 31163.420509149302}
I0726 23:52:38.707033 140267183036224 train.py:379] starting iteration 165 1216.9720618724823
I0726 23:52:45.860805 140267183036224 train.py:394] {'eval/walltime': 692.7339279651642, 'training/sps': 40427.958814938786, 'training/walltime': 526.1656754016876, 'training/entropy_loss': Array(-0.00671023, dtype=float32), 'training/policy_loss': Array(0.00023458, dtype=float32), 'training/total_loss': Array(2318.6768, dtype=float32), 'training/v_loss': Array(2318.683, dtype=float32), 'eval/episode_goal_distance': (Array(0.28250846, dtype=float32), Array(0.03733535, dtype=float32)), 'eval/episode_reward': (Array(-41976.668, dtype=float32), Array(20529.37, dtype=float32)), 'eval/avg_episode_length': (Array(829.1875, dtype=float32), Array(374.9406, dtype=float32)), 'eval/epoch_eval_time': 4.110170125961304, 'eval/sps': 31142.26323419224}
I0726 23:52:45.863297 140267183036224 train.py:379] starting iteration 166 1224.12832736969
I0726 23:52:53.018785 140267183036224 train.py:394] {'eval/walltime': 696.8421776294708, 'training/sps': 40378.43163618584, 'training/walltime': 529.2088842391968, 'training/entropy_loss': Array(-0.00675832, dtype=float32), 'training/policy_loss': Array(0.00025746, dtype=float32), 'training/total_loss': Array(12154.956, dtype=float32), 'training/v_loss': Array(12154.963, dtype=float32), 'eval/episode_goal_distance': (Array(0.28469348, dtype=float32), Array(0.03381513, dtype=float32)), 'eval/episode_reward': (Array(-41207.535, dtype=float32), Array(20331.852, dtype=float32)), 'eval/avg_episode_length': (Array(821.3828, dtype=float32), Array(381.64124, dtype=float32)), 'eval/epoch_eval_time': 4.108249664306641, 'eval/sps': 31156.821142612538}
I0726 23:52:53.021121 140267183036224 train.py:379] starting iteration 167 1231.2861504554749
I0726 23:53:00.201138 140267183036224 train.py:394] {'eval/walltime': 700.973037481308, 'training/sps': 40353.512894000756, 'training/walltime': 532.2539722919464, 'training/entropy_loss': Array(-0.00756649, dtype=float32), 'training/policy_loss': Array(0.00026366, dtype=float32), 'training/total_loss': Array(3859.166, dtype=float32), 'training/v_loss': Array(3859.1733, dtype=float32), 'eval/episode_goal_distance': (Array(0.28431153, dtype=float32), Array(0.04084364, dtype=float32)), 'eval/episode_reward': (Array(-38108.062, dtype=float32), Array(22585.514, dtype=float32)), 'eval/avg_episode_length': (Array(759.21094, dtype=float32), Array(425.9345, dtype=float32)), 'eval/epoch_eval_time': 4.130859851837158, 'eval/sps': 30986.284839238324}
I0726 23:53:00.203387 140267183036224 train.py:379] starting iteration 168 1238.4684171676636
I0726 23:53:07.362139 140267183036224 train.py:394] {'eval/walltime': 705.0802419185638, 'training/sps': 40322.9045480577, 'training/walltime': 535.3013718128204, 'training/entropy_loss': Array(-0.00784448, dtype=float32), 'training/policy_loss': Array(-6.089337e-05, dtype=float32), 'training/total_loss': Array(3362.2056, dtype=float32), 'training/v_loss': Array(3362.2136, dtype=float32), 'eval/episode_goal_distance': (Array(0.28667235, dtype=float32), Array(0.03961577, dtype=float32)), 'eval/episode_reward': (Array(-40839.5, dtype=float32), Array(21233.643, dtype=float32)), 'eval/avg_episode_length': (Array(805.89844, dtype=float32), Array(393.9841, dtype=float32)), 'eval/epoch_eval_time': 4.107204437255859, 'eval/sps': 31164.750125153365}
I0726 23:53:07.364612 140267183036224 train.py:379] starting iteration 169 1245.6296422481537
I0726 23:53:14.529802 140267183036224 train.py:394] {'eval/walltime': 709.1962161064148, 'training/sps': 40350.100897158525, 'training/walltime': 538.3467173576355, 'training/entropy_loss': Array(-0.00911126, dtype=float32), 'training/policy_loss': Array(-0.00016195, dtype=float32), 'training/total_loss': Array(3028.254, dtype=float32), 'training/v_loss': Array(3028.2634, dtype=float32), 'eval/episode_goal_distance': (Array(0.2841829, dtype=float32), Array(0.03197197, dtype=float32)), 'eval/episode_reward': (Array(-41660.152, dtype=float32), Array(20322.643, dtype=float32)), 'eval/avg_episode_length': (Array(821.2656, dtype=float32), Array(381.89114, dtype=float32)), 'eval/epoch_eval_time': 4.115974187850952, 'eval/sps': 31098.34857026444}
I0726 23:53:14.532073 140267183036224 train.py:379] starting iteration 170 1252.7971031665802
I0726 23:53:21.690670 140267183036224 train.py:394] {'eval/walltime': 713.3044686317444, 'training/sps': 40338.450750634314, 'training/walltime': 541.3929424285889, 'training/entropy_loss': Array(-0.00835269, dtype=float32), 'training/policy_loss': Array(-0.00039982, dtype=float32), 'training/total_loss': Array(2483.5825, dtype=float32), 'training/v_loss': Array(2483.5913, dtype=float32), 'eval/episode_goal_distance': (Array(0.285305, dtype=float32), Array(0.03636632, dtype=float32)), 'eval/episode_reward': (Array(-40056.65, dtype=float32), Array(21121.734, dtype=float32)), 'eval/avg_episode_length': (Array(797.96094, dtype=float32), Array(400.1749, dtype=float32)), 'eval/epoch_eval_time': 4.10825252532959, 'eval/sps': 31156.799444730103}
I0726 23:53:21.693017 140267183036224 train.py:379] starting iteration 171 1259.9580466747284
I0726 23:53:28.873027 140267183036224 train.py:394] {'eval/walltime': 717.437374830246, 'training/sps': 40381.750343607906, 'training/walltime': 544.4359011650085, 'training/entropy_loss': Array(-0.00716516, dtype=float32), 'training/policy_loss': Array(0.00027281, dtype=float32), 'training/total_loss': Array(2702.487, dtype=float32), 'training/v_loss': Array(2702.494, dtype=float32), 'eval/episode_goal_distance': (Array(0.28414878, dtype=float32), Array(0.03621456, dtype=float32)), 'eval/episode_reward': (Array(-39607.234, dtype=float32), Array(20875.592, dtype=float32)), 'eval/avg_episode_length': (Array(798.1406, dtype=float32), Array(399.81873, dtype=float32)), 'eval/epoch_eval_time': 4.132906198501587, 'eval/sps': 30970.942443941087}
I0726 23:53:28.877731 140267183036224 train.py:379] starting iteration 172 1267.1427443027496
I0726 23:53:36.035528 140267183036224 train.py:394] {'eval/walltime': 721.5427579879761, 'training/sps': 40317.30882965174, 'training/walltime': 547.4837236404419, 'training/entropy_loss': Array(-0.00622429, dtype=float32), 'training/policy_loss': Array(0.00023832, dtype=float32), 'training/total_loss': Array(2864.7842, dtype=float32), 'training/v_loss': Array(2864.7905, dtype=float32), 'eval/episode_goal_distance': (Array(0.28457707, dtype=float32), Array(0.03650609, dtype=float32)), 'eval/episode_reward': (Array(-39076.57, dtype=float32), Array(21727.764, dtype=float32)), 'eval/avg_episode_length': (Array(782.4297, dtype=float32), Array(411.1699, dtype=float32)), 'eval/epoch_eval_time': 4.1053831577301025, 'eval/sps': 31178.575806983183}
I0726 23:53:36.038045 140267183036224 train.py:379] starting iteration 173 1274.3030750751495
I0726 23:53:43.198732 140267183036224 train.py:394] {'eval/walltime': 725.655065536499, 'training/sps': 40363.85034279774, 'training/walltime': 550.5280318260193, 'training/entropy_loss': Array(-0.00613996, dtype=float32), 'training/policy_loss': Array(0.00021843, dtype=float32), 'training/total_loss': Array(2515.5037, dtype=float32), 'training/v_loss': Array(2515.5095, dtype=float32), 'eval/episode_goal_distance': (Array(0.28276452, dtype=float32), Array(0.03695631, dtype=float32)), 'eval/episode_reward': (Array(-40018.906, dtype=float32), Array(21398.795, dtype=float32)), 'eval/avg_episode_length': (Array(797.96875, dtype=float32), Array(400.15912, dtype=float32)), 'eval/epoch_eval_time': 4.112307548522949, 'eval/sps': 31126.076658827424}
I0726 23:53:43.201256 140267183036224 train.py:379] starting iteration 174 1281.4662864208221
I0726 23:53:50.349334 140267183036224 train.py:394] {'eval/walltime': 729.7564063072205, 'training/sps': 40386.85443484185, 'training/walltime': 553.5706059932709, 'training/entropy_loss': Array(-0.00574276, dtype=float32), 'training/policy_loss': Array(9.043724e-05, dtype=float32), 'training/total_loss': Array(2481.6206, dtype=float32), 'training/v_loss': Array(2481.6262, dtype=float32), 'eval/episode_goal_distance': (Array(0.28714412, dtype=float32), Array(0.04017328, dtype=float32)), 'eval/episode_reward': (Array(-37901.086, dtype=float32), Array(23318.217, dtype=float32)), 'eval/avg_episode_length': (Array(743.6719, dtype=float32), Array(434.91275, dtype=float32)), 'eval/epoch_eval_time': 4.1013407707214355, 'eval/sps': 31209.306213656688}
I0726 23:53:50.351814 140267183036224 train.py:379] starting iteration 175 1288.616843700409
I0726 23:53:57.519654 140267183036224 train.py:394] {'eval/walltime': 733.8740904331207, 'training/sps': 40341.131362102766, 'training/walltime': 556.6166286468506, 'training/entropy_loss': Array(-0.00555109, dtype=float32), 'training/policy_loss': Array(-1.565559e-05, dtype=float32), 'training/total_loss': Array(2516.6868, dtype=float32), 'training/v_loss': Array(2516.6924, dtype=float32), 'eval/episode_goal_distance': (Array(0.28070593, dtype=float32), Array(0.03608258, dtype=float32)), 'eval/episode_reward': (Array(-40396.67, dtype=float32), Array(19359.426, dtype=float32)), 'eval/avg_episode_length': (Array(829.1875, dtype=float32), Array(374.94043, dtype=float32)), 'eval/epoch_eval_time': 4.1176841259002686, 'eval/sps': 31085.434454498074}
I0726 23:53:57.522128 140267183036224 train.py:379] starting iteration 176 1295.7871582508087
I0726 23:54:04.712147 140267183036224 train.py:394] {'eval/walltime': 738.0126371383667, 'training/sps': 40323.37145383995, 'training/walltime': 559.6639928817749, 'training/entropy_loss': Array(-0.00638441, dtype=float32), 'training/policy_loss': Array(0.00019357, dtype=float32), 'training/total_loss': Array(2418.4146, dtype=float32), 'training/v_loss': Array(2418.4207, dtype=float32), 'eval/episode_goal_distance': (Array(0.2932102, dtype=float32), Array(0.03581964, dtype=float32)), 'eval/episode_reward': (Array(-40407.664, dtype=float32), Array(21693.52, dtype=float32)), 'eval/avg_episode_length': (Array(790.16406, dtype=float32), Array(405.84436, dtype=float32)), 'eval/epoch_eval_time': 4.138546705245972, 'eval/sps': 30928.731537026935}
I0726 23:54:04.714593 140267183036224 train.py:379] starting iteration 177 1302.9796226024628
I0726 23:54:11.868472 140267183036224 train.py:394] {'eval/walltime': 742.1186525821686, 'training/sps': 40371.16971459308, 'training/walltime': 562.7077491283417, 'training/entropy_loss': Array(-0.00699266, dtype=float32), 'training/policy_loss': Array(1.3586381e-05, dtype=float32), 'training/total_loss': Array(2299.9583, dtype=float32), 'training/v_loss': Array(2299.9653, dtype=float32), 'eval/episode_goal_distance': (Array(0.27913967, dtype=float32), Array(0.03951588, dtype=float32)), 'eval/episode_reward': (Array(-38787.59, dtype=float32), Array(21868.164, dtype=float32)), 'eval/avg_episode_length': (Array(774.91406, dtype=float32), Array(415.87985, dtype=float32)), 'eval/epoch_eval_time': 4.10601544380188, 'eval/sps': 31173.774612372392}
I0726 23:54:11.870950 140267183036224 train.py:379] starting iteration 178 1310.1359794139862
I0726 23:54:19.024773 140267183036224 train.py:394] {'eval/walltime': 746.221031665802, 'training/sps': 40324.573470500516, 'training/walltime': 565.7550225257874, 'training/entropy_loss': Array(-0.00707326, dtype=float32), 'training/policy_loss': Array(9.1488444e-05, dtype=float32), 'training/total_loss': Array(2397.4268, dtype=float32), 'training/v_loss': Array(2397.4336, dtype=float32), 'eval/episode_goal_distance': (Array(0.28217226, dtype=float32), Array(0.03697064, dtype=float32)), 'eval/episode_reward': (Array(-36670.9, dtype=float32), Array(22858.275, dtype=float32)), 'eval/avg_episode_length': (Array(735.90625, dtype=float32), Array(439.12027, dtype=float32)), 'eval/epoch_eval_time': 4.102379083633423, 'eval/sps': 31201.40713242719}
I0726 23:54:19.027278 140267183036224 train.py:379] starting iteration 179 1317.2923085689545
I0726 23:54:26.204348 140267183036224 train.py:394] {'eval/walltime': 750.3440997600555, 'training/sps': 40289.40075978227, 'training/walltime': 568.8049561977386, 'training/entropy_loss': Array(-0.006603, dtype=float32), 'training/policy_loss': Array(-0.00017441, dtype=float32), 'training/total_loss': Array(2244.6016, dtype=float32), 'training/v_loss': Array(2244.6084, dtype=float32), 'eval/episode_goal_distance': (Array(0.28814536, dtype=float32), Array(0.03587358, dtype=float32)), 'eval/episode_reward': (Array(-39271.688, dtype=float32), Array(21883.877, dtype=float32)), 'eval/avg_episode_length': (Array(782.47656, dtype=float32), Array(411.0817, dtype=float32)), 'eval/epoch_eval_time': 4.12306809425354, 'eval/sps': 31044.84259631752}
I0726 23:54:26.206736 140267183036224 train.py:379] starting iteration 180 1324.47176527977
I0726 23:54:33.356317 140267183036224 train.py:394] {'eval/walltime': 754.442387342453, 'training/sps': 40324.04343857332, 'training/walltime': 571.8522696495056, 'training/entropy_loss': Array(-0.00602924, dtype=float32), 'training/policy_loss': Array(0.00026941, dtype=float32), 'training/total_loss': Array(2245.5437, dtype=float32), 'training/v_loss': Array(2245.5493, dtype=float32), 'eval/episode_goal_distance': (Array(0.2880553, dtype=float32), Array(0.03957491, dtype=float32)), 'eval/episode_reward': (Array(-44743.203, dtype=float32), Array(18972.299, dtype=float32)), 'eval/avg_episode_length': (Array(868.0156, dtype=float32), Array(337.25665, dtype=float32)), 'eval/epoch_eval_time': 4.098287582397461, 'eval/sps': 31232.55687321024}
I0726 23:54:33.358911 140267183036224 train.py:379] starting iteration 181 1331.6239404678345
I0726 23:54:40.528097 140267183036224 train.py:394] {'eval/walltime': 758.5581843852997, 'training/sps': 40297.21931588499, 'training/walltime': 574.9016115665436, 'training/entropy_loss': Array(-0.00556901, dtype=float32), 'training/policy_loss': Array(0.00022274, dtype=float32), 'training/total_loss': Array(2383.8738, dtype=float32), 'training/v_loss': Array(2383.8792, dtype=float32), 'eval/episode_goal_distance': (Array(0.28048098, dtype=float32), Array(0.03626356, dtype=float32)), 'eval/episode_reward': (Array(-38388.4, dtype=float32), Array(21870.945, dtype=float32)), 'eval/avg_episode_length': (Array(774.8281, dtype=float32), Array(416.0391, dtype=float32)), 'eval/epoch_eval_time': 4.11579704284668, 'eval/sps': 31099.68705149493}
I0726 23:54:40.530474 140267183036224 train.py:379] starting iteration 182 1338.795503616333
I0726 23:54:47.702438 140267183036224 train.py:394] {'eval/walltime': 762.6800599098206, 'training/sps': 40339.11692351547, 'training/walltime': 577.9477863311768, 'training/entropy_loss': Array(-0.00432065, dtype=float32), 'training/policy_loss': Array(0.00016723, dtype=float32), 'training/total_loss': Array(2183.562, dtype=float32), 'training/v_loss': Array(2183.566, dtype=float32), 'eval/episode_goal_distance': (Array(0.2844695, dtype=float32), Array(0.0363283, dtype=float32)), 'eval/episode_reward': (Array(-38439.86, dtype=float32), Array(21710.531, dtype=float32)), 'eval/avg_episode_length': (Array(774.7578, dtype=float32), Array(416.16864, dtype=float32)), 'eval/epoch_eval_time': 4.121875524520874, 'eval/sps': 31053.824706382587}
I0726 23:54:47.704907 140267183036224 train.py:379] starting iteration 183 1345.9699366092682
I0726 23:54:54.877705 140267183036224 train.py:394] {'eval/walltime': 766.805294752121, 'training/sps': 40370.338049717146, 'training/walltime': 580.9916052818298, 'training/entropy_loss': Array(-0.00511504, dtype=float32), 'training/policy_loss': Array(0.00054339, dtype=float32), 'training/total_loss': Array(12323.307, dtype=float32), 'training/v_loss': Array(12323.312, dtype=float32), 'eval/episode_goal_distance': (Array(0.28751314, dtype=float32), Array(0.04165791, dtype=float32)), 'eval/episode_reward': (Array(-40387.465, dtype=float32), Array(21266.285, dtype=float32)), 'eval/avg_episode_length': (Array(798.14844, dtype=float32), Array(399.804, dtype=float32)), 'eval/epoch_eval_time': 4.125234842300415, 'eval/sps': 31028.536530206722}
I0726 23:54:54.880199 140267183036224 train.py:379] starting iteration 184 1353.1452298164368
I0726 23:55:02.027798 140267183036224 train.py:394] {'eval/walltime': 770.9044489860535, 'training/sps': 40358.448673317485, 'training/walltime': 584.0363209247589, 'training/entropy_loss': Array(-0.00673177, dtype=float32), 'training/policy_loss': Array(0.00043288, dtype=float32), 'training/total_loss': Array(3909.832, dtype=float32), 'training/v_loss': Array(3909.8384, dtype=float32), 'eval/episode_goal_distance': (Array(0.29243353, dtype=float32), Array(0.03965035, dtype=float32)), 'eval/episode_reward': (Array(-40987.383, dtype=float32), Array(22430.316, dtype=float32)), 'eval/avg_episode_length': (Array(790.27344, dtype=float32), Array(405.63287, dtype=float32)), 'eval/epoch_eval_time': 4.099154233932495, 'eval/sps': 31225.953622438865}
I0726 23:55:02.030301 140267183036224 train.py:379] starting iteration 185 1360.2953305244446
I0726 23:55:09.195945 140267183036224 train.py:394] {'eval/walltime': 775.0195178985596, 'training/sps': 40330.24694678378, 'training/walltime': 587.0831656455994, 'training/entropy_loss': Array(-0.00661804, dtype=float32), 'training/policy_loss': Array(3.6777536e-05, dtype=float32), 'training/total_loss': Array(2988.149, dtype=float32), 'training/v_loss': Array(2988.1558, dtype=float32), 'eval/episode_goal_distance': (Array(0.28564858, dtype=float32), Array(0.04131848, dtype=float32)), 'eval/episode_reward': (Array(-39991.758, dtype=float32), Array(21570.58, dtype=float32)), 'eval/avg_episode_length': (Array(790.21875, dtype=float32), Array(405.7388, dtype=float32)), 'eval/epoch_eval_time': 4.1150689125061035, 'eval/sps': 31105.189906053645}
I0726 23:55:09.198577 140267183036224 train.py:379] starting iteration 186 1367.4636061191559
I0726 23:55:16.371068 140267183036224 train.py:394] {'eval/walltime': 779.1418759822845, 'training/sps': 40335.985155242684, 'training/walltime': 590.129576921463, 'training/entropy_loss': Array(-0.00646232, dtype=float32), 'training/policy_loss': Array(6.525524e-05, dtype=float32), 'training/total_loss': Array(2504.7502, dtype=float32), 'training/v_loss': Array(2504.7568, dtype=float32), 'eval/episode_goal_distance': (Array(0.28557572, dtype=float32), Array(0.0345879, dtype=float32)), 'eval/episode_reward': (Array(-39217.906, dtype=float32), Array(21617.756, dtype=float32)), 'eval/avg_episode_length': (Array(782.34375, dtype=float32), Array(411.3323, dtype=float32)), 'eval/epoch_eval_time': 4.122358083724976, 'eval/sps': 31050.189576044497}
I0726 23:55:16.373490 140267183036224 train.py:379] starting iteration 187 1374.638520002365
I0726 23:55:23.539940 140267183036224 train.py:394] {'eval/walltime': 783.2623860836029, 'training/sps': 40391.1209640213, 'training/walltime': 593.17182970047, 'training/entropy_loss': Array(-0.00641093, dtype=float32), 'training/policy_loss': Array(0.00040066, dtype=float32), 'training/total_loss': Array(2462.9893, dtype=float32), 'training/v_loss': Array(2462.9954, dtype=float32), 'eval/episode_goal_distance': (Array(0.28650153, dtype=float32), Array(0.04094015, dtype=float32)), 'eval/episode_reward': (Array(-37153.367, dtype=float32), Array(22254.107, dtype=float32)), 'eval/avg_episode_length': (Array(751.46094, dtype=float32), Array(430.48358, dtype=float32)), 'eval/epoch_eval_time': 4.120510101318359, 'eval/sps': 31064.11508590801}
I0726 23:55:23.542446 140267183036224 train.py:379] starting iteration 188 1381.8074758052826
I0726 23:55:30.689536 140267183036224 train.py:394] {'eval/walltime': 787.3630187511444, 'training/sps': 40384.20255127615, 'training/walltime': 596.2146036624908, 'training/entropy_loss': Array(-0.00552879, dtype=float32), 'training/policy_loss': Array(7.3380565e-05, dtype=float32), 'training/total_loss': Array(2395.6826, dtype=float32), 'training/v_loss': Array(2395.688, dtype=float32), 'eval/episode_goal_distance': (Array(0.28485847, dtype=float32), Array(0.03659392, dtype=float32)), 'eval/episode_reward': (Array(-40694.586, dtype=float32), Array(20859.51, dtype=float32)), 'eval/avg_episode_length': (Array(805.7344, dtype=float32), Array(394.3168, dtype=float32)), 'eval/epoch_eval_time': 4.100632667541504, 'eval/sps': 31214.695481792864}
I0726 23:55:30.692041 140267183036224 train.py:379] starting iteration 189 1388.9570705890656
I0726 23:55:37.850862 140267183036224 train.py:394] {'eval/walltime': 791.4733231067657, 'training/sps': 40357.05819079512, 'training/walltime': 599.2594242095947, 'training/entropy_loss': Array(-0.00473031, dtype=float32), 'training/policy_loss': Array(0.00018412, dtype=float32), 'training/total_loss': Array(2405.4653, dtype=float32), 'training/v_loss': Array(2405.47, dtype=float32), 'eval/episode_goal_distance': (Array(0.28293672, dtype=float32), Array(0.03417283, dtype=float32)), 'eval/episode_reward': (Array(-38215.83, dtype=float32), Array(21850.531, dtype=float32)), 'eval/avg_episode_length': (Array(767.0156, dtype=float32), Array(421.09515, dtype=float32)), 'eval/epoch_eval_time': 4.110304355621338, 'eval/sps': 31141.246225463703}
I0726 23:55:37.853523 140267183036224 train.py:379] starting iteration 190 1396.1185529232025
I0726 23:55:45.022318 140267183036224 train.py:394] {'eval/walltime': 795.5839755535126, 'training/sps': 40228.961476066004, 'training/walltime': 602.3139400482178, 'training/entropy_loss': Array(-0.00300221, dtype=float32), 'training/policy_loss': Array(0.00016586, dtype=float32), 'training/total_loss': Array(2497.972, dtype=float32), 'training/v_loss': Array(2497.9746, dtype=float32), 'eval/episode_goal_distance': (Array(0.28151077, dtype=float32), Array(0.03473438, dtype=float32)), 'eval/episode_reward': (Array(-41752.508, dtype=float32), Array(19163.826, dtype=float32)), 'eval/avg_episode_length': (Array(844.6875, dtype=float32), Array(360.91452, dtype=float32)), 'eval/epoch_eval_time': 4.110652446746826, 'eval/sps': 31138.609176579575}
I0726 23:55:45.024786 140267183036224 train.py:379] starting iteration 191 1403.28981590271
I0726 23:55:52.191172 140267183036224 train.py:394] {'eval/walltime': 799.6989603042603, 'training/sps': 40319.12237885572, 'training/walltime': 605.3616254329681, 'training/entropy_loss': Array(-0.00284784, dtype=float32), 'training/policy_loss': Array(9.964507e-05, dtype=float32), 'training/total_loss': Array(2320.9126, dtype=float32), 'training/v_loss': Array(2320.915, dtype=float32), 'eval/episode_goal_distance': (Array(0.28732508, dtype=float32), Array(0.03482946, dtype=float32)), 'eval/episode_reward': (Array(-40206.61, dtype=float32), Array(22034.85, dtype=float32)), 'eval/avg_episode_length': (Array(782.5703, dtype=float32), Array(410.90424, dtype=float32)), 'eval/epoch_eval_time': 4.114984750747681, 'eval/sps': 31105.826085198194}
I0726 23:55:52.193750 140267183036224 train.py:379] starting iteration 192 1410.4587795734406
I0726 23:55:59.374078 140267183036224 train.py:394] {'eval/walltime': 803.8215067386627, 'training/sps': 40232.75501391062, 'training/walltime': 608.4158532619476, 'training/entropy_loss': Array(-0.00300533, dtype=float32), 'training/policy_loss': Array(0.00018029, dtype=float32), 'training/total_loss': Array(2327.4146, dtype=float32), 'training/v_loss': Array(2327.4172, dtype=float32), 'eval/episode_goal_distance': (Array(0.27766502, dtype=float32), Array(0.03711544, dtype=float32)), 'eval/episode_reward': (Array(-37953.914, dtype=float32), Array(21450.334, dtype=float32)), 'eval/avg_episode_length': (Array(774.8125, dtype=float32), Array(416.06815, dtype=float32)), 'eval/epoch_eval_time': 4.122546434402466, 'eval/sps': 31048.770956670305}
I0726 23:55:59.376405 140267183036224 train.py:379] starting iteration 193 1417.6414349079132
I0726 23:56:06.532117 140267183036224 train.py:394] {'eval/walltime': 807.9319972991943, 'training/sps': 40399.61872628298, 'training/walltime': 611.4574661254883, 'training/entropy_loss': Array(-0.00271652, dtype=float32), 'training/policy_loss': Array(0.00027774, dtype=float32), 'training/total_loss': Array(2364.5378, dtype=float32), 'training/v_loss': Array(2364.5405, dtype=float32), 'eval/episode_goal_distance': (Array(0.29055023, dtype=float32), Array(0.03545294, dtype=float32)), 'eval/episode_reward': (Array(-38987.906, dtype=float32), Array(21600.213, dtype=float32)), 'eval/avg_episode_length': (Array(782.3828, dtype=float32), Array(411.25854, dtype=float32)), 'eval/epoch_eval_time': 4.110490560531616, 'eval/sps': 31139.835529374275}
I0726 23:56:06.534662 140267183036224 train.py:379] starting iteration 194 1424.7996921539307
I0726 23:56:13.687240 140267183036224 train.py:394] {'eval/walltime': 812.0391569137573, 'training/sps': 40397.46545281052, 'training/walltime': 614.4992411136627, 'training/entropy_loss': Array(-0.0041032, dtype=float32), 'training/policy_loss': Array(0.00014884, dtype=float32), 'training/total_loss': Array(2427.3455, dtype=float32), 'training/v_loss': Array(2427.3494, dtype=float32), 'eval/episode_goal_distance': (Array(0.28356126, dtype=float32), Array(0.03834938, dtype=float32)), 'eval/episode_reward': (Array(-38169.94, dtype=float32), Array(22327.236, dtype=float32)), 'eval/avg_episode_length': (Array(759.3203, dtype=float32), Array(425.7411, dtype=float32)), 'eval/epoch_eval_time': 4.107159614562988, 'eval/sps': 31165.090235632226}
I0726 23:56:13.689592 140267183036224 train.py:379] starting iteration 195 1431.9546220302582
I0726 23:56:20.847565 140267183036224 train.py:394] {'eval/walltime': 816.1443417072296, 'training/sps': 40299.41863462747, 'training/walltime': 617.5484166145325, 'training/entropy_loss': Array(-0.00334114, dtype=float32), 'training/policy_loss': Array(9.886378e-06, dtype=float32), 'training/total_loss': Array(2404.5527, dtype=float32), 'training/v_loss': Array(2404.5562, dtype=float32), 'eval/episode_goal_distance': (Array(0.28688878, dtype=float32), Array(0.03436569, dtype=float32)), 'eval/episode_reward': (Array(-38401.406, dtype=float32), Array(22036.375, dtype=float32)), 'eval/avg_episode_length': (Array(767.02344, dtype=float32), Array(421.08115, dtype=float32)), 'eval/epoch_eval_time': 4.10518479347229, 'eval/sps': 31180.082368894706}
I0726 23:56:20.850125 140267183036224 train.py:379] starting iteration 196 1439.1151549816132
I0726 23:56:28.008186 140267183036224 train.py:394] {'eval/walltime': 820.2491114139557, 'training/sps': 40292.679647773315, 'training/walltime': 620.5981020927429, 'training/entropy_loss': Array(-0.003395, dtype=float32), 'training/policy_loss': Array(0.00035165, dtype=float32), 'training/total_loss': Array(2349.843, dtype=float32), 'training/v_loss': Array(2349.846, dtype=float32), 'eval/episode_goal_distance': (Array(0.28210294, dtype=float32), Array(0.04293218, dtype=float32)), 'eval/episode_reward': (Array(-38300.344, dtype=float32), Array(21539.87, dtype=float32)), 'eval/avg_episode_length': (Array(782.5156, dtype=float32), Array(411.00775, dtype=float32)), 'eval/epoch_eval_time': 4.104769706726074, 'eval/sps': 31183.23539326926}
I0726 23:56:28.010631 140267183036224 train.py:379] starting iteration 197 1446.2756597995758
I0726 23:56:35.169516 140267183036224 train.py:394] {'eval/walltime': 824.3497257232666, 'training/sps': 40227.8687684184, 'training/walltime': 623.6527009010315, 'training/entropy_loss': Array(-0.00357175, dtype=float32), 'training/policy_loss': Array(-0.00017933, dtype=float32), 'training/total_loss': Array(2187.6248, dtype=float32), 'training/v_loss': Array(2187.6284, dtype=float32), 'eval/episode_goal_distance': (Array(0.28382513, dtype=float32), Array(0.03419503, dtype=float32)), 'eval/episode_reward': (Array(-38714.516, dtype=float32), Array(21023.404, dtype=float32)), 'eval/avg_episode_length': (Array(790.28906, dtype=float32), Array(405.60236, dtype=float32)), 'eval/epoch_eval_time': 4.100614309310913, 'eval/sps': 31214.83522831235}
I0726 23:56:35.172368 140267183036224 train.py:379] starting iteration 198 1453.437397480011
I0726 23:56:42.329864 140267183036224 train.py:394] {'eval/walltime': 828.4479141235352, 'training/sps': 40213.78506278244, 'training/walltime': 626.7083694934845, 'training/entropy_loss': Array(-0.00346392, dtype=float32), 'training/policy_loss': Array(0.00032845, dtype=float32), 'training/total_loss': Array(2107.4395, dtype=float32), 'training/v_loss': Array(2107.4429, dtype=float32), 'eval/episode_goal_distance': (Array(0.2891839, dtype=float32), Array(0.03191208, dtype=float32)), 'eval/episode_reward': (Array(-42850.68, dtype=float32), Array(19834.168, dtype=float32)), 'eval/avg_episode_length': (Array(836.8047, dtype=float32), Array(368.3755, dtype=float32)), 'eval/epoch_eval_time': 4.098188400268555, 'eval/sps': 31233.31274658143}
I0726 23:56:42.332406 140267183036224 train.py:379] starting iteration 199 1460.5974354743958
I0726 23:56:49.491602 140267183036224 train.py:394] {'eval/walltime': 832.5466351509094, 'training/sps': 40226.06969808911, 'training/walltime': 629.7631049156189, 'training/entropy_loss': Array(-0.00401943, dtype=float32), 'training/policy_loss': Array(5.2753385e-06, dtype=float32), 'training/total_loss': Array(2106.5278, dtype=float32), 'training/v_loss': Array(2106.532, dtype=float32), 'eval/episode_goal_distance': (Array(0.28551027, dtype=float32), Array(0.03948896, dtype=float32)), 'eval/episode_reward': (Array(-40801.97, dtype=float32), Array(21224.62, dtype=float32)), 'eval/avg_episode_length': (Array(805.64844, dtype=float32), Array(394.49115, dtype=float32)), 'eval/epoch_eval_time': 4.098721027374268, 'eval/sps': 31229.25399048192}
I0726 23:56:49.494000 140267183036224 train.py:379] starting iteration 200 1467.7590301036835
I0726 23:56:56.634742 140267183036224 train.py:394] {'eval/walltime': 836.6349873542786, 'training/sps': 40304.89591849997, 'training/walltime': 632.8118660449982, 'training/entropy_loss': Array(-0.00462892, dtype=float32), 'training/policy_loss': Array(0.00053342, dtype=float32), 'training/total_loss': Array(13045.72, dtype=float32), 'training/v_loss': Array(13045.723, dtype=float32), 'eval/episode_goal_distance': (Array(0.2880421, dtype=float32), Array(0.0362629, dtype=float32)), 'eval/episode_reward': (Array(-36953.11, dtype=float32), Array(23434.129, dtype=float32)), 'eval/avg_episode_length': (Array(728.1406, dtype=float32), Array(443.1521, dtype=float32)), 'eval/epoch_eval_time': 4.088352203369141, 'eval/sps': 31308.457205452458}
I0726 23:56:56.637342 140267183036224 train.py:379] starting iteration 201 1474.9023718833923
I0726 23:57:03.779447 140267183036224 train.py:394] {'eval/walltime': 840.7280328273773, 'training/sps': 40347.248531549376, 'training/walltime': 635.8574268817902, 'training/entropy_loss': Array(-0.00633189, dtype=float32), 'training/policy_loss': Array(-8.5312946e-05, dtype=float32), 'training/total_loss': Array(3100.5923, dtype=float32), 'training/v_loss': Array(3100.5984, dtype=float32), 'eval/episode_goal_distance': (Array(0.28198624, dtype=float32), Array(0.03752298, dtype=float32)), 'eval/episode_reward': (Array(-41210.223, dtype=float32), Array(18921.79, dtype=float32)), 'eval/avg_episode_length': (Array(844.65625, dtype=float32), Array(360.98694, dtype=float32)), 'eval/epoch_eval_time': 4.093045473098755, 'eval/sps': 31272.557522575975}
I0726 23:57:03.781856 140267183036224 train.py:379] starting iteration 202 1482.0468862056732
I0726 23:57:10.935029 140267183036224 train.py:394] {'eval/walltime': 844.8200268745422, 'training/sps': 40198.394354399046, 'training/walltime': 638.9142653942108, 'training/entropy_loss': Array(-0.00657525, dtype=float32), 'training/policy_loss': Array(-0.00025977, dtype=float32), 'training/total_loss': Array(2769.379, dtype=float32), 'training/v_loss': Array(2769.3857, dtype=float32), 'eval/episode_goal_distance': (Array(0.28569508, dtype=float32), Array(0.03455298, dtype=float32)), 'eval/episode_reward': (Array(-38188.992, dtype=float32), Array(22419.14, dtype=float32)), 'eval/avg_episode_length': (Array(759.09375, dtype=float32), Array(426.142, dtype=float32)), 'eval/epoch_eval_time': 4.091994047164917, 'eval/sps': 31280.592915007557}
I0726 23:57:10.937588 140267183036224 train.py:379] starting iteration 203 1489.2026176452637
I0726 23:57:18.096654 140267183036224 train.py:394] {'eval/walltime': 848.9152460098267, 'training/sps': 40160.72794172364, 'training/walltime': 641.9739708900452, 'training/entropy_loss': Array(-0.00616171, dtype=float32), 'training/policy_loss': Array(0.00039314, dtype=float32), 'training/total_loss': Array(2404.582, dtype=float32), 'training/v_loss': Array(2404.5876, dtype=float32), 'eval/episode_goal_distance': (Array(0.28157204, dtype=float32), Array(0.03813125, dtype=float32)), 'eval/episode_reward': (Array(-41028.867, dtype=float32), Array(20672.316, dtype=float32)), 'eval/avg_episode_length': (Array(821.28906, dtype=float32), Array(381.8411, dtype=float32)), 'eval/epoch_eval_time': 4.095219135284424, 'eval/sps': 31255.95866095455}
I0726 23:57:18.099219 140267183036224 train.py:379] starting iteration 204 1496.3642482757568
I0726 23:57:25.261623 140267183036224 train.py:394] {'eval/walltime': 853.0117633342743, 'training/sps': 40129.43011726541, 'training/walltime': 645.0360627174377, 'training/entropy_loss': Array(-0.00520618, dtype=float32), 'training/policy_loss': Array(0.00022888, dtype=float32), 'training/total_loss': Array(2284.8042, dtype=float32), 'training/v_loss': Array(2284.8093, dtype=float32), 'eval/episode_goal_distance': (Array(0.28492242, dtype=float32), Array(0.03526804, dtype=float32)), 'eval/episode_reward': (Array(-35742.906, dtype=float32), Array(23073.48, dtype=float32)), 'eval/avg_episode_length': (Array(720.3828, dtype=float32), Array(447.00012, dtype=float32)), 'eval/epoch_eval_time': 4.096517324447632, 'eval/sps': 31246.053626115037}
I0726 23:57:25.357306 140267183036224 train.py:379] starting iteration 205 1503.6223347187042
I0726 23:57:32.521501 140267183036224 train.py:394] {'eval/walltime': 857.1150887012482, 'training/sps': 40198.70788482522, 'training/walltime': 648.0928773880005, 'training/entropy_loss': Array(-0.00461503, dtype=float32), 'training/policy_loss': Array(7.189409e-05, dtype=float32), 'training/total_loss': Array(2285.6177, dtype=float32), 'training/v_loss': Array(2285.622, dtype=float32), 'eval/episode_goal_distance': (Array(0.2832142, dtype=float32), Array(0.03391052, dtype=float32)), 'eval/episode_reward': (Array(-38845.64, dtype=float32), Array(21246.588, dtype=float32)), 'eval/avg_episode_length': (Array(782.5, dtype=float32), Array(411.0368, dtype=float32)), 'eval/epoch_eval_time': 4.103325366973877, 'eval/sps': 31194.211658237942}
I0726 23:57:32.524342 140267183036224 train.py:379] starting iteration 206 1510.7893714904785
I0726 23:57:39.682177 140267183036224 train.py:394] {'eval/walltime': 861.2154099941254, 'training/sps': 40241.99690523102, 'training/walltime': 651.1464037895203, 'training/entropy_loss': Array(-0.00540351, dtype=float32), 'training/policy_loss': Array(0.00019471, dtype=float32), 'training/total_loss': Array(2337.3755, dtype=float32), 'training/v_loss': Array(2337.3806, dtype=float32), 'eval/episode_goal_distance': (Array(0.28340966, dtype=float32), Array(0.03942701, dtype=float32)), 'eval/episode_reward': (Array(-38669.035, dtype=float32), Array(22320.05, dtype=float32)), 'eval/avg_episode_length': (Array(766.9219, dtype=float32), Array(421.2643, dtype=float32)), 'eval/epoch_eval_time': 4.100321292877197, 'eval/sps': 31217.06589733663}
I0726 23:57:39.684581 140267183036224 train.py:379] starting iteration 207 1517.949610710144
I0726 23:57:46.846669 140267183036224 train.py:394] {'eval/walltime': 865.3247947692871, 'training/sps': 40306.32063445166, 'training/walltime': 654.1950571537018, 'training/entropy_loss': Array(-0.00517535, dtype=float32), 'training/policy_loss': Array(5.182294e-05, dtype=float32), 'training/total_loss': Array(2341.3267, dtype=float32), 'training/v_loss': Array(2341.3315, dtype=float32), 'eval/episode_goal_distance': (Array(0.28840446, dtype=float32), Array(0.03795696, dtype=float32)), 'eval/episode_reward': (Array(-36388.984, dtype=float32), Array(23186.787, dtype=float32)), 'eval/avg_episode_length': (Array(727.96094, dtype=float32), Array(443.44467, dtype=float32)), 'eval/epoch_eval_time': 4.109384775161743, 'eval/sps': 31148.21487967429}
I0726 23:57:46.849076 140267183036224 train.py:379] starting iteration 208 1525.1141057014465
I0726 23:57:54.014083 140267183036224 train.py:394] {'eval/walltime': 869.4291815757751, 'training/sps': 40200.288152618115, 'training/walltime': 657.2517516613007, 'training/entropy_loss': Array(-0.00413933, dtype=float32), 'training/policy_loss': Array(0.00017872, dtype=float32), 'training/total_loss': Array(2106.1995, dtype=float32), 'training/v_loss': Array(2106.2036, dtype=float32), 'eval/episode_goal_distance': (Array(0.28302383, dtype=float32), Array(0.03932922, dtype=float32)), 'eval/episode_reward': (Array(-40542.96, dtype=float32), Array(21067.26, dtype=float32)), 'eval/avg_episode_length': (Array(805.7656, dtype=float32), Array(394.2533, dtype=float32)), 'eval/epoch_eval_time': 4.104386806488037, 'eval/sps': 31186.144492439926}
I0726 23:57:54.016479 140267183036224 train.py:379] starting iteration 209 1532.2815086841583
I0726 23:58:01.176440 140267183036224 train.py:394] {'eval/walltime': 873.5317220687866, 'training/sps': 40242.062889125715, 'training/walltime': 660.3052730560303, 'training/entropy_loss': Array(-0.00372482, dtype=float32), 'training/policy_loss': Array(8.532823e-05, dtype=float32), 'training/total_loss': Array(2264.8794, dtype=float32), 'training/v_loss': Array(2264.8828, dtype=float32), 'eval/episode_goal_distance': (Array(0.28406364, dtype=float32), Array(0.04100043, dtype=float32)), 'eval/episode_reward': (Array(-40871.25, dtype=float32), Array(20433.246, dtype=float32)), 'eval/avg_episode_length': (Array(821.28125, dtype=float32), Array(381.85776, dtype=float32)), 'eval/epoch_eval_time': 4.102540493011475, 'eval/sps': 31200.179551681023}
I0726 23:58:01.178881 140267183036224 train.py:379] starting iteration 210 1539.4439105987549
I0726 23:58:08.340236 140267183036224 train.py:394] {'eval/walltime': 877.6310911178589, 'training/sps': 40183.287984762355, 'training/walltime': 663.3632607460022, 'training/entropy_loss': Array(-0.00362632, dtype=float32), 'training/policy_loss': Array(-3.6079444e-05, dtype=float32), 'training/total_loss': Array(2254.3535, dtype=float32), 'training/v_loss': Array(2254.3574, dtype=float32), 'eval/episode_goal_distance': (Array(0.28086913, dtype=float32), Array(0.03793343, dtype=float32)), 'eval/episode_reward': (Array(-41788.15, dtype=float32), Array(20281.643, dtype=float32)), 'eval/avg_episode_length': (Array(829.25, dtype=float32), Array(374.8032, dtype=float32)), 'eval/epoch_eval_time': 4.099369049072266, 'eval/sps': 31224.31731999535}
I0726 23:58:08.342824 140267183036224 train.py:379] starting iteration 211 1546.607854127884
I0726 23:58:15.504910 140267183036224 train.py:394] {'eval/walltime': 881.7379186153412, 'training/sps': 40271.3528105227, 'training/walltime': 666.4145612716675, 'training/entropy_loss': Array(-0.00162472, dtype=float32), 'training/policy_loss': Array(2.6668087e-05, dtype=float32), 'training/total_loss': Array(2190.7969, dtype=float32), 'training/v_loss': Array(2190.7983, dtype=float32), 'eval/episode_goal_distance': (Array(0.28670317, dtype=float32), Array(0.04073027, dtype=float32)), 'eval/episode_reward': (Array(-38302.492, dtype=float32), Array(23036.674, dtype=float32)), 'eval/avg_episode_length': (Array(751.33594, dtype=float32), Array(430.6996, dtype=float32)), 'eval/epoch_eval_time': 4.1068274974823, 'eval/sps': 31167.610540854395}
I0726 23:58:15.507468 140267183036224 train.py:379] starting iteration 212 1553.7724986076355
I0726 23:58:22.669481 140267183036224 train.py:394] {'eval/walltime': 885.8383195400238, 'training/sps': 40187.97851915139, 'training/walltime': 669.4721920490265, 'training/entropy_loss': Array(-0.0005788, dtype=float32), 'training/policy_loss': Array(0.00046443, dtype=float32), 'training/total_loss': Array(2135.349, dtype=float32), 'training/v_loss': Array(2135.349, dtype=float32), 'eval/episode_goal_distance': (Array(0.28188848, dtype=float32), Array(0.0397324, dtype=float32)), 'eval/episode_reward': (Array(-42351.08, dtype=float32), Array(19033.127, dtype=float32)), 'eval/avg_episode_length': (Array(852.4453, dtype=float32), Array(353.41928, dtype=float32)), 'eval/epoch_eval_time': 4.100400924682617, 'eval/sps': 31216.459646542386}
I0726 23:58:22.672043 140267183036224 train.py:379] starting iteration 213 1560.9370725154877
I0726 23:58:29.826452 140267183036224 train.py:394] {'eval/walltime': 889.935298204422, 'training/sps': 40243.5742937644, 'training/walltime': 672.5255987644196, 'training/entropy_loss': Array(-0.00115246, dtype=float32), 'training/policy_loss': Array(0.00040067, dtype=float32), 'training/total_loss': Array(2134.4268, dtype=float32), 'training/v_loss': Array(2134.4277, dtype=float32), 'eval/episode_goal_distance': (Array(0.28835374, dtype=float32), Array(0.03597841, dtype=float32)), 'eval/episode_reward': (Array(-42038.46, dtype=float32), Array(20371.424, dtype=float32)), 'eval/avg_episode_length': (Array(829.1875, dtype=float32), Array(374.9406, dtype=float32)), 'eval/epoch_eval_time': 4.096978664398193, 'eval/sps': 31242.53516677807}
I0726 23:58:29.829048 140267183036224 train.py:379] starting iteration 214 1568.0940783023834
I0726 23:58:36.988270 140267183036224 train.py:394] {'eval/walltime': 894.0307056903839, 'training/sps': 40159.41676301416, 'training/walltime': 675.5854041576385, 'training/entropy_loss': Array(-0.00094036, dtype=float32), 'training/policy_loss': Array(0.00070514, dtype=float32), 'training/total_loss': Array(2242.5625, dtype=float32), 'training/v_loss': Array(2242.563, dtype=float32), 'eval/episode_goal_distance': (Array(0.2796807, dtype=float32), Array(0.03754561, dtype=float32)), 'eval/episode_reward': (Array(-42337.062, dtype=float32), Array(18489.285, dtype=float32)), 'eval/avg_episode_length': (Array(860.16406, dtype=float32), Array(345.6841, dtype=float32)), 'eval/epoch_eval_time': 4.095407485961914, 'eval/sps': 31254.521177380677}
I0726 23:58:36.990683 140267183036224 train.py:379] starting iteration 215 1575.2557129859924
I0726 23:58:44.141665 140267183036224 train.py:394] {'eval/walltime': 898.1249740123749, 'training/sps': 40250.02967778958, 'training/walltime': 678.6383211612701, 'training/entropy_loss': Array(-0.00116879, dtype=float32), 'training/policy_loss': Array(0.00033146, dtype=float32), 'training/total_loss': Array(2288.6372, dtype=float32), 'training/v_loss': Array(2288.6382, dtype=float32), 'eval/episode_goal_distance': (Array(0.28347233, dtype=float32), Array(0.03467649, dtype=float32)), 'eval/episode_reward': (Array(-38439.86, dtype=float32), Array(22007.379, dtype=float32)), 'eval/avg_episode_length': (Array(766.85156, dtype=float32), Array(421.39133, dtype=float32)), 'eval/epoch_eval_time': 4.094268321990967, 'eval/sps': 31263.217242624676}
I0726 23:58:44.144129 140267183036224 train.py:379] starting iteration 216 1582.409159898758
I0726 23:58:51.300781 140267183036224 train.py:394] {'eval/walltime': 902.2197751998901, 'training/sps': 40186.33341749084, 'training/walltime': 681.6960771083832, 'training/entropy_loss': Array(-0.00054034, dtype=float32), 'training/policy_loss': Array(0.00034212, dtype=float32), 'training/total_loss': Array(11907.5205, dtype=float32), 'training/v_loss': Array(11907.521, dtype=float32), 'eval/episode_goal_distance': (Array(0.28298283, dtype=float32), Array(0.03936822, dtype=float32)), 'eval/episode_reward': (Array(-38260.184, dtype=float32), Array(21983.914, dtype=float32)), 'eval/avg_episode_length': (Array(766.89844, dtype=float32), Array(421.3071, dtype=float32)), 'eval/epoch_eval_time': 4.094801187515259, 'eval/sps': 31259.148891101817}
I0726 23:58:51.303339 140267183036224 train.py:379] starting iteration 217 1589.568368434906
I0726 23:58:58.460375 140267183036224 train.py:394] {'eval/walltime': 906.3150804042816, 'training/sps': 40185.57201603287, 'training/walltime': 684.7538909912109, 'training/entropy_loss': Array(-0.002032, dtype=float32), 'training/policy_loss': Array(0.00074905, dtype=float32), 'training/total_loss': Array(3512.2407, dtype=float32), 'training/v_loss': Array(3512.2422, dtype=float32), 'eval/episode_goal_distance': (Array(0.28469834, dtype=float32), Array(0.0373459, dtype=float32)), 'eval/episode_reward': (Array(-38197.754, dtype=float32), Array(21957.672, dtype=float32)), 'eval/avg_episode_length': (Array(766.8047, dtype=float32), Array(421.47623, dtype=float32)), 'eval/epoch_eval_time': 4.0953052043914795, 'eval/sps': 31255.3017691436}
I0726 23:58:58.462810 140267183036224 train.py:379] starting iteration 218 1596.7278399467468
I0726 23:59:05.628556 140267183036224 train.py:394] {'eval/walltime': 910.4094591140747, 'training/sps': 40071.66581375724, 'training/walltime': 687.820396900177, 'training/entropy_loss': Array(-0.00426218, dtype=float32), 'training/policy_loss': Array(0.00043979, dtype=float32), 'training/total_loss': Array(2991.7256, dtype=float32), 'training/v_loss': Array(2991.7295, dtype=float32), 'eval/episode_goal_distance': (Array(0.27775055, dtype=float32), Array(0.03720562, dtype=float32)), 'eval/episode_reward': (Array(-39147.684, dtype=float32), Array(20524.807, dtype=float32)), 'eval/avg_episode_length': (Array(798.1094, dtype=float32), Array(399.8809, dtype=float32)), 'eval/epoch_eval_time': 4.094378709793091, 'eval/sps': 31262.374360692313}
I0726 23:59:05.631049 140267183036224 train.py:379] starting iteration 219 1603.8960785865784
I0726 23:59:12.794009 140267183036224 train.py:394] {'eval/walltime': 914.5098309516907, 'training/sps': 40176.26206534531, 'training/walltime': 690.8789193630219, 'training/entropy_loss': Array(-0.00404874, dtype=float32), 'training/policy_loss': Array(0.00035677, dtype=float32), 'training/total_loss': Array(2693.7056, dtype=float32), 'training/v_loss': Array(2693.7092, dtype=float32), 'eval/episode_goal_distance': (Array(0.28220835, dtype=float32), Array(0.03814313, dtype=float32)), 'eval/episode_reward': (Array(-38409.777, dtype=float32), Array(21340.299, dtype=float32)), 'eval/avg_episode_length': (Array(782.4844, dtype=float32), Array(411.06693, dtype=float32)), 'eval/epoch_eval_time': 4.100371837615967, 'eval/sps': 31216.681088713554}
I0726 23:59:12.796443 140267183036224 train.py:379] starting iteration 220 1611.0614726543427
I0726 23:59:19.952970 140267183036224 train.py:394] {'eval/walltime': 918.6013765335083, 'training/sps': 40145.14954026288, 'training/walltime': 693.9398121833801, 'training/entropy_loss': Array(-0.00364328, dtype=float32), 'training/policy_loss': Array(0.00063688, dtype=float32), 'training/total_loss': Array(2406.4185, dtype=float32), 'training/v_loss': Array(2406.4214, dtype=float32), 'eval/episode_goal_distance': (Array(0.28312582, dtype=float32), Array(0.03675644, dtype=float32)), 'eval/episode_reward': (Array(-38548.28, dtype=float32), Array(22230.52, dtype=float32)), 'eval/avg_episode_length': (Array(767.0078, dtype=float32), Array(421.10925, dtype=float32)), 'eval/epoch_eval_time': 4.091545581817627, 'eval/sps': 31284.02151226611}
I0726 23:59:19.955530 140267183036224 train.py:379] starting iteration 221 1618.2205607891083
I0726 23:59:27.116726 140267183036224 train.py:394] {'eval/walltime': 922.6997036933899, 'training/sps': 40171.909295596495, 'training/walltime': 696.9986660480499, 'training/entropy_loss': Array(-0.00446232, dtype=float32), 'training/policy_loss': Array(0.00059762, dtype=float32), 'training/total_loss': Array(2564.5757, dtype=float32), 'training/v_loss': Array(2564.5796, dtype=float32), 'eval/episode_goal_distance': (Array(0.28261206, dtype=float32), Array(0.03665489, dtype=float32)), 'eval/episode_reward': (Array(-41288.29, dtype=float32), Array(20949.664, dtype=float32)), 'eval/avg_episode_length': (Array(813.6406, dtype=float32), Array(387.93896, dtype=float32)), 'eval/epoch_eval_time': 4.098327159881592, 'eval/sps': 31232.255260875307}
I0726 23:59:27.119166 140267183036224 train.py:379] starting iteration 222 1625.384196281433
I0726 23:59:34.278266 140267183036224 train.py:394] {'eval/walltime': 926.7965226173401, 'training/sps': 40179.06837954182, 'training/walltime': 700.0569748878479, 'training/entropy_loss': Array(-0.00396626, dtype=float32), 'training/policy_loss': Array(0.00038123, dtype=float32), 'training/total_loss': Array(2413.1628, dtype=float32), 'training/v_loss': Array(2413.1665, dtype=float32), 'eval/episode_goal_distance': (Array(0.2842165, dtype=float32), Array(0.03677497, dtype=float32)), 'eval/episode_reward': (Array(-38729.883, dtype=float32), Array(22471.34, dtype=float32)), 'eval/avg_episode_length': (Array(767.0547, dtype=float32), Array(421.0246, dtype=float32)), 'eval/epoch_eval_time': 4.096818923950195, 'eval/sps': 31243.753355000877}
I0726 23:59:34.280704 140267183036224 train.py:379] starting iteration 223 1632.545734167099
I0726 23:59:41.440493 140267183036224 train.py:394] {'eval/walltime': 930.8942220211029, 'training/sps': 40181.04493653389, 'training/walltime': 703.1151332855225, 'training/entropy_loss': Array(-0.00351692, dtype=float32), 'training/policy_loss': Array(0.00033356, dtype=float32), 'training/total_loss': Array(2239.4587, dtype=float32), 'training/v_loss': Array(2239.462, dtype=float32), 'eval/episode_goal_distance': (Array(0.28746197, dtype=float32), Array(0.03485498, dtype=float32)), 'eval/episode_reward': (Array(-39550.33, dtype=float32), Array(21822.73, dtype=float32)), 'eval/avg_episode_length': (Array(782.5156, dtype=float32), Array(411.00827, dtype=float32)), 'eval/epoch_eval_time': 4.097699403762817, 'eval/sps': 31237.039955263856}
I0726 23:59:41.442922 140267183036224 train.py:379] starting iteration 224 1639.7079524993896
I0726 23:59:48.600564 140267183036224 train.py:394] {'eval/walltime': 934.9891309738159, 'training/sps': 40175.31001209244, 'training/walltime': 706.1737282276154, 'training/entropy_loss': Array(-0.00351065, dtype=float32), 'training/policy_loss': Array(0.00042875, dtype=float32), 'training/total_loss': Array(2345.6338, dtype=float32), 'training/v_loss': Array(2345.637, dtype=float32), 'eval/episode_goal_distance': (Array(0.28702697, dtype=float32), Array(0.03391531, dtype=float32)), 'eval/episode_reward': (Array(-34623.934, dtype=float32), Array(23217.803, dtype=float32)), 'eval/avg_episode_length': (Array(704.6875, dtype=float32), Array(454.47693, dtype=float32)), 'eval/epoch_eval_time': 4.094908952713013, 'eval/sps': 31258.326248058765}
I0726 23:59:48.603058 140267183036224 train.py:379] starting iteration 225 1646.8680877685547
I0726 23:59:55.757205 140267183036224 train.py:394] {'eval/walltime': 939.0840699672699, 'training/sps': 40220.152425705695, 'training/walltime': 709.2289130687714, 'training/entropy_loss': Array(-0.00424441, dtype=float32), 'training/policy_loss': Array(0.00069959, dtype=float32), 'training/total_loss': Array(2268.5845, dtype=float32), 'training/v_loss': Array(2268.588, dtype=float32), 'eval/episode_goal_distance': (Array(0.28580368, dtype=float32), Array(0.03744876, dtype=float32)), 'eval/episode_reward': (Array(-39739.504, dtype=float32), Array(21671.45, dtype=float32)), 'eval/avg_episode_length': (Array(790.35156, dtype=float32), Array(405.4818, dtype=float32)), 'eval/epoch_eval_time': 4.0949389934539795, 'eval/sps': 31258.09693492776}
I0726 23:59:55.759642 140267183036224 train.py:379] starting iteration 226 1654.0246725082397
I0727 00:00:02.912542 140267183036224 train.py:394] {'eval/walltime': 943.1784520149231, 'training/sps': 40228.94577582316, 'training/walltime': 712.2834300994873, 'training/entropy_loss': Array(-0.00449665, dtype=float32), 'training/policy_loss': Array(0.00041422, dtype=float32), 'training/total_loss': Array(2145.2563, dtype=float32), 'training/v_loss': Array(2145.2603, dtype=float32), 'eval/episode_goal_distance': (Array(0.27898014, dtype=float32), Array(0.03552664, dtype=float32)), 'eval/episode_reward': (Array(-41807.76, dtype=float32), Array(20010.715, dtype=float32)), 'eval/avg_episode_length': (Array(829.1406, dtype=float32), Array(375.0431, dtype=float32)), 'eval/epoch_eval_time': 4.094382047653198, 'eval/sps': 31262.348874689535}
I0727 00:00:02.914989 140267183036224 train.py:379] starting iteration 227 1661.1800181865692
I0727 00:00:10.053992 140267183036224 train.py:394] {'eval/walltime': 947.2692456245422, 'training/sps': 40366.71137949705, 'training/walltime': 715.3275225162506, 'training/entropy_loss': Array(-0.00462012, dtype=float32), 'training/policy_loss': Array(0.00017233, dtype=float32), 'training/total_loss': Array(2544.561, dtype=float32), 'training/v_loss': Array(2544.5654, dtype=float32), 'eval/episode_goal_distance': (Array(0.28323656, dtype=float32), Array(0.03490531, dtype=float32)), 'eval/episode_reward': (Array(-36912.16, dtype=float32), Array(22037.95, dtype=float32)), 'eval/avg_episode_length': (Array(751.46094, dtype=float32), Array(430.4834, dtype=float32)), 'eval/epoch_eval_time': 4.090793609619141, 'eval/sps': 31289.772160350323}
I0727 00:00:10.056389 140267183036224 train.py:379] starting iteration 228 1668.3214194774628
I0727 00:00:17.237102 140267183036224 train.py:394] {'eval/walltime': 951.3967213630676, 'training/sps': 40299.204363794335, 'training/walltime': 718.3767142295837, 'training/entropy_loss': Array(-0.00433492, dtype=float32), 'training/policy_loss': Array(4.0427898e-05, dtype=float32), 'training/total_loss': Array(2339.2068, dtype=float32), 'training/v_loss': Array(2339.211, dtype=float32), 'eval/episode_goal_distance': (Array(0.28591764, dtype=float32), Array(0.04094627, dtype=float32)), 'eval/episode_reward': (Array(-37488.195, dtype=float32), Array(22348.643, dtype=float32)), 'eval/avg_episode_length': (Array(759.02344, dtype=float32), Array(426.26566, dtype=float32)), 'eval/epoch_eval_time': 4.127475738525391, 'eval/sps': 31011.690463801522}
I0727 00:00:17.239552 140267183036224 train.py:379] starting iteration 229 1675.5045824050903
I0727 00:00:24.377850 140267183036224 train.py:394] {'eval/walltime': 955.4854400157928, 'training/sps': 40346.63262514303, 'training/walltime': 721.4223215579987, 'training/entropy_loss': Array(-0.00365593, dtype=float32), 'training/policy_loss': Array(6.624419e-05, dtype=float32), 'training/total_loss': Array(2489.7168, dtype=float32), 'training/v_loss': Array(2489.7202, dtype=float32), 'eval/episode_goal_distance': (Array(0.2875653, dtype=float32), Array(0.03267704, dtype=float32)), 'eval/episode_reward': (Array(-38572.42, dtype=float32), Array(22615.451, dtype=float32)), 'eval/avg_episode_length': (Array(759.21094, dtype=float32), Array(425.93423, dtype=float32)), 'eval/epoch_eval_time': 4.08871865272522, 'eval/sps': 31305.651200697124}
I0727 00:00:24.380368 140267183036224 train.py:379] starting iteration 230 1682.6453974246979
I0727 00:00:31.544860 140267183036224 train.py:394] {'eval/walltime': 959.5933141708374, 'training/sps': 40253.72658467257, 'training/walltime': 724.4749581813812, 'training/entropy_loss': Array(-0.0030585, dtype=float32), 'training/policy_loss': Array(0.00038543, dtype=float32), 'training/total_loss': Array(2135.6887, dtype=float32), 'training/v_loss': Array(2135.6914, dtype=float32), 'eval/episode_goal_distance': (Array(0.28227386, dtype=float32), Array(0.03489431, dtype=float32)), 'eval/episode_reward': (Array(-38602.12, dtype=float32), Array(21379.2, dtype=float32)), 'eval/avg_episode_length': (Array(782.5156, dtype=float32), Array(411.00748, dtype=float32)), 'eval/epoch_eval_time': 4.107874155044556, 'eval/sps': 31159.66925199335}
I0727 00:00:31.547480 140267183036224 train.py:379] starting iteration 231 1689.8125088214874
I0727 00:00:38.724309 140267183036224 train.py:394] {'eval/walltime': 963.7177307605743, 'training/sps': 40305.21741639712, 'training/walltime': 727.5236949920654, 'training/entropy_loss': Array(-0.00383376, dtype=float32), 'training/policy_loss': Array(0.00055217, dtype=float32), 'training/total_loss': Array(2115.5823, dtype=float32), 'training/v_loss': Array(2115.5854, dtype=float32), 'eval/episode_goal_distance': (Array(0.28596556, dtype=float32), Array(0.03679629, dtype=float32)), 'eval/episode_reward': (Array(-38687.734, dtype=float32), Array(21476.14, dtype=float32)), 'eval/avg_episode_length': (Array(782.4844, dtype=float32), Array(411.0668, dtype=float32)), 'eval/epoch_eval_time': 4.1244165897369385, 'eval/sps': 31034.69235346181}
I0727 00:00:38.726814 140267183036224 train.py:379] starting iteration 232 1696.991843700409
I0727 00:00:45.897606 140267183036224 train.py:394] {'eval/walltime': 967.8387544155121, 'training/sps': 40338.813827912556, 'training/walltime': 730.5698926448822, 'training/entropy_loss': Array(-0.00417134, dtype=float32), 'training/policy_loss': Array(0.00026565, dtype=float32), 'training/total_loss': Array(2118.8066, dtype=float32), 'training/v_loss': Array(2118.8103, dtype=float32), 'eval/episode_goal_distance': (Array(0.2819575, dtype=float32), Array(0.03883249, dtype=float32)), 'eval/episode_reward': (Array(-40505.305, dtype=float32), Array(20663.22, dtype=float32)), 'eval/avg_episode_length': (Array(813.5469, dtype=float32), Array(388.13406, dtype=float32)), 'eval/epoch_eval_time': 4.121023654937744, 'eval/sps': 31060.243938816624}
I0727 00:00:45.900147 140267183036224 train.py:379] starting iteration 233 1704.1651768684387
I0727 00:00:53.039680 140267183036224 train.py:394] {'eval/walltime': 971.9309310913086, 'training/sps': 40370.67956574792, 'training/walltime': 733.6136858463287, 'training/entropy_loss': Array(-0.00321933, dtype=float32), 'training/policy_loss': Array(-1.6473776e-05, dtype=float32), 'training/total_loss': Array(13786.707, dtype=float32), 'training/v_loss': Array(13786.71, dtype=float32), 'eval/episode_goal_distance': (Array(0.28636768, dtype=float32), Array(0.03720343, dtype=float32)), 'eval/episode_reward': (Array(-40699.47, dtype=float32), Array(21260.406, dtype=float32)), 'eval/avg_episode_length': (Array(805.8125, dtype=float32), Array(394.15836, dtype=float32)), 'eval/epoch_eval_time': 4.092176675796509, 'eval/sps': 31279.19690199736}
I0727 00:00:53.042251 140267183036224 train.py:379] starting iteration 234 1711.307280778885
I0727 00:01:00.209433 140267183036224 train.py:394] {'eval/walltime': 976.0403416156769, 'training/sps': 40234.11809873062, 'training/walltime': 736.6678102016449, 'training/entropy_loss': Array(-0.00359543, dtype=float32), 'training/policy_loss': Array(0.00016368, dtype=float32), 'training/total_loss': Array(3276.174, dtype=float32), 'training/v_loss': Array(3276.1772, dtype=float32), 'eval/episode_goal_distance': (Array(0.28411657, dtype=float32), Array(0.03518926, dtype=float32)), 'eval/episode_reward': (Array(-40544.61, dtype=float32), Array(21311.617, dtype=float32)), 'eval/avg_episode_length': (Array(798.0547, dtype=float32), Array(399.989, dtype=float32)), 'eval/epoch_eval_time': 4.109410524368286, 'eval/sps': 31148.01970768706}
I0727 00:01:00.211944 140267183036224 train.py:379] starting iteration 235 1718.4769747257233
I0727 00:01:07.370913 140267183036224 train.py:394] {'eval/walltime': 980.1498506069183, 'training/sps': 40343.174423015415, 'training/walltime': 739.7136785984039, 'training/entropy_loss': Array(-0.00404162, dtype=float32), 'training/policy_loss': Array(-0.00010786, dtype=float32), 'training/total_loss': Array(2899.5815, dtype=float32), 'training/v_loss': Array(2899.586, dtype=float32), 'eval/episode_goal_distance': (Array(0.28544068, dtype=float32), Array(0.03292955, dtype=float32)), 'eval/episode_reward': (Array(-40579.414, dtype=float32), Array(20698.205, dtype=float32)), 'eval/avg_episode_length': (Array(805.77344, dtype=float32), Array(394.23795, dtype=float32)), 'eval/epoch_eval_time': 4.109508991241455, 'eval/sps': 31147.273378110327}
I0727 00:01:07.373416 140267183036224 train.py:379] starting iteration 236 1725.6384453773499
I0727 00:01:14.537524 140267183036224 train.py:394] {'eval/walltime': 984.2588095664978, 'training/sps': 40267.52367610132, 'training/walltime': 742.76526927948, 'training/entropy_loss': Array(-0.00411593, dtype=float32), 'training/policy_loss': Array(0.00012515, dtype=float32), 'training/total_loss': Array(2455.8613, dtype=float32), 'training/v_loss': Array(2455.8652, dtype=float32), 'eval/episode_goal_distance': (Array(0.282765, dtype=float32), Array(0.03864816, dtype=float32)), 'eval/episode_reward': (Array(-44237.6, dtype=float32), Array(17530.521, dtype=float32)), 'eval/avg_episode_length': (Array(883.46094, dtype=float32), Array(319.86417, dtype=float32)), 'eval/epoch_eval_time': 4.108958959579468, 'eval/sps': 31151.44280075754}
I0727 00:01:14.540208 140267183036224 train.py:379] starting iteration 237 1732.8052380084991
I0727 00:01:21.704026 140267183036224 train.py:394] {'eval/walltime': 988.370397567749, 'training/sps': 40306.99520324335, 'training/walltime': 745.8138716220856, 'training/entropy_loss': Array(-0.00520226, dtype=float32), 'training/policy_loss': Array(0.00028297, dtype=float32), 'training/total_loss': Array(2266.4775, dtype=float32), 'training/v_loss': Array(2266.4822, dtype=float32), 'eval/episode_goal_distance': (Array(0.281569, dtype=float32), Array(0.03574486, dtype=float32)), 'eval/episode_reward': (Array(-40100.58, dtype=float32), Array(21239.047, dtype=float32)), 'eval/avg_episode_length': (Array(797.96094, dtype=float32), Array(400.1746, dtype=float32)), 'eval/epoch_eval_time': 4.111588001251221, 'eval/sps': 31131.523868891432}
I0727 00:01:21.706664 140267183036224 train.py:379] starting iteration 238 1739.9716930389404
I0727 00:01:28.875128 140267183036224 train.py:394] {'eval/walltime': 992.4813497066498, 'training/sps': 40236.96077319258, 'training/walltime': 748.8677802085876, 'training/entropy_loss': Array(-0.00464729, dtype=float32), 'training/policy_loss': Array(0.00041369, dtype=float32), 'training/total_loss': Array(2263.8267, dtype=float32), 'training/v_loss': Array(2263.831, dtype=float32), 'eval/episode_goal_distance': (Array(0.28045177, dtype=float32), Array(0.03955132, dtype=float32)), 'eval/episode_reward': (Array(-38374.344, dtype=float32), Array(22150.926, dtype=float32)), 'eval/avg_episode_length': (Array(767.10156, dtype=float32), Array(420.93994, dtype=float32)), 'eval/epoch_eval_time': 4.110952138900757, 'eval/sps': 31136.33914361903}
I0727 00:01:28.879576 140267183036224 train.py:379] starting iteration 239 1747.144590139389
I0727 00:01:36.055927 140267183036224 train.py:394] {'eval/walltime': 996.6018686294556, 'training/sps': 40264.635788596395, 'training/walltime': 751.9195897579193, 'training/entropy_loss': Array(-0.00453803, dtype=float32), 'training/policy_loss': Array(0.00033203, dtype=float32), 'training/total_loss': Array(2285.9492, dtype=float32), 'training/v_loss': Array(2285.9536, dtype=float32), 'eval/episode_goal_distance': (Array(0.2834228, dtype=float32), Array(0.03764385, dtype=float32)), 'eval/episode_reward': (Array(-39235.336, dtype=float32), Array(20821.475, dtype=float32)), 'eval/avg_episode_length': (Array(798.14844, dtype=float32), Array(399.80392, dtype=float32)), 'eval/epoch_eval_time': 4.120518922805786, 'eval/sps': 31064.048581735653}
I0727 00:01:36.058384 140267183036224 train.py:379] starting iteration 240 1754.323413848877
I0727 00:01:43.205813 140267183036224 train.py:394] {'eval/walltime': 1000.7016773223877, 'training/sps': 40365.37723235981, 'training/walltime': 754.963782787323, 'training/entropy_loss': Array(-0.00493775, dtype=float32), 'training/policy_loss': Array(0.0002322, dtype=float32), 'training/total_loss': Array(2374.5571, dtype=float32), 'training/v_loss': Array(2374.5618, dtype=float32), 'eval/episode_goal_distance': (Array(0.27772143, dtype=float32), Array(0.03413444, dtype=float32)), 'eval/episode_reward': (Array(-38264.625, dtype=float32), Array(21106.73, dtype=float32)), 'eval/avg_episode_length': (Array(782.7422, dtype=float32), Array(410.57944, dtype=float32)), 'eval/epoch_eval_time': 4.099808692932129, 'eval/sps': 31220.968973665964}
I0727 00:01:43.208168 140267183036224 train.py:379] starting iteration 241 1761.4731976985931
I0727 00:01:50.359904 140267183036224 train.py:394] {'eval/walltime': 1004.8088347911835, 'training/sps': 40410.02100255093, 'training/walltime': 758.0046126842499, 'training/entropy_loss': Array(-0.00484601, dtype=float32), 'training/policy_loss': Array(0.00012004, dtype=float32), 'training/total_loss': Array(2216.263, dtype=float32), 'training/v_loss': Array(2216.2676, dtype=float32), 'eval/episode_goal_distance': (Array(0.28314522, dtype=float32), Array(0.03818431, dtype=float32)), 'eval/episode_reward': (Array(-36952.523, dtype=float32), Array(22759.504, dtype=float32)), 'eval/avg_episode_length': (Array(743.4844, dtype=float32), Array(435.23056, dtype=float32)), 'eval/epoch_eval_time': 4.107157468795776, 'eval/sps': 31165.106517703047}
I0727 00:01:50.362436 140267183036224 train.py:379] starting iteration 242 1768.627465248108
I0727 00:01:57.535264 140267183036224 train.py:394] {'eval/walltime': 1008.9210515022278, 'training/sps': 40227.14974892963, 'training/walltime': 761.0592660903931, 'training/entropy_loss': Array(-0.00484431, dtype=float32), 'training/policy_loss': Array(0.00047064, dtype=float32), 'training/total_loss': Array(2184.9468, dtype=float32), 'training/v_loss': Array(2184.951, dtype=float32), 'eval/episode_goal_distance': (Array(0.2857321, dtype=float32), Array(0.03830719, dtype=float32)), 'eval/episode_reward': (Array(-39169., dtype=float32), Array(20780.816, dtype=float32)), 'eval/avg_episode_length': (Array(798.0469, dtype=float32), Array(400.0047, dtype=float32)), 'eval/epoch_eval_time': 4.1122167110443115, 'eval/sps': 31126.764223350954}
I0727 00:01:57.537587 140267183036224 train.py:379] starting iteration 243 1775.802616596222
I0727 00:02:04.699749 140267183036224 train.py:394] {'eval/walltime': 1013.028039932251, 'training/sps': 40267.77221736949, 'training/walltime': 764.1108379364014, 'training/entropy_loss': Array(-0.00602891, dtype=float32), 'training/policy_loss': Array(0.00038896, dtype=float32), 'training/total_loss': Array(2210.1594, dtype=float32), 'training/v_loss': Array(2210.165, dtype=float32), 'eval/episode_goal_distance': (Array(0.28115916, dtype=float32), Array(0.04196856, dtype=float32)), 'eval/episode_reward': (Array(-38624.07, dtype=float32), Array(21110.053, dtype=float32)), 'eval/avg_episode_length': (Array(790.25, dtype=float32), Array(405.67825, dtype=float32)), 'eval/epoch_eval_time': 4.106988430023193, 'eval/sps': 31166.38923652316}
I0727 00:02:04.701945 140267183036224 train.py:379] starting iteration 244 1782.9669749736786
I0727 00:02:11.859617 140267183036224 train.py:394] {'eval/walltime': 1017.1308410167694, 'training/sps': 40273.247195541626, 'training/walltime': 767.161994934082, 'training/entropy_loss': Array(-0.00654889, dtype=float32), 'training/policy_loss': Array(0.00055692, dtype=float32), 'training/total_loss': Array(2189.2246, dtype=float32), 'training/v_loss': Array(2189.2305, dtype=float32), 'eval/episode_goal_distance': (Array(0.28903237, dtype=float32), Array(0.03906321, dtype=float32)), 'eval/episode_reward': (Array(-36964.082, dtype=float32), Array(24042.877, dtype=float32)), 'eval/avg_episode_length': (Array(712.7578, dtype=float32), Array(450.47345, dtype=float32)), 'eval/epoch_eval_time': 4.102801084518433, 'eval/sps': 31198.19785633698}
I0727 00:02:11.862054 140267183036224 train.py:379] starting iteration 245 1790.1270837783813
I0727 00:02:19.021461 140267183036224 train.py:394] {'eval/walltime': 1021.2364900112152, 'training/sps': 40286.509733596584, 'training/walltime': 770.2121474742889, 'training/entropy_loss': Array(-0.00725371, dtype=float32), 'training/policy_loss': Array(0.00026121, dtype=float32), 'training/total_loss': Array(2068.213, dtype=float32), 'training/v_loss': Array(2068.2197, dtype=float32), 'eval/episode_goal_distance': (Array(0.28488272, dtype=float32), Array(0.03663156, dtype=float32)), 'eval/episode_reward': (Array(-40617.043, dtype=float32), Array(20036.584, dtype=float32)), 'eval/avg_episode_length': (Array(821.5469, dtype=float32), Array(381.29016, dtype=float32)), 'eval/epoch_eval_time': 4.105648994445801, 'eval/sps': 31176.557025006474}
I0727 00:02:19.023689 140267183036224 train.py:379] starting iteration 246 1797.288718700409
I0727 00:02:26.183504 140267183036224 train.py:394] {'eval/walltime': 1025.335638999939, 'training/sps': 40196.45684486764, 'training/walltime': 773.2691333293915, 'training/entropy_loss': Array(-0.00691582, dtype=float32), 'training/policy_loss': Array(0.00024052, dtype=float32), 'training/total_loss': Array(2104.6396, dtype=float32), 'training/v_loss': Array(2104.6462, dtype=float32), 'eval/episode_goal_distance': (Array(0.28521988, dtype=float32), Array(0.03551932, dtype=float32)), 'eval/episode_reward': (Array(-38503.52, dtype=float32), Array(20790.19, dtype=float32)), 'eval/avg_episode_length': (Array(790.2969, dtype=float32), Array(405.58768, dtype=float32)), 'eval/epoch_eval_time': 4.099148988723755, 'eval/sps': 31225.993578694495}
I0727 00:02:26.185865 140267183036224 train.py:379] starting iteration 247 1804.4508950710297
I0727 00:02:33.341691 140267183036224 train.py:394] {'eval/walltime': 1029.4334523677826, 'training/sps': 40230.69485823763, 'training/walltime': 776.3235175609589, 'training/entropy_loss': Array(-0.00497778, dtype=float32), 'training/policy_loss': Array(0.0003354, dtype=float32), 'training/total_loss': Array(2136.8684, dtype=float32), 'training/v_loss': Array(2136.873, dtype=float32), 'eval/episode_goal_distance': (Array(0.27785218, dtype=float32), Array(0.04043704, dtype=float32)), 'eval/episode_reward': (Array(-40503.5, dtype=float32), Array(19730.432, dtype=float32)), 'eval/avg_episode_length': (Array(829.0781, dtype=float32), Array(375.1804, dtype=float32)), 'eval/epoch_eval_time': 4.097813367843628, 'eval/sps': 31236.17122352178}
I0727 00:02:33.344135 140267183036224 train.py:379] starting iteration 248 1811.609165430069
I0727 00:02:40.499785 140267183036224 train.py:394] {'eval/walltime': 1033.5313866138458, 'training/sps': 40258.336094833554, 'training/walltime': 779.3758046627045, 'training/entropy_loss': Array(-0.00420403, dtype=float32), 'training/policy_loss': Array(0.000434, dtype=float32), 'training/total_loss': Array(2157.1353, dtype=float32), 'training/v_loss': Array(2157.139, dtype=float32), 'eval/episode_goal_distance': (Array(0.28190243, dtype=float32), Array(0.03853324, dtype=float32)), 'eval/episode_reward': (Array(-38884.453, dtype=float32), Array(21492.627, dtype=float32)), 'eval/avg_episode_length': (Array(782.4531, dtype=float32), Array(411.12582, dtype=float32)), 'eval/epoch_eval_time': 4.097934246063232, 'eval/sps': 31235.249839102693}
I0727 00:02:40.502053 140267183036224 train.py:379] starting iteration 249 1818.7670834064484
I0727 00:02:47.657360 140267183036224 train.py:394] {'eval/walltime': 1037.6317920684814, 'training/sps': 40270.6574082517, 'training/walltime': 782.4271578788757, 'training/entropy_loss': Array(-0.00523943, dtype=float32), 'training/policy_loss': Array(-4.39171e-06, dtype=float32), 'training/total_loss': Array(1967.5264, dtype=float32), 'training/v_loss': Array(1967.5316, dtype=float32), 'eval/episode_goal_distance': (Array(0.28448, dtype=float32), Array(0.03627881, dtype=float32)), 'eval/episode_reward': (Array(-37110.23, dtype=float32), Array(23468.969, dtype=float32)), 'eval/avg_episode_length': (Array(728.15625, dtype=float32), Array(443.12643, dtype=float32)), 'eval/epoch_eval_time': 4.10040545463562, 'eval/sps': 31216.425159929615}
I0727 00:02:47.659835 140267183036224 train.py:379] starting iteration 250 1825.924864768982
I0727 00:02:54.816108 140267183036224 train.py:394] {'eval/walltime': 1041.7309939861298, 'training/sps': 40243.44545858942, 'training/walltime': 785.4805743694305, 'training/entropy_loss': Array(-0.00534405, dtype=float32), 'training/policy_loss': Array(4.1576663e-05, dtype=float32), 'training/total_loss': Array(13136.369, dtype=float32), 'training/v_loss': Array(13136.374, dtype=float32), 'eval/episode_goal_distance': (Array(0.28344673, dtype=float32), Array(0.03880994, dtype=float32)), 'eval/episode_reward': (Array(-40162.47, dtype=float32), Array(20765.553, dtype=float32)), 'eval/avg_episode_length': (Array(805.7656, dtype=float32), Array(394.25336, dtype=float32)), 'eval/epoch_eval_time': 4.099201917648315, 'eval/sps': 31225.59038844145}
I0727 00:02:54.818638 140267183036224 train.py:379] starting iteration 251 1833.0836679935455
I0727 00:03:01.977669 140267183036224 train.py:394] {'eval/walltime': 1045.8326954841614, 'training/sps': 40240.53903057508, 'training/walltime': 788.534211397171, 'training/entropy_loss': Array(-0.00550592, dtype=float32), 'training/policy_loss': Array(0.00020747, dtype=float32), 'training/total_loss': Array(2982.4478, dtype=float32), 'training/v_loss': Array(2982.4531, dtype=float32), 'eval/episode_goal_distance': (Array(0.2902602, dtype=float32), Array(0.04038567, dtype=float32)), 'eval/episode_reward': (Array(-41444.97, dtype=float32), Array(21026.758, dtype=float32)), 'eval/avg_episode_length': (Array(813.64844, dtype=float32), Array(387.92267, dtype=float32)), 'eval/epoch_eval_time': 4.101701498031616, 'eval/sps': 31206.56148708688}
I0727 00:03:01.980146 140267183036224 train.py:379] starting iteration 252 1840.2451756000519
I0727 00:03:09.141598 140267183036224 train.py:394] {'eval/walltime': 1049.9338610172272, 'training/sps': 40202.07550996339, 'training/walltime': 791.5907700061798, 'training/entropy_loss': Array(-0.00485872, dtype=float32), 'training/policy_loss': Array(3.2978714e-05, dtype=float32), 'training/total_loss': Array(2645.5942, dtype=float32), 'training/v_loss': Array(2645.599, dtype=float32), 'eval/episode_goal_distance': (Array(0.28230503, dtype=float32), Array(0.03735862, dtype=float32)), 'eval/episode_reward': (Array(-39531.184, dtype=float32), Array(21138.852, dtype=float32)), 'eval/avg_episode_length': (Array(798.09375, dtype=float32), Array(399.9117, dtype=float32)), 'eval/epoch_eval_time': 4.101165533065796, 'eval/sps': 31210.639748138758}
I0727 00:03:09.144170 140267183036224 train.py:379] starting iteration 253 1847.4091997146606
I0727 00:03:16.311996 140267183036224 train.py:394] {'eval/walltime': 1054.0412023067474, 'training/sps': 40199.510545003126, 'training/walltime': 794.6475236415863, 'training/entropy_loss': Array(-0.00420504, dtype=float32), 'training/policy_loss': Array(0.00047448, dtype=float32), 'training/total_loss': Array(2281.3633, dtype=float32), 'training/v_loss': Array(2281.3672, dtype=float32), 'eval/episode_goal_distance': (Array(0.28549224, dtype=float32), Array(0.03844739, dtype=float32)), 'eval/episode_reward': (Array(-38940.38, dtype=float32), Array(22415.336, dtype=float32)), 'eval/avg_episode_length': (Array(766.90625, dtype=float32), Array(421.29276, dtype=float32)), 'eval/epoch_eval_time': 4.107341289520264, 'eval/sps': 31163.711748665122}
I0727 00:03:16.314605 140267183036224 train.py:379] starting iteration 254 1854.5796341896057
I0727 00:03:23.470769 140267183036224 train.py:394] {'eval/walltime': 1058.135690689087, 'training/sps': 40183.93651045621, 'training/walltime': 797.7054619789124, 'training/entropy_loss': Array(-0.00528047, dtype=float32), 'training/policy_loss': Array(0.0004814, dtype=float32), 'training/total_loss': Array(2242.1064, dtype=float32), 'training/v_loss': Array(2242.1113, dtype=float32), 'eval/episode_goal_distance': (Array(0.28762448, dtype=float32), Array(0.03993362, dtype=float32)), 'eval/episode_reward': (Array(-40694.383, dtype=float32), Array(21686.254, dtype=float32)), 'eval/avg_episode_length': (Array(797.96094, dtype=float32), Array(400.17432, dtype=float32)), 'eval/epoch_eval_time': 4.0944883823394775, 'eval/sps': 31261.536985205545}
I0727 00:03:23.473252 140267183036224 train.py:379] starting iteration 255 1861.7382819652557
I0727 00:03:30.635571 140267183036224 train.py:394] {'eval/walltime': 1062.2380013465881, 'training/sps': 40202.02533636107, 'training/walltime': 800.7620244026184, 'training/entropy_loss': Array(-0.00702305, dtype=float32), 'training/policy_loss': Array(0.00022104, dtype=float32), 'training/total_loss': Array(2065.2285, dtype=float32), 'training/v_loss': Array(2065.2354, dtype=float32), 'eval/episode_goal_distance': (Array(0.2856573, dtype=float32), Array(0.03627098, dtype=float32)), 'eval/episode_reward': (Array(-35341.42, dtype=float32), Array(23458.11, dtype=float32)), 'eval/avg_episode_length': (Array(712.52344, dtype=float32), Array(450.84103, dtype=float32)), 'eval/epoch_eval_time': 4.102310657501221, 'eval/sps': 31201.927568783085}
I0727 00:03:30.753705 140267183036224 train.py:379] starting iteration 256 1869.0187258720398
I0727 00:03:37.917511 140267183036224 train.py:394] {'eval/walltime': 1066.351599931717, 'training/sps': 40338.169764879276, 'training/walltime': 803.8082706928253, 'training/entropy_loss': Array(-0.0057265, dtype=float32), 'training/policy_loss': Array(0.0001303, dtype=float32), 'training/total_loss': Array(2053.2095, dtype=float32), 'training/v_loss': Array(2053.2153, dtype=float32), 'eval/episode_goal_distance': (Array(0.2862359, dtype=float32), Array(0.03626393, dtype=float32)), 'eval/episode_reward': (Array(-45454.246, dtype=float32), Array(17236.127, dtype=float32)), 'eval/avg_episode_length': (Array(891.2969, dtype=float32), Array(310.19244, dtype=float32)), 'eval/epoch_eval_time': 4.113598585128784, 'eval/sps': 31116.307863080594}
I0727 00:03:37.920116 140267183036224 train.py:379] starting iteration 257 1876.18514585495
I0727 00:03:45.070935 140267183036224 train.py:394] {'eval/walltime': 1070.4580564498901, 'training/sps': 40411.94746845919, 'training/walltime': 806.8489556312561, 'training/entropy_loss': Array(-0.00473026, dtype=float32), 'training/policy_loss': Array(-1.668217e-05, dtype=float32), 'training/total_loss': Array(2313.6108, dtype=float32), 'training/v_loss': Array(2313.6155, dtype=float32), 'eval/episode_goal_distance': (Array(0.28761116, dtype=float32), Array(0.0364931, dtype=float32)), 'eval/episode_reward': (Array(-39913.324, dtype=float32), Array(21570.93, dtype=float32)), 'eval/avg_episode_length': (Array(790.2031, dtype=float32), Array(405.76895, dtype=float32)), 'eval/epoch_eval_time': 4.106456518173218, 'eval/sps': 31170.426238177137}
I0727 00:03:45.073505 140267183036224 train.py:379] starting iteration 258 1883.3385355472565
I0727 00:03:52.237785 140267183036224 train.py:394] {'eval/walltime': 1074.5675959587097, 'training/sps': 40273.41083849036, 'training/walltime': 809.9001002311707, 'training/entropy_loss': Array(-0.00541576, dtype=float32), 'training/policy_loss': Array(0.00022435, dtype=float32), 'training/total_loss': Array(2214.7427, dtype=float32), 'training/v_loss': Array(2214.7478, dtype=float32), 'eval/episode_goal_distance': (Array(0.28554988, dtype=float32), Array(0.03801569, dtype=float32)), 'eval/episode_reward': (Array(-42433.742, dtype=float32), Array(21068.637, dtype=float32)), 'eval/avg_episode_length': (Array(821.2422, dtype=float32), Array(381.94107, dtype=float32)), 'eval/epoch_eval_time': 4.10953950881958, 'eval/sps': 31147.04207741431}
I0727 00:03:52.240370 140267183036224 train.py:379] starting iteration 259 1890.5053992271423
I0727 00:03:59.398658 140267183036224 train.py:394] {'eval/walltime': 1078.6771001815796, 'training/sps': 40352.55242172356, 'training/walltime': 812.9452607631683, 'training/entropy_loss': Array(-0.00612936, dtype=float32), 'training/policy_loss': Array(0.00034149, dtype=float32), 'training/total_loss': Array(2080.1318, dtype=float32), 'training/v_loss': Array(2080.1375, dtype=float32), 'eval/episode_goal_distance': (Array(0.28505185, dtype=float32), Array(0.03635466, dtype=float32)), 'eval/episode_reward': (Array(-38860.53, dtype=float32), Array(22028.555, dtype=float32)), 'eval/avg_episode_length': (Array(774.71094, dtype=float32), Array(416.2553, dtype=float32)), 'eval/epoch_eval_time': 4.109504222869873, 'eval/sps': 31147.309519154398}
I0727 00:03:59.401154 140267183036224 train.py:379] starting iteration 260 1897.6661834716797
I0727 00:04:06.566117 140267183036224 train.py:394] {'eval/walltime': 1082.7834842205048, 'training/sps': 40221.23529765697, 'training/walltime': 816.0003633499146, 'training/entropy_loss': Array(-0.0059561, dtype=float32), 'training/policy_loss': Array(-0.00012699, dtype=float32), 'training/total_loss': Array(2119.0798, dtype=float32), 'training/v_loss': Array(2119.086, dtype=float32), 'eval/episode_goal_distance': (Array(0.28176582, dtype=float32), Array(0.04054384, dtype=float32)), 'eval/episode_reward': (Array(-40286.367, dtype=float32), Array(19825.44, dtype=float32)), 'eval/avg_episode_length': (Array(829.21875, dtype=float32), Array(374.87207, dtype=float32)), 'eval/epoch_eval_time': 4.106384038925171, 'eval/sps': 31170.976408115854}
I0727 00:04:06.568446 140267183036224 train.py:379] starting iteration 261 1904.8334755897522
I0727 00:04:13.735031 140267183036224 train.py:394] {'eval/walltime': 1086.8972005844116, 'training/sps': 40300.47111562546, 'training/walltime': 819.0494592189789, 'training/entropy_loss': Array(-0.00549145, dtype=float32), 'training/policy_loss': Array(5.283197e-06, dtype=float32), 'training/total_loss': Array(2217.6453, dtype=float32), 'training/v_loss': Array(2217.6506, dtype=float32), 'eval/episode_goal_distance': (Array(0.28440052, dtype=float32), Array(0.03550924, dtype=float32)), 'eval/episode_reward': (Array(-38667.668, dtype=float32), Array(21724.959, dtype=float32)), 'eval/avg_episode_length': (Array(774.875, dtype=float32), Array(415.95215, dtype=float32)), 'eval/epoch_eval_time': 4.11371636390686, 'eval/sps': 31115.416979899997}
I0727 00:04:13.737530 140267183036224 train.py:379] starting iteration 262 1912.0025608539581
I0727 00:04:20.897945 140267183036224 train.py:394] {'eval/walltime': 1091.0068275928497, 'training/sps': 40323.74687916937, 'training/walltime': 822.0967950820923, 'training/entropy_loss': Array(-0.00675864, dtype=float32), 'training/policy_loss': Array(0.00021384, dtype=float32), 'training/total_loss': Array(2255.0493, dtype=float32), 'training/v_loss': Array(2255.056, dtype=float32), 'eval/episode_goal_distance': (Array(0.27943188, dtype=float32), Array(0.03975519, dtype=float32)), 'eval/episode_reward': (Array(-41179.29, dtype=float32), Array(20114.445, dtype=float32)), 'eval/avg_episode_length': (Array(829.0547, dtype=float32), Array(375.2319, dtype=float32)), 'eval/epoch_eval_time': 4.10962700843811, 'eval/sps': 31146.37891399473}
I0727 00:04:20.900310 140267183036224 train.py:379] starting iteration 263 1919.1653409004211
I0727 00:04:28.065945 140267183036224 train.py:394] {'eval/walltime': 1095.1236293315887, 'training/sps': 40352.3470635184, 'training/walltime': 825.1419711112976, 'training/entropy_loss': Array(-0.00707868, dtype=float32), 'training/policy_loss': Array(0.00017812, dtype=float32), 'training/total_loss': Array(2222.181, dtype=float32), 'training/v_loss': Array(2222.1875, dtype=float32), 'eval/episode_goal_distance': (Array(0.27680576, dtype=float32), Array(0.03811614, dtype=float32)), 'eval/episode_reward': (Array(-38781.633, dtype=float32), Array(20549.924, dtype=float32)), 'eval/avg_episode_length': (Array(798.2578, dtype=float32), Array(399.58698, dtype=float32)), 'eval/epoch_eval_time': 4.116801738739014, 'eval/sps': 31092.097245180117}
I0727 00:04:28.068341 140267183036224 train.py:379] starting iteration 264 1926.333370923996
I0727 00:04:35.237360 140267183036224 train.py:394] {'eval/walltime': 1099.235916376114, 'training/sps': 40247.62516665525, 'training/walltime': 828.1950705051422, 'training/entropy_loss': Array(-0.00573003, dtype=float32), 'training/policy_loss': Array(0.00027616, dtype=float32), 'training/total_loss': Array(2149.978, dtype=float32), 'training/v_loss': Array(2149.9836, dtype=float32), 'eval/episode_goal_distance': (Array(0.2813629, dtype=float32), Array(0.03278049, dtype=float32)), 'eval/episode_reward': (Array(-39041.47, dtype=float32), Array(22128.51, dtype=float32)), 'eval/avg_episode_length': (Array(774.8203, dtype=float32), Array(416.05307, dtype=float32)), 'eval/epoch_eval_time': 4.1122870445251465, 'eval/sps': 31126.23185446443}
I0727 00:04:35.239984 140267183036224 train.py:379] starting iteration 265 1933.5050110816956
I0727 00:04:42.419999 140267183036224 train.py:394] {'eval/walltime': 1103.353352546692, 'training/sps': 40199.37885636966, 'training/walltime': 831.251834154129, 'training/entropy_loss': Array(-0.00490847, dtype=float32), 'training/policy_loss': Array(2.1905289e-05, dtype=float32), 'training/total_loss': Array(2072.8933, dtype=float32), 'training/v_loss': Array(2072.8982, dtype=float32), 'eval/episode_goal_distance': (Array(0.27843186, dtype=float32), Array(0.0391217, dtype=float32)), 'eval/episode_reward': (Array(-39231.094, dtype=float32), Array(19900.162, dtype=float32)), 'eval/avg_episode_length': (Array(813.4844, dtype=float32), Array(388.2638, dtype=float32)), 'eval/epoch_eval_time': 4.117436170578003, 'eval/sps': 31087.306444396305}
I0727 00:04:42.422502 140267183036224 train.py:379] starting iteration 266 1940.6875314712524
I0727 00:04:49.594025 140267183036224 train.py:394] {'eval/walltime': 1107.4764354228973, 'training/sps': 40373.68072823582, 'training/walltime': 834.295401096344, 'training/entropy_loss': Array(-0.00451131, dtype=float32), 'training/policy_loss': Array(0.00033711, dtype=float32), 'training/total_loss': Array(12917.59, dtype=float32), 'training/v_loss': Array(12917.594, dtype=float32), 'eval/episode_goal_distance': (Array(0.2877726, dtype=float32), Array(0.04120986, dtype=float32)), 'eval/episode_reward': (Array(-36379.15, dtype=float32), Array(22550.537, dtype=float32)), 'eval/avg_episode_length': (Array(735.9219, dtype=float32), Array(439.0943, dtype=float32)), 'eval/epoch_eval_time': 4.123082876205444, 'eval/sps': 31044.731295287704}
I0727 00:04:49.596322 140267183036224 train.py:379] starting iteration 267 1947.8613517284393
I0727 00:04:56.767317 140267183036224 train.py:394] {'eval/walltime': 1111.595520734787, 'training/sps': 40311.1628998698, 'training/walltime': 837.343688249588, 'training/entropy_loss': Array(-0.00631397, dtype=float32), 'training/policy_loss': Array(6.773641e-05, dtype=float32), 'training/total_loss': Array(3257.651, dtype=float32), 'training/v_loss': Array(3257.657, dtype=float32), 'eval/episode_goal_distance': (Array(0.28274882, dtype=float32), Array(0.03409962, dtype=float32)), 'eval/episode_reward': (Array(-38911.203, dtype=float32), Array(21013.818, dtype=float32)), 'eval/avg_episode_length': (Array(790.2578, dtype=float32), Array(405.66345, dtype=float32)), 'eval/epoch_eval_time': 4.119085311889648, 'eval/sps': 31074.860146870677}
I0727 00:04:56.769714 140267183036224 train.py:379] starting iteration 268 1955.0347430706024
I0727 00:05:03.922669 140267183036224 train.py:394] {'eval/walltime': 1115.6978492736816, 'training/sps': 40329.22131145617, 'training/walltime': 840.3906104564667, 'training/entropy_loss': Array(-0.0061233, dtype=float32), 'training/policy_loss': Array(0.00016566, dtype=float32), 'training/total_loss': Array(2827.5718, dtype=float32), 'training/v_loss': Array(2827.5781, dtype=float32), 'eval/episode_goal_distance': (Array(0.28520542, dtype=float32), Array(0.03209686, dtype=float32)), 'eval/episode_reward': (Array(-37668.547, dtype=float32), Array(21410.674, dtype=float32)), 'eval/avg_episode_length': (Array(767.0781, dtype=float32), Array(420.9822, dtype=float32)), 'eval/epoch_eval_time': 4.102328538894653, 'eval/sps': 31201.79156457537}
I0727 00:05:03.924924 140267183036224 train.py:379] starting iteration 269 1962.1899540424347
I0727 00:05:11.092183 140267183036224 train.py:394] {'eval/walltime': 1119.8129692077637, 'training/sps': 40312.04257684923, 'training/walltime': 843.4388310909271, 'training/entropy_loss': Array(-0.00630516, dtype=float32), 'training/policy_loss': Array(0.00026278, dtype=float32), 'training/total_loss': Array(2990.4468, dtype=float32), 'training/v_loss': Array(2990.4526, dtype=float32), 'eval/episode_goal_distance': (Array(0.28343022, dtype=float32), Array(0.03715466, dtype=float32)), 'eval/episode_reward': (Array(-39197.945, dtype=float32), Array(20526.855, dtype=float32)), 'eval/avg_episode_length': (Array(797.9922, dtype=float32), Array(400.11307, dtype=float32)), 'eval/epoch_eval_time': 4.115119934082031, 'eval/sps': 31104.804246380547}
I0727 00:05:11.096843 140267183036224 train.py:379] starting iteration 270 1969.3618569374084
I0727 00:05:18.270329 140267183036224 train.py:394] {'eval/walltime': 1123.9348130226135, 'training/sps': 40318.81958364283, 'training/walltime': 846.4865393638611, 'training/entropy_loss': Array(-0.00642231, dtype=float32), 'training/policy_loss': Array(8.582304e-05, dtype=float32), 'training/total_loss': Array(2759.2986, dtype=float32), 'training/v_loss': Array(2759.305, dtype=float32), 'eval/episode_goal_distance': (Array(0.28613645, dtype=float32), Array(0.03678841, dtype=float32)), 'eval/episode_reward': (Array(-43194.17, dtype=float32), Array(19077.797, dtype=float32)), 'eval/avg_episode_length': (Array(852.2578, dtype=float32), Array(353.8682, dtype=float32)), 'eval/epoch_eval_time': 4.1218438148498535, 'eval/sps': 31054.06360591629}
I0727 00:05:18.272778 140267183036224 train.py:379] starting iteration 271 1976.5378074645996
I0727 00:05:25.447738 140267183036224 train.py:394] {'eval/walltime': 1128.0585906505585, 'training/sps': 40321.078040812536, 'training/walltime': 849.5340769290924, 'training/entropy_loss': Array(-0.00605679, dtype=float32), 'training/policy_loss': Array(7.608045e-05, dtype=float32), 'training/total_loss': Array(2148.4695, dtype=float32), 'training/v_loss': Array(2148.4756, dtype=float32), 'eval/episode_goal_distance': (Array(0.2846405, dtype=float32), Array(0.03382874, dtype=float32)), 'eval/episode_reward': (Array(-36429.727, dtype=float32), Array(22623.691, dtype=float32)), 'eval/avg_episode_length': (Array(735.8203, dtype=float32), Array(439.263, dtype=float32)), 'eval/epoch_eval_time': 4.123777627944946, 'eval/sps': 31039.50104695336}
I0727 00:05:25.450104 140267183036224 train.py:379] starting iteration 272 1983.7151336669922
I0727 00:05:32.605225 140267183036224 train.py:394] {'eval/walltime': 1132.166524887085, 'training/sps': 40373.8483510988, 'training/walltime': 852.5776312351227, 'training/entropy_loss': Array(-0.00599593, dtype=float32), 'training/policy_loss': Array(-5.153913e-05, dtype=float32), 'training/total_loss': Array(2154.2222, dtype=float32), 'training/v_loss': Array(2154.2285, dtype=float32), 'eval/episode_goal_distance': (Array(0.28405023, dtype=float32), Array(0.03600533, dtype=float32)), 'eval/episode_reward': (Array(-39701.03, dtype=float32), Array(20387.879, dtype=float32)), 'eval/avg_episode_length': (Array(805.875, dtype=float32), Array(394.03192, dtype=float32)), 'eval/epoch_eval_time': 4.107934236526489, 'eval/sps': 31159.21351950168}
I0727 00:05:32.607594 140267183036224 train.py:379] starting iteration 273 1990.872623682022
I0727 00:05:39.790295 140267183036224 train.py:394] {'eval/walltime': 1136.2994742393494, 'training/sps': 40340.92296257416, 'training/walltime': 855.6236696243286, 'training/entropy_loss': Array(-0.00523641, dtype=float32), 'training/policy_loss': Array(0.00019353, dtype=float32), 'training/total_loss': Array(2274.8735, dtype=float32), 'training/v_loss': Array(2274.8784, dtype=float32), 'eval/episode_goal_distance': (Array(0.29025823, dtype=float32), Array(0.03420069, dtype=float32)), 'eval/episode_reward': (Array(-40094.973, dtype=float32), Array(21155.93, dtype=float32)), 'eval/avg_episode_length': (Array(797.8594, dtype=float32), Array(400.37564, dtype=float32)), 'eval/epoch_eval_time': 4.132949352264404, 'eval/sps': 30970.61906405168}
I0727 00:05:39.795003 140267183036224 train.py:379] starting iteration 274 1998.0600185394287
I0727 00:05:46.946207 140267183036224 train.py:394] {'eval/walltime': 1140.3977015018463, 'training/sps': 40301.91757890134, 'training/walltime': 858.6726560592651, 'training/entropy_loss': Array(-0.0048879, dtype=float32), 'training/policy_loss': Array(0.00045024, dtype=float32), 'training/total_loss': Array(2115.8315, dtype=float32), 'training/v_loss': Array(2115.836, dtype=float32), 'eval/episode_goal_distance': (Array(0.28361446, dtype=float32), Array(0.03595807, dtype=float32)), 'eval/episode_reward': (Array(-39274.3, dtype=float32), Array(22037.955, dtype=float32)), 'eval/avg_episode_length': (Array(774.7344, dtype=float32), Array(416.21207, dtype=float32)), 'eval/epoch_eval_time': 4.098227262496948, 'eval/sps': 31233.016570684948}
I0727 00:05:46.951116 140267183036224 train.py:379] starting iteration 275 2005.2161319255829
I0727 00:05:54.108153 140267183036224 train.py:394] {'eval/walltime': 1144.5085475444794, 'training/sps': 40392.34918252677, 'training/walltime': 861.7148163318634, 'training/entropy_loss': Array(-0.00398994, dtype=float32), 'training/policy_loss': Array(-5.6638437e-06, dtype=float32), 'training/total_loss': Array(2096.9517, dtype=float32), 'training/v_loss': Array(2096.9558, dtype=float32), 'eval/episode_goal_distance': (Array(0.2757293, dtype=float32), Array(0.04006149, dtype=float32)), 'eval/episode_reward': (Array(-39640.547, dtype=float32), Array(19019.422, dtype=float32)), 'eval/avg_episode_length': (Array(836.77344, dtype=float32), Array(368.44608, dtype=float32)), 'eval/epoch_eval_time': 4.110846042633057, 'eval/sps': 31137.142737170994}
I0727 00:05:54.110641 140267183036224 train.py:379] starting iteration 276 2012.3756701946259
I0727 00:06:01.265508 140267183036224 train.py:394] {'eval/walltime': 1148.6221339702606, 'training/sps': 40451.37259207918, 'training/walltime': 864.752537727356, 'training/entropy_loss': Array(-0.00423702, dtype=float32), 'training/policy_loss': Array(0.00032676, dtype=float32), 'training/total_loss': Array(2064.1484, dtype=float32), 'training/v_loss': Array(2064.1523, dtype=float32), 'eval/episode_goal_distance': (Array(0.28330845, dtype=float32), Array(0.03715837, dtype=float32)), 'eval/episode_reward': (Array(-38006.82, dtype=float32), Array(21000.701, dtype=float32)), 'eval/avg_episode_length': (Array(782.53125, dtype=float32), Array(410.97845, dtype=float32)), 'eval/epoch_eval_time': 4.11358642578125, 'eval/sps': 31116.39983975548}
I0727 00:06:01.268119 140267183036224 train.py:379] starting iteration 277 2019.533148765564
I0727 00:06:08.436810 140267183036224 train.py:394] {'eval/walltime': 1152.741265296936, 'training/sps': 40342.928107947664, 'training/walltime': 867.7984247207642, 'training/entropy_loss': Array(-0.00396124, dtype=float32), 'training/policy_loss': Array(-8.267309e-05, dtype=float32), 'training/total_loss': Array(1957.0342, dtype=float32), 'training/v_loss': Array(1957.0382, dtype=float32), 'eval/episode_goal_distance': (Array(0.28703317, dtype=float32), Array(0.03685298, dtype=float32)), 'eval/episode_reward': (Array(-35962.723, dtype=float32), Array(23185.482, dtype=float32)), 'eval/avg_episode_length': (Array(720.3828, dtype=float32), Array(447.00006, dtype=float32)), 'eval/epoch_eval_time': 4.119131326675415, 'eval/sps': 31074.513009836435}
I0727 00:06:08.439371 140267183036224 train.py:379] starting iteration 278 2026.7044010162354
I0727 00:06:15.613125 140267183036224 train.py:394] {'eval/walltime': 1156.8648109436035, 'training/sps': 40332.989594103354, 'training/walltime': 870.8450622558594, 'training/entropy_loss': Array(-0.00366251, dtype=float32), 'training/policy_loss': Array(0.000292, dtype=float32), 'training/total_loss': Array(2155.4038, dtype=float32), 'training/v_loss': Array(2155.407, dtype=float32), 'eval/episode_goal_distance': (Array(0.28132474, dtype=float32), Array(0.0354692, dtype=float32)), 'eval/episode_reward': (Array(-40370.188, dtype=float32), Array(20726.541, dtype=float32)), 'eval/avg_episode_length': (Array(805.7578, dtype=float32), Array(394.26913, dtype=float32)), 'eval/epoch_eval_time': 4.1235456466674805, 'eval/sps': 31041.24725852024}
I0727 00:06:15.615666 140267183036224 train.py:379] starting iteration 279 2033.8806958198547
I0727 00:06:22.802602 140267183036224 train.py:394] {'eval/walltime': 1160.999826669693, 'training/sps': 40311.21334625742, 'training/walltime': 873.8933455944061, 'training/entropy_loss': Array(-0.00325757, dtype=float32), 'training/policy_loss': Array(-8.4003565e-05, dtype=float32), 'training/total_loss': Array(2121.7654, dtype=float32), 'training/v_loss': Array(2121.7686, dtype=float32), 'eval/episode_goal_distance': (Array(0.28868973, dtype=float32), Array(0.04006951, dtype=float32)), 'eval/episode_reward': (Array(-39624.79, dtype=float32), Array(21341.896, dtype=float32)), 'eval/avg_episode_length': (Array(790.25, dtype=float32), Array(405.67828, dtype=float32)), 'eval/epoch_eval_time': 4.1350157260894775, 'eval/sps': 30955.142248286145}
I0727 00:06:22.805082 140267183036224 train.py:379] starting iteration 280 2041.0701117515564
I0727 00:06:29.957473 140267183036224 train.py:394] {'eval/walltime': 1165.1046245098114, 'training/sps': 40366.376253675706, 'training/walltime': 876.9374632835388, 'training/entropy_loss': Array(-0.00378638, dtype=float32), 'training/policy_loss': Array(7.420125e-05, dtype=float32), 'training/total_loss': Array(2109.4697, dtype=float32), 'training/v_loss': Array(2109.4731, dtype=float32), 'eval/episode_goal_distance': (Array(0.28061962, dtype=float32), Array(0.03647498, dtype=float32)), 'eval/episode_reward': (Array(-40640.883, dtype=float32), Array(20230.867, dtype=float32)), 'eval/avg_episode_length': (Array(821.4219, dtype=float32), Array(381.55756, dtype=float32)), 'eval/epoch_eval_time': 4.104797840118408, 'eval/sps': 31183.021670150672}
I0727 00:06:29.959932 140267183036224 train.py:379] starting iteration 281 2048.2249619960785
I0727 00:06:37.123348 140267183036224 train.py:394] {'eval/walltime': 1169.2195491790771, 'training/sps': 40357.12455255636, 'training/walltime': 879.9822788238525, 'training/entropy_loss': Array(-0.00309027, dtype=float32), 'training/policy_loss': Array(3.4888384e-05, dtype=float32), 'training/total_loss': Array(2134.8247, dtype=float32), 'training/v_loss': Array(2134.8276, dtype=float32), 'eval/episode_goal_distance': (Array(0.27715933, dtype=float32), Array(0.03931744, dtype=float32)), 'eval/episode_reward': (Array(-35362.97, dtype=float32), Array(21661.342, dtype=float32)), 'eval/avg_episode_length': (Array(743.6328, dtype=float32), Array(434.97852, dtype=float32)), 'eval/epoch_eval_time': 4.114924669265747, 'eval/sps': 31106.280257334547}
I0727 00:06:37.125869 140267183036224 train.py:379] starting iteration 282 2055.390899181366
I0727 00:06:44.300579 140267183036224 train.py:394] {'eval/walltime': 1173.3453395366669, 'training/sps': 40362.77558298185, 'training/walltime': 883.0266680717468, 'training/entropy_loss': Array(-0.00332607, dtype=float32), 'training/policy_loss': Array(0.00031542, dtype=float32), 'training/total_loss': Array(1986.1272, dtype=float32), 'training/v_loss': Array(1986.1301, dtype=float32), 'eval/episode_goal_distance': (Array(0.28518197, dtype=float32), Array(0.04038961, dtype=float32)), 'eval/episode_reward': (Array(-35972.273, dtype=float32), Array(23499.67, dtype=float32)), 'eval/avg_episode_length': (Array(720.375, dtype=float32), Array(447.0128, dtype=float32)), 'eval/epoch_eval_time': 4.125790357589722, 'eval/sps': 31024.358706092215}
I0727 00:06:44.303111 140267183036224 train.py:379] starting iteration 283 2062.568140745163
I0727 00:06:51.450480 140267183036224 train.py:394] {'eval/walltime': 1177.4481837749481, 'training/sps': 40406.41887742054, 'training/walltime': 886.0677690505981, 'training/entropy_loss': Array(-0.00314931, dtype=float32), 'training/policy_loss': Array(0.00036105, dtype=float32), 'training/total_loss': Array(13714.443, dtype=float32), 'training/v_loss': Array(13714.446, dtype=float32), 'eval/episode_goal_distance': (Array(0.27916044, dtype=float32), Array(0.04108252, dtype=float32)), 'eval/episode_reward': (Array(-40556.094, dtype=float32), Array(19909.2, dtype=float32)), 'eval/avg_episode_length': (Array(829.03906, dtype=float32), Array(375.26617, dtype=float32)), 'eval/epoch_eval_time': 4.10284423828125, 'eval/sps': 31197.86971333363}
I0727 00:06:51.452814 140267183036224 train.py:379] starting iteration 284 2069.7178435325623
I0727 00:06:58.612842 140267183036224 train.py:394] {'eval/walltime': 1181.5668728351593, 'training/sps': 40449.97569922129, 'training/walltime': 889.1055953502655, 'training/entropy_loss': Array(-0.00492163, dtype=float32), 'training/policy_loss': Array(-2.0839976e-05, dtype=float32), 'training/total_loss': Array(3059.2954, dtype=float32), 'training/v_loss': Array(3059.3003, dtype=float32), 'eval/episode_goal_distance': (Array(0.2864861, dtype=float32), Array(0.04156669, dtype=float32)), 'eval/episode_reward': (Array(-37976.965, dtype=float32), Array(21641.756, dtype=float32)), 'eval/avg_episode_length': (Array(774.8047, dtype=float32), Array(416.0819, dtype=float32)), 'eval/epoch_eval_time': 4.118689060211182, 'eval/sps': 31077.849803363628}
I0727 00:06:58.615191 140267183036224 train.py:379] starting iteration 285 2076.8802211284637
I0727 00:07:05.783881 140267183036224 train.py:394] {'eval/walltime': 1185.6890523433685, 'training/sps': 40382.73119093765, 'training/walltime': 892.1484801769257, 'training/entropy_loss': Array(-0.0052869, dtype=float32), 'training/policy_loss': Array(-0.00066089, dtype=float32), 'training/total_loss': Array(2499.0269, dtype=float32), 'training/v_loss': Array(2499.0327, dtype=float32), 'eval/episode_goal_distance': (Array(0.28202608, dtype=float32), Array(0.04094044, dtype=float32)), 'eval/episode_reward': (Array(-38394.61, dtype=float32), Array(21657.346, dtype=float32)), 'eval/avg_episode_length': (Array(774.8047, dtype=float32), Array(416.08234, dtype=float32)), 'eval/epoch_eval_time': 4.1221795082092285, 'eval/sps': 31051.534690590466}
I0727 00:07:05.786396 140267183036224 train.py:379] starting iteration 286 2084.051426410675
I0727 00:07:12.955627 140267183036224 train.py:394] {'eval/walltime': 1189.8124380111694, 'training/sps': 40391.68441581495, 'training/walltime': 895.1906905174255, 'training/entropy_loss': Array(-0.00486646, dtype=float32), 'training/policy_loss': Array(0.00034038, dtype=float32), 'training/total_loss': Array(2458.0798, dtype=float32), 'training/v_loss': Array(2458.0845, dtype=float32), 'eval/episode_goal_distance': (Array(0.27802044, dtype=float32), Array(0.03641076, dtype=float32)), 'eval/episode_reward': (Array(-41425.117, dtype=float32), Array(20265.254, dtype=float32)), 'eval/avg_episode_length': (Array(821.28125, dtype=float32), Array(381.85776, dtype=float32)), 'eval/epoch_eval_time': 4.123385667800903, 'eval/sps': 31042.45159494512}
I0727 00:07:12.957997 140267183036224 train.py:379] starting iteration 287 2091.2230269908905
I0727 00:07:20.131062 140267183036224 train.py:394] {'eval/walltime': 1193.9406805038452, 'training/sps': 40404.35356853749, 'training/walltime': 898.2319469451904, 'training/entropy_loss': Array(-0.00547852, dtype=float32), 'training/policy_loss': Array(0.00012352, dtype=float32), 'training/total_loss': Array(2544.8945, dtype=float32), 'training/v_loss': Array(2544.9, dtype=float32), 'eval/episode_goal_distance': (Array(0.28129315, dtype=float32), Array(0.03291896, dtype=float32)), 'eval/episode_reward': (Array(-39575.297, dtype=float32), Array(20902.174, dtype=float32)), 'eval/avg_episode_length': (Array(798.14844, dtype=float32), Array(399.8034, dtype=float32)), 'eval/epoch_eval_time': 4.128242492675781, 'eval/sps': 31005.930544800656}
I0727 00:07:20.133425 140267183036224 train.py:379] starting iteration 288 2098.3984541893005
I0727 00:07:27.292999 140267183036224 train.py:394] {'eval/walltime': 1198.0586531162262, 'training/sps': 40448.47097258594, 'training/walltime': 901.2698862552643, 'training/entropy_loss': Array(-0.00389133, dtype=float32), 'training/policy_loss': Array(0.00010901, dtype=float32), 'training/total_loss': Array(2143.771, dtype=float32), 'training/v_loss': Array(2143.775, dtype=float32), 'eval/episode_goal_distance': (Array(0.28095728, dtype=float32), Array(0.04012645, dtype=float32)), 'eval/episode_reward': (Array(-40268.14, dtype=float32), Array(20102.143, dtype=float32)), 'eval/avg_episode_length': (Array(821.22656, dtype=float32), Array(381.97446, dtype=float32)), 'eval/epoch_eval_time': 4.1179726123809814, 'eval/sps': 31083.256749974193}
I0727 00:07:27.295427 140267183036224 train.py:379] starting iteration 289 2105.560456752777
I0727 00:07:34.473795 140267183036224 train.py:394] {'eval/walltime': 1202.1881701946259, 'training/sps': 40352.16698188938, 'training/walltime': 904.3150758743286, 'training/entropy_loss': Array(-0.00413645, dtype=float32), 'training/policy_loss': Array(0.00036845, dtype=float32), 'training/total_loss': Array(2149.865, dtype=float32), 'training/v_loss': Array(2149.8687, dtype=float32), 'eval/episode_goal_distance': (Array(0.28382334, dtype=float32), Array(0.03491631, dtype=float32)), 'eval/episode_reward': (Array(-37951.555, dtype=float32), Array(22398.459, dtype=float32)), 'eval/avg_episode_length': (Array(759.21094, dtype=float32), Array(425.93423, dtype=float32)), 'eval/epoch_eval_time': 4.129517078399658, 'eval/sps': 30996.36048716979}
I0727 00:07:34.476245 140267183036224 train.py:379] starting iteration 290 2112.741274833679
I0727 00:07:41.648702 140267183036224 train.py:394] {'eval/walltime': 1206.318779706955, 'training/sps': 40446.4140590734, 'training/walltime': 907.3531696796417, 'training/entropy_loss': Array(-0.00448824, dtype=float32), 'training/policy_loss': Array(-2.943055e-05, dtype=float32), 'training/total_loss': Array(2072.9863, dtype=float32), 'training/v_loss': Array(2072.991, dtype=float32), 'eval/episode_goal_distance': (Array(0.28216818, dtype=float32), Array(0.03632857, dtype=float32)), 'eval/episode_reward': (Array(-39193.992, dtype=float32), Array(19762.877, dtype=float32)), 'eval/avg_episode_length': (Array(813.40625, dtype=float32), Array(388.42648, dtype=float32)), 'eval/epoch_eval_time': 4.130609512329102, 'eval/sps': 30988.16279242659}
I0727 00:07:41.651245 140267183036224 train.py:379] starting iteration 291 2119.9162747859955
I0727 00:07:48.800559 140267183036224 train.py:394] {'eval/walltime': 1210.4244050979614, 'training/sps': 40419.24625766219, 'training/walltime': 910.3933055400848, 'training/entropy_loss': Array(-0.00457614, dtype=float32), 'training/policy_loss': Array(-0.00027411, dtype=float32), 'training/total_loss': Array(1992.634, dtype=float32), 'training/v_loss': Array(1992.6389, dtype=float32), 'eval/episode_goal_distance': (Array(0.28877184, dtype=float32), Array(0.03685603, dtype=float32)), 'eval/episode_reward': (Array(-40083.78, dtype=float32), Array(21391.992, dtype=float32)), 'eval/avg_episode_length': (Array(797.9375, dtype=float32), Array(400.22134, dtype=float32)), 'eval/epoch_eval_time': 4.10562539100647, 'eval/sps': 31176.736260543625}
I0727 00:07:48.802999 140267183036224 train.py:379] starting iteration 292 2127.0680289268494
I0727 00:07:55.962729 140267183036224 train.py:394] {'eval/walltime': 1214.5417079925537, 'training/sps': 40435.888349652436, 'training/walltime': 913.4321901798248, 'training/entropy_loss': Array(-0.00499587, dtype=float32), 'training/policy_loss': Array(0.00018459, dtype=float32), 'training/total_loss': Array(1994.0203, dtype=float32), 'training/v_loss': Array(1994.0251, dtype=float32), 'eval/episode_goal_distance': (Array(0.28425848, dtype=float32), Array(0.03706653, dtype=float32)), 'eval/episode_reward': (Array(-38074.6, dtype=float32), Array(22211.576, dtype=float32)), 'eval/avg_episode_length': (Array(759.2031, dtype=float32), Array(425.94812, dtype=float32)), 'eval/epoch_eval_time': 4.117302894592285, 'eval/sps': 31088.31273213266}
I0727 00:07:55.965187 140267183036224 train.py:379] starting iteration 293 2134.2302174568176
I0727 00:08:03.132008 140267183036224 train.py:394] {'eval/walltime': 1218.6569738388062, 'training/sps': 40316.6433768614, 'training/walltime': 916.4800629615784, 'training/entropy_loss': Array(-0.00523533, dtype=float32), 'training/policy_loss': Array(6.52663e-05, dtype=float32), 'training/total_loss': Array(2088.463, dtype=float32), 'training/v_loss': Array(2088.468, dtype=float32), 'eval/episode_goal_distance': (Array(0.28342932, dtype=float32), Array(0.03847389, dtype=float32)), 'eval/episode_reward': (Array(-36071.742, dtype=float32), Array(22671.928, dtype=float32)), 'eval/avg_episode_length': (Array(735.78906, dtype=float32), Array(439.31512, dtype=float32)), 'eval/epoch_eval_time': 4.115265846252441, 'eval/sps': 31103.701384580767}
I0727 00:08:03.134521 140267183036224 train.py:379] starting iteration 294 2141.3995509147644
I0727 00:08:10.304366 140267183036224 train.py:394] {'eval/walltime': 1222.7771608829498, 'training/sps': 40342.12918361984, 'training/walltime': 919.5260102748871, 'training/entropy_loss': Array(-0.00517883, dtype=float32), 'training/policy_loss': Array(0.00042591, dtype=float32), 'training/total_loss': Array(1961.8698, dtype=float32), 'training/v_loss': Array(1961.8745, dtype=float32), 'eval/episode_goal_distance': (Array(0.2850404, dtype=float32), Array(0.04108403, dtype=float32)), 'eval/episode_reward': (Array(-37634.984, dtype=float32), Array(23264.03, dtype=float32)), 'eval/avg_episode_length': (Array(735.78906, dtype=float32), Array(439.31488, dtype=float32)), 'eval/epoch_eval_time': 4.120187044143677, 'eval/sps': 31066.550772721777}
I0727 00:08:10.306806 140267183036224 train.py:379] starting iteration 295 2148.571835756302
I0727 00:08:17.461265 140267183036224 train.py:394] {'eval/walltime': 1226.8818004131317, 'training/sps': 40338.48863665358, 'training/walltime': 922.5722324848175, 'training/entropy_loss': Array(-0.00506586, dtype=float32), 'training/policy_loss': Array(9.760263e-05, dtype=float32), 'training/total_loss': Array(2040.3154, dtype=float32), 'training/v_loss': Array(2040.3206, dtype=float32), 'eval/episode_goal_distance': (Array(0.2892829, dtype=float32), Array(0.03967838, dtype=float32)), 'eval/episode_reward': (Array(-38922.336, dtype=float32), Array(22826.68, dtype=float32)), 'eval/avg_episode_length': (Array(759.2422, dtype=float32), Array(425.87915, dtype=float32)), 'eval/epoch_eval_time': 4.104639530181885, 'eval/sps': 31184.224353637226}
I0727 00:08:17.463870 140267183036224 train.py:379] starting iteration 296 2155.728900194168
I0727 00:08:24.640321 140267183036224 train.py:394] {'eval/walltime': 1231.0023522377014, 'training/sps': 40258.037356524976, 'training/walltime': 925.6245422363281, 'training/entropy_loss': Array(-0.00431732, dtype=float32), 'training/policy_loss': Array(-2.991328e-08, dtype=float32), 'training/total_loss': Array(2073.669, dtype=float32), 'training/v_loss': Array(2073.673, dtype=float32), 'eval/episode_goal_distance': (Array(0.28696647, dtype=float32), Array(0.04307295, dtype=float32)), 'eval/episode_reward': (Array(-41199.47, dtype=float32), Array(21502.875, dtype=float32)), 'eval/avg_episode_length': (Array(805.71094, dtype=float32), Array(394.3642, dtype=float32)), 'eval/epoch_eval_time': 4.120551824569702, 'eval/sps': 31063.800541658442}
I0727 00:08:24.642703 140267183036224 train.py:379] starting iteration 297 2162.9077332019806
I0727 00:08:31.796215 140267183036224 train.py:394] {'eval/walltime': 1235.1062016487122, 'training/sps': 40341.25450829084, 'training/walltime': 928.6705555915833, 'training/entropy_loss': Array(-0.00387736, dtype=float32), 'training/policy_loss': Array(0.00025255, dtype=float32), 'training/total_loss': Array(1933.0345, dtype=float32), 'training/v_loss': Array(1933.0382, dtype=float32), 'eval/episode_goal_distance': (Array(0.28501847, dtype=float32), Array(0.03645708, dtype=float32)), 'eval/episode_reward': (Array(-39350.695, dtype=float32), Array(21728.3, dtype=float32)), 'eval/avg_episode_length': (Array(782.3906, dtype=float32), Array(411.244, dtype=float32)), 'eval/epoch_eval_time': 4.103849411010742, 'eval/sps': 31190.22829068056}
I0727 00:08:31.798754 140267183036224 train.py:379] starting iteration 298 2170.0637838840485
I0727 00:08:38.975390 140267183036224 train.py:394] {'eval/walltime': 1239.225715637207, 'training/sps': 40243.162652561514, 'training/walltime': 931.7239935398102, 'training/entropy_loss': Array(-0.00374601, dtype=float32), 'training/policy_loss': Array(0.00011994, dtype=float32), 'training/total_loss': Array(2255.1655, dtype=float32), 'training/v_loss': Array(2255.1692, dtype=float32), 'eval/episode_goal_distance': (Array(0.28804362, dtype=float32), Array(0.03362551, dtype=float32)), 'eval/episode_reward': (Array(-38210.28, dtype=float32), Array(23358.273, dtype=float32)), 'eval/avg_episode_length': (Array(743.78125, dtype=float32), Array(434.7272, dtype=float32)), 'eval/epoch_eval_time': 4.119513988494873, 'eval/sps': 31071.626497077814}
I0727 00:08:38.978164 140267183036224 train.py:379] starting iteration 299 2177.2431938648224
I0727 00:08:46.131051 140267183036224 train.py:394] {'eval/walltime': 1243.3314204216003, 'training/sps': 40373.1051282459, 'training/walltime': 934.7676038742065, 'training/entropy_loss': Array(-0.00455302, dtype=float32), 'training/policy_loss': Array(-0.00012963, dtype=float32), 'training/total_loss': Array(1952.117, dtype=float32), 'training/v_loss': Array(1952.1216, dtype=float32), 'eval/episode_goal_distance': (Array(0.28056204, dtype=float32), Array(0.03781762, dtype=float32)), 'eval/episode_reward': (Array(-35537.742, dtype=float32), Array(22834.758, dtype=float32)), 'eval/avg_episode_length': (Array(728.21094, dtype=float32), Array(443.0377, dtype=float32)), 'eval/epoch_eval_time': 4.1057047843933105, 'eval/sps': 31176.13338556543}
I0727 00:08:46.133603 140267183036224 train.py:379] starting iteration 300 2184.3986332416534
I0727 00:08:53.304431 140267183036224 train.py:394] {'eval/walltime': 1247.4471113681793, 'training/sps': 40267.97986179399, 'training/walltime': 937.8191599845886, 'training/entropy_loss': Array(-0.00394787, dtype=float32), 'training/policy_loss': Array(0.00018397, dtype=float32), 'training/total_loss': Array(13111.422, dtype=float32), 'training/v_loss': Array(13111.428, dtype=float32), 'eval/episode_goal_distance': (Array(0.28254944, dtype=float32), Array(0.03677649, dtype=float32)), 'eval/episode_reward': (Array(-43509.21, dtype=float32), Array(18107.145, dtype=float32)), 'eval/avg_episode_length': (Array(875.71875, dtype=float32), Array(328.8177, dtype=float32)), 'eval/epoch_eval_time': 4.1156909465789795, 'eval/sps': 31100.488754238315}
I0727 00:08:53.306943 140267183036224 train.py:379] starting iteration 301 2191.5719726085663
I0727 00:09:00.481090 140267183036224 train.py:394] {'eval/walltime': 1251.572538137436, 'training/sps': 40352.91575289922, 'training/walltime': 940.8642930984497, 'training/entropy_loss': Array(-0.00566928, dtype=float32), 'training/policy_loss': Array(0.00041169, dtype=float32), 'training/total_loss': Array(2792.108, dtype=float32), 'training/v_loss': Array(2792.1133, dtype=float32), 'eval/episode_goal_distance': (Array(0.28122348, dtype=float32), Array(0.03722577, dtype=float32)), 'eval/episode_reward': (Array(-37965.305, dtype=float32), Array(22437.322, dtype=float32)), 'eval/avg_episode_length': (Array(759.1719, dtype=float32), Array(426.00327, dtype=float32)), 'eval/epoch_eval_time': 4.125426769256592, 'eval/sps': 31027.09299165812}
I0727 00:09:00.483482 140267183036224 train.py:379] starting iteration 302 2198.7485122680664
I0727 00:09:07.651593 140267183036224 train.py:394] {'eval/walltime': 1255.6879739761353, 'training/sps': 40300.052007323815, 'training/walltime': 943.9134206771851, 'training/entropy_loss': Array(-0.00739628, dtype=float32), 'training/policy_loss': Array(0.00023408, dtype=float32), 'training/total_loss': Array(2563.3665, dtype=float32), 'training/v_loss': Array(2563.3735, dtype=float32), 'eval/episode_goal_distance': (Array(0.2801212, dtype=float32), Array(0.03797686, dtype=float32)), 'eval/episode_reward': (Array(-38982.168, dtype=float32), Array(21628.838, dtype=float32)), 'eval/avg_episode_length': (Array(782.5, dtype=float32), Array(411.03714, dtype=float32)), 'eval/epoch_eval_time': 4.115435838699341, 'eval/sps': 31102.416613170586}
I0727 00:09:07.654014 140267183036224 train.py:379] starting iteration 303 2205.919043779373
I0727 00:09:14.804831 140267183036224 train.py:394] {'eval/walltime': 1259.7923781871796, 'training/sps': 40382.53185359035, 'training/walltime': 946.9563205242157, 'training/entropy_loss': Array(-0.00721912, dtype=float32), 'training/policy_loss': Array(9.2402035e-05, dtype=float32), 'training/total_loss': Array(2356.823, dtype=float32), 'training/v_loss': Array(2356.83, dtype=float32), 'eval/episode_goal_distance': (Array(0.28572792, dtype=float32), Array(0.03610651, dtype=float32)), 'eval/episode_reward': (Array(-39960.793, dtype=float32), Array(21606.246, dtype=float32)), 'eval/avg_episode_length': (Array(790.125, dtype=float32), Array(405.9201, dtype=float32)), 'eval/epoch_eval_time': 4.1044042110443115, 'eval/sps': 31186.01224888425}
I0727 00:09:14.807258 140267183036224 train.py:379] starting iteration 304 2213.072287797928
I0727 00:09:21.966948 140267183036224 train.py:394] {'eval/walltime': 1263.905309677124, 'training/sps': 40379.8805376347, 'training/walltime': 949.9994201660156, 'training/entropy_loss': Array(-0.00563383, dtype=float32), 'training/policy_loss': Array(0.00021463, dtype=float32), 'training/total_loss': Array(2175.3203, dtype=float32), 'training/v_loss': Array(2175.3257, dtype=float32), 'eval/episode_goal_distance': (Array(0.28385144, dtype=float32), Array(0.03857146, dtype=float32)), 'eval/episode_reward': (Array(-39898.125, dtype=float32), Array(21744.104, dtype=float32)), 'eval/avg_episode_length': (Array(790.3906, dtype=float32), Array(405.40637, dtype=float32)), 'eval/epoch_eval_time': 4.112931489944458, 'eval/sps': 31121.354759480455}
I0727 00:09:21.969536 140267183036224 train.py:379] starting iteration 305 2220.2345662117004
I0727 00:09:29.130435 140267183036224 train.py:394] {'eval/walltime': 1268.0196447372437, 'training/sps': 40381.5953108102, 'training/walltime': 953.0423905849457, 'training/entropy_loss': Array(-0.00530715, dtype=float32), 'training/policy_loss': Array(0.00026621, dtype=float32), 'training/total_loss': Array(2259.7356, dtype=float32), 'training/v_loss': Array(2259.7407, dtype=float32), 'eval/episode_goal_distance': (Array(0.28999925, dtype=float32), Array(0.03909104, dtype=float32)), 'eval/episode_reward': (Array(-35730.93, dtype=float32), Array(23924.227, dtype=float32)), 'eval/avg_episode_length': (Array(704.72656, dtype=float32), Array(454.41684, dtype=float32)), 'eval/epoch_eval_time': 4.114335060119629, 'eval/sps': 31110.737975792923}
I0727 00:09:29.132887 140267183036224 train.py:379] starting iteration 306 2227.3979167938232
I0727 00:09:36.296761 140267183036224 train.py:394] {'eval/walltime': 1272.13405752182, 'training/sps': 40343.689168059675, 'training/walltime': 956.0882201194763, 'training/entropy_loss': Array(-0.0044278, dtype=float32), 'training/policy_loss': Array(0.00010916, dtype=float32), 'training/total_loss': Array(2225.9795, dtype=float32), 'training/v_loss': Array(2225.9836, dtype=float32), 'eval/episode_goal_distance': (Array(0.28577414, dtype=float32), Array(0.03848984, dtype=float32)), 'eval/episode_reward': (Array(-39352.773, dtype=float32), Array(21090.271, dtype=float32)), 'eval/avg_episode_length': (Array(798.22656, dtype=float32), Array(399.64917, dtype=float32)), 'eval/epoch_eval_time': 4.114412784576416, 'eval/sps': 31110.150269761463}
I0727 00:09:36.616140 140267183036224 train.py:379] starting iteration 307 2234.8811490535736
I0727 00:09:43.782416 140267183036224 train.py:394] {'eval/walltime': 1276.2364575862885, 'training/sps': 40161.065920520225, 'training/walltime': 959.1478998661041, 'training/entropy_loss': Array(-0.0045084, dtype=float32), 'training/policy_loss': Array(0.00013318, dtype=float32), 'training/total_loss': Array(2063.1304, dtype=float32), 'training/v_loss': Array(2063.1348, dtype=float32), 'eval/episode_goal_distance': (Array(0.28808245, dtype=float32), Array(0.03150757, dtype=float32)), 'eval/episode_reward': (Array(-37011.586, dtype=float32), Array(23023.715, dtype=float32)), 'eval/avg_episode_length': (Array(735.85156, dtype=float32), Array(439.21133, dtype=float32)), 'eval/epoch_eval_time': 4.102400064468384, 'eval/sps': 31201.24755960072}
I0727 00:09:43.784997 140267183036224 train.py:379] starting iteration 308 2242.05002784729
I0727 00:09:50.949183 140267183036224 train.py:394] {'eval/walltime': 1280.3376762866974, 'training/sps': 40165.50711481967, 'training/walltime': 962.2072412967682, 'training/entropy_loss': Array(-0.00426856, dtype=float32), 'training/policy_loss': Array(0.00015023, dtype=float32), 'training/total_loss': Array(1895.3616, dtype=float32), 'training/v_loss': Array(1895.3658, dtype=float32), 'eval/episode_goal_distance': (Array(0.28535244, dtype=float32), Array(0.03817074, dtype=float32)), 'eval/episode_reward': (Array(-37520.445, dtype=float32), Array(22210.975, dtype=float32)), 'eval/avg_episode_length': (Array(759.10156, dtype=float32), Array(426.1278, dtype=float32)), 'eval/epoch_eval_time': 4.1012187004089355, 'eval/sps': 31210.23513992}
I0727 00:09:50.951793 140267183036224 train.py:379] starting iteration 309 2249.2168231010437
I0727 00:09:58.119186 140267183036224 train.py:394] {'eval/walltime': 1284.4441149234772, 'training/sps': 40192.619998577575, 'training/walltime': 965.2645189762115, 'training/entropy_loss': Array(-0.00480639, dtype=float32), 'training/policy_loss': Array(0.00011686, dtype=float32), 'training/total_loss': Array(1970.1625, dtype=float32), 'training/v_loss': Array(1970.167, dtype=float32), 'eval/episode_goal_distance': (Array(0.28203505, dtype=float32), Array(0.03642879, dtype=float32)), 'eval/episode_reward': (Array(-37802.97, dtype=float32), Array(21467.943, dtype=float32)), 'eval/avg_episode_length': (Array(774.6875, dtype=float32), Array(416.29822, dtype=float32)), 'eval/epoch_eval_time': 4.106438636779785, 'eval/sps': 31170.561969087623}
I0727 00:09:58.121694 140267183036224 train.py:379] starting iteration 310 2256.3867242336273
I0727 00:10:05.287373 140267183036224 train.py:394] {'eval/walltime': 1288.5474781990051, 'training/sps': 40173.51877105199, 'training/walltime': 968.3232502937317, 'training/entropy_loss': Array(-0.00502136, dtype=float32), 'training/policy_loss': Array(-8.516939e-05, dtype=float32), 'training/total_loss': Array(2164.232, dtype=float32), 'training/v_loss': Array(2164.237, dtype=float32), 'eval/episode_goal_distance': (Array(0.27991998, dtype=float32), Array(0.0367902, dtype=float32)), 'eval/episode_reward': (Array(-35367.87, dtype=float32), Array(22087.543, dtype=float32)), 'eval/avg_episode_length': (Array(736.0625, dtype=float32), Array(438.86084, dtype=float32)), 'eval/epoch_eval_time': 4.103363275527954, 'eval/sps': 31193.92347330765}
I0727 00:10:05.289807 140267183036224 train.py:379] starting iteration 311 2263.554836511612
I0727 00:10:12.451335 140267183036224 train.py:394] {'eval/walltime': 1292.6495690345764, 'training/sps': 40212.671216855306, 'training/walltime': 971.3790035247803, 'training/entropy_loss': Array(-0.004565, dtype=float32), 'training/policy_loss': Array(0.00048188, dtype=float32), 'training/total_loss': Array(1971.3601, dtype=float32), 'training/v_loss': Array(1971.3641, dtype=float32), 'eval/episode_goal_distance': (Array(0.2872572, dtype=float32), Array(0.04016048, dtype=float32)), 'eval/episode_reward': (Array(-39441.86, dtype=float32), Array(21843.434, dtype=float32)), 'eval/avg_episode_length': (Array(782.5156, dtype=float32), Array(411.00778, dtype=float32)), 'eval/epoch_eval_time': 4.102090835571289, 'eval/sps': 31203.599610727226}
I0727 00:10:12.453930 140267183036224 train.py:379] starting iteration 312 2270.718960046768
I0727 00:10:19.615928 140267183036224 train.py:394] {'eval/walltime': 1296.753676891327, 'training/sps': 40231.803421883786, 'training/walltime': 974.4333035945892, 'training/entropy_loss': Array(-0.00342676, dtype=float32), 'training/policy_loss': Array(0.00022343, dtype=float32), 'training/total_loss': Array(1992.9312, dtype=float32), 'training/v_loss': Array(1992.9343, dtype=float32), 'eval/episode_goal_distance': (Array(0.2852931, dtype=float32), Array(0.03906394, dtype=float32)), 'eval/episode_reward': (Array(-34666.39, dtype=float32), Array(23600.373, dtype=float32)), 'eval/avg_episode_length': (Array(697.1094, dtype=float32), Array(457.56143, dtype=float32)), 'eval/epoch_eval_time': 4.104107856750488, 'eval/sps': 31188.264165490677}
I0727 00:10:19.618517 140267183036224 train.py:379] starting iteration 313 2277.883546590805
I0727 00:10:26.778596 140267183036224 train.py:394] {'eval/walltime': 1300.8549406528473, 'training/sps': 40219.80089720337, 'training/walltime': 977.4885151386261, 'training/entropy_loss': Array(-0.00357907, dtype=float32), 'training/policy_loss': Array(0.00025565, dtype=float32), 'training/total_loss': Array(1979.0571, dtype=float32), 'training/v_loss': Array(1979.0605, dtype=float32), 'eval/episode_goal_distance': (Array(0.28644562, dtype=float32), Array(0.03502808, dtype=float32)), 'eval/episode_reward': (Array(-40624.3, dtype=float32), Array(21457.111, dtype=float32)), 'eval/avg_episode_length': (Array(798.10156, dtype=float32), Array(399.8962, dtype=float32)), 'eval/epoch_eval_time': 4.101263761520386, 'eval/sps': 31209.892229059886}
I0727 00:10:26.781096 140267183036224 train.py:379] starting iteration 314 2285.046125650406
I0727 00:10:33.945065 140267183036224 train.py:394] {'eval/walltime': 1304.9588022232056, 'training/sps': 40202.93788832812, 'training/walltime': 980.5450081825256, 'training/entropy_loss': Array(-0.00382945, dtype=float32), 'training/policy_loss': Array(0.00053923, dtype=float32), 'training/total_loss': Array(1898.687, dtype=float32), 'training/v_loss': Array(1898.6904, dtype=float32), 'eval/episode_goal_distance': (Array(0.28377092, dtype=float32), Array(0.03832766, dtype=float32)), 'eval/episode_reward': (Array(-36813.977, dtype=float32), Array(22835.434, dtype=float32)), 'eval/avg_episode_length': (Array(735.90625, dtype=float32), Array(439.12033, dtype=float32)), 'eval/epoch_eval_time': 4.103861570358276, 'eval/sps': 31190.13587703089}
I0727 00:10:33.947512 140267183036224 train.py:379] starting iteration 315 2292.212542295456
I0727 00:10:41.111429 140267183036224 train.py:394] {'eval/walltime': 1309.058609008789, 'training/sps': 40148.9710731238, 'training/walltime': 983.6056096553802, 'training/entropy_loss': Array(-0.00359784, dtype=float32), 'training/policy_loss': Array(-6.501652e-05, dtype=float32), 'training/total_loss': Array(1864.4731, dtype=float32), 'training/v_loss': Array(1864.4767, dtype=float32), 'eval/episode_goal_distance': (Array(0.2922725, dtype=float32), Array(0.03820527, dtype=float32)), 'eval/episode_reward': (Array(-38005.97, dtype=float32), Array(24109.934, dtype=float32)), 'eval/avg_episode_length': (Array(728.16406, dtype=float32), Array(443.11392, dtype=float32)), 'eval/epoch_eval_time': 4.099806785583496, 'eval/sps': 31220.983498563255}
I0727 00:10:41.113864 140267183036224 train.py:379] starting iteration 316 2299.378894805908
I0727 00:10:48.273740 140267183036224 train.py:394] {'eval/walltime': 1313.1553463935852, 'training/sps': 40163.63850112275, 'training/walltime': 986.665093421936, 'training/entropy_loss': Array(-0.00283119, dtype=float32), 'training/policy_loss': Array(-6.0890074e-05, dtype=float32), 'training/total_loss': Array(12575.699, dtype=float32), 'training/v_loss': Array(12575.703, dtype=float32), 'eval/episode_goal_distance': (Array(0.28615355, dtype=float32), Array(0.03823844, dtype=float32)), 'eval/episode_reward': (Array(-41868.19, dtype=float32), Array(19747.227, dtype=float32)), 'eval/avg_episode_length': (Array(836.8906, dtype=float32), Array(368.1815, dtype=float32)), 'eval/epoch_eval_time': 4.096737384796143, 'eval/sps': 31244.37521307444}
I0727 00:10:48.276182 140267183036224 train.py:379] starting iteration 317 2306.5412125587463
I0727 00:10:55.442816 140267183036224 train.py:394] {'eval/walltime': 1317.2605669498444, 'training/sps': 40186.43055308536, 'training/walltime': 989.7228419780731, 'training/entropy_loss': Array(-0.0054048, dtype=float32), 'training/policy_loss': Array(0.00027752, dtype=float32), 'training/total_loss': Array(3046.4426, dtype=float32), 'training/v_loss': Array(3046.4478, dtype=float32), 'eval/episode_goal_distance': (Array(0.28643873, dtype=float32), Array(0.03760383, dtype=float32)), 'eval/episode_reward': (Array(-41035.508, dtype=float32), Array(20785.176, dtype=float32)), 'eval/avg_episode_length': (Array(813.5781, dtype=float32), Array(388.06876, dtype=float32)), 'eval/epoch_eval_time': 4.105220556259155, 'eval/sps': 31179.810742407182}
I0727 00:10:55.445376 140267183036224 train.py:379] starting iteration 318 2313.71040558815
I0727 00:11:02.606458 140267183036224 train.py:394] {'eval/walltime': 1321.361911058426, 'training/sps': 40205.829477775034, 'training/walltime': 992.7791152000427, 'training/entropy_loss': Array(-0.00556257, dtype=float32), 'training/policy_loss': Array(0.00020194, dtype=float32), 'training/total_loss': Array(2783.4297, dtype=float32), 'training/v_loss': Array(2783.435, dtype=float32), 'eval/episode_goal_distance': (Array(0.287803, dtype=float32), Array(0.03889549, dtype=float32)), 'eval/episode_reward': (Array(-39488.164, dtype=float32), Array(21505.49, dtype=float32)), 'eval/avg_episode_length': (Array(790.3203, dtype=float32), Array(405.5423, dtype=float32)), 'eval/epoch_eval_time': 4.101344108581543, 'eval/sps': 31209.28081410585}
I0727 00:11:02.608892 140267183036224 train.py:379] starting iteration 319 2320.873922109604
I0727 00:11:09.774551 140267183036224 train.py:394] {'eval/walltime': 1325.4677925109863, 'training/sps': 40208.47367034855, 'training/walltime': 995.8351874351501, 'training/entropy_loss': Array(-0.00548939, dtype=float32), 'training/policy_loss': Array(-4.289279e-06, dtype=float32), 'training/total_loss': Array(2523.235, dtype=float32), 'training/v_loss': Array(2523.2407, dtype=float32), 'eval/episode_goal_distance': (Array(0.28821307, dtype=float32), Array(0.03523648, dtype=float32)), 'eval/episode_reward': (Array(-41239.117, dtype=float32), Array(20568.54, dtype=float32)), 'eval/avg_episode_length': (Array(813.5156, dtype=float32), Array(388.19897, dtype=float32)), 'eval/epoch_eval_time': 4.105881452560425, 'eval/sps': 31174.791936620404}
I0727 00:11:09.777068 140267183036224 train.py:379] starting iteration 320 2328.0420973300934
I0727 00:11:16.939762 140267183036224 train.py:394] {'eval/walltime': 1329.5669264793396, 'training/sps': 40157.73645151103, 'training/walltime': 998.8951208591461, 'training/entropy_loss': Array(-0.00460763, dtype=float32), 'training/policy_loss': Array(0.00017473, dtype=float32), 'training/total_loss': Array(2038.5927, dtype=float32), 'training/v_loss': Array(2038.597, dtype=float32), 'eval/episode_goal_distance': (Array(0.28443122, dtype=float32), Array(0.03874844, dtype=float32)), 'eval/episode_reward': (Array(-41804.477, dtype=float32), Array(19279.516, dtype=float32)), 'eval/avg_episode_length': (Array(844.6172, dtype=float32), Array(361.07785, dtype=float32)), 'eval/epoch_eval_time': 4.0991339683532715, 'eval/sps': 31226.10799944675}
I0727 00:11:16.942357 140267183036224 train.py:379] starting iteration 321 2335.207381248474
I0727 00:11:24.112260 140267183036224 train.py:394] {'eval/walltime': 1333.675255537033, 'training/sps': 40183.40703621961, 'training/walltime': 1001.953099489212, 'training/entropy_loss': Array(-0.00393021, dtype=float32), 'training/policy_loss': Array(0.00034779, dtype=float32), 'training/total_loss': Array(2213.481, dtype=float32), 'training/v_loss': Array(2213.4849, dtype=float32), 'eval/episode_goal_distance': (Array(0.28031173, dtype=float32), Array(0.03611585, dtype=float32)), 'eval/episode_reward': (Array(-36946.727, dtype=float32), Array(21786.932, dtype=float32)), 'eval/avg_episode_length': (Array(759.22656, dtype=float32), Array(425.9069, dtype=float32)), 'eval/epoch_eval_time': 4.1083290576934814, 'eval/sps': 31156.2190375915}
I0727 00:11:24.114779 140267183036224 train.py:379] starting iteration 322 2342.3798089027405
I0727 00:11:31.277404 140267183036224 train.py:394] {'eval/walltime': 1337.7751626968384, 'training/sps': 40169.4295780658, 'training/walltime': 1005.0121421813965, 'training/entropy_loss': Array(-0.00301159, dtype=float32), 'training/policy_loss': Array(-6.456608e-05, dtype=float32), 'training/total_loss': Array(2038.7495, dtype=float32), 'training/v_loss': Array(2038.7527, dtype=float32), 'eval/episode_goal_distance': (Array(0.28114915, dtype=float32), Array(0.04129596, dtype=float32)), 'eval/episode_reward': (Array(-37456.457, dtype=float32), Array(21701.096, dtype=float32)), 'eval/avg_episode_length': (Array(767.03125, dtype=float32), Array(421.06738, dtype=float32)), 'eval/epoch_eval_time': 4.099907159805298, 'eval/sps': 31220.219144200975}
I0727 00:11:31.279933 140267183036224 train.py:379] starting iteration 323 2349.544962644577
I0727 00:11:38.442334 140267183036224 train.py:394] {'eval/walltime': 1341.8769526481628, 'training/sps': 40197.516495224074, 'training/walltime': 1008.0690474510193, 'training/entropy_loss': Array(-0.00227747, dtype=float32), 'training/policy_loss': Array(0.00025416, dtype=float32), 'training/total_loss': Array(2136.285, dtype=float32), 'training/v_loss': Array(2136.287, dtype=float32), 'eval/episode_goal_distance': (Array(0.29149702, dtype=float32), Array(0.03362046, dtype=float32)), 'eval/episode_reward': (Array(-36330.656, dtype=float32), Array(23768.23, dtype=float32)), 'eval/avg_episode_length': (Array(712.60156, dtype=float32), Array(450.71838, dtype=float32)), 'eval/epoch_eval_time': 4.101789951324463, 'eval/sps': 31205.888531339584}
I0727 00:11:38.445038 140267183036224 train.py:379] starting iteration 324 2356.710068464279
I0727 00:11:45.612924 140267183036224 train.py:394] {'eval/walltime': 1345.9813108444214, 'training/sps': 40158.41231409353, 'training/walltime': 1011.1289293766022, 'training/entropy_loss': Array(-0.00152533, dtype=float32), 'training/policy_loss': Array(0.00029714, dtype=float32), 'training/total_loss': Array(1899.5077, dtype=float32), 'training/v_loss': Array(1899.5089, dtype=float32), 'eval/episode_goal_distance': (Array(0.27949038, dtype=float32), Array(0.03954212, dtype=float32)), 'eval/episode_reward': (Array(-37809.28, dtype=float32), Array(21986.031, dtype=float32)), 'eval/avg_episode_length': (Array(766.8672, dtype=float32), Array(421.36322, dtype=float32)), 'eval/epoch_eval_time': 4.104358196258545, 'eval/sps': 31186.36188154396}
I0727 00:11:45.615593 140267183036224 train.py:379] starting iteration 325 2363.880623102188
I0727 00:11:52.781114 140267183036224 train.py:394] {'eval/walltime': 1350.0859875679016, 'training/sps': 40193.46629876186, 'training/walltime': 1014.1861426830292, 'training/entropy_loss': Array(-0.00172409, dtype=float32), 'training/policy_loss': Array(0.00039863, dtype=float32), 'training/total_loss': Array(1903.1837, dtype=float32), 'training/v_loss': Array(1903.185, dtype=float32), 'eval/episode_goal_distance': (Array(0.28729966, dtype=float32), Array(0.03836664, dtype=float32)), 'eval/episode_reward': (Array(-36161.47, dtype=float32), Array(24260.764, dtype=float32)), 'eval/avg_episode_length': (Array(704.75, dtype=float32), Array(454.38107, dtype=float32)), 'eval/epoch_eval_time': 4.104676723480225, 'eval/sps': 31183.94178713126}
I0727 00:11:52.783529 140267183036224 train.py:379] starting iteration 326 2371.048558950424
I0727 00:11:59.949722 140267183036224 train.py:394] {'eval/walltime': 1354.1902527809143, 'training/sps': 40178.889841180695, 'training/walltime': 1017.2444651126862, 'training/entropy_loss': Array(-0.0022928, dtype=float32), 'training/policy_loss': Array(0.00020252, dtype=float32), 'training/total_loss': Array(2000.7656, dtype=float32), 'training/v_loss': Array(2000.7678, dtype=float32), 'eval/episode_goal_distance': (Array(0.29046255, dtype=float32), Array(0.03738746, dtype=float32)), 'eval/episode_reward': (Array(-37951.836, dtype=float32), Array(23036.865, dtype=float32)), 'eval/avg_episode_length': (Array(743.66406, dtype=float32), Array(434.92633, dtype=float32)), 'eval/epoch_eval_time': 4.104265213012695, 'eval/sps': 31187.068417063347}
I0727 00:11:59.952329 140267183036224 train.py:379] starting iteration 327 2378.217358112335
I0727 00:12:07.112006 140267183036224 train.py:394] {'eval/walltime': 1358.2901203632355, 'training/sps': 40206.63869551009, 'training/walltime': 1020.3006768226624, 'training/entropy_loss': Array(-0.00217484, dtype=float32), 'training/policy_loss': Array(0.00033528, dtype=float32), 'training/total_loss': Array(2028.6609, dtype=float32), 'training/v_loss': Array(2028.6626, dtype=float32), 'eval/episode_goal_distance': (Array(0.28877482, dtype=float32), Array(0.03930451, dtype=float32)), 'eval/episode_reward': (Array(-37816.83, dtype=float32), Array(23436.377, dtype=float32)), 'eval/avg_episode_length': (Array(735.9375, dtype=float32), Array(439.0685, dtype=float32)), 'eval/epoch_eval_time': 4.099867582321167, 'eval/sps': 31220.520524112137}
I0727 00:12:07.114650 140267183036224 train.py:379] starting iteration 328 2385.3796803951263
I0727 00:12:14.277087 140267183036224 train.py:394] {'eval/walltime': 1362.3946006298065, 'training/sps': 40232.34359401213, 'training/walltime': 1023.3549358844757, 'training/entropy_loss': Array(-0.00273553, dtype=float32), 'training/policy_loss': Array(0.00049906, dtype=float32), 'training/total_loss': Array(1942.109, dtype=float32), 'training/v_loss': Array(1942.1113, dtype=float32), 'eval/episode_goal_distance': (Array(0.2845217, dtype=float32), Array(0.03827032, dtype=float32)), 'eval/episode_reward': (Array(-38410.535, dtype=float32), Array(21611.824, dtype=float32)), 'eval/avg_episode_length': (Array(774.6797, dtype=float32), Array(416.31302, dtype=float32)), 'eval/epoch_eval_time': 4.104480266571045, 'eval/sps': 31185.43437582012}
I0727 00:12:14.279492 140267183036224 train.py:379] starting iteration 329 2392.5445222854614
I0727 00:12:21.426001 140267183036224 train.py:394] {'eval/walltime': 1366.4917759895325, 'training/sps': 40344.671325413176, 'training/walltime': 1026.4006912708282, 'training/entropy_loss': Array(-0.00292913, dtype=float32), 'training/policy_loss': Array(0.00020355, dtype=float32), 'training/total_loss': Array(2060.8027, dtype=float32), 'training/v_loss': Array(2060.8057, dtype=float32), 'eval/episode_goal_distance': (Array(0.28789178, dtype=float32), Array(0.04032508, dtype=float32)), 'eval/episode_reward': (Array(-40007.47, dtype=float32), Array(21540.416, dtype=float32)), 'eval/avg_episode_length': (Array(790.3672, dtype=float32), Array(405.45175, dtype=float32)), 'eval/epoch_eval_time': 4.097175359725952, 'eval/sps': 31241.035289385694}
I0727 00:12:21.428684 140267183036224 train.py:379] starting iteration 330 2399.693713903427
I0727 00:12:28.580084 140267183036224 train.py:394] {'eval/walltime': 1370.5916576385498, 'training/sps': 40316.02209716564, 'training/walltime': 1029.4486110210419, 'training/entropy_loss': Array(-0.00270002, dtype=float32), 'training/policy_loss': Array(-3.0906613e-05, dtype=float32), 'training/total_loss': Array(2130.747, dtype=float32), 'training/v_loss': Array(2130.75, dtype=float32), 'eval/episode_goal_distance': (Array(0.29011208, dtype=float32), Array(0.0378428, dtype=float32)), 'eval/episode_reward': (Array(-43344.1, dtype=float32), Array(19412.844, dtype=float32)), 'eval/avg_episode_length': (Array(852.53125, dtype=float32), Array(353.2134, dtype=float32)), 'eval/epoch_eval_time': 4.099881649017334, 'eval/sps': 31220.413406489242}
I0727 00:12:28.582708 140267183036224 train.py:379] starting iteration 331 2406.8477370738983
I0727 00:12:35.724236 140267183036224 train.py:394] {'eval/walltime': 1374.6906986236572, 'training/sps': 40433.883469507804, 'training/walltime': 1032.4876463413239, 'training/entropy_loss': Array(-0.00314955, dtype=float32), 'training/policy_loss': Array(0.00037144, dtype=float32), 'training/total_loss': Array(2161.5898, dtype=float32), 'training/v_loss': Array(2161.5925, dtype=float32), 'eval/episode_goal_distance': (Array(0.2830106, dtype=float32), Array(0.03680509, dtype=float32)), 'eval/episode_reward': (Array(-39610.875, dtype=float32), Array(20918.656, dtype=float32)), 'eval/avg_episode_length': (Array(798.0625, dtype=float32), Array(399.9734, dtype=float32)), 'eval/epoch_eval_time': 4.099040985107422, 'eval/sps': 31226.81633705245}
I0727 00:12:35.726693 140267183036224 train.py:379] starting iteration 332 2413.9917228221893
I0727 00:12:42.898613 140267183036224 train.py:394] {'eval/walltime': 1378.8130712509155, 'training/sps': 40339.7041842078, 'training/walltime': 1035.5337767601013, 'training/entropy_loss': Array(-0.00297179, dtype=float32), 'training/policy_loss': Array(0.00010738, dtype=float32), 'training/total_loss': Array(1861.6084, dtype=float32), 'training/v_loss': Array(1861.6113, dtype=float32), 'eval/episode_goal_distance': (Array(0.28116727, dtype=float32), Array(0.03536668, dtype=float32)), 'eval/episode_reward': (Array(-37990.56, dtype=float32), Array(21926.549, dtype=float32)), 'eval/avg_episode_length': (Array(766.89844, dtype=float32), Array(421.3066, dtype=float32)), 'eval/epoch_eval_time': 4.122372627258301, 'eval/sps': 31050.080032461785}
I0727 00:12:42.901066 140267183036224 train.py:379] starting iteration 333 2421.166095972061
I0727 00:12:50.049873 140267183036224 train.py:394] {'eval/walltime': 1382.9174139499664, 'training/sps': 40410.169916884886, 'training/walltime': 1038.574595451355, 'training/entropy_loss': Array(-0.00212568, dtype=float32), 'training/policy_loss': Array(0.00029554, dtype=float32), 'training/total_loss': Array(13427.658, dtype=float32), 'training/v_loss': Array(13427.659, dtype=float32), 'eval/episode_goal_distance': (Array(0.2845395, dtype=float32), Array(0.0356217, dtype=float32)), 'eval/episode_reward': (Array(-36709.54, dtype=float32), Array(22639.523, dtype=float32)), 'eval/avg_episode_length': (Array(735.7969, dtype=float32), Array(439.302, dtype=float32)), 'eval/epoch_eval_time': 4.104342699050903, 'eval/sps': 31186.47963524074}
I0727 00:12:50.052381 140267183036224 train.py:379] starting iteration 334 2428.3174107074738
I0727 00:12:57.222161 140267183036224 train.py:394] {'eval/walltime': 1387.0382056236267, 'training/sps': 40348.676243326445, 'training/walltime': 1041.6200485229492, 'training/entropy_loss': Array(-0.00288857, dtype=float32), 'training/policy_loss': Array(0.00026256, dtype=float32), 'training/total_loss': Array(2573.534, dtype=float32), 'training/v_loss': Array(2573.5364, dtype=float32), 'eval/episode_goal_distance': (Array(0.2795565, dtype=float32), Array(0.03926326, dtype=float32)), 'eval/episode_reward': (Array(-40998.25, dtype=float32), Array(20528.127, dtype=float32)), 'eval/avg_episode_length': (Array(821.52344, dtype=float32), Array(381.3403, dtype=float32)), 'eval/epoch_eval_time': 4.120791673660278, 'eval/sps': 31061.9924851247}
I0727 00:12:57.224609 140267183036224 train.py:379] starting iteration 335 2435.4896392822266
I0727 00:13:04.405661 140267183036224 train.py:394] {'eval/walltime': 1391.1706228256226, 'training/sps': 40352.29335479417, 'training/walltime': 1044.6652286052704, 'training/entropy_loss': Array(-0.00318865, dtype=float32), 'training/policy_loss': Array(-3.727254e-05, dtype=float32), 'training/total_loss': Array(2544.2664, dtype=float32), 'training/v_loss': Array(2544.2695, dtype=float32), 'eval/episode_goal_distance': (Array(0.28402635, dtype=float32), Array(0.03722625, dtype=float32)), 'eval/episode_reward': (Array(-41959.82, dtype=float32), Array(20405.945, dtype=float32)), 'eval/avg_episode_length': (Array(829.09375, dtype=float32), Array(375.14615, dtype=float32)), 'eval/epoch_eval_time': 4.13241720199585, 'eval/sps': 30974.60729235648}
I0727 00:13:04.408081 140267183036224 train.py:379] starting iteration 336 2442.673111438751
I0727 00:13:11.574894 140267183036224 train.py:394] {'eval/walltime': 1395.2890539169312, 'training/sps': 40356.220787323444, 'training/walltime': 1047.7101123332977, 'training/entropy_loss': Array(-0.00316822, dtype=float32), 'training/policy_loss': Array(0.00014745, dtype=float32), 'training/total_loss': Array(2287.6304, dtype=float32), 'training/v_loss': Array(2287.6333, dtype=float32), 'eval/episode_goal_distance': (Array(0.29085338, dtype=float32), Array(0.03749334, dtype=float32)), 'eval/episode_reward': (Array(-37221.848, dtype=float32), Array(22757.82, dtype=float32)), 'eval/avg_episode_length': (Array(743.6875, dtype=float32), Array(434.8861, dtype=float32)), 'eval/epoch_eval_time': 4.118431091308594, 'eval/sps': 31079.796447275065}
I0727 00:13:11.577313 140267183036224 train.py:379] starting iteration 337 2449.842342853546
I0727 00:13:18.725301 140267183036224 train.py:394] {'eval/walltime': 1399.3934614658356, 'training/sps': 40421.785447152346, 'training/walltime': 1050.750057220459, 'training/entropy_loss': Array(-0.00289639, dtype=float32), 'training/policy_loss': Array(0.00030946, dtype=float32), 'training/total_loss': Array(2029.241, dtype=float32), 'training/v_loss': Array(2029.2434, dtype=float32), 'eval/episode_goal_distance': (Array(0.27861392, dtype=float32), Array(0.03706324, dtype=float32)), 'eval/episode_reward': (Array(-36362.273, dtype=float32), Array(22009.555, dtype=float32)), 'eval/avg_episode_length': (Array(751.46875, dtype=float32), Array(430.46967, dtype=float32)), 'eval/epoch_eval_time': 4.104407548904419, 'eval/sps': 31185.98688723462}
I0727 00:13:18.727909 140267183036224 train.py:379] starting iteration 338 2456.9929382801056
I0727 00:13:25.890213 140267183036224 train.py:394] {'eval/walltime': 1403.507210969925, 'training/sps': 40355.67096394412, 'training/walltime': 1053.794982433319, 'training/entropy_loss': Array(-0.00299075, dtype=float32), 'training/policy_loss': Array(-0.00017746, dtype=float32), 'training/total_loss': Array(2158.5933, dtype=float32), 'training/v_loss': Array(2158.5967, dtype=float32), 'eval/episode_goal_distance': (Array(0.28911003, dtype=float32), Array(0.03973961, dtype=float32)), 'eval/episode_reward': (Array(-41370.383, dtype=float32), Array(20885.373, dtype=float32)), 'eval/avg_episode_length': (Array(813.64844, dtype=float32), Array(387.9229, dtype=float32)), 'eval/epoch_eval_time': 4.1137495040893555, 'eval/sps': 31115.16631548883}
I0727 00:13:25.892816 140267183036224 train.py:379] starting iteration 339 2464.15784573555
I0727 00:13:33.069641 140267183036224 train.py:394] {'eval/walltime': 1407.6358518600464, 'training/sps': 40359.722312972684, 'training/walltime': 1056.8396019935608, 'training/entropy_loss': Array(-0.00266928, dtype=float32), 'training/policy_loss': Array(-0.00011213, dtype=float32), 'training/total_loss': Array(2205.8586, dtype=float32), 'training/v_loss': Array(2205.8616, dtype=float32), 'eval/episode_goal_distance': (Array(0.28374863, dtype=float32), Array(0.03583013, dtype=float32)), 'eval/episode_reward': (Array(-39807.324, dtype=float32), Array(21398.98, dtype=float32)), 'eval/avg_episode_length': (Array(790.21875, dtype=float32), Array(405.73868, dtype=float32)), 'eval/epoch_eval_time': 4.12864089012146, 'eval/sps': 31002.938595668074}
I0727 00:13:33.072262 140267183036224 train.py:379] starting iteration 340 2471.3372914791107
I0727 00:13:40.251506 140267183036224 train.py:394] {'eval/walltime': 1411.7667436599731, 'training/sps': 40355.402377510356, 'training/walltime': 1059.8845474720001, 'training/entropy_loss': Array(-0.00312582, dtype=float32), 'training/policy_loss': Array(0.00010503, dtype=float32), 'training/total_loss': Array(1952.873, dtype=float32), 'training/v_loss': Array(1952.876, dtype=float32), 'eval/episode_goal_distance': (Array(0.27913922, dtype=float32), Array(0.03750294, dtype=float32)), 'eval/episode_reward': (Array(-41111.58, dtype=float32), Array(19837.938, dtype=float32)), 'eval/avg_episode_length': (Array(829.25, dtype=float32), Array(374.80298, dtype=float32)), 'eval/epoch_eval_time': 4.130891799926758, 'eval/sps': 30986.045193018486}
I0727 00:13:40.253946 140267183036224 train.py:379] starting iteration 341 2478.518975496292
I0727 00:13:47.401600 140267183036224 train.py:394] {'eval/walltime': 1415.86843252182, 'training/sps': 40389.155336745775, 'training/walltime': 1062.9269483089447, 'training/entropy_loss': Array(-0.00229741, dtype=float32), 'training/policy_loss': Array(0.00028087, dtype=float32), 'training/total_loss': Array(1991.6748, dtype=float32), 'training/v_loss': Array(1991.6769, dtype=float32), 'eval/episode_goal_distance': (Array(0.27914086, dtype=float32), Array(0.03951048, dtype=float32)), 'eval/episode_reward': (Array(-41163.25, dtype=float32), Array(19572.41, dtype=float32)), 'eval/avg_episode_length': (Array(836.8594, dtype=float32), Array(368.25208, dtype=float32)), 'eval/epoch_eval_time': 4.101688861846924, 'eval/sps': 31206.65762599157}
I0727 00:13:47.404099 140267183036224 train.py:379] starting iteration 342 2485.6691296100616
I0727 00:13:54.577451 140267183036224 train.py:394] {'eval/walltime': 1419.9933705329895, 'training/sps': 40356.359825493826, 'training/walltime': 1065.9718215465546, 'training/entropy_loss': Array(-0.00193841, dtype=float32), 'training/policy_loss': Array(0.00035833, dtype=float32), 'training/total_loss': Array(2050.932, dtype=float32), 'training/v_loss': Array(2050.933, dtype=float32), 'eval/episode_goal_distance': (Array(0.2830438, dtype=float32), Array(0.03898902, dtype=float32)), 'eval/episode_reward': (Array(-38485.65, dtype=float32), Array(22442.102, dtype=float32)), 'eval/avg_episode_length': (Array(766.90625, dtype=float32), Array(421.2925, dtype=float32)), 'eval/epoch_eval_time': 4.124938011169434, 'eval/sps': 31030.769348146296}
I0727 00:13:54.580110 140267183036224 train.py:379] starting iteration 343 2492.8451397418976
I0727 00:14:01.729613 140267183036224 train.py:394] {'eval/walltime': 1424.0951237678528, 'training/sps': 40364.46993622929, 'training/walltime': 1069.0160830020905, 'training/entropy_loss': Array(-0.00297692, dtype=float32), 'training/policy_loss': Array(0.00053335, dtype=float32), 'training/total_loss': Array(2181.402, dtype=float32), 'training/v_loss': Array(2181.4043, dtype=float32), 'eval/episode_goal_distance': (Array(0.28245652, dtype=float32), Array(0.03476243, dtype=float32)), 'eval/episode_reward': (Array(-38686.938, dtype=float32), Array(21359.297, dtype=float32)), 'eval/avg_episode_length': (Array(782.5, dtype=float32), Array(411.03735, dtype=float32)), 'eval/epoch_eval_time': 4.101753234863281, 'eval/sps': 31206.16786793769}
I0727 00:14:01.732081 140267183036224 train.py:379] starting iteration 344 2499.9971108436584
I0727 00:14:08.897186 140267183036224 train.py:394] {'eval/walltime': 1428.2112126350403, 'training/sps': 40348.71730732812, 'training/walltime': 1072.0615329742432, 'training/entropy_loss': Array(-0.0034021, dtype=float32), 'training/policy_loss': Array(0.0002262, dtype=float32), 'training/total_loss': Array(1979.249, dtype=float32), 'training/v_loss': Array(1979.2523, dtype=float32), 'eval/episode_goal_distance': (Array(0.2826932, dtype=float32), Array(0.03920688, dtype=float32)), 'eval/episode_reward': (Array(-40755.83, dtype=float32), Array(19422.045, dtype=float32)), 'eval/avg_episode_length': (Array(836.8281, dtype=float32), Array(368.32294, dtype=float32)), 'eval/epoch_eval_time': 4.1160888671875, 'eval/sps': 31097.48213173582}
I0727 00:14:08.899828 140267183036224 train.py:379] starting iteration 345 2507.1648581027985
I0727 00:14:16.078888 140267183036224 train.py:394] {'eval/walltime': 1432.3374156951904, 'training/sps': 40298.97749127806, 'training/walltime': 1075.110741853714, 'training/entropy_loss': Array(-0.00342839, dtype=float32), 'training/policy_loss': Array(0.00034536, dtype=float32), 'training/total_loss': Array(2060.7366, dtype=float32), 'training/v_loss': Array(2060.7395, dtype=float32), 'eval/episode_goal_distance': (Array(0.28335848, dtype=float32), Array(0.03317108, dtype=float32)), 'eval/episode_reward': (Array(-39852.51, dtype=float32), Array(21395.5, dtype=float32)), 'eval/avg_episode_length': (Array(790.3125, dtype=float32), Array(405.55756, dtype=float32)), 'eval/epoch_eval_time': 4.1262030601501465, 'eval/sps': 31021.255651761905}
I0727 00:14:16.081394 140267183036224 train.py:379] starting iteration 346 2514.3464238643646
I0727 00:14:23.247503 140267183036224 train.py:394] {'eval/walltime': 1436.4530081748962, 'training/sps': 40328.30932878206, 'training/walltime': 1078.157732963562, 'training/entropy_loss': Array(-0.00278358, dtype=float32), 'training/policy_loss': Array(-0.00026846, dtype=float32), 'training/total_loss': Array(1907.1843, dtype=float32), 'training/v_loss': Array(1907.1873, dtype=float32), 'eval/episode_goal_distance': (Array(0.2846898, dtype=float32), Array(0.03490867, dtype=float32)), 'eval/episode_reward': (Array(-38805.023, dtype=float32), Array(21929.15, dtype=float32)), 'eval/avg_episode_length': (Array(774.77344, dtype=float32), Array(416.13962, dtype=float32)), 'eval/epoch_eval_time': 4.1155924797058105, 'eval/sps': 31101.232843430033}
I0727 00:14:23.249897 140267183036224 train.py:379] starting iteration 347 2521.5149269104004
I0727 00:14:30.399123 140267183036224 train.py:394] {'eval/walltime': 1440.5548043251038, 'training/sps': 40370.28745524104, 'training/walltime': 1081.2015557289124, 'training/entropy_loss': Array(-0.00348387, dtype=float32), 'training/policy_loss': Array(0.00031646, dtype=float32), 'training/total_loss': Array(2069.3467, dtype=float32), 'training/v_loss': Array(2069.3496, dtype=float32), 'eval/episode_goal_distance': (Array(0.29000655, dtype=float32), Array(0.03818141, dtype=float32)), 'eval/episode_reward': (Array(-41600.164, dtype=float32), Array(21417.662, dtype=float32)), 'eval/avg_episode_length': (Array(813.46094, dtype=float32), Array(388.31287, dtype=float32)), 'eval/epoch_eval_time': 4.1017961502075195, 'eval/sps': 31205.84137110865}
I0727 00:14:30.401604 140267183036224 train.py:379] starting iteration 348 2528.6666345596313
I0727 00:14:37.573674 140267183036224 train.py:394] {'eval/walltime': 1444.672157049179, 'training/sps': 40273.04579066108, 'training/walltime': 1084.252727985382, 'training/entropy_loss': Array(-0.00312164, dtype=float32), 'training/policy_loss': Array(5.260852e-05, dtype=float32), 'training/total_loss': Array(1952.8479, dtype=float32), 'training/v_loss': Array(1952.8508, dtype=float32), 'eval/episode_goal_distance': (Array(0.28325865, dtype=float32), Array(0.0398656, dtype=float32)), 'eval/episode_reward': (Array(-36614.22, dtype=float32), Array(23398.193, dtype=float32)), 'eval/avg_episode_length': (Array(728.08594, dtype=float32), Array(443.24133, dtype=float32)), 'eval/epoch_eval_time': 4.117352724075317, 'eval/sps': 31087.936491704502}
I0727 00:14:37.576325 140267183036224 train.py:379] starting iteration 349 2535.8413558006287
I0727 00:14:44.750527 140267183036224 train.py:394] {'eval/walltime': 1448.7976746559143, 'training/sps': 40359.68438705749, 'training/walltime': 1087.2973504066467, 'training/entropy_loss': Array(-0.0032827, dtype=float32), 'training/policy_loss': Array(0.00028109, dtype=float32), 'training/total_loss': Array(1969.2516, dtype=float32), 'training/v_loss': Array(1969.2546, dtype=float32), 'eval/episode_goal_distance': (Array(0.28520477, dtype=float32), Array(0.03567841, dtype=float32)), 'eval/episode_reward': (Array(-36771.19, dtype=float32), Array(23177.701, dtype=float32)), 'eval/avg_episode_length': (Array(728.1953, dtype=float32), Array(443.06308, dtype=float32)), 'eval/epoch_eval_time': 4.1255176067352295, 'eval/sps': 31026.409823346774}
I0727 00:14:44.753051 140267183036224 train.py:379] starting iteration 350 2543.018079996109
I0727 00:14:51.921170 140267183036224 train.py:394] {'eval/walltime': 1452.9182212352753, 'training/sps': 40368.61474678136, 'training/walltime': 1090.3412992954254, 'training/entropy_loss': Array(-0.00328182, dtype=float32), 'training/policy_loss': Array(0.00077327, dtype=float32), 'training/total_loss': Array(13238.337, dtype=float32), 'training/v_loss': Array(13238.34, dtype=float32), 'eval/episode_goal_distance': (Array(0.28263697, dtype=float32), Array(0.03942687, dtype=float32)), 'eval/episode_reward': (Array(-40903.227, dtype=float32), Array(20372.184, dtype=float32)), 'eval/avg_episode_length': (Array(821.3672, dtype=float32), Array(381.67438, dtype=float32)), 'eval/epoch_eval_time': 4.120546579360962, 'eval/sps': 31063.840084014042}
I0727 00:14:51.923848 140267183036224 train.py:379] starting iteration 351 2550.18887758255
I0727 00:14:59.066592 140267183036224 train.py:394] {'eval/walltime': 1457.015790462494, 'training/sps': 40397.5097825972, 'training/walltime': 1093.3830709457397, 'training/entropy_loss': Array(-0.00408579, dtype=float32), 'training/policy_loss': Array(-0.00015849, dtype=float32), 'training/total_loss': Array(2565.5908, dtype=float32), 'training/v_loss': Array(2565.595, dtype=float32), 'eval/episode_goal_distance': (Array(0.28023994, dtype=float32), Array(0.03837998, dtype=float32)), 'eval/episode_reward': (Array(-42064.727, dtype=float32), Array(19640.79, dtype=float32)), 'eval/avg_episode_length': (Array(836.90625, dtype=float32), Array(368.14606, dtype=float32)), 'eval/epoch_eval_time': 4.097569227218628, 'eval/sps': 31238.03233139873}
I0727 00:14:59.069015 140267183036224 train.py:379] starting iteration 352 2557.3340451717377
I0727 00:15:06.229719 140267183036224 train.py:394] {'eval/walltime': 1461.1249494552612, 'training/sps': 40316.06624839016, 'training/walltime': 1096.4309873580933, 'training/entropy_loss': Array(-0.00282445, dtype=float32), 'training/policy_loss': Array(0.00013852, dtype=float32), 'training/total_loss': Array(2333.7852, dtype=float32), 'training/v_loss': Array(2333.7876, dtype=float32), 'eval/episode_goal_distance': (Array(0.28347498, dtype=float32), Array(0.0408003, dtype=float32)), 'eval/episode_reward': (Array(-39766.594, dtype=float32), Array(20454.783, dtype=float32)), 'eval/avg_episode_length': (Array(805.8047, dtype=float32), Array(394.17468, dtype=float32)), 'eval/epoch_eval_time': 4.109158992767334, 'eval/sps': 31149.92635361567}
I0727 00:15:06.232080 140267183036224 train.py:379] starting iteration 353 2564.4971101284027
I0727 00:15:13.417899 140267183036224 train.py:394] {'eval/walltime': 1465.255448102951, 'training/sps': 40292.60089781104, 'training/walltime': 1099.4806787967682, 'training/entropy_loss': Array(-0.00346619, dtype=float32), 'training/policy_loss': Array(0.0003612, dtype=float32), 'training/total_loss': Array(2387.17, dtype=float32), 'training/v_loss': Array(2387.1729, dtype=float32), 'eval/episode_goal_distance': (Array(0.28652844, dtype=float32), Array(0.03835722, dtype=float32)), 'eval/episode_reward': (Array(-38243.203, dtype=float32), Array(21962.422, dtype=float32)), 'eval/avg_episode_length': (Array(766.9219, dtype=float32), Array(421.2643, dtype=float32)), 'eval/epoch_eval_time': 4.130498647689819, 'eval/sps': 30988.994530137465}
I0727 00:15:13.420372 140267183036224 train.py:379] starting iteration 354 2571.6854021549225
I0727 00:15:20.592406 140267183036224 train.py:394] {'eval/walltime': 1469.375672340393, 'training/sps': 40312.45247529259, 'training/walltime': 1102.5288684368134, 'training/entropy_loss': Array(-0.00339775, dtype=float32), 'training/policy_loss': Array(0.00020057, dtype=float32), 'training/total_loss': Array(2039.7388, dtype=float32), 'training/v_loss': Array(2039.7418, dtype=float32), 'eval/episode_goal_distance': (Array(0.28388673, dtype=float32), Array(0.03603756, dtype=float32)), 'eval/episode_reward': (Array(-39986.625, dtype=float32), Array(20073.754, dtype=float32)), 'eval/avg_episode_length': (Array(813.4531, dtype=float32), Array(388.3291, dtype=float32)), 'eval/epoch_eval_time': 4.120224237442017, 'eval/sps': 31066.270334710473}
I0727 00:15:20.594783 140267183036224 train.py:379] starting iteration 355 2578.8598132133484
I0727 00:15:27.741305 140267183036224 train.py:394] {'eval/walltime': 1473.4758722782135, 'training/sps': 40384.806947556084, 'training/walltime': 1105.5715968608856, 'training/entropy_loss': Array(-0.00286967, dtype=float32), 'training/policy_loss': Array(0.00020994, dtype=float32), 'training/total_loss': Array(2009.0238, dtype=float32), 'training/v_loss': Array(2009.0264, dtype=float32), 'eval/episode_goal_distance': (Array(0.2835845, dtype=float32), Array(0.03976359, dtype=float32)), 'eval/episode_reward': (Array(-41531.625, dtype=float32), Array(19998.701, dtype=float32)), 'eval/avg_episode_length': (Array(829., dtype=float32), Array(375.35175, dtype=float32)), 'eval/epoch_eval_time': 4.100199937820435, 'eval/sps': 31217.989839793434}
I0727 00:15:27.743943 140267183036224 train.py:379] starting iteration 356 2586.0089733600616
I0727 00:15:34.917130 140267183036224 train.py:394] {'eval/walltime': 1477.5959050655365, 'training/sps': 40294.74930713065, 'training/walltime': 1108.6211256980896, 'training/entropy_loss': Array(-0.00274761, dtype=float32), 'training/policy_loss': Array(0.00059204, dtype=float32), 'training/total_loss': Array(2081.4434, dtype=float32), 'training/v_loss': Array(2081.4453, dtype=float32), 'eval/episode_goal_distance': (Array(0.28320622, dtype=float32), Array(0.03814863, dtype=float32)), 'eval/episode_reward': (Array(-41094.15, dtype=float32), Array(20750.332, dtype=float32)), 'eval/avg_episode_length': (Array(813.6875, dtype=float32), Array(387.84195, dtype=float32)), 'eval/epoch_eval_time': 4.120032787322998, 'eval/sps': 31067.713925443863}
I0727 00:15:34.919502 140267183036224 train.py:379] starting iteration 357 2593.184531211853
I0727 00:15:42.073786 140267183036224 train.py:394] {'eval/walltime': 1481.7009978294373, 'training/sps': 40344.39341108573, 'training/walltime': 1111.666902065277, 'training/entropy_loss': Array(-0.00338412, dtype=float32), 'training/policy_loss': Array(0.00017218, dtype=float32), 'training/total_loss': Array(1883.3969, dtype=float32), 'training/v_loss': Array(1883.3999, dtype=float32), 'eval/episode_goal_distance': (Array(0.28333318, dtype=float32), Array(0.03584637, dtype=float32)), 'eval/episode_reward': (Array(-36102.42, dtype=float32), Array(22431.354, dtype=float32)), 'eval/avg_episode_length': (Array(735.90625, dtype=float32), Array(439.12033, dtype=float32)), 'eval/epoch_eval_time': 4.105092763900757, 'eval/sps': 31180.781376148818}
I0727 00:15:42.562071 140267183036224 train.py:379] starting iteration 358 2600.827077627182
I0727 00:15:49.725838 140267183036224 train.py:394] {'eval/walltime': 1485.7952816486359, 'training/sps': 40086.62600106494, 'training/walltime': 1114.7322635650635, 'training/entropy_loss': Array(-0.00395586, dtype=float32), 'training/policy_loss': Array(0.00037143, dtype=float32), 'training/total_loss': Array(1896.2997, dtype=float32), 'training/v_loss': Array(1896.3032, dtype=float32), 'eval/episode_goal_distance': (Array(0.28907654, dtype=float32), Array(0.03388289, dtype=float32)), 'eval/episode_reward': (Array(-37179.445, dtype=float32), Array(23070.592, dtype=float32)), 'eval/avg_episode_length': (Array(735.77344, dtype=float32), Array(439.34113, dtype=float32)), 'eval/epoch_eval_time': 4.094283819198608, 'eval/sps': 31263.09890872538}
I0727 00:15:49.728550 140267183036224 train.py:379] starting iteration 359 2607.993580341339
I0727 00:15:56.870785 140267183036224 train.py:394] {'eval/walltime': 1489.8888683319092, 'training/sps': 40353.449703630075, 'training/walltime': 1117.7773563861847, 'training/entropy_loss': Array(-0.00426901, dtype=float32), 'training/policy_loss': Array(0.00024485, dtype=float32), 'training/total_loss': Array(2073.2205, dtype=float32), 'training/v_loss': Array(2073.2246, dtype=float32), 'eval/episode_goal_distance': (Array(0.2800488, dtype=float32), Array(0.0356884, dtype=float32)), 'eval/episode_reward': (Array(-38880.188, dtype=float32), Array(21558.438, dtype=float32)), 'eval/avg_episode_length': (Array(782.52344, dtype=float32), Array(410.99295, dtype=float32)), 'eval/epoch_eval_time': 4.093586683273315, 'eval/sps': 31268.42300005935}
I0727 00:15:56.873391 140267183036224 train.py:379] starting iteration 360 2615.138420820236
I0727 00:16:04.019908 140267183036224 train.py:394] {'eval/walltime': 1493.9797604084015, 'training/sps': 40259.19459907153, 'training/walltime': 1120.8295783996582, 'training/entropy_loss': Array(-0.00411755, dtype=float32), 'training/policy_loss': Array(0.00023361, dtype=float32), 'training/total_loss': Array(1881.7213, dtype=float32), 'training/v_loss': Array(1881.7252, dtype=float32), 'eval/episode_goal_distance': (Array(0.28061426, dtype=float32), Array(0.03586501, dtype=float32)), 'eval/episode_reward': (Array(-41118.324, dtype=float32), Array(19246.215, dtype=float32)), 'eval/avg_episode_length': (Array(836.8672, dtype=float32), Array(368.23438, dtype=float32)), 'eval/epoch_eval_time': 4.09089207649231, 'eval/sps': 31289.019022411412}
I0727 00:16:04.022281 140267183036224 train.py:379] starting iteration 361 2622.287311077118
I0727 00:16:11.161778 140267183036224 train.py:394] {'eval/walltime': 1498.07395362854, 'training/sps': 40397.54461321212, 'training/walltime': 1123.8713474273682, 'training/entropy_loss': Array(-0.0025492, dtype=float32), 'training/policy_loss': Array(2.0895663e-05, dtype=float32), 'training/total_loss': Array(1994.4407, dtype=float32), 'training/v_loss': Array(1994.4434, dtype=float32), 'eval/episode_goal_distance': (Array(0.2854706, dtype=float32), Array(0.03863048, dtype=float32)), 'eval/episode_reward': (Array(-40370.387, dtype=float32), Array(20611.68, dtype=float32)), 'eval/avg_episode_length': (Array(813.5156, dtype=float32), Array(388.19897, dtype=float32)), 'eval/epoch_eval_time': 4.09419322013855, 'eval/sps': 31263.790719595887}
I0727 00:16:11.164342 140267183036224 train.py:379] starting iteration 362 2629.42937207222
I0727 00:16:18.315439 140267183036224 train.py:394] {'eval/walltime': 1502.1766376495361, 'training/sps': 40356.57786455387, 'training/walltime': 1126.916204214096, 'training/entropy_loss': Array(-0.00282981, dtype=float32), 'training/policy_loss': Array(0.00034707, dtype=float32), 'training/total_loss': Array(1932.3163, dtype=float32), 'training/v_loss': Array(1932.3188, dtype=float32), 'eval/episode_goal_distance': (Array(0.28138524, dtype=float32), Array(0.03622722, dtype=float32)), 'eval/episode_reward': (Array(-36887.492, dtype=float32), Array(22064.785, dtype=float32)), 'eval/avg_episode_length': (Array(751.53125, dtype=float32), Array(430.3615, dtype=float32)), 'eval/epoch_eval_time': 4.102684020996094, 'eval/sps': 31199.0880469812}
I0727 00:16:18.317800 140267183036224 train.py:379] starting iteration 363 2636.58282995224
I0727 00:16:25.475419 140267183036224 train.py:394] {'eval/walltime': 1506.2831366062164, 'training/sps': 40321.59222214503, 'training/walltime': 1129.963702917099, 'training/entropy_loss': Array(-0.00283078, dtype=float32), 'training/policy_loss': Array(0.0001276, dtype=float32), 'training/total_loss': Array(1971.6558, dtype=float32), 'training/v_loss': Array(1971.6584, dtype=float32), 'eval/episode_goal_distance': (Array(0.28724417, dtype=float32), Array(0.03946656, dtype=float32)), 'eval/episode_reward': (Array(-38200.01, dtype=float32), Array(22001.225, dtype=float32)), 'eval/avg_episode_length': (Array(766.9453, dtype=float32), Array(421.222, dtype=float32)), 'eval/epoch_eval_time': 4.106498956680298, 'eval/sps': 31170.104108214717}
I0727 00:16:25.477918 140267183036224 train.py:379] starting iteration 364 2643.7429478168488
I0727 00:16:32.634359 140267183036224 train.py:394] {'eval/walltime': 1510.3852314949036, 'training/sps': 40275.90026348329, 'training/walltime': 1133.0146589279175, 'training/entropy_loss': Array(-0.00313432, dtype=float32), 'training/policy_loss': Array(0.00024625, dtype=float32), 'training/total_loss': Array(1891.1022, dtype=float32), 'training/v_loss': Array(1891.105, dtype=float32), 'eval/episode_goal_distance': (Array(0.28356877, dtype=float32), Array(0.03864719, dtype=float32)), 'eval/episode_reward': (Array(-39464.32, dtype=float32), Array(20155.566, dtype=float32)), 'eval/avg_episode_length': (Array(813.52344, dtype=float32), Array(388.1827, dtype=float32)), 'eval/epoch_eval_time': 4.102094888687134, 'eval/sps': 31203.56877969883}
I0727 00:16:32.636710 140267183036224 train.py:379] starting iteration 365 2650.9017400741577
I0727 00:16:39.797309 140267183036224 train.py:394] {'eval/walltime': 1514.490201473236, 'training/sps': 40262.02823737998, 'training/walltime': 1136.0666661262512, 'training/entropy_loss': Array(-0.00319021, dtype=float32), 'training/policy_loss': Array(0.00021907, dtype=float32), 'training/total_loss': Array(2008.136, dtype=float32), 'training/v_loss': Array(2008.1389, dtype=float32), 'eval/episode_goal_distance': (Array(0.2820794, dtype=float32), Array(0.03688147, dtype=float32)), 'eval/episode_reward': (Array(-40712.047, dtype=float32), Array(20654.836, dtype=float32)), 'eval/avg_episode_length': (Array(813.6406, dtype=float32), Array(387.9385, dtype=float32)), 'eval/epoch_eval_time': 4.1049699783325195, 'eval/sps': 31181.714038258302}
I0727 00:16:39.799847 140267183036224 train.py:379] starting iteration 366 2658.0648770332336
I0727 00:16:46.964060 140267183036224 train.py:394] {'eval/walltime': 1518.5946214199066, 'training/sps': 40204.051195108455, 'training/walltime': 1139.1230745315552, 'training/entropy_loss': Array(-0.00080748, dtype=float32), 'training/policy_loss': Array(0.00031409, dtype=float32), 'training/total_loss': Array(13003.816, dtype=float32), 'training/v_loss': Array(13003.817, dtype=float32), 'eval/episode_goal_distance': (Array(0.28095955, dtype=float32), Array(0.03780352, dtype=float32)), 'eval/episode_reward': (Array(-40386.348, dtype=float32), Array(20631.576, dtype=float32)), 'eval/avg_episode_length': (Array(813.5547, dtype=float32), Array(388.1178, dtype=float32)), 'eval/epoch_eval_time': 4.104419946670532, 'eval/sps': 31185.892687182855}
I0727 00:16:46.966405 140267183036224 train.py:379] starting iteration 367 2665.231434583664
I0727 00:16:54.136140 140267183036224 train.py:394] {'eval/walltime': 1522.7006578445435, 'training/sps': 40155.40239735966, 'training/walltime': 1142.1831858158112, 'training/entropy_loss': Array(-0.00300728, dtype=float32), 'training/policy_loss': Array(0.00015, dtype=float32), 'training/total_loss': Array(3034.056, dtype=float32), 'training/v_loss': Array(3034.0586, dtype=float32), 'eval/episode_goal_distance': (Array(0.28645945, dtype=float32), Array(0.03733135, dtype=float32)), 'eval/episode_reward': (Array(-37965.453, dtype=float32), Array(23115.922, dtype=float32)), 'eval/avg_episode_length': (Array(743.6797, dtype=float32), Array(434.8994, dtype=float32)), 'eval/epoch_eval_time': 4.106036424636841, 'eval/sps': 31173.6153220611}
I0727 00:16:54.138682 140267183036224 train.py:379] starting iteration 368 2672.403712272644
I0727 00:17:01.300278 140267183036224 train.py:394] {'eval/walltime': 1526.803743839264, 'training/sps': 40223.80303578861, 'training/walltime': 1145.2380933761597, 'training/entropy_loss': Array(-0.00175585, dtype=float32), 'training/policy_loss': Array(2.6548769e-05, dtype=float32), 'training/total_loss': Array(2468.4058, dtype=float32), 'training/v_loss': Array(2468.4075, dtype=float32), 'eval/episode_goal_distance': (Array(0.2823592, dtype=float32), Array(0.03670069, dtype=float32)), 'eval/episode_reward': (Array(-40257.312, dtype=float32), Array(21160.244, dtype=float32)), 'eval/avg_episode_length': (Array(797.96094, dtype=float32), Array(400.17468, dtype=float32)), 'eval/epoch_eval_time': 4.103085994720459, 'eval/sps': 31196.031514986702}
I0727 00:17:01.302826 140267183036224 train.py:379] starting iteration 369 2679.5678555965424
I0727 00:17:08.464981 140267183036224 train.py:394] {'eval/walltime': 1530.9080917835236, 'training/sps': 40232.39384331962, 'training/walltime': 1148.2923486232758, 'training/entropy_loss': Array(-0.00214073, dtype=float32), 'training/policy_loss': Array(0.00022492, dtype=float32), 'training/total_loss': Array(2208.668, dtype=float32), 'training/v_loss': Array(2208.67, dtype=float32), 'eval/episode_goal_distance': (Array(0.28558436, dtype=float32), Array(0.0390623, dtype=float32)), 'eval/episode_reward': (Array(-36745.47, dtype=float32), Array(22797.992, dtype=float32)), 'eval/avg_episode_length': (Array(743.6328, dtype=float32), Array(434.979, dtype=float32)), 'eval/epoch_eval_time': 4.1043479442596436, 'eval/sps': 31186.439780043813}
I0727 00:17:08.467341 140267183036224 train.py:379] starting iteration 370 2686.7323710918427
I0727 00:17:15.624559 140267183036224 train.py:394] {'eval/walltime': 1535.008188009262, 'training/sps': 40239.5933537177, 'training/walltime': 1151.3460574150085, 'training/entropy_loss': Array(-0.00220962, dtype=float32), 'training/policy_loss': Array(0.00083284, dtype=float32), 'training/total_loss': Array(2267.6865, dtype=float32), 'training/v_loss': Array(2267.688, dtype=float32), 'eval/episode_goal_distance': (Array(0.28518772, dtype=float32), Array(0.03965581, dtype=float32)), 'eval/episode_reward': (Array(-32595.059, dtype=float32), Array(23661.266, dtype=float32)), 'eval/avg_episode_length': (Array(673.71875, dtype=float32), Array(466.89343, dtype=float32)), 'eval/epoch_eval_time': 4.100096225738525, 'eval/sps': 31218.779499972377}
I0727 00:17:15.626940 140267183036224 train.py:379] starting iteration 371 2693.891969680786
I0727 00:17:22.791383 140267183036224 train.py:394] {'eval/walltime': 1539.1143991947174, 'training/sps': 40225.74318250802, 'training/walltime': 1154.4008176326752, 'training/entropy_loss': Array(-0.00117793, dtype=float32), 'training/policy_loss': Array(0.00026508, dtype=float32), 'training/total_loss': Array(2032.5803, dtype=float32), 'training/v_loss': Array(2032.5813, dtype=float32), 'eval/episode_goal_distance': (Array(0.29297978, dtype=float32), Array(0.03675351, dtype=float32)), 'eval/episode_reward': (Array(-41474.17, dtype=float32), Array(21792.104, dtype=float32)), 'eval/avg_episode_length': (Array(798.0703, dtype=float32), Array(399.9581, dtype=float32)), 'eval/epoch_eval_time': 4.106211185455322, 'eval/sps': 31172.288569421584}
I0727 00:17:22.793866 140267183036224 train.py:379] starting iteration 372 2701.0588960647583
I0727 00:17:29.950049 140267183036224 train.py:394] {'eval/walltime': 1543.2125039100647, 'training/sps': 40228.65375353994, 'training/walltime': 1157.455356836319, 'training/entropy_loss': Array(-0.00068281, dtype=float32), 'training/policy_loss': Array(0.00011917, dtype=float32), 'training/total_loss': Array(2049.2866, dtype=float32), 'training/v_loss': Array(2049.287, dtype=float32), 'eval/episode_goal_distance': (Array(0.2881307, dtype=float32), Array(0.03703284, dtype=float32)), 'eval/episode_reward': (Array(-40489.277, dtype=float32), Array(21639.543, dtype=float32)), 'eval/avg_episode_length': (Array(798.1094, dtype=float32), Array(399.88116, dtype=float32)), 'eval/epoch_eval_time': 4.09810471534729, 'eval/sps': 31233.95054319708}
I0727 00:17:29.952403 140267183036224 train.py:379] starting iteration 373 2708.217433691025
I0727 00:17:37.108189 140267183036224 train.py:394] {'eval/walltime': 1547.3117909431458, 'training/sps': 40250.7778065174, 'training/walltime': 1160.5082170963287, 'training/entropy_loss': Array(-0.00115039, dtype=float32), 'training/policy_loss': Array(0.0005003, dtype=float32), 'training/total_loss': Array(2095.163, dtype=float32), 'training/v_loss': Array(2095.1636, dtype=float32), 'eval/episode_goal_distance': (Array(0.28718197, dtype=float32), Array(0.0387569, dtype=float32)), 'eval/episode_reward': (Array(-38629.086, dtype=float32), Array(22397.73, dtype=float32)), 'eval/avg_episode_length': (Array(767.0625, dtype=float32), Array(421.01038, dtype=float32)), 'eval/epoch_eval_time': 4.099287033081055, 'eval/sps': 31224.942036760534}
I0727 00:17:37.110608 140267183036224 train.py:379] starting iteration 374 2715.3756380081177
I0727 00:17:44.271795 140267183036224 train.py:394] {'eval/walltime': 1551.4113273620605, 'training/sps': 40181.93147455012, 'training/walltime': 1163.5663080215454, 'training/entropy_loss': Array(-0.00120257, dtype=float32), 'training/policy_loss': Array(0.00020851, dtype=float32), 'training/total_loss': Array(1858.4211, dtype=float32), 'training/v_loss': Array(1858.422, dtype=float32), 'eval/episode_goal_distance': (Array(0.2787757, dtype=float32), Array(0.03977482, dtype=float32)), 'eval/episode_reward': (Array(-40001.133, dtype=float32), Array(20052.719, dtype=float32)), 'eval/avg_episode_length': (Array(821.28906, dtype=float32), Array(381.84113, dtype=float32)), 'eval/epoch_eval_time': 4.099536418914795, 'eval/sps': 31223.042539498503}
I0727 00:17:44.274147 140267183036224 train.py:379] starting iteration 375 2722.5391771793365
I0727 00:17:51.438644 140267183036224 train.py:394] {'eval/walltime': 1555.5174114704132, 'training/sps': 40224.32415713829, 'training/walltime': 1166.6211760044098, 'training/entropy_loss': Array(-0.00192634, dtype=float32), 'training/policy_loss': Array(0.00021616, dtype=float32), 'training/total_loss': Array(1933.5188, dtype=float32), 'training/v_loss': Array(1933.5205, dtype=float32), 'eval/episode_goal_distance': (Array(0.2811308, dtype=float32), Array(0.0384881, dtype=float32)), 'eval/episode_reward': (Array(-40234.15, dtype=float32), Array(20832.854, dtype=float32)), 'eval/avg_episode_length': (Array(805.7578, dtype=float32), Array(394.26926, dtype=float32)), 'eval/epoch_eval_time': 4.106084108352661, 'eval/sps': 31173.253304680336}
I0727 00:17:51.441255 140267183036224 train.py:379] starting iteration 376 2729.7062849998474
I0727 00:17:58.603708 140267183036224 train.py:394] {'eval/walltime': 1559.6172552108765, 'training/sps': 40167.53242516402, 'training/walltime': 1169.6803631782532, 'training/entropy_loss': Array(-0.00277034, dtype=float32), 'training/policy_loss': Array(0.00043061, dtype=float32), 'training/total_loss': Array(1998.7247, dtype=float32), 'training/v_loss': Array(1998.727, dtype=float32), 'eval/episode_goal_distance': (Array(0.28324586, dtype=float32), Array(0.0379504, dtype=float32)), 'eval/episode_reward': (Array(-41533.57, dtype=float32), Array(19924.457, dtype=float32)), 'eval/avg_episode_length': (Array(836.9375, dtype=float32), Array(368.07578, dtype=float32)), 'eval/epoch_eval_time': 4.099843740463257, 'eval/sps': 31220.7020810839}
I0727 00:17:58.606052 140267183036224 train.py:379] starting iteration 377 2736.871081829071
I0727 00:18:05.768143 140267183036224 train.py:394] {'eval/walltime': 1563.7253923416138, 'training/sps': 40282.43213952928, 'training/walltime': 1172.73082447052, 'training/entropy_loss': Array(-0.00354579, dtype=float32), 'training/policy_loss': Array(0.00056084, dtype=float32), 'training/total_loss': Array(1959.1859, dtype=float32), 'training/v_loss': Array(1959.189, dtype=float32), 'eval/episode_goal_distance': (Array(0.283978, dtype=float32), Array(0.03717051, dtype=float32)), 'eval/episode_reward': (Array(-38304.773, dtype=float32), Array(22029.773, dtype=float32)), 'eval/avg_episode_length': (Array(766.9375, dtype=float32), Array(421.236, dtype=float32)), 'eval/epoch_eval_time': 4.108137130737305, 'eval/sps': 31157.674616627828}
I0727 00:18:05.770519 140267183036224 train.py:379] starting iteration 378 2744.035548210144
I0727 00:18:12.931291 140267183036224 train.py:394] {'eval/walltime': 1567.8273742198944, 'training/sps': 40219.540393438125, 'training/walltime': 1175.786055803299, 'training/entropy_loss': Array(-0.00333382, dtype=float32), 'training/policy_loss': Array(0.0001895, dtype=float32), 'training/total_loss': Array(1816.6439, dtype=float32), 'training/v_loss': Array(1816.647, dtype=float32), 'eval/episode_goal_distance': (Array(0.28829822, dtype=float32), Array(0.03762608, dtype=float32)), 'eval/episode_reward': (Array(-39281.36, dtype=float32), Array(22654.783, dtype=float32)), 'eval/avg_episode_length': (Array(766.91406, dtype=float32), Array(421.27863, dtype=float32)), 'eval/epoch_eval_time': 4.10198187828064, 'eval/sps': 31204.428444147994}
I0727 00:18:12.933641 140267183036224 train.py:379] starting iteration 379 2751.198670864105
I0727 00:18:20.101704 140267183036224 train.py:394] {'eval/walltime': 1571.9367270469666, 'training/sps': 40219.76637241399, 'training/walltime': 1178.8412699699402, 'training/entropy_loss': Array(-0.0035675, dtype=float32), 'training/policy_loss': Array(0.00048623, dtype=float32), 'training/total_loss': Array(1834.6595, dtype=float32), 'training/v_loss': Array(1834.6626, dtype=float32), 'eval/episode_goal_distance': (Array(0.2861764, dtype=float32), Array(0.03857042, dtype=float32)), 'eval/episode_reward': (Array(-39731.758, dtype=float32), Array(21175.912, dtype=float32)), 'eval/avg_episode_length': (Array(797.8906, dtype=float32), Array(400.31357, dtype=float32)), 'eval/epoch_eval_time': 4.1093528270721436, 'eval/sps': 31148.45704091031}
I0727 00:18:20.104105 140267183036224 train.py:379] starting iteration 380 2758.3691351413727
I0727 00:18:27.265927 140267183036224 train.py:394] {'eval/walltime': 1576.0363054275513, 'training/sps': 40179.95796274368, 'training/walltime': 1181.8995110988617, 'training/entropy_loss': Array(-0.00396192, dtype=float32), 'training/policy_loss': Array(0.00019762, dtype=float32), 'training/total_loss': Array(1855.4615, dtype=float32), 'training/v_loss': Array(1855.4652, dtype=float32), 'eval/episode_goal_distance': (Array(0.2808758, dtype=float32), Array(0.04149376, dtype=float32)), 'eval/episode_reward': (Array(-42286.477, dtype=float32), Array(18827.08, dtype=float32)), 'eval/avg_episode_length': (Array(852.5156, dtype=float32), Array(353.25107, dtype=float32)), 'eval/epoch_eval_time': 4.099578380584717, 'eval/sps': 31222.72295273046}
I0727 00:18:27.268428 140267183036224 train.py:379] starting iteration 381 2765.5334582328796
I0727 00:18:34.437164 140267183036224 train.py:394] {'eval/walltime': 1580.145913362503, 'training/sps': 40216.28595011426, 'training/walltime': 1184.9549896717072, 'training/entropy_loss': Array(-0.00348383, dtype=float32), 'training/policy_loss': Array(0.00045728, dtype=float32), 'training/total_loss': Array(1999.7402, dtype=float32), 'training/v_loss': Array(1999.7432, dtype=float32), 'eval/episode_goal_distance': (Array(0.28288496, dtype=float32), Array(0.03764948, dtype=float32)), 'eval/episode_reward': (Array(-41954.18, dtype=float32), Array(21011.068, dtype=float32)), 'eval/avg_episode_length': (Array(821.3906, dtype=float32), Array(381.62387, dtype=float32)), 'eval/epoch_eval_time': 4.109607934951782, 'eval/sps': 31146.523470371343}
I0727 00:18:34.439655 140267183036224 train.py:379] starting iteration 382 2772.704684495926
I0727 00:18:41.599992 140267183036224 train.py:394] {'eval/walltime': 1584.2453591823578, 'training/sps': 40191.21897994471, 'training/walltime': 1188.0123739242554, 'training/entropy_loss': Array(-0.00305333, dtype=float32), 'training/policy_loss': Array(0.00021161, dtype=float32), 'training/total_loss': Array(1845.2097, dtype=float32), 'training/v_loss': Array(1845.2126, dtype=float32), 'eval/episode_goal_distance': (Array(0.2845689, dtype=float32), Array(0.03573825, dtype=float32)), 'eval/episode_reward': (Array(-39582.523, dtype=float32), Array(21049.941, dtype=float32)), 'eval/avg_episode_length': (Array(798.1094, dtype=float32), Array(399.88107, dtype=float32)), 'eval/epoch_eval_time': 4.099445819854736, 'eval/sps': 31223.732578696618}
I0727 00:18:41.602391 140267183036224 train.py:379] starting iteration 383 2779.867420911789
I0727 00:18:48.769386 140267183036224 train.py:394] {'eval/walltime': 1588.3540527820587, 'training/sps': 40225.589345273205, 'training/walltime': 1191.0671458244324, 'training/entropy_loss': Array(-0.00209635, dtype=float32), 'training/policy_loss': Array(0.00035696, dtype=float32), 'training/total_loss': Array(14235.223, dtype=float32), 'training/v_loss': Array(14235.226, dtype=float32), 'eval/episode_goal_distance': (Array(0.28105074, dtype=float32), Array(0.03782376, dtype=float32)), 'eval/episode_reward': (Array(-36477.164, dtype=float32), Array(21907.082, dtype=float32)), 'eval/avg_episode_length': (Array(751.4297, dtype=float32), Array(430.5373, dtype=float32)), 'eval/epoch_eval_time': 4.108693599700928, 'eval/sps': 31153.454715950866}
I0727 00:18:48.771774 140267183036224 train.py:379] starting iteration 384 2787.036804676056
I0727 00:18:55.933502 140267183036224 train.py:394] {'eval/walltime': 1592.4539232254028, 'training/sps': 40180.03000817796, 'training/walltime': 1194.1253814697266, 'training/entropy_loss': Array(-0.0036337, dtype=float32), 'training/policy_loss': Array(0.00037309, dtype=float32), 'training/total_loss': Array(2830.891, dtype=float32), 'training/v_loss': Array(2830.8943, dtype=float32), 'eval/episode_goal_distance': (Array(0.28386256, dtype=float32), Array(0.03664147, dtype=float32)), 'eval/episode_reward': (Array(-40680.656, dtype=float32), Array(21136.205, dtype=float32)), 'eval/avg_episode_length': (Array(805.83594, dtype=float32), Array(394.1109, dtype=float32)), 'eval/epoch_eval_time': 4.099870443344116, 'eval/sps': 31220.498737417427}
I0727 00:18:55.935901 140267183036224 train.py:379] starting iteration 385 2794.2009315490723
I0727 00:19:03.101094 140267183036224 train.py:394] {'eval/walltime': 1596.561974287033, 'training/sps': 40240.94433426378, 'training/walltime': 1197.1789877414703, 'training/entropy_loss': Array(-0.00386164, dtype=float32), 'training/policy_loss': Array(0.00033047, dtype=float32), 'training/total_loss': Array(2630.5952, dtype=float32), 'training/v_loss': Array(2630.5989, dtype=float32), 'eval/episode_goal_distance': (Array(0.28509027, dtype=float32), Array(0.04206409, dtype=float32)), 'eval/episode_reward': (Array(-37700.11, dtype=float32), Array(22473.537, dtype=float32)), 'eval/avg_episode_length': (Array(759.25, dtype=float32), Array(425.8656, dtype=float32)), 'eval/epoch_eval_time': 4.108051061630249, 'eval/sps': 31158.327411150574}
I0727 00:19:03.103498 140267183036224 train.py:379] starting iteration 386 2801.368528366089
I0727 00:19:10.263141 140267183036224 train.py:394] {'eval/walltime': 1600.6596500873566, 'training/sps': 40178.95875053557, 'training/walltime': 1200.2373049259186, 'training/entropy_loss': Array(-0.00453376, dtype=float32), 'training/policy_loss': Array(7.9009515e-05, dtype=float32), 'training/total_loss': Array(2130.3904, dtype=float32), 'training/v_loss': Array(2130.3948, dtype=float32), 'eval/episode_goal_distance': (Array(0.28189176, dtype=float32), Array(0.03666077, dtype=float32)), 'eval/episode_reward': (Array(-41541.008, dtype=float32), Array(19048.54, dtype=float32)), 'eval/avg_episode_length': (Array(852.2969, dtype=float32), Array(353.77448, dtype=float32)), 'eval/epoch_eval_time': 4.097675800323486, 'eval/sps': 31237.219886916184}
I0727 00:19:10.265644 140267183036224 train.py:379] starting iteration 387 2808.5306737422943
I0727 00:19:17.432855 140267183036224 train.py:394] {'eval/walltime': 1604.7674055099487, 'training/sps': 40211.38488002966, 'training/walltime': 1203.2931559085846, 'training/entropy_loss': Array(-0.00485262, dtype=float32), 'training/policy_loss': Array(0.00053977, dtype=float32), 'training/total_loss': Array(2161.8987, dtype=float32), 'training/v_loss': Array(2161.9028, dtype=float32), 'eval/episode_goal_distance': (Array(0.28169292, dtype=float32), Array(0.04014608, dtype=float32)), 'eval/episode_reward': (Array(-39593.93, dtype=float32), Array(20265.252, dtype=float32)), 'eval/avg_episode_length': (Array(813.47656, dtype=float32), Array(388.2802, dtype=float32)), 'eval/epoch_eval_time': 4.107755422592163, 'eval/sps': 31160.56990540754}
I0727 00:19:17.435227 140267183036224 train.py:379] starting iteration 388 2815.700256586075
I0727 00:19:24.598223 140267183036224 train.py:394] {'eval/walltime': 1608.8692758083344, 'training/sps': 40186.853568407685, 'training/walltime': 1206.3508722782135, 'training/entropy_loss': Array(-0.00529537, dtype=float32), 'training/policy_loss': Array(1.4653429e-05, dtype=float32), 'training/total_loss': Array(2112.4666, dtype=float32), 'training/v_loss': Array(2112.4717, dtype=float32), 'eval/episode_goal_distance': (Array(0.28649202, dtype=float32), Array(0.03995501, dtype=float32)), 'eval/episode_reward': (Array(-42322.094, dtype=float32), Array(20082.605, dtype=float32)), 'eval/avg_episode_length': (Array(836.8047, dtype=float32), Array(368.3756, dtype=float32)), 'eval/epoch_eval_time': 4.10187029838562, 'eval/sps': 31205.2772732422}
I0727 00:19:24.600605 140267183036224 train.py:379] starting iteration 389 2822.865634918213
I0727 00:19:31.770614 140267183036224 train.py:394] {'eval/walltime': 1612.9789700508118, 'training/sps': 40200.25679689028, 'training/walltime': 1209.4075691699982, 'training/entropy_loss': Array(-0.00451545, dtype=float32), 'training/policy_loss': Array(0.00030971, dtype=float32), 'training/total_loss': Array(2067.1284, dtype=float32), 'training/v_loss': Array(2067.1326, dtype=float32), 'eval/episode_goal_distance': (Array(0.28670707, dtype=float32), Array(0.03642732, dtype=float32)), 'eval/episode_reward': (Array(-38896.453, dtype=float32), Array(21406.908, dtype=float32)), 'eval/avg_episode_length': (Array(782.5625, dtype=float32), Array(410.91965, dtype=float32)), 'eval/epoch_eval_time': 4.109694242477417, 'eval/sps': 31145.869363468435}
I0727 00:19:31.773008 140267183036224 train.py:379] starting iteration 390 2830.0380375385284
I0727 00:19:38.932466 140267183036224 train.py:394] {'eval/walltime': 1617.0783360004425, 'training/sps': 40203.201313096164, 'training/walltime': 1212.464042186737, 'training/entropy_loss': Array(-0.00492882, dtype=float32), 'training/policy_loss': Array(6.503607e-05, dtype=float32), 'training/total_loss': Array(2048.8076, dtype=float32), 'training/v_loss': Array(2048.8125, dtype=float32), 'eval/episode_goal_distance': (Array(0.28029808, dtype=float32), Array(0.03474792, dtype=float32)), 'eval/episode_reward': (Array(-40901.79, dtype=float32), Array(20436.73, dtype=float32)), 'eval/avg_episode_length': (Array(821.1875, dtype=float32), Array(382.0582, dtype=float32)), 'eval/epoch_eval_time': 4.099365949630737, 'eval/sps': 31224.340928023266}
I0727 00:19:38.935243 140267183036224 train.py:379] starting iteration 391 2837.200273513794
I0727 00:19:46.104980 140267183036224 train.py:394] {'eval/walltime': 1621.1904828548431, 'training/sps': 40236.078087627255, 'training/walltime': 1215.5180177688599, 'training/entropy_loss': Array(-0.00494599, dtype=float32), 'training/policy_loss': Array(0.00015499, dtype=float32), 'training/total_loss': Array(1947.0447, dtype=float32), 'training/v_loss': Array(1947.0494, dtype=float32), 'eval/episode_goal_distance': (Array(0.28154603, dtype=float32), Array(0.03648067, dtype=float32)), 'eval/episode_reward': (Array(-39783.953, dtype=float32), Array(20993.479, dtype=float32)), 'eval/avg_episode_length': (Array(798.02344, dtype=float32), Array(400.05093, dtype=float32)), 'eval/epoch_eval_time': 4.112146854400635, 'eval/sps': 31127.29300098321}
I0727 00:19:46.107400 140267183036224 train.py:379] starting iteration 392 2844.372430086136
I0727 00:19:53.273013 140267183036224 train.py:394] {'eval/walltime': 1625.2947146892548, 'training/sps': 40186.483821191396, 'training/walltime': 1218.575762271881, 'training/entropy_loss': Array(-0.0056853, dtype=float32), 'training/policy_loss': Array(-0.00019649, dtype=float32), 'training/total_loss': Array(1963.9636, dtype=float32), 'training/v_loss': Array(1963.9695, dtype=float32), 'eval/episode_goal_distance': (Array(0.28155306, dtype=float32), Array(0.03657204, dtype=float32)), 'eval/episode_reward': (Array(-41611.035, dtype=float32), Array(19180.502, dtype=float32)), 'eval/avg_episode_length': (Array(844.6094, dtype=float32), Array(361.09573, dtype=float32)), 'eval/epoch_eval_time': 4.104231834411621, 'eval/sps': 31187.32205300726}
I0727 00:19:53.275553 140267183036224 train.py:379] starting iteration 393 2851.5405836105347
I0727 00:20:00.448016 140267183036224 train.py:394] {'eval/walltime': 1629.4102602005005, 'training/sps': 40244.88783116196, 'training/walltime': 1221.629069328308, 'training/entropy_loss': Array(-0.00596146, dtype=float32), 'training/policy_loss': Array(-5.463546e-05, dtype=float32), 'training/total_loss': Array(2026.9199, dtype=float32), 'training/v_loss': Array(2026.9259, dtype=float32), 'eval/episode_goal_distance': (Array(0.28136414, dtype=float32), Array(0.03724339, dtype=float32)), 'eval/episode_reward': (Array(-36568.71, dtype=float32), Array(22856.848, dtype=float32)), 'eval/avg_episode_length': (Array(735.83594, dtype=float32), Array(439.23712, dtype=float32)), 'eval/epoch_eval_time': 4.1155455112457275, 'eval/sps': 31101.5877847153}
I0727 00:20:00.450416 140267183036224 train.py:379] starting iteration 394 2858.7154462337494
I0727 00:20:07.610242 140267183036224 train.py:394] {'eval/walltime': 1633.5110049247742, 'training/sps': 40216.64055530287, 'training/walltime': 1224.6845209598541, 'training/entropy_loss': Array(-0.00579735, dtype=float32), 'training/policy_loss': Array(-7.551811e-05, dtype=float32), 'training/total_loss': Array(2128.3545, dtype=float32), 'training/v_loss': Array(2128.3606, dtype=float32), 'eval/episode_goal_distance': (Array(0.28827775, dtype=float32), Array(0.03606764, dtype=float32)), 'eval/episode_reward': (Array(-40853.22, dtype=float32), Array(22515.127, dtype=float32)), 'eval/avg_episode_length': (Array(782.4297, dtype=float32), Array(411.17014, dtype=float32)), 'eval/epoch_eval_time': 4.100744724273682, 'eval/sps': 31213.842510684735}
I0727 00:20:07.612715 140267183036224 train.py:379] starting iteration 395 2865.877744913101
I0727 00:20:14.783533 140267183036224 train.py:394] {'eval/walltime': 1637.6221950054169, 'training/sps': 40208.643061085306, 'training/walltime': 1227.7405803203583, 'training/entropy_loss': Array(-0.00483371, dtype=float32), 'training/policy_loss': Array(0.00061163, dtype=float32), 'training/total_loss': Array(1934.037, dtype=float32), 'training/v_loss': Array(1934.0413, dtype=float32), 'eval/episode_goal_distance': (Array(0.28595015, dtype=float32), Array(0.0390065, dtype=float32)), 'eval/episode_reward': (Array(-41063.32, dtype=float32), Array(20928.354, dtype=float32)), 'eval/avg_episode_length': (Array(813.5781, dtype=float32), Array(388.06863, dtype=float32)), 'eval/epoch_eval_time': 4.1111900806427, 'eval/sps': 31134.53707788423}
I0727 00:20:14.785930 140267183036224 train.py:379] starting iteration 396 2873.050960302353
I0727 00:20:21.949239 140267183036224 train.py:394] {'eval/walltime': 1641.7225675582886, 'training/sps': 40164.533661198126, 'training/walltime': 1230.7999958992004, 'training/entropy_loss': Array(-0.00501125, dtype=float32), 'training/policy_loss': Array(0.00015569, dtype=float32), 'training/total_loss': Array(1821.4745, dtype=float32), 'training/v_loss': Array(1821.4794, dtype=float32), 'eval/episode_goal_distance': (Array(0.28593153, dtype=float32), Array(0.03313072, dtype=float32)), 'eval/episode_reward': (Array(-36480.305, dtype=float32), Array(23554.275, dtype=float32)), 'eval/avg_episode_length': (Array(720.2969, dtype=float32), Array(447.13742, dtype=float32)), 'eval/epoch_eval_time': 4.100372552871704, 'eval/sps': 31216.675643376588}
I0727 00:20:21.951717 140267183036224 train.py:379] starting iteration 397 2880.216745853424
I0727 00:20:29.122492 140267183036224 train.py:394] {'eval/walltime': 1645.8370802402496, 'training/sps': 40251.959780847734, 'training/walltime': 1233.8527665138245, 'training/entropy_loss': Array(-0.00377718, dtype=float32), 'training/policy_loss': Array(-0.00025754, dtype=float32), 'training/total_loss': Array(1867.7107, dtype=float32), 'training/v_loss': Array(1867.7148, dtype=float32), 'eval/episode_goal_distance': (Array(0.28842697, dtype=float32), Array(0.03514175, dtype=float32)), 'eval/episode_reward': (Array(-41316.42, dtype=float32), Array(21273.93, dtype=float32)), 'eval/avg_episode_length': (Array(805.83594, dtype=float32), Array(394.11093, dtype=float32)), 'eval/epoch_eval_time': 4.11451268196106, 'eval/sps': 31109.39493786968}
I0727 00:20:29.124886 140267183036224 train.py:379] starting iteration 398 2887.3899159431458
I0727 00:20:36.288812 140267183036224 train.py:394] {'eval/walltime': 1649.9417996406555, 'training/sps': 40214.877005443566, 'training/walltime': 1236.908352136612, 'training/entropy_loss': Array(-0.00404326, dtype=float32), 'training/policy_loss': Array(0.00037669, dtype=float32), 'training/total_loss': Array(1909.4346, dtype=float32), 'training/v_loss': Array(1909.4381, dtype=float32), 'eval/episode_goal_distance': (Array(0.28757614, dtype=float32), Array(0.03841702, dtype=float32)), 'eval/episode_reward': (Array(-40794.07, dtype=float32), Array(20764.139, dtype=float32)), 'eval/avg_episode_length': (Array(805.8203, dtype=float32), Array(394.14273, dtype=float32)), 'eval/epoch_eval_time': 4.104719400405884, 'eval/sps': 31183.617566487756}
I0727 00:20:36.291198 140267183036224 train.py:379] starting iteration 399 2894.556227207184
I0727 00:20:43.465265 140267183036224 train.py:394] {'eval/walltime': 1654.0577070713043, 'training/sps': 40228.28951663399, 'training/walltime': 1239.962918996811, 'training/entropy_loss': Array(-0.00488648, dtype=float32), 'training/policy_loss': Array(0.00037579, dtype=float32), 'training/total_loss': Array(1804.1991, dtype=float32), 'training/v_loss': Array(1804.2036, dtype=float32), 'eval/episode_goal_distance': (Array(0.2796042, dtype=float32), Array(0.0412304, dtype=float32)), 'eval/episode_reward': (Array(-37756.734, dtype=float32), Array(21325.441, dtype=float32)), 'eval/avg_episode_length': (Array(774.7969, dtype=float32), Array(416.0965, dtype=float32)), 'eval/epoch_eval_time': 4.115907430648804, 'eval/sps': 31098.8529641987}
I0727 00:20:43.467666 140267183036224 train.py:379] starting iteration 400 2901.7326962947845
I0727 00:20:50.636111 140267183036224 train.py:394] {'eval/walltime': 1658.168845653534, 'training/sps': 40239.96722065099, 'training/walltime': 1243.0165994167328, 'training/entropy_loss': Array(-0.00430584, dtype=float32), 'training/policy_loss': Array(0.00029312, dtype=float32), 'training/total_loss': Array(13989.876, dtype=float32), 'training/v_loss': Array(13989.881, dtype=float32), 'eval/episode_goal_distance': (Array(0.28387585, dtype=float32), Array(0.04069762, dtype=float32)), 'eval/episode_reward': (Array(-38695.688, dtype=float32), Array(20626.006, dtype=float32)), 'eval/avg_episode_length': (Array(797.9922, dtype=float32), Array(400.11273, dtype=float32)), 'eval/epoch_eval_time': 4.111138582229614, 'eval/sps': 31134.927086447453}
I0727 00:20:50.638673 140267183036224 train.py:379] starting iteration 401 2908.903702735901
I0727 00:20:57.815723 140267183036224 train.py:394] {'eval/walltime': 1662.2878947257996, 'training/sps': 40230.2614986909, 'training/walltime': 1246.071016550064, 'training/entropy_loss': Array(-0.00587038, dtype=float32), 'training/policy_loss': Array(0.0005439, dtype=float32), 'training/total_loss': Array(2375.3794, dtype=float32), 'training/v_loss': Array(2375.3848, dtype=float32), 'eval/episode_goal_distance': (Array(0.2847488, dtype=float32), Array(0.03732203, dtype=float32)), 'eval/episode_reward': (Array(-39283.285, dtype=float32), Array(21567.002, dtype=float32)), 'eval/avg_episode_length': (Array(790.3672, dtype=float32), Array(405.45135, dtype=float32)), 'eval/epoch_eval_time': 4.119049072265625, 'eval/sps': 31075.133545227563}
I0727 00:20:57.818192 140267183036224 train.py:379] starting iteration 402 2916.0832211971283
I0727 00:21:04.987079 140267183036224 train.py:394] {'eval/walltime': 1666.3985188007355, 'training/sps': 40227.88132793918, 'training/walltime': 1249.1256144046783, 'training/entropy_loss': Array(-0.00494973, dtype=float32), 'training/policy_loss': Array(0.00030536, dtype=float32), 'training/total_loss': Array(2550.742, dtype=float32), 'training/v_loss': Array(2550.7466, dtype=float32), 'eval/episode_goal_distance': (Array(0.28269607, dtype=float32), Array(0.03641231, dtype=float32)), 'eval/episode_reward': (Array(-39207.766, dtype=float32), Array(21750.201, dtype=float32)), 'eval/avg_episode_length': (Array(782.4531, dtype=float32), Array(411.12592, dtype=float32)), 'eval/epoch_eval_time': 4.110624074935913, 'eval/sps': 31138.82409740803}
I0727 00:21:04.989544 140267183036224 train.py:379] starting iteration 403 2923.2545731067657
I0727 00:21:12.159258 140267183036224 train.py:394] {'eval/walltime': 1670.512689113617, 'training/sps': 40262.996986579994, 'training/walltime': 1252.1775481700897, 'training/entropy_loss': Array(-0.00454467, dtype=float32), 'training/policy_loss': Array(0.00025116, dtype=float32), 'training/total_loss': Array(2275.1836, dtype=float32), 'training/v_loss': Array(2275.188, dtype=float32), 'eval/episode_goal_distance': (Array(0.28751323, dtype=float32), Array(0.03885662, dtype=float32)), 'eval/episode_reward': (Array(-40648.977, dtype=float32), Array(21979.828, dtype=float32)), 'eval/avg_episode_length': (Array(790.3047, dtype=float32), Array(405.57257, dtype=float32)), 'eval/epoch_eval_time': 4.11417031288147, 'eval/sps': 31111.983769663577}
I0727 00:21:12.161778 140267183036224 train.py:379] starting iteration 404 2930.4268083572388
I0727 00:21:19.338046 140267183036224 train.py:394] {'eval/walltime': 1674.6293168067932, 'training/sps': 40206.00511916678, 'training/walltime': 1255.233808040619, 'training/entropy_loss': Array(-0.00541808, dtype=float32), 'training/policy_loss': Array(0.00034171, dtype=float32), 'training/total_loss': Array(2019.896, dtype=float32), 'training/v_loss': Array(2019.9012, dtype=float32), 'eval/episode_goal_distance': (Array(0.28286573, dtype=float32), Array(0.03908418, dtype=float32)), 'eval/episode_reward': (Array(-40610.227, dtype=float32), Array(21754.002, dtype=float32)), 'eval/avg_episode_length': (Array(798.1328, dtype=float32), Array(399.83484, dtype=float32)), 'eval/epoch_eval_time': 4.1166276931762695, 'eval/sps': 31093.411777842593}
I0727 00:21:19.340415 140267183036224 train.py:379] starting iteration 405 2937.605444431305
I0727 00:21:26.518155 140267183036224 train.py:394] {'eval/walltime': 1678.750648021698, 'training/sps': 40252.044659335384, 'training/walltime': 1258.2865722179413, 'training/entropy_loss': Array(-0.00567033, dtype=float32), 'training/policy_loss': Array(-0.00011686, dtype=float32), 'training/total_loss': Array(1927.1462, dtype=float32), 'training/v_loss': Array(1927.152, dtype=float32), 'eval/episode_goal_distance': (Array(0.28221238, dtype=float32), Array(0.04293307, dtype=float32)), 'eval/episode_reward': (Array(-39064.33, dtype=float32), Array(21299.293, dtype=float32)), 'eval/avg_episode_length': (Array(790.3594, dtype=float32), Array(405.46695, dtype=float32)), 'eval/epoch_eval_time': 4.121331214904785, 'eval/sps': 31057.926025719138}
I0727 00:21:26.520583 140267183036224 train.py:379] starting iteration 406 2944.7856135368347
I0727 00:21:33.699539 140267183036224 train.py:394] {'eval/walltime': 1682.865547657013, 'training/sps': 40150.347253291024, 'training/walltime': 1261.347068786621, 'training/entropy_loss': Array(-0.00483629, dtype=float32), 'training/policy_loss': Array(-5.131802e-05, dtype=float32), 'training/total_loss': Array(1905.2234, dtype=float32), 'training/v_loss': Array(1905.2283, dtype=float32), 'eval/episode_goal_distance': (Array(0.28382295, dtype=float32), Array(0.0388131, dtype=float32)), 'eval/episode_reward': (Array(-38916.44, dtype=float32), Array(21379.445, dtype=float32)), 'eval/avg_episode_length': (Array(790.27344, dtype=float32), Array(405.6328, dtype=float32)), 'eval/epoch_eval_time': 4.114899635314941, 'eval/sps': 31106.46949963903}
I0727 00:21:33.701888 140267183036224 train.py:379] starting iteration 407 2951.9669177532196
I0727 00:21:40.872605 140267183036224 train.py:394] {'eval/walltime': 1686.9823832511902, 'training/sps': 40284.52593607718, 'training/walltime': 1264.3973715305328, 'training/entropy_loss': Array(-0.00446019, dtype=float32), 'training/policy_loss': Array(0.00068035, dtype=float32), 'training/total_loss': Array(1961.7646, dtype=float32), 'training/v_loss': Array(1961.7686, dtype=float32), 'eval/episode_goal_distance': (Array(0.28205094, dtype=float32), Array(0.03747195, dtype=float32)), 'eval/episode_reward': (Array(-40306.25, dtype=float32), Array(19609.684, dtype=float32)), 'eval/avg_episode_length': (Array(829.0547, dtype=float32), Array(375.2316, dtype=float32)), 'eval/epoch_eval_time': 4.116835594177246, 'eval/sps': 31091.8415544794}
I0727 00:21:40.875138 140267183036224 train.py:379] starting iteration 408 2959.1401677131653
I0727 00:21:48.061972 140267183036224 train.py:394] {'eval/walltime': 1691.111862897873, 'training/sps': 40238.73568523794, 'training/walltime': 1267.4511454105377, 'training/entropy_loss': Array(-0.00373747, dtype=float32), 'training/policy_loss': Array(-0.00022338, dtype=float32), 'training/total_loss': Array(2120.377, dtype=float32), 'training/v_loss': Array(2120.3809, dtype=float32), 'eval/episode_goal_distance': (Array(0.28880545, dtype=float32), Array(0.03893272, dtype=float32)), 'eval/episode_reward': (Array(-38956.28, dtype=float32), Array(22062.45, dtype=float32)), 'eval/avg_episode_length': (Array(774.6328, dtype=float32), Array(416.3994, dtype=float32)), 'eval/epoch_eval_time': 4.129479646682739, 'eval/sps': 30996.641454044686}
I0727 00:21:48.233050 140267183036224 train.py:379] starting iteration 409 2966.4980754852295
I0727 00:21:55.408969 140267183036224 train.py:394] {'eval/walltime': 1695.2323153018951, 'training/sps': 40268.70663414414, 'training/walltime': 1270.502646446228, 'training/entropy_loss': Array(-0.00376505, dtype=float32), 'training/policy_loss': Array(0.00028356, dtype=float32), 'training/total_loss': Array(1929.447, dtype=float32), 'training/v_loss': Array(1929.4504, dtype=float32), 'eval/episode_goal_distance': (Array(0.28317055, dtype=float32), Array(0.03877563, dtype=float32)), 'eval/episode_reward': (Array(-41796.984, dtype=float32), Array(19881.87, dtype=float32)), 'eval/avg_episode_length': (Array(836.96875, dtype=float32), Array(368.00537, dtype=float32)), 'eval/epoch_eval_time': 4.120452404022217, 'eval/sps': 31064.550066165462}
I0727 00:21:55.411437 140267183036224 train.py:379] starting iteration 410 2973.676466703415
I0727 00:22:02.598261 140267183036224 train.py:394] {'eval/walltime': 1699.3614249229431, 'training/sps': 40234.89704581177, 'training/walltime': 1273.5567116737366, 'training/entropy_loss': Array(-0.00507382, dtype=float32), 'training/policy_loss': Array(0.0002688, dtype=float32), 'training/total_loss': Array(1848.972, dtype=float32), 'training/v_loss': Array(1848.9769, dtype=float32), 'eval/episode_goal_distance': (Array(0.28502837, dtype=float32), Array(0.03003895, dtype=float32)), 'eval/episode_reward': (Array(-38566.633, dtype=float32), Array(22394.908, dtype=float32)), 'eval/avg_episode_length': (Array(759.1328, dtype=float32), Array(426.0726, dtype=float32)), 'eval/epoch_eval_time': 4.129109621047974, 'eval/sps': 30999.41918410813}
I0727 00:22:02.600574 140267183036224 train.py:379] starting iteration 411 2980.8656044006348
I0727 00:22:09.793806 140267183036224 train.py:394] {'eval/walltime': 1703.5048179626465, 'training/sps': 40345.138735416855, 'training/walltime': 1276.6024317741394, 'training/entropy_loss': Array(-0.00611118, dtype=float32), 'training/policy_loss': Array(0.00045413, dtype=float32), 'training/total_loss': Array(1881.2593, dtype=float32), 'training/v_loss': Array(1881.2649, dtype=float32), 'eval/episode_goal_distance': (Array(0.28339428, dtype=float32), Array(0.03761089, dtype=float32)), 'eval/episode_reward': (Array(-40497.938, dtype=float32), Array(20480.441, dtype=float32)), 'eval/avg_episode_length': (Array(813.6172, dtype=float32), Array(387.98773, dtype=float32)), 'eval/epoch_eval_time': 4.143393039703369, 'eval/sps': 30892.555635794495}
I0727 00:22:09.796328 140267183036224 train.py:379] starting iteration 412 2988.061357498169
I0727 00:22:16.955029 140267183036224 train.py:394] {'eval/walltime': 1707.614274263382, 'training/sps': 40350.100897158525, 'training/walltime': 1279.6477773189545, 'training/entropy_loss': Array(-0.00563625, dtype=float32), 'training/policy_loss': Array(0.0002411, dtype=float32), 'training/total_loss': Array(1885.978, dtype=float32), 'training/v_loss': Array(1885.9836, dtype=float32), 'eval/episode_goal_distance': (Array(0.28611743, dtype=float32), Array(0.03735903, dtype=float32)), 'eval/episode_reward': (Array(-38201.555, dtype=float32), Array(22707.66, dtype=float32)), 'eval/avg_episode_length': (Array(751.4844, dtype=float32), Array(430.44293, dtype=float32)), 'eval/epoch_eval_time': 4.109456300735474, 'eval/sps': 31147.672741304417}
I0727 00:22:16.957344 140267183036224 train.py:379] starting iteration 413 2995.2223744392395
I0727 00:22:24.123748 140267183036224 train.py:394] {'eval/walltime': 1711.7322294712067, 'training/sps': 40360.683126600066, 'training/walltime': 1282.6923243999481, 'training/entropy_loss': Array(-0.00529546, dtype=float32), 'training/policy_loss': Array(0.00024081, dtype=float32), 'training/total_loss': Array(2095.6472, dtype=float32), 'training/v_loss': Array(2095.6523, dtype=float32), 'eval/episode_goal_distance': (Array(0.28337073, dtype=float32), Array(0.03430835, dtype=float32)), 'eval/episode_reward': (Array(-39976.21, dtype=float32), Array(20496.057, dtype=float32)), 'eval/avg_episode_length': (Array(805.75, dtype=float32), Array(394.28528, dtype=float32)), 'eval/epoch_eval_time': 4.117955207824707, 'eval/sps': 31083.388123499157}
I0727 00:22:24.126667 140267183036224 train.py:379] starting iteration 414 3002.3916969299316
I0727 00:22:31.284225 140267183036224 train.py:394] {'eval/walltime': 1715.839093208313, 'training/sps': 40331.99222464637, 'training/walltime': 1285.7390372753143, 'training/entropy_loss': Array(-0.00512157, dtype=float32), 'training/policy_loss': Array(0.00016299, dtype=float32), 'training/total_loss': Array(1794.8806, dtype=float32), 'training/v_loss': Array(1794.8855, dtype=float32), 'eval/episode_goal_distance': (Array(0.27736664, dtype=float32), Array(0.03631072, dtype=float32)), 'eval/episode_reward': (Array(-40905.766, dtype=float32), Array(20042.326, dtype=float32)), 'eval/avg_episode_length': (Array(829.08594, dtype=float32), Array(375.1629, dtype=float32)), 'eval/epoch_eval_time': 4.106863737106323, 'eval/sps': 31167.335512862228}
I0727 00:22:31.286727 140267183036224 train.py:379] starting iteration 415 3009.551756620407
I0727 00:22:38.453106 140267183036224 train.py:394] {'eval/walltime': 1719.9582479000092, 'training/sps': 40376.71080536483, 'training/walltime': 1288.7823758125305, 'training/entropy_loss': Array(-0.0055845, dtype=float32), 'training/policy_loss': Array(-0.0001584, dtype=float32), 'training/total_loss': Array(1886.6711, dtype=float32), 'training/v_loss': Array(1886.677, dtype=float32), 'eval/episode_goal_distance': (Array(0.2865785, dtype=float32), Array(0.03697203, dtype=float32)), 'eval/episode_reward': (Array(-39145.55, dtype=float32), Array(22091.562, dtype=float32)), 'eval/avg_episode_length': (Array(774.78906, dtype=float32), Array(416.11154, dtype=float32)), 'eval/epoch_eval_time': 4.119154691696167, 'eval/sps': 31074.336746331985}
I0727 00:22:38.455530 140267183036224 train.py:379] starting iteration 416 3016.7205595970154
I0727 00:22:45.611238 140267183036224 train.py:394] {'eval/walltime': 1724.0636184215546, 'training/sps': 40336.70807067446, 'training/walltime': 1291.8287324905396, 'training/entropy_loss': Array(-0.00462066, dtype=float32), 'training/policy_loss': Array(0.00035828, dtype=float32), 'training/total_loss': Array(11910.408, dtype=float32), 'training/v_loss': Array(11910.412, dtype=float32), 'eval/episode_goal_distance': (Array(0.28877038, dtype=float32), Array(0.03667187, dtype=float32)), 'eval/episode_reward': (Array(-37939.562, dtype=float32), Array(23188.314, dtype=float32)), 'eval/avg_episode_length': (Array(743.59375, dtype=float32), Array(435.04504, dtype=float32)), 'eval/epoch_eval_time': 4.10537052154541, 'eval/sps': 31178.67177353243}
I0727 00:22:45.613662 140267183036224 train.py:379] starting iteration 417 3023.878692150116
I0727 00:22:52.797437 140267183036224 train.py:394] {'eval/walltime': 1728.1990947723389, 'training/sps': 40363.49945903571, 'training/walltime': 1294.8730671405792, 'training/entropy_loss': Array(-0.00580092, dtype=float32), 'training/policy_loss': Array(-5.3273565e-05, dtype=float32), 'training/total_loss': Array(2926.1226, dtype=float32), 'training/v_loss': Array(2926.1284, dtype=float32), 'eval/episode_goal_distance': (Array(0.2826112, dtype=float32), Array(0.03475571, dtype=float32)), 'eval/episode_reward': (Array(-38456.914, dtype=float32), Array(21267.406, dtype=float32)), 'eval/avg_episode_length': (Array(782.4453, dtype=float32), Array(411.14062, dtype=float32)), 'eval/epoch_eval_time': 4.135476350784302, 'eval/sps': 30951.694349726975}
I0727 00:22:52.799981 140267183036224 train.py:379] starting iteration 418 3031.0650103092194
I0727 00:22:59.954957 140267183036224 train.py:394] {'eval/walltime': 1732.3016591072083, 'training/sps': 40328.807916275066, 'training/walltime': 1297.9200205802917, 'training/entropy_loss': Array(-0.00649587, dtype=float32), 'training/policy_loss': Array(0.00044249, dtype=float32), 'training/total_loss': Array(2643.317, dtype=float32), 'training/v_loss': Array(2643.3228, dtype=float32), 'eval/episode_goal_distance': (Array(0.28761923, dtype=float32), Array(0.03525168, dtype=float32)), 'eval/episode_reward': (Array(-38840.844, dtype=float32), Array(22377.656, dtype=float32)), 'eval/avg_episode_length': (Array(767.0156, dtype=float32), Array(421.09488, dtype=float32)), 'eval/epoch_eval_time': 4.102564334869385, 'eval/sps': 31199.998233318427}
I0727 00:22:59.957468 140267183036224 train.py:379] starting iteration 419 3038.222498178482
I0727 00:23:07.120604 140267183036224 train.py:394] {'eval/walltime': 1736.4188966751099, 'training/sps': 40394.31828660951, 'training/walltime': 1300.9620325565338, 'training/entropy_loss': Array(-0.00692104, dtype=float32), 'training/policy_loss': Array(-3.4726843e-05, dtype=float32), 'training/total_loss': Array(2268.7861, dtype=float32), 'training/v_loss': Array(2268.793, dtype=float32), 'eval/episode_goal_distance': (Array(0.2855418, dtype=float32), Array(0.03371729, dtype=float32)), 'eval/episode_reward': (Array(-40492.42, dtype=float32), Array(21205.39, dtype=float32)), 'eval/avg_episode_length': (Array(798.0547, dtype=float32), Array(399.989, dtype=float32)), 'eval/epoch_eval_time': 4.117237567901611, 'eval/sps': 31088.805998930104}
I0727 00:23:07.123004 140267183036224 train.py:379] starting iteration 420 3045.388034105301
I0727 00:23:14.295851 140267183036224 train.py:394] {'eval/walltime': 1740.5367975234985, 'training/sps': 40276.24018356853, 'training/walltime': 1304.0129628181458, 'training/entropy_loss': Array(-0.00704782, dtype=float32), 'training/policy_loss': Array(0.00013569, dtype=float32), 'training/total_loss': Array(2110.3618, dtype=float32), 'training/v_loss': Array(2110.3687, dtype=float32), 'eval/episode_goal_distance': (Array(0.28095144, dtype=float32), Array(0.03490505, dtype=float32)), 'eval/episode_reward': (Array(-38493.28, dtype=float32), Array(21220.783, dtype=float32)), 'eval/avg_episode_length': (Array(782.3203, dtype=float32), Array(411.37646, dtype=float32)), 'eval/epoch_eval_time': 4.117900848388672, 'eval/sps': 31083.798447960737}
I0727 00:23:14.298229 140267183036224 train.py:379] starting iteration 421 3052.5632588863373
I0727 00:23:21.459547 140267183036224 train.py:394] {'eval/walltime': 1744.6517848968506, 'training/sps': 40390.225171342376, 'training/walltime': 1307.0552830696106, 'training/entropy_loss': Array(-0.00681467, dtype=float32), 'training/policy_loss': Array(0.00030713, dtype=float32), 'training/total_loss': Array(1964.8955, dtype=float32), 'training/v_loss': Array(1964.9021, dtype=float32), 'eval/episode_goal_distance': (Array(0.29479325, dtype=float32), Array(0.04333344, dtype=float32)), 'eval/episode_reward': (Array(-40178.14, dtype=float32), Array(22351.459, dtype=float32)), 'eval/avg_episode_length': (Array(782.3906, dtype=float32), Array(411.24374, dtype=float32)), 'eval/epoch_eval_time': 4.114987373352051, 'eval/sps': 31105.806260526082}
I0727 00:23:21.462148 140267183036224 train.py:379] starting iteration 422 3059.7271780967712
I0727 00:23:28.627542 140267183036224 train.py:394] {'eval/walltime': 1748.7661004066467, 'training/sps': 40326.89883473623, 'training/walltime': 1310.1023807525635, 'training/entropy_loss': Array(-0.00722969, dtype=float32), 'training/policy_loss': Array(-0.00053788, dtype=float32), 'training/total_loss': Array(1884.832, dtype=float32), 'training/v_loss': Array(1884.8398, dtype=float32), 'eval/episode_goal_distance': (Array(0.2845835, dtype=float32), Array(0.03970315, dtype=float32)), 'eval/episode_reward': (Array(-41846.44, dtype=float32), Array(20128.219, dtype=float32)), 'eval/avg_episode_length': (Array(829., dtype=float32), Array(375.35187, dtype=float32)), 'eval/epoch_eval_time': 4.114315509796143, 'eval/sps': 31110.885807185503}
I0727 00:23:28.629944 140267183036224 train.py:379] starting iteration 423 3066.89497423172
I0727 00:23:35.792515 140267183036224 train.py:394] {'eval/walltime': 1752.8801510334015, 'training/sps': 40361.590252499926, 'training/walltime': 1313.146859407425, 'training/entropy_loss': Array(-0.00719168, dtype=float32), 'training/policy_loss': Array(8.212724e-05, dtype=float32), 'training/total_loss': Array(2087.629, dtype=float32), 'training/v_loss': Array(2087.6357, dtype=float32), 'eval/episode_goal_distance': (Array(0.28295928, dtype=float32), Array(0.03902938, dtype=float32)), 'eval/episode_reward': (Array(-40646.875, dtype=float32), Array(20865.156, dtype=float32)), 'eval/avg_episode_length': (Array(813.6875, dtype=float32), Array(387.84122, dtype=float32)), 'eval/epoch_eval_time': 4.114050626754761, 'eval/sps': 31112.888880749815}
I0727 00:23:35.795069 140267183036224 train.py:379] starting iteration 424 3074.0600991249084
I0727 00:23:42.962453 140267183036224 train.py:394] {'eval/walltime': 1756.9964969158173, 'training/sps': 40327.952751617355, 'training/walltime': 1316.1938774585724, 'training/entropy_loss': Array(-0.00624195, dtype=float32), 'training/policy_loss': Array(0.00038429, dtype=float32), 'training/total_loss': Array(1939.0917, dtype=float32), 'training/v_loss': Array(1939.0977, dtype=float32), 'eval/episode_goal_distance': (Array(0.28366095, dtype=float32), Array(0.03748852, dtype=float32)), 'eval/episode_reward': (Array(-35012.504, dtype=float32), Array(22159.834, dtype=float32)), 'eval/avg_episode_length': (Array(728.0156, dtype=float32), Array(443.3557, dtype=float32)), 'eval/epoch_eval_time': 4.1163458824157715, 'eval/sps': 31095.54047603023}
I0727 00:23:42.964835 140267183036224 train.py:379] starting iteration 425 3081.2298650741577
I0727 00:23:50.123695 140267183036224 train.py:394] {'eval/walltime': 1761.1104114055634, 'training/sps': 40409.035660892696, 'training/walltime': 1319.2347815036774, 'training/entropy_loss': Array(-0.00486231, dtype=float32), 'training/policy_loss': Array(-0.00011258, dtype=float32), 'training/total_loss': Array(1856.3765, dtype=float32), 'training/v_loss': Array(1856.3813, dtype=float32), 'eval/episode_goal_distance': (Array(0.27789104, dtype=float32), Array(0.03588815, dtype=float32)), 'eval/episode_reward': (Array(-38565.098, dtype=float32), Array(21295.873, dtype=float32)), 'eval/avg_episode_length': (Array(782.5, dtype=float32), Array(411.0374, dtype=float32)), 'eval/epoch_eval_time': 4.113914489746094, 'eval/sps': 31113.918463555623}
I0727 00:23:50.126119 140267183036224 train.py:379] starting iteration 426 3088.3911485671997
I0727 00:23:57.311963 140267183036224 train.py:394] {'eval/walltime': 1765.2456958293915, 'training/sps': 40333.83550306082, 'training/walltime': 1322.2813551425934, 'training/entropy_loss': Array(-0.00424788, dtype=float32), 'training/policy_loss': Array(0.00020059, dtype=float32), 'training/total_loss': Array(1898.343, dtype=float32), 'training/v_loss': Array(1898.347, dtype=float32), 'eval/episode_goal_distance': (Array(0.2822609, dtype=float32), Array(0.03797628, dtype=float32)), 'eval/episode_reward': (Array(-39282.902, dtype=float32), Array(20852.758, dtype=float32)), 'eval/avg_episode_length': (Array(798.10156, dtype=float32), Array(399.89627, dtype=float32)), 'eval/epoch_eval_time': 4.135284423828125, 'eval/sps': 30953.130880779307}
I0727 00:23:57.314361 140267183036224 train.py:379] starting iteration 427 3095.579379081726
I0727 00:24:04.473289 140267183036224 train.py:394] {'eval/walltime': 1769.3566999435425, 'training/sps': 40369.098521558626, 'training/walltime': 1325.3252675533295, 'training/entropy_loss': Array(-0.00351103, dtype=float32), 'training/policy_loss': Array(0.00039562, dtype=float32), 'training/total_loss': Array(1764.9885, dtype=float32), 'training/v_loss': Array(1764.9917, dtype=float32), 'eval/episode_goal_distance': (Array(0.28719732, dtype=float32), Array(0.03417919, dtype=float32)), 'eval/episode_reward': (Array(-38666.492, dtype=float32), Array(21625.305, dtype=float32)), 'eval/avg_episode_length': (Array(774.7422, dtype=float32), Array(416.19785, dtype=float32)), 'eval/epoch_eval_time': 4.111004114151001, 'eval/sps': 31135.945488206933}
I0727 00:24:04.475795 140267183036224 train.py:379] starting iteration 428 3102.7408249378204
I0727 00:24:11.632599 140267183036224 train.py:394] {'eval/walltime': 1773.4634716510773, 'training/sps': 40341.346079020645, 'training/walltime': 1328.3712739944458, 'training/entropy_loss': Array(-0.00292809, dtype=float32), 'training/policy_loss': Array(0.00013997, dtype=float32), 'training/total_loss': Array(2041.1327, dtype=float32), 'training/v_loss': Array(2041.1354, dtype=float32), 'eval/episode_goal_distance': (Array(0.27781144, dtype=float32), Array(0.03942061, dtype=float32)), 'eval/episode_reward': (Array(-34475.746, dtype=float32), Array(22480.502, dtype=float32)), 'eval/avg_episode_length': (Array(720.3125, dtype=float32), Array(447.11273, dtype=float32)), 'eval/epoch_eval_time': 4.10677170753479, 'eval/sps': 31168.033948698783}
I0727 00:24:11.635056 140267183036224 train.py:379] starting iteration 429 3109.9000866413116
I0727 00:24:18.796600 140267183036224 train.py:394] {'eval/walltime': 1777.577009677887, 'training/sps': 40367.35003090233, 'training/walltime': 1331.4153182506561, 'training/entropy_loss': Array(-0.00255777, dtype=float32), 'training/policy_loss': Array(0.00035523, dtype=float32), 'training/total_loss': Array(1980.3988, dtype=float32), 'training/v_loss': Array(1980.4009, dtype=float32), 'eval/episode_goal_distance': (Array(0.2849862, dtype=float32), Array(0.0370128, dtype=float32)), 'eval/episode_reward': (Array(-40829.46, dtype=float32), Array(21062.162, dtype=float32)), 'eval/avg_episode_length': (Array(805.65625, dtype=float32), Array(394.47513, dtype=float32)), 'eval/epoch_eval_time': 4.113538026809692, 'eval/sps': 31116.765948380464}
I0727 00:24:18.799132 140267183036224 train.py:379] starting iteration 430 3117.064162015915
I0727 00:24:25.973526 140267183036224 train.py:394] {'eval/walltime': 1781.6973643302917, 'training/sps': 40287.063972264405, 'training/walltime': 1334.4654288291931, 'training/entropy_loss': Array(-0.00175956, dtype=float32), 'training/policy_loss': Array(0.00032353, dtype=float32), 'training/total_loss': Array(1899.8228, dtype=float32), 'training/v_loss': Array(1899.8242, dtype=float32), 'eval/episode_goal_distance': (Array(0.28150988, dtype=float32), Array(0.03907476, dtype=float32)), 'eval/episode_reward': (Array(-41445.11, dtype=float32), Array(19601.477, dtype=float32)), 'eval/avg_episode_length': (Array(836.91406, dtype=float32), Array(368.12878, dtype=float32)), 'eval/epoch_eval_time': 4.120354652404785, 'eval/sps': 31065.287043991386}
I0727 00:24:25.975972 140267183036224 train.py:379] starting iteration 431 3124.241001844406
I0727 00:24:33.140476 140267183036224 train.py:394] {'eval/walltime': 1785.8120532035828, 'training/sps': 40343.6639041927, 'training/walltime': 1337.5112602710724, 'training/entropy_loss': Array(-0.00244469, dtype=float32), 'training/policy_loss': Array(0.00034944, dtype=float32), 'training/total_loss': Array(1827.9425, dtype=float32), 'training/v_loss': Array(1827.9447, dtype=float32), 'eval/episode_goal_distance': (Array(0.28479546, dtype=float32), Array(0.03838471, dtype=float32)), 'eval/episode_reward': (Array(-37355.367, dtype=float32), Array(22501.479, dtype=float32)), 'eval/avg_episode_length': (Array(751.47656, dtype=float32), Array(430.456, dtype=float32)), 'eval/epoch_eval_time': 4.114688873291016, 'eval/sps': 31108.062830914085}
I0727 00:24:33.143001 140267183036224 train.py:379] starting iteration 432 3131.408030986786
I0727 00:24:40.315763 140267183036224 train.py:394] {'eval/walltime': 1789.929556131363, 'training/sps': 40299.13504136557, 'training/walltime': 1340.5604572296143, 'training/entropy_loss': Array(-0.0020712, dtype=float32), 'training/policy_loss': Array(0.00039882, dtype=float32), 'training/total_loss': Array(1791.955, dtype=float32), 'training/v_loss': Array(1791.9564, dtype=float32), 'eval/episode_goal_distance': (Array(0.28905183, dtype=float32), Array(0.03489433, dtype=float32)), 'eval/episode_reward': (Array(-37257.746, dtype=float32), Array(22433.262, dtype=float32)), 'eval/avg_episode_length': (Array(743.5703, dtype=float32), Array(435.08456, dtype=float32)), 'eval/epoch_eval_time': 4.117502927780151, 'eval/sps': 31086.802424936708}
I0727 00:24:40.318215 140267183036224 train.py:379] starting iteration 433 3138.583245038986
I0727 00:24:47.480756 140267183036224 train.py:394] {'eval/walltime': 1794.044718503952, 'training/sps': 40375.708109453, 'training/walltime': 1343.60387134552, 'training/entropy_loss': Array(-0.00081818, dtype=float32), 'training/policy_loss': Array(0.00044131, dtype=float32), 'training/total_loss': Array(13267.275, dtype=float32), 'training/v_loss': Array(13267.275, dtype=float32), 'eval/episode_goal_distance': (Array(0.28162462, dtype=float32), Array(0.0377275, dtype=float32)), 'eval/episode_reward': (Array(-38023.59, dtype=float32), Array(23401.5, dtype=float32)), 'eval/avg_episode_length': (Array(743.64844, dtype=float32), Array(434.95224, dtype=float32)), 'eval/epoch_eval_time': 4.115162372589111, 'eval/sps': 31104.48347132097}
I0727 00:24:47.483142 140267183036224 train.py:379] starting iteration 434 3145.748172044754
I0727 00:24:54.652660 140267183036224 train.py:394] {'eval/walltime': 1798.1628530025482, 'training/sps': 40323.529194723444, 'training/walltime': 1346.6512236595154, 'training/entropy_loss': Array(-0.00228583, dtype=float32), 'training/policy_loss': Array(0.00017758, dtype=float32), 'training/total_loss': Array(2524.3142, dtype=float32), 'training/v_loss': Array(2524.3164, dtype=float32), 'eval/episode_goal_distance': (Array(0.2828018, dtype=float32), Array(0.03879903, dtype=float32)), 'eval/episode_reward': (Array(-39737.68, dtype=float32), Array(21768.082, dtype=float32)), 'eval/avg_episode_length': (Array(790.1406, dtype=float32), Array(405.88962, dtype=float32)), 'eval/epoch_eval_time': 4.118134498596191, 'eval/sps': 31082.034849428357}
I0727 00:24:54.655180 140267183036224 train.py:379] starting iteration 435 3152.920210123062
I0727 00:25:01.824394 140267183036224 train.py:394] {'eval/walltime': 1802.2820510864258, 'training/sps': 40340.66404494707, 'training/walltime': 1349.6972815990448, 'training/entropy_loss': Array(-0.00302651, dtype=float32), 'training/policy_loss': Array(0.00030821, dtype=float32), 'training/total_loss': Array(2357.497, dtype=float32), 'training/v_loss': Array(2357.4998, dtype=float32), 'eval/episode_goal_distance': (Array(0.28823957, dtype=float32), Array(0.03852657, dtype=float32)), 'eval/episode_reward': (Array(-39066.863, dtype=float32), Array(22493.32, dtype=float32)), 'eval/avg_episode_length': (Array(766.90625, dtype=float32), Array(421.29276, dtype=float32)), 'eval/epoch_eval_time': 4.1191980838775635, 'eval/sps': 31074.009405128814}
I0727 00:25:01.826798 140267183036224 train.py:379] starting iteration 436 3160.0918283462524
I0727 00:25:08.990807 140267183036224 train.py:394] {'eval/walltime': 1806.3943955898285, 'training/sps': 40316.44784514611, 'training/walltime': 1352.7451691627502, 'training/entropy_loss': Array(-0.00390177, dtype=float32), 'training/policy_loss': Array(0.00018306, dtype=float32), 'training/total_loss': Array(2195.0073, dtype=float32), 'training/v_loss': Array(2195.0112, dtype=float32), 'eval/episode_goal_distance': (Array(0.28522223, dtype=float32), Array(0.03814009, dtype=float32)), 'eval/episode_reward': (Array(-41568.32, dtype=float32), Array(20576.428, dtype=float32)), 'eval/avg_episode_length': (Array(821.3672, dtype=float32), Array(381.6743, dtype=float32)), 'eval/epoch_eval_time': 4.11234450340271, 'eval/sps': 31125.796949668966}
I0727 00:25:08.993216 140267183036224 train.py:379] starting iteration 437 3167.258246898651
I0727 00:25:16.154060 140267183036224 train.py:394] {'eval/walltime': 1810.5070116519928, 'training/sps': 40362.45000527441, 'training/walltime': 1355.7895829677582, 'training/entropy_loss': Array(-0.00371632, dtype=float32), 'training/policy_loss': Array(0.00016463, dtype=float32), 'training/total_loss': Array(1893.2542, dtype=float32), 'training/v_loss': Array(1893.2576, dtype=float32), 'eval/episode_goal_distance': (Array(0.28000355, dtype=float32), Array(0.04105858, dtype=float32)), 'eval/episode_reward': (Array(-37236.086, dtype=float32), Array(21545.125, dtype=float32)), 'eval/avg_episode_length': (Array(767.03125, dtype=float32), Array(421.06677, dtype=float32)), 'eval/epoch_eval_time': 4.112616062164307, 'eval/sps': 31123.741692687618}
I0727 00:25:16.156424 140267183036224 train.py:379] starting iteration 438 3174.4214544296265
I0727 00:25:23.316754 140267183036224 train.py:394] {'eval/walltime': 1814.6174635887146, 'training/sps': 40342.40390896919, 'training/walltime': 1358.8355095386505, 'training/entropy_loss': Array(-0.00295263, dtype=float32), 'training/policy_loss': Array(0.0003147, dtype=float32), 'training/total_loss': Array(2203.5684, dtype=float32), 'training/v_loss': Array(2203.571, dtype=float32), 'eval/episode_goal_distance': (Array(0.2831083, dtype=float32), Array(0.03894536, dtype=float32)), 'eval/episode_reward': (Array(-37953.14, dtype=float32), Array(22371.324, dtype=float32)), 'eval/avg_episode_length': (Array(759.2578, dtype=float32), Array(425.8512, dtype=float32)), 'eval/epoch_eval_time': 4.110451936721802, 'eval/sps': 31140.128134446335}
I0727 00:25:23.319390 140267183036224 train.py:379] starting iteration 439 3181.5844190120697
I0727 00:25:30.486533 140267183036224 train.py:394] {'eval/walltime': 1818.7343637943268, 'training/sps': 40338.722268678284, 'training/walltime': 1361.881714105606, 'training/entropy_loss': Array(-0.00380908, dtype=float32), 'training/policy_loss': Array(0.00030187, dtype=float32), 'training/total_loss': Array(1996.3518, dtype=float32), 'training/v_loss': Array(1996.3552, dtype=float32), 'eval/episode_goal_distance': (Array(0.28145072, dtype=float32), Array(0.03921774, dtype=float32)), 'eval/episode_reward': (Array(-40228.312, dtype=float32), Array(20383.19, dtype=float32)), 'eval/avg_episode_length': (Array(813.5625, dtype=float32), Array(388.10114, dtype=float32)), 'eval/epoch_eval_time': 4.116900205612183, 'eval/sps': 31091.353593052765}
I0727 00:25:30.489052 140267183036224 train.py:379] starting iteration 440 3188.754081726074
I0727 00:25:37.645524 140267183036224 train.py:394] {'eval/walltime': 1822.8417353630066, 'training/sps': 40354.53344589468, 'training/walltime': 1364.9267251491547, 'training/entropy_loss': Array(-0.0034596, dtype=float32), 'training/policy_loss': Array(0.00029074, dtype=float32), 'training/total_loss': Array(1948.41, dtype=float32), 'training/v_loss': Array(1948.4131, dtype=float32), 'eval/episode_goal_distance': (Array(0.28531933, dtype=float32), Array(0.03587217, dtype=float32)), 'eval/episode_reward': (Array(-40845.81, dtype=float32), Array(20820.387, dtype=float32)), 'eval/avg_episode_length': (Array(813.5703, dtype=float32), Array(388.0852, dtype=float32)), 'eval/epoch_eval_time': 4.10737156867981, 'eval/sps': 31163.48201269303}
I0727 00:25:37.650303 140267183036224 train.py:379] starting iteration 441 3195.915316581726
I0727 00:25:44.811585 140267183036224 train.py:394] {'eval/walltime': 1826.9550726413727, 'training/sps': 40374.69281197306, 'training/walltime': 1367.9702157974243, 'training/entropy_loss': Array(-0.00392969, dtype=float32), 'training/policy_loss': Array(-6.530194e-05, dtype=float32), 'training/total_loss': Array(2173.6704, dtype=float32), 'training/v_loss': Array(2173.6746, dtype=float32), 'eval/episode_goal_distance': (Array(0.28279674, dtype=float32), Array(0.04058203, dtype=float32)), 'eval/episode_reward': (Array(-38589.56, dtype=float32), Array(21632.748, dtype=float32)), 'eval/avg_episode_length': (Array(782.4219, dtype=float32), Array(411.18478, dtype=float32)), 'eval/epoch_eval_time': 4.113337278366089, 'eval/sps': 31118.28457958218}
I0727 00:25:44.813952 140267183036224 train.py:379] starting iteration 442 3203.0789818763733
I0727 00:25:51.994189 140267183036224 train.py:394] {'eval/walltime': 1831.0837306976318, 'training/sps': 40320.087568950104, 'training/walltime': 1371.0178282260895, 'training/entropy_loss': Array(-0.00352258, dtype=float32), 'training/policy_loss': Array(-0.00055152, dtype=float32), 'training/total_loss': Array(1894.7666, dtype=float32), 'training/v_loss': Array(1894.7706, dtype=float32), 'eval/episode_goal_distance': (Array(0.28076363, dtype=float32), Array(0.03463481, dtype=float32)), 'eval/episode_reward': (Array(-38877.86, dtype=float32), Array(21084.684, dtype=float32)), 'eval/avg_episode_length': (Array(790.3203, dtype=float32), Array(405.54224, dtype=float32)), 'eval/epoch_eval_time': 4.128658056259155, 'eval/sps': 31002.809691625735}
I0727 00:25:51.997406 140267183036224 train.py:379] starting iteration 443 3210.262419939041
I0727 00:25:59.168398 140267183036224 train.py:394] {'eval/walltime': 1835.2058169841766, 'training/sps': 40359.49791901117, 'training/walltime': 1374.0624647140503, 'training/entropy_loss': Array(-0.00242694, dtype=float32), 'training/policy_loss': Array(0.00033328, dtype=float32), 'training/total_loss': Array(1992.3258, dtype=float32), 'training/v_loss': Array(1992.3278, dtype=float32), 'eval/episode_goal_distance': (Array(0.28485522, dtype=float32), Array(0.03788131, dtype=float32)), 'eval/episode_reward': (Array(-42355.33, dtype=float32), Array(19882.857, dtype=float32)), 'eval/avg_episode_length': (Array(836.7656, dtype=float32), Array(368.46347, dtype=float32)), 'eval/epoch_eval_time': 4.1220862865448, 'eval/sps': 31052.23692619295}
I0727 00:25:59.170717 140267183036224 train.py:379] starting iteration 444 3217.4357471466064
I0727 00:26:06.340029 140267183036224 train.py:394] {'eval/walltime': 1839.324198961258, 'training/sps': 40329.783037574125, 'training/walltime': 1377.1093444824219, 'training/entropy_loss': Array(-0.00291494, dtype=float32), 'training/policy_loss': Array(0.0004612, dtype=float32), 'training/total_loss': Array(1735.541, dtype=float32), 'training/v_loss': Array(1735.5436, dtype=float32), 'eval/episode_goal_distance': (Array(0.28389126, dtype=float32), Array(0.03993326, dtype=float32)), 'eval/episode_reward': (Array(-40444.633, dtype=float32), Array(20755.16, dtype=float32)), 'eval/avg_episode_length': (Array(813.6328, dtype=float32), Array(387.95514, dtype=float32)), 'eval/epoch_eval_time': 4.118381977081299, 'eval/sps': 31080.16709288188}
I0727 00:26:06.342574 140267183036224 train.py:379] starting iteration 445 3224.6076045036316
I0727 00:26:13.514986 140267183036224 train.py:394] {'eval/walltime': 1843.4525775909424, 'training/sps': 40420.992906822474, 'training/walltime': 1380.149348974228, 'training/entropy_loss': Array(-0.00334157, dtype=float32), 'training/policy_loss': Array(-3.7396821e-06, dtype=float32), 'training/total_loss': Array(1807.1738, dtype=float32), 'training/v_loss': Array(1807.1771, dtype=float32), 'eval/episode_goal_distance': (Array(0.2771693, dtype=float32), Array(0.03804124, dtype=float32)), 'eval/episode_reward': (Array(-40820.883, dtype=float32), Array(19856.406, dtype=float32)), 'eval/avg_episode_length': (Array(829.0547, dtype=float32), Array(375.23172, dtype=float32)), 'eval/epoch_eval_time': 4.128378629684448, 'eval/sps': 31004.90809627693}
I0727 00:26:13.517269 140267183036224 train.py:379] starting iteration 446 3231.782298564911
I0727 00:26:20.679441 140267183036224 train.py:394] {'eval/walltime': 1847.566067457199, 'training/sps': 40356.075430260586, 'training/walltime': 1383.19424366951, 'training/entropy_loss': Array(-0.00420841, dtype=float32), 'training/policy_loss': Array(-7.225208e-05, dtype=float32), 'training/total_loss': Array(1823.3046, dtype=float32), 'training/v_loss': Array(1823.3088, dtype=float32), 'eval/episode_goal_distance': (Array(0.28097373, dtype=float32), Array(0.03586646, dtype=float32)), 'eval/episode_reward': (Array(-37937.184, dtype=float32), Array(21857.326, dtype=float32)), 'eval/avg_episode_length': (Array(766.9844, dtype=float32), Array(421.15128, dtype=float32)), 'eval/epoch_eval_time': 4.113489866256714, 'eval/sps': 31117.130262066337}
I0727 00:26:20.681763 140267183036224 train.py:379] starting iteration 447 3238.9467928409576
I0727 00:26:27.857525 140267183036224 train.py:394] {'eval/walltime': 1851.6902437210083, 'training/sps': 40320.907701422424, 'training/walltime': 1386.2417941093445, 'training/entropy_loss': Array(-0.00361662, dtype=float32), 'training/policy_loss': Array(0.00011743, dtype=float32), 'training/total_loss': Array(1798.0581, dtype=float32), 'training/v_loss': Array(1798.0618, dtype=float32), 'eval/episode_goal_distance': (Array(0.27744102, dtype=float32), Array(0.03620236, dtype=float32)), 'eval/episode_reward': (Array(-37796.53, dtype=float32), Array(20964.64, dtype=float32)), 'eval/avg_episode_length': (Array(782.46094, dtype=float32), Array(411.11084, dtype=float32)), 'eval/epoch_eval_time': 4.124176263809204, 'eval/sps': 31036.50082156664}
I0727 00:26:27.859810 140267183036224 train.py:379] starting iteration 448 3246.124840736389
I0727 00:26:35.026205 140267183036224 train.py:394] {'eval/walltime': 1855.8067045211792, 'training/sps': 40341.7281544826, 'training/walltime': 1389.2877717018127, 'training/entropy_loss': Array(-0.00299723, dtype=float32), 'training/policy_loss': Array(-3.4908502e-05, dtype=float32), 'training/total_loss': Array(1912.9514, dtype=float32), 'training/v_loss': Array(1912.9543, dtype=float32), 'eval/episode_goal_distance': (Array(0.281568, dtype=float32), Array(0.03789087, dtype=float32)), 'eval/episode_reward': (Array(-38792.188, dtype=float32), Array(21887.273, dtype=float32)), 'eval/avg_episode_length': (Array(774.6797, dtype=float32), Array(416.31302, dtype=float32)), 'eval/epoch_eval_time': 4.116460800170898, 'eval/sps': 31094.6723930144}
I0727 00:26:35.029778 140267183036224 train.py:379] starting iteration 449 3253.2947909832
I0727 00:26:42.214617 140267183036224 train.py:394] {'eval/walltime': 1859.9368226528168, 'training/sps': 40283.33260279831, 'training/walltime': 1392.338164806366, 'training/entropy_loss': Array(-0.00300751, dtype=float32), 'training/policy_loss': Array(0.00013489, dtype=float32), 'training/total_loss': Array(1808.9915, dtype=float32), 'training/v_loss': Array(1808.9944, dtype=float32), 'eval/episode_goal_distance': (Array(0.28703672, dtype=float32), Array(0.0352126, dtype=float32)), 'eval/episode_reward': (Array(-41741.15, dtype=float32), Array(18942.9, dtype=float32)), 'eval/avg_episode_length': (Array(844.5156, dtype=float32), Array(361.31357, dtype=float32)), 'eval/epoch_eval_time': 4.130118131637573, 'eval/sps': 30991.849608245608}
I0727 00:26:42.216907 140267183036224 train.py:379] starting iteration 450 3260.4819374084473
I0727 00:26:49.400820 140267183036224 train.py:394] {'eval/walltime': 1864.0585243701935, 'training/sps': 40180.882043343605, 'training/walltime': 1395.3963356018066, 'training/entropy_loss': Array(-0.00360226, dtype=float32), 'training/policy_loss': Array(0.00058702, dtype=float32), 'training/total_loss': Array(12817.338, dtype=float32), 'training/v_loss': Array(12817.341, dtype=float32), 'eval/episode_goal_distance': (Array(0.2844987, dtype=float32), Array(0.03795044, dtype=float32)), 'eval/episode_reward': (Array(-39359.188, dtype=float32), Array(20681.914, dtype=float32)), 'eval/avg_episode_length': (Array(797.9453, dtype=float32), Array(400.2054, dtype=float32)), 'eval/epoch_eval_time': 4.121701717376709, 'eval/sps': 31055.134208369316}
I0727 00:26:49.403165 140267183036224 train.py:379] starting iteration 451 3267.6681945323944
I0727 00:26:56.591040 140267183036224 train.py:394] {'eval/walltime': 1868.18390250206, 'training/sps': 40178.21015702597, 'training/walltime': 1398.4547097682953, 'training/entropy_loss': Array(-0.00526383, dtype=float32), 'training/policy_loss': Array(-3.1786476e-07, dtype=float32), 'training/total_loss': Array(2511.5237, dtype=float32), 'training/v_loss': Array(2511.5288, dtype=float32), 'eval/episode_goal_distance': (Array(0.2854569, dtype=float32), Array(0.03255487, dtype=float32)), 'eval/episode_reward': (Array(-40403.05, dtype=float32), Array(20669.21, dtype=float32)), 'eval/avg_episode_length': (Array(805.6953, dtype=float32), Array(394.3963, dtype=float32)), 'eval/epoch_eval_time': 4.125378131866455, 'eval/sps': 31027.458794932005}
I0727 00:26:56.593433 140267183036224 train.py:379] starting iteration 452 3274.8584628105164
I0727 00:27:03.779643 140267183036224 train.py:394] {'eval/walltime': 1872.305589914322, 'training/sps': 40150.63501197867, 'training/walltime': 1401.5151844024658, 'training/entropy_loss': Array(-0.0065728, dtype=float32), 'training/policy_loss': Array(0.00016168, dtype=float32), 'training/total_loss': Array(2428.6592, dtype=float32), 'training/v_loss': Array(2428.6655, dtype=float32), 'eval/episode_goal_distance': (Array(0.28980237, dtype=float32), Array(0.03428407, dtype=float32)), 'eval/episode_reward': (Array(-39488.152, dtype=float32), Array(23225.492, dtype=float32)), 'eval/avg_episode_length': (Array(759.2344, dtype=float32), Array(425.8932, dtype=float32)), 'eval/epoch_eval_time': 4.121687412261963, 'eval/sps': 31055.241991229555}
I0727 00:27:03.781825 140267183036224 train.py:379] starting iteration 453 3282.0468549728394
I0727 00:27:10.962617 140267183036224 train.py:394] {'eval/walltime': 1876.426570892334, 'training/sps': 40212.737104615626, 'training/walltime': 1404.5709326267242, 'training/entropy_loss': Array(-0.00560155, dtype=float32), 'training/policy_loss': Array(-0.00035842, dtype=float32), 'training/total_loss': Array(2183.5034, dtype=float32), 'training/v_loss': Array(2183.5095, dtype=float32), 'eval/episode_goal_distance': (Array(0.2892145, dtype=float32), Array(0.04157294, dtype=float32)), 'eval/episode_reward': (Array(-36223.31, dtype=float32), Array(23996.582, dtype=float32)), 'eval/avg_episode_length': (Array(712.53125, dtype=float32), Array(450.8285, dtype=float32)), 'eval/epoch_eval_time': 4.120980978012085, 'eval/sps': 31060.565599054466}
I0727 00:27:10.964968 140267183036224 train.py:379] starting iteration 454 3289.229998111725
I0727 00:27:18.150000 140267183036224 train.py:394] {'eval/walltime': 1880.5476472377777, 'training/sps': 40158.20579811302, 'training/walltime': 1407.6308302879333, 'training/entropy_loss': Array(-0.00524468, dtype=float32), 'training/policy_loss': Array(0.00011488, dtype=float32), 'training/total_loss': Array(1810.7915, dtype=float32), 'training/v_loss': Array(1810.7966, dtype=float32), 'eval/episode_goal_distance': (Array(0.28431362, dtype=float32), Array(0.04215094, dtype=float32)), 'eval/episode_reward': (Array(-39763.305, dtype=float32), Array(21645.498, dtype=float32)), 'eval/avg_episode_length': (Array(790.3281, dtype=float32), Array(405.52716, dtype=float32)), 'eval/epoch_eval_time': 4.121076345443726, 'eval/sps': 31059.84681441711}
I0727 00:27:18.152355 140267183036224 train.py:379] starting iteration 455 3296.417384624481
I0727 00:27:25.338611 140267183036224 train.py:394] {'eval/walltime': 1884.6760456562042, 'training/sps': 40238.00056992557, 'training/walltime': 1410.6846599578857, 'training/entropy_loss': Array(-0.00575625, dtype=float32), 'training/policy_loss': Array(0.00024765, dtype=float32), 'training/total_loss': Array(2092.7815, dtype=float32), 'training/v_loss': Array(2092.787, dtype=float32), 'eval/episode_goal_distance': (Array(0.28427103, dtype=float32), Array(0.03330818, dtype=float32)), 'eval/episode_reward': (Array(-38808.76, dtype=float32), Array(21972.932, dtype=float32)), 'eval/avg_episode_length': (Array(774.6406, dtype=float32), Array(416.38495, dtype=float32)), 'eval/epoch_eval_time': 4.128398418426514, 'eval/sps': 31004.75947977559}
I0727 00:27:25.340871 140267183036224 train.py:379] starting iteration 456 3303.6059007644653
I0727 00:27:32.523144 140267183036224 train.py:394] {'eval/walltime': 1888.8023715019226, 'training/sps': 40264.336956786676, 'training/walltime': 1413.7364921569824, 'training/entropy_loss': Array(-0.00582404, dtype=float32), 'training/policy_loss': Array(-0.00033053, dtype=float32), 'training/total_loss': Array(2055.1045, dtype=float32), 'training/v_loss': Array(2055.1108, dtype=float32), 'eval/episode_goal_distance': (Array(0.28592157, dtype=float32), Array(0.03759713, dtype=float32)), 'eval/episode_reward': (Array(-39742.59, dtype=float32), Array(21713.541, dtype=float32)), 'eval/avg_episode_length': (Array(790.22656, dtype=float32), Array(405.72354, dtype=float32)), 'eval/epoch_eval_time': 4.126325845718384, 'eval/sps': 31020.33256360914}
I0727 00:27:32.525681 140267183036224 train.py:379] starting iteration 457 3310.7907104492188
I0727 00:27:39.725829 140267183036224 train.py:394] {'eval/walltime': 1892.949891090393, 'training/sps': 40307.15912056523, 'training/walltime': 1416.785082101822, 'training/entropy_loss': Array(-0.0054634, dtype=float32), 'training/policy_loss': Array(0.0002233, dtype=float32), 'training/total_loss': Array(2024.8408, dtype=float32), 'training/v_loss': Array(2024.8462, dtype=float32), 'eval/episode_goal_distance': (Array(0.28455085, dtype=float32), Array(0.03805896, dtype=float32)), 'eval/episode_reward': (Array(-42106.79, dtype=float32), Array(19973.08, dtype=float32)), 'eval/avg_episode_length': (Array(836.85156, dtype=float32), Array(368.26984, dtype=float32)), 'eval/epoch_eval_time': 4.147519588470459, 'eval/sps': 30861.819280087937}
I0727 00:27:39.728298 140267183036224 train.py:379] starting iteration 458 3317.993327856064
