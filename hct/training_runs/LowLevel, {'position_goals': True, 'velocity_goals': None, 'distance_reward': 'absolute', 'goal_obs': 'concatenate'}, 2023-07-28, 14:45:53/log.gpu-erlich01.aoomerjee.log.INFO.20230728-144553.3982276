I0728 14:45:53.704112 140071325378368 low_level_env.py:191] Initialising environment...
I0728 14:45:54.039795 140071325378368 low_level_env.py:299] Environment initialised.
I0728 14:45:54.048738 140071325378368 train.py:118] JAX is running on GPU.
I0728 14:45:54.048813 140071325378368 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 14:45:57.922231 140071325378368 train.py:367] Running initial eval
I0728 14:46:09.905103 140071325378368 train.py:373] {'eval/walltime': 11.837650775909424, 'eval/episode_distance_from_origin': Array([1.6427811 , 0.82216835], dtype=float32), 'eval/episode_forward_reward': Array([-7.3783727, 23.02631  ], dtype=float32), 'eval/episode_reward': Array([512.20654, 382.0578 ], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-122.3136,   91.2117], dtype=float32), 'eval/episode_reward_forward': Array([-7.3783727, 23.02631  ], dtype=float32), 'eval/episode_reward_survive': Array([641.89844, 478.11844], dtype=float32), 'eval/episode_x_position': Array([-0.37057436,  1.1523817 ], dtype=float32), 'eval/episode_x_velocity': Array([0.02743337, 0.43630973], dtype=float32), 'eval/episode_y_position': Array([0.09432691, 1.1454856 ], dtype=float32), 'eval/episode_y_velocity': Array([0.00779693, 0.39299652], dtype=float32), 'eval/avg_episode_length': Array([641.89844, 478.11844], dtype=float32), 'eval/epoch_eval_time': 11.837650775909424, 'eval/sps': 10812.956254841572}
I0728 14:46:09.907173 140071325378368 train.py:379] starting iteration 0, 0 steps, 15.858466386795044
I0728 14:49:06.463223 140071325378368 train.py:394] {'eval/walltime': 15.331683874130249, 'training/sps': 321894.26676827326, 'training/walltime': 173.0555830001831, 'training/entropy_loss': Array(0.01615423, dtype=float32), 'training/policy_loss': Array(-0.0001341, dtype=float32), 'training/total_loss': Array(39.41494, dtype=float32), 'training/v_loss': Array(39.39892, dtype=float32), 'eval/episode_distance_from_origin': Array([55.673954, 30.987755], dtype=float32), 'eval/episode_forward_reward': Array([1023.43555,  580.4658 ], dtype=float32), 'eval/episode_reward': Array([1683.9604,  947.0599], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-99.584465,  56.116627], dtype=float32), 'eval/episode_reward_forward': Array([1023.43555,  580.4658 ], dtype=float32), 'eval/episode_reward_survive': Array([760.1094 , 424.42734], dtype=float32), 'eval/episode_x_position': Array([51.174816, 29.022202], dtype=float32), 'eval/episode_x_velocity': Array([1.1957312, 0.7264421], dtype=float32), 'eval/episode_y_position': Array([21.145748, 12.291332], dtype=float32), 'eval/episode_y_velocity': Array([0.40888333, 0.40909448], dtype=float32), 'eval/avg_episode_length': Array([760.1094 , 424.42734], dtype=float32), 'eval/epoch_eval_time': 3.494033098220825, 'eval/sps': 36633.88308060908}
I0728 14:49:06.492052 140071325378368 train.py:379] starting iteration 1, 55705600 steps, 192.44334650039673
I0728 14:51:55.486062 140071325378368 train.py:394] {'eval/walltime': 18.90436577796936, 'training/sps': 336763.18344496435, 'training/walltime': 338.47033953666687, 'training/entropy_loss': Array(0.02398097, dtype=float32), 'training/policy_loss': Array(0.00278928, dtype=float32), 'training/total_loss': Array(23.460793, dtype=float32), 'training/v_loss': Array(23.434023, dtype=float32), 'eval/episode_distance_from_origin': Array([38.381954, 26.553156], dtype=float32), 'eval/episode_forward_reward': Array([758.20447, 540.5617 ], dtype=float32), 'eval/episode_reward': Array([1334.8031,  947.1038], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-89.0887  ,  63.111855], dtype=float32), 'eval/episode_reward_forward': Array([758.20447, 540.5617 ], dtype=float32), 'eval/episode_reward_survive': Array([665.6875 , 470.04477], dtype=float32), 'eval/episode_x_position': Array([37.912323, 27.033318], dtype=float32), 'eval/episode_x_velocity': Array([0.80871034, 0.7348089 ], dtype=float32), 'eval/episode_y_position': Array([1.386581 , 2.7618313], dtype=float32), 'eval/episode_y_velocity': Array([0.11902394, 0.46067977], dtype=float32), 'eval/avg_episode_length': Array([665.6875 , 470.04477], dtype=float32), 'eval/epoch_eval_time': 3.5726819038391113, 'eval/sps': 35827.42697088552}
I0728 14:51:55.488818 140071325378368 train.py:379] starting iteration 2, 111411200 steps, 361.44011330604553
I0728 14:54:47.106851 140071325378368 train.py:394] {'eval/walltime': 22.483006238937378, 'training/sps': 331514.7419341095, 'training/walltime': 506.503892660141, 'training/entropy_loss': Array(0.02752932, dtype=float32), 'training/policy_loss': Array(0.00345424, dtype=float32), 'training/total_loss': Array(14.415456, dtype=float32), 'training/v_loss': Array(14.384472, dtype=float32), 'eval/episode_distance_from_origin': Array([49.04818 , 31.732418], dtype=float32), 'eval/episode_forward_reward': Array([965.72424, 638.2525 ], dtype=float32), 'eval/episode_reward': Array([1569.5674, 1032.9166], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-93.82094 ,  61.875835], dtype=float32), 'eval/episode_reward_forward': Array([965.72424, 638.2525 ], dtype=float32), 'eval/episode_reward_survive': Array([697.66406, 456.76956], dtype=float32), 'eval/episode_x_position': Array([48.293694, 31.904121], dtype=float32), 'eval/episode_x_velocity': Array([1.0418458, 0.7850319], dtype=float32), 'eval/episode_y_position': Array([6.3250017, 4.6917534], dtype=float32), 'eval/episode_y_velocity': Array([0.14437225, 0.43649197], dtype=float32), 'eval/avg_episode_length': Array([697.66406, 456.76956], dtype=float32), 'eval/epoch_eval_time': 3.5786404609680176, 'eval/sps': 35767.773095980745}
I0728 14:54:47.109427 140071325378368 train.py:379] starting iteration 3, 167116800 steps, 533.0607221126556
I0728 14:57:39.908601 140071325378368 train.py:394] {'eval/walltime': 26.09038758277893, 'training/sps': 329256.75856811815, 'training/walltime': 675.6897895336151, 'training/entropy_loss': Array(0.03017422, dtype=float32), 'training/policy_loss': Array(0.0045731, dtype=float32), 'training/total_loss': Array(12.955568, dtype=float32), 'training/v_loss': Array(12.92082, dtype=float32), 'eval/episode_distance_from_origin': Array([55.919815, 31.669369], dtype=float32), 'eval/episode_forward_reward': Array([1087.3223 ,  627.33673], dtype=float32), 'eval/episode_reward': Array([1738.355  ,  997.81354], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-101.185875,   58.234077], dtype=float32), 'eval/episode_reward_forward': Array([1087.3223 ,  627.33673], dtype=float32), 'eval/episode_reward_survive': Array([752.21875, 429.21707], dtype=float32), 'eval/episode_x_position': Array([54.361763, 31.358248], dtype=float32), 'eval/episode_x_velocity': Array([1.1863648, 0.7907562], dtype=float32), 'eval/episode_y_position': Array([11.8681555,  7.0801134], dtype=float32), 'eval/episode_y_velocity': Array([0.2921304 , 0.40888497], dtype=float32), 'eval/avg_episode_length': Array([752.21875, 429.21707], dtype=float32), 'eval/epoch_eval_time': 3.6073813438415527, 'eval/sps': 35482.80256494617}
I0728 14:57:39.911203 140071325378368 train.py:379] starting iteration 4, 222822400 steps, 705.8624982833862
I0728 15:00:33.170036 140071325378368 train.py:394] {'eval/walltime': 29.698448419570923, 'training/sps': 328366.54756346, 'training/walltime': 845.3343541622162, 'training/entropy_loss': Array(0.03106368, dtype=float32), 'training/policy_loss': Array(0.00481643, dtype=float32), 'training/total_loss': Array(15.420229, dtype=float32), 'training/v_loss': Array(15.38435, dtype=float32), 'eval/episode_distance_from_origin': Array([50.01783 , 32.404884], dtype=float32), 'eval/episode_forward_reward': Array([984.84314, 652.5324 ], dtype=float32), 'eval/episode_reward': Array([1573.3014, 1038.0703], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-108.61993,   71.75568], dtype=float32), 'eval/episode_reward_forward': Array([984.84314, 652.5324 ], dtype=float32), 'eval/episode_reward_survive': Array([697.0781 , 457.64108], dtype=float32), 'eval/episode_x_position': Array([49.242878, 32.63243 ], dtype=float32), 'eval/episode_x_velocity': Array([1.0227892 , 0.75319964], dtype=float32), 'eval/episode_y_position': Array([6.295226, 4.687184], dtype=float32), 'eval/episode_y_velocity': Array([0.14024591, 0.39710292], dtype=float32), 'eval/avg_episode_length': Array([697.0781 , 457.64108], dtype=float32), 'eval/epoch_eval_time': 3.608060836791992, 'eval/sps': 35476.12021803038}
I0728 15:00:33.172607 140071325378368 train.py:379] starting iteration 5, 278528000 steps, 879.1239025592804
