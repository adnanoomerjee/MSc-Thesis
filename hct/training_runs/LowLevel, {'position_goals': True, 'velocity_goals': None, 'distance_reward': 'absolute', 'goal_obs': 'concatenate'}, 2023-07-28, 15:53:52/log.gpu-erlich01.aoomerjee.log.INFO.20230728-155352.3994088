I0728 15:53:52.849274 140645753800512 low_level_env.py:188] Initialising environment...
I0728 15:53:53.175166 140645753800512 low_level_env.py:294] Environment initialised.
I0728 15:53:53.183255 140645753800512 train.py:118] JAX is running on GPU.
I0728 15:53:53.183310 140645753800512 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 15:53:59.387416 140645753800512 train.py:367] Running initial eval
I0728 15:54:14.566798 140645753800512 train.py:373] {'eval/walltime': 15.042504787445068, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2941268, 0.1339755], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.903973, 11.512859], dtype=float32), 'eval/episode_reward': Array([-90.79061,  84.19881], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2901661 , 0.13717452], dtype=float32), 'eval/avg_episode_length': Array([643.5    , 465.76727], dtype=float32), 'eval/epoch_eval_time': 15.042504787445068, 'eval/sps': 8509.221157558328}
I0728 15:54:14.568074 140645753800512 train.py:379] starting iteration 0, 0 steps, 21.384832859039307
I0728 15:54:47.481646 140645753800512 train.py:394] {'eval/walltime': 18.64928698539734, 'training/sps': 190105.47440565305, 'training/walltime': 29.302470207214355, 'training/entropy_loss': Array(-0.03424175, dtype=float32), 'training/policy_loss': Array(-0.01221298, dtype=float32), 'training/total_loss': Array(102.087006, dtype=float32), 'training/v_loss': Array(102.13345, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.6688887, 0.3299302], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([56.879974, 28.061722], dtype=float32), 'eval/episode_reward': Array([698.47424, 453.61557], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.668042  , 0.33162656], dtype=float32), 'eval/avg_episode_length': Array([712.0469 , 451.58795], dtype=float32), 'eval/epoch_eval_time': 3.6067821979522705, 'eval/sps': 35488.69684248504}
I0728 15:54:47.500566 140645753800512 train.py:379] starting iteration 1, 5570560 steps, 54.31733274459839
I0728 15:55:08.168987 140645753800512 train.py:394] {'eval/walltime': 22.278377532958984, 'training/sps': 326995.6235233526, 'training/walltime': 46.33804988861084, 'training/entropy_loss': Array(0.00227098, dtype=float32), 'training/policy_loss': Array(-0.01232361, dtype=float32), 'training/total_loss': Array(249.85419, dtype=float32), 'training/v_loss': Array(249.86424, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([3.0729063, 1.6328743], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([261.53876, 139.0196 ], dtype=float32), 'eval/episode_reward': Array([1987.1147, 1115.7402], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([3.076163 , 1.6362969], dtype=float32), 'eval/avg_episode_length': Array([759.53906, 415.53903], dtype=float32), 'eval/epoch_eval_time': 3.6290905475616455, 'eval/sps': 35270.54459580847}
I0728 15:55:08.170581 140645753800512 train.py:379] starting iteration 2, 11141120 steps, 74.9873480796814
I0728 15:55:28.967276 140645753800512 train.py:394] {'eval/walltime': 25.905293464660645, 'training/sps': 324508.2750704128, 'training/walltime': 63.50420689582825, 'training/entropy_loss': Array(0.02678911, dtype=float32), 'training/policy_loss': Array(-0.00637159, dtype=float32), 'training/total_loss': Array(420.68433, dtype=float32), 'training/v_loss': Array(420.6639, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([5.0955944, 3.186229 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([433.64008, 271.1495 ], dtype=float32), 'eval/episode_reward': Array([2974.4932, 1935.6533], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([5.1013374, 3.192205 ], dtype=float32), 'eval/avg_episode_length': Array([711.2656 , 447.94556], dtype=float32), 'eval/epoch_eval_time': 3.62691593170166, 'eval/sps': 35291.6920078557}
I0728 15:55:28.968824 140645753800512 train.py:379] starting iteration 3, 16711680 steps, 95.78559064865112
I0728 15:55:49.700255 140645753800512 train.py:394] {'eval/walltime': 29.548069715499878, 'training/sps': 326063.8605979858, 'training/walltime': 80.58846759796143, 'training/entropy_loss': Array(0.0491895, dtype=float32), 'training/policy_loss': Array(-0.00708403, dtype=float32), 'training/total_loss': Array(250.04929, dtype=float32), 'training/v_loss': Array(250.0072, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([7.0754013, 4.679489 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([602.11316, 398.23917], dtype=float32), 'eval/episode_reward': Array([4026.5159, 2738.4146], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([7.083913 , 4.6873865], dtype=float32), 'eval/avg_episode_length': Array([696.3047 , 458.77634], dtype=float32), 'eval/epoch_eval_time': 3.6427762508392334, 'eval/sps': 35138.035164940746}
I0728 15:55:49.701818 140645753800512 train.py:379] starting iteration 4, 22282240 steps, 116.51858568191528
I0728 15:56:10.517913 140645753800512 train.py:394] {'eval/walltime': 33.188711404800415, 'training/sps': 324406.9746009042, 'training/walltime': 97.75998497009277, 'training/entropy_loss': Array(0.07173611, dtype=float32), 'training/policy_loss': Array(-0.00562699, dtype=float32), 'training/total_loss': Array(123.60545, dtype=float32), 'training/v_loss': Array(123.539345, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([9.038703, 4.851319], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([769.17957, 412.86514], dtype=float32), 'eval/episode_reward': Array([5114.167 , 2785.6997], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([9.050118, 4.859399], dtype=float32), 'eval/avg_episode_length': Array([789.6875, 406.7656], dtype=float32), 'eval/epoch_eval_time': 3.640641689300537, 'eval/sps': 35158.63710954542}
I0728 15:56:10.519501 140645753800512 train.py:379] starting iteration 5, 27852800 steps, 137.3362681865692
I0728 15:56:31.279300 140645753800512 train.py:394] {'eval/walltime': 36.83083939552307, 'training/sps': 325495.4156609557, 'training/walltime': 114.8740816116333, 'training/entropy_loss': Array(0.08833086, dtype=float32), 'training/policy_loss': Array(-0.00240566, dtype=float32), 'training/total_loss': Array(107.773544, dtype=float32), 'training/v_loss': Array(107.68762, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([8.760346, 5.411452], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([745.4713, 460.5276], dtype=float32), 'eval/episode_reward': Array([4980.25 , 3116.607], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([8.77112 , 5.420524], dtype=float32), 'eval/avg_episode_length': Array([743.03125, 435.99945], dtype=float32), 'eval/epoch_eval_time': 3.6421279907226562, 'eval/sps': 35144.28936216565}
I0728 15:56:31.280861 140645753800512 train.py:379] starting iteration 6, 33423360 steps, 158.0976276397705
I0728 15:56:52.297497 140645753800512 train.py:394] {'eval/walltime': 40.4744017124176, 'training/sps': 320709.9070167809, 'training/walltime': 132.24354815483093, 'training/entropy_loss': Array(0.09813803, dtype=float32), 'training/policy_loss': Array(-0.00118249, dtype=float32), 'training/total_loss': Array(99.958694, dtype=float32), 'training/v_loss': Array(99.86174, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([8.774538, 5.504164], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([746.70044, 468.43234], dtype=float32), 'eval/episode_reward': Array([4998.3154, 3203.5498], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([8.785637 , 5.5128646], dtype=float32), 'eval/avg_episode_length': Array([727.6328 , 443.98083], dtype=float32), 'eval/epoch_eval_time': 3.6435623168945312, 'eval/sps': 35130.45444742016}
I0728 15:56:52.299087 140645753800512 train.py:379] starting iteration 7, 38993920 steps, 179.11585426330566
I0728 15:57:13.455254 140645753800512 train.py:394] {'eval/walltime': 44.1594340801239, 'training/sps': 318916.6231892948, 'training/walltime': 149.71068406105042, 'training/entropy_loss': Array(0.10504787, dtype=float32), 'training/policy_loss': Array(-0.00017466, dtype=float32), 'training/total_loss': Array(90.25377, dtype=float32), 'training/v_loss': Array(90.14891, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([9.4623375, 5.3145876], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([805.2075 , 452.29416], dtype=float32), 'eval/episode_reward': Array([5423.8623, 3083.078 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([9.47401  , 5.3237953], dtype=float32), 'eval/avg_episode_length': Array([774.16406, 417.26514], dtype=float32), 'eval/epoch_eval_time': 3.685032367706299, 'eval/sps': 34735.10873926786}
I0728 15:57:13.456843 140645753800512 train.py:379] starting iteration 8, 44564480 steps, 200.273610830307
I0728 15:57:34.859416 140645753800512 train.py:394] {'eval/walltime': 47.881187200546265, 'training/sps': 315134.2539526014, 'training/walltime': 167.38746762275696, 'training/entropy_loss': Array(0.10902345, dtype=float32), 'training/policy_loss': Array(0.00014525, dtype=float32), 'training/total_loss': Array(97.95079, dtype=float32), 'training/v_loss': Array(97.84163, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([9.702912 , 5.3257456], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([825.6908 , 453.24014], dtype=float32), 'eval/episode_reward': Array([5540.5225, 3067.716 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([9.714962, 5.334955], dtype=float32), 'eval/avg_episode_length': Array([789.7422 , 406.65973], dtype=float32), 'eval/epoch_eval_time': 3.7217531204223633, 'eval/sps': 34392.3940837521}
I0728 15:57:35.203280 140645753800512 train.py:410] total steps: 50135040
