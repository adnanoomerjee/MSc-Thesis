I0728 16:07:25.973051 139784540235584 low_level_env.py:188] Initialising environment...
I0728 16:07:26.331696 139784540235584 low_level_env.py:294] Environment initialised.
I0728 16:07:26.341572 139784540235584 train.py:118] JAX is running on GPU.
I0728 16:07:26.341630 139784540235584 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 16:07:32.759777 139784540235584 train.py:367] Running initial eval
I0728 16:07:48.744280 139784540235584 train.py:373] {'eval/walltime': 15.844541788101196, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30619544, 0.13022521], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.998539, 11.10886 ], dtype=float32), 'eval/episode_reward': Array([-49.831394,  58.428074], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30291456, 0.13214017], dtype=float32), 'eval/avg_episode_length': Array([624.2656 , 471.48715], dtype=float32), 'eval/epoch_eval_time': 15.844541788101196, 'eval/sps': 8078.491742571211}
I0728 16:07:48.747640 139784540235584 train.py:379] starting iteration 0, 0 steps, 22.406072854995728
I0728 16:08:22.759480 139784540235584 train.py:394] {'eval/walltime': 19.484705686569214, 'training/sps': 183439.04578937183, 'training/walltime': 30.36736249923706, 'training/entropy_loss': Array(-0.03418916, dtype=float32), 'training/policy_loss': Array(-0.0173723, dtype=float32), 'training/total_loss': Array(142.30934, dtype=float32), 'training/v_loss': Array(142.3609, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([1.1586345 , 0.71729875], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([98.54395 , 61.024063], dtype=float32), 'eval/episode_reward': Array([854.8667, 577.9927], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([1.1583705, 0.7199206], dtype=float32), 'eval/avg_episode_length': Array([698.53906, 449.92343], dtype=float32), 'eval/epoch_eval_time': 3.6401638984680176, 'eval/sps': 35163.251867277046}
I0728 16:08:22.793644 139784540235584 train.py:379] starting iteration 1, 5570560 steps, 56.45206904411316
I0728 16:08:44.181073 139784540235584 train.py:394] {'eval/walltime': 23.128851652145386, 'training/sps': 314027.184700418, 'training/walltime': 48.10646367073059, 'training/entropy_loss': Array(0.00256071, dtype=float32), 'training/policy_loss': Array(-0.00837251, dtype=float32), 'training/total_loss': Array(395.17188, dtype=float32), 'training/v_loss': Array(395.1777, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([1.7597096, 1.5022781], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([149.72754, 127.89278], dtype=float32), 'eval/episode_reward': Array([1109.8604,  974.4445], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([1.7604787, 1.5056087], dtype=float32), 'eval/avg_episode_length': Array([529.78906, 440.33557], dtype=float32), 'eval/epoch_eval_time': 3.644145965576172, 'eval/sps': 35124.82793201234}
I0728 16:08:44.182623 139784540235584 train.py:379] starting iteration 2, 11141120 steps, 77.84106683731079
I0728 16:09:05.794433 139784540235584 train.py:394] {'eval/walltime': 26.7851300239563, 'training/sps': 310309.8937696362, 'training/walltime': 66.0580666065216, 'training/entropy_loss': Array(0.01976861, dtype=float32), 'training/policy_loss': Array(-0.00530554, dtype=float32), 'training/total_loss': Array(543.3665, dtype=float32), 'training/v_loss': Array(543.35205, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([2.2793677, 1.9193178], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([193.92282, 163.34178], dtype=float32), 'eval/episode_reward': Array([1349.9415, 1214.1213], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([2.280782 , 1.9234575], dtype=float32), 'eval/avg_episode_length': Array([477.90625, 421.39487], dtype=float32), 'eval/epoch_eval_time': 3.656278371810913, 'eval/sps': 35008.275350928234}
I0728 16:09:05.795990 139784540235584 train.py:379] starting iteration 3, 16711680 steps, 99.45443367958069
I0728 16:09:27.574836 139784540235584 train.py:394] {'eval/walltime': 30.49621868133545, 'training/sps': 308389.29448073095, 'training/walltime': 84.12146925926208, 'training/entropy_loss': Array(0.03083861, dtype=float32), 'training/policy_loss': Array(-0.00388036, dtype=float32), 'training/total_loss': Array(634.73206, dtype=float32), 'training/v_loss': Array(634.7051, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([3.0303547, 2.3625104], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([257.88055, 201.10233], dtype=float32), 'eval/episode_reward': Array([1758.3418, 1448.4792], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([3.0330434, 2.3671803], dtype=float32), 'eval/avg_episode_length': Array([538.4219, 430.7663], dtype=float32), 'eval/epoch_eval_time': 3.7110886573791504, 'eval/sps': 34491.22665002466}
I0728 16:09:27.576399 139784540235584 train.py:379] starting iteration 4, 22282240 steps, 121.23484349250793
I0728 16:09:49.425526 139784540235584 train.py:394] {'eval/walltime': 34.27673530578613, 'training/sps': 308360.25879822206, 'training/walltime': 102.18657279014587, 'training/entropy_loss': Array(0.03789947, dtype=float32), 'training/policy_loss': Array(-0.00329916, dtype=float32), 'training/total_loss': Array(658.38916, dtype=float32), 'training/v_loss': Array(658.3545, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([4.330715 , 2.6471946], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([368.57205, 225.3044 ], dtype=float32), 'eval/episode_reward': Array([2502.29  , 1611.9888], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([4.335792 , 2.6518216], dtype=float32), 'eval/avg_episode_length': Array([650.53125, 413.98563], dtype=float32), 'eval/epoch_eval_time': 3.7805166244506836, 'eval/sps': 33857.806409884164}
I0728 16:09:49.426937 139784540235584 train.py:379] starting iteration 5, 27852800 steps, 143.08538126945496
I0728 16:10:11.296880 139784540235584 train.py:394] {'eval/walltime': 38.07520627975464, 'training/sps': 308308.7292465876, 'training/walltime': 120.2546956539154, 'training/entropy_loss': Array(0.04640566, dtype=float32), 'training/policy_loss': Array(-0.00341167, dtype=float32), 'training/total_loss': Array(610.5137, dtype=float32), 'training/v_loss': Array(610.4707, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([5.2614613, 3.7061589], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([447.72803, 315.4238 ], dtype=float32), 'eval/episode_reward': Array([3033.733 , 2212.2935], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([5.2671013, 3.7131422], dtype=float32), 'eval/avg_episode_length': Array([653.5  , 465.819], dtype=float32), 'eval/epoch_eval_time': 3.798470973968506, 'eval/sps': 33697.7696755361}
I0728 16:10:11.298258 139784540235584 train.py:379] starting iteration 6, 33423360 steps, 164.95670199394226
I0728 16:10:33.196841 139784540235584 train.py:394] {'eval/walltime': 41.890310287475586, 'training/sps': 308103.1122377859, 'training/walltime': 138.334876537323, 'training/entropy_loss': Array(0.06134526, dtype=float32), 'training/policy_loss': Array(-0.00395248, dtype=float32), 'training/total_loss': Array(308.1621, dtype=float32), 'training/v_loss': Array(308.1047, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([6.959383 , 4.3426404], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([592.21893, 369.61673], dtype=float32), 'eval/episode_reward': Array([3975.1626, 2564.6506], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([6.9676304, 4.350327 ], dtype=float32), 'eval/avg_episode_length': Array([713.6094 , 449.33463], dtype=float32), 'eval/epoch_eval_time': 3.8151040077209473, 'eval/sps': 33550.854640123995}
I0728 16:10:33.198233 139784540235584 train.py:379] starting iteration 7, 38993920 steps, 186.8566770553589
I0728 16:10:55.102671 139784540235584 train.py:394] {'eval/walltime': 45.722105503082275, 'training/sps': 308297.0455341715, 'training/walltime': 156.4036841392517, 'training/entropy_loss': Array(0.07388619, dtype=float32), 'training/policy_loss': Array(-0.00308834, dtype=float32), 'training/total_loss': Array(213.30573, dtype=float32), 'training/v_loss': Array(213.23492, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([7.691801, 5.000087], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([654.6046 , 425.55353], dtype=float32), 'eval/episode_reward': Array([4312.08  , 2883.1167], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([7.7012997, 5.0082307], dtype=float32), 'eval/avg_episode_length': Array([698.1094 , 456.17056], dtype=float32), 'eval/epoch_eval_time': 3.8317952156066895, 'eval/sps': 33404.70792349839}
I0728 16:10:55.104073 139784540235584 train.py:379] starting iteration 8, 44564480 steps, 208.76251673698425
I0728 16:11:17.089245 139784540235584 train.py:394] {'eval/walltime': 49.60084939002991, 'training/sps': 307727.6447658415, 'training/walltime': 174.50592517852783, 'training/entropy_loss': Array(0.08304958, dtype=float32), 'training/policy_loss': Array(-0.00302266, dtype=float32), 'training/total_loss': Array(154.52763, dtype=float32), 'training/v_loss': Array(154.4476, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([8.336988 , 5.3478203], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([709.5206 , 455.16183], dtype=float32), 'eval/episode_reward': Array([4675.91 , 3046.407], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([8.347103 , 5.3568544], dtype=float32), 'eval/avg_episode_length': Array([728.3906 , 442.79257], dtype=float32), 'eval/epoch_eval_time': 3.878743886947632, 'eval/sps': 33000.37427857328}
I0728 16:11:17.486626 139784540235584 train.py:410] total steps: 50135040
