I0728 01:06:02.612230 139827419338560 low_level_env.py:188] Initialising environment...
I0728 01:06:42.239899 139827419338560 low_level_env.py:293] Environment initialised.
I0728 01:06:42.246147 139827419338560 train.py:118] JAX is running on GPU.
I0728 01:06:42.246192 139827419338560 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 01:06:50.762132 139827419338560 train.py:367] Running initial eval
I0728 01:07:06.769511 139827419338560 train.py:373] {'eval/walltime': 15.859057188034058, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3185051 , 0.15653692], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.851025, 13.48903 ], dtype=float32), 'eval/episode_reward': Array([-24507.627,  10461.829], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3128673 , 0.16039659], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.859057188034058, 'eval/sps': 8071.097700346165}
I0728 01:07:06.772368 139827419338560 train.py:379] starting iteration 0, 0 steps, 24.526223182678223
I0728 01:07:42.193026 139827419338560 train.py:394] {'eval/walltime': 19.71497416496277, 'training/sps': 12979.04088327775, 'training/walltime': 31.55857229232788, 'training/entropy_loss': Array(-0.04491015, dtype=float32), 'training/policy_loss': Array(0.02053574, dtype=float32), 'training/total_loss': Array(137863.55, dtype=float32), 'training/v_loss': Array(137863.56, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3487377, 0.1487098], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([29.428425, 12.803132], dtype=float32), 'eval/episode_reward': Array([-27446.2  ,   9901.631], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.34412682, 0.15173076], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.855916976928711, 'eval/sps': 33195.73548026796}
I0728 01:07:42.248028 139827419338560 train.py:379] starting iteration 1, 409600 steps, 60.001877307891846
I0728 01:07:55.378521 139827419338560 train.py:394] {'eval/walltime': 23.603037357330322, 'training/sps': 44344.6961261722, 'training/walltime': 40.795302629470825, 'training/entropy_loss': Array(-0.04455439, dtype=float32), 'training/policy_loss': Array(0.00476765, dtype=float32), 'training/total_loss': Array(150260.9, dtype=float32), 'training/v_loss': Array(150260.95, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33477467, 0.13611032], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.277584, 11.683944], dtype=float32), 'eval/episode_reward': Array([-26323.758,   9606.008], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.330103 , 0.1388426], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.8880631923675537, 'eval/sps': 32921.275624138485}
I0728 01:07:55.380925 139827419338560 train.py:379] starting iteration 2, 819200 steps, 73.1347930431366
I0728 01:08:08.659967 139827419338560 train.py:394] {'eval/walltime': 27.590866565704346, 'training/sps': 44123.15020117536, 'training/walltime': 50.0784113407135, 'training/entropy_loss': Array(-0.04402994, dtype=float32), 'training/policy_loss': Array(0.00473839, dtype=float32), 'training/total_loss': Array(160968.17, dtype=float32), 'training/v_loss': Array(160968.22, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.34046066, 0.13986844], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.781582, 12.004747], dtype=float32), 'eval/episode_reward': Array([-26182.115,   9475.547], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.33611667, 0.14219978], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9878292083740234, 'eval/sps': 32097.663493515072}
I0728 01:08:08.662155 139827419338560 train.py:379] starting iteration 3, 1228800 steps, 86.4160225391388
I0728 01:08:21.986782 139827419338560 train.py:394] {'eval/walltime': 31.63101291656494, 'training/sps': 44141.784759472845, 'training/walltime': 59.357601165771484, 'training/entropy_loss': Array(-0.0426353, dtype=float32), 'training/policy_loss': Array(0.00705491, dtype=float32), 'training/total_loss': Array(170884.94, dtype=float32), 'training/v_loss': Array(170884.97, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33502102, 0.15760908], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.29088, 13.51967], dtype=float32), 'eval/episode_reward': Array([-26435.986,  10036.977], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3305425 , 0.16037944], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.040146350860596, 'eval/sps': 31682.02062104374}
I0728 01:08:21.989030 139827419338560 train.py:379] starting iteration 4, 1638400 steps, 99.74289798736572
I0728 01:08:35.346552 139827419338560 train.py:394] {'eval/walltime': 35.70258021354675, 'training/sps': 44134.962619482896, 'training/walltime': 68.63822531700134, 'training/entropy_loss': Array(-0.04084088, dtype=float32), 'training/policy_loss': Array(0.01000399, dtype=float32), 'training/total_loss': Array(177281.69, dtype=float32), 'training/v_loss': Array(177281.73, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3183602 , 0.15230069], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.86898 , 13.085407], dtype=float32), 'eval/episode_reward': Array([-24388.76 ,   9720.474], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3133446 , 0.15591669], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0715672969818115, 'eval/sps': 31437.525322222817}
I0728 01:08:35.348757 139827419338560 train.py:379] starting iteration 5, 2048000 steps, 113.10262489318848
I0728 01:08:48.728811 139827419338560 train.py:394] {'eval/walltime': 39.800981760025024, 'training/sps': 44156.14354269185, 'training/walltime': 77.91439771652222, 'training/entropy_loss': Array(-0.03776014, dtype=float32), 'training/policy_loss': Array(0.01386983, dtype=float32), 'training/total_loss': Array(129178.22, dtype=float32), 'training/v_loss': Array(129178.234, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33143175, 0.13792644], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.057858, 11.817272], dtype=float32), 'eval/episode_reward': Array([-25482.543,   9578.061], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32765532, 0.13999894], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0984015464782715, 'eval/sps': 31231.688390804342}
I0728 01:08:48.731088 139827419338560 train.py:379] starting iteration 6, 2457600 steps, 126.48495531082153
I0728 01:09:02.164830 139827419338560 train.py:394] {'eval/walltime': 43.88305187225342, 'training/sps': 43824.2639348892, 'training/walltime': 87.26081824302673, 'training/entropy_loss': Array(-0.03435344, dtype=float32), 'training/policy_loss': Array(0.01549722, dtype=float32), 'training/total_loss': Array(137792.06, dtype=float32), 'training/v_loss': Array(137792.08, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.34473622, 0.1349444 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([29.131338, 11.579788], dtype=float32), 'eval/episode_reward': Array([-26979.053,   9758.319], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.34041125, 0.13805717], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0820701122283936, 'eval/sps': 31356.639273921013}
I0728 01:09:02.167075 139827419338560 train.py:379] starting iteration 7, 2867200 steps, 139.9209427833557
I0728 01:09:15.692979 139827419338560 train.py:394] {'eval/walltime': 47.9689416885376, 'training/sps': 43416.58797202758, 'training/walltime': 96.69500041007996, 'training/entropy_loss': Array(-0.03047702, dtype=float32), 'training/policy_loss': Array(0.01567082, dtype=float32), 'training/total_loss': Array(146080.94, dtype=float32), 'training/v_loss': Array(146080.97, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3096484 , 0.15317437], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.132915 , 13.1499605], dtype=float32), 'eval/episode_reward': Array([-24234.832,  10208.441], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30500048, 0.15618649], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.08588981628418, 'eval/sps': 31327.325443251113}
I0728 01:09:15.695251 139827419338560 train.py:379] starting iteration 8, 3276800 steps, 153.44911909103394
I0728 01:09:29.352378 139827419338560 train.py:394] {'eval/walltime': 52.08527231216431, 'training/sps': 42956.099192439186, 'training/walltime': 106.23031687736511, 'training/entropy_loss': Array(-0.02472314, dtype=float32), 'training/policy_loss': Array(0.01852575, dtype=float32), 'training/total_loss': Array(153125.84, dtype=float32), 'training/v_loss': Array(153125.84, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32039547, 0.14979807], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.093084, 12.779956], dtype=float32), 'eval/episode_reward': Array([-25572.447,  10291.255], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31632194, 0.15199341], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.116330623626709, 'eval/sps': 31095.655743810275}
I0728 01:09:29.354703 139827419338560 train.py:379] starting iteration 9, 3686400 steps, 167.10857105255127
I0728 01:09:43.049817 139827419338560 train.py:394] {'eval/walltime': 56.15995407104492, 'training/sps': 42600.08283522721, 'training/walltime': 115.84532165527344, 'training/entropy_loss': Array(-0.01795965, dtype=float32), 'training/policy_loss': Array(0.02298289, dtype=float32), 'training/total_loss': Array(156926.53, dtype=float32), 'training/v_loss': Array(156926.53, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30225724, 0.12638886], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.582485, 10.796267], dtype=float32), 'eval/episode_reward': Array([-23897.75 ,   8924.493], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2978608 , 0.12860413], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.074681758880615, 'eval/sps': 31413.49621256404}
I0728 01:09:43.052121 139827419338560 train.py:379] starting iteration 10, 4096000 steps, 180.8059892654419
I0728 01:09:56.811773 139827419338560 train.py:394] {'eval/walltime': 60.24294424057007, 'training/sps': 42351.78957025253, 'training/walltime': 125.51669573783875, 'training/entropy_loss': Array(-0.01541464, dtype=float32), 'training/policy_loss': Array(0.02265575, dtype=float32), 'training/total_loss': Array(112376.53, dtype=float32), 'training/v_loss': Array(112376.516, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29319012, 0.11903091], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.814646, 10.170919], dtype=float32), 'eval/episode_reward': Array([-23475.559,   9460.83 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28890395, 0.12121398], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0829901695251465, 'eval/sps': 31349.573397304175}
I0728 01:09:56.814032 139827419338560 train.py:379] starting iteration 11, 4505600 steps, 194.5678994655609
I0728 01:10:10.667255 139827419338560 train.py:394] {'eval/walltime': 64.34748768806458, 'training/sps': 42039.94844463364, 'training/walltime': 135.25980949401855, 'training/entropy_loss': Array(-0.0161223, dtype=float32), 'training/policy_loss': Array(0.02009615, dtype=float32), 'training/total_loss': Array(114733.914, dtype=float32), 'training/v_loss': Array(114733.914, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30443832, 0.11870392], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.752567, 10.199322], dtype=float32), 'eval/episode_reward': Array([-24657.145,   9341.259], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3002168 , 0.12160897], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.104543447494507, 'eval/sps': 31184.95434081315}
I0728 01:10:10.669500 139827419338560 train.py:379] starting iteration 12, 4915200 steps, 208.42336773872375
I0728 01:10:24.484025 139827419338560 train.py:394] {'eval/walltime': 68.42411422729492, 'training/sps': 42085.492611134905, 'training/walltime': 144.99237942695618, 'training/entropy_loss': Array(-0.01325165, dtype=float32), 'training/policy_loss': Array(0.02331031, dtype=float32), 'training/total_loss': Array(115502.59, dtype=float32), 'training/v_loss': Array(115502.58, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31200475, 0.10833358], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.382755,  9.264132], dtype=float32), 'eval/episode_reward': Array([-25903.383,   8411.936], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30804214, 0.11048015], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.076626539230347, 'eval/sps': 31398.51020647233}
I0728 01:10:24.486223 139827419338560 train.py:379] starting iteration 13, 5324800 steps, 222.24009132385254
I0728 01:10:38.392820 139827419338560 train.py:394] {'eval/walltime': 72.49759721755981, 'training/sps': 41677.380278690856, 'training/walltime': 154.82025241851807, 'training/entropy_loss': Array(-0.00633017, dtype=float32), 'training/policy_loss': Array(0.01944317, dtype=float32), 'training/total_loss': Array(115618.94, dtype=float32), 'training/v_loss': Array(115618.92, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30672795, 0.10636986], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.952343,  9.11841 ], dtype=float32), 'eval/episode_reward': Array([-25430.03 ,   8561.138], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30259916, 0.10877846], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.073482990264893, 'eval/sps': 31422.74076162924}
I0728 01:10:38.395037 139827419338560 train.py:379] starting iteration 14, 5734400 steps, 236.14890480041504
I0728 01:10:52.342719 139827419338560 train.py:394] {'eval/walltime': 76.57430481910706, 'training/sps': 41515.648761501194, 'training/walltime': 164.6864116191864, 'training/entropy_loss': Array(0.00603342, dtype=float32), 'training/policy_loss': Array(0.02362231, dtype=float32), 'training/total_loss': Array(112510.14, dtype=float32), 'training/v_loss': Array(112510.09, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29766434, 0.11141563], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.163517,  9.568181], dtype=float32), 'eval/episode_reward': Array([-24935.348,   9398.575], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29326326, 0.11386234], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.076707601547241, 'eval/sps': 31397.88587030865}
I0728 01:10:52.344930 139827419338560 train.py:379] starting iteration 15, 6144000 steps, 250.09879755973816
I0728 01:11:06.367831 139827419338560 train.py:394] {'eval/walltime': 80.68439888954163, 'training/sps': 41342.22224511008, 'training/walltime': 174.59395837783813, 'training/entropy_loss': Array(0.01818863, dtype=float32), 'training/policy_loss': Array(0.02654327, dtype=float32), 'training/total_loss': Array(91540.734, dtype=float32), 'training/v_loss': Array(91540.69, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3026904 , 0.10588935], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.561604 ,  9.1058655], dtype=float32), 'eval/episode_reward': Array([-25050.41,   8541.76], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2983076 , 0.10858479], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.11009407043457, 'eval/sps': 31142.839508407225}
I0728 01:11:06.370031 139827419338560 train.py:379] starting iteration 16, 6553600 steps, 264.1238989830017
I0728 01:11:20.320958 139827419338560 train.py:394] {'eval/walltime': 84.7754864692688, 'training/sps': 41563.32211631209, 'training/walltime': 184.4488010406494, 'training/entropy_loss': Array(0.03660758, dtype=float32), 'training/policy_loss': Array(0.01592638, dtype=float32), 'training/total_loss': Array(90632.08, dtype=float32), 'training/v_loss': Array(90632.016, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2889473 , 0.10343791], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.386517,  8.874319], dtype=float32), 'eval/episode_reward': Array([-24133.957,   8908.147], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28412136, 0.10604158], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.091087579727173, 'eval/sps': 31287.523795453944}
I0728 01:11:20.323144 139827419338560 train.py:379] starting iteration 17, 6963200 steps, 278.07701230049133
I0728 01:11:34.322888 139827419338560 train.py:394] {'eval/walltime': 88.8559627532959, 'training/sps': 41313.25791257716, 'training/walltime': 194.3632938861847, 'training/entropy_loss': Array(0.06236017, dtype=float32), 'training/policy_loss': Array(0.01263972, dtype=float32), 'training/total_loss': Array(87673.55, dtype=float32), 'training/v_loss': Array(87673.47, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2860476 , 0.09790538], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.142601 ,  8.4209585], dtype=float32), 'eval/episode_reward': Array([-23838.02 ,   8375.982], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28154963, 0.100071  ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0804762840271, 'eval/sps': 31368.887132380136}
I0728 01:11:34.325133 139827419338560 train.py:379] starting iteration 18, 7372800 steps, 292.07900071144104
I0728 01:11:48.339666 139827419338560 train.py:394] {'eval/walltime': 92.95374584197998, 'training/sps': 41324.572771503124, 'training/walltime': 204.27507209777832, 'training/entropy_loss': Array(0.09425154, dtype=float32), 'training/policy_loss': Array(0.01162985, dtype=float32), 'training/total_loss': Array(83781.29, dtype=float32), 'training/v_loss': Array(83781.19, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28639466, 0.11815561], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.150414, 10.189148], dtype=float32), 'eval/episode_reward': Array([-24067.29 ,   9915.695], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2813509 , 0.12112863], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.097783088684082, 'eval/sps': 31236.402032471793}
I0728 01:11:48.341869 139827419338560 train.py:379] starting iteration 19, 7782400 steps, 306.09573698043823
I0728 01:12:02.356953 139827419338560 train.py:394] {'eval/walltime': 97.02768802642822, 'training/sps': 41224.392951794114, 'training/walltime': 214.21093702316284, 'training/entropy_loss': Array(0.13006076, dtype=float32), 'training/policy_loss': Array(0.00634285, dtype=float32), 'training/total_loss': Array(78560.766, dtype=float32), 'training/v_loss': Array(78560.64, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27682   , 0.10203398], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.337755,  8.784811], dtype=float32), 'eval/episode_reward': Array([-23240.898,   8673.94 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2718286 , 0.10510685], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.073942184448242, 'eval/sps': 31419.198949023816}
I0728 01:12:02.359163 139827419338560 train.py:379] starting iteration 20, 8192000 steps, 320.11303091049194
I0728 01:12:16.351721 139827419338560 train.py:394] {'eval/walltime': 101.1180248260498, 'training/sps': 41386.12382562366, 'training/walltime': 224.1079740524292, 'training/entropy_loss': Array(0.1416803, dtype=float32), 'training/policy_loss': Array(0.02142192, dtype=float32), 'training/total_loss': Array(70087.25, dtype=float32), 'training/v_loss': Array(70087.09, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27998635, 0.09895737], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.638924,  8.506027], dtype=float32), 'eval/episode_reward': Array([-23535.18 ,   8454.825], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27526468, 0.10122242], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.090336799621582, 'eval/sps': 31293.266611160707}
I0728 01:12:16.353932 139827419338560 train.py:379] starting iteration 21, 8601600 steps, 334.1078004837036
I0728 01:12:30.407892 139827419338560 train.py:394] {'eval/walltime': 105.19720244407654, 'training/sps': 41083.225070160064, 'training/walltime': 234.0779800415039, 'training/entropy_loss': Array(0.18577552, dtype=float32), 'training/policy_loss': Array(0.01922942, dtype=float32), 'training/total_loss': Array(69090.66, dtype=float32), 'training/v_loss': Array(69090.45, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30120838, 0.1080768 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.446661,  9.303169], dtype=float32), 'eval/episode_reward': Array([-25116.047,   9041.013], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29641613, 0.11127695], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.079177618026733, 'eval/sps': 31378.873877504477}
I0728 01:12:30.409992 139827419338560 train.py:379] starting iteration 22, 9011200 steps, 348.1638524532318
I0728 01:12:44.477322 139827419338560 train.py:394] {'eval/walltime': 109.31058430671692, 'training/sps': 41170.85330622203, 'training/walltime': 244.02676582336426, 'training/entropy_loss': Array(0.05573418, dtype=float32), 'training/policy_loss': Array(0.05582413, dtype=float32), 'training/total_loss': Array(69946.57, dtype=float32), 'training/v_loss': Array(69946.45, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30274737, 0.13807654], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.420998, 11.919519], dtype=float32), 'eval/episode_reward': Array([-24421.512,  10544.442], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29632717, 0.1422718 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.113381862640381, 'eval/sps': 31117.947293577254}
I0728 01:12:44.479519 139827419338560 train.py:379] starting iteration 23, 9420800 steps, 362.2333867549896
I0728 01:12:58.507647 139827419338560 train.py:394] {'eval/walltime': 113.3891053199768, 'training/sps': 41192.28743113277, 'training/walltime': 253.97037482261658, 'training/entropy_loss': Array(0.03658922, dtype=float32), 'training/policy_loss': Array(0.01181702, dtype=float32), 'training/total_loss': Array(78339.414, dtype=float32), 'training/v_loss': Array(78339.375, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29883522, 0.13138343], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.090946, 11.387304], dtype=float32), 'eval/episode_reward': Array([-24442.027,  10306.319], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2922782 , 0.13574994], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.078521013259888, 'eval/sps': 31383.925590637555}
I0728 01:12:58.510942 139827419338560 train.py:379] starting iteration 24, 9830400 steps, 376.2648034095764
I0728 01:13:12.564971 139827419338560 train.py:394] {'eval/walltime': 117.46837043762207, 'training/sps': 41084.73907703794, 'training/walltime': 263.9400134086609, 'training/entropy_loss': Array(0.04353823, dtype=float32), 'training/policy_loss': Array(0.00704947, dtype=float32), 'training/total_loss': Array(78988.92, dtype=float32), 'training/v_loss': Array(78988.875, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30934066, 0.13535933], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.031197, 11.708762], dtype=float32), 'eval/episode_reward': Array([-25420.756,  10711.748], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30341807, 0.1388427 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.079265117645264, 'eval/sps': 31378.20080541551}
I0728 01:13:12.567182 139827419338560 train.py:379] starting iteration 25, 10240000 steps, 390.32104992866516
I0728 01:13:26.674546 139827419338560 train.py:394] {'eval/walltime': 121.57521915435791, 'training/sps': 40979.31379020225, 'training/walltime': 273.9353003501892, 'training/entropy_loss': Array(0.02527968, dtype=float32), 'training/policy_loss': Array(0.00393371, dtype=float32), 'training/total_loss': Array(53343.656, dtype=float32), 'training/v_loss': Array(53343.625, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2938461 , 0.12718976], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.664793, 11.007265], dtype=float32), 'eval/episode_reward': Array([-23963.852,   9785.153], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2874382, 0.1308447], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.10684871673584, 'eval/sps': 31167.44950413843}
I0728 01:13:26.708156 139827419338560 train.py:379] starting iteration 26, 10649600 steps, 404.46202278137207
I0728 01:13:40.811180 139827419338560 train.py:394] {'eval/walltime': 125.68975877761841, 'training/sps': 41029.081156522596, 'training/walltime': 283.91846323013306, 'training/entropy_loss': Array(0.02305986, dtype=float32), 'training/policy_loss': Array(0.00265687, dtype=float32), 'training/total_loss': Array(59500.85, dtype=float32), 'training/v_loss': Array(59500.832, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31129593, 0.13396622], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.194616, 11.593252], dtype=float32), 'eval/episode_reward': Array([-25679.047,  10532.05 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3048699, 0.1385541], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.114539623260498, 'eval/sps': 31109.191238889696}
I0728 01:13:40.813493 139827419338560 train.py:379] starting iteration 27, 11059200 steps, 418.5673611164093
I0728 01:13:54.933134 139827419338560 train.py:394] {'eval/walltime': 129.80152344703674, 'training/sps': 40948.101113882636, 'training/walltime': 293.92136907577515, 'training/entropy_loss': Array(0.01908279, dtype=float32), 'training/policy_loss': Array(0.00228478, dtype=float32), 'training/total_loss': Array(60341.508, dtype=float32), 'training/v_loss': Array(60341.484, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31859165, 0.12229905], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.839615, 10.551391], dtype=float32), 'eval/episode_reward': Array([-25577.629,   9455.323], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31307626, 0.12543191], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.111764669418335, 'eval/sps': 31130.186256040608}
I0728 01:13:54.935361 139827419338560 train.py:379] starting iteration 28, 11468800 steps, 432.6892282962799
I0728 01:14:09.014513 139827419338560 train.py:394] {'eval/walltime': 133.8916654586792, 'training/sps': 41026.81976585018, 'training/walltime': 303.90508222579956, 'training/entropy_loss': Array(0.0149302, dtype=float32), 'training/policy_loss': Array(0.00213712, dtype=float32), 'training/total_loss': Array(59517.773, dtype=float32), 'training/v_loss': Array(59517.758, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31652495, 0.12642598], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.688711, 10.973006], dtype=float32), 'eval/episode_reward': Array([-25660.773,   9270.256], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31063092, 0.13035966], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.090142011642456, 'eval/sps': 31294.75691446707}
I0728 01:14:09.016770 139827419338560 train.py:379] starting iteration 29, 11878400 steps, 446.7706379890442
I0728 01:14:23.138206 139827419338560 train.py:394] {'eval/walltime': 137.97232151031494, 'training/sps': 40815.17889574049, 'training/walltime': 313.9405643939972, 'training/entropy_loss': Array(0.00833471, dtype=float32), 'training/policy_loss': Array(0.00183606, dtype=float32), 'training/total_loss': Array(56589.777, dtype=float32), 'training/v_loss': Array(56589.77, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31698397, 0.1181328 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.68616  , 10.1986475], dtype=float32), 'eval/episode_reward': Array([-26098.314,   8864.79 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.311447  , 0.12125879], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.080656051635742, 'eval/sps': 31367.50521982632}
I0728 01:14:23.140392 139827419338560 train.py:379] starting iteration 30, 12288000 steps, 460.89426016807556
I0728 01:14:37.292155 139827419338560 train.py:394] {'eval/walltime': 142.07526278495789, 'training/sps': 40781.485520361595, 'training/walltime': 323.98433780670166, 'training/entropy_loss': Array(0.00078294, dtype=float32), 'training/policy_loss': Array(0.00185733, dtype=float32), 'training/total_loss': Array(34103.777, dtype=float32), 'training/v_loss': Array(34103.773, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33635497, 0.13719329], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.332943, 11.83847 ], dtype=float32), 'eval/episode_reward': Array([-26773.168,  10232.448], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3310917, 0.1404184], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.102941274642944, 'eval/sps': 31197.131870023928}
I0728 01:14:37.294349 139827419338560 train.py:379] starting iteration 31, 12697600 steps, 475.0482110977173
I0728 01:14:51.437984 139827419338560 train.py:394] {'eval/walltime': 146.18180179595947, 'training/sps': 40830.017316527206, 'training/walltime': 334.0161728858948, 'training/entropy_loss': Array(-0.00449602, dtype=float32), 'training/policy_loss': Array(0.00143119, dtype=float32), 'training/total_loss': Array(39654.22, dtype=float32), 'training/v_loss': Array(39654.227, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33408755, 0.14488924], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.155212, 12.451306], dtype=float32), 'eval/episode_reward': Array([-27188.418,  10758.777], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32889614, 0.1476374 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.106539011001587, 'eval/sps': 31169.800081548656}
I0728 01:14:51.440263 139827419338560 train.py:379] starting iteration 32, 13107200 steps, 489.19412994384766
I0728 01:15:05.514399 139827419338560 train.py:394] {'eval/walltime': 150.27358627319336, 'training/sps': 41054.32511683784, 'training/walltime': 343.9931972026825, 'training/entropy_loss': Array(-0.01131133, dtype=float32), 'training/policy_loss': Array(0.00112149, dtype=float32), 'training/total_loss': Array(40825.65, dtype=float32), 'training/v_loss': Array(40825.66, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31625468, 0.13779844], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.597645, 11.906022], dtype=float32), 'eval/episode_reward': Array([-25548.629,  10141.031], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3105873 , 0.14135505], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.091784477233887, 'eval/sps': 31282.195020821353}
I0728 01:15:05.516568 139827419338560 train.py:379] starting iteration 33, 13516800 steps, 503.2704360485077
I0728 01:15:19.600188 139827419338560 train.py:394] {'eval/walltime': 154.3460099697113, 'training/sps': 40935.36541084444, 'training/walltime': 353.9992151260376, 'training/entropy_loss': Array(-0.01972945, dtype=float32), 'training/policy_loss': Array(0.00088651, dtype=float32), 'training/total_loss': Array(39515.19, dtype=float32), 'training/v_loss': Array(39515.21, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29932198, 0.14001645], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.139862, 12.106529], dtype=float32), 'eval/episode_reward': Array([-24426.234,  10477.077], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29300365, 0.14397363], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.072423696517944, 'eval/sps': 31430.914251246548}
I0728 01:15:19.602374 139827419338560 train.py:379] starting iteration 34, 13926400 steps, 517.3562207221985
I0728 01:15:33.693436 139827419338560 train.py:394] {'eval/walltime': 158.42670273780823, 'training/sps': 40939.89659046704, 'training/walltime': 364.0041255950928, 'training/entropy_loss': Array(-0.02554958, dtype=float32), 'training/policy_loss': Array(0.00068287, dtype=float32), 'training/total_loss': Array(37257.41, dtype=float32), 'training/v_loss': Array(37257.438, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31084976, 0.13199677], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.166739, 11.40767 ], dtype=float32), 'eval/episode_reward': Array([-24554.938,  10118.925], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30505687, 0.13571963], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.080692768096924, 'eval/sps': 31367.222987408146}
I0728 01:15:33.695637 139827419338560 train.py:379] starting iteration 35, 14336000 steps, 531.4495050907135
I0728 01:15:47.824832 139827419338560 train.py:394] {'eval/walltime': 162.50489783287048, 'training/sps': 40772.4709204987, 'training/walltime': 374.050119638443, 'training/entropy_loss': Array(-0.02883195, dtype=float32), 'training/policy_loss': Array(0.00070049, dtype=float32), 'training/total_loss': Array(19834.57, dtype=float32), 'training/v_loss': Array(19834.598, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28594616, 0.12248705], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.053148, 10.544401], dtype=float32), 'eval/episode_reward': Array([-23538.629,   8888.878], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2798751, 0.1257007], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.078195095062256, 'eval/sps': 31386.43370813186}
I0728 01:15:47.827070 139827419338560 train.py:379] starting iteration 36, 14745600 steps, 545.5809373855591
I0728 01:16:01.949541 139827419338560 train.py:394] {'eval/walltime': 166.58738088607788, 'training/sps': 40818.21127629017, 'training/walltime': 384.0848562717438, 'training/entropy_loss': Array(-0.03285372, dtype=float32), 'training/policy_loss': Array(0.00059139, dtype=float32), 'training/total_loss': Array(23956.984, dtype=float32), 'training/v_loss': Array(23957.018, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32590812, 0.1300464 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.459312, 11.174203], dtype=float32), 'eval/episode_reward': Array([-25942.371,   9398.479], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3210087 , 0.13269208], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0824830532073975, 'eval/sps': 31353.46756661659}
I0728 01:16:01.951743 139827419338560 train.py:379] starting iteration 37, 15155200 steps, 559.7056114673615
I0728 01:16:16.078327 139827419338560 train.py:394] {'eval/walltime': 170.69945287704468, 'training/sps': 40921.557597816725, 'training/walltime': 394.09425044059753, 'training/entropy_loss': Array(-0.0376702, dtype=float32), 'training/policy_loss': Array(0.00055055, dtype=float32), 'training/total_loss': Array(25754.37, dtype=float32), 'training/v_loss': Array(25754.406, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32560685, 0.13619581], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.43678 , 11.721902], dtype=float32), 'eval/episode_reward': Array([-25903.373,   9915.418], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32036394, 0.1394389 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.112071990966797, 'eval/sps': 31127.859697297197}
I0728 01:16:16.080621 139827419338560 train.py:379] starting iteration 38, 15564800 steps, 573.8344874382019
I0728 01:16:30.187883 139827419338560 train.py:394] {'eval/walltime': 174.77697157859802, 'training/sps': 40858.48755805582, 'training/walltime': 404.11909532546997, 'training/entropy_loss': Array(-0.04129223, dtype=float32), 'training/policy_loss': Array(0.00029744, dtype=float32), 'training/total_loss': Array(26435.7, dtype=float32), 'training/v_loss': Array(26435.742, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3155897 , 0.13330922], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.616003, 11.472868], dtype=float32), 'eval/episode_reward': Array([-25692.787,  10294.163], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31027842, 0.13637802], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.077518701553345, 'eval/sps': 31391.640202959206}
I0728 01:16:30.190050 139827419338560 train.py:379] starting iteration 39, 15974400 steps, 587.9439175128937
I0728 01:16:44.312730 139827419338560 train.py:394] {'eval/walltime': 178.8545277118683, 'training/sps': 40796.17447702076, 'training/walltime': 414.1592524051666, 'training/entropy_loss': Array(-0.04338809, dtype=float32), 'training/policy_loss': Array(0.00012235, dtype=float32), 'training/total_loss': Array(26949.55, dtype=float32), 'training/v_loss': Array(26949.594, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32963774, 0.13622348], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.808773, 11.725162], dtype=float32), 'eval/episode_reward': Array([-26416.064,  10379.659], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32465914, 0.13931318], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.077556133270264, 'eval/sps': 31391.352029614365}
I0728 01:16:44.314901 139827419338560 train.py:379] starting iteration 40, 16384000 steps, 602.0687692165375
I0728 01:16:58.448122 139827419338560 train.py:394] {'eval/walltime': 182.96350479125977, 'training/sps': 40880.8670445804, 'training/walltime': 424.1786093711853, 'training/entropy_loss': Array(-0.04490917, dtype=float32), 'training/policy_loss': Array(0.00030047, dtype=float32), 'training/total_loss': Array(38288.008, dtype=float32), 'training/v_loss': Array(38288.055, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3195625 , 0.13294342], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.895758, 11.471167], dtype=float32), 'eval/episode_reward': Array([-25862.31 ,   9311.499], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3140681 , 0.13629718], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.1089770793914795, 'eval/sps': 31151.305428784774}
I0728 01:16:58.450199 139827419338560 train.py:379] starting iteration 41, 16793600 steps, 616.2040672302246
I0728 01:17:12.579016 139827419338560 train.py:394] {'eval/walltime': 187.075519323349, 'training/sps': 40910.67373082062, 'training/walltime': 434.19066643714905, 'training/entropy_loss': Array(-0.04578305, dtype=float32), 'training/policy_loss': Array(0.00055502, dtype=float32), 'training/total_loss': Array(55068.363, dtype=float32), 'training/v_loss': Array(55068.406, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33396375, 0.12981418], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.120731 , 11.2204075], dtype=float32), 'eval/episode_reward': Array([-26577.105,   9230.13 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32904938, 0.13277848], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.112014532089233, 'eval/sps': 31128.294659738403}
I0728 01:17:12.581115 139827419338560 train.py:379] starting iteration 42, 17203200 steps, 630.3349831104279
I0728 01:17:26.729492 139827419338560 train.py:394] {'eval/walltime': 191.1559808254242, 'training/sps': 40704.060471815304, 'training/walltime': 444.2535445690155, 'training/entropy_loss': Array(-0.0474084, dtype=float32), 'training/policy_loss': Array(0.00044714, dtype=float32), 'training/total_loss': Array(53508.617, dtype=float32), 'training/v_loss': Array(53508.664, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2943313 , 0.12000647], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.759216, 10.390122], dtype=float32), 'eval/episode_reward': Array([-23691.31 ,   9067.571], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28827184, 0.12400094], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.080461502075195, 'eval/sps': 31369.000769864684}
I0728 01:17:26.731582 139827419338560 train.py:379] starting iteration 43, 17612800 steps, 644.4854497909546
I0728 01:17:40.858677 139827419338560 train.py:394] {'eval/walltime': 195.2490737438202, 'training/sps': 40843.67796269232, 'training/walltime': 454.2820243835449, 'training/entropy_loss': Array(-0.04883213, dtype=float32), 'training/policy_loss': Array(0.00022441, dtype=float32), 'training/total_loss': Array(51510.793, dtype=float32), 'training/v_loss': Array(51510.844, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30706444, 0.11937363], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.861933, 10.237517], dtype=float32), 'eval/episode_reward': Array([-24741.355,   8734.674], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3017215 , 0.12284838], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.093092918395996, 'eval/sps': 31272.195025115805}
I0728 01:17:40.860819 139827419338560 train.py:379] starting iteration 44, 18022400 steps, 658.6146867275238
I0728 01:17:54.978366 139827419338560 train.py:394] {'eval/walltime': 199.3267846107483, 'training/sps': 40816.7498201424, 'training/walltime': 464.3171203136444, 'training/entropy_loss': Array(-0.05006749, dtype=float32), 'training/policy_loss': Array(0.0001992, dtype=float32), 'training/total_loss': Array(50453.977, dtype=float32), 'training/v_loss': Array(50454.023, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3025943 , 0.13065404], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.46822 , 11.298577], dtype=float32), 'eval/episode_reward': Array([-24032.36 ,   9517.068], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29685932, 0.13445535], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.077710866928101, 'eval/sps': 31390.16084689384}
I0728 01:17:54.980451 139827419338560 train.py:379] starting iteration 45, 18432000 steps, 672.7343194484711
I0728 01:18:09.109045 139827419338560 train.py:394] {'eval/walltime': 203.44010257720947, 'training/sps': 40916.71473623368, 'training/walltime': 474.3276991844177, 'training/entropy_loss': Array(-0.05010078, dtype=float32), 'training/policy_loss': Array(0.00027001, dtype=float32), 'training/total_loss': Array(50242.137, dtype=float32), 'training/v_loss': Array(50242.188, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30202147, 0.12137292], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.472527 , 10.4114685], dtype=float32), 'eval/episode_reward': Array([-24183.559,   8925.814], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29654723, 0.12478513], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.113317966461182, 'eval/sps': 31118.43067899817}
I0728 01:18:09.111164 139827419338560 train.py:379] starting iteration 46, 18841600 steps, 686.8650319576263
I0728 01:18:23.266532 139827419338560 train.py:394] {'eval/walltime': 207.52277255058289, 'training/sps': 40683.8471204, 'training/walltime': 484.39557695388794, 'training/entropy_loss': Array(-0.05094647, dtype=float32), 'training/policy_loss': Array(0.00028591, dtype=float32), 'training/total_loss': Array(46096.008, dtype=float32), 'training/v_loss': Array(46096.062, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31877285, 0.12878835], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.875767, 11.093867], dtype=float32), 'eval/episode_reward': Array([-25698.934,   9607.413], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31361717, 0.13200465], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.082669973373413, 'eval/sps': 31352.032085570867}
I0728 01:18:23.268605 139827419338560 train.py:379] starting iteration 47, 19251200 steps, 701.0224735736847
I0728 01:18:37.372132 139827419338560 train.py:394] {'eval/walltime': 211.632470369339, 'training/sps': 41006.52740094688, 'training/walltime': 494.3842306137085, 'training/entropy_loss': Array(-0.05119674, dtype=float32), 'training/policy_loss': Array(0.00012577, dtype=float32), 'training/total_loss': Array(45475.242, dtype=float32), 'training/v_loss': Array(45475.297, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31535858, 0.13278207], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.57945  , 11.4342785], dtype=float32), 'eval/episode_reward': Array([-25537.832,   9745.083], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31042758, 0.13544604], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.1096978187561035, 'eval/sps': 31145.842260184036}
I0728 01:18:37.374200 139827419338560 train.py:379] starting iteration 48, 19660800 steps, 715.1280677318573
I0728 01:18:51.506996 139827419338560 train.py:394] {'eval/walltime': 215.752055644989, 'training/sps': 40926.78184427441, 'training/walltime': 504.39234709739685, 'training/entropy_loss': Array(-0.05169355, dtype=float32), 'training/policy_loss': Array(0.00014835, dtype=float32), 'training/total_loss': Array(44063.812, dtype=float32), 'training/v_loss': Array(44063.863, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28599948, 0.11218589], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.10128 ,  9.695182], dtype=float32), 'eval/episode_reward': Array([-23536.332 ,   7932.7964], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28058267, 0.11514696], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.119585275650024, 'eval/sps': 31071.08881968781}
I0728 01:18:51.509095 139827419338560 train.py:379] starting iteration 49, 20070400 steps, 729.2629632949829
I0728 01:19:05.666614 139827419338560 train.py:394] {'eval/walltime': 219.83120822906494, 'training/sps': 40662.24769599157, 'training/walltime': 514.4655728340149, 'training/entropy_loss': Array(-0.0519271, dtype=float32), 'training/policy_loss': Array(0.00014603, dtype=float32), 'training/total_loss': Array(42915.203, dtype=float32), 'training/v_loss': Array(42915.25, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2952369 , 0.12878035], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.897099, 11.117249], dtype=float32), 'eval/episode_reward': Array([-23887.172,   9092.138], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29003203, 0.1319692 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.079152584075928, 'eval/sps': 31379.066451125786}
I0728 01:19:05.668691 139827419338560 train.py:379] starting iteration 50, 20480000 steps, 743.4225594997406
I0728 01:19:19.809472 139827419338560 train.py:394] {'eval/walltime': 223.91335225105286, 'training/sps': 40741.2497219768, 'training/walltime': 524.5192654132843, 'training/entropy_loss': Array(-0.05141301, dtype=float32), 'training/policy_loss': Array(0.00015443, dtype=float32), 'training/total_loss': Array(42152.33, dtype=float32), 'training/v_loss': Array(42152.38, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30680072, 0.12775935], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.878042, 11.026229], dtype=float32), 'eval/episode_reward': Array([-24714.055,   9173.361], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30138856, 0.13113977], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.082144021987915, 'eval/sps': 31356.071542440775}
I0728 01:19:19.865077 139827419338560 train.py:379] starting iteration 51, 20889600 steps, 757.6189367771149
I0728 01:19:34.005270 139827419338560 train.py:394] {'eval/walltime': 228.00994658470154, 'training/sps': 40804.95145016985, 'training/walltime': 534.5572628974915, 'training/entropy_loss': Array(-0.05149536, dtype=float32), 'training/policy_loss': Array(0.00016867, dtype=float32), 'training/total_loss': Array(39439.34, dtype=float32), 'training/v_loss': Array(39439.39, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29831362, 0.13769841], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.13393 , 11.868421], dtype=float32), 'eval/episode_reward': Array([-23634.746,  10245.396], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29250902, 0.14150411], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.096594333648682, 'eval/sps': 31245.46625196233}
I0728 01:19:34.007636 139827419338560 train.py:379] starting iteration 52, 21299200 steps, 771.7615036964417
I0728 01:19:48.158368 139827419338560 train.py:394] {'eval/walltime': 232.10240387916565, 'training/sps': 40743.13188768423, 'training/walltime': 544.6104910373688, 'training/entropy_loss': Array(-0.05205978, dtype=float32), 'training/policy_loss': Array(0.00022257, dtype=float32), 'training/total_loss': Array(37948.406, dtype=float32), 'training/v_loss': Array(37948.453, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29152048, 0.13837738], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.53226 , 11.921977], dtype=float32), 'eval/episode_reward': Array([-23462.246,  10012.519], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2854967 , 0.14219044], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.092457294464111, 'eval/sps': 31277.052096095485}
I0728 01:19:48.160634 139827419338560 train.py:379] starting iteration 53, 21708800 steps, 785.9145021438599
I0728 01:20:02.362851 139827419338560 train.py:394] {'eval/walltime': 236.2211935520172, 'training/sps': 40642.93636710796, 'training/walltime': 554.6885030269623, 'training/entropy_loss': Array(-0.05260821, dtype=float32), 'training/policy_loss': Array(0.00018174, dtype=float32), 'training/total_loss': Array(37134.35, dtype=float32), 'training/v_loss': Array(37134.402, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2947309 , 0.14176886], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.83149 , 12.226211], dtype=float32), 'eval/episode_reward': Array([-23700.078,  10465.653], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28891116, 0.14559704], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.1187896728515625, 'eval/sps': 31077.090642354102}
I0728 01:20:02.365117 139827419338560 train.py:379] starting iteration 54, 22118400 steps, 800.118985414505
I0728 01:20:16.497750 139827419338560 train.py:394] {'eval/walltime': 240.29895114898682, 'training/sps': 40756.77408957795, 'training/walltime': 564.7383661270142, 'training/entropy_loss': Array(-0.05274612, dtype=float32), 'training/policy_loss': Array(0.00012963, dtype=float32), 'training/total_loss': Array(36374.703, dtype=float32), 'training/v_loss': Array(36374.758, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3194099, 0.1298886], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.910091, 11.228358], dtype=float32), 'eval/episode_reward': Array([-25294.758,   9751.745], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31384718, 0.13429607], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0777575969696045, 'eval/sps': 31389.801123814595}
I0728 01:20:16.499964 139827419338560 train.py:379] starting iteration 55, 22528000 steps, 814.2538321018219
I0728 01:20:30.609173 139827419338560 train.py:394] {'eval/walltime': 244.37325048446655, 'training/sps': 40839.7728787374, 'training/walltime': 574.7678048610687, 'training/entropy_loss': Array(-0.05267522, dtype=float32), 'training/policy_loss': Array(0.00013017, dtype=float32), 'training/total_loss': Array(35414.56, dtype=float32), 'training/v_loss': Array(35414.613, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30640668, 0.13628744], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.772018, 11.734621], dtype=float32), 'eval/episode_reward': Array([-25022.484,   9988.702], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30095622, 0.1398966 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.074299335479736, 'eval/sps': 31416.444757839177}
I0728 01:20:30.611385 139827419338560 train.py:379] starting iteration 56, 22937600 steps, 828.3652532100677
I0728 01:20:44.760113 139827419338560 train.py:394] {'eval/walltime': 248.4893832206726, 'training/sps': 40848.16361378609, 'training/walltime': 584.7951834201813, 'training/entropy_loss': Array(-0.05294311, dtype=float32), 'training/policy_loss': Array(0.00019991, dtype=float32), 'training/total_loss': Array(33204.664, dtype=float32), 'training/v_loss': Array(33204.72, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2945511 , 0.13546203], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.777359, 11.633607], dtype=float32), 'eval/episode_reward': Array([-24337.664,   9852.776], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2888145 , 0.13894053], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.116132736206055, 'eval/sps': 31097.15070024221}
I0728 01:20:44.762326 139827419338560 train.py:379] starting iteration 57, 23347200 steps, 842.5161945819855
I0728 01:20:58.899855 139827419338560 train.py:394] {'eval/walltime': 252.57390809059143, 'training/sps': 40764.26987860555, 'training/walltime': 594.8431985378265, 'training/entropy_loss': Array(-0.05294252, dtype=float32), 'training/policy_loss': Array(0.00027605, dtype=float32), 'training/total_loss': Array(31936.828, dtype=float32), 'training/v_loss': Array(31936.883, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.324982  , 0.14405419], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.438839, 12.339941], dtype=float32), 'eval/episode_reward': Array([-25868.867,  10545.35 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.320268  , 0.14661996], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.084524869918823, 'eval/sps': 31337.794254278564}
I0728 01:20:58.901965 139827419338560 train.py:379] starting iteration 58, 23756800 steps, 856.6558327674866
I0728 01:21:13.026536 139827419338560 train.py:394] {'eval/walltime': 256.64681696891785, 'training/sps': 40770.00939070038, 'training/walltime': 604.889799118042, 'training/entropy_loss': Array(-0.05312723, dtype=float32), 'training/policy_loss': Array(0.000152, dtype=float32), 'training/total_loss': Array(31470.574, dtype=float32), 'training/v_loss': Array(31470.627, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32189944, 0.14037092], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.177464, 12.057562], dtype=float32), 'eval/episode_reward': Array([-25546.797,  10233.731], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31726676, 0.14300103], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.072908878326416, 'eval/sps': 31427.170070300224}
I0728 01:21:13.028646 139827419338560 train.py:379] starting iteration 59, 24166400 steps, 870.7825140953064
I0728 01:21:27.149198 139827419338560 train.py:394] {'eval/walltime': 260.72141790390015, 'training/sps': 40793.84569574754, 'training/walltime': 614.9305293560028, 'training/entropy_loss': Array(-0.05368722, dtype=float32), 'training/policy_loss': Array(4.2614593e-05, dtype=float32), 'training/total_loss': Array(30889.33, dtype=float32), 'training/v_loss': Array(30889.387, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.321741 , 0.1276032], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.1638  , 10.939331], dtype=float32), 'eval/episode_reward': Array([-26611.512,   8716.401], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3172294, 0.1301705], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0746009349823, 'eval/sps': 31414.119331555114}
I0728 01:21:27.151343 139827419338560 train.py:379] starting iteration 60, 24576000 steps, 884.9052102565765
I0728 01:21:41.310041 139827419338560 train.py:394] {'eval/walltime': 264.79544830322266, 'training/sps': 40636.14545012443, 'training/walltime': 625.0102255344391, 'training/entropy_loss': Array(-0.05364393, dtype=float32), 'training/policy_loss': Array(0.00013707, dtype=float32), 'training/total_loss': Array(29612.11, dtype=float32), 'training/v_loss': Array(29612.164, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32220003, 0.14213915], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.149662, 12.195057], dtype=float32), 'eval/episode_reward': Array([-26422.867,   9767.42 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31749135, 0.14510556], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.07403039932251, 'eval/sps': 31418.518629926213}
I0728 01:21:41.312147 139827419338560 train.py:379] starting iteration 61, 24985600 steps, 899.066015958786
I0728 01:21:55.417897 139827419338560 train.py:394] {'eval/walltime': 268.90962862968445, 'training/sps': 41014.60590417148, 'training/walltime': 634.9969117641449, 'training/entropy_loss': Array(-0.05330008, dtype=float32), 'training/policy_loss': Array(0.00019824, dtype=float32), 'training/total_loss': Array(28611.645, dtype=float32), 'training/v_loss': Array(28611.7, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30961302, 0.12202994], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.124332, 10.518177], dtype=float32), 'eval/episode_reward': Array([-24826.621,   9330.177], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30456507, 0.12480727], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.114180326461792, 'eval/sps': 31111.90804562531}
I0728 01:21:55.420011 139827419338560 train.py:379] starting iteration 62, 25395200 steps, 913.1738784313202
I0728 01:22:09.544342 139827419338560 train.py:394] {'eval/walltime': 272.9849100112915, 'training/sps': 40780.61524506844, 'training/walltime': 645.040899515152, 'training/entropy_loss': Array(-0.0527833, dtype=float32), 'training/policy_loss': Array(0.00079846, dtype=float32), 'training/total_loss': Array(28113.252, dtype=float32), 'training/v_loss': Array(28113.305, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3154647 , 0.12241859], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.648176, 10.57103 ], dtype=float32), 'eval/episode_reward': Array([-25398.621,   9242.53 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3106104, 0.1256644], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.075281381607056, 'eval/sps': 31408.874140004584}
I0728 01:22:09.546434 139827419338560 train.py:379] starting iteration 63, 25804800 steps, 927.3003015518188
I0728 01:22:23.679338 139827419338560 train.py:394] {'eval/walltime': 277.05720806121826, 'training/sps': 40733.12594055294, 'training/walltime': 655.0965971946716, 'training/entropy_loss': Array(-0.05275391, dtype=float32), 'training/policy_loss': Array(0.00060061, dtype=float32), 'training/total_loss': Array(27834.566, dtype=float32), 'training/v_loss': Array(27834.617, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28479868, 0.13336965], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.992666, 11.509227], dtype=float32), 'eval/episode_reward': Array([-23224.617,   9831.603], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27887306, 0.1372442 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.072298049926758, 'eval/sps': 31431.88401995825}
I0728 01:22:23.681431 139827419338560 train.py:379] starting iteration 64, 26214400 steps, 941.4352986812592
I0728 01:22:37.832120 139827419338560 train.py:394] {'eval/walltime': 281.1486508846283, 'training/sps': 40746.44542427453, 'training/walltime': 665.1490077972412, 'training/entropy_loss': Array(-0.05244621, dtype=float32), 'training/policy_loss': Array(0.00056554, dtype=float32), 'training/total_loss': Array(27678.129, dtype=float32), 'training/v_loss': Array(27678.184, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31314677, 0.13466018], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.44231 , 11.571557], dtype=float32), 'eval/episode_reward': Array([-25453.889,   9770.715], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3079275, 0.1376494], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.091442823410034, 'eval/sps': 31284.80722439077}
I0728 01:22:37.836090 139827419338560 train.py:379] starting iteration 65, 26624000 steps, 955.5899574756622
I0728 01:22:51.946776 139827419338560 train.py:394] {'eval/walltime': 285.22178506851196, 'training/sps': 40827.059829968974, 'training/walltime': 675.1815695762634, 'training/entropy_loss': Array(-0.05231171, dtype=float32), 'training/policy_loss': Array(0.00118101, dtype=float32), 'training/total_loss': Array(26747.86, dtype=float32), 'training/v_loss': Array(26747.91, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30051923, 0.12004419], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.387432, 10.258517], dtype=float32), 'eval/episode_reward': Array([-24189.922,   8561.023], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2956222 , 0.12261982], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.073134183883667, 'eval/sps': 31425.43167530859}
I0728 01:22:51.948899 139827419338560 train.py:379] starting iteration 66, 27033600 steps, 969.7027671337128
I0728 01:23:06.070262 139827419338560 train.py:394] {'eval/walltime': 289.30396914482117, 'training/sps': 40821.54576640992, 'training/walltime': 685.2154865264893, 'training/entropy_loss': Array(-0.05200084, dtype=float32), 'training/policy_loss': Array(0.00206584, dtype=float32), 'training/total_loss': Array(25734.77, dtype=float32), 'training/v_loss': Array(25734.82, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28476155, 0.10581803], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.013977 ,  9.1119585], dtype=float32), 'eval/episode_reward': Array([-23430.338,   8410.223], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27920997, 0.10908248], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.082184076309204, 'eval/sps': 31355.76387719579}
I0728 01:23:06.072448 139827419338560 train.py:379] starting iteration 67, 27443200 steps, 983.8263154029846
I0728 01:23:20.188920 139827419338560 train.py:394] {'eval/walltime': 293.40283250808716, 'training/sps': 40909.35663879499, 'training/walltime': 695.227865934372, 'training/entropy_loss': Array(-0.05132901, dtype=float32), 'training/policy_loss': Array(0.00365195, dtype=float32), 'training/total_loss': Array(25908.514, dtype=float32), 'training/v_loss': Array(25908.56, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31503296, 0.1403432 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.605719, 12.073283], dtype=float32), 'eval/episode_reward': Array([-25298.047,  10639.775], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30945542, 0.14433967], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.098863363265991, 'eval/sps': 31228.169532835822}
I0728 01:23:20.191041 139827419338560 train.py:379] starting iteration 68, 27852800 steps, 997.9449093341827
I0728 01:23:34.329685 139827419338560 train.py:394] {'eval/walltime': 297.48320722579956, 'training/sps': 40743.78991374346, 'training/walltime': 705.2809317111969, 'training/entropy_loss': Array(-0.04947346, dtype=float32), 'training/policy_loss': Array(0.00572015, dtype=float32), 'training/total_loss': Array(26132.504, dtype=float32), 'training/v_loss': Array(26132.549, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32235628, 0.13461962], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.302202, 11.611664], dtype=float32), 'eval/episode_reward': Array([-24707.938,   9669.408], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31726593, 0.1383201 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.080374717712402, 'eval/sps': 31369.667948477334}
I0728 01:23:34.331817 139827419338560 train.py:379] starting iteration 69, 28262400 steps, 1012.0856847763062
I0728 01:23:48.448745 139827419338560 train.py:394] {'eval/walltime': 301.5580561161041, 'training/sps': 40809.68161245933, 'training/walltime': 715.317765712738, 'training/entropy_loss': Array(-0.04742445, dtype=float32), 'training/policy_loss': Array(0.00638562, dtype=float32), 'training/total_loss': Array(26248.82, dtype=float32), 'training/v_loss': Array(26248.86, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3154622 , 0.14577521], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.683512, 12.495091], dtype=float32), 'eval/episode_reward': Array([-25137.32 ,   9968.379], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31064117, 0.14836799], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.074848890304565, 'eval/sps': 31412.207776478535}
I0728 01:23:48.450888 139827419338560 train.py:379] starting iteration 70, 28672000 steps, 1026.2047555446625
I0728 01:24:02.577566 139827419338560 train.py:394] {'eval/walltime': 305.66455364227295, 'training/sps': 40898.21346054251, 'training/walltime': 725.3328731060028, 'training/entropy_loss': Array(-0.04525343, dtype=float32), 'training/policy_loss': Array(0.0077385, dtype=float32), 'training/total_loss': Array(24616.549, dtype=float32), 'training/v_loss': Array(24616.586, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31489578, 0.13755202], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.617495, 11.902238], dtype=float32), 'eval/episode_reward': Array([-25052.98 ,  10151.138], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30954435, 0.14100797], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.106497526168823, 'eval/sps': 31170.114966419624}
I0728 01:24:02.579670 139827419338560 train.py:379] starting iteration 71, 29081600 steps, 1040.3335378170013
I0728 01:24:16.736347 139827419338560 train.py:394] {'eval/walltime': 309.7827887535095, 'training/sps': 40824.60431615356, 'training/walltime': 735.3660383224487, 'training/entropy_loss': Array(-0.03961975, dtype=float32), 'training/policy_loss': Array(0.01174709, dtype=float32), 'training/total_loss': Array(23525.32, dtype=float32), 'training/v_loss': Array(23525.348, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2943215 , 0.12951964], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.881739, 11.162247], dtype=float32), 'eval/episode_reward': Array([-23753.383,   9958.337], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28886333, 0.13235912], dtype=float32), 'eval/avg_episode_length': Array([995.2422  ,  53.617825], dtype=float32), 'eval/epoch_eval_time': 4.118235111236572, 'eval/sps': 31081.275483945294}
I0728 01:24:16.738428 139827419338560 train.py:379] starting iteration 72, 29491200 steps, 1054.4922959804535
I0728 01:24:30.848653 139827419338560 train.py:394] {'eval/walltime': 313.89410734176636, 'training/sps': 40985.298819218384, 'training/walltime': 745.3598656654358, 'training/entropy_loss': Array(-0.03406093, dtype=float32), 'training/policy_loss': Array(0.01471665, dtype=float32), 'training/total_loss': Array(23632.867, dtype=float32), 'training/v_loss': Array(23632.885, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30873075, 0.14529565], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.122288, 12.53605 ], dtype=float32), 'eval/episode_reward': Array([-24767.78 ,  10607.079], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3030743 , 0.14862731], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.111318588256836, 'eval/sps': 31133.563904681712}
I0728 01:24:30.850780 139827419338560 train.py:379] starting iteration 73, 29900800 steps, 1068.604648590088
I0728 01:24:44.964525 139827419338560 train.py:394] {'eval/walltime': 317.9703919887543, 'training/sps': 40828.61323323418, 'training/walltime': 755.3920457363129, 'training/entropy_loss': Array(-0.02793441, dtype=float32), 'training/policy_loss': Array(0.01681932, dtype=float32), 'training/total_loss': Array(23936.852, dtype=float32), 'training/v_loss': Array(23936.863, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29694536, 0.13365994], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.133707, 11.468661], dtype=float32), 'eval/episode_reward': Array([-24195.33 ,  10017.624], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29095012, 0.13689743], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.076284646987915, 'eval/sps': 31401.143709280193}
I0728 01:24:44.966631 139827419338560 train.py:379] starting iteration 74, 30310400 steps, 1082.7204978466034
I0728 01:24:59.105595 139827419338560 train.py:394] {'eval/walltime': 322.08118867874146, 'training/sps': 40867.8989676947, 'training/walltime': 765.4145820140839, 'training/entropy_loss': Array(-0.02400226, dtype=float32), 'training/policy_loss': Array(0.01771856, dtype=float32), 'training/total_loss': Array(24070.064, dtype=float32), 'training/v_loss': Array(24070.07, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29635483, 0.13071945], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.064709, 11.264459], dtype=float32), 'eval/episode_reward': Array([-23620.998,  10203.794], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2902486 , 0.13430826], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.110796689987183, 'eval/sps': 31137.516557745184}
I0728 01:24:59.107756 139827419338560 train.py:379] starting iteration 75, 30720000 steps, 1096.8616244792938
I0728 01:25:13.250023 139827419338560 train.py:394] {'eval/walltime': 326.1887426376343, 'training/sps': 40838.38462753529, 'training/walltime': 775.4443616867065, 'training/entropy_loss': Array(-0.01978641, dtype=float32), 'training/policy_loss': Array(0.0192017, dtype=float32), 'training/total_loss': Array(22195.305, dtype=float32), 'training/v_loss': Array(22195.307, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29911533, 0.12432443], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.38446 , 10.780638], dtype=float32), 'eval/episode_reward': Array([-24165.34 ,   8869.075], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29347926, 0.12740453], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.107553958892822, 'eval/sps': 31162.098241675194}
I0728 01:25:13.298680 139827419338560 train.py:379] starting iteration 76, 31129600 steps, 1111.0525455474854
I0728 01:25:27.413749 139827419338560 train.py:394] {'eval/walltime': 330.2637996673584, 'training/sps': 40816.79830738719, 'training/walltime': 785.4794456958771, 'training/entropy_loss': Array(-0.01155512, dtype=float32), 'training/policy_loss': Array(0.02138306, dtype=float32), 'training/total_loss': Array(21506.795, dtype=float32), 'training/v_loss': Array(21506.785, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30469987, 0.14579196], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.807838, 12.575907], dtype=float32), 'eval/episode_reward': Array([-24584.684 ,  10407.3545], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2988922 , 0.14915793], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.075057029724121, 'eval/sps': 31410.603352627317}
I0728 01:25:27.415977 139827419338560 train.py:379] starting iteration 77, 31539200 steps, 1125.169844865799
I0728 01:25:41.524154 139827419338560 train.py:394] {'eval/walltime': 334.3447711467743, 'training/sps': 40870.03397710233, 'training/walltime': 795.5014584064484, 'training/entropy_loss': Array(-0.00731012, dtype=float32), 'training/policy_loss': Array(0.02154838, dtype=float32), 'training/total_loss': Array(22291.941, dtype=float32), 'training/v_loss': Array(22291.926, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29231414, 0.12945834], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.745014, 11.177567], dtype=float32), 'eval/episode_reward': Array([-23726.434,   9696.961], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28573033, 0.1338078 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0809714794158936, 'eval/sps': 31365.0807523704}
I0728 01:25:41.526253 139827419338560 train.py:379] starting iteration 78, 31948800 steps, 1139.280121088028
I0728 01:25:55.627961 139827419338560 train.py:394] {'eval/walltime': 338.4280152320862, 'training/sps': 40912.27539781597, 'training/walltime': 805.5131235122681, 'training/entropy_loss': Array(-0.00284027, dtype=float32), 'training/policy_loss': Array(0.02241024, dtype=float32), 'training/total_loss': Array(22598.164, dtype=float32), 'training/v_loss': Array(22598.145, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3212964 , 0.13967511], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.26923 , 12.049158], dtype=float32), 'eval/episode_reward': Array([-25505.227,  10177.321], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31556594, 0.14329544], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.08324408531189, 'eval/sps': 31347.623929815356}
I0728 01:25:55.631719 139827419338560 train.py:379] starting iteration 79, 32358400 steps, 1153.3855862617493
I0728 01:26:09.762013 139827419338560 train.py:394] {'eval/walltime': 342.5447793006897, 'training/sps': 40924.380611453016, 'training/walltime': 815.5218272209167, 'training/entropy_loss': Array(0.0003251, dtype=float32), 'training/policy_loss': Array(0.02309319, dtype=float32), 'training/total_loss': Array(22471.172, dtype=float32), 'training/v_loss': Array(22471.148, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30044943, 0.13272837], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.538427, 11.461044], dtype=float32), 'eval/episode_reward': Array([-25108.82 ,   9059.773], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2944449 , 0.13660839], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.116764068603516, 'eval/sps': 31092.381751043613}
I0728 01:26:09.764198 139827419338560 train.py:379] starting iteration 80, 32768000 steps, 1167.518065214157
I0728 01:26:23.900772 139827419338560 train.py:394] {'eval/walltime': 346.619873046875, 'training/sps': 40733.03708951461, 'training/walltime': 825.5775468349457, 'training/entropy_loss': Array(0.00171547, dtype=float32), 'training/policy_loss': Array(0.02762804, dtype=float32), 'training/total_loss': Array(20605.611, dtype=float32), 'training/v_loss': Array(20605.582, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31289485, 0.12156103], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.588661, 10.478776], dtype=float32), 'eval/episode_reward': Array([-25335.367,   8651.763], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30764014, 0.12426576], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.075093746185303, 'eval/sps': 31410.320344119904}
I0728 01:26:23.902889 139827419338560 train.py:379] starting iteration 81, 33177600 steps, 1181.6567573547363
I0728 01:26:38.022411 139827419338560 train.py:394] {'eval/walltime': 350.6937470436096, 'training/sps': 40794.45014586976, 'training/walltime': 835.6181282997131, 'training/entropy_loss': Array(0.00390801, dtype=float32), 'training/policy_loss': Array(0.02595964, dtype=float32), 'training/total_loss': Array(19362.164, dtype=float32), 'training/v_loss': Array(19362.133, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3178657 , 0.13987246], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.058435, 12.072166], dtype=float32), 'eval/episode_reward': Array([-25339.1  ,  10183.602], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31270874, 0.14252536], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.073873996734619, 'eval/sps': 31419.724837488193}
I0728 01:26:38.024511 139827419338560 train.py:379] starting iteration 82, 33587200 steps, 1195.7783789634705
I0728 01:26:52.183139 139827419338560 train.py:394] {'eval/walltime': 354.7726397514343, 'training/sps': 40656.84252296987, 'training/walltime': 845.69269323349, 'training/entropy_loss': Array(0.00647693, dtype=float32), 'training/policy_loss': Array(0.02954996, dtype=float32), 'training/total_loss': Array(20050.3, dtype=float32), 'training/v_loss': Array(20050.266, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30282304, 0.14508891], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.764791, 12.518456], dtype=float32), 'eval/episode_reward': Array([-24317.629,   9550.617], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29714513, 0.14819217], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.078892707824707, 'eval/sps': 31381.065688355164}
I0728 01:26:52.185295 139827419338560 train.py:379] starting iteration 83, 33996800 steps, 1209.9391632080078
I0728 01:27:06.269840 139827419338560 train.py:394] {'eval/walltime': 358.8422055244446, 'training/sps': 40926.714570821765, 'training/walltime': 855.7008261680603, 'training/entropy_loss': Array(0.00852101, dtype=float32), 'training/policy_loss': Array(0.02539519, dtype=float32), 'training/total_loss': Array(20571.098, dtype=float32), 'training/v_loss': Array(20571.066, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3186099, 0.146019 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.11401, 12.56349], dtype=float32), 'eval/episode_reward': Array([-24881.695,   9486.852], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31333798, 0.14874004], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.069565773010254, 'eval/sps': 31452.98715870576}
I0728 01:27:06.274032 139827419338560 train.py:379] starting iteration 84, 34406400 steps, 1224.0278873443604
I0728 01:27:20.429330 139827419338560 train.py:394] {'eval/walltime': 362.95040917396545, 'training/sps': 40796.8429367292, 'training/walltime': 865.7408187389374, 'training/entropy_loss': Array(0.00963379, dtype=float32), 'training/policy_loss': Array(0.03100841, dtype=float32), 'training/total_loss': Array(20990.941, dtype=float32), 'training/v_loss': Array(20990.9, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33116537, 0.16450468], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.173569, 14.111491], dtype=float32), 'eval/episode_reward': Array([-26098.086,  10580.816], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32577664, 0.16792682], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.108203649520874, 'eval/sps': 31157.17012104018}
I0728 01:27:20.433900 139827419338560 train.py:379] starting iteration 85, 34816000 steps, 1238.187753200531
I0728 01:27:34.603345 139827419338560 train.py:394] {'eval/walltime': 367.03511452674866, 'training/sps': 40640.39046865351, 'training/walltime': 875.8194620609283, 'training/entropy_loss': Array(0.01262929, dtype=float32), 'training/policy_loss': Array(0.03009648, dtype=float32), 'training/total_loss': Array(19843.5, dtype=float32), 'training/v_loss': Array(19843.457, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31825754, 0.14755623], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.11459 , 12.727524], dtype=float32), 'eval/episode_reward': Array([-25028.918,  10193.286], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31286865, 0.15094234], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.084705352783203, 'eval/sps': 31336.409592624448}
I0728 01:27:34.605431 139827419338560 train.py:379] starting iteration 86, 35225600 steps, 1252.3592989444733
I0728 01:27:48.724291 139827419338560 train.py:394] {'eval/walltime': 371.11261463165283, 'training/sps': 40813.25904123318, 'training/walltime': 885.8554162979126, 'training/entropy_loss': Array(0.01792203, dtype=float32), 'training/policy_loss': Array(0.0282677, dtype=float32), 'training/total_loss': Array(18489.213, dtype=float32), 'training/v_loss': Array(18489.168, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3074367 , 0.14868434], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.158703, 12.873673], dtype=float32), 'eval/episode_reward': Array([-23720.018,  10814.599], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30151874, 0.1527914 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.077500104904175, 'eval/sps': 31391.783373849386}
I0728 01:27:48.726570 139827419338560 train.py:379] starting iteration 87, 35635200 steps, 1266.4804377555847
I0728 01:28:02.849848 139827419338560 train.py:394] {'eval/walltime': 375.1842637062073, 'training/sps': 40770.973067434636, 'training/walltime': 895.9017794132233, 'training/entropy_loss': Array(0.02578192, dtype=float32), 'training/policy_loss': Array(0.0305314, dtype=float32), 'training/total_loss': Array(19687.006, dtype=float32), 'training/v_loss': Array(19686.95, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33961084, 0.13661727], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.96265 , 11.776756], dtype=float32), 'eval/episode_reward': Array([-26447.328,   9489.514], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.33499748, 0.13944945], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.071649074554443, 'eval/sps': 31436.89391110085}
I0728 01:28:02.852125 139827419338560 train.py:379] starting iteration 88, 36044800 steps, 1280.6059930324554
I0728 01:28:16.950482 139827419338560 train.py:394] {'eval/walltime': 379.25906133651733, 'training/sps': 40886.125665963424, 'training/walltime': 905.9198477268219, 'training/entropy_loss': Array(0.03526895, dtype=float32), 'training/policy_loss': Array(0.02896775, dtype=float32), 'training/total_loss': Array(20386.508, dtype=float32), 'training/v_loss': Array(20386.445, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31977412, 0.15632665], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.194134, 13.497675], dtype=float32), 'eval/episode_reward': Array([-25149.55 ,  11178.282], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31360832, 0.1610435 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.074797630310059, 'eval/sps': 31412.602934654267}
I0728 01:28:16.952811 139827419338560 train.py:379] starting iteration 89, 36454400 steps, 1294.70667886734
I0728 01:28:31.106917 139827419338560 train.py:394] {'eval/walltime': 383.3350579738617, 'training/sps': 40664.56147611304, 'training/walltime': 915.9925003051758, 'training/entropy_loss': Array(0.04636867, dtype=float32), 'training/policy_loss': Array(0.02476188, dtype=float32), 'training/total_loss': Array(20939.984, dtype=float32), 'training/v_loss': Array(20939.914, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29371977, 0.12351103], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.08184 , 10.627589], dtype=float32), 'eval/episode_reward': Array([-23796.398,   9198.252], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28822955, 0.12656951], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.07599663734436, 'eval/sps': 31403.36251194653}
I0728 01:28:31.109201 139827419338560 train.py:379] starting iteration 90, 36864000 steps, 1308.8630695343018
I0728 01:28:45.239062 139827419338560 train.py:394] {'eval/walltime': 387.41035056114197, 'training/sps': 40758.73698293512, 'training/walltime': 926.0418794155121, 'training/entropy_loss': Array(0.06020582, dtype=float32), 'training/policy_loss': Array(0.0335841, dtype=float32), 'training/total_loss': Array(18457.195, dtype=float32), 'training/v_loss': Array(18457.102, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29334658, 0.12999156], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.017235, 11.193936], dtype=float32), 'eval/episode_reward': Array([-23845.223,   9894.982], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28736892, 0.13386275], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.075292587280273, 'eval/sps': 31408.78777624733}
I0728 01:28:45.241333 139827419338560 train.py:379] starting iteration 91, 37273600 steps, 1322.9952018260956
I0728 01:28:59.319377 139827419338560 train.py:394] {'eval/walltime': 391.48689317703247, 'training/sps': 40975.85672212736, 'training/walltime': 936.0380096435547, 'training/entropy_loss': Array(0.07805403, dtype=float32), 'training/policy_loss': Array(0.01849064, dtype=float32), 'training/total_loss': Array(15668.32, dtype=float32), 'training/v_loss': Array(15668.223, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28810427, 0.13723719], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.513432, 11.864267], dtype=float32), 'eval/episode_reward': Array([-23901.184,  10476.372], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28147924, 0.14233337], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.076542615890503, 'eval/sps': 31399.156604189935}
I0728 01:28:59.321633 139827419338560 train.py:379] starting iteration 92, 37683200 steps, 1337.0755014419556
I0728 01:29:13.485179 139827419338560 train.py:394] {'eval/walltime': 395.59970259666443, 'training/sps': 40780.02282084544, 'training/walltime': 946.0821433067322, 'training/entropy_loss': Array(0.09867527, dtype=float32), 'training/policy_loss': Array(0.01837754, dtype=float32), 'training/total_loss': Array(16480.477, dtype=float32), 'training/v_loss': Array(16480.36, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30313522, 0.14056344], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.787867, 12.148531], dtype=float32), 'eval/episode_reward': Array([-25162.71,  10956.76], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2976421, 0.1437824], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.112809419631958, 'eval/sps': 31122.278457399152}
I0728 01:29:13.487316 139827419338560 train.py:379] starting iteration 93, 38092800 steps, 1351.2411847114563
I0728 01:29:27.630887 139827419338560 train.py:394] {'eval/walltime': 399.6742753982544, 'training/sps': 40700.47805420857, 'training/walltime': 956.14590716362, 'training/entropy_loss': Array(0.12514943, dtype=float32), 'training/policy_loss': Array(0.01848827, dtype=float32), 'training/total_loss': Array(16927.05, dtype=float32), 'training/v_loss': Array(16926.908, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2737365 , 0.12528606], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.31736 , 10.840397], dtype=float32), 'eval/episode_reward': Array([-23094.775,  10091.69 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2675156 , 0.12960869], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.074572801589966, 'eval/sps': 31414.336234231054}
I0728 01:29:27.632987 139827419338560 train.py:379] starting iteration 94, 38502400 steps, 1365.3868551254272
I0728 01:29:41.743935 139827419338560 train.py:394] {'eval/walltime': 403.7433729171753, 'training/sps': 40809.46446624223, 'training/walltime': 966.1827945709229, 'training/entropy_loss': Array(0.15990739, dtype=float32), 'training/policy_loss': Array(0.02009768, dtype=float32), 'training/total_loss': Array(16898.625, dtype=float32), 'training/v_loss': Array(16898.445, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29123384, 0.12165374], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.749622, 10.487337], dtype=float32), 'eval/episode_reward': Array([-24434.998,   9906.773], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.285418 , 0.1250386], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.069097518920898, 'eval/sps': 31456.60663201428}
I0728 01:29:41.746030 139827419338560 train.py:379] starting iteration 95, 38912000 steps, 1379.4998984336853
I0728 01:29:55.858189 139827419338560 train.py:394] {'eval/walltime': 407.8425986766815, 'training/sps': 40929.083899590805, 'training/walltime': 976.190348148346, 'training/entropy_loss': Array(0.15178147, dtype=float32), 'training/policy_loss': Array(0.03510052, dtype=float32), 'training/total_loss': Array(17787.307, dtype=float32), 'training/v_loss': Array(17787.12, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2931882 , 0.13015063], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.851334, 11.197004], dtype=float32), 'eval/episode_reward': Array([-24827.805,   9761.128], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2873954 , 0.13375044], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.099225759506226, 'eval/sps': 31225.408774611697}
I0728 01:29:55.860376 139827419338560 train.py:379] starting iteration 96, 39321600 steps, 1393.614244222641
I0728 01:30:09.996894 139827419338560 train.py:394] {'eval/walltime': 411.9573962688446, 'training/sps': 40893.211620434886, 'training/walltime': 986.2066805362701, 'training/entropy_loss': Array(0.16939735, dtype=float32), 'training/policy_loss': Array(0.03280457, dtype=float32), 'training/total_loss': Array(16231.877, dtype=float32), 'training/v_loss': Array(16231.674, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2750728 , 0.14677833], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.235456, 12.665484], dtype=float32), 'eval/episode_reward': Array([-23201.36 ,  10766.893], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26826853, 0.1509627 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.114797592163086, 'eval/sps': 31107.240911140994}
I0728 01:30:09.998999 139827419338560 train.py:379] starting iteration 97, 39731200 steps, 1407.7528669834137
I0728 01:30:24.128222 139827419338560 train.py:394] {'eval/walltime': 416.07879638671875, 'training/sps': 40948.5686209206, 'training/walltime': 996.2094721794128, 'training/entropy_loss': Array(0.18642905, dtype=float32), 'training/policy_loss': Array(0.03162558, dtype=float32), 'training/total_loss': Array(16393.332, dtype=float32), 'training/v_loss': Array(16393.113, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.284284  , 0.12889573], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.048285, 11.081898], dtype=float32), 'eval/episode_reward': Array([-23633.223,  10102.851], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.278796  , 0.13171956], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.1214001178741455, 'eval/sps': 31057.40678874526}
I0728 01:30:24.130437 139827419338560 train.py:379] starting iteration 98, 40140800 steps, 1421.8843047618866
I0728 01:30:38.258706 139827419338560 train.py:394] {'eval/walltime': 420.188170671463, 'training/sps': 40903.36747274691, 'training/walltime': 1006.2233176231384, 'training/entropy_loss': Array(0.20972323, dtype=float32), 'training/policy_loss': Array(0.03191501, dtype=float32), 'training/total_loss': Array(16001.15, dtype=float32), 'training/v_loss': Array(16000.908, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27333322, 0.12233637], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.1052  , 10.524939], dtype=float32), 'eval/episode_reward': Array([-23321.434,   9733.187], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26731375, 0.12585458], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.109374284744263, 'eval/sps': 31148.294394888828}
I0728 01:30:38.260935 139827419338560 train.py:379] starting iteration 99, 40550400 steps, 1436.0148029327393
I0728 01:30:52.347597 139827419338560 train.py:394] {'eval/walltime': 424.29178047180176, 'training/sps': 41050.37965080389, 'training/walltime': 1016.2013008594513, 'training/entropy_loss': Array(0.15027207, dtype=float32), 'training/policy_loss': Array(0.04213053, dtype=float32), 'training/total_loss': Array(15992.986, dtype=float32), 'training/v_loss': Array(15992.794, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30306515, 0.12338184], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.63559 , 10.578692], dtype=float32), 'eval/episode_reward': Array([-25272.898,   9972.27 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29850143, 0.1264199 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.103609800338745, 'eval/sps': 31192.049494918803}
I0728 01:30:52.349807 139827419338560 train.py:379] starting iteration 100, 40960000 steps, 1450.1036758422852
I0728 01:31:06.514213 139827419338560 train.py:394] {'eval/walltime': 428.41116285324097, 'training/sps': 40797.859231210496, 'training/walltime': 1026.241043329239, 'training/entropy_loss': Array(0.12147569, dtype=float32), 'training/policy_loss': Array(0.01938399, dtype=float32), 'training/total_loss': Array(16130.504, dtype=float32), 'training/v_loss': Array(16130.363, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2963881 , 0.11832958], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.014168, 10.156924], dtype=float32), 'eval/episode_reward': Array([-24891.637,   9421.625], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2913792 , 0.12149207], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.119382381439209, 'eval/sps': 31072.619181150163}
I0728 01:31:06.582592 139827419338560 train.py:379] starting iteration 101, 41369600 steps, 1464.336436510086
I0728 01:31:20.661891 139827419338560 train.py:394] {'eval/walltime': 432.49525570869446, 'training/sps': 41001.931554128714, 'training/walltime': 1036.230816602707, 'training/entropy_loss': Array(0.14115483, dtype=float32), 'training/policy_loss': Array(0.00807996, dtype=float32), 'training/total_loss': Array(12948.961, dtype=float32), 'training/v_loss': Array(12948.8125, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2897938, 0.1145029], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.44293 ,  9.875774], dtype=float32), 'eval/episode_reward': Array([-24057.645,   9268.261], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28427178, 0.11791315], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.084092855453491, 'eval/sps': 31341.109159426072}
I0728 01:31:20.664284 139827419338560 train.py:379] starting iteration 102, 41779200 steps, 1478.4181516170502
I0728 01:31:34.783819 139827419338560 train.py:394] {'eval/walltime': 436.60876750946045, 'training/sps': 40955.6000820644, 'training/walltime': 1046.2318909168243, 'training/entropy_loss': Array(0.15342292, dtype=float32), 'training/policy_loss': Array(0.00579378, dtype=float32), 'training/total_loss': Array(12413.592, dtype=float32), 'training/v_loss': Array(12413.434, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28859338, 0.10898533], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.35161,  9.37371], dtype=float32), 'eval/episode_reward': Array([-24309.648,   8933.341], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2835483 , 0.11180229], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.113511800765991, 'eval/sps': 31116.964335963417}
I0728 01:31:34.786015 139827419338560 train.py:379] starting iteration 103, 42188800 steps, 1492.5398831367493
I0728 01:31:48.880219 139827419338560 train.py:394] {'eval/walltime': 440.7173218727112, 'training/sps': 41038.86444454095, 'training/walltime': 1056.2126739025116, 'training/entropy_loss': Array(0.16200322, dtype=float32), 'training/policy_loss': Array(0.00484024, dtype=float32), 'training/total_loss': Array(12060.043, dtype=float32), 'training/v_loss': Array(12059.875, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27784768, 0.09814412], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.438908,  8.446909], dtype=float32), 'eval/episode_reward': Array([-23396.246,   8437.674], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2725261 , 0.10094352], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.108554363250732, 'eval/sps': 31154.510487899453}
I0728 01:31:48.882321 139827419338560 train.py:379] starting iteration 104, 42598400 steps, 1506.6361882686615
I0728 01:32:02.940803 139827419338560 train.py:394] {'eval/walltime': 444.7998697757721, 'training/sps': 41080.551023030675, 'training/walltime': 1066.1833288669586, 'training/entropy_loss': Array(0.16659716, dtype=float32), 'training/policy_loss': Array(0.00534248, dtype=float32), 'training/total_loss': Array(11817.185, dtype=float32), 'training/v_loss': Array(11817.012, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30596748, 0.11758323], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.874668, 10.118519], dtype=float32), 'eval/episode_reward': Array([-25262.2  ,   9045.781], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30130887, 0.12015317], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.082547903060913, 'eval/sps': 31352.96952768914}
I0728 01:32:02.942934 139827419338560 train.py:379] starting iteration 105, 43008000 steps, 1520.696801185608
I0728 01:32:17.052691 139827419338560 train.py:394] {'eval/walltime': 448.8844974040985, 'training/sps': 40877.963464340035, 'training/walltime': 1076.203397512436, 'training/entropy_loss': Array(0.15551895, dtype=float32), 'training/policy_loss': Array(0.01364057, dtype=float32), 'training/total_loss': Array(15827.399, dtype=float32), 'training/v_loss': Array(15827.23, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29707384, 0.12182573], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.081005, 10.502511], dtype=float32), 'eval/episode_reward': Array([-24846.654,   9511.252], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2920605, 0.1251441], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.084627628326416, 'eval/sps': 31337.0058784147}
I0728 01:32:17.054859 139827419338560 train.py:379] starting iteration 106, 43417600 steps, 1534.8087265491486
I0728 01:32:31.117256 139827419338560 train.py:394] {'eval/walltime': 452.95987129211426, 'training/sps': 41034.85333988749, 'training/walltime': 1086.1851561069489, 'training/entropy_loss': Array(0.1587801, dtype=float32), 'training/policy_loss': Array(0.00751758, dtype=float32), 'training/total_loss': Array(12202.82, dtype=float32), 'training/v_loss': Array(12202.654, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28474557, 0.11130176], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.011822,  9.655618], dtype=float32), 'eval/episode_reward': Array([-23672.824,   9202.782], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2791955 , 0.11500351], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.075373888015747, 'eval/sps': 31408.16119384858}
I0728 01:32:31.119399 139827419338560 train.py:379] starting iteration 107, 43827200 steps, 1548.8732676506042
I0728 01:32:45.224167 139827419338560 train.py:394] {'eval/walltime': 457.0757794380188, 'training/sps': 41027.51442129307, 'training/walltime': 1096.1687002182007, 'training/entropy_loss': Array(0.15110494, dtype=float32), 'training/policy_loss': Array(0.00662096, dtype=float32), 'training/total_loss': Array(12070.289, dtype=float32), 'training/v_loss': Array(12070.131, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27060583, 0.10329585], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.803043,  8.905225], dtype=float32), 'eval/episode_reward': Array([-22753.664,   8612.791], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.265281  , 0.10633997], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.115908145904541, 'eval/sps': 31098.847559891263}
I0728 01:32:45.226440 139827419338560 train.py:379] starting iteration 108, 44236800 steps, 1562.9803082942963
I0728 01:32:59.332098 139827419338560 train.py:394] {'eval/walltime': 461.1875147819519, 'training/sps': 41005.38519374965, 'training/walltime': 1106.157632112503, 'training/entropy_loss': Array(0.13785602, dtype=float32), 'training/policy_loss': Array(0.00659815, dtype=float32), 'training/total_loss': Array(12146.239, dtype=float32), 'training/v_loss': Array(12146.095, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2961511 , 0.14405286], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.989067, 12.394678], dtype=float32), 'eval/episode_reward': Array([-24144.95 ,  10418.561], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29075313, 0.1478141 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.1117353439331055, 'eval/sps': 31130.408280986496}
I0728 01:32:59.334357 139827419338560 train.py:379] starting iteration 109, 44646400 steps, 1577.0882189273834
I0728 01:33:13.461414 139827419338560 train.py:394] {'eval/walltime': 465.3005394935608, 'training/sps': 40924.86317520613, 'training/walltime': 1116.166217803955, 'training/entropy_loss': Array(0.12093746, dtype=float32), 'training/policy_loss': Array(0.00742472, dtype=float32), 'training/total_loss': Array(12524.98, dtype=float32), 'training/v_loss': Array(12524.852, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3127099 , 0.13363954], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.38993 , 11.517167], dtype=float32), 'eval/episode_reward': Array([-25319.855,  10257.408], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3075572, 0.1372698], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.113024711608887, 'eval/sps': 31120.649394282485}
I0728 01:33:13.463669 139827419338560 train.py:379] starting iteration 110, 45056000 steps, 1591.217536687851
I0728 01:33:27.589112 139827419338560 train.py:394] {'eval/walltime': 469.40935802459717, 'training/sps': 40914.3331978641, 'training/walltime': 1126.1773793697357, 'training/entropy_loss': Array(0.09804446, dtype=float32), 'training/policy_loss': Array(0.00902685, dtype=float32), 'training/total_loss': Array(15332.481, dtype=float32), 'training/v_loss': Array(15332.375, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3082357, 0.1291595], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.055557, 11.089909], dtype=float32), 'eval/episode_reward': Array([-24467.178,   9285.296], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30346766, 0.13201348], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.108818531036377, 'eval/sps': 31152.50747462781}
I0728 01:33:27.591391 139827419338560 train.py:379] starting iteration 111, 45465600 steps, 1605.3452589511871
I0728 01:33:41.688953 139827419338560 train.py:394] {'eval/walltime': 473.5040354728699, 'training/sps': 40970.153883686566, 'training/walltime': 1136.174901008606, 'training/entropy_loss': Array(0.08014823, dtype=float32), 'training/policy_loss': Array(0.00594861, dtype=float32), 'training/total_loss': Array(13498.688, dtype=float32), 'training/v_loss': Array(13498.602, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30720925, 0.14235215], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.915352, 12.288969], dtype=float32), 'eval/episode_reward': Array([-24868.   ,  10199.354], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30152133, 0.1463162 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.094677448272705, 'eval/sps': 31260.093528000696}
I0728 01:33:41.691177 139827419338560 train.py:379] starting iteration 112, 45875200 steps, 1619.4450449943542
I0728 01:33:55.725929 139827419338560 train.py:394] {'eval/walltime': 477.5811870098114, 'training/sps': 41155.69912195156, 'training/walltime': 1146.1273500919342, 'training/entropy_loss': Array(0.0610561, dtype=float32), 'training/policy_loss': Array(0.0055892, dtype=float32), 'training/total_loss': Array(14371.746, dtype=float32), 'training/v_loss': Array(14371.68, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32654366, 0.13753448], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.584286, 11.815483], dtype=float32), 'eval/episode_reward': Array([-26060.709,   8947.421], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32179976, 0.14046487], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.077151536941528, 'eval/sps': 31394.46715194184}
I0728 01:33:55.728183 139827419338560 train.py:379] starting iteration 113, 46284800 steps, 1633.482050895691
I0728 01:34:09.796087 139827419338560 train.py:394] {'eval/walltime': 481.6550281047821, 'training/sps': 41006.55187052047, 'training/walltime': 1156.1159977912903, 'training/entropy_loss': Array(0.04417648, dtype=float32), 'training/policy_loss': Array(0.00458017, dtype=float32), 'training/total_loss': Array(15096.836, dtype=float32), 'training/v_loss': Array(15096.786, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31935108, 0.14002292], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.991526, 12.050203], dtype=float32), 'eval/episode_reward': Array([-25192.746,   9697.686], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31436998, 0.14330386], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.073841094970703, 'eval/sps': 31419.97859416274}
I0728 01:34:09.798370 139827419338560 train.py:379] starting iteration 114, 46694400 steps, 1647.5522384643555
I0728 01:34:23.943305 139827419338560 train.py:394] {'eval/walltime': 485.7662830352783, 'training/sps': 40844.1479435754, 'training/walltime': 1166.1443622112274, 'training/entropy_loss': Array(0.0272585, dtype=float32), 'training/policy_loss': Array(0.00413576, dtype=float32), 'training/total_loss': Array(15792.301, dtype=float32), 'training/v_loss': Array(15792.269, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.35761103, 0.14958979], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([30.273655, 12.855275], dtype=float32), 'eval/episode_reward': Array([-27291.63 ,  10493.587], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.35327244, 0.15250333], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.111254930496216, 'eval/sps': 31134.045969888517}
I0728 01:34:23.945544 139827419338560 train.py:379] starting iteration 115, 47104000 steps, 1661.699411392212
I0728 01:34:38.040133 139827419338560 train.py:394] {'eval/walltime': 489.86403942108154, 'training/sps': 40994.9908034984, 'training/walltime': 1176.1358268260956, 'training/entropy_loss': Array(0.01321925, dtype=float32), 'training/policy_loss': Array(0.00378868, dtype=float32), 'training/total_loss': Array(15791.379, dtype=float32), 'training/v_loss': Array(15791.362, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.35372669, 0.16486882], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([29.884932, 14.203949], dtype=float32), 'eval/episode_reward': Array([-27166.648,  10556.914], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.34909728, 0.16789155], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.097756385803223, 'eval/sps': 31236.60558335267}
I0728 01:34:38.042391 139827419338560 train.py:379] starting iteration 116, 47513600 steps, 1675.7962582111359
I0728 01:34:52.078047 139827419338560 train.py:394] {'eval/walltime': 493.93598341941833, 'training/sps': 41130.30182186359, 'training/walltime': 1186.0944213867188, 'training/entropy_loss': Array(0.00122382, dtype=float32), 'training/policy_loss': Array(0.00334955, dtype=float32), 'training/total_loss': Array(14732.172, dtype=float32), 'training/v_loss': Array(14732.168, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33872285, 0.13898888], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.663448, 11.966518], dtype=float32), 'eval/episode_reward': Array([-26009.281,   9676.399], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.33416033, 0.14160195], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.071943998336792, 'eval/sps': 31434.616991855073}
I0728 01:34:52.080321 139827419338560 train.py:379] starting iteration 117, 47923200 steps, 1689.8341896533966
I0728 01:35:06.179316 139827419338560 train.py:394] {'eval/walltime': 498.01136326789856, 'training/sps': 40885.71893800464, 'training/walltime': 1196.1125893592834, 'training/entropy_loss': Array(-0.00847578, dtype=float32), 'training/policy_loss': Array(0.00338833, dtype=float32), 'training/total_loss': Array(15326.383, dtype=float32), 'training/v_loss': Array(15326.387, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.34877616, 0.13136052], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([29.518578, 11.309314], dtype=float32), 'eval/episode_reward': Array([-27411.832,   9533.414], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3444016 , 0.13446659], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.075379848480225, 'eval/sps': 31408.11525770617}
I0728 01:35:06.181579 139827419338560 train.py:379] starting iteration 118, 48332800 steps, 1703.9354474544525
I0728 01:35:20.273168 139827419338560 train.py:394] {'eval/walltime': 502.0842332839966, 'training/sps': 40904.8380601321, 'training/walltime': 1206.1260747909546, 'training/entropy_loss': Array(-0.01646047, dtype=float32), 'training/policy_loss': Array(0.00318201, dtype=float32), 'training/total_loss': Array(15575.51, dtype=float32), 'training/v_loss': Array(15575.522, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33899012, 0.1395611 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.705818, 12.01746 ], dtype=float32), 'eval/episode_reward': Array([-26359.383,   9766.791], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3343411 , 0.14262271], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0728700160980225, 'eval/sps': 31427.469939890023}
I0728 01:35:20.275470 139827419338560 train.py:379] starting iteration 119, 48742400 steps, 1718.0293369293213
I0728 01:35:34.358905 139827419338560 train.py:394] {'eval/walltime': 506.1840262413025, 'training/sps': 41048.97508315408, 'training/walltime': 1216.1043994426727, 'training/entropy_loss': Array(-0.02266153, dtype=float32), 'training/policy_loss': Array(0.00324935, dtype=float32), 'training/total_loss': Array(15666.714, dtype=float32), 'training/v_loss': Array(15666.734, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.34139407, 0.1482846 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.92952 , 12.743553], dtype=float32), 'eval/episode_reward': Array([-26368.781,  10030.398], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.33716768, 0.1506324 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.099792957305908, 'eval/sps': 31221.08880447282}
I0728 01:35:34.361107 139827419338560 train.py:379] starting iteration 120, 49152000 steps, 1732.1149756908417
I0728 01:35:48.422410 139827419338560 train.py:394] {'eval/walltime': 510.2702775001526, 'training/sps': 41085.33449253211, 'training/walltime': 1226.073893547058, 'training/entropy_loss': Array(-0.02635756, dtype=float32), 'training/policy_loss': Array(0.00399309, dtype=float32), 'training/total_loss': Array(14530.822, dtype=float32), 'training/v_loss': Array(14530.846, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.34939718, 0.14385264], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([29.630018, 12.370136], dtype=float32), 'eval/episode_reward': Array([-27284.73 ,   9766.446], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3450464 , 0.14682892], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.086251258850098, 'eval/sps': 31324.55443672844}
I0728 01:35:48.424696 139827419338560 train.py:379] starting iteration 121, 49561600 steps, 1746.1785645484924
I0728 01:36:02.532321 139827419338560 train.py:394] {'eval/walltime': 514.3564038276672, 'training/sps': 40893.783976108134, 'training/walltime': 1236.0900857448578, 'training/entropy_loss': Array(-0.02647468, dtype=float32), 'training/policy_loss': Array(0.00582363, dtype=float32), 'training/total_loss': Array(13881.742, dtype=float32), 'training/v_loss': Array(13881.762, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3243922 , 0.14515442], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.455944, 12.520386], dtype=float32), 'eval/episode_reward': Array([-25486.123,  10157.346], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31914812, 0.14873886], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.086126327514648, 'eval/sps': 31325.5121698244}
I0728 01:36:02.534595 139827419338560 train.py:379] starting iteration 122, 49971200 steps, 1760.2884631156921
I0728 01:36:16.592332 139827419338560 train.py:394] {'eval/walltime': 518.4327096939087, 'training/sps': 41060.06220814092, 'training/walltime': 1246.0657160282135, 'training/entropy_loss': Array(-0.02600372, dtype=float32), 'training/policy_loss': Array(0.00637727, dtype=float32), 'training/total_loss': Array(14348.821, dtype=float32), 'training/v_loss': Array(14348.84, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.34880745, 0.16087826], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([29.674475, 13.813365], dtype=float32), 'eval/episode_reward': Array([-27227.754,  10613.925], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.34404176, 0.1641256 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.076305866241455, 'eval/sps': 31400.98025029265}
I0728 01:36:16.594603 139827419338560 train.py:379] starting iteration 123, 50380800 steps, 1774.3484709262848
I0728 01:36:30.705968 139827419338560 train.py:394] {'eval/walltime': 522.5430603027344, 'training/sps': 40977.65897717506, 'training/walltime': 1256.0614066123962, 'training/entropy_loss': Array(-0.02551962, dtype=float32), 'training/policy_loss': Array(0.00717512, dtype=float32), 'training/total_loss': Array(14658.553, dtype=float32), 'training/v_loss': Array(14658.571, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32238263, 0.13581227], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.353136, 11.725718], dtype=float32), 'eval/episode_reward': Array([-25290.7  ,  10044.482], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3174138 , 0.13923877], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.110350608825684, 'eval/sps': 31140.895797346413}
I0728 01:36:30.708223 139827419338560 train.py:379] starting iteration 124, 50790400 steps, 1788.46209025383
I0728 01:36:44.838546 139827419338560 train.py:394] {'eval/walltime': 526.6419258117676, 'training/sps': 40853.10003673478, 'training/walltime': 1266.0875735282898, 'training/entropy_loss': Array(-0.02347995, dtype=float32), 'training/policy_loss': Array(0.00896548, dtype=float32), 'training/total_loss': Array(14756.732, dtype=float32), 'training/v_loss': Array(14756.748, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.35404205, 0.16007774], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([30.073935, 13.756008], dtype=float32), 'eval/episode_reward': Array([-26727.281,  10347.449], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.34935638, 0.16336432], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.098865509033203, 'eval/sps': 31228.153184804367}
I0728 01:36:44.840792 139827419338560 train.py:379] starting iteration 125, 51200000 steps, 1802.5946600437164
I0728 01:36:58.915272 139827419338560 train.py:394] {'eval/walltime': 530.7337439060211, 'training/sps': 41052.946766157336, 'training/walltime': 1276.0649328231812, 'training/entropy_loss': Array(-0.02197488, dtype=float32), 'training/policy_loss': Array(0.00964326, dtype=float32), 'training/total_loss': Array(13775.443, dtype=float32), 'training/v_loss': Array(13775.457, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3473849 , 0.15224947], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([29.495552, 13.103952], dtype=float32), 'eval/episode_reward': Array([-27339.836,  10432.507], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.34246063, 0.15553503], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.09181809425354, 'eval/sps': 31281.938016687596}
I0728 01:36:58.985265 139827419338560 train.py:379] starting iteration 126, 51609600 steps, 1816.7391242980957
I0728 01:37:13.099022 139827419338560 train.py:394] {'eval/walltime': 534.8069097995758, 'training/sps': 40816.09719303804, 'training/walltime': 1286.1001892089844, 'training/entropy_loss': Array(-0.01880033, dtype=float32), 'training/policy_loss': Array(0.01084015, dtype=float32), 'training/total_loss': Array(12808.953, dtype=float32), 'training/v_loss': Array(12808.962, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.34111935, 0.15488063], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([29.010092, 13.34793 ], dtype=float32), 'eval/episode_reward': Array([-25860.459,  11000.255], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3360494 , 0.15821633], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0731658935546875, 'eval/sps': 31425.187027747914}
I0728 01:37:13.101373 139827419338560 train.py:379] starting iteration 127, 52019200 steps, 1830.855241060257
I0728 01:37:27.220884 139827419338560 train.py:394] {'eval/walltime': 538.9145221710205, 'training/sps': 40931.335503631875, 'training/walltime': 1296.1071922779083, 'training/entropy_loss': Array(-0.01589295, dtype=float32), 'training/policy_loss': Array(0.011543, dtype=float32), 'training/total_loss': Array(13662.739, dtype=float32), 'training/v_loss': Array(13662.742, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3363341 , 0.14932454], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.585682, 12.858672], dtype=float32), 'eval/episode_reward': Array([-26190.309 ,  10566.4375], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.33122694, 0.15265098], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.107612371444702, 'eval/sps': 31161.65509915939}
I0728 01:37:27.222988 139827419338560 train.py:379] starting iteration 128, 52428800 steps, 1844.9768562316895
I0728 01:37:41.370489 139827419338560 train.py:394] {'eval/walltime': 543.0372552871704, 'training/sps': 40879.06356630952, 'training/walltime': 1306.1269912719727, 'training/entropy_loss': Array(-0.01320598, dtype=float32), 'training/policy_loss': Array(0.01187563, dtype=float32), 'training/total_loss': Array(14257.152, dtype=float32), 'training/v_loss': Array(14257.153, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32299474, 0.14935209], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.434406, 12.82374 ], dtype=float32), 'eval/episode_reward': Array([-25221.2  ,  10689.923], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31780002, 0.1526122 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.122733116149902, 'eval/sps': 31047.36503524521}
I0728 01:37:41.372593 139827419338560 train.py:379] starting iteration 129, 52838400 steps, 1859.12646150589
I0728 01:37:55.502084 139827419338560 train.py:394] {'eval/walltime': 547.1528344154358, 'training/sps': 40924.01796497077, 'training/walltime': 1316.1357836723328, 'training/entropy_loss': Array(-0.01113672, dtype=float32), 'training/policy_loss': Array(0.01215534, dtype=float32), 'training/total_loss': Array(14786.369, dtype=float32), 'training/v_loss': Array(14786.369, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32819492, 0.14994007], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.887934, 12.941434], dtype=float32), 'eval/episode_reward': Array([-25770.99 ,   9547.515], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32256332, 0.15412593], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.115579128265381, 'eval/sps': 31101.333739620983}
I0728 01:37:55.504273 139827419338560 train.py:379] starting iteration 130, 53248000 steps, 1873.258141040802
I0728 01:38:09.629731 139827419338560 train.py:394] {'eval/walltime': 551.2587413787842, 'training/sps': 40901.311749338856, 'training/walltime': 1326.1501324176788, 'training/entropy_loss': Array(-0.01147184, dtype=float32), 'training/policy_loss': Array(0.01260111, dtype=float32), 'training/total_loss': Array(13014.389, dtype=float32), 'training/v_loss': Array(13014.389, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31523722, 0.13254824], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.793583, 11.405629], dtype=float32), 'eval/episode_reward': Array([-24563.227,   8761.731], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30998075, 0.13560976], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.105906963348389, 'eval/sps': 31174.598241655072}
I0728 01:38:09.631902 139827419338560 train.py:379] starting iteration 131, 53657600 steps, 1887.3857688903809
I0728 01:38:23.778805 139827419338560 train.py:394] {'eval/walltime': 555.3805165290833, 'training/sps': 40877.397387195844, 'training/walltime': 1336.1703398227692, 'training/entropy_loss': Array(-0.00971961, dtype=float32), 'training/policy_loss': Array(0.01236899, dtype=float32), 'training/total_loss': Array(12196.045, dtype=float32), 'training/v_loss': Array(12196.043, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32563126, 0.15245216], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.618618, 13.141615], dtype=float32), 'eval/episode_reward': Array([-25067.93 ,   9509.411], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3201508 , 0.15624426], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.121775150299072, 'eval/sps': 31054.58093479758}
I0728 01:38:23.781013 139827419338560 train.py:379] starting iteration 132, 54067200 steps, 1901.5348806381226
I0728 01:38:37.891428 139827419338560 train.py:394] {'eval/walltime': 559.4556877613068, 'training/sps': 40837.335249929485, 'training/walltime': 1346.2003772258759, 'training/entropy_loss': Array(-0.00962315, dtype=float32), 'training/policy_loss': Array(0.01311949, dtype=float32), 'training/total_loss': Array(12825.932, dtype=float32), 'training/v_loss': Array(12825.928, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32884786, 0.14274703], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.937351, 12.286412], dtype=float32), 'eval/episode_reward': Array([-25637.805,  10620.191], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32383966, 0.14578651], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.075171232223511, 'eval/sps': 31409.723102643748}
I0728 01:38:37.893697 139827419338560 train.py:379] starting iteration 133, 54476800 steps, 1915.6475648880005
I0728 01:38:51.996771 139827419338560 train.py:394] {'eval/walltime': 563.566565990448, 'training/sps': 41013.83335513354, 'training/walltime': 1356.1872515678406, 'training/entropy_loss': Array(-0.00928573, dtype=float32), 'training/policy_loss': Array(0.01489917, dtype=float32), 'training/total_loss': Array(13351.865, dtype=float32), 'training/v_loss': Array(13351.859, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32630903, 0.14472014], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.703754, 12.474894], dtype=float32), 'eval/episode_reward': Array([-25811.814,  10641.011], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32092127, 0.14814542], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.110878229141235, 'eval/sps': 31136.898945980032}
I0728 01:38:51.998994 139827419338560 train.py:379] starting iteration 134, 54886400 steps, 1929.7528622150421
I0728 01:39:06.032994 139827419338560 train.py:394] {'eval/walltime': 567.6565544605255, 'training/sps': 41211.752741034215, 'training/walltime': 1366.1261639595032, 'training/entropy_loss': Array(-0.0085447, dtype=float32), 'training/policy_loss': Array(0.01462642, dtype=float32), 'training/total_loss': Array(13643.092, dtype=float32), 'training/v_loss': Array(13643.086, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29612386, 0.15016419], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.050175, 12.976113], dtype=float32), 'eval/episode_reward': Array([-23742.97 ,  10151.672], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28972274, 0.15421923], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.089988470077515, 'eval/sps': 31295.931745639886}
I0728 01:39:06.035207 139827419338560 train.py:379] starting iteration 135, 55296000 steps, 1943.789074897766
I0728 01:39:20.110509 139827419338560 train.py:394] {'eval/walltime': 571.7492713928223, 'training/sps': 41052.9516711558, 'training/walltime': 1376.1035220623016, 'training/entropy_loss': Array(-0.00572148, dtype=float32), 'training/policy_loss': Array(0.01727364, dtype=float32), 'training/total_loss': Array(12421.776, dtype=float32), 'training/v_loss': Array(12421.766, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31326675, 0.13940182], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.580307, 12.023323], dtype=float32), 'eval/episode_reward': Array([-24744.008,   9916.606], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3079493 , 0.14231114], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.092716932296753, 'eval/sps': 31275.067911468996}
I0728 01:39:20.112611 139827419338560 train.py:379] starting iteration 136, 55705600 steps, 1957.866478919983
I0728 01:39:34.211321 139827419338560 train.py:394] {'eval/walltime': 575.8324217796326, 'training/sps': 40916.75274175293, 'training/walltime': 1386.1140916347504, 'training/entropy_loss': Array(-0.00150115, dtype=float32), 'training/policy_loss': Array(0.01829446, dtype=float32), 'training/total_loss': Array(11688.979, dtype=float32), 'training/v_loss': Array(11688.962, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31245455, 0.13955171], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.468782, 12.031915], dtype=float32), 'eval/episode_reward': Array([-25367.96 ,   9717.875], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30677027, 0.14306688], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.083150386810303, 'eval/sps': 31348.343282548485}
I0728 01:39:34.213528 139827419338560 train.py:379] starting iteration 137, 56115200 steps, 1971.9673962593079
I0728 01:39:48.340362 139827419338560 train.py:394] {'eval/walltime': 579.9592697620392, 'training/sps': 40982.5338744214, 'training/walltime': 1396.1085932254791, 'training/entropy_loss': Array(0.00162831, dtype=float32), 'training/policy_loss': Array(0.01811787, dtype=float32), 'training/total_loss': Array(12453.576, dtype=float32), 'training/v_loss': Array(12453.556, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32573307, 0.14446168], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.65233 , 12.466226], dtype=float32), 'eval/episode_reward': Array([-25872.148,  10416.268], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3203186 , 0.14797102], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.126847982406616, 'eval/sps': 31016.40781189023}
I0728 01:39:48.342589 139827419338560 train.py:379] starting iteration 138, 56524800 steps, 1986.0964570045471
I0728 01:40:02.457448 139827419338560 train.py:394] {'eval/walltime': 584.0384685993195, 'training/sps': 40835.87630666589, 'training/walltime': 1406.1389889717102, 'training/entropy_loss': Array(0.00776978, dtype=float32), 'training/policy_loss': Array(0.01981349, dtype=float32), 'training/total_loss': Array(12699.951, dtype=float32), 'training/v_loss': Array(12699.925, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33518645, 0.1380902 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.434471, 11.939534], dtype=float32), 'eval/episode_reward': Array([-27017.4  ,   9965.856], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32966405, 0.14216329], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.079198837280273, 'eval/sps': 31378.710650285808}
I0728 01:40:02.459654 139827419338560 train.py:379] starting iteration 139, 56934400 steps, 2000.2135226726532
I0728 01:40:16.581283 139827419338560 train.py:394] {'eval/walltime': 588.1485276222229, 'training/sps': 40945.66907830073, 'training/walltime': 1416.1424889564514, 'training/entropy_loss': Array(0.00906262, dtype=float32), 'training/policy_loss': Array(0.02060687, dtype=float32), 'training/total_loss': Array(12798.324, dtype=float32), 'training/v_loss': Array(12798.294, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33525032, 0.14737126], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.437145, 12.681251], dtype=float32), 'eval/episode_reward': Array([-26741.504,  10033.31 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3299852 , 0.15061043], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.110059022903442, 'eval/sps': 31143.1050714152}
I0728 01:40:16.583387 139827419338560 train.py:379] starting iteration 140, 57344000 steps, 2014.3372550010681
I0728 01:40:30.736827 139827419338560 train.py:394] {'eval/walltime': 592.2589464187622, 'training/sps': 40807.10799676239, 'training/walltime': 1426.17995595932, 'training/entropy_loss': Array(0.01187617, dtype=float32), 'training/policy_loss': Array(0.02228602, dtype=float32), 'training/total_loss': Array(12207.702, dtype=float32), 'training/v_loss': Array(12207.668, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29044187, 0.131135  ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.553135, 11.330755], dtype=float32), 'eval/episode_reward': Array([-23263.088,   9907.273], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2839861, 0.1349251], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.110418796539307, 'eval/sps': 31140.379201206288}
I0728 01:40:30.740197 139827419338560 train.py:379] starting iteration 141, 57753600 steps, 2028.4940507411957
I0728 01:40:44.867587 139827419338560 train.py:394] {'eval/walltime': 596.3831899166107, 'training/sps': 40969.94186564227, 'training/walltime': 1436.177529335022, 'training/entropy_loss': Array(0.01253651, dtype=float32), 'training/policy_loss': Array(0.02135784, dtype=float32), 'training/total_loss': Array(11420.328, dtype=float32), 'training/v_loss': Array(11420.294, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3122749 , 0.13495848], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.384136, 11.653272], dtype=float32), 'eval/episode_reward': Array([-25163.086,   9930.584], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30608565, 0.13905795], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.124243497848511, 'eval/sps': 31035.99485985093}
I0728 01:40:44.869825 139827419338560 train.py:379] starting iteration 142, 58163200 steps, 2042.6236922740936
I0728 01:40:58.982829 139827419338560 train.py:394] {'eval/walltime': 600.4935989379883, 'training/sps': 40970.42550447963, 'training/walltime': 1446.1749846935272, 'training/entropy_loss': Array(0.01642697, dtype=float32), 'training/policy_loss': Array(0.02640379, dtype=float32), 'training/total_loss': Array(11580.554, dtype=float32), 'training/v_loss': Array(11580.512, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30562708, 0.13982753], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.851225, 12.0823  ], dtype=float32), 'eval/episode_reward': Array([-25081.22 ,   9730.413], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29933217, 0.14386497], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.1104090213775635, 'eval/sps': 31140.453257642483}
I0728 01:40:58.985092 139827419338560 train.py:379] starting iteration 143, 58572800 steps, 2056.7389602661133
I0728 01:41:13.072497 139827419338560 train.py:394] {'eval/walltime': 604.5918300151825, 'training/sps': 41025.12290675955, 'training/walltime': 1456.1591107845306, 'training/entropy_loss': Array(0.01838101, dtype=float32), 'training/policy_loss': Array(0.02259538, dtype=float32), 'training/total_loss': Array(11896.212, dtype=float32), 'training/v_loss': Array(11896.172, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31167066, 0.12633918], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.38425 , 10.908849], dtype=float32), 'eval/episode_reward': Array([-25146.572,   9385.353], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30613586, 0.12948906], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.098231077194214, 'eval/sps': 31232.98749850706}
I0728 01:41:13.074721 139827419338560 train.py:379] starting iteration 144, 58982400 steps, 2070.8285887241364
I0728 01:41:27.123173 139827419338560 train.py:394] {'eval/walltime': 608.6681299209595, 'training/sps': 41097.639678556596, 'training/walltime': 1466.1256198883057, 'training/entropy_loss': Array(0.01814749, dtype=float32), 'training/policy_loss': Array(0.02254577, dtype=float32), 'training/total_loss': Array(11784.971, dtype=float32), 'training/v_loss': Array(11784.93, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3150733 , 0.13699809], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.581772, 11.821026], dtype=float32), 'eval/episode_reward': Array([-25643.371,  10212.807], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30921924, 0.14078137], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0762999057769775, 'eval/sps': 31401.026165566713}
I0728 01:41:27.125364 139827419338560 train.py:379] starting iteration 145, 59392000 steps, 2084.8792321681976
I0728 01:41:41.201715 139827419338560 train.py:394] {'eval/walltime': 612.7448213100433, 'training/sps': 40981.38127246822, 'training/walltime': 1476.1204025745392, 'training/entropy_loss': Array(0.01955704, dtype=float32), 'training/policy_loss': Array(0.02812076, dtype=float32), 'training/total_loss': Array(12319.758, dtype=float32), 'training/v_loss': Array(12319.711, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32164663, 0.13168961], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.20092 , 11.375381], dtype=float32), 'eval/episode_reward': Array([-25589.637,  10239.256], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31599873, 0.13508928], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.076691389083862, 'eval/sps': 31398.010735555064}
I0728 01:41:41.203811 139827419338560 train.py:379] starting iteration 146, 59801600 steps, 2098.957679271698
I0728 01:41:55.317086 139827419338560 train.py:394] {'eval/walltime': 616.8421533107758, 'training/sps': 40923.60170932373, 'training/walltime': 1486.1292967796326, 'training/entropy_loss': Array(0.01788929, dtype=float32), 'training/policy_loss': Array(0.02860318, dtype=float32), 'training/total_loss': Array(11095.71, dtype=float32), 'training/v_loss': Array(11095.663, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32682493, 0.13322029], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.648127, 11.487129], dtype=float32), 'eval/episode_reward': Array([-26388.951,   9846.736], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3213646 , 0.13646829], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.097332000732422, 'eval/sps': 31239.84094457546}
I0728 01:41:55.319180 139827419338560 train.py:379] starting iteration 147, 60211200 steps, 2113.0730481147766
I0728 01:42:09.456348 139827419338560 train.py:394] {'eval/walltime': 620.9251821041107, 'training/sps': 40768.29210784401, 'training/walltime': 1496.176320552826, 'training/entropy_loss': Array(0.01612725, dtype=float32), 'training/policy_loss': Array(0.0295819, dtype=float32), 'training/total_loss': Array(11278.547, dtype=float32), 'training/v_loss': Array(11278.501, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33967346, 0.1500683 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.760517, 12.923234], dtype=float32), 'eval/episode_reward': Array([-26378.945,  10421.777], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.33425975, 0.15391193], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.083028793334961, 'eval/sps': 31349.276842951524}
I0728 01:42:09.458648 139827419338560 train.py:379] starting iteration 148, 60620800 steps, 2127.2125165462494
I0728 01:42:23.580154 139827419338560 train.py:394] {'eval/walltime': 625.0135385990143, 'training/sps': 40847.68771319778, 'training/walltime': 1506.2038159370422, 'training/entropy_loss': Array(0.01422508, dtype=float32), 'training/policy_loss': Array(0.03123927, dtype=float32), 'training/total_loss': Array(11175.442, dtype=float32), 'training/v_loss': Array(11175.396, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3340617 , 0.12704463], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.270908, 10.919933], dtype=float32), 'eval/episode_reward': Array([-26381.67 ,   8833.672], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32906324, 0.12974364], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0883564949035645, 'eval/sps': 31308.42434106746}
I0728 01:42:23.582522 139827419338560 train.py:379] starting iteration 149, 61030400 steps, 2141.3363897800446
I0728 01:42:37.712023 139827419338560 train.py:394] {'eval/walltime': 629.1314187049866, 'training/sps': 40936.50567139335, 'training/walltime': 1516.2095551490784, 'training/entropy_loss': Array(0.01289459, dtype=float32), 'training/policy_loss': Array(0.03134914, dtype=float32), 'training/total_loss': Array(11325.036, dtype=float32), 'training/v_loss': Array(11324.992, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32614446, 0.16796117], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.517986, 14.475029], dtype=float32), 'eval/episode_reward': Array([-26665.422,  11371.187], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3198665 , 0.17226432], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.11788010597229, 'eval/sps': 31083.95502199241}
I0728 01:42:37.714277 139827419338560 train.py:379] starting iteration 150, 61440000 steps, 2155.468145608902
I0728 01:42:51.841280 139827419338560 train.py:394] {'eval/walltime': 633.2044637203217, 'training/sps': 40762.24745572713, 'training/walltime': 1526.2580687999725, 'training/entropy_loss': Array(0.01366089, dtype=float32), 'training/policy_loss': Array(0.03558452, dtype=float32), 'training/total_loss': Array(12374.01, dtype=float32), 'training/v_loss': Array(12373.961, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33008996, 0.15483595], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.842373, 13.405545], dtype=float32), 'eval/episode_reward': Array([-25999.004,  11093.48 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.324211  , 0.15899459], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.073045015335083, 'eval/sps': 31426.11965202394}
I0728 01:42:51.924263 139827419338560 train.py:379] starting iteration 151, 61849600 steps, 2169.6781134605408
I0728 01:43:06.042209 139827419338560 train.py:394] {'eval/walltime': 637.2999613285065, 'training/sps': 40890.66736243214, 'training/walltime': 1536.2750244140625, 'training/entropy_loss': Array(0.00822254, dtype=float32), 'training/policy_loss': Array(0.02990409, dtype=float32), 'training/total_loss': Array(10900.826, dtype=float32), 'training/v_loss': Array(10900.787, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32485944, 0.14100693], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.3674  , 12.140971], dtype=float32), 'eval/episode_reward': Array([-25387.725,   9433.329], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31942782, 0.14418648], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0954976081848145, 'eval/sps': 31253.83341555204}
I0728 01:43:06.044688 139827419338560 train.py:379] starting iteration 152, 62259200 steps, 2183.79855632782
I0728 01:43:20.161728 139827419338560 train.py:394] {'eval/walltime': 641.3838205337524, 'training/sps': 40852.99220365357, 'training/walltime': 1546.3012177944183, 'training/entropy_loss': Array(-0.01265573, dtype=float32), 'training/policy_loss': Array(0.0248209, dtype=float32), 'training/total_loss': Array(10914.486, dtype=float32), 'training/v_loss': Array(10914.474, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31390077, 0.13035586], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.471405, 11.269137], dtype=float32), 'eval/episode_reward': Array([-25137.207,   9073.088], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3083206 , 0.13365865], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.083859205245972, 'eval/sps': 31342.90228115015}
I0728 01:43:20.164032 139827419338560 train.py:379] starting iteration 153, 62668800 steps, 2197.917899131775
I0728 01:43:34.281550 139827419338560 train.py:394] {'eval/walltime': 645.4661347866058, 'training/sps': 40837.72451322096, 'training/walltime': 1556.3311595916748, 'training/entropy_loss': Array(-0.01925643, dtype=float32), 'training/policy_loss': Array(0.00452137, dtype=float32), 'training/total_loss': Array(11095.59, dtype=float32), 'training/v_loss': Array(11095.6045, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.34729356, 0.15386927], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([29.345436, 13.215708], dtype=float32), 'eval/episode_reward': Array([-27559.309,  10428.764], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.34254166, 0.15660793], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0823142528533936, 'eval/sps': 31354.764006845508}
I0728 01:43:34.283812 139827419338560 train.py:379] starting iteration 154, 63078400 steps, 2212.03767991066
I0728 01:43:48.447149 139827419338560 train.py:394] {'eval/walltime': 649.54856300354, 'training/sps': 40658.55908945339, 'training/walltime': 1566.4052991867065, 'training/entropy_loss': Array(-0.02083494, dtype=float32), 'training/policy_loss': Array(0.0044279, dtype=float32), 'training/total_loss': Array(10952.845, dtype=float32), 'training/v_loss': Array(10952.861, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3072775 , 0.14033668], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.875761, 12.086386], dtype=float32), 'eval/episode_reward': Array([-24978.766,   9920.659], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30129415, 0.14377591], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.082428216934204, 'eval/sps': 31353.88871482096}
I0728 01:43:48.449388 139827419338560 train.py:379] starting iteration 155, 63488000 steps, 2226.2032566070557
I0728 01:44:02.571148 139827419338560 train.py:394] {'eval/walltime': 653.6352562904358, 'training/sps': 40838.640912961186, 'training/walltime': 1576.4350159168243, 'training/entropy_loss': Array(-0.02098656, dtype=float32), 'training/policy_loss': Array(0.00470616, dtype=float32), 'training/total_loss': Array(11931.764, dtype=float32), 'training/v_loss': Array(11931.779, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.34439978, 0.14244708], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([29.084534, 12.261309], dtype=float32), 'eval/episode_reward': Array([-27092.074,  10173.18 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.33935234, 0.14597905], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.086693286895752, 'eval/sps': 31321.16628630789}
I0728 01:44:02.573356 139827419338560 train.py:379] starting iteration 156, 63897600 steps, 2240.327224254608
I0728 01:44:16.732858 139827419338560 train.py:394] {'eval/walltime': 657.7225861549377, 'training/sps': 40687.070078801604, 'training/walltime': 1586.5020961761475, 'training/entropy_loss': Array(-0.02078357, dtype=float32), 'training/policy_loss': Array(0.00449324, dtype=float32), 'training/total_loss': Array(10630.248, dtype=float32), 'training/v_loss': Array(10630.265, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31859362, 0.1403981 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.881641, 12.106146], dtype=float32), 'eval/episode_reward': Array([-24681.   ,   9761.014], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31330246, 0.14381997], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.087329864501953, 'eval/sps': 31316.288198725302}
I0728 01:44:16.735096 139827419338560 train.py:379] starting iteration 157, 64307200 steps, 2254.488963842392
I0728 01:44:30.839244 139827419338560 train.py:394] {'eval/walltime': 661.8144752979279, 'training/sps': 40931.85333933799, 'training/walltime': 1596.508972644806, 'training/entropy_loss': Array(-0.02084523, dtype=float32), 'training/policy_loss': Array(0.00507036, dtype=float32), 'training/total_loss': Array(10918.333, dtype=float32), 'training/v_loss': Array(10918.349, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30831096, 0.13286847], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.023655, 11.447784], dtype=float32), 'eval/episode_reward': Array([-24561.508,   9485.148], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3027588 , 0.13612095], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.091889142990112, 'eval/sps': 31281.394858724132}
I0728 01:44:30.841440 139827419338560 train.py:379] starting iteration 158, 64716800 steps, 2268.595308303833
I0728 01:44:45.001991 139827419338560 train.py:394] {'eval/walltime': 665.9019505977631, 'training/sps': 40684.484926946636, 'training/walltime': 1606.5766925811768, 'training/entropy_loss': Array(-0.02153774, dtype=float32), 'training/policy_loss': Array(0.00531257, dtype=float32), 'training/total_loss': Array(11235.959, dtype=float32), 'training/v_loss': Array(11235.975, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3275539 , 0.14210387], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.582943, 12.234154], dtype=float32), 'eval/episode_reward': Array([-26059.072,  10468.507], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32186115, 0.14583887], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.087475299835205, 'eval/sps': 31315.173942497117}
I0728 01:44:45.004199 139827419338560 train.py:379] starting iteration 159, 65126400 steps, 2282.7580671310425
I0728 01:44:59.154742 139827419338560 train.py:394] {'eval/walltime': 669.9935669898987, 'training/sps': 40741.55792919756, 'training/walltime': 1616.6303091049194, 'training/entropy_loss': Array(-0.02193772, dtype=float32), 'training/policy_loss': Array(0.00541674, dtype=float32), 'training/total_loss': Array(11068.559, dtype=float32), 'training/v_loss': Array(11068.574, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30956703, 0.14542198], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.14354 , 12.532672], dtype=float32), 'eval/episode_reward': Array([-24455.082,  10645.676], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3034997 , 0.14937231], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.09161639213562, 'eval/sps': 31283.480104837094}
I0728 01:44:59.157011 139827419338560 train.py:379] starting iteration 160, 65536000 steps, 2296.9108788967133
I0728 01:45:13.328358 139827419338560 train.py:394] {'eval/walltime': 674.1030242443085, 'training/sps': 40730.91250622793, 'training/walltime': 1626.6865532398224, 'training/entropy_loss': Array(-0.02000998, dtype=float32), 'training/policy_loss': Array(0.00586295, dtype=float32), 'training/total_loss': Array(11294.209, dtype=float32), 'training/v_loss': Array(11294.223, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31589794, 0.14852893], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.672268, 12.808935], dtype=float32), 'eval/episode_reward': Array([-25438.455,  10159.584], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31030846, 0.15198372], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.10945725440979, 'eval/sps': 31147.66551292031}
I0728 01:45:13.330631 139827419338560 train.py:379] starting iteration 161, 65945600 steps, 2311.0844991207123
I0728 01:45:27.506413 139827419338560 train.py:394] {'eval/walltime': 678.2268657684326, 'training/sps': 40770.81245148195, 'training/walltime': 1636.7329559326172, 'training/entropy_loss': Array(-0.02039961, dtype=float32), 'training/policy_loss': Array(0.00699957, dtype=float32), 'training/total_loss': Array(10101.309, dtype=float32), 'training/v_loss': Array(10101.322, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3429469 , 0.14720187], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([29.040518, 12.689043], dtype=float32), 'eval/episode_reward': Array([-26688.54 ,   9461.159], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.33787274, 0.150784  ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.1238415241241455, 'eval/sps': 31039.02011054745}
I0728 01:45:27.508703 139827419338560 train.py:379] starting iteration 162, 66355200 steps, 2325.262570619583
I0728 01:45:41.665761 139827419338560 train.py:394] {'eval/walltime': 682.3074851036072, 'training/sps': 40671.918428271696, 'training/walltime': 1646.8037865161896, 'training/entropy_loss': Array(-0.01934309, dtype=float32), 'training/policy_loss': Array(0.00747793, dtype=float32), 'training/total_loss': Array(10260.025, dtype=float32), 'training/v_loss': Array(10260.037, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33414382, 0.1257058 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.331356, 10.806377], dtype=float32), 'eval/episode_reward': Array([-26108.621,   8572.158], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32970226, 0.12812172], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0806193351745605, 'eval/sps': 31367.78745732342}
I0728 01:45:41.668074 139827419338560 train.py:379] starting iteration 163, 66764800 steps, 2339.421940803528
I0728 01:45:55.809299 139827419338560 train.py:394] {'eval/walltime': 686.3982329368591, 'training/sps': 40777.2322696194, 'training/walltime': 1656.8486075401306, 'training/entropy_loss': Array(-0.01906696, dtype=float32), 'training/policy_loss': Array(0.00816763, dtype=float32), 'training/total_loss': Array(10404.528, dtype=float32), 'training/v_loss': Array(10404.539, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32350123, 0.1485375 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.314072, 12.852961], dtype=float32), 'eval/episode_reward': Array([-25535.895,  10577.223], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31755134, 0.1529666 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.090747833251953, 'eval/sps': 31290.122299776664}
I0728 01:45:55.811534 139827419338560 train.py:379] starting iteration 164, 67174400 steps, 2353.565402030945
I0728 01:46:09.907495 139827419338560 train.py:394] {'eval/walltime': 690.4741358757019, 'training/sps': 40898.39065987913, 'training/walltime': 1666.863671541214, 'training/entropy_loss': Array(-0.01840915, dtype=float32), 'training/policy_loss': Array(0.0088199, dtype=float32), 'training/total_loss': Array(10618.713, dtype=float32), 'training/v_loss': Array(10618.723, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32155192, 0.13686143], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.193998, 11.762487], dtype=float32), 'eval/episode_reward': Array([-25772.926,  10051.474], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31633314, 0.13995618], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.075902938842773, 'eval/sps': 31404.084425116765}
I0728 01:46:09.909630 139827419338560 train.py:379] starting iteration 165, 67584000 steps, 2367.6634969711304
I0728 01:46:24.059994 139827419338560 train.py:394] {'eval/walltime': 694.5635995864868, 'training/sps': 40734.80355953762, 'training/walltime': 1676.9189550876617, 'training/entropy_loss': Array(-0.01556904, dtype=float32), 'training/policy_loss': Array(0.01071786, dtype=float32), 'training/total_loss': Array(10901.143, dtype=float32), 'training/v_loss': Array(10901.148, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.34179652, 0.13819247], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.950754, 11.877756], dtype=float32), 'eval/episode_reward': Array([-26914.104,   9690.444], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.33696634, 0.14125492], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.089463710784912, 'eval/sps': 31299.947634315184}
I0728 01:46:24.062197 139827419338560 train.py:379] starting iteration 166, 67993600 steps, 2381.816065788269
I0728 01:46:38.169984 139827419338560 train.py:394] {'eval/walltime': 698.6393823623657, 'training/sps': 40850.6870434045, 'training/walltime': 1686.9457142353058, 'training/entropy_loss': Array(-0.01257421, dtype=float32), 'training/policy_loss': Array(0.01208836, dtype=float32), 'training/total_loss': Array(9582.016, dtype=float32), 'training/v_loss': Array(9582.018, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33474207, 0.15203078], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.352749, 13.119947], dtype=float32), 'eval/episode_reward': Array([-26263.846,  10136.02 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32945913, 0.15589096], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.075782775878906, 'eval/sps': 31405.01028600523}
I0728 01:46:38.172266 139827419338560 train.py:379] starting iteration 167, 68403200 steps, 2395.926134109497
I0728 01:46:52.322798 139827419338560 train.py:394] {'eval/walltime': 702.7197062969208, 'training/sps': 40695.80112781763, 'training/walltime': 1697.0106346607208, 'training/entropy_loss': Array(-0.01144149, dtype=float32), 'training/policy_loss': Array(0.01375229, dtype=float32), 'training/total_loss': Array(10035.103, dtype=float32), 'training/v_loss': Array(10035.1, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3291499 , 0.15041602], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.87526 , 12.940081], dtype=float32), 'eval/episode_reward': Array([-26573.307,  10147.255], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32380378, 0.1536409 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.080323934555054, 'eval/sps': 31370.05837110283}
I0728 01:46:52.325047 139827419338560 train.py:379] starting iteration 168, 68812800 steps, 2410.0789148807526
I0728 01:47:06.449531 139827419338560 train.py:394] {'eval/walltime': 706.8065860271454, 'training/sps': 40827.60219781552, 'training/walltime': 1707.0430631637573, 'training/entropy_loss': Array(-0.01066097, dtype=float32), 'training/policy_loss': Array(0.01427323, dtype=float32), 'training/total_loss': Array(10442.102, dtype=float32), 'training/v_loss': Array(10442.098, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32449043, 0.13688695], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.473019, 11.862353], dtype=float32), 'eval/episode_reward': Array([-26182.812,   9786.506], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31904948, 0.14023457], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.086879730224609, 'eval/sps': 31319.73741565556}
I0728 01:47:06.451829 139827419338560 train.py:379] starting iteration 169, 69222400 steps, 2424.2056968212128
