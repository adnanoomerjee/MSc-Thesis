I0728 01:47:29.680667 139742084949824 low_level_env.py:188] Initialising environment...
I0728 01:48:09.039068 139742084949824 low_level_env.py:293] Environment initialised.
I0728 01:48:09.044154 139742084949824 train.py:118] JAX is running on GPU.
I0728 01:48:09.044197 139742084949824 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 01:48:17.711224 139742084949824 train.py:367] Running initial eval
I0728 01:48:34.734927 139742084949824 train.py:373] {'eval/walltime': 16.88216757774353, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3121909, 0.1452056], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.323   , 12.496906], dtype=float32), 'eval/episode_reward': Array([-24817.852,  10288.948], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30690867, 0.14883886], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 16.88216757774353, 'eval/sps': 7581.964780917574}
I0728 01:48:34.736150 139742084949824 train.py:379] starting iteration 0, 0 steps, 25.692008018493652
I0728 01:49:45.759361 139742084949824 train.py:394] {'eval/walltime': 21.080465078353882, 'training/sps': 6130.39411312112, 'training/walltime': 66.8146276473999, 'training/entropy_loss': Array(-0.00448015, dtype=float32), 'training/policy_loss': Array(0.02096428, dtype=float32), 'training/total_loss': Array(136636.23, dtype=float32), 'training/v_loss': Array(136636.22, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31424528, 0.13176358], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.549814, 11.318439], dtype=float32), 'eval/episode_reward': Array([-24743.77 ,   8891.335], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30970615, 0.1345637 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.198297500610352, 'eval/sps': 30488.549222962705}
I0728 01:49:45.799330 139742084949824 train.py:379] starting iteration 1, 409600 steps, 96.7551703453064
I0728 01:50:12.401049 139742084949824 train.py:394] {'eval/walltime': 25.49152946472168, 'training/sps': 18465.218254522722, 'training/walltime': 88.9968729019165, 'training/entropy_loss': Array(-0.00422569, dtype=float32), 'training/policy_loss': Array(0.00698467, dtype=float32), 'training/total_loss': Array(143305.67, dtype=float32), 'training/v_loss': Array(143305.67, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3066803 , 0.13042054], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.932426, 11.159367], dtype=float32), 'eval/episode_reward': Array([-24721.47 ,   9350.952], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30225393, 0.1331647 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.411064386367798, 'eval/sps': 29017.93961466045}
I0728 01:50:12.404601 139742084949824 train.py:379] starting iteration 2, 819200 steps, 123.36045908927917
I0728 01:50:39.198363 139742084949824 train.py:394] {'eval/walltime': 29.91474223136902, 'training/sps': 18316.34333916518, 'training/walltime': 111.35941505432129, 'training/entropy_loss': Array(-0.00381026, dtype=float32), 'training/policy_loss': Array(0.00928032, dtype=float32), 'training/total_loss': Array(148439.98, dtype=float32), 'training/v_loss': Array(148439.97, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2865526 , 0.12042887], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.185144, 10.375383], dtype=float32), 'eval/episode_reward': Array([-23572.158,   8948.172], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28152657, 0.12379608], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.423212766647339, 'eval/sps': 28938.241670210253}
I0728 01:50:39.201587 139742084949824 train.py:379] starting iteration 3, 1228800 steps, 150.1574456691742
I0728 01:51:06.763905 139742084949824 train.py:394] {'eval/walltime': 34.360679149627686, 'training/sps': 17724.843423597114, 'training/walltime': 134.4682228565216, 'training/entropy_loss': Array(-0.00305486, dtype=float32), 'training/policy_loss': Array(0.01558822, dtype=float32), 'training/total_loss': Array(153828.4, dtype=float32), 'training/v_loss': Array(153828.39, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29281723, 0.11083627], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.686646,  9.575858], dtype=float32), 'eval/episode_reward': Array([-24474.262,   8858.951], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2879437 , 0.11351299], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.445936918258667, 'eval/sps': 28790.332016256667}
I0728 01:51:06.767148 139742084949824 train.py:379] starting iteration 4, 1638400 steps, 177.72300696372986
I0728 01:51:34.768933 139742084949824 train.py:394] {'eval/walltime': 38.782859325408936, 'training/sps': 17376.810833494383, 'training/walltime': 158.0398669242859, 'training/entropy_loss': Array(-0.00183583, dtype=float32), 'training/policy_loss': Array(0.02339261, dtype=float32), 'training/total_loss': Array(156318.98, dtype=float32), 'training/v_loss': Array(156318.97, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27628076, 0.10777692], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.27959 ,  9.242176], dtype=float32), 'eval/episode_reward': Array([-23102.797,   8988.555], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2716993 , 0.10999535], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.42218017578125, 'eval/sps': 28944.998826825667}
I0728 01:51:34.772190 139742084949824 train.py:379] starting iteration 5, 2048000 steps, 205.72804832458496
I0728 01:52:03.130401 139742084949824 train.py:394] {'eval/walltime': 43.19245481491089, 'training/sps': 17108.917476689123, 'training/walltime': 181.98059844970703, 'training/entropy_loss': Array(-0.00044993, dtype=float32), 'training/policy_loss': Array(0.02092555, dtype=float32), 'training/total_loss': Array(122540.78, dtype=float32), 'training/v_loss': Array(122540.75, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27636975, 0.10385103], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.332876,  8.934369], dtype=float32), 'eval/episode_reward': Array([-23241.617,   8674.69 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2713439 , 0.10628751], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.409595489501953, 'eval/sps': 29027.605889187154}
I0728 01:52:03.133658 139742084949824 train.py:379] starting iteration 6, 2457600 steps, 234.0895164012909
I0728 01:52:31.707849 139742084949824 train.py:394] {'eval/walltime': 47.6183705329895, 'training/sps': 16967.461508087166, 'training/walltime': 206.1209213733673, 'training/entropy_loss': Array(0.00026068, dtype=float32), 'training/policy_loss': Array(0.01672947, dtype=float32), 'training/total_loss': Array(119983.164, dtype=float32), 'training/v_loss': Array(119983.15, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29724884, 0.09677225], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.052101,  8.308778], dtype=float32), 'eval/episode_reward': Array([-25014.984,   8206.993], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29290146, 0.0991608 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.425915718078613, 'eval/sps': 28920.56879374278}
I0728 01:52:31.711184 139742084949824 train.py:379] starting iteration 7, 2867200 steps, 262.66704201698303
I0728 01:53:00.369572 139742084949824 train.py:394] {'eval/walltime': 52.0469753742218, 'training/sps': 16910.20135744279, 'training/walltime': 230.34298658370972, 'training/entropy_loss': Array(0.00165444, dtype=float32), 'training/policy_loss': Array(0.00322116, dtype=float32), 'training/total_loss': Array(117356.13, dtype=float32), 'training/v_loss': Array(117356.125, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27308434, 0.09799469], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.984917,  8.469196], dtype=float32), 'eval/episode_reward': Array([-22981.863 ,   8413.3545], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26774278, 0.10127117], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4286048412323, 'eval/sps': 28903.00773920096}
I0728 01:53:00.372857 139742084949824 train.py:379] starting iteration 8, 3276800 steps, 291.3287162780762
I0728 01:53:29.096822 139742084949824 train.py:394] {'eval/walltime': 56.480490922927856, 'training/sps': 16868.3220710734, 'training/walltime': 254.6251883506775, 'training/entropy_loss': Array(0.00352989, dtype=float32), 'training/policy_loss': Array(0.00162371, dtype=float32), 'training/total_loss': Array(114364.266, dtype=float32), 'training/v_loss': Array(114364.26, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28770518, 0.1075401 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.261433,  9.280363], dtype=float32), 'eval/episode_reward': Array([-24260.848,   9260.003], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2830004 , 0.11041088], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.433515548706055, 'eval/sps': 28870.993818293362}
I0728 01:53:29.100051 139742084949824 train.py:379] starting iteration 9, 3686400 steps, 320.05590891838074
I0728 01:53:57.811240 139742084949824 train.py:394] {'eval/walltime': 60.89400792121887, 'training/sps': 16863.13465916273, 'training/walltime': 278.9148597717285, 'training/entropy_loss': Array(0.00541728, dtype=float32), 'training/policy_loss': Array(0.00372768, dtype=float32), 'training/total_loss': Array(110106.734, dtype=float32), 'training/v_loss': Array(110106.72, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26496527, 0.09596154], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.313534,  8.293897], dtype=float32), 'eval/episode_reward': Array([-22343.215 ,   8265.9375], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2598487 , 0.09875093], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.413516998291016, 'eval/sps': 29001.814210654145}
I0728 01:53:57.814496 139742084949824 train.py:379] starting iteration 10, 4096000 steps, 348.7703547477722
I0728 01:54:26.544996 139742084949824 train.py:394] {'eval/walltime': 65.30510950088501, 'training/sps': 16848.063422948453, 'training/walltime': 303.2262592315674, 'training/entropy_loss': Array(0.00741919, dtype=float32), 'training/policy_loss': Array(0.00425655, dtype=float32), 'training/total_loss': Array(107502.44, dtype=float32), 'training/v_loss': Array(107502.42, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2586535, 0.1031843], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.750374,  8.955506], dtype=float32), 'eval/episode_reward': Array([-21763.96 ,   8958.355], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2530011 , 0.10740945], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.411101579666138, 'eval/sps': 29017.694942696813}
I0728 01:54:26.548472 139742084949824 train.py:379] starting iteration 11, 4505600 steps, 377.50433015823364
I0728 01:54:55.196676 139742084949824 train.py:394] {'eval/walltime': 69.7190933227539, 'training/sps': 16907.355248588385, 'training/walltime': 327.4524018764496, 'training/entropy_loss': Array(0.01055768, dtype=float32), 'training/policy_loss': Array(0.00225893, dtype=float32), 'training/total_loss': Array(103640.58, dtype=float32), 'training/v_loss': Array(103640.56, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27913892, 0.10247021], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.482742,  8.840955], dtype=float32), 'eval/episode_reward': Array([-23494.479,   8804.452], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27398026, 0.10559375], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4139838218688965, 'eval/sps': 28998.7469745198}
I0728 01:54:55.200042 139742084949824 train.py:379] starting iteration 12, 4915200 steps, 406.1559011936188
I0728 01:55:23.990889 139742084949824 train.py:394] {'eval/walltime': 74.15582704544067, 'training/sps': 16824.08932582237, 'training/walltime': 351.7984447479248, 'training/entropy_loss': Array(0.01338012, dtype=float32), 'training/policy_loss': Array(0.00238099, dtype=float32), 'training/total_loss': Array(100102.46, dtype=float32), 'training/v_loss': Array(100102.445, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29316607, 0.10019782], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.67937 ,  8.680881], dtype=float32), 'eval/episode_reward': Array([-24629.547,   8461.926], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28821734, 0.10340186], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.436733722686768, 'eval/sps': 28850.052313368633}
I0728 01:55:23.994271 139742084949824 train.py:379] starting iteration 13, 5324800 steps, 434.9501302242279
I0728 01:55:52.739561 139742084949824 train.py:394] {'eval/walltime': 78.57312297821045, 'training/sps': 16842.27537422071, 'training/walltime': 376.1181991100311, 'training/entropy_loss': Array(0.01608461, dtype=float32), 'training/policy_loss': Array(0.00283026, dtype=float32), 'training/total_loss': Array(96260.84, dtype=float32), 'training/v_loss': Array(96260.81, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28621733, 0.09780502], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.102972,  8.457716], dtype=float32), 'eval/episode_reward': Array([-24108.465,   8403.39 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28149134, 0.10021947], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.417295932769775, 'eval/sps': 28977.003566917512}
I0728 01:55:52.742981 139742084949824 train.py:379] starting iteration 14, 5734400 steps, 463.6988379955292
I0728 01:56:21.541926 139742084949824 train.py:394] {'eval/walltime': 83.0117928981781, 'training/sps': 16821.31593804112, 'training/walltime': 400.4682559967041, 'training/entropy_loss': Array(0.01888742, dtype=float32), 'training/policy_loss': Array(0.00343201, dtype=float32), 'training/total_loss': Array(91360.66, dtype=float32), 'training/v_loss': Array(91360.625, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28175092, 0.10086749], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.721262,  8.714918], dtype=float32), 'eval/episode_reward': Array([-23722.016,   8686.31 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2765992 , 0.10414996], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.438669919967651, 'eval/sps': 28837.46759906239}
I0728 01:56:21.545307 139742084949824 train.py:379] starting iteration 15, 6144000 steps, 492.5011646747589
I0728 01:56:50.390618 139742084949824 train.py:394] {'eval/walltime': 87.44743800163269, 'training/sps': 16785.594864076927, 'training/walltime': 424.8701317310333, 'training/entropy_loss': Array(0.02055443, dtype=float32), 'training/policy_loss': Array(0.00450015, dtype=float32), 'training/total_loss': Array(88687.1, dtype=float32), 'training/v_loss': Array(88687.08, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28495014, 0.0971432 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.017073,  8.351109], dtype=float32), 'eval/episode_reward': Array([-24009.004,   8303.438], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28043652, 0.09928466], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.43564510345459, 'eval/sps': 28857.13284417422}
I0728 01:56:50.394027 139742084949824 train.py:379] starting iteration 16, 6553600 steps, 521.3498857021332
I0728 01:57:19.147738 139742084949824 train.py:394] {'eval/walltime': 91.85336375236511, 'training/sps': 16828.73707996443, 'training/walltime': 449.2094507217407, 'training/entropy_loss': Array(0.02397809, dtype=float32), 'training/policy_loss': Array(0.00460148, dtype=float32), 'training/total_loss': Array(84416.45, dtype=float32), 'training/v_loss': Array(84416.42, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2800771, 0.0970724], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.600292,  8.342692], dtype=float32), 'eval/episode_reward': Array([-23576.852,   8320.519], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27510458, 0.09989706], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.405925750732422, 'eval/sps': 29051.78326682465}
I0728 01:57:19.151208 139742084949824 train.py:379] starting iteration 17, 6963200 steps, 550.1070671081543
I0728 01:57:47.891557 139742084949824 train.py:394] {'eval/walltime': 96.27870583534241, 'training/sps': 16851.012904639254, 'training/walltime': 473.5165948867798, 'training/entropy_loss': Array(0.02665676, dtype=float32), 'training/policy_loss': Array(0.00540189, dtype=float32), 'training/total_loss': Array(80136.1, dtype=float32), 'training/v_loss': Array(80136.07, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27083933, 0.09346221], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.792068,  8.100753], dtype=float32), 'eval/episode_reward': Array([-22798.764,   8044.596], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26596308, 0.09584232], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.425342082977295, 'eval/sps': 28924.31762334716}
I0728 01:57:47.894917 139742084949824 train.py:379] starting iteration 18, 7372800 steps, 578.850775718689
I0728 01:58:16.685653 139742084949824 train.py:394] {'eval/walltime': 100.6823582649231, 'training/sps': 16801.412495137407, 'training/walltime': 497.8954975605011, 'training/entropy_loss': Array(0.02898351, dtype=float32), 'training/policy_loss': Array(0.00732075, dtype=float32), 'training/total_loss': Array(75824.06, dtype=float32), 'training/v_loss': Array(75824.03, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28224534, 0.10556357], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.75859 ,  9.141362], dtype=float32), 'eval/episode_reward': Array([-23742.615,   9094.844], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27716625, 0.10828048], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4036524295806885, 'eval/sps': 29066.78082497715}
I0728 01:58:16.689052 139742084949824 train.py:379] starting iteration 19, 7782400 steps, 607.6449105739594
I0728 01:58:45.496594 139742084949824 train.py:394] {'eval/walltime': 105.1203887462616, 'training/sps': 16813.620937435404, 'training/walltime': 522.2566986083984, 'training/entropy_loss': Array(0.03116776, dtype=float32), 'training/policy_loss': Array(0.00701143, dtype=float32), 'training/total_loss': Array(70755.68, dtype=float32), 'training/v_loss': Array(70755.64, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27237654, 0.10334196], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.908604,  8.959783], dtype=float32), 'eval/episode_reward': Array([-22894.977,   8916.723], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2666108 , 0.10768934], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.438030481338501, 'eval/sps': 28841.622548161376}
I0728 01:58:45.500072 139742084949824 train.py:379] starting iteration 20, 8192000 steps, 636.4559307098389
I0728 01:59:14.339729 139742084949824 train.py:394] {'eval/walltime': 109.55663585662842, 'training/sps': 16790.349529459087, 'training/walltime': 546.6516642570496, 'training/entropy_loss': Array(0.03164981, dtype=float32), 'training/policy_loss': Array(0.00819908, dtype=float32), 'training/total_loss': Array(68003.55, dtype=float32), 'training/v_loss': Array(68003.5, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27636606, 0.09909075], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.305614 ,  8.5123825], dtype=float32), 'eval/episode_reward': Array([-23300.852,   8445.351], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2717136 , 0.10134217], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.436247110366821, 'eval/sps': 28853.21687804177}
I0728 01:59:14.343195 139742084949824 train.py:379] starting iteration 21, 8601600 steps, 665.2990539073944
I0728 01:59:43.135266 139742084949824 train.py:394] {'eval/walltime': 113.964688539505, 'training/sps': 16803.287350529805, 'training/walltime': 571.0278468132019, 'training/entropy_loss': Array(0.03481126, dtype=float32), 'training/policy_loss': Array(0.00814892, dtype=float32), 'training/total_loss': Array(63857.008, dtype=float32), 'training/v_loss': Array(63856.97, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28137356, 0.1005711 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.727451,  8.701286], dtype=float32), 'eval/episode_reward': Array([-23709.072,   8673.853], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2761836 , 0.10410511], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.408052682876587, 'eval/sps': 29037.765473453994}
I0728 01:59:43.138717 139742084949824 train.py:379] starting iteration 22, 9011200 steps, 694.0945756435394
I0728 02:00:11.946653 139742084949824 train.py:394] {'eval/walltime': 118.37452340126038, 'training/sps': 16793.760313932708, 'training/walltime': 595.4178578853607, 'training/entropy_loss': Array(0.03667995, dtype=float32), 'training/policy_loss': Array(0.00980549, dtype=float32), 'training/total_loss': Array(59614.9, dtype=float32), 'training/v_loss': Array(59614.848, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25637788, 0.09516209], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.52404,  8.24932], dtype=float32), 'eval/episode_reward': Array([-21533.816,   8228.972], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25066164, 0.09828264], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.409834861755371, 'eval/sps': 29026.030228498974}
I0728 02:00:11.950077 139742084949824 train.py:379] starting iteration 23, 9420800 steps, 722.9059362411499
I0728 02:00:40.735328 139742084949824 train.py:394] {'eval/walltime': 122.79330706596375, 'training/sps': 16815.735200307863, 'training/walltime': 619.7759959697723, 'training/entropy_loss': Array(0.03828695, dtype=float32), 'training/policy_loss': Array(0.00963499, dtype=float32), 'training/total_loss': Array(55317.066, dtype=float32), 'training/v_loss': Array(55317.02, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26764366, 0.10329977], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.52537 ,  8.905017], dtype=float32), 'eval/episode_reward': Array([-22548.084,   8916.327], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2623528 , 0.10633428], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.418783664703369, 'eval/sps': 28967.247485421438}
I0728 02:00:40.738772 139742084949824 train.py:379] starting iteration 24, 9830400 steps, 751.6946315765381
I0728 02:01:09.596537 139742084949824 train.py:394] {'eval/walltime': 127.21227717399597, 'training/sps': 16765.8463329975, 'training/walltime': 644.2066147327423, 'training/entropy_loss': Array(0.03959773, dtype=float32), 'training/policy_loss': Array(0.01244318, dtype=float32), 'training/total_loss': Array(50600.848, dtype=float32), 'training/v_loss': Array(50600.797, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2727391 , 0.10214633], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.959156,  8.862821], dtype=float32), 'eval/episode_reward': Array([-22977.771,   8814.091], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26789248, 0.10439327], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.418970108032227, 'eval/sps': 28966.02531149471}
I0728 02:01:09.600004 139742084949824 train.py:379] starting iteration 25, 10240000 steps, 780.5558624267578
I0728 02:01:38.328016 139742084949824 train.py:394] {'eval/walltime': 131.64919781684875, 'training/sps': 16867.898414890504, 'training/walltime': 668.4894263744354, 'training/entropy_loss': Array(0.04029186, dtype=float32), 'training/policy_loss': Array(0.01078397, dtype=float32), 'training/total_loss': Array(47692.7, dtype=float32), 'training/v_loss': Array(47692.65, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2692448 , 0.10791975], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.621117,  9.378833], dtype=float32), 'eval/episode_reward': Array([-22611.318,   9322.459], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2636837 , 0.11155955], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.436920642852783, 'eval/sps': 28848.836908135578}
I0728 02:01:38.376496 139742084949824 train.py:379] starting iteration 26, 10649600 steps, 809.3323369026184
I0728 02:02:07.157430 139742084949824 train.py:394] {'eval/walltime': 136.07227182388306, 'training/sps': 16821.74714013189, 'training/walltime': 692.8388590812683, 'training/entropy_loss': Array(0.04073279, dtype=float32), 'training/policy_loss': Array(0.00950826, dtype=float32), 'training/total_loss': Array(43704.73, dtype=float32), 'training/v_loss': Array(43704.68, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28738138, 0.11418507], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.181456,  9.898546], dtype=float32), 'eval/episode_reward': Array([-23955.152,   9593.37 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28175932, 0.11821352], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.423074007034302, 'eval/sps': 28939.149513761986}
I0728 02:02:07.161250 139742084949824 train.py:379] starting iteration 27, 11059200 steps, 838.1171090602875
I0728 02:02:36.002274 139742084949824 train.py:394] {'eval/walltime': 140.4865381717682, 'training/sps': 16773.996015237582, 'training/walltime': 717.2576081752777, 'training/entropy_loss': Array(0.04180349, dtype=float32), 'training/policy_loss': Array(0.01124703, dtype=float32), 'training/total_loss': Array(39785.066, dtype=float32), 'training/v_loss': Array(39785.016, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28531647, 0.10506052], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.004433,  9.115692], dtype=float32), 'eval/episode_reward': Array([-23735.861,   8593.709], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2797543 , 0.10804942], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.414266347885132, 'eval/sps': 28996.890969509488}
I0728 02:02:36.005743 139742084949824 train.py:379] starting iteration 28, 11468800 steps, 866.9616017341614
I0728 02:03:04.796378 139742084949824 train.py:394] {'eval/walltime': 144.92040753364563, 'training/sps': 16822.39925587636, 'training/walltime': 741.6060969829559, 'training/entropy_loss': Array(0.04325534, dtype=float32), 'training/policy_loss': Array(0.01276993, dtype=float32), 'training/total_loss': Array(36261.344, dtype=float32), 'training/v_loss': Array(36261.28, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29704082, 0.11850745], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.998413, 10.246207], dtype=float32), 'eval/episode_reward': Array([-24836.889,   9874.097], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29171413, 0.12192664], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.433869361877441, 'eval/sps': 28868.689975520778}
I0728 02:03:04.799852 139742084949824 train.py:379] starting iteration 29, 11878400 steps, 895.7557098865509
I0728 02:03:33.653259 139742084949824 train.py:394] {'eval/walltime': 149.36168432235718, 'training/sps': 16783.916782088723, 'training/walltime': 766.0104124546051, 'training/entropy_loss': Array(0.04472928, dtype=float32), 'training/policy_loss': Array(0.01464104, dtype=float32), 'training/total_loss': Array(32691.523, dtype=float32), 'training/v_loss': Array(32691.465, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29108986, 0.10353611], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.518656,  8.913411], dtype=float32), 'eval/episode_reward': Array([-24573.67 ,   8783.597], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28658462, 0.1054876 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.441276788711548, 'eval/sps': 28820.54104921794}
I0728 02:03:33.656687 139742084949824 train.py:379] starting iteration 30, 12288000 steps, 924.6125454902649
I0728 02:04:02.523855 139742084949824 train.py:394] {'eval/walltime': 153.80358695983887, 'training/sps': 16775.009529264164, 'training/walltime': 790.427686214447, 'training/entropy_loss': Array(0.04376865, dtype=float32), 'training/policy_loss': Array(0.01435693, dtype=float32), 'training/total_loss': Array(31173.246, dtype=float32), 'training/v_loss': Array(31173.186, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.286313  , 0.10661028], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.109226,  9.177585], dtype=float32), 'eval/episode_reward': Array([-23978.129,   8828.148], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28149477, 0.10900757], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4419026374816895, 'eval/sps': 28816.480334330077}
I0728 02:04:02.527260 139742084949824 train.py:379] starting iteration 31, 12697600 steps, 953.4831182956696
I0728 02:04:31.388012 139742084949824 train.py:394] {'eval/walltime': 158.23809027671814, 'training/sps': 16774.371400787357, 'training/walltime': 814.8458888530731, 'training/entropy_loss': Array(0.04614065, dtype=float32), 'training/policy_loss': Array(0.01427413, dtype=float32), 'training/total_loss': Array(28533.996, dtype=float32), 'training/v_loss': Array(28533.934, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29481393, 0.09919782], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.85852 ,  8.565737], dtype=float32), 'eval/episode_reward': Array([-24819.668,   8507.818], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2902118 , 0.10160993], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4345033168792725, 'eval/sps': 28864.562917968102}
I0728 02:04:31.391432 139742084949824 train.py:379] starting iteration 32, 13107200 steps, 982.3472900390625
I0728 02:05:00.213406 139742084949824 train.py:394] {'eval/walltime': 162.65550136566162, 'training/sps': 16789.255076384266, 'training/walltime': 839.2424447536469, 'training/entropy_loss': Array(0.04734472, dtype=float32), 'training/policy_loss': Array(0.01252342, dtype=float32), 'training/total_loss': Array(25896.14, dtype=float32), 'training/v_loss': Array(25896.082, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2866052 , 0.10453821], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.146675,  9.083595], dtype=float32), 'eval/episode_reward': Array([-24129.238,   9019.305], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28124988, 0.10862983], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4174110889434814, 'eval/sps': 28976.24817404394}
I0728 02:05:00.216860 139742084949824 train.py:379] starting iteration 33, 13516800 steps, 1011.1727185249329
I0728 02:05:29.049699 139742084949824 train.py:394] {'eval/walltime': 167.06630969047546, 'training/sps': 16777.28038936712, 'training/walltime': 863.6564135551453, 'training/entropy_loss': Array(0.04849442, dtype=float32), 'training/policy_loss': Array(0.0162653, dtype=float32), 'training/total_loss': Array(23470.793, dtype=float32), 'training/v_loss': Array(23470.727, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2793411 , 0.11522654], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.486122, 10.009345], dtype=float32), 'eval/episode_reward': Array([-23464.885,   9871.311], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27373037, 0.11900929], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.410808324813843, 'eval/sps': 29019.62419901849}
I0728 02:05:29.053051 139742084949824 train.py:379] starting iteration 34, 13926400 steps, 1040.0089092254639
I0728 02:05:57.918545 139742084949824 train.py:394] {'eval/walltime': 171.46899890899658, 'training/sps': 16749.11722949475, 'training/walltime': 888.1114337444305, 'training/entropy_loss': Array(0.04919074, dtype=float32), 'training/policy_loss': Array(0.01380909, dtype=float32), 'training/total_loss': Array(21165.143, dtype=float32), 'training/v_loss': Array(21165.08, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2593094 , 0.11533218], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.748274, 10.032384], dtype=float32), 'eval/episode_reward': Array([-21693.479,   9987.821], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25252748, 0.12023299], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.402689218521118, 'eval/sps': 29073.139993968445}
I0728 02:05:57.921949 139742084949824 train.py:379] starting iteration 35, 14336000 steps, 1068.8778076171875
I0728 02:06:26.751765 139742084949824 train.py:394] {'eval/walltime': 175.8759524822235, 'training/sps': 16776.86260456408, 'training/walltime': 912.5260105133057, 'training/entropy_loss': Array(0.04699364, dtype=float32), 'training/policy_loss': Array(0.01898359, dtype=float32), 'training/total_loss': Array(21153.328, dtype=float32), 'training/v_loss': Array(21153.262, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26897046, 0.09041876], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.614922,  7.796906], dtype=float32), 'eval/episode_reward': Array([-22560.273,   7753.656], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26352516, 0.0932284 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.406953573226929, 'eval/sps': 29045.007593822647}
I0728 02:06:26.755217 139742084949824 train.py:379] starting iteration 36, 14745600 steps, 1097.7110750675201
I0728 02:06:55.580703 139742084949824 train.py:394] {'eval/walltime': 180.27743291854858, 'training/sps': 16775.74468200206, 'training/walltime': 936.9422142505646, 'training/entropy_loss': Array(0.04981377, dtype=float32), 'training/policy_loss': Array(0.0160243, dtype=float32), 'training/total_loss': Array(19162.154, dtype=float32), 'training/v_loss': Array(19162.09, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28998828, 0.10004957], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.449135,  8.605999], dtype=float32), 'eval/episode_reward': Array([-24455.902,   8547.168], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28503853, 0.10305082], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.401480436325073, 'eval/sps': 29081.12437434142}
I0728 02:06:55.584175 139742084949824 train.py:379] starting iteration 37, 15155200 steps, 1126.5400331020355
I0728 02:07:24.431020 139742084949824 train.py:394] {'eval/walltime': 184.71437168121338, 'training/sps': 16785.516306676807, 'training/walltime': 961.3442041873932, 'training/entropy_loss': Array(0.05055224, dtype=float32), 'training/policy_loss': Array(0.01486801, dtype=float32), 'training/total_loss': Array(17768.484, dtype=float32), 'training/v_loss': Array(17768.42, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2936837 , 0.09868217], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.723007,  8.516589], dtype=float32), 'eval/episode_reward': Array([-24715.504,   8443.278], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.288803  , 0.10165565], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.436938762664795, 'eval/sps': 28848.71909368523}
I0728 02:07:24.434462 139742084949824 train.py:379] starting iteration 38, 15564800 steps, 1155.3903210163116
I0728 02:07:53.278220 139742084949824 train.py:394] {'eval/walltime': 189.1304361820221, 'training/sps': 16773.35501610269, 'training/walltime': 985.7638864517212, 'training/entropy_loss': Array(0.05119656, dtype=float32), 'training/policy_loss': Array(0.01774977, dtype=float32), 'training/total_loss': Array(17045.76, dtype=float32), 'training/v_loss': Array(17045.691, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29080808, 0.10589226], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.519829,  9.121102], dtype=float32), 'eval/episode_reward': Array([-24432.902,   9070.071], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28577477, 0.1085343 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.416064500808716, 'eval/sps': 28985.08388556356}
I0728 02:07:53.281794 139742084949824 train.py:379] starting iteration 39, 15974400 steps, 1184.2376527786255
I0728 02:08:22.084853 139742084949824 train.py:394] {'eval/walltime': 193.53701186180115, 'training/sps': 16794.729918303965, 'training/walltime': 1010.1524894237518, 'training/entropy_loss': Array(0.05157985, dtype=float32), 'training/policy_loss': Array(0.02353648, dtype=float32), 'training/total_loss': Array(16554.06, dtype=float32), 'training/v_loss': Array(16553.984, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29010817, 0.10932275], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.419212,  9.454473], dtype=float32), 'eval/episode_reward': Array([-24430.95 ,   9356.013], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2848446, 0.1127615], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.406575679779053, 'eval/sps': 29047.49839821609}
I0728 02:08:22.088377 139742084949824 train.py:379] starting iteration 40, 16384000 steps, 1213.0442357063293
I0728 02:08:50.917460 139742084949824 train.py:394] {'eval/walltime': 197.93040680885315, 'training/sps': 16767.921593460498, 'training/walltime': 1034.5800845623016, 'training/entropy_loss': Array(0.04927357, dtype=float32), 'training/policy_loss': Array(0.02323882, dtype=float32), 'training/total_loss': Array(19339.893, dtype=float32), 'training/v_loss': Array(19339.816, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29520315, 0.10357331], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.871752,  8.918623], dtype=float32), 'eval/episode_reward': Array([-24746.762,   8846.669], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29036117, 0.10615741], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.393394947052002, 'eval/sps': 29134.644515829124}
I0728 02:08:50.920896 139742084949824 train.py:379] starting iteration 41, 16793600 steps, 1241.876754283905
I0728 02:09:19.763051 139742084949824 train.py:394] {'eval/walltime': 202.3352587223053, 'training/sps': 16766.65611764653, 'training/walltime': 1059.0095233917236, 'training/entropy_loss': Array(0.05100491, dtype=float32), 'training/policy_loss': Array(0.02269581, dtype=float32), 'training/total_loss': Array(17534.08, dtype=float32), 'training/v_loss': Array(17534.008, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30273923, 0.10032437], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.517385,  8.652373], dtype=float32), 'eval/episode_reward': Array([-25527.996,   8470.291], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29800802, 0.10262336], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.404851913452148, 'eval/sps': 29058.8656588195}
I0728 02:09:19.766472 139742084949824 train.py:379] starting iteration 42, 17203200 steps, 1270.722331047058
I0728 02:09:48.622593 139742084949824 train.py:394] {'eval/walltime': 206.76672220230103, 'training/sps': 16775.73501714302, 'training/walltime': 1083.4257411956787, 'training/entropy_loss': Array(0.0515215, dtype=float32), 'training/policy_loss': Array(0.02398396, dtype=float32), 'training/total_loss': Array(18141.39, dtype=float32), 'training/v_loss': Array(18141.316, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2804596 , 0.10793225], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.519684,  9.37518 ], dtype=float32), 'eval/episode_reward': Array([-23325.695,   9031.612], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2746601 , 0.11143181], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4314634799957275, 'eval/sps': 28884.363050222724}
I0728 02:09:48.625976 139742084949824 train.py:379] starting iteration 43, 17612800 steps, 1299.5818347930908
I0728 02:10:17.503072 139742084949824 train.py:394] {'eval/walltime': 211.211905002594, 'training/sps': 16770.495355005976, 'training/walltime': 1107.8495874404907, 'training/entropy_loss': Array(0.05177203, dtype=float32), 'training/policy_loss': Array(0.02058454, dtype=float32), 'training/total_loss': Array(19071., dtype=float32), 'training/v_loss': Array(19070.928, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27579182, 0.11286949], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.16181 ,  9.797164], dtype=float32), 'eval/episode_reward': Array([-23186.582,   9736.02 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2694455 , 0.11729198], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.445182800292969, 'eval/sps': 28795.216248826462}
I0728 02:10:17.508172 139742084949824 train.py:379] starting iteration 44, 18022400 steps, 1328.464023590088
I0728 02:10:46.334328 139742084949824 train.py:394] {'eval/walltime': 215.6409261226654, 'training/sps': 16794.456723488573, 'training/walltime': 1132.238587141037, 'training/entropy_loss': Array(0.05202924, dtype=float32), 'training/policy_loss': Array(0.01859685, dtype=float32), 'training/total_loss': Array(19746.262, dtype=float32), 'training/v_loss': Array(19746.191, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2749989 , 0.11629096], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.095451, 10.057614], dtype=float32), 'eval/episode_reward': Array([-22616.184,   9371.045], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26913255, 0.11937921], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.429021120071411, 'eval/sps': 28900.291177192714}
I0728 02:10:46.337692 139742084949824 train.py:379] starting iteration 45, 18432000 steps, 1357.2935514450073
I0728 02:11:15.189568 139742084949824 train.py:394] {'eval/walltime': 220.0603790283203, 'training/sps': 16771.918432326413, 'training/walltime': 1156.6603610515594, 'training/entropy_loss': Array(0.05212371, dtype=float32), 'training/policy_loss': Array(0.01990756, dtype=float32), 'training/total_loss': Array(26162.02, dtype=float32), 'training/v_loss': Array(26161.945, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2775928, 0.1099341], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.343407,  9.492017], dtype=float32), 'eval/episode_reward': Array([-23263.568,   9146.179], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.271904  , 0.11386957], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.419452905654907, 'eval/sps': 28962.860954173244}
I0728 02:11:15.193637 139742084949824 train.py:379] starting iteration 46, 18841600 steps, 1386.149495601654
I0728 02:11:44.046652 139742084949824 train.py:394] {'eval/walltime': 224.46981549263, 'training/sps': 16762.2684435483, 'training/walltime': 1181.0961945056915, 'training/entropy_loss': Array(0.05330699, dtype=float32), 'training/policy_loss': Array(0.02650023, dtype=float32), 'training/total_loss': Array(23267.785, dtype=float32), 'training/v_loss': Array(23267.705, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2821573 , 0.11124808], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.721718,  9.578377], dtype=float32), 'eval/episode_reward': Array([-23664.05 ,   9362.453], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2768621 , 0.11421431], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.409436464309692, 'eval/sps': 29028.652762329504}
I0728 02:11:44.050053 139742084949824 train.py:379] starting iteration 47, 19251200 steps, 1415.0059118270874
I0728 02:12:12.863672 139742084949824 train.py:394] {'eval/walltime': 228.8713195323944, 'training/sps': 16784.2670312417, 'training/walltime': 1205.5000007152557, 'training/entropy_loss': Array(0.05287642, dtype=float32), 'training/policy_loss': Array(0.01836482, dtype=float32), 'training/total_loss': Array(23255.174, dtype=float32), 'training/v_loss': Array(23255.102, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28715438, 0.10667893], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.205023,  9.188184], dtype=float32), 'eval/episode_reward': Array([-24203.754,   9177.576], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2822576 , 0.10973618], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.401504039764404, 'eval/sps': 29080.968424341456}
I0728 02:12:12.867094 139742084949824 train.py:379] starting iteration 48, 19660800 steps, 1443.822952747345
I0728 02:12:41.677309 139742084949824 train.py:394] {'eval/walltime': 233.25934076309204, 'training/sps': 16776.756113726453, 'training/walltime': 1229.9147324562073, 'training/entropy_loss': Array(0.05255762, dtype=float32), 'training/policy_loss': Array(0.01866972, dtype=float32), 'training/total_loss': Array(23430.617, dtype=float32), 'training/v_loss': Array(23430.545, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27388245, 0.09606363], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.087786,  8.278682], dtype=float32), 'eval/episode_reward': Array([-23072.047,   8252.601], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26903692, 0.09866615], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.388021230697632, 'eval/sps': 29170.32376792987}
I0728 02:12:41.680691 139742084949824 train.py:379] starting iteration 49, 20070400 steps, 1472.6365501880646
I0728 02:13:10.561845 139742084949824 train.py:394] {'eval/walltime': 237.70005559921265, 'training/sps': 16764.286053240245, 'training/walltime': 1254.3476250171661, 'training/entropy_loss': Array(0.05279686, dtype=float32), 'training/policy_loss': Array(0.01741004, dtype=float32), 'training/total_loss': Array(23544.514, dtype=float32), 'training/v_loss': Array(23544.443, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2753175 , 0.10158437], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.165934,  8.782843], dtype=float32), 'eval/episode_reward': Array([-23127.39,   8730.59], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27025533, 0.10433473], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4407148361206055, 'eval/sps': 28824.188159719888}
I0728 02:13:10.565249 139742084949824 train.py:379] starting iteration 50, 20480000 steps, 1501.5211079120636
I0728 02:13:39.432142 139742084949824 train.py:394] {'eval/walltime': 242.13080167770386, 'training/sps': 16767.398066204576, 'training/walltime': 1278.7759828567505, 'training/entropy_loss': Array(0.05361524, dtype=float32), 'training/policy_loss': Array(0.03022319, dtype=float32), 'training/total_loss': Array(24523.436, dtype=float32), 'training/v_loss': Array(24523.352, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27429482, 0.0988297 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.069254,  8.543255], dtype=float32), 'eval/episode_reward': Array([-23035.006,   8481.652], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2690572 , 0.10199035], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.430746078491211, 'eval/sps': 28889.039843959523}
I0728 02:13:39.491930 139742084949824 train.py:379] starting iteration 51, 20889600 steps, 1530.4477698802948
I0728 02:14:08.386507 139742084949824 train.py:394] {'eval/walltime': 246.5470097064972, 'training/sps': 16738.92750146737, 'training/walltime': 1303.2458899021149, 'training/entropy_loss': Array(0.05273315, dtype=float32), 'training/policy_loss': Array(0.02519873, dtype=float32), 'training/total_loss': Array(20765.062, dtype=float32), 'training/v_loss': Array(20764.984, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26808512, 0.09453548], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.574055,  8.175883], dtype=float32), 'eval/episode_reward': Array([-22552.676 ,   8146.2173], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26311043, 0.09692783], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.416208028793335, 'eval/sps': 28984.141862305827}
I0728 02:14:08.390245 139742084949824 train.py:379] starting iteration 52, 21299200 steps, 1559.34610414505
I0728 02:14:37.221871 139742084949824 train.py:394] {'eval/walltime': 250.96767163276672, 'training/sps': 16784.769802066, 'training/walltime': 1327.6489651203156, 'training/entropy_loss': Array(0.05272184, dtype=float32), 'training/policy_loss': Array(0.01963632, dtype=float32), 'training/total_loss': Array(19795.219, dtype=float32), 'training/v_loss': Array(19795.146, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26866686, 0.10465302], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.60925 ,  9.075951], dtype=float32), 'eval/episode_reward': Array([-22582.078,   9066.217], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26324362, 0.10804822], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.420661926269531, 'eval/sps': 28954.939811019547}
I0728 02:14:37.225298 139742084949824 train.py:379] starting iteration 53, 21708800 steps, 1588.1811575889587
I0728 02:15:06.069691 139742084949824 train.py:394] {'eval/walltime': 255.4002296924591, 'training/sps': 16784.16372589857, 'training/walltime': 1352.0529215335846, 'training/entropy_loss': Array(0.05279474, dtype=float32), 'training/policy_loss': Array(0.03017511, dtype=float32), 'training/total_loss': Array(19159.889, dtype=float32), 'training/v_loss': Array(19159.805, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26595584, 0.11109414], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.361828,  9.628359], dtype=float32), 'eval/episode_reward': Array([-22349.566,   9616.341], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2599091 , 0.11530285], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.432558059692383, 'eval/sps': 28877.230320787527}
I0728 02:15:06.073163 139742084949824 train.py:379] starting iteration 54, 22118400 steps, 1617.0290215015411
I0728 02:15:34.937908 139742084949824 train.py:394] {'eval/walltime': 259.8385269641876, 'training/sps': 16775.3926589815, 'training/walltime': 1376.46963763237, 'training/entropy_loss': Array(0.05265996, dtype=float32), 'training/policy_loss': Array(0.0248496, dtype=float32), 'training/total_loss': Array(18364.658, dtype=float32), 'training/v_loss': Array(18364.582, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.279528  , 0.10731811], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.53258 ,  9.328821], dtype=float32), 'eval/episode_reward': Array([-23513.812,   9293.71 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27411318, 0.11123563], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.438297271728516, 'eval/sps': 28839.888850020587}
I0728 02:15:34.944239 139742084949824 train.py:379] starting iteration 55, 22528000 steps, 1645.9000978469849
I0728 02:16:03.801518 139742084949824 train.py:394] {'eval/walltime': 264.28359508514404, 'training/sps': 16785.195032603537, 'training/walltime': 1400.872094631195, 'training/entropy_loss': Array(0.05342529, dtype=float32), 'training/policy_loss': Array(0.01650079, dtype=float32), 'training/total_loss': Array(22293.656, dtype=float32), 'training/v_loss': Array(22293.588, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28137472, 0.10440343], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.642927,  9.031668], dtype=float32), 'eval/episode_reward': Array([-23648.146,   8982.78 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27612853, 0.1072396 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.445068120956421, 'eval/sps': 28795.959143244567}
I0728 02:16:03.805695 139742084949824 train.py:379] starting iteration 56, 22937600 steps, 1674.7615540027618
I0728 02:16:32.677424 139742084949824 train.py:394] {'eval/walltime': 268.7178919315338, 'training/sps': 16767.83600049203, 'training/walltime': 1425.2998144626617, 'training/entropy_loss': Array(0.05269635, dtype=float32), 'training/policy_loss': Array(0.02004483, dtype=float32), 'training/total_loss': Array(17404.725, dtype=float32), 'training/v_loss': Array(17404.652, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27775875, 0.10711303], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.347336,  9.244335], dtype=float32), 'eval/episode_reward': Array([-23325.867,   9203.572], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2721036 , 0.11082135], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4342968463897705, 'eval/sps': 28865.906914692136}
I0728 02:16:32.681737 139742084949824 train.py:379] starting iteration 57, 23347200 steps, 1703.6375963687897
I0728 02:17:01.535976 139742084949824 train.py:394] {'eval/walltime': 273.1563868522644, 'training/sps': 16783.00302200883, 'training/walltime': 1449.7054586410522, 'training/entropy_loss': Array(0.05248321, dtype=float32), 'training/policy_loss': Array(0.02578248, dtype=float32), 'training/total_loss': Array(16812.318, dtype=float32), 'training/v_loss': Array(16812.238, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28605327, 0.1068548 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.069351,  9.25743 ], dtype=float32), 'eval/episode_reward': Array([-24048.865,   9195.696], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28069502, 0.11016691], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.438494920730591, 'eval/sps': 28838.604591425505}
I0728 02:17:01.539336 139742084949824 train.py:379] starting iteration 58, 23756800 steps, 1732.4951946735382
I0728 02:17:30.340238 139742084949824 train.py:394] {'eval/walltime': 277.57023787498474, 'training/sps': 16801.67457834664, 'training/walltime': 1474.08398103714, 'training/entropy_loss': Array(0.05266442, dtype=float32), 'training/policy_loss': Array(0.01340075, dtype=float32), 'training/total_loss': Array(16315.947, dtype=float32), 'training/v_loss': Array(16315.879, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29032308, 0.11560898], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.415207,  9.994998], dtype=float32), 'eval/episode_reward': Array([-24396.855,   9917.181], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28514552, 0.11872122], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.413851022720337, 'eval/sps': 28999.619457276396}
I0728 02:17:30.343641 139742084949824 train.py:379] starting iteration 59, 24166400 steps, 1761.299500465393
I0728 02:17:59.193229 139742084949824 train.py:394] {'eval/walltime': 281.99628829956055, 'training/sps': 16777.264496780186, 'training/walltime': 1498.4979729652405, 'training/entropy_loss': Array(0.05286011, dtype=float32), 'training/policy_loss': Array(0.02078943, dtype=float32), 'training/total_loss': Array(15880.916, dtype=float32), 'training/v_loss': Array(15880.842, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29515904, 0.10004807], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.877174,  8.607311], dtype=float32), 'eval/episode_reward': Array([-24853.852,   8588.542], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29024374, 0.10282575], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.426050424575806, 'eval/sps': 28919.68859850203}
I0728 02:17:59.196745 139742084949824 train.py:379] starting iteration 60, 24576000 steps, 1790.1526038646698
I0728 02:18:28.001754 139742084949824 train.py:394] {'eval/walltime': 286.4031066894531, 'training/sps': 16793.709751791477, 'training/walltime': 1522.8880574703217, 'training/entropy_loss': Array(0.05400507, dtype=float32), 'training/policy_loss': Array(0.01590719, dtype=float32), 'training/total_loss': Array(19209.117, dtype=float32), 'training/v_loss': Array(19209.047, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29004687, 0.09796857], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.453499,  8.439202], dtype=float32), 'eval/episode_reward': Array([-24430.94 ,   8412.079], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2850151 , 0.10074097], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.406818389892578, 'eval/sps': 29045.89857698224}
I0728 02:18:28.005160 139742084949824 train.py:379] starting iteration 61, 24985600 steps, 1818.9610195159912
I0728 02:18:56.791475 139742084949824 train.py:394] {'eval/walltime': 290.81957149505615, 'training/sps': 16813.395339937244, 'training/walltime': 1547.249585390091, 'training/entropy_loss': Array(0.05323565, dtype=float32), 'training/policy_loss': Array(0.01613065, dtype=float32), 'training/total_loss': Array(15348.568, dtype=float32), 'training/v_loss': Array(15348.498, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26571152, 0.1064088 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.339455,  9.254031], dtype=float32), 'eval/episode_reward': Array([-22344.562,   9184.035], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25951555, 0.1109656 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.416464805603027, 'eval/sps': 28982.456701027142}
I0728 02:18:56.794917 139742084949824 train.py:379] starting iteration 62, 25395200 steps, 1847.7507758140564
I0728 02:19:25.597543 139742084949824 train.py:394] {'eval/walltime': 295.2298686504364, 'training/sps': 16797.904653702302, 'training/walltime': 1571.6335790157318, 'training/entropy_loss': Array(0.05316946, dtype=float32), 'training/policy_loss': Array(0.0116073, dtype=float32), 'training/total_loss': Array(14860.825, dtype=float32), 'training/v_loss': Array(14860.76, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28981388, 0.10061872], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.442299,  8.717456], dtype=float32), 'eval/episode_reward': Array([-24404.355,   8653.191], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28502178, 0.10358331], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.410297155380249, 'eval/sps': 29022.987678698497}
I0728 02:19:25.600913 139742084949824 train.py:379] starting iteration 63, 25804800 steps, 1876.556771993637
I0728 02:19:54.441487 139742084949824 train.py:394] {'eval/walltime': 299.67209672927856, 'training/sps': 16793.50504389629, 'training/walltime': 1596.0239608287811, 'training/entropy_loss': Array(0.05305769, dtype=float32), 'training/policy_loss': Array(0.01971953, dtype=float32), 'training/total_loss': Array(14545.82, dtype=float32), 'training/v_loss': Array(14545.748, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26911855, 0.10807476], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.614164,  9.3365  ], dtype=float32), 'eval/episode_reward': Array([-22618.504,   9287.522], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26339406, 0.11174363], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.442228078842163, 'eval/sps': 28814.369214775288}
I0728 02:19:54.444936 139742084949824 train.py:379] starting iteration 64, 26214400 steps, 1905.400795698166
I0728 02:20:23.268057 139742084949824 train.py:394] {'eval/walltime': 304.10222840309143, 'training/sps': 16797.844868980996, 'training/walltime': 1620.4080412387848, 'training/entropy_loss': Array(0.05318521, dtype=float32), 'training/policy_loss': Array(0.01708595, dtype=float32), 'training/total_loss': Array(14131.567, dtype=float32), 'training/v_loss': Array(14131.497, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2837687 , 0.10136955], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.876797 ,  8.7584715], dtype=float32), 'eval/episode_reward': Array([-23879.033,   8716.894], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2786014 , 0.10426592], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.430131673812866, 'eval/sps': 28893.04639783645}
I0728 02:20:23.271438 139742084949824 train.py:379] starting iteration 65, 26624000 steps, 1934.227296113968
I0728 02:20:52.115452 139742084949824 train.py:394] {'eval/walltime': 308.5243835449219, 'training/sps': 16777.373287874572, 'training/walltime': 1644.8218748569489, 'training/entropy_loss': Array(0.05403268, dtype=float32), 'training/policy_loss': Array(0.02946116, dtype=float32), 'training/total_loss': Array(16905.207, dtype=float32), 'training/v_loss': Array(16905.121, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26995885, 0.10260973], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.703484,  8.848781], dtype=float32), 'eval/episode_reward': Array([-22665.215,   8718.647], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2648955 , 0.10539622], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.422155141830444, 'eval/sps': 28945.16268531852}
I0728 02:20:52.118890 139742084949824 train.py:379] starting iteration 66, 27033600 steps, 1963.0747485160828
I0728 02:21:20.960872 139742084949824 train.py:394] {'eval/walltime': 312.9393117427826, 'training/sps': 16773.629817601595, 'training/walltime': 1669.2411570549011, 'training/entropy_loss': Array(0.05309877, dtype=float32), 'training/policy_loss': Array(0.00801871, dtype=float32), 'training/total_loss': Array(13359.619, dtype=float32), 'training/v_loss': Array(13359.559, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27079552, 0.0971034 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.785187,  8.383078], dtype=float32), 'eval/episode_reward': Array([-22780.094,   8353.182], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26593885, 0.09923733], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.414928197860718, 'eval/sps': 28992.54399245343}
I0728 02:21:20.964252 139742084949824 train.py:379] starting iteration 67, 27443200 steps, 1991.9201111793518
I0728 02:21:49.786454 139742084949824 train.py:394] {'eval/walltime': 317.3453149795532, 'training/sps': 16781.23120454827, 'training/walltime': 1693.6493780612946, 'training/entropy_loss': Array(0.05315385, dtype=float32), 'training/policy_loss': Array(0.01805652, dtype=float32), 'training/total_loss': Array(12911.92, dtype=float32), 'training/v_loss': Array(12911.848, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28807187, 0.1087034 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.226633,  9.436899], dtype=float32), 'eval/episode_reward': Array([-24223.121,   9397.509], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28245246, 0.11213282], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.40600323677063, 'eval/sps': 29051.272348546277}
I0728 02:21:49.789887 139742084949824 train.py:379] starting iteration 68, 27852800 steps, 2020.7457456588745
I0728 02:22:18.619459 139742084949824 train.py:394] {'eval/walltime': 321.7443494796753, 'training/sps': 16771.73357580095, 'training/walltime': 1718.0714211463928, 'training/entropy_loss': Array(0.05324126, dtype=float32), 'training/policy_loss': Array(0.01292462, dtype=float32), 'training/total_loss': Array(12651.736, dtype=float32), 'training/v_loss': Array(12651.67, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2719478 , 0.10631246], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.882076,  9.191846], dtype=float32), 'eval/episode_reward': Array([-22892.783,   9184.228], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26651782, 0.10994627], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.39903450012207, 'eval/sps': 29097.293962220137}
I0728 02:22:18.622858 139742084949824 train.py:379] starting iteration 69, 28262400 steps, 2049.5787167549133
I0728 02:22:47.442982 139742084949824 train.py:394] {'eval/walltime': 326.1707167625427, 'training/sps': 16797.588325090524, 'training/walltime': 1742.455873966217, 'training/entropy_loss': Array(0.05349248, dtype=float32), 'training/policy_loss': Array(0.01249199, dtype=float32), 'training/total_loss': Array(12270.736, dtype=float32), 'training/v_loss': Array(12270.67, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28418607, 0.11266845], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.897274 ,  9.7072115], dtype=float32), 'eval/episode_reward': Array([-23897.156,   9657.386], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27865773, 0.11618158], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.426367282867432, 'eval/sps': 28917.618403568333}
I0728 02:22:47.446198 139742084949824 train.py:379] starting iteration 70, 28672000 steps, 2078.4020574092865
I0728 02:23:16.258340 139742084949824 train.py:394] {'eval/walltime': 330.5868966579437, 'training/sps': 16795.307695508254, 'training/walltime': 1766.8436379432678, 'training/entropy_loss': Array(0.05454344, dtype=float32), 'training/policy_loss': Array(0.03329797, dtype=float32), 'training/total_loss': Array(14570.535, dtype=float32), 'training/v_loss': Array(14570.446, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28284055, 0.10703678], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.781292,  9.268146], dtype=float32), 'eval/episode_reward': Array([-23800.74  ,   9249.4375], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2774145, 0.1106433], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.416179895401001, 'eval/sps': 28984.326506558053}
I0728 02:23:16.261672 139742084949824 train.py:379] starting iteration 71, 29081600 steps, 2107.2175312042236
I0728 02:23:45.070799 139742084949824 train.py:394] {'eval/walltime': 335.0028192996979, 'training/sps': 16797.055717695308, 'training/walltime': 1791.228863954544, 'training/entropy_loss': Array(0.05344282, dtype=float32), 'training/policy_loss': Array(0.03062435, dtype=float32), 'training/total_loss': Array(11856.34, dtype=float32), 'training/v_loss': Array(11856.255, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26690608, 0.09613916], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.464668,  8.332953], dtype=float32), 'eval/episode_reward': Array([-22456.285,   8307.107], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2617281 , 0.09930044], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.41592264175415, 'eval/sps': 28986.015015234545}
I0728 02:23:45.074099 139742084949824 train.py:379] starting iteration 72, 29491200 steps, 2136.0299587249756
I0728 02:24:13.907639 139742084949824 train.py:394] {'eval/walltime': 339.4419050216675, 'training/sps': 16797.2726653202, 'training/walltime': 1815.6137750148773, 'training/entropy_loss': Array(0.05351804, dtype=float32), 'training/policy_loss': Array(0.01375686, dtype=float32), 'training/total_loss': Array(11528.657, dtype=float32), 'training/v_loss': Array(11528.591, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27748784, 0.10859251], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.379549,  9.328837], dtype=float32), 'eval/episode_reward': Array([-23355.79 ,   9261.665], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27259076, 0.11121803], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4390857219696045, 'eval/sps': 28834.76643997019}
I0728 02:24:13.910870 139742084949824 train.py:379] starting iteration 73, 29900800 steps, 2164.8667290210724
I0728 02:24:42.793959 139742084949824 train.py:394] {'eval/walltime': 343.8812344074249, 'training/sps': 16764.535037203354, 'training/walltime': 1840.0463047027588, 'training/entropy_loss': Array(0.05357286, dtype=float32), 'training/policy_loss': Array(0.02352442, dtype=float32), 'training/total_loss': Array(11325.338, dtype=float32), 'training/v_loss': Array(11325.261, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27734318, 0.10249373], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.348843,  8.842691], dtype=float32), 'eval/episode_reward': Array([-23336.207,   8804.948], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2722863 , 0.10523952], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.439329385757446, 'eval/sps': 28833.183771102493}
I0728 02:24:42.797242 139742084949824 train.py:379] starting iteration 74, 30310400 steps, 2193.753099679947
I0728 02:25:11.631294 139742084949824 train.py:394] {'eval/walltime': 348.3139154911041, 'training/sps': 16791.480395391747, 'training/walltime': 1864.4396274089813, 'training/entropy_loss': Array(0.05354344, dtype=float32), 'training/policy_loss': Array(0.01799947, dtype=float32), 'training/total_loss': Array(11101.319, dtype=float32), 'training/v_loss': Array(11101.248, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2677626, 0.1054141], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.504227,  9.103622], dtype=float32), 'eval/episode_reward': Array([-22492.916,   9087.861], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26203918, 0.10898639], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.432681083679199, 'eval/sps': 28876.428866332488}
I0728 02:25:11.634536 139742084949824 train.py:379] starting iteration 75, 30720000 steps, 2222.590394973755
I0728 02:25:40.426639 139742084949824 train.py:394] {'eval/walltime': 352.7221143245697, 'training/sps': 16803.49147521479, 'training/walltime': 1888.8155138492584, 'training/entropy_loss': Array(0.05431034, dtype=float32), 'training/policy_loss': Array(0.01244723, dtype=float32), 'training/total_loss': Array(13114.441, dtype=float32), 'training/v_loss': Array(13114.375, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27306744, 0.095355  ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.988806,  8.26261 ], dtype=float32), 'eval/episode_reward': Array([-22995.96 ,   8252.817], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26818419, 0.09815504], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.408198833465576, 'eval/sps': 29036.80274770427}
I0728 02:25:40.488721 139742084949824 train.py:379] starting iteration 76, 31129600 steps, 2251.4445695877075
I0728 02:26:09.342480 139742084949824 train.py:394] {'eval/walltime': 357.1346278190613, 'training/sps': 16763.822786074692, 'training/walltime': 1913.2490816116333, 'training/entropy_loss': Array(0.05361477, dtype=float32), 'training/policy_loss': Array(0.02163212, dtype=float32), 'training/total_loss': Array(10741.595, dtype=float32), 'training/v_loss': Array(10741.5205, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28153425, 0.10276758], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.722748,  8.873079], dtype=float32), 'eval/episode_reward': Array([-23715.566,   8852.109], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27660686, 0.10583056], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.412513494491577, 'eval/sps': 29008.409868840194}
I0728 02:26:09.346031 139742084949824 train.py:379] starting iteration 77, 31539200 steps, 2280.301889896393
I0728 02:26:38.140800 139742084949824 train.py:394] {'eval/walltime': 361.5541343688965, 'training/sps': 16809.536766635934, 'training/walltime': 1937.6162016391754, 'training/entropy_loss': Array(0.0535977, dtype=float32), 'training/policy_loss': Array(0.01621168, dtype=float32), 'training/total_loss': Array(10483.121, dtype=float32), 'training/v_loss': Array(10483.051, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.275139  , 0.09484214], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.203701,  8.17999 ], dtype=float32), 'eval/episode_reward': Array([-23185.404,   8178.721], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2700635 , 0.09768873], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.419506549835205, 'eval/sps': 28962.509401592102}
I0728 02:26:38.144032 139742084949824 train.py:379] starting iteration 78, 31948800 steps, 2309.099891424179
I0728 02:27:06.990855 139742084949824 train.py:394] {'eval/walltime': 365.9622550010681, 'training/sps': 16765.89034635812, 'training/walltime': 1962.0467562675476, 'training/entropy_loss': Array(0.05351699, dtype=float32), 'training/policy_loss': Array(0.03276981, dtype=float32), 'training/total_loss': Array(10337.215, dtype=float32), 'training/v_loss': Array(10337.129, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29104894, 0.10417105], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.51998 ,  9.026173], dtype=float32), 'eval/episode_reward': Array([-24513.502,   8993.58 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28596565, 0.10752611], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.408120632171631, 'eval/sps': 29037.317868713057}
I0728 02:27:06.994081 139742084949824 train.py:379] starting iteration 79, 32358400 steps, 2337.9499402046204
I0728 02:27:35.830990 139742084949824 train.py:394] {'eval/walltime': 370.39897656440735, 'training/sps': 16792.36126676908, 'training/walltime': 1986.438799381256, 'training/entropy_loss': Array(0.0533936, dtype=float32), 'training/policy_loss': Array(0.02151637, dtype=float32), 'training/total_loss': Array(10114.168, dtype=float32), 'training/v_loss': Array(10114.092, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28784978, 0.08981209], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.2844   ,  7.7489896], dtype=float32), 'eval/episode_reward': Array([-24296.766,   7741.64 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2834944 , 0.09217593], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.436721563339233, 'eval/sps': 28850.131380266892}
I0728 02:27:35.834390 139742084949824 train.py:379] starting iteration 80, 32768000 steps, 2366.7902488708496
I0728 02:28:04.683483 139742084949824 train.py:394] {'eval/walltime': 374.83039450645447, 'training/sps': 16780.13826763405, 'training/walltime': 2010.848610162735, 'training/entropy_loss': Array(0.05385417, dtype=float32), 'training/policy_loss': Array(0.01750583, dtype=float32), 'training/total_loss': Array(11803.275, dtype=float32), 'training/v_loss': Array(11803.203, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28569242, 0.09410606], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.109037,  8.121983], dtype=float32), 'eval/episode_reward': Array([-24093.    ,   8091.8945], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28118783, 0.09639724], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.431417942047119, 'eval/sps': 28884.659870485983}
I0728 02:28:04.686828 139742084949824 train.py:379] starting iteration 81, 33177600 steps, 2395.6426858901978
I0728 02:28:33.479950 139742084949824 train.py:394] {'eval/walltime': 379.2493634223938, 'training/sps': 16811.416890410423, 'training/walltime': 2035.213005065918, 'training/entropy_loss': Array(0.05388482, dtype=float32), 'training/policy_loss': Array(0.01019265, dtype=float32), 'training/total_loss': Array(9623.677, dtype=float32), 'training/v_loss': Array(9623.613, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27819395, 0.09920408], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.429708,  8.597007], dtype=float32), 'eval/episode_reward': Array([-23413.238,   8592.488], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2729511 , 0.10180848], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.418968915939331, 'eval/sps': 28966.033125578415}
I0728 02:28:33.486981 139742084949824 train.py:379] starting iteration 82, 33587200 steps, 2424.4428236484528
I0728 02:29:02.262380 139742084949824 train.py:394] {'eval/walltime': 383.68256974220276, 'training/sps': 16832.31503812338, 'training/walltime': 2059.547150373459, 'training/entropy_loss': Array(0.05398089, dtype=float32), 'training/policy_loss': Array(0.02566022, dtype=float32), 'training/total_loss': Array(9401.662, dtype=float32), 'training/v_loss': Array(9401.583, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27778903, 0.09494331], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.382566,  8.234868], dtype=float32), 'eval/episode_reward': Array([-23380.092,   8232.264], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27279067, 0.09796995], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.43320631980896, 'eval/sps': 28873.007653186756}
I0728 02:29:02.265705 139742084949824 train.py:379] starting iteration 83, 33996800 steps, 2453.221563577652
I0728 02:29:31.027679 139742084949824 train.py:394] {'eval/walltime': 388.09890031814575, 'training/sps': 16829.741887395277, 'training/walltime': 2083.8850162029266, 'training/entropy_loss': Array(0.05395885, dtype=float32), 'training/policy_loss': Array(0.02014498, dtype=float32), 'training/total_loss': Array(9279.502, dtype=float32), 'training/v_loss': Array(9279.429, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27864486, 0.09439376], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.458725,  8.176294], dtype=float32), 'eval/episode_reward': Array([-23441.072 ,   8126.4497], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27354848, 0.09741256], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.416330575942993, 'eval/sps': 28983.337591903186}
I0728 02:29:31.031042 139742084949824 train.py:379] starting iteration 84, 34406400 steps, 2481.9869010448456
I0728 02:29:59.871026 139742084949824 train.py:394] {'eval/walltime': 392.50718879699707, 'training/sps': 16772.206941087185, 'training/walltime': 2108.3063700199127, 'training/entropy_loss': Array(0.05400918, dtype=float32), 'training/policy_loss': Array(0.02434678, dtype=float32), 'training/total_loss': Array(9089.481, dtype=float32), 'training/v_loss': Array(9089.403, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29235065, 0.09601865], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.655754,  8.319639], dtype=float32), 'eval/episode_reward': Array([-24652.688,   8297.172], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2877419 , 0.09864952], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.408288478851318, 'eval/sps': 29036.21226561683}
I0728 02:29:59.878849 139742084949824 train.py:379] starting iteration 85, 34816000 steps, 2510.8347063064575
I0728 02:30:28.707862 139742084949824 train.py:394] {'eval/walltime': 396.9450397491455, 'training/sps': 16798.649041966142, 'training/walltime': 2132.689283132553, 'training/entropy_loss': Array(0.05423862, dtype=float32), 'training/policy_loss': Array(0.03657623, dtype=float32), 'training/total_loss': Array(10669.888, dtype=float32), 'training/v_loss': Array(10669.797, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28140455, 0.10450597], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.682564,  9.085873], dtype=float32), 'eval/episode_reward': Array([-23681.05 ,   9058.137], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27617595, 0.10792356], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4378509521484375, 'eval/sps': 28842.78930954927}
I0728 02:30:28.711274 139742084949824 train.py:379] starting iteration 86, 35225600 steps, 2539.6671323776245
I0728 02:30:57.550402 139742084949824 train.py:394] {'eval/walltime': 401.3798129558563, 'training/sps': 16789.44901553759, 'training/walltime': 2157.0855572223663, 'training/entropy_loss': Array(0.05401102, dtype=float32), 'training/policy_loss': Array(0.02417607, dtype=float32), 'training/total_loss': Array(8812.659, dtype=float32), 'training/v_loss': Array(8812.582, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2610396 , 0.10976817], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.983456,  9.473847], dtype=float32), 'eval/episode_reward': Array([-21964.71 ,   9433.775], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25517663, 0.11358325], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.434773206710815, 'eval/sps': 28862.806288787673}
I0728 02:30:57.553812 139742084949824 train.py:379] starting iteration 87, 35635200 steps, 2568.509670972824
I0728 02:31:26.339799 139742084949824 train.py:394] {'eval/walltime': 405.7930061817169, 'training/sps': 16810.962694252252, 'training/walltime': 2181.450610399246, 'training/entropy_loss': Array(0.05394242, dtype=float32), 'training/policy_loss': Array(0.01664911, dtype=float32), 'training/total_loss': Array(8592.686, dtype=float32), 'training/v_loss': Array(8592.615, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2825732, 0.1013608], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.80466,  8.72787], dtype=float32), 'eval/episode_reward': Array([-23803.426,   8708.5  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27768508, 0.10404745], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.413193225860596, 'eval/sps': 29003.941918958088}
I0728 02:31:26.342992 139742084949824 train.py:379] starting iteration 88, 36044800 steps, 2597.2988505363464
I0728 02:31:55.171016 139742084949824 train.py:394] {'eval/walltime': 410.21577548980713, 'training/sps': 16788.534981118763, 'training/walltime': 2205.8482127189636, 'training/entropy_loss': Array(0.05375115, dtype=float32), 'training/policy_loss': Array(0.02483942, dtype=float32), 'training/total_loss': Array(8513.314, dtype=float32), 'training/v_loss': Array(8513.236, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27317154, 0.10775305], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.95277 ,  9.349968], dtype=float32), 'eval/episode_reward': Array([-22944.096,   9350.724], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2673803 , 0.11210282], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.42276930809021, 'eval/sps': 28941.14322577487}
I0728 02:31:55.174224 139742084949824 train.py:379] starting iteration 89, 36454400 steps, 2626.1300823688507
I0728 02:32:24.012489 139742084949824 train.py:394] {'eval/walltime': 414.6263689994812, 'training/sps': 16773.110027841132, 'training/walltime': 2230.268251657486, 'training/entropy_loss': Array(0.05409024, dtype=float32), 'training/policy_loss': Array(0.0235255, dtype=float32), 'training/total_loss': Array(8339.083, dtype=float32), 'training/v_loss': Array(8339.005, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27286392, 0.10067429], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.975508,  8.714111], dtype=float32), 'eval/episode_reward': Array([-22979.113,   8706.072], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2674476 , 0.10398195], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.410593509674072, 'eval/sps': 29021.037581279794}
I0728 02:32:24.015665 139742084949824 train.py:379] starting iteration 90, 36864000 steps, 2654.971524000168
