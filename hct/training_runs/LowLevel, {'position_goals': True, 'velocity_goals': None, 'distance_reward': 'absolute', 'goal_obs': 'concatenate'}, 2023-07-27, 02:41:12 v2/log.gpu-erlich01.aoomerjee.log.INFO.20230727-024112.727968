I0727 02:41:12.992748 140120985872192 low_level_env.py:187] Initialising environment...
I0727 02:41:52.283673 140120985872192 low_level_env.py:289] Environment initialised.
I0727 02:41:52.288265 140120985872192 train.py:118] JAX is running on GPU.
I0727 02:41:52.288320 140120985872192 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0727 02:41:59.783187 140120985872192 train.py:367] Running initial eval
I0727 02:42:15.317938 140120985872192 train.py:373] {'eval/walltime': 15.396107196807861, 'eval/episode_goal_distance': (Array(0.70632434, dtype=float32), Array(0.4034848, dtype=float32)), 'eval/episode_reward': (Array(-13463.598, dtype=float32), Array(7097.274, dtype=float32)), 'eval/avg_episode_length': (Array(922.375, dtype=float32), Array(266.65134, dtype=float32)), 'eval/epoch_eval_time': 15.396107196807861, 'eval/sps': 8313.789866735844}
I0727 02:42:15.319232 140120985872192 train.py:379] starting iteration 0 23.03097701072693
I0727 02:42:46.174649 140120985872192 train.py:394] {'eval/walltime': 19.18573808670044, 'training/sps': 9081.63296968699, 'training/walltime': 27.06121253967285, 'training/entropy_loss': Array(-0.0454259, dtype=float32), 'training/policy_loss': Array(0.01924485, dtype=float32), 'training/total_loss': Array(152.0128, dtype=float32), 'training/v_loss': Array(152.03897, dtype=float32), 'eval/episode_goal_distance': (Array(0.7939373, dtype=float32), Array(0.47939697, dtype=float32)), 'eval/episode_reward': (Array(-14191.457, dtype=float32), Array(6877.2837, dtype=float32)), 'eval/avg_episode_length': (Array(937.9297, dtype=float32), Array(240.39757, dtype=float32)), 'eval/epoch_eval_time': 3.789630889892578, 'eval/sps': 33776.37657044439}
I0727 02:42:46.200030 140120985872192 train.py:379] starting iteration 1 53.911776542663574
I0727 02:42:55.389287 140120985872192 train.py:394] {'eval/walltime': 22.981783390045166, 'training/sps': 45599.90525339859, 'training/walltime': 32.45069742202759, 'training/entropy_loss': Array(-0.04481579, dtype=float32), 'training/policy_loss': Array(0.00396745, dtype=float32), 'training/total_loss': Array(146.57571, dtype=float32), 'training/v_loss': Array(146.61656, dtype=float32), 'eval/episode_goal_distance': (Array(0.7491059, dtype=float32), Array(0.40168887, dtype=float32)), 'eval/episode_reward': (Array(-13751.089, dtype=float32), Array(6582.021, dtype=float32)), 'eval/avg_episode_length': (Array(937.83594, dtype=float32), Array(240.76077, dtype=float32)), 'eval/epoch_eval_time': 3.7960453033447266, 'eval/sps': 33719.30252971906}
I0727 02:42:55.391745 140120985872192 train.py:379] starting iteration 2 63.10349416732788
I0727 02:43:04.617225 140120985872192 train.py:394] {'eval/walltime': 26.805644273757935, 'training/sps': 45528.8178914246, 'training/walltime': 37.848597288131714, 'training/entropy_loss': Array(-0.04456441, dtype=float32), 'training/policy_loss': Array(0.00320313, dtype=float32), 'training/total_loss': Array(159.52448, dtype=float32), 'training/v_loss': Array(159.56584, dtype=float32), 'eval/episode_goal_distance': (Array(0.70518494, dtype=float32), Array(0.381641, dtype=float32)), 'eval/episode_reward': (Array(-13323.642, dtype=float32), Array(6728.697, dtype=float32)), 'eval/avg_episode_length': (Array(914.6406, dtype=float32), Array(278.3866, dtype=float32)), 'eval/epoch_eval_time': 3.8238608837127686, 'eval/sps': 33474.02112487908}
I0727 02:43:04.619706 140120985872192 train.py:379] starting iteration 3 72.33145427703857
I0727 02:43:13.846482 140120985872192 train.py:394] {'eval/walltime': 30.62718391418457, 'training/sps': 45498.18959435945, 'training/walltime': 43.25013089179993, 'training/entropy_loss': Array(-0.04360252, dtype=float32), 'training/policy_loss': Array(0.00470253, dtype=float32), 'training/total_loss': Array(133.72345, dtype=float32), 'training/v_loss': Array(133.76236, dtype=float32), 'eval/episode_goal_distance': (Array(0.7414833, dtype=float32), Array(0.41474497, dtype=float32)), 'eval/episode_reward': (Array(-13310.236, dtype=float32), Array(6734.756, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.75842, dtype=float32)), 'eval/epoch_eval_time': 3.8215396404266357, 'eval/sps': 33494.35359663314}
I0727 02:43:13.849074 140120985872192 train.py:379] starting iteration 4 81.56082320213318
I0727 02:43:23.064188 140120985872192 train.py:394] {'eval/walltime': 34.4403920173645, 'training/sps': 45525.28894469033, 'training/walltime': 48.648449182510376, 'training/entropy_loss': Array(-0.04261852, dtype=float32), 'training/policy_loss': Array(0.00716801, dtype=float32), 'training/total_loss': Array(172.31827, dtype=float32), 'training/v_loss': Array(172.35371, dtype=float32), 'eval/episode_goal_distance': (Array(0.74004865, dtype=float32), Array(0.46694568, dtype=float32)), 'eval/episode_reward': (Array(-12854.66, dtype=float32), Array(7460.43, dtype=float32)), 'eval/avg_episode_length': (Array(891.28906, dtype=float32), Array(310.21445, dtype=float32)), 'eval/epoch_eval_time': 3.8132081031799316, 'eval/sps': 33567.53592683744}
I0727 02:43:23.066674 140120985872192 train.py:379] starting iteration 5 90.77842259407043
I0727 02:43:32.307997 140120985872192 train.py:394] {'eval/walltime': 38.270644426345825, 'training/sps': 45448.54973975569, 'training/walltime': 54.05588245391846, 'training/entropy_loss': Array(-0.04233227, dtype=float32), 'training/policy_loss': Array(0.00819906, dtype=float32), 'training/total_loss': Array(151.8707, dtype=float32), 'training/v_loss': Array(151.90482, dtype=float32), 'eval/episode_goal_distance': (Array(0.7736119, dtype=float32), Array(0.40481198, dtype=float32)), 'eval/episode_reward': (Array(-13509.676, dtype=float32), Array(6531.9893, dtype=float32)), 'eval/avg_episode_length': (Array(906.8672, dtype=float32), Array(289.56223, dtype=float32)), 'eval/epoch_eval_time': 3.8302524089813232, 'eval/sps': 33418.163173753426}
I0727 02:43:32.310369 140120985872192 train.py:379] starting iteration 6 100.02211737632751
I0727 02:43:41.570236 140120985872192 train.py:394] {'eval/walltime': 42.103477001190186, 'training/sps': 45315.06855934572, 'training/walltime': 59.479243993759155, 'training/entropy_loss': Array(-0.04049197, dtype=float32), 'training/policy_loss': Array(0.00861448, dtype=float32), 'training/total_loss': Array(153.9963, dtype=float32), 'training/v_loss': Array(154.02818, dtype=float32), 'eval/episode_goal_distance': (Array(0.7232431, dtype=float32), Array(0.38943693, dtype=float32)), 'eval/episode_reward': (Array(-13640.25, dtype=float32), Array(6478.7935, dtype=float32)), 'eval/avg_episode_length': (Array(953.39844, dtype=float32), Array(210.1384, dtype=float32)), 'eval/epoch_eval_time': 3.8328325748443604, 'eval/sps': 33395.66691226989}
I0727 02:43:41.572696 140120985872192 train.py:379] starting iteration 7 109.28444504737854
I0727 02:43:50.839940 140120985872192 train.py:394] {'eval/walltime': 45.94574809074402, 'training/sps': 45332.440473892086, 'training/walltime': 64.90052723884583, 'training/entropy_loss': Array(-0.03895803, dtype=float32), 'training/policy_loss': Array(0.01026581, dtype=float32), 'training/total_loss': Array(155.1073, dtype=float32), 'training/v_loss': Array(155.13599, dtype=float32), 'eval/episode_goal_distance': (Array(0.69271696, dtype=float32), Array(0.4017616, dtype=float32)), 'eval/episode_reward': (Array(-12905.676, dtype=float32), Array(6654.886, dtype=float32)), 'eval/avg_episode_length': (Array(922.3672, dtype=float32), Array(266.67798, dtype=float32)), 'eval/epoch_eval_time': 3.842271089553833, 'eval/sps': 33313.63066702913}
I0727 02:43:50.842293 140120985872192 train.py:379] starting iteration 8 118.55404233932495
I0727 02:44:00.094078 140120985872192 train.py:394] {'eval/walltime': 49.782374143600464, 'training/sps': 45412.96689471716, 'training/walltime': 70.31219744682312, 'training/entropy_loss': Array(-0.03834082, dtype=float32), 'training/policy_loss': Array(0.0092456, dtype=float32), 'training/total_loss': Array(154.50815, dtype=float32), 'training/v_loss': Array(154.53723, dtype=float32), 'eval/episode_goal_distance': (Array(0.7390821, dtype=float32), Array(0.32384282, dtype=float32)), 'eval/episode_reward': (Array(-13315.392, dtype=float32), Array(5872.922, dtype=float32)), 'eval/avg_episode_length': (Array(937.9531, dtype=float32), Array(240.30728, dtype=float32)), 'eval/epoch_eval_time': 3.8366260528564453, 'eval/sps': 33362.64682472805}
I0727 02:44:00.096410 140120985872192 train.py:379] starting iteration 9 127.80815887451172
I0727 02:44:09.371089 140120985872192 train.py:394] {'eval/walltime': 53.628461837768555, 'training/sps': 45299.87979883325, 'training/walltime': 75.73737740516663, 'training/entropy_loss': Array(-0.03669319, dtype=float32), 'training/policy_loss': Array(0.01351666, dtype=float32), 'training/total_loss': Array(71.261734, dtype=float32), 'training/v_loss': Array(71.28491, dtype=float32), 'eval/episode_goal_distance': (Array(0.69035804, dtype=float32), Array(0.34083813, dtype=float32)), 'eval/episode_reward': (Array(-12627.049, dtype=float32), Array(5910.154, dtype=float32)), 'eval/avg_episode_length': (Array(922.3594, dtype=float32), Array(266.70502, dtype=float32)), 'eval/epoch_eval_time': 3.846087694168091, 'eval/sps': 33280.57240974752}
I0727 02:44:09.373232 140120985872192 train.py:379] starting iteration 10 137.0849804878235
I0727 02:44:18.653900 140120985872192 train.py:394] {'eval/walltime': 57.476969480514526, 'training/sps': 45287.18815955077, 'training/walltime': 81.16407775878906, 'training/entropy_loss': Array(-0.03507406, dtype=float32), 'training/policy_loss': Array(0.01069716, dtype=float32), 'training/total_loss': Array(65.84946, dtype=float32), 'training/v_loss': Array(65.87383, dtype=float32), 'eval/episode_goal_distance': (Array(0.651142, dtype=float32), Array(0.31787083, dtype=float32)), 'eval/episode_reward': (Array(-11485.035, dtype=float32), Array(5307.479, dtype=float32)), 'eval/avg_episode_length': (Array(914.6172, dtype=float32), Array(278.46323, dtype=float32)), 'eval/epoch_eval_time': 3.8485076427459717, 'eval/sps': 33259.645525523745}
I0727 02:44:18.656450 140120985872192 train.py:379] starting iteration 11 146.3681983947754
I0727 02:44:27.944201 140120985872192 train.py:394] {'eval/walltime': 61.332141160964966, 'training/sps': 45266.33813219602, 'training/walltime': 86.5932776927948, 'training/entropy_loss': Array(-0.03270957, dtype=float32), 'training/policy_loss': Array(0.01155696, dtype=float32), 'training/total_loss': Array(70.13277, dtype=float32), 'training/v_loss': Array(70.153915, dtype=float32), 'eval/episode_goal_distance': (Array(0.5952772, dtype=float32), Array(0.27637818, dtype=float32)), 'eval/episode_reward': (Array(-11692.219, dtype=float32), Array(5586.92, dtype=float32)), 'eval/avg_episode_length': (Array(930.1172, dtype=float32), Array(254.11107, dtype=float32)), 'eval/epoch_eval_time': 3.8551716804504395, 'eval/sps': 33202.153006333676}
I0727 02:44:27.946733 140120985872192 train.py:379] starting iteration 12 155.65848207473755
I0727 02:44:37.260272 140120985872192 train.py:394] {'eval/walltime': 65.18466114997864, 'training/sps': 45029.79428723601, 'training/walltime': 92.05099749565125, 'training/entropy_loss': Array(-0.03117936, dtype=float32), 'training/policy_loss': Array(0.01273068, dtype=float32), 'training/total_loss': Array(73.349884, dtype=float32), 'training/v_loss': Array(73.36833, dtype=float32), 'eval/episode_goal_distance': (Array(0.52387667, dtype=float32), Array(0.25231156, dtype=float32)), 'eval/episode_reward': (Array(-10063.037, dtype=float32), Array(5491.823, dtype=float32)), 'eval/avg_episode_length': (Array(844.7422, dtype=float32), Array(360.7873, dtype=float32)), 'eval/epoch_eval_time': 3.852519989013672, 'eval/sps': 33225.006064866844}
I0727 02:44:37.262425 140120985872192 train.py:379] starting iteration 13 164.97417426109314
I0727 02:44:46.606045 140120985872192 train.py:394] {'eval/walltime': 69.04588294029236, 'training/sps': 44856.93956630216, 'training/walltime': 97.52974843978882, 'training/entropy_loss': Array(-0.03296534, dtype=float32), 'training/policy_loss': Array(0.02189848, dtype=float32), 'training/total_loss': Array(74.092285, dtype=float32), 'training/v_loss': Array(74.103355, dtype=float32), 'eval/episode_goal_distance': (Array(0.5578589, dtype=float32), Array(0.2880173, dtype=float32)), 'eval/episode_reward': (Array(-10754.342, dtype=float32), Array(5933.0303, dtype=float32)), 'eval/avg_episode_length': (Array(860.1797, dtype=float32), Array(345.6458, dtype=float32)), 'eval/epoch_eval_time': 3.8612217903137207, 'eval/sps': 33150.12888436023}
I0727 02:44:46.610931 140120985872192 train.py:379] starting iteration 14 174.32267308235168
I0727 02:44:55.965888 140120985872192 train.py:394] {'eval/walltime': 72.90286684036255, 'training/sps': 44731.649546815206, 'training/walltime': 103.02384495735168, 'training/entropy_loss': Array(-0.03565937, dtype=float32), 'training/policy_loss': Array(0.01536846, dtype=float32), 'training/total_loss': Array(81.019005, dtype=float32), 'training/v_loss': Array(81.03929, dtype=float32), 'eval/episode_goal_distance': (Array(0.5276493, dtype=float32), Array(0.25339985, dtype=float32)), 'eval/episode_reward': (Array(-10806.644, dtype=float32), Array(5292.891, dtype=float32)), 'eval/avg_episode_length': (Array(899.08594, dtype=float32), Array(300.1441, dtype=float32)), 'eval/epoch_eval_time': 3.8569839000701904, 'eval/sps': 33186.55283929773}
I0727 02:44:55.968363 140120985872192 train.py:379] starting iteration 15 183.6801118850708
I0727 02:45:05.321846 140120985872192 train.py:394] {'eval/walltime': 76.75969123840332, 'training/sps': 44739.559183513324, 'training/walltime': 108.51697015762329, 'training/entropy_loss': Array(-0.03186776, dtype=float32), 'training/policy_loss': Array(0.01963941, dtype=float32), 'training/total_loss': Array(81.08098, dtype=float32), 'training/v_loss': Array(81.09321, dtype=float32), 'eval/episode_goal_distance': (Array(0.53651035, dtype=float32), Array(0.2598496, dtype=float32)), 'eval/episode_reward': (Array(-10142.49, dtype=float32), Array(5039.7124, dtype=float32)), 'eval/avg_episode_length': (Array(883.5, dtype=float32), Array(319.75717, dtype=float32)), 'eval/epoch_eval_time': 3.8568243980407715, 'eval/sps': 33187.92529548992}
I0727 02:45:05.324190 140120985872192 train.py:379] starting iteration 16 193.03593802452087
I0727 02:45:14.705283 140120985872192 train.py:394] {'eval/walltime': 80.63195371627808, 'training/sps': 44641.48285687744, 'training/walltime': 114.02216362953186, 'training/entropy_loss': Array(-0.02868214, dtype=float32), 'training/policy_loss': Array(0.02816407, dtype=float32), 'training/total_loss': Array(101.86837, dtype=float32), 'training/v_loss': Array(101.86888, dtype=float32), 'eval/episode_goal_distance': (Array(0.513137, dtype=float32), Array(0.24614075, dtype=float32)), 'eval/episode_reward': (Array(-11029.03, dtype=float32), Array(4595.0503, dtype=float32)), 'eval/avg_episode_length': (Array(953.3828, dtype=float32), Array(210.20894, dtype=float32)), 'eval/epoch_eval_time': 3.872262477874756, 'eval/sps': 33055.61044256774}
I0727 02:45:14.707641 140120985872192 train.py:379] starting iteration 17 202.41938948631287
I0727 02:45:24.120219 140120985872192 train.py:394] {'eval/walltime': 84.51893711090088, 'training/sps': 44506.38387540605, 'training/walltime': 119.54406809806824, 'training/entropy_loss': Array(-0.02936879, dtype=float32), 'training/policy_loss': Array(0.02379218, dtype=float32), 'training/total_loss': Array(51.481895, dtype=float32), 'training/v_loss': Array(51.487473, dtype=float32), 'eval/episode_goal_distance': (Array(0.5514646, dtype=float32), Array(0.281088, dtype=float32)), 'eval/episode_reward': (Array(-10685.199, dtype=float32), Array(4783.1616, dtype=float32)), 'eval/avg_episode_length': (Array(914.5469, dtype=float32), Array(278.69272, dtype=float32)), 'eval/epoch_eval_time': 3.8869833946228027, 'eval/sps': 32930.42110163716}
I0727 02:45:24.122672 140120985872192 train.py:379] starting iteration 18 211.83442068099976
I0727 02:45:33.554801 140120985872192 train.py:394] {'eval/walltime': 88.42064881324768, 'training/sps': 44466.08887228947, 'training/walltime': 125.0709764957428, 'training/entropy_loss': Array(-0.0298754, dtype=float32), 'training/policy_loss': Array(0.0270215, dtype=float32), 'training/total_loss': Array(37.609146, dtype=float32), 'training/v_loss': Array(37.612003, dtype=float32), 'eval/episode_goal_distance': (Array(0.49713716, dtype=float32), Array(0.24394056, dtype=float32)), 'eval/episode_reward': (Array(-9657.154, dtype=float32), Array(4819.2, dtype=float32)), 'eval/avg_episode_length': (Array(883.5469, dtype=float32), Array(319.62897, dtype=float32)), 'eval/epoch_eval_time': 3.9017117023468018, 'eval/sps': 32806.1142813321}
I0727 02:45:33.557176 140120985872192 train.py:379] starting iteration 19 221.26892399787903
I0727 02:45:43.041696 140120985872192 train.py:394] {'eval/walltime': 92.36610317230225, 'training/sps': 44396.15901560717, 'training/walltime': 130.60659050941467, 'training/entropy_loss': Array(-0.02177208, dtype=float32), 'training/policy_loss': Array(0.0580868, dtype=float32), 'training/total_loss': Array(41.42598, dtype=float32), 'training/v_loss': Array(41.389664, dtype=float32), 'eval/episode_goal_distance': (Array(0.48673788, dtype=float32), Array(0.23044643, dtype=float32)), 'eval/episode_reward': (Array(-9879.961, dtype=float32), Array(4762.34, dtype=float32)), 'eval/avg_episode_length': (Array(906.8125, dtype=float32), Array(289.732, dtype=float32)), 'eval/epoch_eval_time': 3.9454543590545654, 'eval/sps': 32442.39784608031}
I0727 02:45:43.043967 140120985872192 train.py:379] starting iteration 20 230.7557156085968
I0727 02:45:52.544077 140120985872192 train.py:394] {'eval/walltime': 96.32513976097107, 'training/sps': 44380.79455315889, 'training/walltime': 136.14412093162537, 'training/entropy_loss': Array(-0.0256163, dtype=float32), 'training/policy_loss': Array(0.05702481, dtype=float32), 'training/total_loss': Array(42.50015, dtype=float32), 'training/v_loss': Array(42.468742, dtype=float32), 'eval/episode_goal_distance': (Array(0.49271572, dtype=float32), Array(0.23103087, dtype=float32)), 'eval/episode_reward': (Array(-10378.215, dtype=float32), Array(4955.4185, dtype=float32)), 'eval/avg_episode_length': (Array(930.0469, dtype=float32), Array(254.36646, dtype=float32)), 'eval/epoch_eval_time': 3.9590365886688232, 'eval/sps': 32331.09801671179}
I0727 02:45:52.546267 140120985872192 train.py:379] starting iteration 21 240.25801634788513
I0727 02:46:02.068152 140120985872192 train.py:394] {'eval/walltime': 100.3053343296051, 'training/sps': 44375.204207565956, 'training/walltime': 141.6823489665985, 'training/entropy_loss': Array(-0.03978117, dtype=float32), 'training/policy_loss': Array(0.01018363, dtype=float32), 'training/total_loss': Array(42.04109, dtype=float32), 'training/v_loss': Array(42.070686, dtype=float32), 'eval/episode_goal_distance': (Array(0.45357224, dtype=float32), Array(0.22250563, dtype=float32)), 'eval/episode_reward': (Array(-9127.129, dtype=float32), Array(4379.887, dtype=float32)), 'eval/avg_episode_length': (Array(898.96875, dtype=float32), Array(300.4923, dtype=float32)), 'eval/epoch_eval_time': 3.980194568634033, 'eval/sps': 32159.231864870475}
I0727 02:46:02.070571 140120985872192 train.py:379] starting iteration 22 249.78232049942017
I0727 02:46:11.628590 140120985872192 train.py:394] {'eval/walltime': 104.31798338890076, 'training/sps': 44346.53905022568, 'training/walltime': 147.22415685653687, 'training/entropy_loss': Array(-0.0396771, dtype=float32), 'training/policy_loss': Array(0.00696955, dtype=float32), 'training/total_loss': Array(42.087627, dtype=float32), 'training/v_loss': Array(42.12034, dtype=float32), 'eval/episode_goal_distance': (Array(0.4772333, dtype=float32), Array(0.25339085, dtype=float32)), 'eval/episode_reward': (Array(-9227.685, dtype=float32), Array(4935.3984, dtype=float32)), 'eval/avg_episode_length': (Array(883.46875, dtype=float32), Array(319.84286, dtype=float32)), 'eval/epoch_eval_time': 4.012649059295654, 'eval/sps': 31899.12651431023}
I0727 02:46:11.633385 140120985872192 train.py:379] starting iteration 23 259.34511828422546
I0727 02:46:21.236109 140120985872192 train.py:394] {'eval/walltime': 108.36202454566956, 'training/sps': 44243.49279368108, 'training/walltime': 152.77887201309204, 'training/entropy_loss': Array(-0.03942008, dtype=float32), 'training/policy_loss': Array(0.00787193, dtype=float32), 'training/total_loss': Array(45.47004, dtype=float32), 'training/v_loss': Array(45.501587, dtype=float32), 'eval/episode_goal_distance': (Array(0.5102592, dtype=float32), Array(0.24565277, dtype=float32)), 'eval/episode_reward': (Array(-9120.434, dtype=float32), Array(5707.3374, dtype=float32)), 'eval/avg_episode_length': (Array(813.53125, dtype=float32), Array(388.1663, dtype=float32)), 'eval/epoch_eval_time': 4.044041156768799, 'eval/sps': 31651.507746343606}
I0727 02:46:21.238649 140120985872192 train.py:379] starting iteration 24 268.9503984451294
I0727 02:46:30.826558 140120985872192 train.py:394] {'eval/walltime': 112.40439987182617, 'training/sps': 44345.21502925331, 'training/walltime': 158.3208453655243, 'training/entropy_loss': Array(-0.03851458, dtype=float32), 'training/policy_loss': Array(0.00775498, dtype=float32), 'training/total_loss': Array(42.938866, dtype=float32), 'training/v_loss': Array(42.969627, dtype=float32), 'eval/episode_goal_distance': (Array(0.4934366, dtype=float32), Array(0.24870643, dtype=float32)), 'eval/episode_reward': (Array(-9462.697, dtype=float32), Array(5132.594, dtype=float32)), 'eval/avg_episode_length': (Array(868.0703, dtype=float32), Array(337.11685, dtype=float32)), 'eval/epoch_eval_time': 4.042375326156616, 'eval/sps': 31664.55108009454}
I0727 02:46:30.829066 140120985872192 train.py:379] starting iteration 25 278.54081535339355
I0727 02:46:40.414115 140120985872192 train.py:394] {'eval/walltime': 116.44581127166748, 'training/sps': 44359.709158525024, 'training/walltime': 163.86100792884827, 'training/entropy_loss': Array(-0.03890052, dtype=float32), 'training/policy_loss': Array(0.0053225, dtype=float32), 'training/total_loss': Array(94.42516, dtype=float32), 'training/v_loss': Array(94.45873, dtype=float32), 'eval/episode_goal_distance': (Array(0.46437743, dtype=float32), Array(0.20934577, dtype=float32)), 'eval/episode_reward': (Array(-8718.946, dtype=float32), Array(4980.9204, dtype=float32)), 'eval/avg_episode_length': (Array(852.46875, dtype=float32), Array(353.36325, dtype=float32)), 'eval/epoch_eval_time': 4.041411399841309, 'eval/sps': 31672.103464899934}
I0727 02:46:40.416711 140120985872192 train.py:379] starting iteration 26 288.1284599304199
I0727 02:46:50.001078 140120985872192 train.py:394] {'eval/walltime': 120.48684477806091, 'training/sps': 44363.17999485955, 'training/walltime': 169.40073704719543, 'training/entropy_loss': Array(-0.03782744, dtype=float32), 'training/policy_loss': Array(0.0055948, dtype=float32), 'training/total_loss': Array(29.345177, dtype=float32), 'training/v_loss': Array(29.377409, dtype=float32), 'eval/episode_goal_distance': (Array(0.43539745, dtype=float32), Array(0.18157305, dtype=float32)), 'eval/episode_reward': (Array(-8814.242, dtype=float32), Array(4909.9917, dtype=float32)), 'eval/avg_episode_length': (Array(852.41406, dtype=float32), Array(353.49432, dtype=float32)), 'eval/epoch_eval_time': 4.041033506393433, 'eval/sps': 31675.06525186876}
I0727 02:46:50.003375 140120985872192 train.py:379] starting iteration 27 297.715124130249
I0727 02:46:59.593074 140120985872192 train.py:394] {'eval/walltime': 124.52702307701111, 'training/sps': 44313.79751372253, 'training/walltime': 174.94663953781128, 'training/entropy_loss': Array(-0.03813122, dtype=float32), 'training/policy_loss': Array(0.00472974, dtype=float32), 'training/total_loss': Array(27.553997, dtype=float32), 'training/v_loss': Array(27.5874, dtype=float32), 'eval/episode_goal_distance': (Array(0.45169622, dtype=float32), Array(0.19533646, dtype=float32)), 'eval/episode_reward': (Array(-8980.282, dtype=float32), Array(4985.6943, dtype=float32)), 'eval/avg_episode_length': (Array(844.65625, dtype=float32), Array(360.9869, dtype=float32)), 'eval/epoch_eval_time': 4.040178298950195, 'eval/sps': 31681.770092488165}
I0727 02:46:59.595488 140120985872192 train.py:379] starting iteration 28 307.30723690986633
I0727 02:47:09.201012 140120985872192 train.py:394] {'eval/walltime': 128.58392119407654, 'training/sps': 44319.80876811405, 'training/walltime': 180.49178981781006, 'training/entropy_loss': Array(-0.03775474, dtype=float32), 'training/policy_loss': Array(0.00426123, dtype=float32), 'training/total_loss': Array(29.191938, dtype=float32), 'training/v_loss': Array(29.225431, dtype=float32), 'eval/episode_goal_distance': (Array(0.45251983, dtype=float32), Array(0.19132988, dtype=float32)), 'eval/episode_reward': (Array(-8991.934, dtype=float32), Array(4957.291, dtype=float32)), 'eval/avg_episode_length': (Array(852.4844, dtype=float32), Array(353.32605, dtype=float32)), 'eval/epoch_eval_time': 4.05689811706543, 'eval/sps': 31551.199045784568}
I0727 02:47:09.203356 140120985872192 train.py:379] starting iteration 29 316.9151039123535
I0727 02:47:18.809839 140120985872192 train.py:394] {'eval/walltime': 132.64888262748718, 'training/sps': 44376.58160215058, 'training/walltime': 186.02984595298767, 'training/entropy_loss': Array(-0.03755194, dtype=float32), 'training/policy_loss': Array(0.01074791, dtype=float32), 'training/total_loss': Array(29.914347, dtype=float32), 'training/v_loss': Array(29.94115, dtype=float32), 'eval/episode_goal_distance': (Array(0.4337118, dtype=float32), Array(0.17829053, dtype=float32)), 'eval/episode_reward': (Array(-8497.216, dtype=float32), Array(4714.9414, dtype=float32)), 'eval/avg_episode_length': (Array(836.8281, dtype=float32), Array(368.3225, dtype=float32)), 'eval/epoch_eval_time': 4.0649614334106445, 'eval/sps': 31488.613630610398}
I0727 02:47:18.812369 140120985872192 train.py:379] starting iteration 30 326.52411794662476
I0727 02:47:28.460296 140120985872192 train.py:394] {'eval/walltime': 136.7445936203003, 'training/sps': 44291.721344170306, 'training/walltime': 191.57851266860962, 'training/entropy_loss': Array(-0.03815446, dtype=float32), 'training/policy_loss': Array(0.00811564, dtype=float32), 'training/total_loss': Array(31.150965, dtype=float32), 'training/v_loss': Array(31.181004, dtype=float32), 'eval/episode_goal_distance': (Array(0.4405172, dtype=float32), Array(0.20893201, dtype=float32)), 'eval/episode_reward': (Array(-9129.234, dtype=float32), Array(5066.585, dtype=float32)), 'eval/avg_episode_length': (Array(860.21094, dtype=float32), Array(345.56845, dtype=float32)), 'eval/epoch_eval_time': 4.09571099281311, 'eval/sps': 31252.20510543985}
I0727 02:47:28.462899 140120985872192 train.py:379] starting iteration 31 336.1746475696564
I0727 02:47:38.111531 140120985872192 train.py:394] {'eval/walltime': 140.84347248077393, 'training/sps': 44310.19344848082, 'training/walltime': 197.12486624717712, 'training/entropy_loss': Array(-0.04014438, dtype=float32), 'training/policy_loss': Array(0.00356122, dtype=float32), 'training/total_loss': Array(30.43679, dtype=float32), 'training/v_loss': Array(30.473373, dtype=float32), 'eval/episode_goal_distance': (Array(0.45588005, dtype=float32), Array(0.21509849, dtype=float32)), 'eval/episode_reward': (Array(-9062.54, dtype=float32), Array(5081.2593, dtype=float32)), 'eval/avg_episode_length': (Array(852.375, dtype=float32), Array(353.58798, dtype=float32)), 'eval/epoch_eval_time': 4.098878860473633, 'eval/sps': 31228.051464104352}
I0727 02:47:38.113930 140120985872192 train.py:379] starting iteration 32 345.8256788253784
I0727 02:47:47.778926 140120985872192 train.py:394] {'eval/walltime': 144.95821857452393, 'training/sps': 44307.829789582625, 'training/walltime': 202.6715157032013, 'training/entropy_loss': Array(-0.04070094, dtype=float32), 'training/policy_loss': Array(0.00102611, dtype=float32), 'training/total_loss': Array(30.823223, dtype=float32), 'training/v_loss': Array(30.862898, dtype=float32), 'eval/episode_goal_distance': (Array(0.4070071, dtype=float32), Array(0.16095911, dtype=float32)), 'eval/episode_reward': (Array(-8437.515, dtype=float32), Array(4168.0903, dtype=float32)), 'eval/avg_episode_length': (Array(883.5547, dtype=float32), Array(319.6072, dtype=float32)), 'eval/epoch_eval_time': 4.11474609375, 'eval/sps': 31107.630236145724}
I0727 02:47:47.781374 140120985872192 train.py:379] starting iteration 33 355.49312353134155
I0727 02:47:57.435512 140120985872192 train.py:394] {'eval/walltime': 149.06570649147034, 'training/sps': 44335.56580062263, 'training/walltime': 208.21469521522522, 'training/entropy_loss': Array(-0.03943004, dtype=float32), 'training/policy_loss': Array(0.00037534, dtype=float32), 'training/total_loss': Array(80.82027, dtype=float32), 'training/v_loss': Array(80.85932, dtype=float32), 'eval/episode_goal_distance': (Array(0.4266011, dtype=float32), Array(0.20344336, dtype=float32)), 'eval/episode_reward': (Array(-8296.619, dtype=float32), Array(5191.5317, dtype=float32)), 'eval/avg_episode_length': (Array(798.1094, dtype=float32), Array(399.88058, dtype=float32)), 'eval/epoch_eval_time': 4.107487916946411, 'eval/sps': 31162.599279210484}
I0727 02:47:57.437773 140120985872192 train.py:379] starting iteration 34 365.14952182769775
I0727 02:48:07.080575 140120985872192 train.py:394] {'eval/walltime': 153.16540932655334, 'training/sps': 44364.77431788533, 'training/walltime': 213.75422525405884, 'training/entropy_loss': Array(-0.03963698, dtype=float32), 'training/policy_loss': Array(0.0003459, dtype=float32), 'training/total_loss': Array(25.776937, dtype=float32), 'training/v_loss': Array(25.816229, dtype=float32), 'eval/episode_goal_distance': (Array(0.3978627, dtype=float32), Array(0.16459447, dtype=float32)), 'eval/episode_reward': (Array(-8011.9336, dtype=float32), Array(4283.7417, dtype=float32)), 'eval/avg_episode_length': (Array(852.5547, dtype=float32), Array(353.1579, dtype=float32)), 'eval/epoch_eval_time': 4.099702835083008, 'eval/sps': 31221.775125905766}
I0727 02:48:07.082997 140120985872192 train.py:379] starting iteration 35 374.7947447299957
I0727 02:48:16.717967 140120985872192 train.py:394] {'eval/walltime': 157.26179575920105, 'training/sps': 44401.41802358083, 'training/walltime': 219.28918361663818, 'training/entropy_loss': Array(-0.0389307, dtype=float32), 'training/policy_loss': Array(0.00035111, dtype=float32), 'training/total_loss': Array(21.9059, dtype=float32), 'training/v_loss': Array(21.944479, dtype=float32), 'eval/episode_goal_distance': (Array(0.39884773, dtype=float32), Array(0.16332752, dtype=float32)), 'eval/episode_reward': (Array(-8585.954, dtype=float32), Array(4159.212, dtype=float32)), 'eval/avg_episode_length': (Array(883.4375, dtype=float32), Array(319.92844, dtype=float32)), 'eval/epoch_eval_time': 4.096386432647705, 'eval/sps': 31247.05203099382}
I0727 02:48:16.720470 140120985872192 train.py:379] starting iteration 36 384.43221950531006
I0727 02:48:26.389492 140120985872192 train.py:394] {'eval/walltime': 161.36974453926086, 'training/sps': 44222.082304494266, 'training/walltime': 224.84658813476562, 'training/entropy_loss': Array(-0.03921329, dtype=float32), 'training/policy_loss': Array(0.00028474, dtype=float32), 'training/total_loss': Array(20.949984, dtype=float32), 'training/v_loss': Array(20.988913, dtype=float32), 'eval/episode_goal_distance': (Array(0.4418512, dtype=float32), Array(0.18174706, dtype=float32)), 'eval/episode_reward': (Array(-8789.842, dtype=float32), Array(4988.2124, dtype=float32)), 'eval/avg_episode_length': (Array(844.66406, dtype=float32), Array(360.96878, dtype=float32)), 'eval/epoch_eval_time': 4.1079487800598145, 'eval/sps': 31159.103205307307}
I0727 02:48:26.392241 140120985872192 train.py:379] starting iteration 37 394.1039900779724
I0727 02:48:36.070899 140120985872192 train.py:394] {'eval/walltime': 165.46790385246277, 'training/sps': 44068.5665191604, 'training/walltime': 230.4233522415161, 'training/entropy_loss': Array(-0.04005161, dtype=float32), 'training/policy_loss': Array(0.00041336, dtype=float32), 'training/total_loss': Array(20.840425, dtype=float32), 'training/v_loss': Array(20.880066, dtype=float32), 'eval/episode_goal_distance': (Array(0.43998152, dtype=float32), Array(0.17998068, dtype=float32)), 'eval/episode_reward': (Array(-8782.264, dtype=float32), Array(4741.1616, dtype=float32)), 'eval/avg_episode_length': (Array(852.4453, dtype=float32), Array(353.41928, dtype=float32)), 'eval/epoch_eval_time': 4.098159313201904, 'eval/sps': 31233.53442792179}
I0727 02:48:36.073423 140120985872192 train.py:379] starting iteration 38 403.7851719856262
I0727 02:48:45.775564 140120985872192 train.py:394] {'eval/walltime': 169.57066750526428, 'training/sps': 43918.925210332476, 'training/walltime': 236.01911759376526, 'training/entropy_loss': Array(-0.04033488, dtype=float32), 'training/policy_loss': Array(0.00029498, dtype=float32), 'training/total_loss': Array(21.326061, dtype=float32), 'training/v_loss': Array(21.3661, dtype=float32), 'eval/episode_goal_distance': (Array(0.41839406, dtype=float32), Array(0.15902345, dtype=float32)), 'eval/episode_reward': (Array(-8277.11, dtype=float32), Array(4270.544, dtype=float32)), 'eval/avg_episode_length': (Array(860.2031, dtype=float32), Array(345.58813, dtype=float32)), 'eval/epoch_eval_time': 4.102763652801514, 'eval/sps': 31198.48249425653}
I0727 02:48:45.778003 140120985872192 train.py:379] starting iteration 39 413.48975229263306
I0727 02:48:55.502745 140120985872192 train.py:394] {'eval/walltime': 173.67688417434692, 'training/sps': 43769.0998095087, 'training/walltime': 241.634037733078, 'training/entropy_loss': Array(-0.04148451, dtype=float32), 'training/policy_loss': Array(0.00039312, dtype=float32), 'training/total_loss': Array(22.929138, dtype=float32), 'training/v_loss': Array(22.97023, dtype=float32), 'eval/episode_goal_distance': (Array(0.44341743, dtype=float32), Array(0.18231432, dtype=float32)), 'eval/episode_reward': (Array(-8625.481, dtype=float32), Array(5091.305, dtype=float32)), 'eval/avg_episode_length': (Array(821.41406, dtype=float32), Array(381.574, dtype=float32)), 'eval/epoch_eval_time': 4.106216669082642, 'eval/sps': 31172.246940538607}
I0727 02:48:55.505319 140120985872192 train.py:379] starting iteration 40 423.21706771850586
I0727 02:49:05.218612 140120985872192 train.py:394] {'eval/walltime': 177.77033615112305, 'training/sps': 43759.01973275975, 'training/walltime': 247.25025129318237, 'training/entropy_loss': Array(-0.04039589, dtype=float32), 'training/policy_loss': Array(-7.133927e-05, dtype=float32), 'training/total_loss': Array(21.993906, dtype=float32), 'training/v_loss': Array(22.034374, dtype=float32), 'eval/episode_goal_distance': (Array(0.43792498, dtype=float32), Array(0.19631806, dtype=float32)), 'eval/episode_reward': (Array(-9145.676, dtype=float32), Array(4781.8457, dtype=float32)), 'eval/avg_episode_length': (Array(875.77344, dtype=float32), Array(328.67325, dtype=float32)), 'eval/epoch_eval_time': 4.093451976776123, 'eval/sps': 31269.451975056236}
I0727 02:49:05.221156 140120985872192 train.py:379] starting iteration 41 432.93290519714355
I0727 02:49:14.953131 140120985872192 train.py:394] {'eval/walltime': 181.88725662231445, 'training/sps': 43796.427616993504, 'training/walltime': 252.86166787147522, 'training/entropy_loss': Array(-0.0403522, dtype=float32), 'training/policy_loss': Array(0.00015734, dtype=float32), 'training/total_loss': Array(64.406265, dtype=float32), 'training/v_loss': Array(64.446465, dtype=float32), 'eval/episode_goal_distance': (Array(0.4759429, dtype=float32), Array(0.21206045, dtype=float32)), 'eval/episode_reward': (Array(-9569.211, dtype=float32), Array(4844.9873, dtype=float32)), 'eval/avg_episode_length': (Array(883.4219, dtype=float32), Array(319.97144, dtype=float32)), 'eval/epoch_eval_time': 4.116920471191406, 'eval/sps': 31091.200545576183}
I0727 02:49:14.955543 140120985872192 train.py:379] starting iteration 42 442.6672921180725
I0727 02:49:24.715991 140120985872192 train.py:394] {'eval/walltime': 185.9891767501831, 'training/sps': 43458.026850586975, 'training/walltime': 258.5167796611786, 'training/entropy_loss': Array(-0.03927169, dtype=float32), 'training/policy_loss': Array(-5.094678e-05, dtype=float32), 'training/total_loss': Array(32.22206, dtype=float32), 'training/v_loss': Array(32.261387, dtype=float32), 'eval/episode_goal_distance': (Array(0.45092773, dtype=float32), Array(0.21579508, dtype=float32)), 'eval/episode_reward': (Array(-8560.475, dtype=float32), Array(5295.762, dtype=float32)), 'eval/avg_episode_length': (Array(821.25, dtype=float32), Array(381.9247, dtype=float32)), 'eval/epoch_eval_time': 4.101920127868652, 'eval/sps': 31204.89819642307}
I0727 02:49:24.718383 140120985872192 train.py:379] starting iteration 43 452.43013167381287
I0727 02:49:34.430891 140120985872192 train.py:394] {'eval/walltime': 190.07056403160095, 'training/sps': 43671.53985844264, 'training/walltime': 264.14424324035645, 'training/entropy_loss': Array(-0.03987183, dtype=float32), 'training/policy_loss': Array(-0.00010013, dtype=float32), 'training/total_loss': Array(22.261084, dtype=float32), 'training/v_loss': Array(22.301056, dtype=float32), 'eval/episode_goal_distance': (Array(0.4885659, dtype=float32), Array(0.21531764, dtype=float32)), 'eval/episode_reward': (Array(-9223.123, dtype=float32), Array(5372.1367, dtype=float32)), 'eval/avg_episode_length': (Array(836.8906, dtype=float32), Array(368.18158, dtype=float32)), 'eval/epoch_eval_time': 4.081387281417847, 'eval/sps': 31361.885352750367}
I0727 02:49:34.433383 140120985872192 train.py:379] starting iteration 44 462.1451327800751
I0727 02:49:44.167229 140120985872192 train.py:394] {'eval/walltime': 194.1645667552948, 'training/sps': 43602.90073339977, 'training/walltime': 269.7805655002594, 'training/entropy_loss': Array(-0.03871225, dtype=float32), 'training/policy_loss': Array(-0.00064783, dtype=float32), 'training/total_loss': Array(19.566235, dtype=float32), 'training/v_loss': Array(19.605595, dtype=float32), 'eval/episode_goal_distance': (Array(0.41954952, dtype=float32), Array(0.18889637, dtype=float32)), 'eval/episode_reward': (Array(-8469.721, dtype=float32), Array(4484.9165, dtype=float32)), 'eval/avg_episode_length': (Array(852.4375, dtype=float32), Array(353.43805, dtype=float32)), 'eval/epoch_eval_time': 4.094002723693848, 'eval/sps': 31265.24544285377}
I0727 02:49:44.169768 140120985872192 train.py:379] starting iteration 45 471.8815174102783
I0727 02:49:53.895709 140120985872192 train.py:394] {'eval/walltime': 198.25848078727722, 'training/sps': 43664.47681971759, 'training/walltime': 275.40893936157227, 'training/entropy_loss': Array(-0.03790914, dtype=float32), 'training/policy_loss': Array(-0.00044589, dtype=float32), 'training/total_loss': Array(19.7585, dtype=float32), 'training/v_loss': Array(19.796854, dtype=float32), 'eval/episode_goal_distance': (Array(0.43247378, dtype=float32), Array(0.18278198, dtype=float32)), 'eval/episode_reward': (Array(-7620.8184, dtype=float32), Array(5178.693, dtype=float32)), 'eval/avg_episode_length': (Array(767.03906, dtype=float32), Array(421.0526, dtype=float32)), 'eval/epoch_eval_time': 4.093914031982422, 'eval/sps': 31265.92278197345}
I0727 02:49:53.898270 140120985872192 train.py:379] starting iteration 46 481.6100187301636
I0727 02:50:03.688192 140120985872192 train.py:394] {'eval/walltime': 202.36957693099976, 'training/sps': 43304.1244358616, 'training/walltime': 281.08414936065674, 'training/entropy_loss': Array(-0.03477422, dtype=float32), 'training/policy_loss': Array(-0.00023071, dtype=float32), 'training/total_loss': Array(18.4346, dtype=float32), 'training/v_loss': Array(18.469604, dtype=float32), 'eval/episode_goal_distance': (Array(0.43807104, dtype=float32), Array(0.16901384, dtype=float32)), 'eval/episode_reward': (Array(-8893.396, dtype=float32), Array(4467.549, dtype=float32)), 'eval/avg_episode_length': (Array(875.7344, dtype=float32), Array(328.77655, dtype=float32)), 'eval/epoch_eval_time': 4.111096143722534, 'eval/sps': 31135.248489736845}
I0727 02:50:03.690751 140120985872192 train.py:379] starting iteration 47 491.4025001525879
I0727 02:50:13.442314 140120985872192 train.py:394] {'eval/walltime': 206.45960235595703, 'training/sps': 43435.34161262982, 'training/walltime': 286.742214679718, 'training/entropy_loss': Array(-0.03111695, dtype=float32), 'training/policy_loss': Array(0.00027086, dtype=float32), 'training/total_loss': Array(18.268204, dtype=float32), 'training/v_loss': Array(18.29905, dtype=float32), 'eval/episode_goal_distance': (Array(0.41021574, dtype=float32), Array(0.14796768, dtype=float32)), 'eval/episode_reward': (Array(-8687.66, dtype=float32), Array(3942.7925, dtype=float32)), 'eval/avg_episode_length': (Array(906.77344, dtype=float32), Array(289.85364, dtype=float32)), 'eval/epoch_eval_time': 4.090025424957275, 'eval/sps': 31295.648975418557}
I0727 02:50:13.444674 140120985872192 train.py:379] starting iteration 48 501.15642285346985
I0727 02:50:23.226785 140120985872192 train.py:394] {'eval/walltime': 210.55515336990356, 'training/sps': 43250.24050639922, 'training/walltime': 292.4244952201843, 'training/entropy_loss': Array(-0.02799325, dtype=float32), 'training/policy_loss': Array(0.0005551, dtype=float32), 'training/total_loss': Array(16.270786, dtype=float32), 'training/v_loss': Array(16.298225, dtype=float32), 'eval/episode_goal_distance': (Array(0.36593366, dtype=float32), Array(0.12856679, dtype=float32)), 'eval/episode_reward': (Array(-7780.6094, dtype=float32), Array(3913.3628, dtype=float32)), 'eval/avg_episode_length': (Array(875.84375, dtype=float32), Array(328.4872, dtype=float32)), 'eval/epoch_eval_time': 4.095551013946533, 'eval/sps': 31253.42586726989}
I0727 02:50:23.229165 140120985872192 train.py:379] starting iteration 49 510.9409146308899
I0727 02:50:33.002367 140120985872192 train.py:394] {'eval/walltime': 214.6530146598816, 'training/sps': 43330.28619392746, 'training/walltime': 298.09627866744995, 'training/entropy_loss': Array(-0.02474186, dtype=float32), 'training/policy_loss': Array(0.00055882, dtype=float32), 'training/total_loss': Array(15.185625, dtype=float32), 'training/v_loss': Array(15.209806, dtype=float32), 'eval/episode_goal_distance': (Array(0.36612105, dtype=float32), Array(0.1287319, dtype=float32)), 'eval/episode_reward': (Array(-8098.63, dtype=float32), Array(4067.8564, dtype=float32)), 'eval/avg_episode_length': (Array(891.2344, dtype=float32), Array(310.3707, dtype=float32)), 'eval/epoch_eval_time': 4.097861289978027, 'eval/sps': 31235.80593443814}
I0727 02:50:33.004704 140120985872192 train.py:379] starting iteration 50 520.7164533138275
I0727 02:50:42.790427 140120985872192 train.py:394] {'eval/walltime': 218.75825333595276, 'training/sps': 43291.609937272464, 'training/walltime': 303.7731292247772, 'training/entropy_loss': Array(-0.02911392, dtype=float32), 'training/policy_loss': Array(0.0011307, dtype=float32), 'training/total_loss': Array(78.230225, dtype=float32), 'training/v_loss': Array(78.25821, dtype=float32), 'eval/episode_goal_distance': (Array(0.38191196, dtype=float32), Array(0.12980196, dtype=float32)), 'eval/episode_reward': (Array(-7615.7153, dtype=float32), Array(4430.6333, dtype=float32)), 'eval/avg_episode_length': (Array(821.35156, dtype=float32), Array(381.70734, dtype=float32)), 'eval/epoch_eval_time': 4.105238676071167, 'eval/sps': 31179.673120126532}
I0727 02:50:42.792874 140120985872192 train.py:379] starting iteration 51 530.5046229362488
I0727 02:50:52.568654 140120985872192 train.py:394] {'eval/walltime': 222.84504318237305, 'training/sps': 43226.65676879958, 'training/walltime': 309.4585099220276, 'training/entropy_loss': Array(-0.02825875, dtype=float32), 'training/policy_loss': Array(0.00109492, dtype=float32), 'training/total_loss': Array(18.430803, dtype=float32), 'training/v_loss': Array(18.457966, dtype=float32), 'eval/episode_goal_distance': (Array(0.3467519, dtype=float32), Array(0.10567646, dtype=float32)), 'eval/episode_reward': (Array(-6454.201, dtype=float32), Array(4593.3784, dtype=float32)), 'eval/avg_episode_length': (Array(720.34375, dtype=float32), Array(447.06226, dtype=float32)), 'eval/epoch_eval_time': 4.086789846420288, 'eval/sps': 31320.42625390148}
I0727 02:50:52.615454 140120985872192 train.py:379] starting iteration 52 540.3272013664246
I0727 02:51:02.419831 140120985872192 train.py:394] {'eval/walltime': 226.9431507587433, 'training/sps': 43095.11358144819, 'training/walltime': 315.1612446308136, 'training/entropy_loss': Array(-0.02362196, dtype=float32), 'training/policy_loss': Array(0.00137223, dtype=float32), 'training/total_loss': Array(14.073109, dtype=float32), 'training/v_loss': Array(14.095358, dtype=float32), 'eval/episode_goal_distance': (Array(0.33026376, dtype=float32), Array(0.12101483, dtype=float32)), 'eval/episode_reward': (Array(-7262.8066, dtype=float32), Array(4014.9192, dtype=float32)), 'eval/avg_episode_length': (Array(852.39844, dtype=float32), Array(353.53174, dtype=float32)), 'eval/epoch_eval_time': 4.098107576370239, 'eval/sps': 31233.928737754533}
I0727 02:51:02.422497 140120985872192 train.py:379] starting iteration 53 550.1342458724976
I0727 02:51:12.237896 140120985872192 train.py:394] {'eval/walltime': 231.04073023796082, 'training/sps': 43008.226868872516, 'training/walltime': 320.87550020217896, 'training/entropy_loss': Array(-0.01969508, dtype=float32), 'training/policy_loss': Array(0.00154902, dtype=float32), 'training/total_loss': Array(11.087805, dtype=float32), 'training/v_loss': Array(11.105951, dtype=float32), 'eval/episode_goal_distance': (Array(0.3152827, dtype=float32), Array(0.12355, dtype=float32)), 'eval/episode_reward': (Array(-7040.6772, dtype=float32), Array(4331.256, dtype=float32)), 'eval/avg_episode_length': (Array(829.15625, dtype=float32), Array(375.00916, dtype=float32)), 'eval/epoch_eval_time': 4.097579479217529, 'eval/sps': 31237.95417494691}
I0727 02:51:12.240413 140120985872192 train.py:379] starting iteration 54 559.9521615505219
I0727 02:51:22.061078 140120985872192 train.py:394] {'eval/walltime': 235.13845300674438, 'training/sps': 42969.54279274641, 'training/walltime': 326.5949001312256, 'training/entropy_loss': Array(-0.01528673, dtype=float32), 'training/policy_loss': Array(0.002295, dtype=float32), 'training/total_loss': Array(9.770731, dtype=float32), 'training/v_loss': Array(9.783723, dtype=float32), 'eval/episode_goal_distance': (Array(0.32409322, dtype=float32), Array(0.12386061, dtype=float32)), 'eval/episode_reward': (Array(-7270.2715, dtype=float32), Array(4249.516, dtype=float32)), 'eval/avg_episode_length': (Array(844.6328, dtype=float32), Array(361.04184, dtype=float32)), 'eval/epoch_eval_time': 4.097722768783569, 'eval/sps': 31236.86184314452}
I0727 02:51:22.063646 140120985872192 train.py:379] starting iteration 55 569.7753946781158
I0727 02:51:31.849440 140120985872192 train.py:394] {'eval/walltime': 239.22732996940613, 'training/sps': 43168.123450047155, 'training/walltime': 332.2879898548126, 'training/entropy_loss': Array(-0.01284732, dtype=float32), 'training/policy_loss': Array(0.00207847, dtype=float32), 'training/total_loss': Array(9.327911, dtype=float32), 'training/v_loss': Array(9.33868, dtype=float32), 'eval/episode_goal_distance': (Array(0.3465644, dtype=float32), Array(0.11926856, dtype=float32)), 'eval/episode_reward': (Array(-8056.678, dtype=float32), Array(4109.434, dtype=float32)), 'eval/avg_episode_length': (Array(883.47656, dtype=float32), Array(319.82135, dtype=float32)), 'eval/epoch_eval_time': 4.088876962661743, 'eval/sps': 31304.439133007225}
I0727 02:51:31.851878 140120985872192 train.py:379] starting iteration 56 579.563627243042
I0727 02:51:41.662655 140120985872192 train.py:394] {'eval/walltime': 243.32085943222046, 'training/sps': 43013.393718344254, 'training/walltime': 338.00155901908875, 'training/entropy_loss': Array(-0.01047327, dtype=float32), 'training/policy_loss': Array(0.00236499, dtype=float32), 'training/total_loss': Array(8.839028, dtype=float32), 'training/v_loss': Array(8.8471365, dtype=float32), 'eval/episode_goal_distance': (Array(0.32683486, dtype=float32), Array(0.11016348, dtype=float32)), 'eval/episode_reward': (Array(-7290.459, dtype=float32), Array(4113.0938, dtype=float32)), 'eval/avg_episode_length': (Array(829.125, dtype=float32), Array(375.07767, dtype=float32)), 'eval/epoch_eval_time': 4.093529462814331, 'eval/sps': 31268.860078510115}
I0727 02:51:41.665131 140120985872192 train.py:379] starting iteration 57 589.3768799304962
I0727 02:51:51.464617 140120985872192 train.py:394] {'eval/walltime': 247.40847158432007, 'training/sps': 43053.05590436769, 'training/walltime': 343.70986461639404, 'training/entropy_loss': Array(-0.00595138, dtype=float32), 'training/policy_loss': Array(0.00290078, dtype=float32), 'training/total_loss': Array(8.411355, dtype=float32), 'training/v_loss': Array(8.414406, dtype=float32), 'eval/episode_goal_distance': (Array(0.31653526, dtype=float32), Array(0.10163952, dtype=float32)), 'eval/episode_reward': (Array(-7179.4854, dtype=float32), Array(3968.5332, dtype=float32)), 'eval/avg_episode_length': (Array(836.8125, dtype=float32), Array(368.35776, dtype=float32)), 'eval/epoch_eval_time': 4.087612152099609, 'eval/sps': 31314.125518061337}
I0727 02:51:51.467239 140120985872192 train.py:379] starting iteration 58 599.1789877414703
I0727 02:52:01.283120 140120985872192 train.py:394] {'eval/walltime': 251.48692321777344, 'training/sps': 42861.05237657987, 'training/walltime': 349.4437415599823, 'training/entropy_loss': Array(-0.01116564, dtype=float32), 'training/policy_loss': Array(0.00527416, dtype=float32), 'training/total_loss': Array(65.97537, dtype=float32), 'training/v_loss': Array(65.98126, dtype=float32), 'eval/episode_goal_distance': (Array(0.34131718, dtype=float32), Array(0.134248, dtype=float32)), 'eval/episode_reward': (Array(-8033.9023, dtype=float32), Array(4452.529, dtype=float32)), 'eval/avg_episode_length': (Array(875.7578, dtype=float32), Array(328.7148, dtype=float32)), 'eval/epoch_eval_time': 4.078451633453369, 'eval/sps': 31384.459472336042}
I0727 02:52:01.285751 140120985872192 train.py:379] starting iteration 59 608.9974999427795
I0727 02:52:11.069786 140120985872192 train.py:394] {'eval/walltime': 255.55647206306458, 'training/sps': 43033.05488296761, 'training/walltime': 355.15470027923584, 'training/entropy_loss': Array(-0.00937857, dtype=float32), 'training/policy_loss': Array(0.00725906, dtype=float32), 'training/total_loss': Array(19.483646, dtype=float32), 'training/v_loss': Array(19.485767, dtype=float32), 'eval/episode_goal_distance': (Array(0.33081448, dtype=float32), Array(0.09926869, dtype=float32)), 'eval/episode_reward': (Array(-7325.4766, dtype=float32), Array(4073.1895, dtype=float32)), 'eval/avg_episode_length': (Array(829.1953, dtype=float32), Array(374.92334, dtype=float32)), 'eval/epoch_eval_time': 4.069548845291138, 'eval/sps': 31453.117990734623}
I0727 02:52:11.072291 140120985872192 train.py:379] starting iteration 60 618.7840404510498
I0727 02:52:20.951135 140120985872192 train.py:394] {'eval/walltime': 259.6940989494324, 'training/sps': 42833.13470874119, 'training/walltime': 360.8923144340515, 'training/entropy_loss': Array(-0.00611127, dtype=float32), 'training/policy_loss': Array(0.00418283, dtype=float32), 'training/total_loss': Array(12.597214, dtype=float32), 'training/v_loss': Array(12.599142, dtype=float32), 'eval/episode_goal_distance': (Array(0.33710545, dtype=float32), Array(0.11162763, dtype=float32)), 'eval/episode_reward': (Array(-6978.4727, dtype=float32), Array(4364.1895, dtype=float32)), 'eval/avg_episode_length': (Array(790.2969, dtype=float32), Array(405.58768, dtype=float32)), 'eval/epoch_eval_time': 4.137626886367798, 'eval/sps': 30935.607176596917}
I0727 02:52:20.953709 140120985872192 train.py:379] starting iteration 61 628.6654579639435
I0727 02:52:30.784860 140120985872192 train.py:394] {'eval/walltime': 263.7787673473358, 'training/sps': 42794.109240559024, 'training/walltime': 366.63516092300415, 'training/entropy_loss': Array(-0.00378213, dtype=float32), 'training/policy_loss': Array(0.00450261, dtype=float32), 'training/total_loss': Array(10.878448, dtype=float32), 'training/v_loss': Array(10.8777275, dtype=float32), 'eval/episode_goal_distance': (Array(0.33258036, dtype=float32), Array(0.14606845, dtype=float32)), 'eval/episode_reward': (Array(-7303.6216, dtype=float32), Array(4881.5796, dtype=float32)), 'eval/avg_episode_length': (Array(813.6406, dtype=float32), Array(387.93878, dtype=float32)), 'eval/epoch_eval_time': 4.084668397903442, 'eval/sps': 31336.6930999097}
I0727 02:52:30.787358 140120985872192 train.py:379] starting iteration 62 638.4991066455841
I0727 02:52:40.629667 140120985872192 train.py:394] {'eval/walltime': 267.8679690361023, 'training/sps': 42744.221080368, 'training/walltime': 372.38471007347107, 'training/entropy_loss': Array(-0.00011689, dtype=float32), 'training/policy_loss': Array(0.00554193, dtype=float32), 'training/total_loss': Array(9.11871, dtype=float32), 'training/v_loss': Array(9.113284, dtype=float32), 'eval/episode_goal_distance': (Array(0.32078898, dtype=float32), Array(0.1237956, dtype=float32)), 'eval/episode_reward': (Array(-7308.3506, dtype=float32), Array(4402.3774, dtype=float32)), 'eval/avg_episode_length': (Array(836.8125, dtype=float32), Array(368.35788, dtype=float32)), 'eval/epoch_eval_time': 4.0892016887664795, 'eval/sps': 31301.953227602135}
I0727 02:52:40.632088 140120985872192 train.py:379] starting iteration 63 648.3438370227814
I0727 02:52:50.425045 140120985872192 train.py:394] {'eval/walltime': 271.9481270313263, 'training/sps': 43045.549747804944, 'training/walltime': 378.0940110683441, 'training/entropy_loss': Array(0.00250789, dtype=float32), 'training/policy_loss': Array(0.0060559, dtype=float32), 'training/total_loss': Array(8.980407, dtype=float32), 'training/v_loss': Array(8.971844, dtype=float32), 'eval/episode_goal_distance': (Array(0.3141773, dtype=float32), Array(0.11802347, dtype=float32)), 'eval/episode_reward': (Array(-6711.619, dtype=float32), Array(4269.093, dtype=float32)), 'eval/avg_episode_length': (Array(813.5625, dtype=float32), Array(388.10156, dtype=float32)), 'eval/epoch_eval_time': 4.080157995223999, 'eval/sps': 31371.334186036307}
I0727 02:52:50.427539 140120985872192 train.py:379] starting iteration 64 658.139288187027
I0727 02:53:00.261209 140120985872192 train.py:394] {'eval/walltime': 276.0724160671234, 'training/sps': 43071.25053782019, 'training/walltime': 383.7999053001404, 'training/entropy_loss': Array(0.00366076, dtype=float32), 'training/policy_loss': Array(0.00775044, dtype=float32), 'training/total_loss': Array(8.490998, dtype=float32), 'training/v_loss': Array(8.479588, dtype=float32), 'eval/episode_goal_distance': (Array(0.34052402, dtype=float32), Array(0.1552974, dtype=float32)), 'eval/episode_reward': (Array(-7314.3496, dtype=float32), Array(5003.867, dtype=float32)), 'eval/avg_episode_length': (Array(790.2031, dtype=float32), Array(405.76913, dtype=float32)), 'eval/epoch_eval_time': 4.124289035797119, 'eval/sps': 31035.652178839326}
I0727 02:53:00.263736 140120985872192 train.py:379] starting iteration 65 667.975485086441
I0727 02:53:10.075918 140120985872192 train.py:394] {'eval/walltime': 280.14784026145935, 'training/sps': 42865.770358817674, 'training/walltime': 389.53315114974976, 'training/entropy_loss': Array(0.00395874, dtype=float32), 'training/policy_loss': Array(0.00926147, dtype=float32), 'training/total_loss': Array(8.629961, dtype=float32), 'training/v_loss': Array(8.616741, dtype=float32), 'eval/episode_goal_distance': (Array(0.32387555, dtype=float32), Array(0.14276782, dtype=float32)), 'eval/episode_reward': (Array(-6892.3228, dtype=float32), Array(4531.0796, dtype=float32)), 'eval/avg_episode_length': (Array(805.66406, dtype=float32), Array(394.45944, dtype=float32)), 'eval/epoch_eval_time': 4.0754241943359375, 'eval/sps': 31407.773497025315}
I0727 02:53:10.078433 140120985872192 train.py:379] starting iteration 66 677.7901821136475
I0727 02:53:19.890784 140120985872192 train.py:394] {'eval/walltime': 284.226092338562, 'training/sps': 42886.25847912886, 'training/walltime': 395.2636580467224, 'training/entropy_loss': Array(-0.00298576, dtype=float32), 'training/policy_loss': Array(0.01377775, dtype=float32), 'training/total_loss': Array(69.912704, dtype=float32), 'training/v_loss': Array(69.90192, dtype=float32), 'eval/episode_goal_distance': (Array(0.3477909, dtype=float32), Array(0.15034829, dtype=float32)), 'eval/episode_reward': (Array(-6846.0107, dtype=float32), Array(5264.7695, dtype=float32)), 'eval/avg_episode_length': (Array(743.5781, dtype=float32), Array(435.07147, dtype=float32)), 'eval/epoch_eval_time': 4.078252077102661, 'eval/sps': 31385.995171474508}
I0727 02:53:19.893240 140120985872192 train.py:379] starting iteration 67 687.6049888134003
I0727 02:53:29.736752 140120985872192 train.py:394] {'eval/walltime': 288.30724120140076, 'training/sps': 42675.60842454721, 'training/walltime': 401.02245116233826, 'training/entropy_loss': Array(-0.00600576, dtype=float32), 'training/policy_loss': Array(0.01159899, dtype=float32), 'training/total_loss': Array(28.953545, dtype=float32), 'training/v_loss': Array(28.94795, dtype=float32), 'eval/episode_goal_distance': (Array(0.3480948, dtype=float32), Array(0.13813914, dtype=float32)), 'eval/episode_reward': (Array(-6827.7617, dtype=float32), Array(5257.4717, dtype=float32)), 'eval/avg_episode_length': (Array(720.39844, dtype=float32), Array(446.9747, dtype=float32)), 'eval/epoch_eval_time': 4.081148862838745, 'eval/sps': 31363.717497667163}
I0727 02:53:29.739248 140120985872192 train.py:379] starting iteration 68 697.4509973526001
I0727 02:53:39.569339 140120985872192 train.py:394] {'eval/walltime': 292.3870463371277, 'training/sps': 42764.538146938874, 'training/walltime': 406.7692687511444, 'training/entropy_loss': Array(0.00136151, dtype=float32), 'training/policy_loss': Array(0.01075444, dtype=float32), 'training/total_loss': Array(18.81591, dtype=float32), 'training/v_loss': Array(18.803795, dtype=float32), 'eval/episode_goal_distance': (Array(0.35601974, dtype=float32), Array(0.13786422, dtype=float32)), 'eval/episode_reward': (Array(-8368.897, dtype=float32), Array(3991.6648, dtype=float32)), 'eval/avg_episode_length': (Array(899.0547, dtype=float32), Array(300.23718, dtype=float32)), 'eval/epoch_eval_time': 4.079805135726929, 'eval/sps': 31374.047470821988}
I0727 02:53:39.571822 140120985872192 train.py:379] starting iteration 69 707.2835712432861
I0727 02:53:49.398754 140120985872192 train.py:394] {'eval/walltime': 296.4617054462433, 'training/sps': 42750.767887276466, 'training/walltime': 412.5179374217987, 'training/entropy_loss': Array(0.00448111, dtype=float32), 'training/policy_loss': Array(0.02793396, dtype=float32), 'training/total_loss': Array(15.537497, dtype=float32), 'training/v_loss': Array(15.505081, dtype=float32), 'eval/episode_goal_distance': (Array(0.37336928, dtype=float32), Array(0.13669096, dtype=float32)), 'eval/episode_reward': (Array(-8110.0713, dtype=float32), Array(5215.5386, dtype=float32)), 'eval/avg_episode_length': (Array(798.1172, dtype=float32), Array(399.86526, dtype=float32)), 'eval/epoch_eval_time': 4.074659109115601, 'eval/sps': 31413.670830437208}
I0727 02:53:49.401241 140120985872192 train.py:379] starting iteration 70 717.1129899024963
I0727 02:53:59.219809 140120985872192 train.py:394] {'eval/walltime': 300.54508113861084, 'training/sps': 42876.40968147385, 'training/walltime': 418.2497606277466, 'training/entropy_loss': Array(0.01113921, dtype=float32), 'training/policy_loss': Array(0.01152598, dtype=float32), 'training/total_loss': Array(18.666162, dtype=float32), 'training/v_loss': Array(18.643497, dtype=float32), 'eval/episode_goal_distance': (Array(0.41351545, dtype=float32), Array(0.2028244, dtype=float32)), 'eval/episode_reward': (Array(-9222.321, dtype=float32), Array(5527.8667, dtype=float32)), 'eval/avg_episode_length': (Array(844.7422, dtype=float32), Array(360.78732, dtype=float32)), 'eval/epoch_eval_time': 4.083375692367554, 'eval/sps': 31346.61359699313}
I0727 02:53:59.222169 140120985872192 train.py:379] starting iteration 71 726.9339184761047
I0727 02:54:09.087296 140120985872192 train.py:394] {'eval/walltime': 304.6266565322876, 'training/sps': 42518.79282832684, 'training/walltime': 424.0297930240631, 'training/entropy_loss': Array(0.01543901, dtype=float32), 'training/policy_loss': Array(0.00999386, dtype=float32), 'training/total_loss': Array(13.521448, dtype=float32), 'training/v_loss': Array(13.496016, dtype=float32), 'eval/episode_goal_distance': (Array(0.4043248, dtype=float32), Array(0.20974234, dtype=float32)), 'eval/episode_reward': (Array(-8882.096, dtype=float32), Array(5451.4565, dtype=float32)), 'eval/avg_episode_length': (Array(844.64844, dtype=float32), Array(361.0052, dtype=float32)), 'eval/epoch_eval_time': 4.081575393676758, 'eval/sps': 31360.439941474477}
I0727 02:54:09.089645 140120985872192 train.py:379] starting iteration 72 736.8013939857483
I0727 02:54:18.959389 140120985872192 train.py:394] {'eval/walltime': 308.72489404678345, 'training/sps': 42607.920158399495, 'training/walltime': 429.79773473739624, 'training/entropy_loss': Array(0.01818318, dtype=float32), 'training/policy_loss': Array(0.01055212, dtype=float32), 'training/total_loss': Array(12.564793, dtype=float32), 'training/v_loss': Array(12.536057, dtype=float32), 'eval/episode_goal_distance': (Array(0.45227355, dtype=float32), Array(0.1858959, dtype=float32)), 'eval/episode_reward': (Array(-9518.383, dtype=float32), Array(5806.794, dtype=float32)), 'eval/avg_episode_length': (Array(813.52344, dtype=float32), Array(388.18256, dtype=float32)), 'eval/epoch_eval_time': 4.09823751449585, 'eval/sps': 31232.938439329595}
I0727 02:54:18.961832 140120985872192 train.py:379] starting iteration 73 746.6735804080963
I0727 02:54:28.824602 140120985872192 train.py:394] {'eval/walltime': 312.81379079818726, 'training/sps': 42590.55648613499, 'training/walltime': 435.56802797317505, 'training/entropy_loss': Array(0.02311706, dtype=float32), 'training/policy_loss': Array(0.01073892, dtype=float32), 'training/total_loss': Array(13.652272, dtype=float32), 'training/v_loss': Array(13.618416, dtype=float32), 'eval/episode_goal_distance': (Array(0.48610073, dtype=float32), Array(0.20072709, dtype=float32)), 'eval/episode_reward': (Array(-10153.764, dtype=float32), Array(6026.399, dtype=float32)), 'eval/avg_episode_length': (Array(813.53125, dtype=float32), Array(388.16617, dtype=float32)), 'eval/epoch_eval_time': 4.088896751403809, 'eval/sps': 31304.28763114519}
I0727 02:54:28.827121 140120985872192 train.py:379] starting iteration 74 756.5388698577881
I0727 02:54:38.674643 140120985872192 train.py:394] {'eval/walltime': 316.9108352661133, 'training/sps': 42763.63333680434, 'training/walltime': 441.31496715545654, 'training/entropy_loss': Array(0.02663359, dtype=float32), 'training/policy_loss': Array(0.01171097, dtype=float32), 'training/total_loss': Array(14.232004, dtype=float32), 'training/v_loss': Array(14.193659, dtype=float32), 'eval/episode_goal_distance': (Array(0.47017854, dtype=float32), Array(0.20232968, dtype=float32)), 'eval/episode_reward': (Array(-9308.795, dtype=float32), Array(6188.8545, dtype=float32)), 'eval/avg_episode_length': (Array(782.53906, dtype=float32), Array(410.9632, dtype=float32)), 'eval/epoch_eval_time': 4.097044467926025, 'eval/sps': 31242.03337358337}
I0727 02:54:38.676973 140120985872192 train.py:379] starting iteration 75 766.3887219429016
I0727 02:54:48.480086 140120985872192 train.py:394] {'eval/walltime': 320.9907319545746, 'training/sps': 42967.063879105066, 'training/walltime': 447.03469705581665, 'training/entropy_loss': Array(-0.00088503, dtype=float32), 'training/policy_loss': Array(0.01362627, dtype=float32), 'training/total_loss': Array(109.22663, dtype=float32), 'training/v_loss': Array(109.21389, dtype=float32), 'eval/episode_goal_distance': (Array(0.49951696, dtype=float32), Array(0.26465222, dtype=float32)), 'eval/episode_reward': (Array(-10342.986, dtype=float32), Array(6996.077, dtype=float32)), 'eval/avg_episode_length': (Array(798.0547, dtype=float32), Array(399.9895, dtype=float32)), 'eval/epoch_eval_time': 4.079896688461304, 'eval/sps': 31373.343438329575}
I0727 02:54:48.482367 140120985872192 train.py:379] starting iteration 76 776.1941165924072
I0727 02:54:58.344011 140120985872192 train.py:394] {'eval/walltime': 325.0933048725128, 'training/sps': 42698.54679434739, 'training/walltime': 452.7903964519501, 'training/entropy_loss': Array(0.00820376, dtype=float32), 'training/policy_loss': Array(0.0141524, dtype=float32), 'training/total_loss': Array(39.01039, dtype=float32), 'training/v_loss': Array(38.988037, dtype=float32), 'eval/episode_goal_distance': (Array(0.52331126, dtype=float32), Array(0.29319194, dtype=float32)), 'eval/episode_reward': (Array(-10489.182, dtype=float32), Array(7420.152, dtype=float32)), 'eval/avg_episode_length': (Array(790.1719, dtype=float32), Array(405.82938, dtype=float32)), 'eval/epoch_eval_time': 4.102572917938232, 'eval/sps': 31199.932959223795}
I0727 02:54:58.346277 140120985872192 train.py:379] starting iteration 77 786.0580260753632
I0727 02:55:08.198951 140120985872192 train.py:394] {'eval/walltime': 329.2005479335785, 'training/sps': 42800.84550334207, 'training/walltime': 458.53233909606934, 'training/entropy_loss': Array(0.01589177, dtype=float32), 'training/policy_loss': Array(0.0147444, dtype=float32), 'training/total_loss': Array(35.93271, dtype=float32), 'training/v_loss': Array(35.902073, dtype=float32), 'eval/episode_goal_distance': (Array(0.551233, dtype=float32), Array(0.27810127, dtype=float32)), 'eval/episode_reward': (Array(-11700.401, dtype=float32), Array(6584.4897, dtype=float32)), 'eval/avg_episode_length': (Array(875.6875, dtype=float32), Array(328.9005, dtype=float32)), 'eval/epoch_eval_time': 4.107243061065674, 'eval/sps': 31164.45705718445}
I0727 02:55:08.201381 140120985872192 train.py:379] starting iteration 78 795.9131298065186
I0727 02:55:18.089724 140120985872192 train.py:394] {'eval/walltime': 333.32910442352295, 'training/sps': 42694.12902961528, 'training/walltime': 464.28863406181335, 'training/entropy_loss': Array(0.01973734, dtype=float32), 'training/policy_loss': Array(0.01833486, dtype=float32), 'training/total_loss': Array(34.101517, dtype=float32), 'training/v_loss': Array(34.063446, dtype=float32), 'eval/episode_goal_distance': (Array(0.52627885, dtype=float32), Array(0.29956973, dtype=float32)), 'eval/episode_reward': (Array(-9238.858, dtype=float32), Array(7679.7427, dtype=float32)), 'eval/avg_episode_length': (Array(704.7969, dtype=float32), Array(454.3088, dtype=float32)), 'eval/epoch_eval_time': 4.128556489944458, 'eval/sps': 31003.572389467776}
I0727 02:55:18.092012 140120985872192 train.py:379] starting iteration 79 805.8037610054016
I0727 02:55:27.918237 140120985872192 train.py:394] {'eval/walltime': 337.40739011764526, 'training/sps': 42781.99428116426, 'training/walltime': 470.03310680389404, 'training/entropy_loss': Array(0.02605891, dtype=float32), 'training/policy_loss': Array(0.01926749, dtype=float32), 'training/total_loss': Array(35.565495, dtype=float32), 'training/v_loss': Array(35.52017, dtype=float32), 'eval/episode_goal_distance': (Array(0.5435807, dtype=float32), Array(0.2532565, dtype=float32)), 'eval/episode_reward': (Array(-11191.098, dtype=float32), Array(7149.95, dtype=float32)), 'eval/avg_episode_length': (Array(829.0703, dtype=float32), Array(375.1973, dtype=float32)), 'eval/epoch_eval_time': 4.0782856941223145, 'eval/sps': 31385.736458942905}
I0727 02:55:27.920597 140120985872192 train.py:379] starting iteration 80 815.6323456764221
I0727 02:55:37.784297 140120985872192 train.py:394] {'eval/walltime': 341.4989504814148, 'training/sps': 42602.61431271929, 'training/walltime': 475.801766872406, 'training/entropy_loss': Array(0.02525252, dtype=float32), 'training/policy_loss': Array(0.03773438, dtype=float32), 'training/total_loss': Array(32.327682, dtype=float32), 'training/v_loss': Array(32.264698, dtype=float32), 'eval/episode_goal_distance': (Array(0.48300797, dtype=float32), Array(0.24266298, dtype=float32)), 'eval/episode_reward': (Array(-9423.93, dtype=float32), Array(6559.768, dtype=float32)), 'eval/avg_episode_length': (Array(767.02344, dtype=float32), Array(421.0816, dtype=float32)), 'eval/epoch_eval_time': 4.091560363769531, 'eval/sps': 31283.908489638983}
I0727 02:55:37.786854 140120985872192 train.py:379] starting iteration 81 825.4986033439636
I0727 02:55:47.659211 140120985872192 train.py:394] {'eval/walltime': 345.5918984413147, 'training/sps': 42550.792962025436, 'training/walltime': 481.57745242118835, 'training/entropy_loss': Array(0.02079953, dtype=float32), 'training/policy_loss': Array(0.01462563, dtype=float32), 'training/total_loss': Array(30.699139, dtype=float32), 'training/v_loss': Array(30.663712, dtype=float32), 'eval/episode_goal_distance': (Array(0.5479715, dtype=float32), Array(0.29160294, dtype=float32)), 'eval/episode_reward': (Array(-10299.084, dtype=float32), Array(7354.6875, dtype=float32)), 'eval/avg_episode_length': (Array(751.39844, dtype=float32), Array(430.59155, dtype=float32)), 'eval/epoch_eval_time': 4.092947959899902, 'eval/sps': 31273.30258143091}
I0727 02:55:47.661431 140120985872192 train.py:379] starting iteration 82 835.3731803894043
I0727 02:55:57.495719 140120985872192 train.py:394] {'eval/walltime': 349.670681476593, 'training/sps': 42727.08443334928, 'training/walltime': 487.32930755615234, 'training/entropy_loss': Array(0.02108005, dtype=float32), 'training/policy_loss': Array(0.01230735, dtype=float32), 'training/total_loss': Array(30.122082, dtype=float32), 'training/v_loss': Array(30.088696, dtype=float32), 'eval/episode_goal_distance': (Array(0.49344918, dtype=float32), Array(0.3060387, dtype=float32)), 'eval/episode_reward': (Array(-10114.546, dtype=float32), Array(7260.5566, dtype=float32)), 'eval/avg_episode_length': (Array(782.5, dtype=float32), Array(411.03726, dtype=float32)), 'eval/epoch_eval_time': 4.07878303527832, 'eval/sps': 31381.90947959206}
I0727 02:55:57.497890 140120985872192 train.py:379] starting iteration 83 845.2096393108368
I0727 02:56:07.373887 140120985872192 train.py:394] {'eval/walltime': 353.77195143699646, 'training/sps': 42584.29615033875, 'training/walltime': 493.1004490852356, 'training/entropy_loss': Array(0.00048038, dtype=float32), 'training/policy_loss': Array(0.01194331, dtype=float32), 'training/total_loss': Array(122.7498, dtype=float32), 'training/v_loss': Array(122.737366, dtype=float32), 'eval/episode_goal_distance': (Array(0.5330157, dtype=float32), Array(0.29514682, dtype=float32)), 'eval/episode_reward': (Array(-10275.395, dtype=float32), Array(7557.2173, dtype=float32)), 'eval/avg_episode_length': (Array(743.6953, dtype=float32), Array(434.87323, dtype=float32)), 'eval/epoch_eval_time': 4.101269960403442, 'eval/sps': 31209.845056726925}
I0727 02:56:07.376116 140120985872192 train.py:379] starting iteration 84 855.087865114212
I0727 02:56:17.265287 140120985872192 train.py:394] {'eval/walltime': 357.8859179019928, 'training/sps': 42593.386381813856, 'training/walltime': 498.8703589439392, 'training/entropy_loss': Array(-0.01110215, dtype=float32), 'training/policy_loss': Array(0.00462667, dtype=float32), 'training/total_loss': Array(47.230915, dtype=float32), 'training/v_loss': Array(47.237392, dtype=float32), 'eval/episode_goal_distance': (Array(0.49467322, dtype=float32), Array(0.25492334, dtype=float32)), 'eval/episode_reward': (Array(-10152.038, dtype=float32), Array(6463.724, dtype=float32)), 'eval/avg_episode_length': (Array(821.2656, dtype=float32), Array(381.8912, dtype=float32)), 'eval/epoch_eval_time': 4.113966464996338, 'eval/sps': 31113.525374863246}
I0727 02:56:17.267632 140120985872192 train.py:379] starting iteration 85 864.9793808460236
I0727 02:56:27.130357 140120985872192 train.py:394] {'eval/walltime': 361.97099900245667, 'training/sps': 42561.86169015563, 'training/walltime': 504.6445424556732, 'training/entropy_loss': Array(-0.01009324, dtype=float32), 'training/policy_loss': Array(0.00571226, dtype=float32), 'training/total_loss': Array(42.44061, dtype=float32), 'training/v_loss': Array(42.444992, dtype=float32), 'eval/episode_goal_distance': (Array(0.5055059, dtype=float32), Array(0.28640005, dtype=float32)), 'eval/episode_reward': (Array(-10379.287, dtype=float32), Array(6492.118, dtype=float32)), 'eval/avg_episode_length': (Array(821.4453, dtype=float32), Array(381.50732, dtype=float32)), 'eval/epoch_eval_time': 4.085081100463867, 'eval/sps': 31333.527254933913}
I0727 02:56:27.132646 140120985872192 train.py:379] starting iteration 86 874.8443944454193
I0727 02:56:37.011248 140120985872192 train.py:394] {'eval/walltime': 366.0637414455414, 'training/sps': 42510.32695960882, 'training/walltime': 510.42572593688965, 'training/entropy_loss': Array(-0.00995303, dtype=float32), 'training/policy_loss': Array(0.00540645, dtype=float32), 'training/total_loss': Array(41.96411, dtype=float32), 'training/v_loss': Array(41.96866, dtype=float32), 'eval/episode_goal_distance': (Array(0.48209947, dtype=float32), Array(0.24772364, dtype=float32)), 'eval/episode_reward': (Array(-9816.522, dtype=float32), Array(6239.8877, dtype=float32)), 'eval/avg_episode_length': (Array(805.89844, dtype=float32), Array(393.98438, dtype=float32)), 'eval/epoch_eval_time': 4.092742443084717, 'eval/sps': 31274.872968435775}
I0727 02:56:37.013797 140120985872192 train.py:379] starting iteration 87 884.7255456447601
I0727 02:56:46.864977 140120985872192 train.py:394] {'eval/walltime': 370.1557035446167, 'training/sps': 42699.44531473786, 'training/walltime': 516.1813042163849, 'training/entropy_loss': Array(-0.00747537, dtype=float32), 'training/policy_loss': Array(0.00614903, dtype=float32), 'training/total_loss': Array(39.459496, dtype=float32), 'training/v_loss': Array(39.460823, dtype=float32), 'eval/episode_goal_distance': (Array(0.5168621, dtype=float32), Array(0.27671388, dtype=float32)), 'eval/episode_reward': (Array(-10191.898, dtype=float32), Array(7483.1567, dtype=float32)), 'eval/avg_episode_length': (Array(735.89844, dtype=float32), Array(439.1333, dtype=float32)), 'eval/epoch_eval_time': 4.091962099075317, 'eval/sps': 31280.837138966865}
I0727 02:56:46.867353 140120985872192 train.py:379] starting iteration 88 894.5791022777557
I0727 02:56:56.728675 140120985872192 train.py:394] {'eval/walltime': 374.26441621780396, 'training/sps': 42747.63871368108, 'training/walltime': 521.9303936958313, 'training/entropy_loss': Array(-0.00713669, dtype=float32), 'training/policy_loss': Array(0.00825198, dtype=float32), 'training/total_loss': Array(39.083664, dtype=float32), 'training/v_loss': Array(39.08255, dtype=float32), 'eval/episode_goal_distance': (Array(0.5552559, dtype=float32), Array(0.28343222, dtype=float32)), 'eval/episode_reward': (Array(-10961.837, dtype=float32), Array(6667.8438, dtype=float32)), 'eval/avg_episode_length': (Array(837., dtype=float32), Array(367.93463, dtype=float32)), 'eval/epoch_eval_time': 4.108712673187256, 'eval/sps': 31153.310095229033}
I0727 02:56:56.730986 140120985872192 train.py:379] starting iteration 89 904.4427349567413
I0727 02:57:06.618969 140120985872192 train.py:394] {'eval/walltime': 378.3939645290375, 'training/sps': 42705.479469602855, 'training/walltime': 527.6851587295532, 'training/entropy_loss': Array(-0.01078891, dtype=float32), 'training/policy_loss': Array(0.00463852, dtype=float32), 'training/total_loss': Array(63.136307, dtype=float32), 'training/v_loss': Array(63.142456, dtype=float32), 'eval/episode_goal_distance': (Array(0.5555726, dtype=float32), Array(0.28283617, dtype=float32)), 'eval/episode_reward': (Array(-11702.848, dtype=float32), Array(6912.203, dtype=float32)), 'eval/avg_episode_length': (Array(875.72656, dtype=float32), Array(328.79712, dtype=float32)), 'eval/epoch_eval_time': 4.1295483112335205, 'eval/sps': 30996.12605373919}
I0727 02:57:06.621306 140120985872192 train.py:379] starting iteration 90 914.3330550193787
I0727 02:57:16.480394 140120985872192 train.py:394] {'eval/walltime': 382.47821021080017, 'training/sps': 42583.858101308135, 'training/walltime': 533.4563596248627, 'training/entropy_loss': Array(-0.00767417, dtype=float32), 'training/policy_loss': Array(0.00560912, dtype=float32), 'training/total_loss': Array(40.70736, dtype=float32), 'training/v_loss': Array(40.709427, dtype=float32), 'eval/episode_goal_distance': (Array(0.5340817, dtype=float32), Array(0.26956955, dtype=float32)), 'eval/episode_reward': (Array(-10922.553, dtype=float32), Array(6278.025, dtype=float32)), 'eval/avg_episode_length': (Array(852.3047, dtype=float32), Array(353.75607, dtype=float32)), 'eval/epoch_eval_time': 4.084245681762695, 'eval/sps': 31339.936422423354}
I0727 02:57:16.482865 140120985872192 train.py:379] starting iteration 91 924.1946136951447
I0727 02:57:26.323023 140120985872192 train.py:394] {'eval/walltime': 386.56400299072266, 'training/sps': 42735.36224092215, 'training/walltime': 539.2071006298065, 'training/entropy_loss': Array(-0.01317271, dtype=float32), 'training/policy_loss': Array(0.00551456, dtype=float32), 'training/total_loss': Array(70.93591, dtype=float32), 'training/v_loss': Array(70.94357, dtype=float32), 'eval/episode_goal_distance': (Array(0.5337263, dtype=float32), Array(0.26642108, dtype=float32)), 'eval/episode_reward': (Array(-10778.09, dtype=float32), Array(6625.488, dtype=float32)), 'eval/avg_episode_length': (Array(829.08594, dtype=float32), Array(375.16296, dtype=float32)), 'eval/epoch_eval_time': 4.085792779922485, 'eval/sps': 31328.069457900503}
I0727 02:57:26.325379 140120985872192 train.py:379] starting iteration 92 934.0371279716492
I0727 02:57:36.179083 140120985872192 train.py:394] {'eval/walltime': 390.6545763015747, 'training/sps': 42670.58952509359, 'training/walltime': 544.9665710926056, 'training/entropy_loss': Array(-0.02296322, dtype=float32), 'training/policy_loss': Array(0.00398204, dtype=float32), 'training/total_loss': Array(50.341896, dtype=float32), 'training/v_loss': Array(50.360878, dtype=float32), 'eval/episode_goal_distance': (Array(0.5786419, dtype=float32), Array(0.27592385, dtype=float32)), 'eval/episode_reward': (Array(-11741.531, dtype=float32), Array(6387.0586, dtype=float32)), 'eval/avg_episode_length': (Array(860.1797, dtype=float32), Array(345.64563, dtype=float32)), 'eval/epoch_eval_time': 4.090573310852051, 'eval/sps': 31291.45727823127}
I0727 02:57:36.181631 140120985872192 train.py:379] starting iteration 93 943.8933794498444
I0727 02:57:46.007171 140120985872192 train.py:394] {'eval/walltime': 394.7309467792511, 'training/sps': 42788.581083650235, 'training/walltime': 550.7101595401764, 'training/entropy_loss': Array(-0.01967608, dtype=float32), 'training/policy_loss': Array(0.00693542, dtype=float32), 'training/total_loss': Array(42.050247, dtype=float32), 'training/v_loss': Array(42.06299, dtype=float32), 'eval/episode_goal_distance': (Array(0.53095716, dtype=float32), Array(0.27804446, dtype=float32)), 'eval/episode_reward': (Array(-10695.841, dtype=float32), Array(7301.2666, dtype=float32)), 'eval/avg_episode_length': (Array(813.4922, dtype=float32), Array(388.24777, dtype=float32)), 'eval/epoch_eval_time': 4.076370477676392, 'eval/sps': 31400.482537338565}
I0727 02:57:46.009517 140120985872192 train.py:379] starting iteration 94 953.7212660312653
I0727 02:57:55.883575 140120985872192 train.py:394] {'eval/walltime': 398.82586693763733, 'training/sps': 42551.855661421374, 'training/walltime': 556.4857008457184, 'training/entropy_loss': Array(-0.0171242, dtype=float32), 'training/policy_loss': Array(0.00803291, dtype=float32), 'training/total_loss': Array(40.086002, dtype=float32), 'training/v_loss': Array(40.095093, dtype=float32), 'eval/episode_goal_distance': (Array(0.53933716, dtype=float32), Array(0.30752116, dtype=float32)), 'eval/episode_reward': (Array(-10900.43, dtype=float32), Array(6503.1836, dtype=float32)), 'eval/avg_episode_length': (Array(836.8828, dtype=float32), Array(368.19937, dtype=float32)), 'eval/epoch_eval_time': 4.0949201583862305, 'eval/sps': 31258.240710227572}
I0727 02:57:55.885980 140120985872192 train.py:379] starting iteration 95 963.5977292060852
I0727 02:58:05.755301 140120985872192 train.py:394] {'eval/walltime': 402.9287545681, 'training/sps': 42645.51448928381, 'training/walltime': 562.248557806015, 'training/entropy_loss': Array(-0.01637598, dtype=float32), 'training/policy_loss': Array(0.01128147, dtype=float32), 'training/total_loss': Array(39.873226, dtype=float32), 'training/v_loss': Array(39.87832, dtype=float32), 'eval/episode_goal_distance': (Array(0.48823333, dtype=float32), Array(0.24943335, dtype=float32)), 'eval/episode_reward': (Array(-9537.445, dtype=float32), Array(7040.4688, dtype=float32)), 'eval/avg_episode_length': (Array(743.64844, dtype=float32), Array(434.95203, dtype=float32)), 'eval/epoch_eval_time': 4.1028876304626465, 'eval/sps': 31197.539764345573}
I0727 02:58:05.757638 140120985872192 train.py:379] starting iteration 96 973.4693870544434
I0727 02:58:15.617213 140120985872192 train.py:394] {'eval/walltime': 407.0009512901306, 'training/sps': 42491.50652046366, 'training/walltime': 568.032301902771, 'training/entropy_loss': Array(-0.01014413, dtype=float32), 'training/policy_loss': Array(0.01629388, dtype=float32), 'training/total_loss': Array(39.67622, dtype=float32), 'training/v_loss': Array(39.67007, dtype=float32), 'eval/episode_goal_distance': (Array(0.5024477, dtype=float32), Array(0.2619691, dtype=float32)), 'eval/episode_reward': (Array(-10105.736, dtype=float32), Array(6602.619, dtype=float32)), 'eval/avg_episode_length': (Array(829., dtype=float32), Array(375.35193, dtype=float32)), 'eval/epoch_eval_time': 4.07219672203064, 'eval/sps': 31432.666135090738}
I0727 02:58:15.619717 140120985872192 train.py:379] starting iteration 97 983.3314664363861
I0727 02:58:25.453170 140120985872192 train.py:394] {'eval/walltime': 411.0856673717499, 'training/sps': 42776.726654137325, 'training/walltime': 573.7774820327759, 'training/entropy_loss': Array(-0.01274786, dtype=float32), 'training/policy_loss': Array(0.02194095, dtype=float32), 'training/total_loss': Array(58.499348, dtype=float32), 'training/v_loss': Array(58.490158, dtype=float32), 'eval/episode_goal_distance': (Array(0.5284804, dtype=float32), Array(0.27308768, dtype=float32)), 'eval/episode_reward': (Array(-10882.298, dtype=float32), Array(5937.1514, dtype=float32)), 'eval/avg_episode_length': (Array(891.4453, dtype=float32), Array(309.76913, dtype=float32)), 'eval/epoch_eval_time': 4.084716081619263, 'eval/sps': 31336.327285018608}
I0727 02:58:25.455533 140120985872192 train.py:379] starting iteration 98 993.167281627655
I0727 02:58:35.317484 140120985872192 train.py:394] {'eval/walltime': 415.16582441329956, 'training/sps': 42532.28591651544, 'training/walltime': 579.5556807518005, 'training/entropy_loss': Array(-0.02209926, dtype=float32), 'training/policy_loss': Array(0.01143893, dtype=float32), 'training/total_loss': Array(43.31475, dtype=float32), 'training/v_loss': Array(43.32541, dtype=float32), 'eval/episode_goal_distance': (Array(0.5128958, dtype=float32), Array(0.23506404, dtype=float32)), 'eval/episode_reward': (Array(-10970.73, dtype=float32), Array(5630.9062, dtype=float32)), 'eval/avg_episode_length': (Array(883.5078, dtype=float32), Array(319.7357, dtype=float32)), 'eval/epoch_eval_time': 4.080157041549683, 'eval/sps': 31371.341518605954}
I0727 02:58:35.319902 140120985872192 train.py:379] starting iteration 99 1003.0316507816315
I0727 02:58:45.160651 140120985872192 train.py:394] {'eval/walltime': 419.25024032592773, 'training/sps': 42720.57851762691, 'training/walltime': 585.3084118366241, 'training/entropy_loss': Array(-0.03199201, dtype=float32), 'training/policy_loss': Array(0.00394384, dtype=float32), 'training/total_loss': Array(36.75534, dtype=float32), 'training/v_loss': Array(36.78339, dtype=float32), 'eval/episode_goal_distance': (Array(0.5608462, dtype=float32), Array(0.29596812, dtype=float32)), 'eval/episode_reward': (Array(-11810.705, dtype=float32), Array(6236.4116, dtype=float32)), 'eval/avg_episode_length': (Array(914.6328, dtype=float32), Array(278.4122, dtype=float32)), 'eval/epoch_eval_time': 4.084415912628174, 'eval/sps': 31338.630232109892}
I0727 02:58:45.163192 140120985872192 train.py:379] starting iteration 100 1012.8749406337738
I0727 02:58:55.036633 140120985872192 train.py:394] {'eval/walltime': 423.3343095779419, 'training/sps': 42476.663454931746, 'training/walltime': 591.0941770076752, 'training/entropy_loss': Array(-0.03429348, dtype=float32), 'training/policy_loss': Array(0.00085391, dtype=float32), 'training/total_loss': Array(69.050545, dtype=float32), 'training/v_loss': Array(69.08398, dtype=float32), 'eval/episode_goal_distance': (Array(0.5736433, dtype=float32), Array(0.29486158, dtype=float32)), 'eval/episode_reward': (Array(-11531.07, dtype=float32), Array(6273.9785, dtype=float32)), 'eval/avg_episode_length': (Array(875.83594, dtype=float32), Array(328.5082, dtype=float32)), 'eval/epoch_eval_time': 4.08406925201416, 'eval/sps': 31341.290291998263}
I0727 02:58:55.038941 140120985872192 train.py:379] starting iteration 101 1022.7506899833679
I0727 02:59:04.943703 140120985872192 train.py:394] {'eval/walltime': 427.44278287887573, 'training/sps': 42425.16883003886, 'training/walltime': 596.8869647979736, 'training/entropy_loss': Array(-0.0347857, dtype=float32), 'training/policy_loss': Array(0.00099699, dtype=float32), 'training/total_loss': Array(40.797577, dtype=float32), 'training/v_loss': Array(40.831367, dtype=float32), 'eval/episode_goal_distance': (Array(0.5106185, dtype=float32), Array(0.26706383, dtype=float32)), 'eval/episode_reward': (Array(-10851.363, dtype=float32), Array(5727.8643, dtype=float32)), 'eval/avg_episode_length': (Array(891.27344, dtype=float32), Array(310.25925, dtype=float32)), 'eval/epoch_eval_time': 4.108473300933838, 'eval/sps': 31155.12518260887}
I0727 02:59:04.946034 140120985872192 train.py:379] starting iteration 102 1032.6577830314636
I0727 02:59:14.841960 140120985872192 train.py:394] {'eval/walltime': 431.56644892692566, 'training/sps': 42601.91001836095, 'training/walltime': 602.6557202339172, 'training/entropy_loss': Array(-0.03500826, dtype=float32), 'training/policy_loss': Array(0.00049262, dtype=float32), 'training/total_loss': Array(36.953472, dtype=float32), 'training/v_loss': Array(36.987988, dtype=float32), 'eval/episode_goal_distance': (Array(0.5551818, dtype=float32), Array(0.26646465, dtype=float32)), 'eval/episode_reward': (Array(-11704.694, dtype=float32), Array(6208.232, dtype=float32)), 'eval/avg_episode_length': (Array(875.8828, dtype=float32), Array(328.38397, dtype=float32)), 'eval/epoch_eval_time': 4.123666048049927, 'eval/sps': 31040.34092686311}
I0727 02:59:14.911645 140120985872192 train.py:379] starting iteration 103 1042.6233761310577
I0727 02:59:24.748624 140120985872192 train.py:394] {'eval/walltime': 435.6581175327301, 'training/sps': 42805.35292904336, 'training/walltime': 608.3970582485199, 'training/entropy_loss': Array(-0.03494999, dtype=float32), 'training/policy_loss': Array(0.00042894, dtype=float32), 'training/total_loss': Array(35.768074, dtype=float32), 'training/v_loss': Array(35.802593, dtype=float32), 'eval/episode_goal_distance': (Array(0.53742045, dtype=float32), Array(0.2812858, dtype=float32)), 'eval/episode_reward': (Array(-11817.375, dtype=float32), Array(6226.9395, dtype=float32)), 'eval/avg_episode_length': (Array(906.7656, dtype=float32), Array(289.87833, dtype=float32)), 'eval/epoch_eval_time': 4.091668605804443, 'eval/sps': 31283.080897221033}
I0727 02:59:24.751279 140120985872192 train.py:379] starting iteration 104 1052.4630281925201
I0727 02:59:34.616085 140120985872192 train.py:394] {'eval/walltime': 439.75405740737915, 'training/sps': 42626.921448133726, 'training/walltime': 614.162428855896, 'training/entropy_loss': Array(-0.03690358, dtype=float32), 'training/policy_loss': Array(0.00046036, dtype=float32), 'training/total_loss': Array(36.679398, dtype=float32), 'training/v_loss': Array(36.715843, dtype=float32), 'eval/episode_goal_distance': (Array(0.5456735, dtype=float32), Array(0.26810217, dtype=float32)), 'eval/episode_reward': (Array(-11618.443, dtype=float32), Array(5612.3154, dtype=float32)), 'eval/avg_episode_length': (Array(906.8203, dtype=float32), Array(289.70773, dtype=float32)), 'eval/epoch_eval_time': 4.095939874649048, 'eval/sps': 31250.458726757413}
I0727 02:59:34.618518 140120985872192 train.py:379] starting iteration 105 1062.3302671909332
I0727 02:59:44.496723 140120985872192 train.py:394] {'eval/walltime': 443.8710448741913, 'training/sps': 42683.36610345975, 'training/walltime': 619.9201753139496, 'training/entropy_loss': Array(-0.03697892, dtype=float32), 'training/policy_loss': Array(0.0002967, dtype=float32), 'training/total_loss': Array(36.03205, dtype=float32), 'training/v_loss': Array(36.068737, dtype=float32), 'eval/episode_goal_distance': (Array(0.5053119, dtype=float32), Array(0.25458634, dtype=float32)), 'eval/episode_reward': (Array(-11664.496, dtype=float32), Array(5839.715, dtype=float32)), 'eval/avg_episode_length': (Array(922.27344, dtype=float32), Array(266.9997, dtype=float32)), 'eval/epoch_eval_time': 4.116987466812134, 'eval/sps': 31090.694599348142}
I0727 02:59:44.499207 140120985872192 train.py:379] starting iteration 106 1072.2109553813934
I0727 02:59:54.355372 140120985872192 train.py:394] {'eval/walltime': 447.94192457199097, 'training/sps': 42507.05058451763, 'training/walltime': 625.7018043994904, 'training/entropy_loss': Array(-0.03843977, dtype=float32), 'training/policy_loss': Array(0.00045213, dtype=float32), 'training/total_loss': Array(55.776703, dtype=float32), 'training/v_loss': Array(55.814693, dtype=float32), 'eval/episode_goal_distance': (Array(0.5076854, dtype=float32), Array(0.29312396, dtype=float32)), 'eval/episode_reward': (Array(-10553.163, dtype=float32), Array(6215.222, dtype=float32)), 'eval/avg_episode_length': (Array(868.1172, dtype=float32), Array(336.99686, dtype=float32)), 'eval/epoch_eval_time': 4.070879697799683, 'eval/sps': 31442.835333400843}
I0727 02:59:54.357705 140120985872192 train.py:379] starting iteration 107 1082.069453716278
I0727 03:00:04.243209 140120985872192 train.py:394] {'eval/walltime': 452.02785062789917, 'training/sps': 42400.39004048881, 'training/walltime': 631.4979774951935, 'training/entropy_loss': Array(-0.04045136, dtype=float32), 'training/policy_loss': Array(0.00015144, dtype=float32), 'training/total_loss': Array(34.649742, dtype=float32), 'training/v_loss': Array(34.690044, dtype=float32), 'eval/episode_goal_distance': (Array(0.5237259, dtype=float32), Array(0.287797, dtype=float32)), 'eval/episode_reward': (Array(-11257.115, dtype=float32), Array(5608.339, dtype=float32)), 'eval/avg_episode_length': (Array(914.5156, dtype=float32), Array(278.7946, dtype=float32)), 'eval/epoch_eval_time': 4.085926055908203, 'eval/sps': 31327.04758934965}
I0727 03:00:04.245460 140120985872192 train.py:379] starting iteration 108 1091.9572083950043
I0727 03:00:14.104656 140120985872192 train.py:394] {'eval/walltime': 456.1162533760071, 'training/sps': 42612.249636761706, 'training/walltime': 637.2653331756592, 'training/entropy_loss': Array(-0.04118699, dtype=float32), 'training/policy_loss': Array(-0.00012018, dtype=float32), 'training/total_loss': Array(67.35113, dtype=float32), 'training/v_loss': Array(67.39243, dtype=float32), 'eval/episode_goal_distance': (Array(0.55536854, dtype=float32), Array(0.2779053, dtype=float32)), 'eval/episode_reward': (Array(-11504.674, dtype=float32), Array(6171.258, dtype=float32)), 'eval/avg_episode_length': (Array(891.28125, dtype=float32), Array(310.23706, dtype=float32)), 'eval/epoch_eval_time': 4.08840274810791, 'eval/sps': 31308.070140408177}
I0727 03:00:14.106967 140120985872192 train.py:379] starting iteration 109 1101.8187148571014
I0727 03:00:23.971165 140120985872192 train.py:394] {'eval/walltime': 460.2114465236664, 'training/sps': 42626.2533669538, 'training/walltime': 643.0307941436768, 'training/entropy_loss': Array(-0.04279415, dtype=float32), 'training/policy_loss': Array(0.00026386, dtype=float32), 'training/total_loss': Array(43.123955, dtype=float32), 'training/v_loss': Array(43.16648, dtype=float32), 'eval/episode_goal_distance': (Array(0.5030134, dtype=float32), Array(0.26651508, dtype=float32)), 'eval/episode_reward': (Array(-11488.323, dtype=float32), Array(5614.1016, dtype=float32)), 'eval/avg_episode_length': (Array(922.4219, dtype=float32), Array(266.4901, dtype=float32)), 'eval/epoch_eval_time': 4.095193147659302, 'eval/sps': 31256.157007676484}
I0727 03:00:23.973444 140120985872192 train.py:379] starting iteration 110 1111.6851937770844
I0727 03:00:33.830749 140120985872192 train.py:394] {'eval/walltime': 464.30196928977966, 'training/sps': 42641.69509392145, 'training/walltime': 648.7941672801971, 'training/entropy_loss': Array(-0.043123, dtype=float32), 'training/policy_loss': Array(0.00028613, dtype=float32), 'training/total_loss': Array(37.176567, dtype=float32), 'training/v_loss': Array(37.219406, dtype=float32), 'eval/episode_goal_distance': (Array(0.531303, dtype=float32), Array(0.2843439, dtype=float32)), 'eval/episode_reward': (Array(-11007.939, dtype=float32), Array(6223.2393, dtype=float32)), 'eval/avg_episode_length': (Array(891.33594, dtype=float32), Array(310.08075, dtype=float32)), 'eval/epoch_eval_time': 4.090522766113281, 'eval/sps': 31291.84393260879}
I0727 03:00:33.833029 140120985872192 train.py:379] starting iteration 111 1121.544776916504
I0727 03:00:43.724121 140120985872192 train.py:394] {'eval/walltime': 468.4378101825714, 'training/sps': 42728.03197613181, 'training/walltime': 654.5458948612213, 'training/entropy_loss': Array(-0.04387225, dtype=float32), 'training/policy_loss': Array(5.628517e-05, dtype=float32), 'training/total_loss': Array(35.910133, dtype=float32), 'training/v_loss': Array(35.95395, dtype=float32), 'eval/episode_goal_distance': (Array(0.5056176, dtype=float32), Array(0.23150513, dtype=float32)), 'eval/episode_reward': (Array(-10649.727, dtype=float32), Array(4991.8994, dtype=float32)), 'eval/avg_episode_length': (Array(906.77344, dtype=float32), Array(289.8536, dtype=float32)), 'eval/epoch_eval_time': 4.135840892791748, 'eval/sps': 30948.96620009922}
I0727 03:00:43.726459 140120985872192 train.py:379] starting iteration 112 1131.438208580017
I0727 03:00:53.612763 140120985872192 train.py:394] {'eval/walltime': 472.51841497421265, 'training/sps': 42355.39674197079, 'training/walltime': 660.3482251167297, 'training/entropy_loss': Array(-0.04428494, dtype=float32), 'training/policy_loss': Array(0.0001199, dtype=float32), 'training/total_loss': Array(33.46692, dtype=float32), 'training/v_loss': Array(33.511078, dtype=float32), 'eval/episode_goal_distance': (Array(0.48698956, dtype=float32), Array(0.2121866, dtype=float32)), 'eval/episode_reward': (Array(-10593.629, dtype=float32), Array(5409.9443, dtype=float32)), 'eval/avg_episode_length': (Array(891.21875, dtype=float32), Array(310.41525, dtype=float32)), 'eval/epoch_eval_time': 4.080604791641235, 'eval/sps': 31367.8992541}
I0727 03:00:53.615040 140120985872192 train.py:379] starting iteration 113 1141.3267886638641
I0727 03:01:03.490870 140120985872192 train.py:394] {'eval/walltime': 476.61288142204285, 'training/sps': 42535.3099214861, 'training/walltime': 666.1260130405426, 'training/entropy_loss': Array(-0.04388413, dtype=float32), 'training/policy_loss': Array(0.0002319, dtype=float32), 'training/total_loss': Array(32.834763, dtype=float32), 'training/v_loss': Array(32.878418, dtype=float32), 'eval/episode_goal_distance': (Array(0.54352754, dtype=float32), Array(0.26831165, dtype=float32)), 'eval/episode_reward': (Array(-10702.695, dtype=float32), Array(6120.5835, dtype=float32)), 'eval/avg_episode_length': (Array(852.46094, dtype=float32), Array(353.3823, dtype=float32)), 'eval/epoch_eval_time': 4.0944664478302, 'eval/sps': 31261.704456714167}
I0727 03:01:03.493219 140120985872192 train.py:379] starting iteration 114 1151.2049679756165
I0727 03:01:13.416742 140120985872192 train.py:394] {'eval/walltime': 480.73281145095825, 'training/sps': 42371.668669279, 'training/walltime': 671.9261150360107, 'training/entropy_loss': Array(-0.04471962, dtype=float32), 'training/policy_loss': Array(5.9331534e-05, dtype=float32), 'training/total_loss': Array(52.863186, dtype=float32), 'training/v_loss': Array(52.90784, dtype=float32), 'eval/episode_goal_distance': (Array(0.5203724, dtype=float32), Array(0.26872775, dtype=float32)), 'eval/episode_reward': (Array(-11037.086, dtype=float32), Array(6228.09, dtype=float32)), 'eval/avg_episode_length': (Array(891.40625, dtype=float32), Array(309.88025, dtype=float32)), 'eval/epoch_eval_time': 4.119930028915405, 'eval/sps': 31068.48880967445}
I0727 03:01:13.419040 140120985872192 train.py:379] starting iteration 115 1161.1307892799377
I0727 03:01:23.294459 140120985872192 train.py:394] {'eval/walltime': 484.8191261291504, 'training/sps': 42477.50890013197, 'training/walltime': 677.7117650508881, 'training/entropy_loss': Array(-0.04519102, dtype=float32), 'training/policy_loss': Array(0.00025204, dtype=float32), 'training/total_loss': Array(35.421642, dtype=float32), 'training/v_loss': Array(35.466583, dtype=float32), 'eval/episode_goal_distance': (Array(0.5223121, dtype=float32), Array(0.23967543, dtype=float32)), 'eval/episode_reward': (Array(-11328.5625, dtype=float32), Array(4995.8096, dtype=float32)), 'eval/avg_episode_length': (Array(930.1328, dtype=float32), Array(254.05414, dtype=float32)), 'eval/epoch_eval_time': 4.086314678192139, 'eval/sps': 31324.068281650198}
I0727 03:01:23.297006 140120985872192 train.py:379] starting iteration 116 1171.0087547302246
I0727 03:01:33.163766 140120985872192 train.py:394] {'eval/walltime': 488.89843678474426, 'training/sps': 42488.74268678382, 'training/walltime': 683.4958853721619, 'training/entropy_loss': Array(-0.04500991, dtype=float32), 'training/policy_loss': Array(0.00030702, dtype=float32), 'training/total_loss': Array(57.783844, dtype=float32), 'training/v_loss': Array(57.82855, dtype=float32), 'eval/episode_goal_distance': (Array(0.5244595, dtype=float32), Array(0.29533684, dtype=float32)), 'eval/episode_reward': (Array(-10809.877, dtype=float32), Array(5961.618, dtype=float32)), 'eval/avg_episode_length': (Array(875.77344, dtype=float32), Array(328.6731, dtype=float32)), 'eval/epoch_eval_time': 4.079310655593872, 'eval/sps': 31377.850525915776}
I0727 03:01:33.166038 140120985872192 train.py:379] starting iteration 117 1180.8777873516083
I0727 03:01:43.046096 140120985872192 train.py:394] {'eval/walltime': 493.0010268688202, 'training/sps': 42563.08311682069, 'training/walltime': 689.2699031829834, 'training/entropy_loss': Array(-0.04512995, dtype=float32), 'training/policy_loss': Array(-0.0001635, dtype=float32), 'training/total_loss': Array(42.7675, dtype=float32), 'training/v_loss': Array(42.81279, dtype=float32), 'eval/episode_goal_distance': (Array(0.51924354, dtype=float32), Array(0.25714874, dtype=float32)), 'eval/episode_reward': (Array(-11425.492, dtype=float32), Array(5358.5586, dtype=float32)), 'eval/avg_episode_length': (Array(937.8906, dtype=float32), Array(240.54886, dtype=float32)), 'eval/epoch_eval_time': 4.102590084075928, 'eval/sps': 31199.802411853892}
I0727 03:01:43.048745 140120985872192 train.py:379] starting iteration 118 1190.7604937553406
I0727 03:01:52.918704 140120985872192 train.py:394] {'eval/walltime': 497.0864489078522, 'training/sps': 42510.25157451616, 'training/walltime': 695.0510969161987, 'training/entropy_loss': Array(-0.04502084, dtype=float32), 'training/policy_loss': Array(0.00016885, dtype=float32), 'training/total_loss': Array(34.510616, dtype=float32), 'training/v_loss': Array(34.555466, dtype=float32), 'eval/episode_goal_distance': (Array(0.51638746, dtype=float32), Array(0.24165396, dtype=float32)), 'eval/episode_reward': (Array(-10460.416, dtype=float32), Array(5413.4683, dtype=float32)), 'eval/avg_episode_length': (Array(883.5547, dtype=float32), Array(319.60706, dtype=float32)), 'eval/epoch_eval_time': 4.085422039031982, 'eval/sps': 31330.912394629584}
I0727 03:01:52.921049 140120985872192 train.py:379] starting iteration 119 1200.6327979564667
I0727 03:02:02.797352 140120985872192 train.py:394] {'eval/walltime': 501.1635410785675, 'training/sps': 42405.44326799206, 'training/walltime': 700.8465793132782, 'training/entropy_loss': Array(-0.04477458, dtype=float32), 'training/policy_loss': Array(0.00042973, dtype=float32), 'training/total_loss': Array(31.25354, dtype=float32), 'training/v_loss': Array(31.297882, dtype=float32), 'eval/episode_goal_distance': (Array(0.50798535, dtype=float32), Array(0.2569287, dtype=float32)), 'eval/episode_reward': (Array(-10238.346, dtype=float32), Array(6041.9717, dtype=float32)), 'eval/avg_episode_length': (Array(844.5156, dtype=float32), Array(361.31348, dtype=float32)), 'eval/epoch_eval_time': 4.077092170715332, 'eval/sps': 31394.924284368633}
I0727 03:02:02.799920 140120985872192 train.py:379] starting iteration 120 1210.5116682052612
I0727 03:02:12.661478 140120985872192 train.py:394] {'eval/walltime': 505.2468674182892, 'training/sps': 42559.621125653575, 'training/walltime': 706.6210668087006, 'training/entropy_loss': Array(-0.04374034, dtype=float32), 'training/policy_loss': Array(0.00073615, dtype=float32), 'training/total_loss': Array(29.514069, dtype=float32), 'training/v_loss': Array(29.557072, dtype=float32), 'eval/episode_goal_distance': (Array(0.508625, dtype=float32), Array(0.2362363, dtype=float32)), 'eval/episode_reward': (Array(-10189.408, dtype=float32), Array(5565.9126, dtype=float32)), 'eval/avg_episode_length': (Array(852.5, dtype=float32), Array(353.28824, dtype=float32)), 'eval/epoch_eval_time': 4.08332633972168, 'eval/sps': 31346.99246416942}
I0727 03:02:12.663818 140120985872192 train.py:379] starting iteration 121 1220.3755671977997
I0727 03:02:22.532270 140120985872192 train.py:394] {'eval/walltime': 509.33470010757446, 'training/sps': 42541.89998574084, 'training/walltime': 712.3979597091675, 'training/entropy_loss': Array(-0.04332942, dtype=float32), 'training/policy_loss': Array(0.00081197, dtype=float32), 'training/total_loss': Array(30.186516, dtype=float32), 'training/v_loss': Array(30.229034, dtype=float32), 'eval/episode_goal_distance': (Array(0.49760997, dtype=float32), Array(0.2600684, dtype=float32)), 'eval/episode_reward': (Array(-9804.756, dtype=float32), Array(6044.1455, dtype=float32)), 'eval/avg_episode_length': (Array(836.8203, dtype=float32), Array(368.3402, dtype=float32)), 'eval/epoch_eval_time': 4.087832689285278, 'eval/sps': 31312.43613162154}
I0727 03:02:22.534645 140120985872192 train.py:379] starting iteration 122 1230.2463936805725
I0727 03:02:32.463117 140120985872192 train.py:394] {'eval/walltime': 513.4480361938477, 'training/sps': 42289.86646343745, 'training/walltime': 718.2092809677124, 'training/entropy_loss': Array(-0.04414168, dtype=float32), 'training/policy_loss': Array(0.00025296, dtype=float32), 'training/total_loss': Array(42.634903, dtype=float32), 'training/v_loss': Array(42.678787, dtype=float32), 'eval/episode_goal_distance': (Array(0.49169463, dtype=float32), Array(0.22856334, dtype=float32)), 'eval/episode_reward': (Array(-9933.035, dtype=float32), Array(5608.4033, dtype=float32)), 'eval/avg_episode_length': (Array(829.0469, dtype=float32), Array(375.24878, dtype=float32)), 'eval/epoch_eval_time': 4.113336086273193, 'eval/sps': 31118.29359802492}
I0727 03:02:32.465629 140120985872192 train.py:379] starting iteration 123 1240.1773777008057
I0727 03:02:42.384133 140120985872192 train.py:394] {'eval/walltime': 517.5397629737854, 'training/sps': 42203.36967239815, 'training/walltime': 724.0325126647949, 'training/entropy_loss': Array(-0.04410355, dtype=float32), 'training/policy_loss': Array(0.00030533, dtype=float32), 'training/total_loss': Array(38.119846, dtype=float32), 'training/v_loss': Array(38.163643, dtype=float32), 'eval/episode_goal_distance': (Array(0.4989144, dtype=float32), Array(0.28414193, dtype=float32)), 'eval/episode_reward': (Array(-9235.24, dtype=float32), Array(6687.081, dtype=float32)), 'eval/avg_episode_length': (Array(759.21094, dtype=float32), Array(425.9345, dtype=float32)), 'eval/epoch_eval_time': 4.091726779937744, 'eval/sps': 31282.63612995869}
I0727 03:02:42.386426 140120985872192 train.py:379] starting iteration 124 1250.0981748104095
I0727 03:02:52.275257 140120985872192 train.py:394] {'eval/walltime': 521.6494514942169, 'training/sps': 42550.38546162628, 'training/walltime': 729.8082535266876, 'training/entropy_loss': Array(-0.04451619, dtype=float32), 'training/policy_loss': Array(0.00068181, dtype=float32), 'training/total_loss': Array(29.407589, dtype=float32), 'training/v_loss': Array(29.451424, dtype=float32), 'eval/episode_goal_distance': (Array(0.45735595, dtype=float32), Array(0.21436113, dtype=float32)), 'eval/episode_reward': (Array(-9033.816, dtype=float32), Array(6049.152, dtype=float32)), 'eval/avg_episode_length': (Array(782.46875, dtype=float32), Array(411.09607, dtype=float32)), 'eval/epoch_eval_time': 4.1096885204315186, 'eval/sps': 31145.91272882159}
I0727 03:02:52.277667 140120985872192 train.py:379] starting iteration 125 1259.989416360855
I0727 03:03:02.149082 140120985872192 train.py:394] {'eval/walltime': 525.7617917060852, 'training/sps': 42699.1570058467, 'training/walltime': 735.5638706684113, 'training/entropy_loss': Array(-0.04676418, dtype=float32), 'training/policy_loss': Array(0.0010989, dtype=float32), 'training/total_loss': Array(78.12454, dtype=float32), 'training/v_loss': Array(78.1702, dtype=float32), 'eval/episode_goal_distance': (Array(0.50277615, dtype=float32), Array(0.22951691, dtype=float32)), 'eval/episode_reward': (Array(-9486.801, dtype=float32), Array(5837.7456, dtype=float32)), 'eval/avg_episode_length': (Array(805.71094, dtype=float32), Array(394.3643, dtype=float32)), 'eval/epoch_eval_time': 4.112340211868286, 'eval/sps': 31125.82943176485}
I0727 03:03:02.151362 140120985872192 train.py:379] starting iteration 126 1269.8631114959717
I0727 03:03:12.035645 140120985872192 train.py:394] {'eval/walltime': 529.8456935882568, 'training/sps': 42394.52199380635, 'training/walltime': 741.360846042633, 'training/entropy_loss': Array(-0.04599622, dtype=float32), 'training/policy_loss': Array(0.00096955, dtype=float32), 'training/total_loss': Array(35.344666, dtype=float32), 'training/v_loss': Array(35.389687, dtype=float32), 'eval/episode_goal_distance': (Array(0.5006537, dtype=float32), Array(0.24489674, dtype=float32)), 'eval/episode_reward': (Array(-8977.38, dtype=float32), Array(6409.6143, dtype=float32)), 'eval/avg_episode_length': (Array(735.8281, dtype=float32), Array(439.2498, dtype=float32)), 'eval/epoch_eval_time': 4.083901882171631, 'eval/sps': 31342.574746662496}
I0727 03:03:12.037933 140120985872192 train.py:379] starting iteration 127 1279.749682188034
I0727 03:03:21.956776 140120985872192 train.py:394] {'eval/walltime': 533.9522540569305, 'training/sps': 42322.707892177605, 'training/walltime': 747.1676578521729, 'training/entropy_loss': Array(-0.04395731, dtype=float32), 'training/policy_loss': Array(0.00110871, dtype=float32), 'training/total_loss': Array(29.965036, dtype=float32), 'training/v_loss': Array(30.007885, dtype=float32), 'eval/episode_goal_distance': (Array(0.4942637, dtype=float32), Array(0.23491229, dtype=float32)), 'eval/episode_reward': (Array(-9970.508, dtype=float32), Array(6019.869, dtype=float32)), 'eval/avg_episode_length': (Array(829.08594, dtype=float32), Array(375.16327, dtype=float32)), 'eval/epoch_eval_time': 4.106560468673706, 'eval/sps': 31169.637212560054}
I0727 03:03:21.959079 140120985872192 train.py:379] starting iteration 128 1289.6708271503448
I0727 03:03:31.855814 140120985872192 train.py:394] {'eval/walltime': 538.0754992961884, 'training/sps': 42591.75668203557, 'training/walltime': 752.9377884864807, 'training/entropy_loss': Array(-0.04209449, dtype=float32), 'training/policy_loss': Array(0.00106684, dtype=float32), 'training/total_loss': Array(27.40218, dtype=float32), 'training/v_loss': Array(27.443207, dtype=float32), 'eval/episode_goal_distance': (Array(0.44781786, dtype=float32), Array(0.20433003, dtype=float32)), 'eval/episode_reward': (Array(-9372.016, dtype=float32), Array(5288.539, dtype=float32)), 'eval/avg_episode_length': (Array(844.7422, dtype=float32), Array(360.78748, dtype=float32)), 'eval/epoch_eval_time': 4.1232452392578125, 'eval/sps': 31043.508831660012}
I0727 03:03:31.857998 140120985872192 train.py:379] starting iteration 129 1299.5697469711304
I0727 03:03:41.713245 140120985872192 train.py:394] {'eval/walltime': 542.1573798656464, 'training/sps': 42592.16673514053, 'training/walltime': 758.7078635692596, 'training/entropy_loss': Array(-0.03902757, dtype=float32), 'training/policy_loss': Array(0.00119066, dtype=float32), 'training/total_loss': Array(26.924461, dtype=float32), 'training/v_loss': Array(26.962296, dtype=float32), 'eval/episode_goal_distance': (Array(0.48868418, dtype=float32), Array(0.19633535, dtype=float32)), 'eval/episode_reward': (Array(-9251.0625, dtype=float32), Array(5888.1753, dtype=float32)), 'eval/avg_episode_length': (Array(782.4531, dtype=float32), Array(411.1262, dtype=float32)), 'eval/epoch_eval_time': 4.081880569458008, 'eval/sps': 31358.09532442931}
I0727 03:03:41.715438 140120985872192 train.py:379] starting iteration 130 1309.4271876811981
I0727 03:03:51.618301 140120985872192 train.py:394] {'eval/walltime': 546.2458696365356, 'training/sps': 42292.43616667597, 'training/walltime': 764.5188317298889, 'training/entropy_loss': Array(-0.03648164, dtype=float32), 'training/policy_loss': Array(0.00156221, dtype=float32), 'training/total_loss': Array(28.033863, dtype=float32), 'training/v_loss': Array(28.068783, dtype=float32), 'eval/episode_goal_distance': (Array(0.4569921, dtype=float32), Array(0.23870039, dtype=float32)), 'eval/episode_reward': (Array(-7812.3945, dtype=float32), Array(5957.1304, dtype=float32)), 'eval/avg_episode_length': (Array(712.6406, dtype=float32), Array(450.65714, dtype=float32)), 'eval/epoch_eval_time': 4.088489770889282, 'eval/sps': 31307.403753674767}
I0727 03:03:51.620618 140120985872192 train.py:379] starting iteration 131 1319.3323669433594
I0727 03:04:01.483107 140120985872192 train.py:394] {'eval/walltime': 550.3405759334564, 'training/sps': 42635.17106768827, 'training/walltime': 770.2830867767334, 'training/entropy_loss': Array(-0.03339532, dtype=float32), 'training/policy_loss': Array(0.00169018, dtype=float32), 'training/total_loss': Array(32.473747, dtype=float32), 'training/v_loss': Array(32.505455, dtype=float32), 'eval/episode_goal_distance': (Array(0.46468258, dtype=float32), Array(0.22505735, dtype=float32)), 'eval/episode_reward': (Array(-9552.969, dtype=float32), Array(5324.333, dtype=float32)), 'eval/avg_episode_length': (Array(860.25, dtype=float32), Array(345.472, dtype=float32)), 'eval/epoch_eval_time': 4.094706296920776, 'eval/sps': 31259.873289631578}
I0727 03:04:01.485277 140120985872192 train.py:379] starting iteration 132 1329.1970257759094
I0727 03:04:11.366523 140120985872192 train.py:394] {'eval/walltime': 554.4179065227509, 'training/sps': 42369.812069873566, 'training/walltime': 776.0834429264069, 'training/entropy_loss': Array(-0.03161475, dtype=float32), 'training/policy_loss': Array(0.00348821, dtype=float32), 'training/total_loss': Array(28.464613, dtype=float32), 'training/v_loss': Array(28.49274, dtype=float32), 'eval/episode_goal_distance': (Array(0.4621536, dtype=float32), Array(0.21224493, dtype=float32)), 'eval/episode_reward': (Array(-8914.244, dtype=float32), Array(5872.586, dtype=float32)), 'eval/avg_episode_length': (Array(790.33594, dtype=float32), Array(405.5123, dtype=float32)), 'eval/epoch_eval_time': 4.077330589294434, 'eval/sps': 31393.088491789407}
I0727 03:04:11.371304 140120985872192 train.py:379] starting iteration 133 1339.0830373764038
I0727 03:04:21.256712 140120985872192 train.py:394] {'eval/walltime': 558.4951210021973, 'training/sps': 42359.11453982747, 'training/walltime': 781.8852639198303, 'training/entropy_loss': Array(-0.03889633, dtype=float32), 'training/policy_loss': Array(0.0037361, dtype=float32), 'training/total_loss': Array(74.78435, dtype=float32), 'training/v_loss': Array(74.8195, dtype=float32), 'eval/episode_goal_distance': (Array(0.49814606, dtype=float32), Array(0.20459354, dtype=float32)), 'eval/episode_reward': (Array(-10504.593, dtype=float32), Array(5133.1953, dtype=float32)), 'eval/avg_episode_length': (Array(883.47656, dtype=float32), Array(319.8218, dtype=float32)), 'eval/epoch_eval_time': 4.077214479446411, 'eval/sps': 31393.98249595625}
I0727 03:04:21.259223 140120985872192 train.py:379] starting iteration 134 1348.9709725379944
I0727 03:04:31.134801 140120985872192 train.py:394] {'eval/walltime': 562.5834259986877, 'training/sps': 42490.6850393149, 'training/walltime': 787.6691198348999, 'training/entropy_loss': Array(-0.03988989, dtype=float32), 'training/policy_loss': Array(0.00087717, dtype=float32), 'training/total_loss': Array(36.37381, dtype=float32), 'training/v_loss': Array(36.41282, dtype=float32), 'eval/episode_goal_distance': (Array(0.48258176, dtype=float32), Array(0.22540079, dtype=float32)), 'eval/episode_reward': (Array(-10083.631, dtype=float32), Array(5457.1445, dtype=float32)), 'eval/avg_episode_length': (Array(875.7969, dtype=float32), Array(328.6112, dtype=float32)), 'eval/epoch_eval_time': 4.0883049964904785, 'eval/sps': 31308.81871824117}
I0727 03:04:31.137203 140120985872192 train.py:379] starting iteration 135 1358.848952293396
I0727 03:04:41.059652 140120985872192 train.py:394] {'eval/walltime': 566.7087106704712, 'training/sps': 42418.63058877327, 'training/walltime': 793.4628005027771, 'training/entropy_loss': Array(-0.03696692, dtype=float32), 'training/policy_loss': Array(0.00199455, dtype=float32), 'training/total_loss': Array(29.405811, dtype=float32), 'training/v_loss': Array(29.44078, dtype=float32), 'eval/episode_goal_distance': (Array(0.4724061, dtype=float32), Array(0.20965046, dtype=float32)), 'eval/episode_reward': (Array(-9661.256, dtype=float32), Array(4928.602, dtype=float32)), 'eval/avg_episode_length': (Array(875.72656, dtype=float32), Array(328.7973, dtype=float32)), 'eval/epoch_eval_time': 4.125284671783447, 'eval/sps': 31028.16173523921}
I0727 03:04:41.062140 140120985872192 train.py:379] starting iteration 136 1368.7738888263702
I0727 03:04:50.957989 140120985872192 train.py:394] {'eval/walltime': 570.7937376499176, 'training/sps': 42319.4238865325, 'training/walltime': 799.2700629234314, 'training/entropy_loss': Array(-0.03394122, dtype=float32), 'training/policy_loss': Array(0.00171841, dtype=float32), 'training/total_loss': Array(27.879644, dtype=float32), 'training/v_loss': Array(27.911867, dtype=float32), 'eval/episode_goal_distance': (Array(0.51667017, dtype=float32), Array(0.21992198, dtype=float32)), 'eval/episode_reward': (Array(-10409.804, dtype=float32), Array(5039.147, dtype=float32)), 'eval/avg_episode_length': (Array(883.41406, dtype=float32), Array(319.99298, dtype=float32)), 'eval/epoch_eval_time': 4.085026979446411, 'eval/sps': 31333.942381292698}
I0727 03:04:50.960678 140120985872192 train.py:379] starting iteration 137 1378.6724274158478
I0727 03:05:00.823119 140120985872192 train.py:394] {'eval/walltime': 574.8784742355347, 'training/sps': 42578.27156758916, 'training/walltime': 805.0420210361481, 'training/entropy_loss': Array(-0.03248654, dtype=float32), 'training/policy_loss': Array(0.00180397, dtype=float32), 'training/total_loss': Array(26.524105, dtype=float32), 'training/v_loss': Array(26.554787, dtype=float32), 'eval/episode_goal_distance': (Array(0.45770103, dtype=float32), Array(0.19711947, dtype=float32)), 'eval/episode_reward': (Array(-9683.569, dtype=float32), Array(4563.5454, dtype=float32)), 'eval/avg_episode_length': (Array(906.77344, dtype=float32), Array(289.85333, dtype=float32)), 'eval/epoch_eval_time': 4.084736585617065, 'eval/sps': 31336.169987241305}
I0727 03:05:00.825505 140120985872192 train.py:379] starting iteration 138 1388.5372545719147
I0727 03:05:10.782963 140120985872192 train.py:394] {'eval/walltime': 579.000529050827, 'training/sps': 42155.50366988374, 'training/walltime': 810.8718647956848, 'training/entropy_loss': Array(-0.03000179, dtype=float32), 'training/policy_loss': Array(0.00239829, dtype=float32), 'training/total_loss': Array(25.224728, dtype=float32), 'training/v_loss': Array(25.25233, dtype=float32), 'eval/episode_goal_distance': (Array(0.46329492, dtype=float32), Array(0.20656723, dtype=float32)), 'eval/episode_reward': (Array(-9783.828, dtype=float32), Array(5405.255, dtype=float32)), 'eval/avg_episode_length': (Array(852.4297, dtype=float32), Array(353.45676, dtype=float32)), 'eval/epoch_eval_time': 4.122054815292358, 'eval/sps': 31052.474005230215}
I0727 03:05:10.785560 140120985872192 train.py:379] starting iteration 139 1398.4973089694977
I0727 03:05:20.695734 140120985872192 train.py:394] {'eval/walltime': 583.0922141075134, 'training/sps': 42263.32362816586, 'training/walltime': 816.6868357658386, 'training/entropy_loss': Array(-0.02960633, dtype=float32), 'training/policy_loss': Array(0.00200468, dtype=float32), 'training/total_loss': Array(30.47232, dtype=float32), 'training/v_loss': Array(30.499922, dtype=float32), 'eval/episode_goal_distance': (Array(0.51119804, dtype=float32), Array(0.22331473, dtype=float32)), 'eval/episode_reward': (Array(-10413.695, dtype=float32), Array(5096.479, dtype=float32)), 'eval/avg_episode_length': (Array(906.84375, dtype=float32), Array(289.6352, dtype=float32)), 'eval/epoch_eval_time': 4.091685056686401, 'eval/sps': 31282.955121589723}
I0727 03:05:20.698249 140120985872192 train.py:379] starting iteration 140 1408.4099979400635
I0727 03:05:30.646293 140120985872192 train.py:394] {'eval/walltime': 587.2265858650208, 'training/sps': 42297.866364150635, 'training/walltime': 822.4970579147339, 'training/entropy_loss': Array(-0.02844914, dtype=float32), 'training/policy_loss': Array(0.0023556, dtype=float32), 'training/total_loss': Array(27.95853, dtype=float32), 'training/v_loss': Array(27.984623, dtype=float32), 'eval/episode_goal_distance': (Array(0.43245882, dtype=float32), Array(0.19792068, dtype=float32)), 'eval/episode_reward': (Array(-9189.006, dtype=float32), Array(4899.2715, dtype=float32)), 'eval/avg_episode_length': (Array(875.6875, dtype=float32), Array(328.9005, dtype=float32)), 'eval/epoch_eval_time': 4.134371757507324, 'eval/sps': 30959.963812536575}
I0727 03:05:30.648809 140120985872192 train.py:379] starting iteration 141 1418.360557794571
I0727 03:05:40.545626 140120985872192 train.py:394] {'eval/walltime': 591.3065853118896, 'training/sps': 42274.402787080566, 'training/walltime': 828.3105049133301, 'training/entropy_loss': Array(-0.02985354, dtype=float32), 'training/policy_loss': Array(0.00633934, dtype=float32), 'training/total_loss': Array(67.39142, dtype=float32), 'training/v_loss': Array(67.41493, dtype=float32), 'eval/episode_goal_distance': (Array(0.44068167, dtype=float32), Array(0.18853557, dtype=float32)), 'eval/episode_reward': (Array(-8430.818, dtype=float32), Array(5599.9565, dtype=float32)), 'eval/avg_episode_length': (Array(766.9297, dtype=float32), Array(421.25043, dtype=float32)), 'eval/epoch_eval_time': 4.0799994468688965, 'eval/sps': 31372.55327282721}
I0727 03:05:40.548131 140120985872192 train.py:379] starting iteration 142 1428.2598798274994
I0727 03:05:50.452884 140120985872192 train.py:394] {'eval/walltime': 595.4065034389496, 'training/sps': 42362.40296765934, 'training/walltime': 834.1118755340576, 'training/entropy_loss': Array(-0.03056807, dtype=float32), 'training/policy_loss': Array(0.00271807, dtype=float32), 'training/total_loss': Array(45.853516, dtype=float32), 'training/v_loss': Array(45.881367, dtype=float32), 'eval/episode_goal_distance': (Array(0.44667202, dtype=float32), Array(0.21102662, dtype=float32)), 'eval/episode_reward': (Array(-8184.161, dtype=float32), Array(6382.49, dtype=float32)), 'eval/avg_episode_length': (Array(712.52344, dtype=float32), Array(450.84103, dtype=float32)), 'eval/epoch_eval_time': 4.0999181270599365, 'eval/sps': 31220.135630315424}
I0727 03:05:50.457368 140120985872192 train.py:379] starting iteration 143 1438.1691024303436
I0727 03:06:00.310143 140120985872192 train.py:394] {'eval/walltime': 599.4797539710999, 'training/sps': 42551.95051650981, 'training/walltime': 839.8874039649963, 'training/entropy_loss': Array(-0.02720629, dtype=float32), 'training/policy_loss': Array(0.00287013, dtype=float32), 'training/total_loss': Array(30.351612, dtype=float32), 'training/v_loss': Array(30.37595, dtype=float32), 'eval/episode_goal_distance': (Array(0.4401262, dtype=float32), Array(0.17558369, dtype=float32)), 'eval/episode_reward': (Array(-8643.936, dtype=float32), Array(5494.475, dtype=float32)), 'eval/avg_episode_length': (Array(782.4922, dtype=float32), Array(411.05182, dtype=float32)), 'eval/epoch_eval_time': 4.0732505321502686, 'eval/sps': 31424.534039753456}
I0727 03:06:00.312403 140120985872192 train.py:379] starting iteration 144 1448.0241520404816
I0727 03:06:10.250440 140120985872192 train.py:394] {'eval/walltime': 603.5738067626953, 'training/sps': 42078.00602616498, 'training/walltime': 845.7279849052429, 'training/entropy_loss': Array(-0.02573348, dtype=float32), 'training/policy_loss': Array(0.00295472, dtype=float32), 'training/total_loss': Array(27.67735, dtype=float32), 'training/v_loss': Array(27.70013, dtype=float32), 'eval/episode_goal_distance': (Array(0.45839292, dtype=float32), Array(0.1954993, dtype=float32)), 'eval/episode_reward': (Array(-8684.419, dtype=float32), Array(5587.494, dtype=float32)), 'eval/avg_episode_length': (Array(774.78125, dtype=float32), Array(416.12543, dtype=float32)), 'eval/epoch_eval_time': 4.094052791595459, 'eval/sps': 31264.863086955505}
I0727 03:06:10.252748 140120985872192 train.py:379] starting iteration 145 1457.9644975662231
I0727 03:06:20.185755 140120985872192 train.py:394] {'eval/walltime': 607.7020995616913, 'training/sps': 42362.9618248367, 'training/walltime': 851.5292789936066, 'training/entropy_loss': Array(-0.02783831, dtype=float32), 'training/policy_loss': Array(0.00429871, dtype=float32), 'training/total_loss': Array(37.296898, dtype=float32), 'training/v_loss': Array(37.32044, dtype=float32), 'eval/episode_goal_distance': (Array(0.47929913, dtype=float32), Array(0.19661973, dtype=float32)), 'eval/episode_reward': (Array(-9607.752, dtype=float32), Array(5465.7065, dtype=float32)), 'eval/avg_episode_length': (Array(852.3906, dtype=float32), Array(353.55032, dtype=float32)), 'eval/epoch_eval_time': 4.128292798995972, 'eval/sps': 31005.55271446116}
I0727 03:06:20.188086 140120985872192 train.py:379] starting iteration 146 1467.8998348712921
I0727 03:06:30.076261 140120985872192 train.py:394] {'eval/walltime': 611.7775349617004, 'training/sps': 42305.362292288446, 'training/walltime': 857.3384716510773, 'training/entropy_loss': Array(-0.02485038, dtype=float32), 'training/policy_loss': Array(0.00274379, dtype=float32), 'training/total_loss': Array(26.434889, dtype=float32), 'training/v_loss': Array(26.456995, dtype=float32), 'eval/episode_goal_distance': (Array(0.44907463, dtype=float32), Array(0.20644872, dtype=float32)), 'eval/episode_reward': (Array(-9176.707, dtype=float32), Array(5109.907, dtype=float32)), 'eval/avg_episode_length': (Array(844.5703, dtype=float32), Array(361.1865, dtype=float32)), 'eval/epoch_eval_time': 4.075435400009155, 'eval/sps': 31407.687139320733}
I0727 03:06:30.078607 140120985872192 train.py:379] starting iteration 147 1477.7903559207916
I0727 03:06:39.986313 140120985872192 train.py:394] {'eval/walltime': 615.8725719451904, 'training/sps': 42306.10196045211, 'training/walltime': 863.1475627422333, 'training/entropy_loss': Array(-0.02116334, dtype=float32), 'training/policy_loss': Array(0.00402937, dtype=float32), 'training/total_loss': Array(26.852442, dtype=float32), 'training/v_loss': Array(26.869576, dtype=float32), 'eval/episode_goal_distance': (Array(0.41718483, dtype=float32), Array(0.19431628, dtype=float32)), 'eval/episode_reward': (Array(-8327.223, dtype=float32), Array(5210.4272, dtype=float32)), 'eval/avg_episode_length': (Array(805.7344, dtype=float32), Array(394.317, dtype=float32)), 'eval/epoch_eval_time': 4.09503698348999, 'eval/sps': 31257.348960719803}
I0727 03:06:39.988502 140120985872192 train.py:379] starting iteration 148 1487.7002515792847
I0727 03:06:49.871249 140120985872192 train.py:394] {'eval/walltime': 619.963526725769, 'training/sps': 42458.30476801578, 'training/walltime': 868.9358296394348, 'training/entropy_loss': Array(-0.01795121, dtype=float32), 'training/policy_loss': Array(0.00392065, dtype=float32), 'training/total_loss': Array(26.759647, dtype=float32), 'training/v_loss': Array(26.77368, dtype=float32), 'eval/episode_goal_distance': (Array(0.45174235, dtype=float32), Array(0.20214565, dtype=float32)), 'eval/episode_reward': (Array(-8913.486, dtype=float32), Array(4889.6274, dtype=float32)), 'eval/avg_episode_length': (Array(837.0078, dtype=float32), Array(367.91718, dtype=float32)), 'eval/epoch_eval_time': 4.090954780578613, 'eval/sps': 31288.53944014899}
I0727 03:06:49.873571 140120985872192 train.py:379] starting iteration 149 1497.5853197574615
I0727 03:06:59.846630 140120985872192 train.py:394] {'eval/walltime': 624.0697033405304, 'training/sps': 41913.95468980794, 'training/walltime': 874.7992706298828, 'training/entropy_loss': Array(-0.0165503, dtype=float32), 'training/policy_loss': Array(0.0034687, dtype=float32), 'training/total_loss': Array(22.811947, dtype=float32), 'training/v_loss': Array(22.82503, dtype=float32), 'eval/episode_goal_distance': (Array(0.44664496, dtype=float32), Array(0.20936456, dtype=float32)), 'eval/episode_reward': (Array(-8157.029, dtype=float32), Array(6134.734, dtype=float32)), 'eval/avg_episode_length': (Array(735.59375, dtype=float32), Array(439.63947, dtype=float32)), 'eval/epoch_eval_time': 4.1061766147613525, 'eval/sps': 31172.551014939538}
I0727 03:06:59.849091 140120985872192 train.py:379] starting iteration 150 1507.5608403682709
I0727 03:07:09.733994 140120985872192 train.py:394] {'eval/walltime': 628.1432340145111, 'training/sps': 42314.780234249476, 'training/walltime': 880.607170343399, 'training/entropy_loss': Array(-0.02141338, dtype=float32), 'training/policy_loss': Array(0.00425105, dtype=float32), 'training/total_loss': Array(94.65392, dtype=float32), 'training/v_loss': Array(94.67108, dtype=float32), 'eval/episode_goal_distance': (Array(0.41854405, dtype=float32), Array(0.1724446, dtype=float32)), 'eval/episode_reward': (Array(-8256.532, dtype=float32), Array(5045.1924, dtype=float32)), 'eval/avg_episode_length': (Array(805.78125, dtype=float32), Array(394.2215, dtype=float32)), 'eval/epoch_eval_time': 4.073530673980713, 'eval/sps': 31422.37293500396}
I0727 03:07:09.738698 140120985872192 train.py:379] starting iteration 151 1517.4504323005676
I0727 03:07:19.673075 140120985872192 train.py:394] {'eval/walltime': 632.2303245067596, 'training/sps': 42056.949371786875, 'training/walltime': 886.4506754875183, 'training/entropy_loss': Array(-0.01880049, dtype=float32), 'training/policy_loss': Array(0.00396561, dtype=float32), 'training/total_loss': Array(27.178013, dtype=float32), 'training/v_loss': Array(27.192848, dtype=float32), 'eval/episode_goal_distance': (Array(0.4285725, dtype=float32), Array(0.17554438, dtype=float32)), 'eval/episode_reward': (Array(-7959.9355, dtype=float32), Array(5026.9375, dtype=float32)), 'eval/avg_episode_length': (Array(782.3281, dtype=float32), Array(411.36185, dtype=float32)), 'eval/epoch_eval_time': 4.087090492248535, 'eval/sps': 31318.12232754849}
I0727 03:07:19.675315 140120985872192 train.py:379] starting iteration 152 1527.3870646953583
I0727 03:07:29.616209 140120985872192 train.py:394] {'eval/walltime': 636.3430123329163, 'training/sps': 42192.12910398634, 'training/walltime': 892.275458574295, 'training/entropy_loss': Array(-0.01277057, dtype=float32), 'training/policy_loss': Array(0.00403389, dtype=float32), 'training/total_loss': Array(22.70752, dtype=float32), 'training/v_loss': Array(22.716255, dtype=float32), 'eval/episode_goal_distance': (Array(0.44144443, dtype=float32), Array(0.18350837, dtype=float32)), 'eval/episode_reward': (Array(-8130.781, dtype=float32), Array(5648.539, dtype=float32)), 'eval/avg_episode_length': (Array(751.3047, dtype=float32), Array(430.7535, dtype=float32)), 'eval/epoch_eval_time': 4.112687826156616, 'eval/sps': 31123.19860163527}
I0727 03:07:29.618627 140120985872192 train.py:379] starting iteration 153 1537.3303761482239
I0727 03:07:39.516927 140120985872192 train.py:394] {'eval/walltime': 640.4258251190186, 'training/sps': 42287.67873131403, 'training/walltime': 898.0870804786682, 'training/entropy_loss': Array(-0.00915185, dtype=float32), 'training/policy_loss': Array(0.00506824, dtype=float32), 'training/total_loss': Array(20.91603, dtype=float32), 'training/v_loss': Array(20.920113, dtype=float32), 'eval/episode_goal_distance': (Array(0.44765598, dtype=float32), Array(0.15787435, dtype=float32)), 'eval/episode_reward': (Array(-9537.154, dtype=float32), Array(4800.207, dtype=float32)), 'eval/avg_episode_length': (Array(875.75, dtype=float32), Array(328.73575, dtype=float32)), 'eval/epoch_eval_time': 4.082812786102295, 'eval/sps': 31350.93542268361}
I0727 03:07:39.657619 140120985872192 train.py:379] starting iteration 154 1547.3693506717682
I0727 03:07:49.607822 140120985872192 train.py:394] {'eval/walltime': 644.5256180763245, 'training/sps': 42039.520837168704, 'training/walltime': 903.9330081939697, 'training/entropy_loss': Array(-0.00345902, dtype=float32), 'training/policy_loss': Array(0.00680377, dtype=float32), 'training/total_loss': Array(19.51611, dtype=float32), 'training/v_loss': Array(19.512764, dtype=float32), 'eval/episode_goal_distance': (Array(0.39232278, dtype=float32), Array(0.1676786, dtype=float32)), 'eval/episode_reward': (Array(-7882.6484, dtype=float32), Array(4816.334, dtype=float32)), 'eval/avg_episode_length': (Array(805.78906, dtype=float32), Array(394.20605, dtype=float32)), 'eval/epoch_eval_time': 4.099792957305908, 'eval/sps': 31221.08880447282}
I0727 03:07:49.610358 140120985872192 train.py:379] starting iteration 155 1557.3220963478088
I0727 03:07:59.498243 140120985872192 train.py:394] {'eval/walltime': 648.6099643707275, 'training/sps': 42375.87709915426, 'training/walltime': 909.7325341701508, 'training/entropy_loss': Array(-0.00232702, dtype=float32), 'training/policy_loss': Array(0.00676723, dtype=float32), 'training/total_loss': Array(19.24416, dtype=float32), 'training/v_loss': Array(19.23972, dtype=float32), 'eval/episode_goal_distance': (Array(0.3971462, dtype=float32), Array(0.15618367, dtype=float32)), 'eval/episode_reward': (Array(-7751.6143, dtype=float32), Array(4797.983, dtype=float32)), 'eval/avg_episode_length': (Array(805.78125, dtype=float32), Array(394.2217, dtype=float32)), 'eval/epoch_eval_time': 4.084346294403076, 'eval/sps': 31339.1644032248}
I0727 03:07:59.500575 140120985872192 train.py:379] starting iteration 156 1567.212323665619
I0727 03:08:09.430966 140120985872192 train.py:394] {'eval/walltime': 652.7045199871063, 'training/sps': 42140.48091043755, 'training/walltime': 915.5644562244415, 'training/entropy_loss': Array(-0.003382, dtype=float32), 'training/policy_loss': Array(0.00908437, dtype=float32), 'training/total_loss': Array(38.682335, dtype=float32), 'training/v_loss': Array(38.676632, dtype=float32), 'eval/episode_goal_distance': (Array(0.43217438, dtype=float32), Array(0.19570766, dtype=float32)), 'eval/episode_reward': (Array(-9563.148, dtype=float32), Array(4173.0166, dtype=float32)), 'eval/avg_episode_length': (Array(937.8828, dtype=float32), Array(240.57939, dtype=float32)), 'eval/epoch_eval_time': 4.094555616378784, 'eval/sps': 31261.023659803872}
I0727 03:08:09.433279 140120985872192 train.py:379] starting iteration 157 1577.1450281143188
I0727 03:08:19.351985 140120985872192 train.py:394] {'eval/walltime': 656.8071439266205, 'training/sps': 42284.40189390275, 'training/walltime': 921.3765285015106, 'training/entropy_loss': Array(-0.00258901, dtype=float32), 'training/policy_loss': Array(0.00552065, dtype=float32), 'training/total_loss': Array(22.365902, dtype=float32), 'training/v_loss': Array(22.362968, dtype=float32), 'eval/episode_goal_distance': (Array(0.45400846, dtype=float32), Array(0.20719829, dtype=float32)), 'eval/episode_reward': (Array(-9628.982, dtype=float32), Array(4480.744, dtype=float32)), 'eval/avg_episode_length': (Array(930.03906, dtype=float32), Array(254.3951, dtype=float32)), 'eval/epoch_eval_time': 4.10262393951416, 'eval/sps': 31199.544946631882}
I0727 03:08:19.354418 140120985872192 train.py:379] starting iteration 158 1587.066166639328
I0727 03:08:29.255635 140120985872192 train.py:394] {'eval/walltime': 660.8799090385437, 'training/sps': 42194.78021215114, 'training/walltime': 927.2009456157684, 'training/entropy_loss': Array(-0.01148052, dtype=float32), 'training/policy_loss': Array(0.00365611, dtype=float32), 'training/total_loss': Array(50.927155, dtype=float32), 'training/v_loss': Array(50.934982, dtype=float32), 'eval/episode_goal_distance': (Array(0.43389636, dtype=float32), Array(0.20646892, dtype=float32)), 'eval/episode_reward': (Array(-9283.59, dtype=float32), Array(4887.8643, dtype=float32)), 'eval/avg_episode_length': (Array(875.8125, dtype=float32), Array(328.5697, dtype=float32)), 'eval/epoch_eval_time': 4.072765111923218, 'eval/sps': 31428.279432387048}
I0727 03:08:29.258115 140120985872192 train.py:379] starting iteration 159 1596.969863653183
I0727 03:08:39.170469 140120985872192 train.py:394] {'eval/walltime': 664.9730699062347, 'training/sps': 42261.992855914046, 'training/walltime': 933.016099691391, 'training/entropy_loss': Array(-0.01658334, dtype=float32), 'training/policy_loss': Array(0.00309031, dtype=float32), 'training/total_loss': Array(30.211548, dtype=float32), 'training/v_loss': Array(30.225039, dtype=float32), 'eval/episode_goal_distance': (Array(0.44679964, dtype=float32), Array(0.19129162, dtype=float32)), 'eval/episode_reward': (Array(-9466.439, dtype=float32), Array(4537.193, dtype=float32)), 'eval/avg_episode_length': (Array(906.8828, dtype=float32), Array(289.5135, dtype=float32)), 'eval/epoch_eval_time': 4.09316086769104, 'eval/sps': 31271.67588509783}
I0727 03:08:39.172919 140120985872192 train.py:379] starting iteration 160 1606.8846678733826
I0727 03:08:49.070911 140120985872192 train.py:394] {'eval/walltime': 669.0454487800598, 'training/sps': 42214.965460089494, 'training/walltime': 938.8377318382263, 'training/entropy_loss': Array(-0.01388827, dtype=float32), 'training/policy_loss': Array(0.00355331, dtype=float32), 'training/total_loss': Array(22.92981, dtype=float32), 'training/v_loss': Array(22.940145, dtype=float32), 'eval/episode_goal_distance': (Array(0.39566687, dtype=float32), Array(0.15044728, dtype=float32)), 'eval/episode_reward': (Array(-8177.783, dtype=float32), Array(4337.189, dtype=float32)), 'eval/avg_episode_length': (Array(844.7344, dtype=float32), Array(360.8056, dtype=float32)), 'eval/epoch_eval_time': 4.072378873825073, 'eval/sps': 31431.260196027175}
I0727 03:08:49.073215 140120985872192 train.py:379] starting iteration 161 1616.7849638462067
I0727 03:08:58.982443 140120985872192 train.py:394] {'eval/walltime': 673.1300945281982, 'training/sps': 42221.15917297507, 'training/walltime': 944.6585099697113, 'training/entropy_loss': Array(-0.01152808, dtype=float32), 'training/policy_loss': Array(0.00405353, dtype=float32), 'training/total_loss': Array(21.387259, dtype=float32), 'training/v_loss': Array(21.394733, dtype=float32), 'eval/episode_goal_distance': (Array(0.4324491, dtype=float32), Array(0.18552835, dtype=float32)), 'eval/episode_reward': (Array(-8861.958, dtype=float32), Array(4771.105, dtype=float32)), 'eval/avg_episode_length': (Array(860.2578, dtype=float32), Array(345.45282, dtype=float32)), 'eval/epoch_eval_time': 4.084645748138428, 'eval/sps': 31336.86686497497}
I0727 03:08:58.984760 140120985872192 train.py:379] starting iteration 162 1626.6965086460114
I0727 03:09:08.896369 140120985872192 train.py:394] {'eval/walltime': 677.2283980846405, 'training/sps': 42305.15741225409, 'training/walltime': 950.4677307605743, 'training/entropy_loss': Array(-0.01147739, dtype=float32), 'training/policy_loss': Array(0.00516583, dtype=float32), 'training/total_loss': Array(20.700813, dtype=float32), 'training/v_loss': Array(20.707125, dtype=float32), 'eval/episode_goal_distance': (Array(0.42708415, dtype=float32), Array(0.17254178, dtype=float32)), 'eval/episode_reward': (Array(-9128.985, dtype=float32), Array(4352.211, dtype=float32)), 'eval/avg_episode_length': (Array(899.02344, dtype=float32), Array(300.32986, dtype=float32)), 'eval/epoch_eval_time': 4.098303556442261, 'eval/sps': 31232.43513741009}
I0727 03:09:08.898848 140120985872192 train.py:379] starting iteration 163 1636.6105971336365
I0727 03:09:18.777187 140120985872192 train.py:394] {'eval/walltime': 681.3036649227142, 'training/sps': 42379.73439939652, 'training/walltime': 956.2667288780212, 'training/entropy_loss': Array(-0.01215555, dtype=float32), 'training/policy_loss': Array(0.00484828, dtype=float32), 'training/total_loss': Array(22.272366, dtype=float32), 'training/v_loss': Array(22.279673, dtype=float32), 'eval/episode_goal_distance': (Array(0.413831, dtype=float32), Array(0.19334233, dtype=float32)), 'eval/episode_reward': (Array(-8972.252, dtype=float32), Array(4727.4624, dtype=float32)), 'eval/avg_episode_length': (Array(891.21875, dtype=float32), Array(310.4151, dtype=float32)), 'eval/epoch_eval_time': 4.0752668380737305, 'eval/sps': 31408.986229844566}
I0727 03:09:18.779575 140120985872192 train.py:379] starting iteration 164 1646.4913239479065
I0727 03:09:28.731478 140120985872192 train.py:394] {'eval/walltime': 685.4070069789886, 'training/sps': 42048.66811583656, 'training/walltime': 962.1113848686218, 'training/entropy_loss': Array(-0.01274712, dtype=float32), 'training/policy_loss': Array(0.0046665, dtype=float32), 'training/total_loss': Array(22.76728, dtype=float32), 'training/v_loss': Array(22.775362, dtype=float32), 'eval/episode_goal_distance': (Array(0.47108817, dtype=float32), Array(0.20752953, dtype=float32)), 'eval/episode_reward': (Array(-9815.975, dtype=float32), Array(4465.1567, dtype=float32)), 'eval/avg_episode_length': (Array(922.4219, dtype=float32), Array(266.48996, dtype=float32)), 'eval/epoch_eval_time': 4.103342056274414, 'eval/sps': 31194.084783713166}
I0727 03:09:28.733916 140120985872192 train.py:379] starting iteration 165 1656.44566488266
I0727 03:09:38.656938 140120985872192 train.py:394] {'eval/walltime': 689.4820392131805, 'training/sps': 42064.18677489939, 'training/walltime': 967.953884601593, 'training/entropy_loss': Array(-0.0154707, dtype=float32), 'training/policy_loss': Array(0.00478568, dtype=float32), 'training/total_loss': Array(45.80355, dtype=float32), 'training/v_loss': Array(45.81423, dtype=float32), 'eval/episode_goal_distance': (Array(0.4839764, dtype=float32), Array(0.17708273, dtype=float32)), 'eval/episode_reward': (Array(-10054.857, dtype=float32), Array(4829.3696, dtype=float32)), 'eval/avg_episode_length': (Array(891.2422, dtype=float32), Array(310.34854, dtype=float32)), 'eval/epoch_eval_time': 4.0750322341918945, 'eval/sps': 31410.79447814067}
I0727 03:09:38.659227 140120985872192 train.py:379] starting iteration 166 1666.3709757328033
I0727 03:09:48.562909 140120985872192 train.py:394] {'eval/walltime': 693.5688781738281, 'training/sps': 42274.50507796215, 'training/walltime': 973.767317533493, 'training/entropy_loss': Array(-0.02084756, dtype=float32), 'training/policy_loss': Array(0.01082337, dtype=float32), 'training/total_loss': Array(55.609715, dtype=float32), 'training/v_loss': Array(55.61974, dtype=float32), 'eval/episode_goal_distance': (Array(0.46124476, dtype=float32), Array(0.22612023, dtype=float32)), 'eval/episode_reward': (Array(-9657.736, dtype=float32), Array(4881.833, dtype=float32)), 'eval/avg_episode_length': (Array(914.5703, dtype=float32), Array(278.61594, dtype=float32)), 'eval/epoch_eval_time': 4.086838960647583, 'eval/sps': 31320.049855773537}
I0727 03:09:48.567568 140120985872192 train.py:379] starting iteration 167 1676.2793021202087
I0727 03:09:58.477713 140120985872192 train.py:394] {'eval/walltime': 697.6502251625061, 'training/sps': 42190.80108449358, 'training/walltime': 979.5922839641571, 'training/entropy_loss': Array(-0.02841407, dtype=float32), 'training/policy_loss': Array(0.00245436, dtype=float32), 'training/total_loss': Array(39.371815, dtype=float32), 'training/v_loss': Array(39.397774, dtype=float32), 'eval/episode_goal_distance': (Array(0.45987374, dtype=float32), Array(0.19718991, dtype=float32)), 'eval/episode_reward': (Array(-9382.901, dtype=float32), Array(4815.3203, dtype=float32)), 'eval/avg_episode_length': (Array(883.5078, dtype=float32), Array(319.73605, dtype=float32)), 'eval/epoch_eval_time': 4.0813469886779785, 'eval/sps': 31362.194970210436}
I0727 03:09:58.480166 140120985872192 train.py:379] starting iteration 168 1686.1919152736664
I0727 03:10:08.406662 140120985872192 train.py:394] {'eval/walltime': 701.7349700927734, 'training/sps': 42095.27567347441, 'training/walltime': 985.4304687976837, 'training/entropy_loss': Array(-0.02896418, dtype=float32), 'training/policy_loss': Array(0.00257781, dtype=float32), 'training/total_loss': Array(26.17395, dtype=float32), 'training/v_loss': Array(26.200336, dtype=float32), 'eval/episode_goal_distance': (Array(0.46269673, dtype=float32), Array(0.20858298, dtype=float32)), 'eval/episode_reward': (Array(-10042.162, dtype=float32), Array(4697.69, dtype=float32)), 'eval/avg_episode_length': (Array(914.5547, dtype=float32), Array(278.6669, dtype=float32)), 'eval/epoch_eval_time': 4.084744930267334, 'eval/sps': 31336.105971156147}
I0727 03:10:08.408963 140120985872192 train.py:379] starting iteration 169 1696.1207122802734
I0727 03:10:18.332076 140120985872192 train.py:394] {'eval/walltime': 705.8381118774414, 'training/sps': 42252.764662319016, 'training/walltime': 991.2468929290771, 'training/entropy_loss': Array(-0.02898743, dtype=float32), 'training/policy_loss': Array(0.00253884, dtype=float32), 'training/total_loss': Array(22.769775, dtype=float32), 'training/v_loss': Array(22.796227, dtype=float32), 'eval/episode_goal_distance': (Array(0.46871123, dtype=float32), Array(0.2219447, dtype=float32)), 'eval/episode_reward': (Array(-9526.701, dtype=float32), Array(4855.5996, dtype=float32)), 'eval/avg_episode_length': (Array(899.0547, dtype=float32), Array(300.23715, dtype=float32)), 'eval/epoch_eval_time': 4.103141784667969, 'eval/sps': 31195.607346129746}
I0727 03:10:18.334350 140120985872192 train.py:379] starting iteration 170 1706.046094417572
I0727 03:10:28.223976 140120985872192 train.py:394] {'eval/walltime': 709.9169976711273, 'training/sps': 42318.84532882071, 'training/walltime': 997.0542347431183, 'training/entropy_loss': Array(-0.02878477, dtype=float32), 'training/policy_loss': Array(0.00608907, dtype=float32), 'training/total_loss': Array(22.034946, dtype=float32), 'training/v_loss': Array(22.057644, dtype=float32), 'eval/episode_goal_distance': (Array(0.43062037, dtype=float32), Array(0.21175984, dtype=float32)), 'eval/episode_reward': (Array(-9067.075, dtype=float32), Array(4948.366, dtype=float32)), 'eval/avg_episode_length': (Array(898.97656, dtype=float32), Array(300.46942, dtype=float32)), 'eval/epoch_eval_time': 4.078885793685913, 'eval/sps': 31381.118882549523}
I0727 03:10:28.226147 140120985872192 train.py:379] starting iteration 171 1715.9378962516785
I0727 03:10:38.360016 140120985872192 train.py:394] {'eval/walltime': 714.0210201740265, 'training/sps': 40785.38462892872, 'training/walltime': 1003.0799226760864, 'training/entropy_loss': Array(-0.02818429, dtype=float32), 'training/policy_loss': Array(0.00570601, dtype=float32), 'training/total_loss': Array(21.551079, dtype=float32), 'training/v_loss': Array(21.573559, dtype=float32), 'eval/episode_goal_distance': (Array(0.43767917, dtype=float32), Array(0.18391056, dtype=float32)), 'eval/episode_reward': (Array(-9113.818, dtype=float32), Array(4773.035, dtype=float32)), 'eval/avg_episode_length': (Array(875.7422, dtype=float32), Array(328.75574, dtype=float32)), 'eval/epoch_eval_time': 4.10402250289917, 'eval/sps': 31188.9128067836}
I0727 03:10:38.362365 140120985872192 train.py:379] starting iteration 172 1726.074114561081
I0727 03:10:48.273238 140120985872192 train.py:394] {'eval/walltime': 718.1178002357483, 'training/sps': 42296.12035758794, 'training/walltime': 1008.8903846740723, 'training/entropy_loss': Array(-0.0302033, dtype=float32), 'training/policy_loss': Array(0.00625081, dtype=float32), 'training/total_loss': Array(22.3215, dtype=float32), 'training/v_loss': Array(22.345453, dtype=float32), 'eval/episode_goal_distance': (Array(0.44923514, dtype=float32), Array(0.20717472, dtype=float32)), 'eval/episode_reward': (Array(-8867.607, dtype=float32), Array(4904.7275, dtype=float32)), 'eval/avg_episode_length': (Array(844.5703, dtype=float32), Array(361.18646, dtype=float32)), 'eval/epoch_eval_time': 4.096780061721802, 'eval/sps': 31244.0497345625}
I0727 03:10:48.275768 140120985872192 train.py:379] starting iteration 173 1735.9875166416168
I0727 03:10:58.256335 140120985872192 train.py:394] {'eval/walltime': 722.2269253730774, 'training/sps': 41882.717706870426, 'training/walltime': 1014.7581987380981, 'training/entropy_loss': Array(-0.03175387, dtype=float32), 'training/policy_loss': Array(0.00681982, dtype=float32), 'training/total_loss': Array(47.717434, dtype=float32), 'training/v_loss': Array(47.742367, dtype=float32), 'eval/episode_goal_distance': (Array(0.47005957, dtype=float32), Array(0.20890969, dtype=float32)), 'eval/episode_reward': (Array(-10239.875, dtype=float32), Array(4468.935, dtype=float32)), 'eval/avg_episode_length': (Array(930.08594, dtype=float32), Array(254.22533, dtype=float32)), 'eval/epoch_eval_time': 4.109125137329102, 'eval/sps': 31150.18300055933}
I0727 03:10:58.258658 140120985872192 train.py:379] starting iteration 174 1745.9704070091248
I0727 03:11:08.137870 140120985872192 train.py:394] {'eval/walltime': 726.3029632568359, 'training/sps': 42374.36677587563, 'training/walltime': 1020.5579314231873, 'training/entropy_loss': Array(-0.03210215, dtype=float32), 'training/policy_loss': Array(0.00427048, dtype=float32), 'training/total_loss': Array(28.492186, dtype=float32), 'training/v_loss': Array(28.520018, dtype=float32), 'eval/episode_goal_distance': (Array(0.4756844, dtype=float32), Array(0.24644707, dtype=float32)), 'eval/episode_reward': (Array(-10651.445, dtype=float32), Array(5064.501, dtype=float32)), 'eval/avg_episode_length': (Array(922.3281, dtype=float32), Array(266.8121, dtype=float32)), 'eval/epoch_eval_time': 4.076037883758545, 'eval/sps': 31403.044733718285}
I0727 03:11:08.140069 140120985872192 train.py:379] starting iteration 175 1755.8518183231354
I0727 03:11:18.068202 140120985872192 train.py:394] {'eval/walltime': 730.3831162452698, 'training/sps': 42049.620114198144, 'training/walltime': 1026.4024550914764, 'training/entropy_loss': Array(-0.03744909, dtype=float32), 'training/policy_loss': Array(0.00151598, dtype=float32), 'training/total_loss': Array(56.431847, dtype=float32), 'training/v_loss': Array(56.46778, dtype=float32), 'eval/episode_goal_distance': (Array(0.45118296, dtype=float32), Array(0.19595778, dtype=float32)), 'eval/episode_reward': (Array(-9985.711, dtype=float32), Array(4097.8965, dtype=float32)), 'eval/avg_episode_length': (Array(945.6406, dtype=float32), Array(226.00525, dtype=float32)), 'eval/epoch_eval_time': 4.080152988433838, 'eval/sps': 31371.3726820652}
I0727 03:11:18.070431 140120985872192 train.py:379] starting iteration 176 1765.782180070877
I0727 03:11:27.978722 140120985872192 train.py:394] {'eval/walltime': 734.4912011623383, 'training/sps': 42397.34508008133, 'training/walltime': 1032.1990444660187, 'training/entropy_loss': Array(-0.03614166, dtype=float32), 'training/policy_loss': Array(0.0033848, dtype=float32), 'training/total_loss': Array(26.251774, dtype=float32), 'training/v_loss': Array(26.28453, dtype=float32), 'eval/episode_goal_distance': (Array(0.42753804, dtype=float32), Array(0.16964749, dtype=float32)), 'eval/episode_reward': (Array(-9494.301, dtype=float32), Array(4235.678, dtype=float32)), 'eval/avg_episode_length': (Array(922.33594, dtype=float32), Array(266.78522, dtype=float32)), 'eval/epoch_eval_time': 4.1080849170684814, 'eval/sps': 31158.070629986018}
I0727 03:11:27.980938 140120985872192 train.py:379] starting iteration 177 1775.692687511444
I0727 03:11:37.910137 140120985872192 train.py:394] {'eval/walltime': 738.5770509243011, 'training/sps': 42083.72835603856, 'training/walltime': 1038.0388312339783, 'training/entropy_loss': Array(-0.03481731, dtype=float32), 'training/policy_loss': Array(0.0020766, dtype=float32), 'training/total_loss': Array(21.284382, dtype=float32), 'training/v_loss': Array(21.31712, dtype=float32), 'eval/episode_goal_distance': (Array(0.4322257, dtype=float32), Array(0.1744496, dtype=float32)), 'eval/episode_reward': (Array(-9365.453, dtype=float32), Array(4058.1665, dtype=float32)), 'eval/avg_episode_length': (Array(930.1406, dtype=float32), Array(254.02554, dtype=float32)), 'eval/epoch_eval_time': 4.085849761962891, 'eval/sps': 31327.6325506661}
I0727 03:11:37.912488 140120985872192 train.py:379] starting iteration 178 1785.6242377758026
I0727 03:11:47.854654 140120985872192 train.py:394] {'eval/walltime': 742.671290397644, 'training/sps': 42050.92896790129, 'training/walltime': 1043.8831729888916, 'training/entropy_loss': Array(-0.03195223, dtype=float32), 'training/policy_loss': Array(0.00277329, dtype=float32), 'training/total_loss': Array(19.247742, dtype=float32), 'training/v_loss': Array(19.27692, dtype=float32), 'eval/episode_goal_distance': (Array(0.41962218, dtype=float32), Array(0.16493003, dtype=float32)), 'eval/episode_reward': (Array(-9160.2295, dtype=float32), Array(4680.217, dtype=float32)), 'eval/avg_episode_length': (Array(891.3203, dtype=float32), Array(310.12564, dtype=float32)), 'eval/epoch_eval_time': 4.0942394733428955, 'eval/sps': 31263.43752811547}
I0727 03:11:47.857141 140120985872192 train.py:379] starting iteration 179 1795.5688905715942
I0727 03:11:57.807215 140120985872192 train.py:394] {'eval/walltime': 746.7701580524445, 'training/sps': 42027.584259057585, 'training/walltime': 1049.730761051178, 'training/entropy_loss': Array(-0.02885371, dtype=float32), 'training/policy_loss': Array(0.00260156, dtype=float32), 'training/total_loss': Array(17.624508, dtype=float32), 'training/v_loss': Array(17.65076, dtype=float32), 'eval/episode_goal_distance': (Array(0.35852304, dtype=float32), Array(0.14795661, dtype=float32)), 'eval/episode_reward': (Array(-8343.413, dtype=float32), Array(3898.102, dtype=float32)), 'eval/avg_episode_length': (Array(922.3047, dtype=float32), Array(266.89255, dtype=float32)), 'eval/epoch_eval_time': 4.098867654800415, 'eval/sps': 31228.13683679003}
I0727 03:11:57.811400 140120985872192 train.py:379] starting iteration 180 1805.523134469986
I0727 03:12:07.724976 140120985872192 train.py:394] {'eval/walltime': 750.8538374900818, 'training/sps': 42183.14890028332, 'training/walltime': 1055.5567841529846, 'training/entropy_loss': Array(-0.02739763, dtype=float32), 'training/policy_loss': Array(0.00494537, dtype=float32), 'training/total_loss': Array(16.342802, dtype=float32), 'training/v_loss': Array(16.365253, dtype=float32), 'eval/episode_goal_distance': (Array(0.3972255, dtype=float32), Array(0.15937695, dtype=float32)), 'eval/episode_reward': (Array(-9050.276, dtype=float32), Array(3637.688, dtype=float32)), 'eval/avg_episode_length': (Array(945.6172, dtype=float32), Array(226.10262, dtype=float32)), 'eval/epoch_eval_time': 4.083679437637329, 'eval/sps': 31344.282026714667}
I0727 03:12:07.727148 140120985872192 train.py:379] starting iteration 181 1815.4388973712921
I0727 03:12:17.633110 140120985872192 train.py:394] {'eval/walltime': 754.9313344955444, 'training/sps': 42190.38490833365, 'training/walltime': 1061.3818080425262, 'training/entropy_loss': Array(-0.02722223, dtype=float32), 'training/policy_loss': Array(0.00570499, dtype=float32), 'training/total_loss': Array(24.024998, dtype=float32), 'training/v_loss': Array(24.046515, dtype=float32), 'eval/episode_goal_distance': (Array(0.37018096, dtype=float32), Array(0.14488304, dtype=float32)), 'eval/episode_reward': (Array(-7695.2036, dtype=float32), Array(4190.5264, dtype=float32)), 'eval/avg_episode_length': (Array(852.4297, dtype=float32), Array(353.45663, dtype=float32)), 'eval/epoch_eval_time': 4.0774970054626465, 'eval/sps': 31391.80723579138}
I0727 03:12:17.635366 140120985872192 train.py:379] starting iteration 182 1825.3471148014069
I0727 03:12:27.522528 140120985872192 train.py:394] {'eval/walltime': 759.0205404758453, 'training/sps': 42413.17283190937, 'training/walltime': 1067.1762342453003, 'training/entropy_loss': Array(-0.02789262, dtype=float32), 'training/policy_loss': Array(0.00539539, dtype=float32), 'training/total_loss': Array(30.49175, dtype=float32), 'training/v_loss': Array(30.51425, dtype=float32), 'eval/episode_goal_distance': (Array(0.35332304, dtype=float32), Array(0.1270188, dtype=float32)), 'eval/episode_reward': (Array(-7804.225, dtype=float32), Array(3992.1367, dtype=float32)), 'eval/avg_episode_length': (Array(860.2031, dtype=float32), Array(345.58792, dtype=float32)), 'eval/epoch_eval_time': 4.089205980300903, 'eval/sps': 31301.92037687012}
I0727 03:12:27.524829 140120985872192 train.py:379] starting iteration 183 1835.2365782260895
I0727 03:12:37.443332 140120985872192 train.py:394] {'eval/walltime': 763.1071791648865, 'training/sps': 42166.28487943881, 'training/walltime': 1073.0045874118805, 'training/entropy_loss': Array(-0.03000412, dtype=float32), 'training/policy_loss': Array(0.00445071, dtype=float32), 'training/total_loss': Array(48.437714, dtype=float32), 'training/v_loss': Array(48.463264, dtype=float32), 'eval/episode_goal_distance': (Array(0.37320706, dtype=float32), Array(0.14020331, dtype=float32)), 'eval/episode_reward': (Array(-8138.333, dtype=float32), Array(4243.4507, dtype=float32)), 'eval/avg_episode_length': (Array(867.96875, dtype=float32), Array(337.3762, dtype=float32)), 'eval/epoch_eval_time': 4.086638689041138, 'eval/sps': 31321.584739861864}
I0727 03:12:37.445545 140120985872192 train.py:379] starting iteration 184 1845.157294511795
I0727 03:12:47.366282 140120985872192 train.py:394] {'eval/walltime': 767.1932663917542, 'training/sps': 42146.9750397482, 'training/walltime': 1078.8356108665466, 'training/entropy_loss': Array(-0.03158338, dtype=float32), 'training/policy_loss': Array(0.0024323, dtype=float32), 'training/total_loss': Array(22.45379, dtype=float32), 'training/v_loss': Array(22.48294, dtype=float32), 'eval/episode_goal_distance': (Array(0.375592, dtype=float32), Array(0.13335349, dtype=float32)), 'eval/episode_reward': (Array(-7598.0664, dtype=float32), Array(4429.284, dtype=float32)), 'eval/avg_episode_length': (Array(821.3906, dtype=float32), Array(381.62427, dtype=float32)), 'eval/epoch_eval_time': 4.086087226867676, 'eval/sps': 31325.811930383727}
I0727 03:12:47.368724 140120985872192 train.py:379] starting iteration 185 1855.0804734230042
I0727 03:12:57.304510 140120985872192 train.py:394] {'eval/walltime': 771.2773520946503, 'training/sps': 42023.25284142217, 'training/walltime': 1084.683801651001, 'training/entropy_loss': Array(-0.02987069, dtype=float32), 'training/policy_loss': Array(0.00248092, dtype=float32), 'training/total_loss': Array(15.45055, dtype=float32), 'training/v_loss': Array(15.47794, dtype=float32), 'eval/episode_goal_distance': (Array(0.39548773, dtype=float32), Array(0.13444747, dtype=float32)), 'eval/episode_reward': (Array(-7856.9014, dtype=float32), Array(4574.027, dtype=float32)), 'eval/avg_episode_length': (Array(821.34375, dtype=float32), Array(381.72446, dtype=float32)), 'eval/epoch_eval_time': 4.084085702896118, 'eval/sps': 31341.164047863218}
I0727 03:12:57.306690 140120985872192 train.py:379] starting iteration 186 1865.0184388160706
I0727 03:13:07.211242 140120985872192 train.py:394] {'eval/walltime': 775.3588814735413, 'training/sps': 42229.527548032456, 'training/walltime': 1090.5034263134003, 'training/entropy_loss': Array(-0.0272968, dtype=float32), 'training/policy_loss': Array(0.00149926, dtype=float32), 'training/total_loss': Array(12.831387, dtype=float32), 'training/v_loss': Array(12.857183, dtype=float32), 'eval/episode_goal_distance': (Array(0.33732337, dtype=float32), Array(0.12588498, dtype=float32)), 'eval/episode_reward': (Array(-7199.4624, dtype=float32), Array(4125.947, dtype=float32)), 'eval/avg_episode_length': (Array(836.7578, dtype=float32), Array(368.48105, dtype=float32)), 'eval/epoch_eval_time': 4.081529378890991, 'eval/sps': 31360.793496181912}
I0727 03:13:07.213696 140120985872192 train.py:379] starting iteration 187 1874.9254446029663
I0727 03:13:17.125916 140120985872192 train.py:394] {'eval/walltime': 779.4321925640106, 'training/sps': 42115.72200045156, 'training/walltime': 1096.3387768268585, 'training/entropy_loss': Array(-0.0248905, dtype=float32), 'training/policy_loss': Array(0.00188573, dtype=float32), 'training/total_loss': Array(11.617447, dtype=float32), 'training/v_loss': Array(11.640452, dtype=float32), 'eval/episode_goal_distance': (Array(0.4014541, dtype=float32), Array(0.15453763, dtype=float32)), 'eval/episode_reward': (Array(-7870.951, dtype=float32), Array(5233.031, dtype=float32)), 'eval/avg_episode_length': (Array(782.5, dtype=float32), Array(411.0374, dtype=float32)), 'eval/epoch_eval_time': 4.07331109046936, 'eval/sps': 31424.066848096}
I0727 03:13:17.128289 140120985872192 train.py:379] starting iteration 188 1884.840038061142
I0727 03:13:27.058902 140120985872192 train.py:394] {'eval/walltime': 783.5018274784088, 'training/sps': 41956.32207135885, 'training/walltime': 1102.196296930313, 'training/entropy_loss': Array(-0.02271045, dtype=float32), 'training/policy_loss': Array(0.00434323, dtype=float32), 'training/total_loss': Array(11.89445, dtype=float32), 'training/v_loss': Array(11.912817, dtype=float32), 'eval/episode_goal_distance': (Array(0.34314728, dtype=float32), Array(0.12675202, dtype=float32)), 'eval/episode_reward': (Array(-7951.9263, dtype=float32), Array(3737.8042, dtype=float32)), 'eval/avg_episode_length': (Array(898.9453, dtype=float32), Array(300.56198, dtype=float32)), 'eval/epoch_eval_time': 4.069634914398193, 'eval/sps': 31452.452785664264}
I0727 03:13:27.061197 140120985872192 train.py:379] starting iteration 189 1894.7729456424713
I0727 03:13:37.022804 140120985872192 train.py:394] {'eval/walltime': 787.624431848526, 'training/sps': 42114.66892870628, 'training/walltime': 1108.0317933559418, 'training/entropy_loss': Array(-0.02066663, dtype=float32), 'training/policy_loss': Array(0.01163219, dtype=float32), 'training/total_loss': Array(12.425512, dtype=float32), 'training/v_loss': Array(12.434547, dtype=float32), 'eval/episode_goal_distance': (Array(0.34194225, dtype=float32), Array(0.13623476, dtype=float32)), 'eval/episode_reward': (Array(-7990.8384, dtype=float32), Array(3710.7922, dtype=float32)), 'eval/avg_episode_length': (Array(906.8203, dtype=float32), Array(289.70776, dtype=float32)), 'eval/epoch_eval_time': 4.1226043701171875, 'eval/sps': 31048.33462260205}
I0727 03:13:37.025229 140120985872192 train.py:379] starting iteration 190 1904.7369785308838
I0727 03:13:46.930245 140120985872192 train.py:394] {'eval/walltime': 791.6971802711487, 'training/sps': 42166.184836477776, 'training/walltime': 1113.8601603507996, 'training/entropy_loss': Array(-0.01922563, dtype=float32), 'training/policy_loss': Array(0.0045123, dtype=float32), 'training/total_loss': Array(26.907282, dtype=float32), 'training/v_loss': Array(26.921997, dtype=float32), 'eval/episode_goal_distance': (Array(0.348631, dtype=float32), Array(0.13301963, dtype=float32)), 'eval/episode_reward': (Array(-8182.7285, dtype=float32), Array(3686.4216, dtype=float32)), 'eval/avg_episode_length': (Array(930.1328, dtype=float32), Array(254.05444, dtype=float32)), 'eval/epoch_eval_time': 4.072748422622681, 'eval/sps': 31428.408219129164}
I0727 03:13:46.932744 140120985872192 train.py:379] starting iteration 191 1914.644493341446
I0727 03:13:56.868112 140120985872192 train.py:394] {'eval/walltime': 795.807599067688, 'training/sps': 42218.74510336852, 'training/walltime': 1119.681271314621, 'training/entropy_loss': Array(-0.01912156, dtype=float32), 'training/policy_loss': Array(0.0033061, dtype=float32), 'training/total_loss': Array(44.324936, dtype=float32), 'training/v_loss': Array(44.34075, dtype=float32), 'eval/episode_goal_distance': (Array(0.358321, dtype=float32), Array(0.12767577, dtype=float32)), 'eval/episode_reward': (Array(-7817.4106, dtype=float32), Array(4151.81, dtype=float32)), 'eval/avg_episode_length': (Array(852.4219, dtype=float32), Array(353.4754, dtype=float32)), 'eval/epoch_eval_time': 4.110418796539307, 'eval/sps': 31140.379201206288}
I0727 03:13:56.870656 140120985872192 train.py:379] starting iteration 192 1924.5824046134949
I0727 03:14:06.793827 140120985872192 train.py:394] {'eval/walltime': 799.8811371326447, 'training/sps': 42039.41968029977, 'training/walltime': 1125.5272130966187, 'training/entropy_loss': Array(-0.01859817, dtype=float32), 'training/policy_loss': Array(0.00297054, dtype=float32), 'training/total_loss': Array(21.05119, dtype=float32), 'training/v_loss': Array(21.066816, dtype=float32), 'eval/episode_goal_distance': (Array(0.35299534, dtype=float32), Array(0.13137548, dtype=float32)), 'eval/episode_reward': (Array(-8102.3486, dtype=float32), Array(4151.019, dtype=float32)), 'eval/avg_episode_length': (Array(883.4922, dtype=float32), Array(319.77866, dtype=float32)), 'eval/epoch_eval_time': 4.073538064956665, 'eval/sps': 31422.315922647867}
I0727 03:14:06.796308 140120985872192 train.py:379] starting iteration 193 1934.5080575942993
I0727 03:14:16.755616 140120985872192 train.py:394] {'eval/walltime': 803.9958047866821, 'training/sps': 42074.11586805036, 'training/walltime': 1131.368334054947, 'training/entropy_loss': Array(-0.01519876, dtype=float32), 'training/policy_loss': Array(0.00312671, dtype=float32), 'training/total_loss': Array(11.946776, dtype=float32), 'training/v_loss': Array(11.958849, dtype=float32), 'eval/episode_goal_distance': (Array(0.34576097, dtype=float32), Array(0.13217345, dtype=float32)), 'eval/episode_reward': (Array(-7688.106, dtype=float32), Array(4141.48, dtype=float32)), 'eval/avg_episode_length': (Array(860.1172, dtype=float32), Array(345.80017, dtype=float32)), 'eval/epoch_eval_time': 4.114667654037476, 'eval/sps': 31108.223254532186}
I0727 03:14:16.757934 140120985872192 train.py:379] starting iteration 194 1944.4696826934814
I0727 03:14:26.653978 140120985872192 train.py:394] {'eval/walltime': 808.073080778122, 'training/sps': 42262.08642309886, 'training/walltime': 1137.1834752559662, 'training/entropy_loss': Array(-0.01355486, dtype=float32), 'training/policy_loss': Array(0.00289916, dtype=float32), 'training/total_loss': Array(9.231567, dtype=float32), 'training/v_loss': Array(9.242223, dtype=float32), 'eval/episode_goal_distance': (Array(0.33856788, dtype=float32), Array(0.12518862, dtype=float32)), 'eval/episode_reward': (Array(-7630.1406, dtype=float32), Array(4077.7207, dtype=float32)), 'eval/avg_episode_length': (Array(875.7344, dtype=float32), Array(328.7764, dtype=float32)), 'eval/epoch_eval_time': 4.077275991439819, 'eval/sps': 31393.50886933681}
I0727 03:14:26.656455 140120985872192 train.py:379] starting iteration 195 1954.3682041168213
I0727 03:14:36.630140 140120985872192 train.py:394] {'eval/walltime': 812.1928658485413, 'training/sps': 42008.87201855675, 'training/walltime': 1143.0336680412292, 'training/entropy_loss': Array(-0.01367957, dtype=float32), 'training/policy_loss': Array(0.00508329, dtype=float32), 'training/total_loss': Array(12.808357, dtype=float32), 'training/v_loss': Array(12.816954, dtype=float32), 'eval/episode_goal_distance': (Array(0.34454834, dtype=float32), Array(0.12168909, dtype=float32)), 'eval/episode_reward': (Array(-8191.32, dtype=float32), Array(3626.8726, dtype=float32)), 'eval/avg_episode_length': (Array(922.3594, dtype=float32), Array(266.70477, dtype=float32)), 'eval/epoch_eval_time': 4.1197850704193115, 'eval/sps': 31069.58198355046}
I0727 03:14:36.632603 140120985872192 train.py:379] starting iteration 196 1964.3443522453308
I0727 03:14:46.530611 140120985872192 train.py:394] {'eval/walltime': 816.2656979560852, 'training/sps': 42216.35551553226, 'training/walltime': 1148.855108499527, 'training/entropy_loss': Array(-0.01233851, dtype=float32), 'training/policy_loss': Array(0.00310778, dtype=float32), 'training/total_loss': Array(9.10847, dtype=float32), 'training/v_loss': Array(9.117701, dtype=float32), 'eval/episode_goal_distance': (Array(0.339168, dtype=float32), Array(0.11847948, dtype=float32)), 'eval/episode_reward': (Array(-8082.715, dtype=float32), Array(3653.329, dtype=float32)), 'eval/avg_episode_length': (Array(922.3281, dtype=float32), Array(266.81198, dtype=float32)), 'eval/epoch_eval_time': 4.072832107543945, 'eval/sps': 31427.76245623056}
I0727 03:14:46.532982 140120985872192 train.py:379] starting iteration 197 1974.2447311878204
I0727 03:14:56.480228 140120985872192 train.py:394] {'eval/walltime': 820.388275384903, 'training/sps': 42219.179131344834, 'training/walltime': 1154.676159620285, 'training/entropy_loss': Array(-0.01057939, dtype=float32), 'training/policy_loss': Array(0.00354799, dtype=float32), 'training/total_loss': Array(9.019238, dtype=float32), 'training/v_loss': Array(9.02627, dtype=float32), 'eval/episode_goal_distance': (Array(0.37669882, dtype=float32), Array(0.14174381, dtype=float32)), 'eval/episode_reward': (Array(-8218.289, dtype=float32), Array(4395.1865, dtype=float32)), 'eval/avg_episode_length': (Array(867.90625, dtype=float32), Array(337.53577, dtype=float32)), 'eval/epoch_eval_time': 4.122577428817749, 'eval/sps': 31048.53752539638}
I0727 03:14:56.482815 140120985872192 train.py:379] starting iteration 198 1984.194563627243
I0727 03:15:06.427471 140120985872192 train.py:394] {'eval/walltime': 824.4741406440735, 'training/sps': 41980.91951213304, 'training/walltime': 1160.5302476882935, 'training/entropy_loss': Array(-0.01133978, dtype=float32), 'training/policy_loss': Array(0.00398686, dtype=float32), 'training/total_loss': Array(21.558275, dtype=float32), 'training/v_loss': Array(21.565628, dtype=float32), 'eval/episode_goal_distance': (Array(0.3509103, dtype=float32), Array(0.12119181, dtype=float32)), 'eval/episode_reward': (Array(-8126.6445, dtype=float32), Array(3711.2695, dtype=float32)), 'eval/avg_episode_length': (Array(906.7031, dtype=float32), Array(290.07224, dtype=float32)), 'eval/epoch_eval_time': 4.085865259170532, 'eval/sps': 31327.51372863068}
I0727 03:15:06.430022 140120985872192 train.py:379] starting iteration 199 1994.1417708396912
I0727 03:15:16.355575 140120985872192 train.py:394] {'eval/walltime': 828.5576770305634, 'training/sps': 42093.01005045646, 'training/walltime': 1166.3687467575073, 'training/entropy_loss': Array(-0.01257728, dtype=float32), 'training/policy_loss': Array(0.00407962, dtype=float32), 'training/total_loss': Array(16.646606, dtype=float32), 'training/v_loss': Array(16.655104, dtype=float32), 'eval/episode_goal_distance': (Array(0.35936177, dtype=float32), Array(0.1352981, dtype=float32)), 'eval/episode_reward': (Array(-8184.9233, dtype=float32), Array(4044.1218, dtype=float32)), 'eval/avg_episode_length': (Array(883.3828, dtype=float32), Array(320.07867, dtype=float32)), 'eval/epoch_eval_time': 4.083536386489868, 'eval/sps': 31345.380054278496}
I0727 03:15:16.357971 140120985872192 train.py:379] starting iteration 200 2004.0697202682495
I0727 03:15:26.276915 140120985872192 train.py:394] {'eval/walltime': 832.6410987377167, 'training/sps': 42140.312079431744, 'training/walltime': 1172.2006921768188, 'training/entropy_loss': Array(-0.01289498, dtype=float32), 'training/policy_loss': Array(0.00687548, dtype=float32), 'training/total_loss': Array(45.253693, dtype=float32), 'training/v_loss': Array(45.25971, dtype=float32), 'eval/episode_goal_distance': (Array(0.3589682, dtype=float32), Array(0.12945959, dtype=float32)), 'eval/episode_reward': (Array(-7835.7856, dtype=float32), Array(4324.6685, dtype=float32)), 'eval/avg_episode_length': (Array(852.41406, dtype=float32), Array(353.49405, dtype=float32)), 'eval/epoch_eval_time': 4.08342170715332, 'eval/sps': 31346.260361933757}
I0727 03:15:26.279294 140120985872192 train.py:379] starting iteration 201 2013.9910435676575
I0727 03:15:36.222076 140120985872192 train.py:394] {'eval/walltime': 836.7222883701324, 'training/sps': 41953.63937120995, 'training/walltime': 1178.0585868358612, 'training/entropy_loss': Array(-0.0116008, dtype=float32), 'training/policy_loss': Array(0.00542319, dtype=float32), 'training/total_loss': Array(14.325222, dtype=float32), 'training/v_loss': Array(14.3314, dtype=float32), 'eval/episode_goal_distance': (Array(0.34622774, dtype=float32), Array(0.12880081, dtype=float32)), 'eval/episode_reward': (Array(-8398.648, dtype=float32), Array(3601.957, dtype=float32)), 'eval/avg_episode_length': (Array(937.875, dtype=float32), Array(240.6094, dtype=float32)), 'eval/epoch_eval_time': 4.0811896324157715, 'eval/sps': 31363.404185713658}
I0727 03:15:36.224611 140120985872192 train.py:379] starting iteration 202 2023.936360359192
I0727 03:15:46.175090 140120985872192 train.py:394] {'eval/walltime': 840.8176746368408, 'training/sps': 41998.64856218317, 'training/walltime': 1183.9102036952972, 'training/entropy_loss': Array(-0.00698932, dtype=float32), 'training/policy_loss': Array(0.01138068, dtype=float32), 'training/total_loss': Array(10.570444, dtype=float32), 'training/v_loss': Array(10.566053, dtype=float32), 'eval/episode_goal_distance': (Array(0.3289705, dtype=float32), Array(0.11671061, dtype=float32)), 'eval/episode_reward': (Array(-7137.24, dtype=float32), Array(4033.6982, dtype=float32)), 'eval/avg_episode_length': (Array(852.2969, dtype=float32), Array(353.7745, dtype=float32)), 'eval/epoch_eval_time': 4.095386266708374, 'eval/sps': 31254.68311512377}
I0727 03:15:46.177394 140120985872192 train.py:379] starting iteration 203 2033.8891434669495
I0727 03:15:56.104602 140120985872192 train.py:394] {'eval/walltime': 844.9142272472382, 'training/sps': 42174.92486214795, 'training/walltime': 1189.7373628616333, 'training/entropy_loss': Array(-0.00335413, dtype=float32), 'training/policy_loss': Array(0.00559879, dtype=float32), 'training/total_loss': Array(9.889191, dtype=float32), 'training/v_loss': Array(9.886947, dtype=float32), 'eval/episode_goal_distance': (Array(0.34192508, dtype=float32), Array(0.11478839, dtype=float32)), 'eval/episode_reward': (Array(-7566.1045, dtype=float32), Array(3788.6387, dtype=float32)), 'eval/avg_episode_length': (Array(867.90625, dtype=float32), Array(337.5362, dtype=float32)), 'eval/epoch_eval_time': 4.096552610397339, 'eval/sps': 31245.78448599122}
I0727 03:15:56.106945 140120985872192 train.py:379] starting iteration 204 2043.8186933994293
I0727 03:16:06.068140 140120985872192 train.py:394] {'eval/walltime': 849.0215232372284, 'training/sps': 42007.77292514845, 'training/walltime': 1195.5877087116241, 'training/entropy_loss': Array(0.00119872, dtype=float32), 'training/policy_loss': Array(0.00561174, dtype=float32), 'training/total_loss': Array(9.5903845, dtype=float32), 'training/v_loss': Array(9.583573, dtype=float32), 'eval/episode_goal_distance': (Array(0.3579983, dtype=float32), Array(0.14082286, dtype=float32)), 'eval/episode_reward': (Array(-7677.043, dtype=float32), Array(4467.0674, dtype=float32)), 'eval/avg_episode_length': (Array(836.9531, dtype=float32), Array(368.04037, dtype=float32)), 'eval/epoch_eval_time': 4.107295989990234, 'eval/sps': 31164.055454475376}
I0727 03:16:06.181941 140120985872192 train.py:379] starting iteration 205 2053.8936738967896
I0727 03:16:16.122987 140120985872192 train.py:394] {'eval/walltime': 853.1359865665436, 'training/sps': 42208.830600111934, 'training/walltime': 1201.4101870059967, 'training/entropy_loss': Array(0.00476733, dtype=float32), 'training/policy_loss': Array(0.00582077, dtype=float32), 'training/total_loss': Array(9.231567, dtype=float32), 'training/v_loss': Array(9.22098, dtype=float32), 'eval/episode_goal_distance': (Array(0.32925162, dtype=float32), Array(0.13831873, dtype=float32)), 'eval/episode_reward': (Array(-7556.785, dtype=float32), Array(4154.2617, dtype=float32)), 'eval/avg_episode_length': (Array(860.25, dtype=float32), Array(345.47192, dtype=float32)), 'eval/epoch_eval_time': 4.1144633293151855, 'eval/sps': 31109.768092477912}
I0727 03:16:16.125661 140120985872192 train.py:379] starting iteration 206 2063.837410211563
I0727 03:16:26.031475 140120985872192 train.py:394] {'eval/walltime': 857.2199778556824, 'training/sps': 42239.47427927119, 'training/walltime': 1207.2284412384033, 'training/entropy_loss': Array(0.00628356, dtype=float32), 'training/policy_loss': Array(0.00586957, dtype=float32), 'training/total_loss': Array(15.620914, dtype=float32), 'training/v_loss': Array(15.608761, dtype=float32), 'eval/episode_goal_distance': (Array(0.3510824, dtype=float32), Array(0.12778097, dtype=float32)), 'eval/episode_reward': (Array(-7628.4863, dtype=float32), Array(4326.1836, dtype=float32)), 'eval/avg_episode_length': (Array(836.8828, dtype=float32), Array(368.19922, dtype=float32)), 'eval/epoch_eval_time': 4.083991289138794, 'eval/sps': 31341.888593252075}
I0727 03:16:26.033949 140120985872192 train.py:379] starting iteration 207 2073.74569773674
I0727 03:16:35.954650 140120985872192 train.py:394] {'eval/walltime': 861.3143224716187, 'training/sps': 42206.02910592185, 'training/walltime': 1213.0513060092926, 'training/entropy_loss': Array(0.00063012, dtype=float32), 'training/policy_loss': Array(0.00823192, dtype=float32), 'training/total_loss': Array(24.54168, dtype=float32), 'training/v_loss': Array(24.53282, dtype=float32), 'eval/episode_goal_distance': (Array(0.3315067, dtype=float32), Array(0.14141941, dtype=float32)), 'eval/episode_reward': (Array(-7439.829, dtype=float32), Array(4248.0493, dtype=float32)), 'eval/avg_episode_length': (Array(860.2031, dtype=float32), Array(345.58783, dtype=float32)), 'eval/epoch_eval_time': 4.094344615936279, 'eval/sps': 31262.634684386343}
I0727 03:16:35.956941 140120985872192 train.py:379] starting iteration 208 2083.668690443039
I0727 03:16:45.862896 140120985872192 train.py:394] {'eval/walltime': 865.3949508666992, 'training/sps': 42214.99830860234, 'training/walltime': 1218.872933626175, 'training/entropy_loss': Array(-0.0025749, dtype=float32), 'training/policy_loss': Array(0.00542921, dtype=float32), 'training/total_loss': Array(32.91255, dtype=float32), 'training/v_loss': Array(32.9097, dtype=float32), 'eval/episode_goal_distance': (Array(0.33073837, dtype=float32), Array(0.11796761, dtype=float32)), 'eval/episode_reward': (Array(-7928.931, dtype=float32), Array(3390.3489, dtype=float32)), 'eval/avg_episode_length': (Array(930.10156, dtype=float32), Array(254.16766, dtype=float32)), 'eval/epoch_eval_time': 4.080628395080566, 'eval/sps': 31367.717813832645}
I0727 03:16:45.865309 140120985872192 train.py:379] starting iteration 209 2093.577057123184
I0727 03:16:55.854402 140120985872192 train.py:394] {'eval/walltime': 869.5079863071442, 'training/sps': 41849.48317808889, 'training/walltime': 1224.7454075813293, 'training/entropy_loss': Array(-0.0045375, dtype=float32), 'training/policy_loss': Array(0.00498846, dtype=float32), 'training/total_loss': Array(15.410662, dtype=float32), 'training/v_loss': Array(15.410212, dtype=float32), 'eval/episode_goal_distance': (Array(0.34666485, dtype=float32), Array(0.15599854, dtype=float32)), 'eval/episode_reward': (Array(-8193.542, dtype=float32), Array(4215.6377, dtype=float32)), 'eval/avg_episode_length': (Array(922.28906, dtype=float32), Array(266.946, dtype=float32)), 'eval/epoch_eval_time': 4.113035440444946, 'eval/sps': 31120.568216196312}
I0727 03:16:55.856775 140120985872192 train.py:379] starting iteration 210 2103.5685245990753
I0727 03:17:05.746928 140120985872192 train.py:394] {'eval/walltime': 873.5830726623535, 'training/sps': 42289.351172289185, 'training/walltime': 1230.5567996501923, 'training/entropy_loss': Array(-0.0036006, dtype=float32), 'training/policy_loss': Array(0.0045124, dtype=float32), 'training/total_loss': Array(9.896208, dtype=float32), 'training/v_loss': Array(9.895296, dtype=float32), 'eval/episode_goal_distance': (Array(0.34340423, dtype=float32), Array(0.1439062, dtype=float32)), 'eval/episode_reward': (Array(-8082.3926, dtype=float32), Array(3996.0432, dtype=float32)), 'eval/avg_episode_length': (Array(906.9219, dtype=float32), Array(289.39218, dtype=float32)), 'eval/epoch_eval_time': 4.075086355209351, 'eval/sps': 31410.377312954934}
I0727 03:17:05.749368 140120985872192 train.py:379] starting iteration 211 2113.461117744446
I0727 03:17:15.715594 140120985872192 train.py:394] {'eval/walltime': 877.6862654685974, 'training/sps': 41943.253334418405, 'training/walltime': 1236.4161448478699, 'training/entropy_loss': Array(-0.00101012, dtype=float32), 'training/policy_loss': Array(0.00874688, dtype=float32), 'training/total_loss': Array(9.173603, dtype=float32), 'training/v_loss': Array(9.165865, dtype=float32), 'eval/episode_goal_distance': (Array(0.3277169, dtype=float32), Array(0.12701131, dtype=float32)), 'eval/episode_reward': (Array(-7428.1694, dtype=float32), Array(3831.841, dtype=float32)), 'eval/avg_episode_length': (Array(883.60156, dtype=float32), Array(319.47833, dtype=float32)), 'eval/epoch_eval_time': 4.1031928062438965, 'eval/sps': 31195.21944111919}
I0727 03:17:15.718020 140120985872192 train.py:379] starting iteration 212 2123.4297688007355
I0727 03:17:25.633156 140120985872192 train.py:394] {'eval/walltime': 881.7699472904205, 'training/sps': 42169.77460599311, 'training/walltime': 1242.2440156936646, 'training/entropy_loss': Array(-0.00063322, dtype=float32), 'training/policy_loss': Array(0.00770404, dtype=float32), 'training/total_loss': Array(9.608537, dtype=float32), 'training/v_loss': Array(9.601466, dtype=float32), 'eval/episode_goal_distance': (Array(0.35384074, dtype=float32), Array(0.15329456, dtype=float32)), 'eval/episode_reward': (Array(-8361.074, dtype=float32), Array(3964.704, dtype=float32)), 'eval/avg_episode_length': (Array(922.40625, dtype=float32), Array(266.54367, dtype=float32)), 'eval/epoch_eval_time': 4.08368182182312, 'eval/sps': 31344.26372690702}
I0727 03:17:25.635507 140120985872192 train.py:379] starting iteration 213 2133.347256422043
I0727 03:17:35.609053 140120985872192 train.py:394] {'eval/walltime': 885.8986129760742, 'training/sps': 42073.82392052248, 'training/walltime': 1248.0851771831512, 'training/entropy_loss': Array(0.00299115, dtype=float32), 'training/policy_loss': Array(0.0089517, dtype=float32), 'training/total_loss': Array(8.905674, dtype=float32), 'training/v_loss': Array(8.893731, dtype=float32), 'eval/episode_goal_distance': (Array(0.3348794, dtype=float32), Array(0.14595172, dtype=float32)), 'eval/episode_reward': (Array(-8008.131, dtype=float32), Array(3688.765, dtype=float32)), 'eval/avg_episode_length': (Array(930.1797, dtype=float32), Array(253.8835, dtype=float32)), 'eval/epoch_eval_time': 4.1286656856536865, 'eval/sps': 31002.75240128432}
I0727 03:17:35.611563 140120985872192 train.py:379] starting iteration 214 2143.3233120441437
I0727 03:17:45.554589 140120985872192 train.py:394] {'eval/walltime': 889.9910545349121, 'training/sps': 42032.519873647034, 'training/walltime': 1253.9320785999298, 'training/entropy_loss': Array(0.00205319, dtype=float32), 'training/policy_loss': Array(0.0063609, dtype=float32), 'training/total_loss': Array(9.463909, dtype=float32), 'training/v_loss': Array(9.455496, dtype=float32), 'eval/episode_goal_distance': (Array(0.35618737, dtype=float32), Array(0.1394434, dtype=float32)), 'eval/episode_reward': (Array(-8664.941, dtype=float32), Array(3802.7952, dtype=float32)), 'eval/avg_episode_length': (Array(937.90625, dtype=float32), Array(240.48828, dtype=float32)), 'eval/epoch_eval_time': 4.092441558837891, 'eval/sps': 31277.172357800875}
I0727 03:17:45.557091 140120985872192 train.py:379] starting iteration 215 2153.2688405513763
I0727 03:17:55.465394 140120985872192 train.py:394] {'eval/walltime': 894.0700426101685, 'training/sps': 42185.30511261716, 'training/walltime': 1259.7578039169312, 'training/entropy_loss': Array(0.00288448, dtype=float32), 'training/policy_loss': Array(0.00713945, dtype=float32), 'training/total_loss': Array(20.257072, dtype=float32), 'training/v_loss': Array(20.247047, dtype=float32), 'eval/episode_goal_distance': (Array(0.37123698, dtype=float32), Array(0.15337506, dtype=float32)), 'eval/episode_reward': (Array(-8142.029, dtype=float32), Array(4523.8013, dtype=float32)), 'eval/avg_episode_length': (Array(860.22656, dtype=float32), Array(345.53015, dtype=float32)), 'eval/epoch_eval_time': 4.078988075256348, 'eval/sps': 31380.331993727566}
I0727 03:17:55.467823 140120985872192 train.py:379] starting iteration 216 2163.179571390152
I0727 03:18:05.388021 140120985872192 train.py:394] {'eval/walltime': 898.1453695297241, 'training/sps': 42073.504500221694, 'training/walltime': 1265.5990097522736, 'training/entropy_loss': Array(0.00709869, dtype=float32), 'training/policy_loss': Array(0.00909203, dtype=float32), 'training/total_loss': Array(49.096424, dtype=float32), 'training/v_loss': Array(49.080235, dtype=float32), 'eval/episode_goal_distance': (Array(0.33415958, dtype=float32), Array(0.12596314, dtype=float32)), 'eval/episode_reward': (Array(-7376.207, dtype=float32), Array(3969.4602, dtype=float32)), 'eval/avg_episode_length': (Array(867.875, dtype=float32), Array(337.61612, dtype=float32)), 'eval/epoch_eval_time': 4.075326919555664, 'eval/sps': 31408.523175352013}
I0727 03:18:05.390373 140120985872192 train.py:379] starting iteration 217 2173.1021218299866
I0727 03:18:15.331211 140120985872192 train.py:394] {'eval/walltime': 902.2308688163757, 'training/sps': 41997.823782341075, 'training/walltime': 1271.4507415294647, 'training/entropy_loss': Array(0.00516493, dtype=float32), 'training/policy_loss': Array(0.00994512, dtype=float32), 'training/total_loss': Array(24.338882, dtype=float32), 'training/v_loss': Array(24.323772, dtype=float32), 'eval/episode_goal_distance': (Array(0.36257964, dtype=float32), Array(0.15561666, dtype=float32)), 'eval/episode_reward': (Array(-7591.8237, dtype=float32), Array(4660.6836, dtype=float32)), 'eval/avg_episode_length': (Array(836.8047, dtype=float32), Array(368.37546, dtype=float32)), 'eval/epoch_eval_time': 4.085499286651611, 'eval/sps': 31330.319997413604}
I0727 03:18:15.333827 140120985872192 train.py:379] starting iteration 218 2183.0455758571625
I0727 03:18:25.274602 140120985872192 train.py:394] {'eval/walltime': 906.316534280777, 'training/sps': 42000.44881659534, 'training/walltime': 1277.3021075725555, 'training/entropy_loss': Array(0.0049725, dtype=float32), 'training/policy_loss': Array(0.008113, dtype=float32), 'training/total_loss': Array(12.938784, dtype=float32), 'training/v_loss': Array(12.925696, dtype=float32), 'eval/episode_goal_distance': (Array(0.37515926, dtype=float32), Array(0.14970462, dtype=float32)), 'eval/episode_reward': (Array(-8502.881, dtype=float32), Array(4554.534, dtype=float32)), 'eval/avg_episode_length': (Array(875.77344, dtype=float32), Array(328.6735, dtype=float32)), 'eval/epoch_eval_time': 4.085665464401245, 'eval/sps': 31329.045687972994}
I0727 03:18:25.277114 140120985872192 train.py:379] starting iteration 219 2192.988862991333
I0727 03:18:35.201727 140120985872192 train.py:394] {'eval/walltime': 910.4093701839447, 'training/sps': 42166.676431454616, 'training/walltime': 1283.1304066181183, 'training/entropy_loss': Array(0.0038324, dtype=float32), 'training/policy_loss': Array(0.00898243, dtype=float32), 'training/total_loss': Array(11.716017, dtype=float32), 'training/v_loss': Array(11.703201, dtype=float32), 'eval/episode_goal_distance': (Array(0.35349068, dtype=float32), Array(0.13566086, dtype=float32)), 'eval/episode_reward': (Array(-7357.217, dtype=float32), Array(4346.8237, dtype=float32)), 'eval/avg_episode_length': (Array(836.875, dtype=float32), Array(368.2169, dtype=float32)), 'eval/epoch_eval_time': 4.092835903167725, 'eval/sps': 31274.158805373085}
I0727 03:18:35.204119 140120985872192 train.py:379] starting iteration 220 2202.915868282318
I0727 03:18:45.136762 140120985872192 train.py:394] {'eval/walltime': 914.4938774108887, 'training/sps': 42049.74018912492, 'training/walltime': 1288.974913597107, 'training/entropy_loss': Array(0.00035749, dtype=float32), 'training/policy_loss': Array(0.012795, dtype=float32), 'training/total_loss': Array(14.890356, dtype=float32), 'training/v_loss': Array(14.877203, dtype=float32), 'eval/episode_goal_distance': (Array(0.36887214, dtype=float32), Array(0.16226117, dtype=float32)), 'eval/episode_reward': (Array(-8741.193, dtype=float32), Array(4051.9417, dtype=float32)), 'eval/avg_episode_length': (Array(930.1094, dtype=float32), Array(254.13972, dtype=float32)), 'eval/epoch_eval_time': 4.08450722694397, 'eval/sps': 31337.92961746567}
I0727 03:18:45.139235 140120985872192 train.py:379] starting iteration 221 2212.850984096527
I0727 03:18:55.072734 140120985872192 train.py:394] {'eval/walltime': 918.6092731952667, 'training/sps': 42266.713317314396, 'training/walltime': 1294.78941822052, 'training/entropy_loss': Array(-0.00224046, dtype=float32), 'training/policy_loss': Array(0.00953174, dtype=float32), 'training/total_loss': Array(13.638409, dtype=float32), 'training/v_loss': Array(13.631117, dtype=float32), 'eval/episode_goal_distance': (Array(0.38119054, dtype=float32), Array(0.16807823, dtype=float32)), 'eval/episode_reward': (Array(-8754.504, dtype=float32), Array(4371.512, dtype=float32)), 'eval/avg_episode_length': (Array(914.6875, dtype=float32), Array(278.2339, dtype=float32)), 'eval/epoch_eval_time': 4.115395784378052, 'eval/sps': 31102.71932674983}
I0727 03:18:55.075107 140120985872192 train.py:379] starting iteration 222 2222.786856174469
I0727 03:19:04.997095 140120985872192 train.py:394] {'eval/walltime': 922.6893947124481, 'training/sps': 42094.82184174334, 'training/walltime': 1300.6276659965515, 'training/entropy_loss': Array(-0.00177196, dtype=float32), 'training/policy_loss': Array(0.00975119, dtype=float32), 'training/total_loss': Array(12.4667635, dtype=float32), 'training/v_loss': Array(12.458784, dtype=float32), 'eval/episode_goal_distance': (Array(0.37177363, dtype=float32), Array(0.14767624, dtype=float32)), 'eval/episode_reward': (Array(-8235.024, dtype=float32), Array(4476.255, dtype=float32)), 'eval/avg_episode_length': (Array(875.8203, dtype=float32), Array(328.54944, dtype=float32)), 'eval/epoch_eval_time': 4.0801215171813965, 'eval/sps': 31371.61465926734}
I0727 03:19:04.999468 140120985872192 train.py:379] starting iteration 223 2232.7112169265747
I0727 03:19:14.912294 140120985872192 train.py:394] {'eval/walltime': 926.7653980255127, 'training/sps': 42131.67421667821, 'training/walltime': 1306.4608070850372, 'training/entropy_loss': Array(-0.00213785, dtype=float32), 'training/policy_loss': Array(0.00739726, dtype=float32), 'training/total_loss': Array(19.40844, dtype=float32), 'training/v_loss': Array(19.40318, dtype=float32), 'eval/episode_goal_distance': (Array(0.34599078, dtype=float32), Array(0.13843046, dtype=float32)), 'eval/episode_reward': (Array(-7743.26, dtype=float32), Array(4469.82, dtype=float32)), 'eval/avg_episode_length': (Array(867.9297, dtype=float32), Array(337.47604, dtype=float32)), 'eval/epoch_eval_time': 4.076003313064575, 'eval/sps': 31403.311079195908}
I0727 03:19:14.914829 140120985872192 train.py:379] starting iteration 224 2242.626577615738
I0727 03:19:24.841880 140120985872192 train.py:394] {'eval/walltime': 930.8444821834564, 'training/sps': 42051.20687393158, 'training/walltime': 1312.3051102161407, 'training/entropy_loss': Array(-0.00221335, dtype=float32), 'training/policy_loss': Array(0.00934963, dtype=float32), 'training/total_loss': Array(19.362022, dtype=float32), 'training/v_loss': Array(19.354885, dtype=float32), 'eval/episode_goal_distance': (Array(0.3944434, dtype=float32), Array(0.17482992, dtype=float32)), 'eval/episode_reward': (Array(-8621.362, dtype=float32), Array(4943.079, dtype=float32)), 'eval/avg_episode_length': (Array(860.25, dtype=float32), Array(345.4719, dtype=float32)), 'eval/epoch_eval_time': 4.079084157943726, 'eval/sps': 31379.592831084185}
I0727 03:19:24.844285 140120985872192 train.py:379] starting iteration 225 2252.5560340881348
I0727 03:19:34.756830 140120985872192 train.py:394] {'eval/walltime': 934.9444851875305, 'training/sps': 42307.30701515326, 'training/walltime': 1318.1140358448029, 'training/entropy_loss': Array(-0.00288126, dtype=float32), 'training/policy_loss': Array(0.01081103, dtype=float32), 'training/total_loss': Array(47.502064, dtype=float32), 'training/v_loss': Array(47.494133, dtype=float32), 'eval/episode_goal_distance': (Array(0.36067885, dtype=float32), Array(0.13374184, dtype=float32)), 'eval/episode_reward': (Array(-8563.484, dtype=float32), Array(3967.2554, dtype=float32)), 'eval/avg_episode_length': (Array(914.5781, dtype=float32), Array(278.59058, dtype=float32)), 'eval/epoch_eval_time': 4.100003004074097, 'eval/sps': 31219.48932057093}
I0727 03:19:34.759233 140120985872192 train.py:379] starting iteration 226 2262.470981836319
I0727 03:19:44.683023 140120985872192 train.py:394] {'eval/walltime': 939.0189337730408, 'training/sps': 42041.84415008857, 'training/walltime': 1323.9596405029297, 'training/entropy_loss': Array(-0.00646244, dtype=float32), 'training/policy_loss': Array(0.00973989, dtype=float32), 'training/total_loss': Array(17.911026, dtype=float32), 'training/v_loss': Array(17.90775, dtype=float32), 'eval/episode_goal_distance': (Array(0.3660822, dtype=float32), Array(0.15548491, dtype=float32)), 'eval/episode_reward': (Array(-9029.419, dtype=float32), Array(3532.799, dtype=float32)), 'eval/avg_episode_length': (Array(968.9297, dtype=float32), Array(172.99237, dtype=float32)), 'eval/epoch_eval_time': 4.074448585510254, 'eval/sps': 31415.293950499126}
I0727 03:19:44.685492 140120985872192 train.py:379] starting iteration 227 2272.397241592407
I0727 03:19:54.659544 140120985872192 train.py:394] {'eval/walltime': 943.1292397975922, 'training/sps': 41938.21410199054, 'training/walltime': 1329.8196897506714, 'training/entropy_loss': Array(-0.00538089, dtype=float32), 'training/policy_loss': Array(0.0104861, dtype=float32), 'training/total_loss': Array(13.284927, dtype=float32), 'training/v_loss': Array(13.279821, dtype=float32), 'eval/episode_goal_distance': (Array(0.40098745, dtype=float32), Array(0.18461543, dtype=float32)), 'eval/episode_reward': (Array(-8823.286, dtype=float32), Array(4898.882, dtype=float32)), 'eval/avg_episode_length': (Array(860.21094, dtype=float32), Array(345.5685, dtype=float32)), 'eval/epoch_eval_time': 4.110306024551392, 'eval/sps': 31141.23358101304}
I0727 03:19:54.661916 140120985872192 train.py:379] starting iteration 228 2282.373665332794
I0727 03:20:04.600313 140120985872192 train.py:394] {'eval/walltime': 947.2123920917511, 'training/sps': 41997.90420550609, 'training/walltime': 1335.6714103221893, 'training/entropy_loss': Array(-0.00386429, dtype=float32), 'training/policy_loss': Array(0.02330352, dtype=float32), 'training/total_loss': Array(12.825263, dtype=float32), 'training/v_loss': Array(12.805823, dtype=float32), 'eval/episode_goal_distance': (Array(0.34864464, dtype=float32), Array(0.15191627, dtype=float32)), 'eval/episode_reward': (Array(-8053.4697, dtype=float32), Array(4165.489, dtype=float32)), 'eval/avg_episode_length': (Array(898.9922, dtype=float32), Array(300.4228, dtype=float32)), 'eval/epoch_eval_time': 4.0831522941589355, 'eval/sps': 31348.328638906663}
I0727 03:20:04.602766 140120985872192 train.py:379] starting iteration 229 2292.314514875412
I0727 03:20:14.533345 140120985872192 train.py:394] {'eval/walltime': 951.306202173233, 'training/sps': 42135.053154215755, 'training/walltime': 1341.5040836334229, 'training/entropy_loss': Array(-0.00877802, dtype=float32), 'training/policy_loss': Array(0.02163464, dtype=float32), 'training/total_loss': Array(16.320377, dtype=float32), 'training/v_loss': Array(16.307522, dtype=float32), 'eval/episode_goal_distance': (Array(0.33886737, dtype=float32), Array(0.13877064, dtype=float32)), 'eval/episode_reward': (Array(-8153.421, dtype=float32), Array(3644.3958, dtype=float32)), 'eval/avg_episode_length': (Array(945.6797, dtype=float32), Array(225.8433, dtype=float32)), 'eval/epoch_eval_time': 4.093810081481934, 'eval/sps': 31266.71668991171}
I0727 03:20:14.535645 140120985872192 train.py:379] starting iteration 230 2302.2473945617676
I0727 03:20:24.477594 140120985872192 train.py:394] {'eval/walltime': 955.3984506130219, 'training/sps': 42041.1085494353, 'training/walltime': 1347.3497905731201, 'training/entropy_loss': Array(-0.01002153, dtype=float32), 'training/policy_loss': Array(0.01170983, dtype=float32), 'training/total_loss': Array(10.296739, dtype=float32), 'training/v_loss': Array(10.29505, dtype=float32), 'eval/episode_goal_distance': (Array(0.343032, dtype=float32), Array(0.14424336, dtype=float32)), 'eval/episode_reward': (Array(-8722.449, dtype=float32), Array(3857.678, dtype=float32)), 'eval/avg_episode_length': (Array(953.34375, dtype=float32), Array(210.38478, dtype=float32)), 'eval/epoch_eval_time': 4.092248439788818, 'eval/sps': 31278.648372239462}
I0727 03:20:24.480072 140120985872192 train.py:379] starting iteration 231 2312.191820859909
I0727 03:20:34.413558 140120985872192 train.py:394] {'eval/walltime': 959.4825479984283, 'training/sps': 42041.237149091365, 'training/walltime': 1353.195479631424, 'training/entropy_loss': Array(-0.00924572, dtype=float32), 'training/policy_loss': Array(0.01067303, dtype=float32), 'training/total_loss': Array(15.947315, dtype=float32), 'training/v_loss': Array(15.9458885, dtype=float32), 'eval/episode_goal_distance': (Array(0.3842937, dtype=float32), Array(0.1623275, dtype=float32)), 'eval/episode_reward': (Array(-8731.229, dtype=float32), Array(4136.55, dtype=float32)), 'eval/avg_episode_length': (Array(906.78125, dtype=float32), Array(289.8295, dtype=float32)), 'eval/epoch_eval_time': 4.084097385406494, 'eval/sps': 31341.074396848653}
I0727 03:20:34.415924 140120985872192 train.py:379] starting iteration 232 2322.127672433853
I0727 03:20:44.381268 140120985872192 train.py:394] {'eval/walltime': 963.5909662246704, 'training/sps': 41985.38243585908, 'training/walltime': 1359.048945426941, 'training/entropy_loss': Array(-0.00868255, dtype=float32), 'training/policy_loss': Array(0.01179735, dtype=float32), 'training/total_loss': Array(18.771248, dtype=float32), 'training/v_loss': Array(18.768133, dtype=float32), 'eval/episode_goal_distance': (Array(0.39965713, dtype=float32), Array(0.1795177, dtype=float32)), 'eval/episode_reward': (Array(-8838.449, dtype=float32), Array(4328.9854, dtype=float32)), 'eval/avg_episode_length': (Array(906.8125, dtype=float32), Array(289.7319, dtype=float32)), 'eval/epoch_eval_time': 4.108418226242065, 'eval/sps': 31155.54282726481}
I0727 03:20:44.383669 140120985872192 train.py:379] starting iteration 233 2332.095418214798
I0727 03:20:54.266238 140120985872192 train.py:394] {'eval/walltime': 967.6710984706879, 'training/sps': 42381.23987779407, 'training/walltime': 1364.8477375507355, 'training/entropy_loss': Array(-0.00975765, dtype=float32), 'training/policy_loss': Array(0.01882869, dtype=float32), 'training/total_loss': Array(44.2556, dtype=float32), 'training/v_loss': Array(44.24653, dtype=float32), 'eval/episode_goal_distance': (Array(0.40628728, dtype=float32), Array(0.18063623, dtype=float32)), 'eval/episode_reward': (Array(-8865.016, dtype=float32), Array(4375.0903, dtype=float32)), 'eval/avg_episode_length': (Array(906.8281, dtype=float32), Array(289.68335, dtype=float32)), 'eval/epoch_eval_time': 4.080132246017456, 'eval/sps': 31371.532166619967}
I0727 03:20:54.268722 140120985872192 train.py:379] starting iteration 234 2341.9804713726044
I0727 03:21:04.179828 140120985872192 train.py:394] {'eval/walltime': 971.7484023571014, 'training/sps': 42153.40221767515, 'training/walltime': 1370.6778719425201, 'training/entropy_loss': Array(-0.01589619, dtype=float32), 'training/policy_loss': Array(0.01607251, dtype=float32), 'training/total_loss': Array(23.179832, dtype=float32), 'training/v_loss': Array(23.179657, dtype=float32), 'eval/episode_goal_distance': (Array(0.38823333, dtype=float32), Array(0.16828203, dtype=float32)), 'eval/episode_reward': (Array(-8619.651, dtype=float32), Array(4416.3516, dtype=float32)), 'eval/avg_episode_length': (Array(899.08594, dtype=float32), Array(300.14438, dtype=float32)), 'eval/epoch_eval_time': 4.077303886413574, 'eval/sps': 31393.294089881958}
I0727 03:21:04.182241 140120985872192 train.py:379] starting iteration 235 2351.893990755081
I0727 03:21:14.127106 140120985872192 train.py:394] {'eval/walltime': 975.8739147186279, 'training/sps': 42258.397764010304, 'training/walltime': 1376.4935207366943, 'training/entropy_loss': Array(-0.02214269, dtype=float32), 'training/policy_loss': Array(0.00302028, dtype=float32), 'training/total_loss': Array(12.978071, dtype=float32), 'training/v_loss': Array(12.997194, dtype=float32), 'eval/episode_goal_distance': (Array(0.39859366, dtype=float32), Array(0.13058102, dtype=float32)), 'eval/episode_reward': (Array(-9048.47, dtype=float32), Array(3833.2834, dtype=float32)), 'eval/avg_episode_length': (Array(922.3125, dtype=float32), Array(266.8659, dtype=float32)), 'eval/epoch_eval_time': 4.125512361526489, 'eval/sps': 31026.44927056732}
I0727 03:21:14.129402 140120985872192 train.py:379] starting iteration 236 2361.841150999069
I0727 03:21:24.029211 140120985872192 train.py:394] {'eval/walltime': 979.947018623352, 'training/sps': 42204.36152069516, 'training/walltime': 1382.3166155815125, 'training/entropy_loss': Array(-0.02114206, dtype=float32), 'training/policy_loss': Array(0.00092898, dtype=float32), 'training/total_loss': Array(10.799309, dtype=float32), 'training/v_loss': Array(10.819522, dtype=float32), 'eval/episode_goal_distance': (Array(0.3826578, dtype=float32), Array(0.15755527, dtype=float32)), 'eval/episode_reward': (Array(-8869.957, dtype=float32), Array(3466.782, dtype=float32)), 'eval/avg_episode_length': (Array(961.16406, dtype=float32), Array(192.62056, dtype=float32)), 'eval/epoch_eval_time': 4.073103904724121, 'eval/sps': 31425.665289692548}
I0727 03:21:24.031546 140120985872192 train.py:379] starting iteration 237 2371.743295431137
I0727 03:21:33.940213 140120985872192 train.py:394] {'eval/walltime': 984.0423495769501, 'training/sps': 42301.95424943972, 'training/walltime': 1388.126276254654, 'training/entropy_loss': Array(-0.02202959, dtype=float32), 'training/policy_loss': Array(0.00088073, dtype=float32), 'training/total_loss': Array(14.95607, dtype=float32), 'training/v_loss': Array(14.977219, dtype=float32), 'eval/episode_goal_distance': (Array(0.37219557, dtype=float32), Array(0.14203493, dtype=float32)), 'eval/episode_reward': (Array(-8532.486, dtype=float32), Array(3904.9731, dtype=float32)), 'eval/avg_episode_length': (Array(914.625, dtype=float32), Array(278.43768, dtype=float32)), 'eval/epoch_eval_time': 4.0953309535980225, 'eval/sps': 31255.105252859583}
I0727 03:21:33.942636 140120985872192 train.py:379] starting iteration 238 2381.654384613037
I0727 03:21:43.833828 140120985872192 train.py:394] {'eval/walltime': 988.1222188472748, 'training/sps': 42316.110858709544, 'training/walltime': 1393.9339933395386, 'training/entropy_loss': Array(-0.02287285, dtype=float32), 'training/policy_loss': Array(0.00059033, dtype=float32), 'training/total_loss': Array(11.942593, dtype=float32), 'training/v_loss': Array(11.964876, dtype=float32), 'eval/episode_goal_distance': (Array(0.3701864, dtype=float32), Array(0.1336246, dtype=float32)), 'eval/episode_reward': (Array(-8538.552, dtype=float32), Array(3421.796, dtype=float32)), 'eval/avg_episode_length': (Array(937.8281, dtype=float32), Array(240.79088, dtype=float32)), 'eval/epoch_eval_time': 4.079869270324707, 'eval/sps': 31373.554278079304}
I0727 03:21:43.836351 140120985872192 train.py:379] starting iteration 239 2391.5480999946594
I0727 03:21:53.781342 140120985872192 train.py:394] {'eval/walltime': 992.2105510234833, 'training/sps': 41989.00134949963, 'training/walltime': 1399.7869546413422, 'training/entropy_loss': Array(-0.02281985, dtype=float32), 'training/policy_loss': Array(0.00062555, dtype=float32), 'training/total_loss': Array(12.452673, dtype=float32), 'training/v_loss': Array(12.474868, dtype=float32), 'eval/episode_goal_distance': (Array(0.35345036, dtype=float32), Array(0.1270327, dtype=float32)), 'eval/episode_reward': (Array(-8073.3535, dtype=float32), Array(3544.8845, dtype=float32)), 'eval/avg_episode_length': (Array(930.1406, dtype=float32), Array(254.02565, dtype=float32)), 'eval/epoch_eval_time': 4.088332176208496, 'eval/sps': 31308.61057349472}
I0727 03:21:53.783846 140120985872192 train.py:379] starting iteration 240 2401.495594739914
I0727 03:22:03.675056 140120985872192 train.py:394] {'eval/walltime': 996.2845778465271, 'training/sps': 42273.51859839827, 'training/walltime': 1405.6005232334137, 'training/entropy_loss': Array(-0.02246842, dtype=float32), 'training/policy_loss': Array(0.00053941, dtype=float32), 'training/total_loss': Array(19.94611, dtype=float32), 'training/v_loss': Array(19.968039, dtype=float32), 'eval/episode_goal_distance': (Array(0.36634743, dtype=float32), Array(0.13066189, dtype=float32)), 'eval/episode_reward': (Array(-8452.528, dtype=float32), Array(3674.1438, dtype=float32)), 'eval/avg_episode_length': (Array(937.90625, dtype=float32), Array(240.48825, dtype=float32)), 'eval/epoch_eval_time': 4.074026823043823, 'eval/sps': 31418.54620985718}
I0727 03:22:03.677474 140120985872192 train.py:379] starting iteration 241 2411.3892228603363
I0727 03:22:13.598326 140120985872192 train.py:394] {'eval/walltime': 1000.3778100013733, 'training/sps': 42198.52514622219, 'training/walltime': 1411.424423456192, 'training/entropy_loss': Array(-0.02333036, dtype=float32), 'training/policy_loss': Array(0.00059036, dtype=float32), 'training/total_loss': Array(32.412125, dtype=float32), 'training/v_loss': Array(32.434868, dtype=float32), 'eval/episode_goal_distance': (Array(0.36714008, dtype=float32), Array(0.12538558, dtype=float32)), 'eval/episode_reward': (Array(-8135.84, dtype=float32), Array(3610.1606, dtype=float32)), 'eval/avg_episode_length': (Array(914.5, dtype=float32), Array(278.8455, dtype=float32)), 'eval/epoch_eval_time': 4.093232154846191, 'eval/sps': 31271.131261991606}
I0727 03:22:13.600804 140120985872192 train.py:379] starting iteration 242 2421.3125524520874
I0727 03:22:23.532272 140120985872192 train.py:394] {'eval/walltime': 1004.4857831001282, 'training/sps': 42227.57266993317, 'training/walltime': 1417.2443175315857, 'training/entropy_loss': Array(-0.02430629, dtype=float32), 'training/policy_loss': Array(0.00050169, dtype=float32), 'training/total_loss': Array(20.194324, dtype=float32), 'training/v_loss': Array(20.218128, dtype=float32), 'eval/episode_goal_distance': (Array(0.38934332, dtype=float32), Array(0.1291778, dtype=float32)), 'eval/episode_reward': (Array(-8902.069, dtype=float32), Array(3449.0664, dtype=float32)), 'eval/avg_episode_length': (Array(937.8906, dtype=float32), Array(240.5488, dtype=float32)), 'eval/epoch_eval_time': 4.107973098754883, 'eval/sps': 31158.9187472519}
I0727 03:22:23.534667 140120985872192 train.py:379] starting iteration 243 2431.2464158535004
I0727 03:22:33.457378 140120985872192 train.py:394] {'eval/walltime': 1008.570333480835, 'training/sps': 42122.08973250308, 'training/walltime': 1423.0787858963013, 'training/entropy_loss': Array(-0.02384098, dtype=float32), 'training/policy_loss': Array(0.00054086, dtype=float32), 'training/total_loss': Array(12.72714, dtype=float32), 'training/v_loss': Array(12.750441, dtype=float32), 'eval/episode_goal_distance': (Array(0.3610489, dtype=float32), Array(0.13982187, dtype=float32)), 'eval/episode_reward': (Array(-8103.0547, dtype=float32), Array(3932.0857, dtype=float32)), 'eval/avg_episode_length': (Array(899.0547, dtype=float32), Array(300.23715, dtype=float32)), 'eval/epoch_eval_time': 4.084550380706787, 'eval/sps': 31337.59852849483}
I0727 03:22:33.459822 140120985872192 train.py:379] starting iteration 244 2441.1715710163116
I0727 03:22:43.373129 140120985872192 train.py:394] {'eval/walltime': 1012.6713242530823, 'training/sps': 42309.429050676394, 'training/walltime': 1428.8874201774597, 'training/entropy_loss': Array(-0.02331793, dtype=float32), 'training/policy_loss': Array(0.00078866, dtype=float32), 'training/total_loss': Array(11.126353, dtype=float32), 'training/v_loss': Array(11.148882, dtype=float32), 'eval/episode_goal_distance': (Array(0.35560083, dtype=float32), Array(0.11977276, dtype=float32)), 'eval/episode_reward': (Array(-7754.6123, dtype=float32), Array(3872.0237, dtype=float32)), 'eval/avg_episode_length': (Array(868.0547, dtype=float32), Array(337.15662, dtype=float32)), 'eval/epoch_eval_time': 4.1009907722473145, 'eval/sps': 31211.96976745619}
I0727 03:22:43.375491 140120985872192 train.py:379] starting iteration 245 2451.0872395038605
I0727 03:22:53.262210 140120985872192 train.py:394] {'eval/walltime': 1016.7480087280273, 'training/sps': 42326.20965536849, 'training/walltime': 1434.6937515735626, 'training/entropy_loss': Array(-0.02403292, dtype=float32), 'training/policy_loss': Array(0.00058408, dtype=float32), 'training/total_loss': Array(12.827888, dtype=float32), 'training/v_loss': Array(12.851336, dtype=float32), 'eval/episode_goal_distance': (Array(0.3698439, dtype=float32), Array(0.1333777, dtype=float32)), 'eval/episode_reward': (Array(-8708.137, dtype=float32), Array(3375.6853, dtype=float32)), 'eval/avg_episode_length': (Array(953.4219, dtype=float32), Array(210.03261, dtype=float32)), 'eval/epoch_eval_time': 4.076684474945068, 'eval/sps': 31398.06398721224}
I0727 03:22:53.264761 140120985872192 train.py:379] starting iteration 246 2460.9765100479126
I0727 03:23:03.168061 140120985872192 train.py:394] {'eval/walltime': 1020.823349237442, 'training/sps': 42196.14475598383, 'training/walltime': 1440.517980337143, 'training/entropy_loss': Array(-0.02413205, dtype=float32), 'training/policy_loss': Array(0.00061038, dtype=float32), 'training/total_loss': Array(15.141102, dtype=float32), 'training/v_loss': Array(15.164624, dtype=float32), 'eval/episode_goal_distance': (Array(0.40940183, dtype=float32), Array(0.15060048, dtype=float32)), 'eval/episode_reward': (Array(-9061.793, dtype=float32), Array(4091.7773, dtype=float32)), 'eval/avg_episode_length': (Array(899.0781, dtype=float32), Array(300.16724, dtype=float32)), 'eval/epoch_eval_time': 4.075340509414673, 'eval/sps': 31408.418438729233}
I0727 03:23:03.170440 140120985872192 train.py:379] starting iteration 247 2470.882188796997
I0727 03:23:13.088474 140120985872192 train.py:394] {'eval/walltime': 1024.9025676250458, 'training/sps': 42117.595978940226, 'training/walltime': 1446.3530712127686, 'training/entropy_loss': Array(-0.02364336, dtype=float32), 'training/policy_loss': Array(0.00041788, dtype=float32), 'training/total_loss': Array(12.570675, dtype=float32), 'training/v_loss': Array(12.593899, dtype=float32), 'eval/episode_goal_distance': (Array(0.3950016, dtype=float32), Array(0.14958836, dtype=float32)), 'eval/episode_reward': (Array(-8848.188, dtype=float32), Array(3773.4229, dtype=float32)), 'eval/avg_episode_length': (Array(930.0781, dtype=float32), Array(254.25272, dtype=float32)), 'eval/epoch_eval_time': 4.07921838760376, 'eval/sps': 31378.560262666047}
I0727 03:23:13.091029 140120985872192 train.py:379] starting iteration 248 2480.802777528763
I0727 03:23:23.044901 140120985872192 train.py:394] {'eval/walltime': 1028.9859528541565, 'training/sps': 41889.97867853748, 'training/walltime': 1452.219868183136, 'training/entropy_loss': Array(-0.02551759, dtype=float32), 'training/policy_loss': Array(0.00041709, dtype=float32), 'training/total_loss': Array(23.769781, dtype=float32), 'training/v_loss': Array(23.794884, dtype=float32), 'eval/episode_goal_distance': (Array(0.42458528, dtype=float32), Array(0.16467683, dtype=float32)), 'eval/episode_reward': (Array(-8826.107, dtype=float32), Array(4078.8467, dtype=float32)), 'eval/avg_episode_length': (Array(906.7969, dtype=float32), Array(289.78052, dtype=float32)), 'eval/epoch_eval_time': 4.083385229110718, 'eval/sps': 31346.5403870004}
I0727 03:23:23.047328 140120985872192 train.py:379] starting iteration 249 2490.759076833725
I0727 03:23:32.951510 140120985872192 train.py:394] {'eval/walltime': 1033.0991291999817, 'training/sps': 42465.87166994357, 'training/walltime': 1458.0071036815643, 'training/entropy_loss': Array(-0.02507199, dtype=float32), 'training/policy_loss': Array(0.00066998, dtype=float32), 'training/total_loss': Array(16.963345, dtype=float32), 'training/v_loss': Array(16.98775, dtype=float32), 'eval/episode_goal_distance': (Array(0.39904326, dtype=float32), Array(0.14090112, dtype=float32)), 'eval/episode_reward': (Array(-8451.665, dtype=float32), Array(4025.153, dtype=float32)), 'eval/avg_episode_length': (Array(891.3047, dtype=float32), Array(310.16998, dtype=float32)), 'eval/epoch_eval_time': 4.113176345825195, 'eval/sps': 31119.502116634958}
I0727 03:23:32.953928 140120985872192 train.py:379] starting iteration 250 2500.6656777858734
I0727 03:23:42.853179 140120985872192 train.py:394] {'eval/walltime': 1037.1903307437897, 'training/sps': 42340.6555556514, 'training/walltime': 1463.8114540576935, 'training/entropy_loss': Array(-0.02604847, dtype=float32), 'training/policy_loss': Array(0.00051478, dtype=float32), 'training/total_loss': Array(43.37292, dtype=float32), 'training/v_loss': Array(43.398453, dtype=float32), 'eval/episode_goal_distance': (Array(0.43785614, dtype=float32), Array(0.17120065, dtype=float32)), 'eval/episode_reward': (Array(-9092.145, dtype=float32), Array(4483.147, dtype=float32)), 'eval/avg_episode_length': (Array(891.34375, dtype=float32), Array(310.0586, dtype=float32)), 'eval/epoch_eval_time': 4.091201543807983, 'eval/sps': 31286.65225347489}
I0727 03:23:42.855587 140120985872192 train.py:379] starting iteration 251 2510.5673360824585
I0727 03:23:52.761810 140120985872192 train.py:394] {'eval/walltime': 1041.2751922607422, 'training/sps': 42242.236937724316, 'training/walltime': 1469.6293277740479, 'training/entropy_loss': Array(-0.02685673, dtype=float32), 'training/policy_loss': Array(0.00018364, dtype=float32), 'training/total_loss': Array(15.990143, dtype=float32), 'training/v_loss': Array(16.016815, dtype=float32), 'eval/episode_goal_distance': (Array(0.42504227, dtype=float32), Array(0.16776952, dtype=float32)), 'eval/episode_reward': (Array(-9045.382, dtype=float32), Array(4189.4883, dtype=float32)), 'eval/avg_episode_length': (Array(914.53125, dtype=float32), Array(278.7432, dtype=float32)), 'eval/epoch_eval_time': 4.084861516952515, 'eval/sps': 31335.211602349154}
I0727 03:23:52.764183 140120985872192 train.py:379] starting iteration 252 2520.475932121277
I0727 03:24:02.688058 140120985872192 train.py:394] {'eval/walltime': 1045.3982017040253, 'training/sps': 42393.21956005472, 'training/walltime': 1475.4264812469482, 'training/entropy_loss': Array(-0.02628506, dtype=float32), 'training/policy_loss': Array(0.0006721, dtype=float32), 'training/total_loss': Array(14.014777, dtype=float32), 'training/v_loss': Array(14.04039, dtype=float32), 'eval/episode_goal_distance': (Array(0.3789603, dtype=float32), Array(0.13008332, dtype=float32)), 'eval/episode_reward': (Array(-8178.1196, dtype=float32), Array(3637.664, dtype=float32)), 'eval/avg_episode_length': (Array(899.14844, dtype=float32), Array(299.95786, dtype=float32)), 'eval/epoch_eval_time': 4.123009443283081, 'eval/sps': 31045.284217946348}
I0727 03:24:02.690469 140120985872192 train.py:379] starting iteration 253 2530.402218103409
I0727 03:24:12.609865 140120985872192 train.py:394] {'eval/walltime': 1049.4823770523071, 'training/sps': 42142.260609347875, 'training/walltime': 1481.2581570148468, 'training/entropy_loss': Array(-0.02577859, dtype=float32), 'training/policy_loss': Array(0.00087603, dtype=float32), 'training/total_loss': Array(13.537443, dtype=float32), 'training/v_loss': Array(13.562346, dtype=float32), 'eval/episode_goal_distance': (Array(0.3465942, dtype=float32), Array(0.11995, dtype=float32)), 'eval/episode_reward': (Array(-7448.5767, dtype=float32), Array(3890.2634, dtype=float32)), 'eval/avg_episode_length': (Array(844.6719, dtype=float32), Array(360.9508, dtype=float32)), 'eval/epoch_eval_time': 4.08417534828186, 'eval/sps': 31340.476126679358}
I0727 03:24:12.612249 140120985872192 train.py:379] starting iteration 254 2540.3239982128143
I0727 03:24:22.519117 140120985872192 train.py:394] {'eval/walltime': 1053.5915277004242, 'training/sps': 42416.408569414256, 'training/walltime': 1487.0521411895752, 'training/entropy_loss': Array(-0.0236981, dtype=float32), 'training/policy_loss': Array(0.0015186, dtype=float32), 'training/total_loss': Array(15.182687, dtype=float32), 'training/v_loss': Array(15.204867, dtype=float32), 'eval/episode_goal_distance': (Array(0.31756073, dtype=float32), Array(0.10880593, dtype=float32)), 'eval/episode_reward': (Array(-7285.121, dtype=float32), Array(3378.7166, dtype=float32)), 'eval/avg_episode_length': (Array(898.97656, dtype=float32), Array(300.46918, dtype=float32)), 'eval/epoch_eval_time': 4.109150648117065, 'eval/sps': 31149.98961127244}
I0727 03:24:22.521534 140120985872192 train.py:379] starting iteration 255 2550.2332825660706
I0727 03:24:32.455378 140120985872192 train.py:394] {'eval/walltime': 1057.6781005859375, 'training/sps': 42056.650798741764, 'training/walltime': 1492.8956878185272, 'training/entropy_loss': Array(-0.02038814, dtype=float32), 'training/policy_loss': Array(0.00298004, dtype=float32), 'training/total_loss': Array(11.563475, dtype=float32), 'training/v_loss': Array(11.580882, dtype=float32), 'eval/episode_goal_distance': (Array(0.33900315, dtype=float32), Array(0.09425402, dtype=float32)), 'eval/episode_reward': (Array(-7084.1016, dtype=float32), Array(4098.444, dtype=float32)), 'eval/avg_episode_length': (Array(813.5469, dtype=float32), Array(388.13385, dtype=float32)), 'eval/epoch_eval_time': 4.086572885513306, 'eval/sps': 31322.08909175547}
I0727 03:24:32.572561 140120985872192 train.py:379] starting iteration 256 2560.2843084335327
I0727 03:24:42.431082 140120985872192 train.py:394] {'eval/walltime': 1061.7672662734985, 'training/sps': 42625.27155477937, 'training/walltime': 1498.6612815856934, 'training/entropy_loss': Array(-0.0165638, dtype=float32), 'training/policy_loss': Array(0.00429502, dtype=float32), 'training/total_loss': Array(14.055412, dtype=float32), 'training/v_loss': Array(14.067682, dtype=float32), 'eval/episode_goal_distance': (Array(0.3107252, dtype=float32), Array(0.11157431, dtype=float32)), 'eval/episode_reward': (Array(-7686.212, dtype=float32), Array(3360.358, dtype=float32)), 'eval/avg_episode_length': (Array(930.16406, dtype=float32), Array(253.94081, dtype=float32)), 'eval/epoch_eval_time': 4.089165687561035, 'eval/sps': 31302.228811458368}
I0727 03:24:42.433707 140120985872192 train.py:379] starting iteration 257 2570.1454560756683
I0727 03:24:52.366892 140120985872192 train.py:394] {'eval/walltime': 1065.8609211444855, 'training/sps': 42110.49501533442, 'training/walltime': 1504.497356414795, 'training/entropy_loss': Array(-0.01269564, dtype=float32), 'training/policy_loss': Array(0.00503866, dtype=float32), 'training/total_loss': Array(17.122482, dtype=float32), 'training/v_loss': Array(17.130142, dtype=float32), 'eval/episode_goal_distance': (Array(0.32180047, dtype=float32), Array(0.10272233, dtype=float32)), 'eval/episode_reward': (Array(-7571.4146, dtype=float32), Array(3763.1646, dtype=float32)), 'eval/avg_episode_length': (Array(875.6953, dtype=float32), Array(328.87973, dtype=float32)), 'eval/epoch_eval_time': 4.0936548709869385, 'eval/sps': 31267.902164195024}
I0727 03:24:52.369319 140120985872192 train.py:379] starting iteration 258 2580.0810680389404
I0727 03:25:02.270897 140120985872192 train.py:394] {'eval/walltime': 1069.9624519348145, 'training/sps': 42398.62335212041, 'training/walltime': 1510.2937710285187, 'training/entropy_loss': Array(-0.01034306, dtype=float32), 'training/policy_loss': Array(0.00456538, dtype=float32), 'training/total_loss': Array(38.648613, dtype=float32), 'training/v_loss': Array(38.654392, dtype=float32), 'eval/episode_goal_distance': (Array(0.28448594, dtype=float32), Array(0.0898411, dtype=float32)), 'eval/episode_reward': (Array(-6843.7534, dtype=float32), Array(2992.7659, dtype=float32)), 'eval/avg_episode_length': (Array(914.53906, dtype=float32), Array(278.718, dtype=float32)), 'eval/epoch_eval_time': 4.1015307903289795, 'eval/sps': 31207.860319325617}
I0727 03:25:02.273370 140120985872192 train.py:379] starting iteration 259 2589.985119342804
I0727 03:25:12.185557 140120985872192 train.py:394] {'eval/walltime': 1074.0392520427704, 'training/sps': 42142.00561905863, 'training/walltime': 1516.125482082367, 'training/entropy_loss': Array(-0.00514201, dtype=float32), 'training/policy_loss': Array(0.00467608, dtype=float32), 'training/total_loss': Array(17.582376, dtype=float32), 'training/v_loss': Array(17.582842, dtype=float32), 'eval/episode_goal_distance': (Array(0.32189918, dtype=float32), Array(0.10477281, dtype=float32)), 'eval/episode_reward': (Array(-6925.0547, dtype=float32), Array(3873.9194, dtype=float32)), 'eval/avg_episode_length': (Array(844.7031, dtype=float32), Array(360.87808, dtype=float32)), 'eval/epoch_eval_time': 4.076800107955933, 'eval/sps': 31397.173422902488}
I0727 03:25:12.188054 140120985872192 train.py:379] starting iteration 260 2599.899802207947
I0727 03:25:22.115072 140120985872192 train.py:394] {'eval/walltime': 1078.1520478725433, 'training/sps': 42295.505991950005, 'training/walltime': 1521.9360284805298, 'training/entropy_loss': Array(0.0028463, dtype=float32), 'training/policy_loss': Array(0.00574433, dtype=float32), 'training/total_loss': Array(8.274496, dtype=float32), 'training/v_loss': Array(8.265906, dtype=float32), 'eval/episode_goal_distance': (Array(0.31342942, dtype=float32), Array(0.1035054, dtype=float32)), 'eval/episode_reward': (Array(-7586.203, dtype=float32), Array(3399.5344, dtype=float32)), 'eval/avg_episode_length': (Array(914.52344, dtype=float32), Array(278.76913, dtype=float32)), 'eval/epoch_eval_time': 4.112795829772949, 'eval/sps': 31122.381294348463}
I0727 03:25:22.117578 140120985872192 train.py:379] starting iteration 261 2609.829327106476
I0727 03:25:32.003235 140120985872192 train.py:394] {'eval/walltime': 1082.2249734401703, 'training/sps': 42306.523895426246, 'training/walltime': 1527.745061635971, 'training/entropy_loss': Array(0.00895563, dtype=float32), 'training/policy_loss': Array(0.00621427, dtype=float32), 'training/total_loss': Array(8.252646, dtype=float32), 'training/v_loss': Array(8.237476, dtype=float32), 'eval/episode_goal_distance': (Array(0.323299, dtype=float32), Array(0.10203909, dtype=float32)), 'eval/episode_reward': (Array(-7365.0303, dtype=float32), Array(3710.8633, dtype=float32)), 'eval/avg_episode_length': (Array(875.71875, dtype=float32), Array(328.81772, dtype=float32)), 'eval/epoch_eval_time': 4.072925567626953, 'eval/sps': 31427.041293705213}
I0727 03:25:32.006202 140120985872192 train.py:379] starting iteration 262 2619.7179510593414
I0727 03:25:41.903423 140120985872192 train.py:394] {'eval/walltime': 1086.3148703575134, 'training/sps': 42345.62844977463, 'training/walltime': 1533.5487303733826, 'training/entropy_loss': Array(0.01475555, dtype=float32), 'training/policy_loss': Array(0.00563507, dtype=float32), 'training/total_loss': Array(9.543405, dtype=float32), 'training/v_loss': Array(9.523014, dtype=float32), 'eval/episode_goal_distance': (Array(0.2864589, dtype=float32), Array(0.10479628, dtype=float32)), 'eval/episode_reward': (Array(-6686.377, dtype=float32), Array(3241.8738, dtype=float32)), 'eval/avg_episode_length': (Array(891.28125, dtype=float32), Array(310.23697, dtype=float32)), 'eval/epoch_eval_time': 4.08989691734314, 'eval/sps': 31296.632308070682}
I0727 03:25:41.905788 140120985872192 train.py:379] starting iteration 263 2629.6175372600555
I0727 03:25:51.845692 140120985872192 train.py:394] {'eval/walltime': 1090.427651643753, 'training/sps': 42202.40724396653, 'training/walltime': 1539.3720948696136, 'training/entropy_loss': Array(0.01957563, dtype=float32), 'training/policy_loss': Array(0.00603853, dtype=float32), 'training/total_loss': Array(8.30262, dtype=float32), 'training/v_loss': Array(8.277006, dtype=float32), 'eval/episode_goal_distance': (Array(0.30478302, dtype=float32), Array(0.1087612, dtype=float32)), 'eval/episode_reward': (Array(-6792.538, dtype=float32), Array(3822.0408, dtype=float32)), 'eval/avg_episode_length': (Array(852.60156, dtype=float32), Array(353.04514, dtype=float32)), 'eval/epoch_eval_time': 4.112781286239624, 'eval/sps': 31122.491348678614}
I0727 03:25:51.848154 140120985872192 train.py:379] starting iteration 264 2639.5599026679993
I0727 03:26:01.754445 140120985872192 train.py:394] {'eval/walltime': 1094.5054919719696, 'training/sps': 42192.5712200512, 'training/walltime': 1545.1968169212341, 'training/entropy_loss': Array(0.02346127, dtype=float32), 'training/policy_loss': Array(0.00531093, dtype=float32), 'training/total_loss': Array(6.660585, dtype=float32), 'training/v_loss': Array(6.631813, dtype=float32), 'eval/episode_goal_distance': (Array(0.3177623, dtype=float32), Array(0.09712046, dtype=float32)), 'eval/episode_reward': (Array(-6850.7173, dtype=float32), Array(3827.3176, dtype=float32)), 'eval/avg_episode_length': (Array(829.3281, dtype=float32), Array(374.63168, dtype=float32)), 'eval/epoch_eval_time': 4.077840328216553, 'eval/sps': 31389.164287357205}
I0727 03:26:01.756826 140120985872192 train.py:379] starting iteration 265 2649.4685747623444
I0727 03:26:11.689396 140120985872192 train.py:394] {'eval/walltime': 1098.6262850761414, 'training/sps': 42313.13009822607, 'training/walltime': 1551.0049431324005, 'training/entropy_loss': Array(0.02880592, dtype=float32), 'training/policy_loss': Array(0.00564384, dtype=float32), 'training/total_loss': Array(15.718184, dtype=float32), 'training/v_loss': Array(15.683734, dtype=float32), 'eval/episode_goal_distance': (Array(0.30674767, dtype=float32), Array(0.10094613, dtype=float32)), 'eval/episode_reward': (Array(-7145.4946, dtype=float32), Array(3690.484, dtype=float32)), 'eval/avg_episode_length': (Array(867.9844, dtype=float32), Array(337.33624, dtype=float32)), 'eval/epoch_eval_time': 4.120793104171753, 'eval/sps': 31061.981702118723}
I0727 03:26:11.691897 140120985872192 train.py:379] starting iteration 266 2659.403645992279
I0727 03:26:21.614458 140120985872192 train.py:394] {'eval/walltime': 1102.7072892189026, 'training/sps': 42095.992541258616, 'training/walltime': 1556.8430285453796, 'training/entropy_loss': Array(0.02617821, dtype=float32), 'training/policy_loss': Array(0.00543362, dtype=float32), 'training/total_loss': Array(39.35608, dtype=float32), 'training/v_loss': Array(39.32447, dtype=float32), 'eval/episode_goal_distance': (Array(0.31357914, dtype=float32), Array(0.10103613, dtype=float32)), 'eval/episode_reward': (Array(-6904.062, dtype=float32), Array(3869.4104, dtype=float32)), 'eval/avg_episode_length': (Array(844.7344, dtype=float32), Array(360.80548, dtype=float32)), 'eval/epoch_eval_time': 4.0810041427612305, 'eval/sps': 31364.829714040545}
I0727 03:26:21.616871 140120985872192 train.py:379] starting iteration 267 2669.32861995697
I0727 03:26:31.554638 140120985872192 train.py:394] {'eval/walltime': 1106.8256916999817, 'training/sps': 42258.03915465896, 'training/walltime': 1562.6587266921997, 'training/entropy_loss': Array(0.0236082, dtype=float32), 'training/policy_loss': Array(0.0086397, dtype=float32), 'training/total_loss': Array(19.371908, dtype=float32), 'training/v_loss': Array(19.339659, dtype=float32), 'eval/episode_goal_distance': (Array(0.32053915, dtype=float32), Array(0.10467881, dtype=float32)), 'eval/episode_reward': (Array(-6994.9395, dtype=float32), Array(3957.137, dtype=float32)), 'eval/avg_episode_length': (Array(829.21094, dtype=float32), Array(374.88898, dtype=float32)), 'eval/epoch_eval_time': 4.118402481079102, 'eval/sps': 31080.012356262352}
I0727 03:26:31.557178 140120985872192 train.py:379] starting iteration 268 2679.268927335739
I0727 03:26:41.484441 140120985872192 train.py:394] {'eval/walltime': 1110.9129815101624, 'training/sps': 42107.9043707014, 'training/walltime': 1568.4951605796814, 'training/entropy_loss': Array(0.03031124, dtype=float32), 'training/policy_loss': Array(0.00803658, dtype=float32), 'training/total_loss': Array(10.8875, dtype=float32), 'training/v_loss': Array(10.849152, dtype=float32), 'eval/episode_goal_distance': (Array(0.31464267, dtype=float32), Array(0.1006674, dtype=float32)), 'eval/episode_reward': (Array(-6934.9946, dtype=float32), Array(3712.6958, dtype=float32)), 'eval/avg_episode_length': (Array(852.39844, dtype=float32), Array(353.5313, dtype=float32)), 'eval/epoch_eval_time': 4.087289810180664, 'eval/sps': 31316.595089777158}
I0727 03:26:41.486850 140120985872192 train.py:379] starting iteration 269 2689.1985976696014
I0727 03:26:51.388806 140120985872192 train.py:394] {'eval/walltime': 1115.0200216770172, 'training/sps': 42436.22819419753, 'training/walltime': 1574.286438703537, 'training/entropy_loss': Array(0.03526368, dtype=float32), 'training/policy_loss': Array(0.0062159, dtype=float32), 'training/total_loss': Array(8.450069, dtype=float32), 'training/v_loss': Array(8.408589, dtype=float32), 'eval/episode_goal_distance': (Array(0.31141138, dtype=float32), Array(0.09908146, dtype=float32)), 'eval/episode_reward': (Array(-6991.8667, dtype=float32), Array(3746.526, dtype=float32)), 'eval/avg_episode_length': (Array(860.0547, dtype=float32), Array(345.95438, dtype=float32)), 'eval/epoch_eval_time': 4.107040166854858, 'eval/sps': 31165.99663012828}
I0727 03:26:51.391216 140120985872192 train.py:379] starting iteration 270 2699.102965593338
I0727 03:27:01.281536 140120985872192 train.py:394] {'eval/walltime': 1119.0991740226746, 'training/sps': 42316.32279356854, 'training/walltime': 1580.094126701355, 'training/entropy_loss': Array(0.03857776, dtype=float32), 'training/policy_loss': Array(0.00579121, dtype=float32), 'training/total_loss': Array(8.533651, dtype=float32), 'training/v_loss': Array(8.489282, dtype=float32), 'eval/episode_goal_distance': (Array(0.32770836, dtype=float32), Array(0.1103127, dtype=float32)), 'eval/episode_reward': (Array(-7411.868, dtype=float32), Array(3980.4216, dtype=float32)), 'eval/avg_episode_length': (Array(867.8672, dtype=float32), Array(337.63596, dtype=float32)), 'eval/epoch_eval_time': 4.079152345657349, 'eval/sps': 31379.068285171637}
I0727 03:27:01.283896 140120985872192 train.py:379] starting iteration 271 2708.995644569397
I0727 03:27:11.172982 140120985872192 train.py:394] {'eval/walltime': 1123.1858868598938, 'training/sps': 42380.588187673755, 'training/walltime': 1585.8930079936981, 'training/entropy_loss': Array(0.03881235, dtype=float32), 'training/policy_loss': Array(0.00625014, dtype=float32), 'training/total_loss': Array(9.256596, dtype=float32), 'training/v_loss': Array(9.211533, dtype=float32), 'eval/episode_goal_distance': (Array(0.3131097, dtype=float32), Array(0.0996784, dtype=float32)), 'eval/episode_reward': (Array(-6352.1387, dtype=float32), Array(3886.8232, dtype=float32)), 'eval/avg_episode_length': (Array(798.0781, dtype=float32), Array(399.94287, dtype=float32)), 'eval/epoch_eval_time': 4.086712837219238, 'eval/sps': 31321.01644976266}
I0727 03:27:11.175380 140120985872192 train.py:379] starting iteration 272 2718.8871290683746
I0727 03:27:21.120078 140120985872192 train.py:394] {'eval/walltime': 1127.308655500412, 'training/sps': 42239.337540525776, 'training/walltime': 1591.7112810611725, 'training/entropy_loss': Array(0.03614514, dtype=float32), 'training/policy_loss': Array(0.00711223, dtype=float32), 'training/total_loss': Array(8.076803, dtype=float32), 'training/v_loss': Array(8.0335455, dtype=float32), 'eval/episode_goal_distance': (Array(0.32983196, dtype=float32), Array(0.11300813, dtype=float32)), 'eval/episode_reward': (Array(-7596.2964, dtype=float32), Array(3752.6638, dtype=float32)), 'eval/avg_episode_length': (Array(883.5547, dtype=float32), Array(319.60712, dtype=float32)), 'eval/epoch_eval_time': 4.1227686405181885, 'eval/sps': 31047.097511615822}
I0727 03:27:21.122552 140120985872192 train.py:379] starting iteration 273 2728.834300518036
I0727 03:27:31.082182 140120985872192 train.py:394] {'eval/walltime': 1131.4042074680328, 'training/sps': 41937.1767130865, 'training/walltime': 1597.5714752674103, 'training/entropy_loss': Array(0.03202694, dtype=float32), 'training/policy_loss': Array(0.00957008, dtype=float32), 'training/total_loss': Array(14.552307, dtype=float32), 'training/v_loss': Array(14.51071, dtype=float32), 'eval/episode_goal_distance': (Array(0.33553874, dtype=float32), Array(0.13478543, dtype=float32)), 'eval/episode_reward': (Array(-7480.605, dtype=float32), Array(4390.884, dtype=float32)), 'eval/avg_episode_length': (Array(844.625, dtype=float32), Array(361.05963, dtype=float32)), 'eval/epoch_eval_time': 4.09555196762085, 'eval/sps': 31253.41858971859}
I0727 03:27:31.084920 140120985872192 train.py:379] starting iteration 274 2738.796668767929
I0727 03:27:41.010113 140120985872192 train.py:394] {'eval/walltime': 1135.5248219966888, 'training/sps': 42367.42973038236, 'training/walltime': 1603.3721575737, 'training/entropy_loss': Array(0.02763187, dtype=float32), 'training/policy_loss': Array(0.00569544, dtype=float32), 'training/total_loss': Array(15.421268, dtype=float32), 'training/v_loss': Array(15.38794, dtype=float32), 'eval/episode_goal_distance': (Array(0.33394313, dtype=float32), Array(0.13153209, dtype=float32)), 'eval/episode_reward': (Array(-7108.812, dtype=float32), Array(4190.066, dtype=float32)), 'eval/avg_episode_length': (Array(836.7656, dtype=float32), Array(368.46365, dtype=float32)), 'eval/epoch_eval_time': 4.120614528656006, 'eval/sps': 31063.327838566092}
I0727 03:27:41.012454 140120985872192 train.py:379] starting iteration 275 2748.724203109741
I0727 03:27:50.921575 140120985872192 train.py:394] {'eval/walltime': 1139.6097347736359, 'training/sps': 42223.954028135675, 'training/walltime': 1609.192550420761, 'training/entropy_loss': Array(0.01987666, dtype=float32), 'training/policy_loss': Array(0.00685975, dtype=float32), 'training/total_loss': Array(48.590675, dtype=float32), 'training/v_loss': Array(48.563934, dtype=float32), 'eval/episode_goal_distance': (Array(0.33309543, dtype=float32), Array(0.11663423, dtype=float32)), 'eval/episode_reward': (Array(-7416.1855, dtype=float32), Array(3605.732, dtype=float32)), 'eval/avg_episode_length': (Array(883.4453, dtype=float32), Array(319.9076, dtype=float32)), 'eval/epoch_eval_time': 4.0849127769470215, 'eval/sps': 31334.818388867665}
I0727 03:27:50.923987 140120985872192 train.py:379] starting iteration 276 2758.6357362270355
I0727 03:28:00.855367 140120985872192 train.py:394] {'eval/walltime': 1143.702318429947, 'training/sps': 42118.23616407654, 'training/walltime': 1615.0275526046753, 'training/entropy_loss': Array(0.01429177, dtype=float32), 'training/policy_loss': Array(0.00766256, dtype=float32), 'training/total_loss': Array(16.706463, dtype=float32), 'training/v_loss': Array(16.684507, dtype=float32), 'eval/episode_goal_distance': (Array(0.33912396, dtype=float32), Array(0.12391447, dtype=float32)), 'eval/episode_reward': (Array(-7750.274, dtype=float32), Array(3994.385, dtype=float32)), 'eval/avg_episode_length': (Array(867.9844, dtype=float32), Array(337.33643, dtype=float32)), 'eval/epoch_eval_time': 4.092583656311035, 'eval/sps': 31276.086391689398}
I0727 03:28:00.857564 140120985872192 train.py:379] starting iteration 277 2768.569313287735
I0727 03:28:10.740856 140120985872192 train.py:394] {'eval/walltime': 1147.7777137756348, 'training/sps': 42342.45916014481, 'training/walltime': 1620.831655740738, 'training/entropy_loss': Array(0.00765209, dtype=float32), 'training/policy_loss': Array(0.00645273, dtype=float32), 'training/total_loss': Array(15.277292, dtype=float32), 'training/v_loss': Array(15.263188, dtype=float32), 'eval/episode_goal_distance': (Array(0.34754908, dtype=float32), Array(0.13834572, dtype=float32)), 'eval/episode_reward': (Array(-7875.3506, dtype=float32), Array(4156.539, dtype=float32)), 'eval/avg_episode_length': (Array(867.91406, dtype=float32), Array(337.5158, dtype=float32)), 'eval/epoch_eval_time': 4.075395345687866, 'eval/sps': 31407.99582436474}
I0727 03:28:10.743121 140120985872192 train.py:379] starting iteration 278 2778.4548695087433
I0727 03:28:20.662389 140120985872192 train.py:394] {'eval/walltime': 1151.8782913684845, 'training/sps': 42263.96998398852, 'training/walltime': 1626.6465377807617, 'training/entropy_loss': Array(0.00211192, dtype=float32), 'training/policy_loss': Array(0.01723295, dtype=float32), 'training/total_loss': Array(14.086195, dtype=float32), 'training/v_loss': Array(14.066848, dtype=float32), 'eval/episode_goal_distance': (Array(0.36069852, dtype=float32), Array(0.14363341, dtype=float32)), 'eval/episode_reward': (Array(-8131.1475, dtype=float32), Array(3829.0503, dtype=float32)), 'eval/avg_episode_length': (Array(891.3047, dtype=float32), Array(310.17017, dtype=float32)), 'eval/epoch_eval_time': 4.1005775928497314, 'eval/sps': 31215.114725105177}
I0727 03:28:20.664548 140120985872192 train.py:379] starting iteration 279 2788.3762969970703
I0727 03:28:30.553913 140120985872192 train.py:394] {'eval/walltime': 1155.9523425102234, 'training/sps': 42287.8591542651, 'training/walltime': 1632.4581348896027, 'training/entropy_loss': Array(-0.00242174, dtype=float32), 'training/policy_loss': Array(0.00526806, dtype=float32), 'training/total_loss': Array(15.891735, dtype=float32), 'training/v_loss': Array(15.888889, dtype=float32), 'eval/episode_goal_distance': (Array(0.35371006, dtype=float32), Array(0.1265032, dtype=float32)), 'eval/episode_reward': (Array(-7807.24, dtype=float32), Array(4213.205, dtype=float32)), 'eval/avg_episode_length': (Array(844.7344, dtype=float32), Array(360.80566, dtype=float32)), 'eval/epoch_eval_time': 4.074051141738892, 'eval/sps': 31418.358667281453}
I0727 03:28:30.556327 140120985872192 train.py:379] starting iteration 280 2798.268076658249
I0727 03:28:40.506029 140120985872192 train.py:394] {'eval/walltime': 1160.0528831481934, 'training/sps': 42042.19052574388, 'training/walltime': 1638.3036913871765, 'training/entropy_loss': Array(-0.00427101, dtype=float32), 'training/policy_loss': Array(0.00349968, dtype=float32), 'training/total_loss': Array(15.668628, dtype=float32), 'training/v_loss': Array(15.669399, dtype=float32), 'eval/episode_goal_distance': (Array(0.35140675, dtype=float32), Array(0.1556309, dtype=float32)), 'eval/episode_reward': (Array(-8552.561, dtype=float32), Array(4361.8203, dtype=float32)), 'eval/avg_episode_length': (Array(914.6094, dtype=float32), Array(278.48898, dtype=float32)), 'eval/epoch_eval_time': 4.100540637969971, 'eval/sps': 31215.396041866363}
I0727 03:28:40.508449 140120985872192 train.py:379] starting iteration 281 2808.2201974391937
I0727 03:28:50.399197 140120985872192 train.py:394] {'eval/walltime': 1164.1273109912872, 'training/sps': 42280.39717460776, 'training/walltime': 1644.1163141727448, 'training/entropy_loss': Array(-0.00812546, dtype=float32), 'training/policy_loss': Array(0.00457659, dtype=float32), 'training/total_loss': Array(17.988758, dtype=float32), 'training/v_loss': Array(17.992308, dtype=float32), 'eval/episode_goal_distance': (Array(0.3714237, dtype=float32), Array(0.14847203, dtype=float32)), 'eval/episode_reward': (Array(-7878.942, dtype=float32), Array(4331.4375, dtype=float32)), 'eval/avg_episode_length': (Array(844.625, dtype=float32), Array(361.0592, dtype=float32)), 'eval/epoch_eval_time': 4.074427843093872, 'eval/sps': 31415.453881938083}
I0727 03:28:50.401705 140120985872192 train.py:379] starting iteration 282 2818.1134538650513
I0727 03:29:00.325910 140120985872192 train.py:394] {'eval/walltime': 1168.2114272117615, 'training/sps': 42108.127986154046, 'training/walltime': 1649.9527170658112, 'training/entropy_loss': Array(-0.00938683, dtype=float32), 'training/policy_loss': Array(0.01234739, dtype=float32), 'training/total_loss': Array(20.019926, dtype=float32), 'training/v_loss': Array(20.016966, dtype=float32), 'eval/episode_goal_distance': (Array(0.33923832, dtype=float32), Array(0.12001114, dtype=float32)), 'eval/episode_reward': (Array(-7178.5156, dtype=float32), Array(4222.754, dtype=float32)), 'eval/avg_episode_length': (Array(821.33594, dtype=float32), Array(381.7411, dtype=float32)), 'eval/epoch_eval_time': 4.084116220474243, 'eval/sps': 31340.92985853786}
I0727 03:29:00.328435 140120985872192 train.py:379] starting iteration 283 2828.0401842594147
I0727 03:29:10.273692 140120985872192 train.py:394] {'eval/walltime': 1172.3245499134064, 'training/sps': 42165.857112861166, 'training/walltime': 1655.781129360199, 'training/entropy_loss': Array(-0.00407965, dtype=float32), 'training/policy_loss': Array(0.0041682, dtype=float32), 'training/total_loss': Array(45.62416, dtype=float32), 'training/v_loss': Array(45.624077, dtype=float32), 'eval/episode_goal_distance': (Array(0.3630417, dtype=float32), Array(0.15208727, dtype=float32)), 'eval/episode_reward': (Array(-8284.539, dtype=float32), Array(3762.9226, dtype=float32)), 'eval/avg_episode_length': (Array(922.375, dtype=float32), Array(266.65167, dtype=float32)), 'eval/epoch_eval_time': 4.1131227016448975, 'eval/sps': 31119.907983491703}
I0727 03:29:10.276271 140120985872192 train.py:379] starting iteration 284 2837.988020181656
I0727 03:29:20.231044 140120985872192 train.py:394] {'eval/walltime': 1176.4213438034058, 'training/sps': 41981.59145491351, 'training/walltime': 1661.6351237297058, 'training/entropy_loss': Array(-0.00598656, dtype=float32), 'training/policy_loss': Array(0.01291516, dtype=float32), 'training/total_loss': Array(24.5251, dtype=float32), 'training/v_loss': Array(24.518173, dtype=float32), 'eval/episode_goal_distance': (Array(0.36276716, dtype=float32), Array(0.14382327, dtype=float32)), 'eval/episode_reward': (Array(-8315.864, dtype=float32), Array(4114.321, dtype=float32)), 'eval/avg_episode_length': (Array(883.4844, dtype=float32), Array(319.80005, dtype=float32)), 'eval/epoch_eval_time': 4.09679388999939, 'eval/sps': 31243.94427370596}
I0727 03:29:20.233541 140120985872192 train.py:379] starting iteration 285 2847.945289850235
I0727 03:29:30.143005 140120985872192 train.py:394] {'eval/walltime': 1180.510290145874, 'training/sps': 42251.05008636159, 'training/walltime': 1667.4517838954926, 'training/entropy_loss': Array(-0.01246593, dtype=float32), 'training/policy_loss': Array(0.0044266, dtype=float32), 'training/total_loss': Array(18.42874, dtype=float32), 'training/v_loss': Array(18.436779, dtype=float32), 'eval/episode_goal_distance': (Array(0.3341165, dtype=float32), Array(0.13583003, dtype=float32)), 'eval/episode_reward': (Array(-8038.0845, dtype=float32), Array(3866.6677, dtype=float32)), 'eval/avg_episode_length': (Array(906.8125, dtype=float32), Array(289.7322, dtype=float32)), 'eval/epoch_eval_time': 4.088946342468262, 'eval/sps': 31303.907970270346}
I0727 03:29:30.145208 140120985872192 train.py:379] starting iteration 286 2857.8569569587708
I0727 03:29:40.059508 140120985872192 train.py:394] {'eval/walltime': 1184.605446100235, 'training/sps': 42259.166975611624, 'training/walltime': 1673.2673268318176, 'training/entropy_loss': Array(-0.01604521, dtype=float32), 'training/policy_loss': Array(0.00578423, dtype=float32), 'training/total_loss': Array(17.972458, dtype=float32), 'training/v_loss': Array(17.98272, dtype=float32), 'eval/episode_goal_distance': (Array(0.35103118, dtype=float32), Array(0.15403645, dtype=float32)), 'eval/episode_reward': (Array(-8272.423, dtype=float32), Array(3822.6775, dtype=float32)), 'eval/avg_episode_length': (Array(930.0547, dtype=float32), Array(254.33817, dtype=float32)), 'eval/epoch_eval_time': 4.095155954360962, 'eval/sps': 31256.440884428797}
I0727 03:29:40.061668 140120985872192 train.py:379] starting iteration 287 2867.773416996002
I0727 03:29:49.959238 140120985872192 train.py:394] {'eval/walltime': 1188.7122251987457, 'training/sps': 42466.00463084019, 'training/walltime': 1679.054544210434, 'training/entropy_loss': Array(-0.0185297, dtype=float32), 'training/policy_loss': Array(0.00691818, dtype=float32), 'training/total_loss': Array(18.591408, dtype=float32), 'training/v_loss': Array(18.60302, dtype=float32), 'eval/episode_goal_distance': (Array(0.3435133, dtype=float32), Array(0.13407665, dtype=float32)), 'eval/episode_reward': (Array(-7534.593, dtype=float32), Array(3497.7244, dtype=float32)), 'eval/avg_episode_length': (Array(899.1094, dtype=float32), Array(300.07446, dtype=float32)), 'eval/epoch_eval_time': 4.106779098510742, 'eval/sps': 31167.97785554552}
I0727 03:29:49.961463 140120985872192 train.py:379] starting iteration 288 2877.673211812973
I0727 03:29:59.870032 140120985872192 train.py:394] {'eval/walltime': 1192.7977502346039, 'training/sps': 42231.15905830845, 'training/walltime': 1684.8739440441132, 'training/entropy_loss': Array(-0.01935045, dtype=float32), 'training/policy_loss': Array(0.00463701, dtype=float32), 'training/total_loss': Array(17.703123, dtype=float32), 'training/v_loss': Array(17.717836, dtype=float32), 'eval/episode_goal_distance': (Array(0.34156892, dtype=float32), Array(0.15816855, dtype=float32)), 'eval/episode_reward': (Array(-8278.521, dtype=float32), Array(3993.5383, dtype=float32)), 'eval/avg_episode_length': (Array(937.8594, dtype=float32), Array(240.67007, dtype=float32)), 'eval/epoch_eval_time': 4.085525035858154, 'eval/sps': 31330.122536653093}
I0727 03:29:59.872470 140120985872192 train.py:379] starting iteration 289 2887.584219455719
I0727 03:30:09.781075 140120985872192 train.py:394] {'eval/walltime': 1196.8847725391388, 'training/sps': 42240.48686588691, 'training/walltime': 1690.692058801651, 'training/entropy_loss': Array(-0.01783765, dtype=float32), 'training/policy_loss': Array(0.00716615, dtype=float32), 'training/total_loss': Array(17.555323, dtype=float32), 'training/v_loss': Array(17.565994, dtype=float32), 'eval/episode_goal_distance': (Array(0.34953752, dtype=float32), Array(0.15779507, dtype=float32)), 'eval/episode_reward': (Array(-7934.9766, dtype=float32), Array(4175.1147, dtype=float32)), 'eval/avg_episode_length': (Array(875.71875, dtype=float32), Array(328.81778, dtype=float32)), 'eval/epoch_eval_time': 4.087022304534912, 'eval/sps': 31318.6448378255}
I0727 03:30:09.783258 140120985872192 train.py:379] starting iteration 290 2897.4950070381165
I0727 03:30:19.704488 140120985872192 train.py:394] {'eval/walltime': 1201.0088107585907, 'training/sps': 42419.47547104371, 'training/walltime': 1696.485624074936, 'training/entropy_loss': Array(-0.01890152, dtype=float32), 'training/policy_loss': Array(0.00940056, dtype=float32), 'training/total_loss': Array(24.35359, dtype=float32), 'training/v_loss': Array(24.36309, dtype=float32), 'eval/episode_goal_distance': (Array(0.35884136, dtype=float32), Array(0.16197997, dtype=float32)), 'eval/episode_reward': (Array(-7716.0264, dtype=float32), Array(4315.8765, dtype=float32)), 'eval/avg_episode_length': (Array(867.9219, dtype=float32), Array(337.49597, dtype=float32)), 'eval/epoch_eval_time': 4.124038219451904, 'eval/sps': 31037.539709564462}
I0727 03:30:19.706738 140120985872192 train.py:379] starting iteration 291 2907.4184868335724
I0727 03:30:29.633736 140120985872192 train.py:394] {'eval/walltime': 1205.0989744663239, 'training/sps': 42131.14899728083, 'training/walltime': 1702.3188378810883, 'training/entropy_loss': Array(-0.01776932, dtype=float32), 'training/policy_loss': Array(0.0051617, dtype=float32), 'training/total_loss': Array(45.019444, dtype=float32), 'training/v_loss': Array(45.032055, dtype=float32), 'eval/episode_goal_distance': (Array(0.345801, dtype=float32), Array(0.1486751, dtype=float32)), 'eval/episode_reward': (Array(-7876.688, dtype=float32), Array(4226.8667, dtype=float32)), 'eval/avg_episode_length': (Array(875.6797, dtype=float32), Array(328.92142, dtype=float32)), 'eval/epoch_eval_time': 4.090163707733154, 'eval/sps': 31294.590912826814}
I0727 03:30:29.636098 140120985872192 train.py:379] starting iteration 292 2917.3478474617004
I0727 03:30:39.558694 140120985872192 train.py:394] {'eval/walltime': 1209.180483341217, 'training/sps': 42099.66493753678, 'training/walltime': 1708.1564140319824, 'training/entropy_loss': Array(-0.01616119, dtype=float32), 'training/policy_loss': Array(0.00527674, dtype=float32), 'training/total_loss': Array(30.82124, dtype=float32), 'training/v_loss': Array(30.832125, dtype=float32), 'eval/episode_goal_distance': (Array(0.40507394, dtype=float32), Array(0.18609978, dtype=float32)), 'eval/episode_reward': (Array(-9104.629, dtype=float32), Array(4890.3604, dtype=float32)), 'eval/avg_episode_length': (Array(883.4922, dtype=float32), Array(319.77856, dtype=float32)), 'eval/epoch_eval_time': 4.0815088748931885, 'eval/sps': 31360.9510412616}
I0727 03:30:39.560930 140120985872192 train.py:379] starting iteration 293 2927.272679567337
I0727 03:30:49.516644 140120985872192 train.py:394] {'eval/walltime': 1213.2615253925323, 'training/sps': 41859.612035436694, 'training/walltime': 1714.0274670124054, 'training/entropy_loss': Array(-0.01824466, dtype=float32), 'training/policy_loss': Array(0.00746087, dtype=float32), 'training/total_loss': Array(23.118616, dtype=float32), 'training/v_loss': Array(23.129398, dtype=float32), 'eval/episode_goal_distance': (Array(0.38976625, dtype=float32), Array(0.17259426, dtype=float32)), 'eval/episode_reward': (Array(-8083.6064, dtype=float32), Array(4807.254, dtype=float32)), 'eval/avg_episode_length': (Array(829.125, dtype=float32), Array(375.07742, dtype=float32)), 'eval/epoch_eval_time': 4.081042051315308, 'eval/sps': 31364.53836802441}
I0727 03:30:49.518965 140120985872192 train.py:379] starting iteration 294 2937.230714082718
I0727 03:30:59.440206 140120985872192 train.py:394] {'eval/walltime': 1217.3456728458405, 'training/sps': 42129.29103113778, 'training/walltime': 1719.8609380722046, 'training/entropy_loss': Array(-0.01932302, dtype=float32), 'training/policy_loss': Array(0.00816553, dtype=float32), 'training/total_loss': Array(23.592894, dtype=float32), 'training/v_loss': Array(23.604053, dtype=float32), 'eval/episode_goal_distance': (Array(0.40175015, dtype=float32), Array(0.15818472, dtype=float32)), 'eval/episode_reward': (Array(-8929.533, dtype=float32), Array(4047.9983, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.75848, dtype=float32)), 'eval/epoch_eval_time': 4.0841474533081055, 'eval/sps': 31340.69018402401}
I0727 03:30:59.442521 140120985872192 train.py:379] starting iteration 295 2947.154269695282
I0727 03:31:09.404087 140120985872192 train.py:394] {'eval/walltime': 1221.4481985569, 'training/sps': 41970.5199656888, 'training/walltime': 1725.7164766788483, 'training/entropy_loss': Array(-0.02028186, dtype=float32), 'training/policy_loss': Array(0.00485103, dtype=float32), 'training/total_loss': Array(22.257713, dtype=float32), 'training/v_loss': Array(22.273144, dtype=float32), 'eval/episode_goal_distance': (Array(0.37675154, dtype=float32), Array(0.15992261, dtype=float32)), 'eval/episode_reward': (Array(-8435.516, dtype=float32), Array(4566.9263, dtype=float32)), 'eval/avg_episode_length': (Array(867.9844, dtype=float32), Array(337.3363, dtype=float32)), 'eval/epoch_eval_time': 4.10252571105957, 'eval/sps': 31200.291970124203}
I0727 03:31:09.406397 140120985872192 train.py:379] starting iteration 296 2957.118145942688
I0727 03:31:19.308702 140120985872192 train.py:394] {'eval/walltime': 1225.5330064296722, 'training/sps': 42270.131284449315, 'training/walltime': 1731.5305111408234, 'training/entropy_loss': Array(-0.02111473, dtype=float32), 'training/policy_loss': Array(0.00384149, dtype=float32), 'training/total_loss': Array(23.082203, dtype=float32), 'training/v_loss': Array(23.099476, dtype=float32), 'eval/episode_goal_distance': (Array(0.41008472, dtype=float32), Array(0.19115743, dtype=float32)), 'eval/episode_reward': (Array(-9110.775, dtype=float32), Array(4353.444, dtype=float32)), 'eval/avg_episode_length': (Array(898.97656, dtype=float32), Array(300.469, dtype=float32)), 'eval/epoch_eval_time': 4.084807872772217, 'eval/sps': 31335.6231153978}
I0727 03:31:19.310961 140120985872192 train.py:379] starting iteration 297 2967.022709608078
I0727 03:31:29.258008 140120985872192 train.py:394] {'eval/walltime': 1229.630743265152, 'training/sps': 42041.29544786128, 'training/walltime': 1737.3761920928955, 'training/entropy_loss': Array(-0.02319284, dtype=float32), 'training/policy_loss': Array(0.00620689, dtype=float32), 'training/total_loss': Array(22.528429, dtype=float32), 'training/v_loss': Array(22.545416, dtype=float32), 'eval/episode_goal_distance': (Array(0.37619415, dtype=float32), Array(0.17872131, dtype=float32)), 'eval/episode_reward': (Array(-7829.293, dtype=float32), Array(4099.3804, dtype=float32)), 'eval/avg_episode_length': (Array(867.9297, dtype=float32), Array(337.47604, dtype=float32)), 'eval/epoch_eval_time': 4.097736835479736, 'eval/sps': 31236.754613358327}
I0727 03:31:29.260363 140120985872192 train.py:379] starting iteration 298 2976.9721121788025
I0727 03:31:39.182428 140120985872192 train.py:394] {'eval/walltime': 1233.719877243042, 'training/sps': 42159.48131500301, 'training/walltime': 1743.2054858207703, 'training/entropy_loss': Array(-0.02447733, dtype=float32), 'training/policy_loss': Array(0.00587507, dtype=float32), 'training/total_loss': Array(26.564447, dtype=float32), 'training/v_loss': Array(26.58305, dtype=float32), 'eval/episode_goal_distance': (Array(0.3824793, dtype=float32), Array(0.15900314, dtype=float32)), 'eval/episode_reward': (Array(-8308.413, dtype=float32), Array(4502.6177, dtype=float32)), 'eval/avg_episode_length': (Array(860.1953, dtype=float32), Array(345.6068, dtype=float32)), 'eval/epoch_eval_time': 4.089133977890015, 'eval/sps': 31302.471548278238}
I0727 03:31:39.184707 140120985872192 train.py:379] starting iteration 299 2986.8964562416077
I0727 03:31:49.137159 140120985872192 train.py:394] {'eval/walltime': 1237.8313598632812, 'training/sps': 42101.76619397666, 'training/walltime': 1749.0427706241608, 'training/entropy_loss': Array(-0.02446292, dtype=float32), 'training/policy_loss': Array(0.0039563, dtype=float32), 'training/total_loss': Array(24.353737, dtype=float32), 'training/v_loss': Array(24.374243, dtype=float32), 'eval/episode_goal_distance': (Array(0.40416008, dtype=float32), Array(0.17733942, dtype=float32)), 'eval/episode_reward': (Array(-8398.361, dtype=float32), Array(4616.3994, dtype=float32)), 'eval/avg_episode_length': (Array(844.6953, dtype=float32), Array(360.89633, dtype=float32)), 'eval/epoch_eval_time': 4.111482620239258, 'eval/sps': 31132.32179795797}
I0727 03:31:49.139355 140120985872192 train.py:379] starting iteration 300 2996.851104259491
I0727 03:31:59.016780 140120985872192 train.py:394] {'eval/walltime': 1241.9048690795898, 'training/sps': 42371.11654908058, 'training/walltime': 1754.8429481983185, 'training/entropy_loss': Array(-0.0209397, dtype=float32), 'training/policy_loss': Array(0.00452336, dtype=float32), 'training/total_loss': Array(49.18637, dtype=float32), 'training/v_loss': Array(49.20279, dtype=float32), 'eval/episode_goal_distance': (Array(0.39987472, dtype=float32), Array(0.18496059, dtype=float32)), 'eval/episode_reward': (Array(-8978.865, dtype=float32), Array(4225.9688, dtype=float32)), 'eval/avg_episode_length': (Array(922.3828, dtype=float32), Array(266.62436, dtype=float32)), 'eval/epoch_eval_time': 4.073509216308594, 'eval/sps': 31422.538455919675}
I0727 03:31:59.019110 140120985872192 train.py:379] starting iteration 301 3006.730859041214
I0727 03:32:08.906777 140120985872192 train.py:394] {'eval/walltime': 1245.9826712608337, 'training/sps': 42326.857936136876, 'training/walltime': 1760.6491906642914, 'training/entropy_loss': Array(-0.02321265, dtype=float32), 'training/policy_loss': Array(0.00406153, dtype=float32), 'training/total_loss': Array(26.440582, dtype=float32), 'training/v_loss': Array(26.459732, dtype=float32), 'eval/episode_goal_distance': (Array(0.3848044, dtype=float32), Array(0.15850425, dtype=float32)), 'eval/episode_reward': (Array(-8783.939, dtype=float32), Array(4246.9023, dtype=float32)), 'eval/avg_episode_length': (Array(906.8125, dtype=float32), Array(289.732, dtype=float32)), 'eval/epoch_eval_time': 4.0778021812438965, 'eval/sps': 31389.45792631725}
I0727 03:32:08.909274 140120985872192 train.py:379] starting iteration 302 3016.6210231781006
I0727 03:32:18.845353 140120985872192 train.py:394] {'eval/walltime': 1250.083919286728, 'training/sps': 42145.63952201869, 'training/walltime': 1766.4803988933563, 'training/entropy_loss': Array(-0.02291286, dtype=float32), 'training/policy_loss': Array(0.00519422, dtype=float32), 'training/total_loss': Array(22.553448, dtype=float32), 'training/v_loss': Array(22.571163, dtype=float32), 'eval/episode_goal_distance': (Array(0.41524994, dtype=float32), Array(0.20410238, dtype=float32)), 'eval/episode_reward': (Array(-9263.471, dtype=float32), Array(4597.669, dtype=float32)), 'eval/avg_episode_length': (Array(899.0703, dtype=float32), Array(300.19046, dtype=float32)), 'eval/epoch_eval_time': 4.101248025894165, 'eval/sps': 31210.011974853216}
I0727 03:32:18.847861 140120985872192 train.py:379] starting iteration 303 3026.5596103668213
I0727 03:32:28.767309 140120985872192 train.py:394] {'eval/walltime': 1254.1696231365204, 'training/sps': 42153.31602640548, 'training/walltime': 1772.31054520607, 'training/entropy_loss': Array(-0.02107933, dtype=float32), 'training/policy_loss': Array(0.00566242, dtype=float32), 'training/total_loss': Array(20.656395, dtype=float32), 'training/v_loss': Array(20.671812, dtype=float32), 'eval/episode_goal_distance': (Array(0.39601636, dtype=float32), Array(0.19883336, dtype=float32)), 'eval/episode_reward': (Array(-8679.849, dtype=float32), Array(4662.0425, dtype=float32)), 'eval/avg_episode_length': (Array(883.46094, dtype=float32), Array(319.86438, dtype=float32)), 'eval/epoch_eval_time': 4.0857038497924805, 'eval/sps': 31328.75135002781}
I0727 03:32:28.769700 140120985872192 train.py:379] starting iteration 304 3036.48144865036
I0727 03:32:38.685388 140120985872192 train.py:394] {'eval/walltime': 1258.2627186775208, 'training/sps': 42234.51936316678, 'training/walltime': 1778.1294820308685, 'training/entropy_loss': Array(-0.02037416, dtype=float32), 'training/policy_loss': Array(0.00421483, dtype=float32), 'training/total_loss': Array(23.34454, dtype=float32), 'training/v_loss': Array(23.3607, dtype=float32), 'eval/episode_goal_distance': (Array(0.41362816, dtype=float32), Array(0.17344047, dtype=float32)), 'eval/episode_reward': (Array(-9178.619, dtype=float32), Array(4638.2627, dtype=float32)), 'eval/avg_episode_length': (Array(883.59375, dtype=float32), Array(319.49988, dtype=float32)), 'eval/epoch_eval_time': 4.093095541000366, 'eval/sps': 31272.17498781286}
I0727 03:32:38.687759 140120985872192 train.py:379] starting iteration 305 3046.39950799942
I0727 03:32:48.597187 140120985872192 train.py:394] {'eval/walltime': 1262.3559358119965, 'training/sps': 42280.40064307059, 'training/walltime': 1783.9421043395996, 'training/entropy_loss': Array(-0.02081166, dtype=float32), 'training/policy_loss': Array(0.00481122, dtype=float32), 'training/total_loss': Array(22.186584, dtype=float32), 'training/v_loss': Array(22.202587, dtype=float32), 'eval/episode_goal_distance': (Array(0.4098012, dtype=float32), Array(0.15863514, dtype=float32)), 'eval/episode_reward': (Array(-8410.219, dtype=float32), Array(4912.6284, dtype=float32)), 'eval/avg_episode_length': (Array(829.22656, dtype=float32), Array(374.85452, dtype=float32)), 'eval/epoch_eval_time': 4.093217134475708, 'eval/sps': 31271.246013777683}
I0727 03:32:48.599548 140120985872192 train.py:379] starting iteration 306 3056.311296224594
I0727 03:32:58.531560 140120985872192 train.py:394] {'eval/walltime': 1266.4468626976013, 'training/sps': 42101.32941800514, 'training/walltime': 1789.7794497013092, 'training/entropy_loss': Array(-0.02041008, dtype=float32), 'training/policy_loss': Array(0.00467109, dtype=float32), 'training/total_loss': Array(25.45606, dtype=float32), 'training/v_loss': Array(25.471798, dtype=float32), 'eval/episode_goal_distance': (Array(0.38948756, dtype=float32), Array(0.15733847, dtype=float32)), 'eval/episode_reward': (Array(-8557.3125, dtype=float32), Array(4385.3506, dtype=float32)), 'eval/avg_episode_length': (Array(891.2656, dtype=float32), Array(310.2818, dtype=float32)), 'eval/epoch_eval_time': 4.090926885604858, 'eval/sps': 31288.75278861767}
I0727 03:32:58.666811 140120985872192 train.py:379] starting iteration 307 3066.3785564899445
I0727 03:33:08.582995 140120985872192 train.py:394] {'eval/walltime': 1270.5496830940247, 'training/sps': 42301.95424943972, 'training/walltime': 1795.5891103744507, 'training/entropy_loss': Array(-0.0216699, dtype=float32), 'training/policy_loss': Array(0.00578928, dtype=float32), 'training/total_loss': Array(24.333382, dtype=float32), 'training/v_loss': Array(24.349262, dtype=float32), 'eval/episode_goal_distance': (Array(0.416485, dtype=float32), Array(0.19922113, dtype=float32)), 'eval/episode_reward': (Array(-8748.599, dtype=float32), Array(5133.416, dtype=float32)), 'eval/avg_episode_length': (Array(844.65625, dtype=float32), Array(360.98682, dtype=float32)), 'eval/epoch_eval_time': 4.10282039642334, 'eval/sps': 31198.05100695727}
I0727 03:33:08.585511 140120985872192 train.py:379] starting iteration 308 3076.297260284424
I0727 03:33:18.506114 140120985872192 train.py:394] {'eval/walltime': 1274.638376712799, 'training/sps': 42166.609159886095, 'training/walltime': 1801.417418718338, 'training/entropy_loss': Array(-0.02013201, dtype=float32), 'training/policy_loss': Array(0.00377082, dtype=float32), 'training/total_loss': Array(49.064613, dtype=float32), 'training/v_loss': Array(49.08097, dtype=float32), 'eval/episode_goal_distance': (Array(0.3951726, dtype=float32), Array(0.16806422, dtype=float32)), 'eval/episode_reward': (Array(-8820.642, dtype=float32), Array(4846.1724, dtype=float32)), 'eval/avg_episode_length': (Array(867.9297, dtype=float32), Array(337.47598, dtype=float32)), 'eval/epoch_eval_time': 4.088693618774414, 'eval/sps': 31305.84287662229}
I0727 03:33:18.508519 140120985872192 train.py:379] starting iteration 309 3086.220267534256
I0727 03:33:28.443262 140120985872192 train.py:394] {'eval/walltime': 1278.733303785324, 'training/sps': 42110.512218555385, 'training/walltime': 1807.2534911632538, 'training/entropy_loss': Array(-0.02040717, dtype=float32), 'training/policy_loss': Array(0.0085925, dtype=float32), 'training/total_loss': Array(32.31926, dtype=float32), 'training/v_loss': Array(32.33107, dtype=float32), 'eval/episode_goal_distance': (Array(0.41780606, dtype=float32), Array(0.20249271, dtype=float32)), 'eval/episode_reward': (Array(-9213.595, dtype=float32), Array(4850.1562, dtype=float32)), 'eval/avg_episode_length': (Array(875.72656, dtype=float32), Array(328.79715, dtype=float32)), 'eval/epoch_eval_time': 4.094927072525024, 'eval/sps': 31258.18793179931}
I0727 03:33:28.445502 140120985872192 train.py:379] starting iteration 310 3096.157250404358
I0727 03:33:38.363702 140120985872192 train.py:394] {'eval/walltime': 1282.844604253769, 'training/sps': 42348.429369508514, 'training/walltime': 1813.056776046753, 'training/entropy_loss': Array(-0.02125324, dtype=float32), 'training/policy_loss': Array(0.00842145, dtype=float32), 'training/total_loss': Array(28.041767, dtype=float32), 'training/v_loss': Array(28.054602, dtype=float32), 'eval/episode_goal_distance': (Array(0.4255476, dtype=float32), Array(0.19793797, dtype=float32)), 'eval/episode_reward': (Array(-9390.022, dtype=float32), Array(4910.033, dtype=float32)), 'eval/avg_episode_length': (Array(891.33594, dtype=float32), Array(310.08115, dtype=float32)), 'eval/epoch_eval_time': 4.111300468444824, 'eval/sps': 31133.70112022447}
I0727 03:33:38.366392 140120985872192 train.py:379] starting iteration 311 3106.078140735626
I0727 03:33:48.328002 140120985872192 train.py:394] {'eval/walltime': 1286.9403159618378, 'training/sps': 41922.83086192947, 'training/walltime': 1818.9189755916595, 'training/entropy_loss': Array(-0.02069276, dtype=float32), 'training/policy_loss': Array(0.00690959, dtype=float32), 'training/total_loss': Array(24.054638, dtype=float32), 'training/v_loss': Array(24.06842, dtype=float32), 'eval/episode_goal_distance': (Array(0.41669124, dtype=float32), Array(0.17389438, dtype=float32)), 'eval/episode_reward': (Array(-8806.039, dtype=float32), Array(4646.7153, dtype=float32)), 'eval/avg_episode_length': (Array(868.03125, dtype=float32), Array(337.21637, dtype=float32)), 'eval/epoch_eval_time': 4.095711708068848, 'eval/sps': 31252.19964770244}
I0727 03:33:48.330348 140120985872192 train.py:379] starting iteration 312 3116.0420920848846
I0727 03:33:58.234160 140120985872192 train.py:394] {'eval/walltime': 1291.0420157909393, 'training/sps': 42383.59414463804, 'training/walltime': 1824.7174456119537, 'training/entropy_loss': Array(-0.02057403, dtype=float32), 'training/policy_loss': Array(0.00933063, dtype=float32), 'training/total_loss': Array(25.183538, dtype=float32), 'training/v_loss': Array(25.194782, dtype=float32), 'eval/episode_goal_distance': (Array(0.4048559, dtype=float32), Array(0.16242541, dtype=float32)), 'eval/episode_reward': (Array(-8274.75, dtype=float32), Array(4702.0654, dtype=float32)), 'eval/avg_episode_length': (Array(837.03906, dtype=float32), Array(367.8467, dtype=float32)), 'eval/epoch_eval_time': 4.1016998291015625, 'eval/sps': 31206.574184644116}
I0727 03:33:58.236591 140120985872192 train.py:379] starting iteration 313 3125.948339700699
I0727 03:34:08.202287 140120985872192 train.py:394] {'eval/walltime': 1295.1425664424896, 'training/sps': 41927.653220036154, 'training/walltime': 1830.5789709091187, 'training/entropy_loss': Array(-0.02118667, dtype=float32), 'training/policy_loss': Array(0.00889199, dtype=float32), 'training/total_loss': Array(23.554523, dtype=float32), 'training/v_loss': Array(23.566818, dtype=float32), 'eval/episode_goal_distance': (Array(0.38965857, dtype=float32), Array(0.16496286, dtype=float32)), 'eval/episode_reward': (Array(-9131.764, dtype=float32), Array(4025.049, dtype=float32)), 'eval/avg_episode_length': (Array(922.40625, dtype=float32), Array(266.5437, dtype=float32)), 'eval/epoch_eval_time': 4.100550651550293, 'eval/sps': 31215.319813597987}
I0727 03:34:08.204528 140120985872192 train.py:379] starting iteration 314 3135.9162764549255
I0727 03:34:18.094421 140120985872192 train.py:394] {'eval/walltime': 1299.22682929039, 'training/sps': 42357.883904325354, 'training/walltime': 1836.3809604644775, 'training/entropy_loss': Array(-0.02153002, dtype=float32), 'training/policy_loss': Array(0.00379604, dtype=float32), 'training/total_loss': Array(22.147213, dtype=float32), 'training/v_loss': Array(22.164948, dtype=float32), 'eval/episode_goal_distance': (Array(0.4101774, dtype=float32), Array(0.1728078, dtype=float32)), 'eval/episode_reward': (Array(-9107.446, dtype=float32), Array(4582.1143, dtype=float32)), 'eval/avg_episode_length': (Array(875.8281, dtype=float32), Array(328.52878, dtype=float32)), 'eval/epoch_eval_time': 4.084262847900391, 'eval/sps': 31339.80470081678}
I0727 03:34:18.099128 140120985872192 train.py:379] starting iteration 315 3145.8108615875244
I0727 03:34:28.032471 140120985872192 train.py:394] {'eval/walltime': 1303.3150119781494, 'training/sps': 42074.31679899687, 'training/walltime': 1842.222053527832, 'training/entropy_loss': Array(-0.02213408, dtype=float32), 'training/policy_loss': Array(0.00566941, dtype=float32), 'training/total_loss': Array(28.228313, dtype=float32), 'training/v_loss': Array(28.24478, dtype=float32), 'eval/episode_goal_distance': (Array(0.40668306, dtype=float32), Array(0.16945632, dtype=float32)), 'eval/episode_reward': (Array(-9389.003, dtype=float32), Array(4606.097, dtype=float32)), 'eval/avg_episode_length': (Array(914.5, dtype=float32), Array(278.8452, dtype=float32)), 'eval/epoch_eval_time': 4.088182687759399, 'eval/sps': 31309.755403849787}
I0727 03:34:28.034690 140120985872192 train.py:379] starting iteration 316 3155.746439218521
I0727 03:34:37.952373 140120985872192 train.py:394] {'eval/walltime': 1307.3927936553955, 'training/sps': 42123.51843412433, 'training/walltime': 1848.056324005127, 'training/entropy_loss': Array(-0.01925719, dtype=float32), 'training/policy_loss': Array(0.00453837, dtype=float32), 'training/total_loss': Array(43.168934, dtype=float32), 'training/v_loss': Array(43.183655, dtype=float32), 'eval/episode_goal_distance': (Array(0.4041104, dtype=float32), Array(0.18589403, dtype=float32)), 'eval/episode_reward': (Array(-9277.156, dtype=float32), Array(4454.3105, dtype=float32)), 'eval/avg_episode_length': (Array(922.3125, dtype=float32), Array(266.8658, dtype=float32)), 'eval/epoch_eval_time': 4.077781677246094, 'eval/sps': 31389.615759528366}
I0727 03:34:37.955243 140120985872192 train.py:379] starting iteration 317 3165.666991710663
I0727 03:34:47.897419 140120985872192 train.py:394] {'eval/walltime': 1311.472318649292, 'training/sps': 41960.00429861292, 'training/walltime': 1853.913330078125, 'training/entropy_loss': Array(-0.01831588, dtype=float32), 'training/policy_loss': Array(0.00508372, dtype=float32), 'training/total_loss': Array(35.78642, dtype=float32), 'training/v_loss': Array(35.79965, dtype=float32), 'eval/episode_goal_distance': (Array(0.41344613, dtype=float32), Array(0.16876097, dtype=float32)), 'eval/episode_reward': (Array(-9522.931, dtype=float32), Array(4079.7034, dtype=float32)), 'eval/avg_episode_length': (Array(930.1328, dtype=float32), Array(254.05432, dtype=float32)), 'eval/epoch_eval_time': 4.079524993896484, 'eval/sps': 31376.201933192013}
I0727 03:34:47.899582 140120985872192 train.py:379] starting iteration 318 3175.6113312244415
I0727 03:34:57.812701 140120985872192 train.py:394] {'eval/walltime': 1315.5488820075989, 'training/sps': 42132.85041121943, 'training/walltime': 1859.7463083267212, 'training/entropy_loss': Array(-0.01989176, dtype=float32), 'training/policy_loss': Array(0.00351657, dtype=float32), 'training/total_loss': Array(26.571106, dtype=float32), 'training/v_loss': Array(26.587482, dtype=float32), 'eval/episode_goal_distance': (Array(0.37666914, dtype=float32), Array(0.16814788, dtype=float32)), 'eval/episode_reward': (Array(-8475.988, dtype=float32), Array(3786.1792, dtype=float32)), 'eval/avg_episode_length': (Array(930.10156, dtype=float32), Array(254.16797, dtype=float32)), 'eval/epoch_eval_time': 4.076563358306885, 'eval/sps': 31398.996838641586}
I0727 03:34:57.815036 140120985872192 train.py:379] starting iteration 319 3185.526785135269
I0727 03:35:07.735999 140120985872192 train.py:394] {'eval/walltime': 1319.6285860538483, 'training/sps': 42100.05697274025, 'training/walltime': 1865.5838301181793, 'training/entropy_loss': Array(-0.02043955, dtype=float32), 'training/policy_loss': Array(0.00744525, dtype=float32), 'training/total_loss': Array(23.880669, dtype=float32), 'training/v_loss': Array(23.893665, dtype=float32), 'eval/episode_goal_distance': (Array(0.38581452, dtype=float32), Array(0.16641133, dtype=float32)), 'eval/episode_reward': (Array(-9131.225, dtype=float32), Array(4108.8643, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.08241, dtype=float32)), 'eval/epoch_eval_time': 4.07970404624939, 'eval/sps': 31374.824876739465}
I0727 03:35:07.738405 140120985872192 train.py:379] starting iteration 320 3195.4501535892487
I0727 03:35:17.669291 140120985872192 train.py:394] {'eval/walltime': 1323.7372860908508, 'training/sps': 42238.10173936418, 'training/walltime': 1871.4022734165192, 'training/entropy_loss': Array(-0.02104463, dtype=float32), 'training/policy_loss': Array(0.00611817, dtype=float32), 'training/total_loss': Array(22.151783, dtype=float32), 'training/v_loss': Array(22.16671, dtype=float32), 'eval/episode_goal_distance': (Array(0.4107602, dtype=float32), Array(0.19173986, dtype=float32)), 'eval/episode_reward': (Array(-9513.727, dtype=float32), Array(4421.4727, dtype=float32)), 'eval/avg_episode_length': (Array(937.9297, dtype=float32), Array(240.3976, dtype=float32)), 'eval/epoch_eval_time': 4.1087000370025635, 'eval/sps': 31153.405906307133}
I0727 03:35:17.671713 140120985872192 train.py:379] starting iteration 321 3205.383462190628
I0727 03:35:27.618225 140120985872192 train.py:394] {'eval/walltime': 1327.8196964263916, 'training/sps': 41934.10750273532, 'training/walltime': 1877.2628965377808, 'training/entropy_loss': Array(-0.02183981, dtype=float32), 'training/policy_loss': Array(0.00811488, dtype=float32), 'training/total_loss': Array(22.570599, dtype=float32), 'training/v_loss': Array(22.584324, dtype=float32), 'eval/episode_goal_distance': (Array(0.41324717, dtype=float32), Array(0.17397664, dtype=float32)), 'eval/episode_reward': (Array(-8920.634, dtype=float32), Array(4398.114, dtype=float32)), 'eval/avg_episode_length': (Array(875.7656, dtype=float32), Array(328.694, dtype=float32)), 'eval/epoch_eval_time': 4.0824103355407715, 'eval/sps': 31354.026048203366}
I0727 03:35:27.620447 140120985872192 train.py:379] starting iteration 322 3215.332195997238
I0727 03:35:37.545888 140120985872192 train.py:394] {'eval/walltime': 1331.9054713249207, 'training/sps': 42110.407279126106, 'training/walltime': 1883.0989835262299, 'training/entropy_loss': Array(-0.02221173, dtype=float32), 'training/policy_loss': Array(0.00852055, dtype=float32), 'training/total_loss': Array(23.854572, dtype=float32), 'training/v_loss': Array(23.868265, dtype=float32), 'eval/episode_goal_distance': (Array(0.428724, dtype=float32), Array(0.22339988, dtype=float32)), 'eval/episode_reward': (Array(-9239.467, dtype=float32), Array(5577.8926, dtype=float32)), 'eval/avg_episode_length': (Array(868.0078, dtype=float32), Array(337.2767, dtype=float32)), 'eval/epoch_eval_time': 4.085774898529053, 'eval/sps': 31328.206565193334}
I0727 03:35:37.548165 140120985872192 train.py:379] starting iteration 323 3225.259914159775
I0727 03:35:47.526667 140120985872192 train.py:394] {'eval/walltime': 1336.0039401054382, 'training/sps': 41830.60832548259, 'training/walltime': 1888.9741072654724, 'training/entropy_loss': Array(-0.02308982, dtype=float32), 'training/policy_loss': Array(0.00594622, dtype=float32), 'training/total_loss': Array(26.256924, dtype=float32), 'training/v_loss': Array(26.274069, dtype=float32), 'eval/episode_goal_distance': (Array(0.40031177, dtype=float32), Array(0.16939709, dtype=float32)), 'eval/episode_reward': (Array(-8743.366, dtype=float32), Array(4542.807, dtype=float32)), 'eval/avg_episode_length': (Array(883.4453, dtype=float32), Array(319.90707, dtype=float32)), 'eval/epoch_eval_time': 4.098468780517578, 'eval/sps': 31231.176045175445}
I0727 03:35:47.528992 140120985872192 train.py:379] starting iteration 324 3235.2407410144806
I0727 03:35:57.433495 140120985872192 train.py:394] {'eval/walltime': 1340.0850238800049, 'training/sps': 42228.06742728292, 'training/walltime': 1894.7939331531525, 'training/entropy_loss': Array(-0.02384826, dtype=float32), 'training/policy_loss': Array(0.00445606, dtype=float32), 'training/total_loss': Array(23.851608, dtype=float32), 'training/v_loss': Array(23.870998, dtype=float32), 'eval/episode_goal_distance': (Array(0.42556483, dtype=float32), Array(0.20477995, dtype=float32)), 'eval/episode_reward': (Array(-9262.828, dtype=float32), Array(4787.224, dtype=float32)), 'eval/avg_episode_length': (Array(891.2578, dtype=float32), Array(310.3038, dtype=float32)), 'eval/epoch_eval_time': 4.08108377456665, 'eval/sps': 31364.217710427096}
I0727 03:35:57.435972 140120985872192 train.py:379] starting iteration 325 3245.147720336914
I0727 03:36:07.402936 140120985872192 train.py:394] {'eval/walltime': 1344.1947257518768, 'training/sps': 41983.58005282526, 'training/walltime': 1900.6476502418518, 'training/entropy_loss': Array(-0.02187033, dtype=float32), 'training/policy_loss': Array(0.00554626, dtype=float32), 'training/total_loss': Array(51.841198, dtype=float32), 'training/v_loss': Array(51.857525, dtype=float32), 'eval/episode_goal_distance': (Array(0.38566926, dtype=float32), Array(0.16000481, dtype=float32)), 'eval/episode_reward': (Array(-8519.857, dtype=float32), Array(4570.363, dtype=float32)), 'eval/avg_episode_length': (Array(867.96875, dtype=float32), Array(337.37613, dtype=float32)), 'eval/epoch_eval_time': 4.109701871871948, 'eval/sps': 31145.811543185406}
I0727 03:36:07.405367 140120985872192 train.py:379] starting iteration 326 3255.1171157360077
I0727 03:36:17.319983 140120985872192 train.py:394] {'eval/walltime': 1348.2744393348694, 'training/sps': 42143.719971246544, 'training/walltime': 1906.4791240692139, 'training/entropy_loss': Array(-0.02378491, dtype=float32), 'training/policy_loss': Array(0.0056427, dtype=float32), 'training/total_loss': Array(28.11131, dtype=float32), 'training/v_loss': Array(28.129452, dtype=float32), 'eval/episode_goal_distance': (Array(0.40142477, dtype=float32), Array(0.17121953, dtype=float32)), 'eval/episode_reward': (Array(-9004.936, dtype=float32), Array(4487.389, dtype=float32)), 'eval/avg_episode_length': (Array(883.47656, dtype=float32), Array(319.8217, dtype=float32)), 'eval/epoch_eval_time': 4.079713582992554, 'eval/sps': 31374.751534912746}
I0727 03:36:17.322331 140120985872192 train.py:379] starting iteration 327 3265.034080028534
I0727 03:36:27.264853 140120985872192 train.py:394] {'eval/walltime': 1352.371418952942, 'training/sps': 42068.433923283534, 'training/walltime': 1912.3210339546204, 'training/entropy_loss': Array(-0.02411516, dtype=float32), 'training/policy_loss': Array(0.00972925, dtype=float32), 'training/total_loss': Array(23.111322, dtype=float32), 'training/v_loss': Array(23.125708, dtype=float32), 'eval/episode_goal_distance': (Array(0.397447, dtype=float32), Array(0.18667044, dtype=float32)), 'eval/episode_reward': (Array(-8526.963, dtype=float32), Array(4543.649, dtype=float32)), 'eval/avg_episode_length': (Array(868.03906, dtype=float32), Array(337.19687, dtype=float32)), 'eval/epoch_eval_time': 4.09697961807251, 'eval/sps': 31242.527894297815}
I0727 03:36:27.267402 140120985872192 train.py:379] starting iteration 328 3274.9791507720947
I0727 03:36:37.149718 140120985872192 train.py:394] {'eval/walltime': 1356.444534778595, 'training/sps': 42331.999697086525, 'training/walltime': 1918.1265711784363, 'training/entropy_loss': Array(-0.02463034, dtype=float32), 'training/policy_loss': Array(0.00572689, dtype=float32), 'training/total_loss': Array(23.306969, dtype=float32), 'training/v_loss': Array(23.325872, dtype=float32), 'eval/episode_goal_distance': (Array(0.39766625, dtype=float32), Array(0.15613396, dtype=float32)), 'eval/episode_reward': (Array(-8378.616, dtype=float32), Array(4501.9946, dtype=float32)), 'eval/avg_episode_length': (Array(844.6875, dtype=float32), Array(360.91467, dtype=float32)), 'eval/epoch_eval_time': 4.073115825653076, 'eval/sps': 31425.573315111094}
I0727 03:36:37.152231 140120985872192 train.py:379] starting iteration 329 3284.863980293274
I0727 03:36:47.119592 140120985872192 train.py:394] {'eval/walltime': 1360.5467739105225, 'training/sps': 41927.985778543705, 'training/walltime': 1923.9880499839783, 'training/entropy_loss': Array(-0.02296181, dtype=float32), 'training/policy_loss': Array(0.00348395, dtype=float32), 'training/total_loss': Array(23.864311, dtype=float32), 'training/v_loss': Array(23.883785, dtype=float32), 'eval/episode_goal_distance': (Array(0.41504055, dtype=float32), Array(0.18850629, dtype=float32)), 'eval/episode_reward': (Array(-8872.846, dtype=float32), Array(5093.3096, dtype=float32)), 'eval/avg_episode_length': (Array(860.27344, dtype=float32), Array(345.4136, dtype=float32)), 'eval/epoch_eval_time': 4.10223913192749, 'eval/sps': 31202.47159747061}
I0727 03:36:47.121969 140120985872192 train.py:379] starting iteration 330 3294.8337178230286
I0727 03:36:57.034074 140120985872192 train.py:394] {'eval/walltime': 1364.6276850700378, 'training/sps': 42173.16483413705, 'training/walltime': 1929.815452337265, 'training/entropy_loss': Array(-0.02341215, dtype=float32), 'training/policy_loss': Array(0.00307485, dtype=float32), 'training/total_loss': Array(22.695915, dtype=float32), 'training/v_loss': Array(22.716255, dtype=float32), 'eval/episode_goal_distance': (Array(0.3943813, dtype=float32), Array(0.17635489, dtype=float32)), 'eval/episode_reward': (Array(-9043.881, dtype=float32), Array(3732.9255, dtype=float32)), 'eval/avg_episode_length': (Array(953.35156, dtype=float32), Array(210.34966, dtype=float32)), 'eval/epoch_eval_time': 4.080911159515381, 'eval/sps': 31365.544359265186}
I0727 03:36:57.036267 140120985872192 train.py:379] starting iteration 331 3304.748015642166
I0727 03:37:06.943921 140120985872192 train.py:394] {'eval/walltime': 1368.7074296474457, 'training/sps': 42195.77511134399, 'training/walltime': 1935.6397321224213, 'training/entropy_loss': Array(-0.02323584, dtype=float32), 'training/policy_loss': Array(0.0037765, dtype=float32), 'training/total_loss': Array(24.48774, dtype=float32), 'training/v_loss': Array(24.5072, dtype=float32), 'eval/episode_goal_distance': (Array(0.42025423, dtype=float32), Array(0.19117582, dtype=float32)), 'eval/episode_reward': (Array(-9249.477, dtype=float32), Array(4541.6396, dtype=float32)), 'eval/avg_episode_length': (Array(891.3828, dtype=float32), Array(309.9471, dtype=float32)), 'eval/epoch_eval_time': 4.079744577407837, 'eval/sps': 31374.51317634396}
I0727 03:37:06.946304 140120985872192 train.py:379] starting iteration 332 3314.6580526828766
I0727 03:37:16.886109 140120985872192 train.py:394] {'eval/walltime': 1372.7830126285553, 'training/sps': 41932.94237821475, 'training/walltime': 1941.5005180835724, 'training/entropy_loss': Array(-0.02421181, dtype=float32), 'training/policy_loss': Array(0.00442417, dtype=float32), 'training/total_loss': Array(27.067684, dtype=float32), 'training/v_loss': Array(27.087471, dtype=float32), 'eval/episode_goal_distance': (Array(0.41177827, dtype=float32), Array(0.15232301, dtype=float32)), 'eval/episode_reward': (Array(-8882.019, dtype=float32), Array(4483.9395, dtype=float32)), 'eval/avg_episode_length': (Array(883.46094, dtype=float32), Array(319.86435, dtype=float32)), 'eval/epoch_eval_time': 4.075582981109619, 'eval/sps': 31406.54983428915}
I0727 03:37:16.888271 140120985872192 train.py:379] starting iteration 333 3324.600019454956
I0727 03:37:26.853954 140120985872192 train.py:394] {'eval/walltime': 1376.8766026496887, 'training/sps': 41878.11665021268, 'training/walltime': 1947.3689768314362, 'training/entropy_loss': Array(-0.02195624, dtype=float32), 'training/policy_loss': Array(0.00509396, dtype=float32), 'training/total_loss': Array(51.47477, dtype=float32), 'training/v_loss': Array(51.49163, dtype=float32), 'eval/episode_goal_distance': (Array(0.38885573, dtype=float32), Array(0.16122277, dtype=float32)), 'eval/episode_reward': (Array(-8009.247, dtype=float32), Array(4779.585, dtype=float32)), 'eval/avg_episode_length': (Array(813.71875, dtype=float32), Array(387.77603, dtype=float32)), 'eval/epoch_eval_time': 4.093590021133423, 'eval/sps': 31268.397504193563}
I0727 03:37:26.856449 140120985872192 train.py:379] starting iteration 334 3334.5681970119476
I0727 03:37:36.756786 140120985872192 train.py:394] {'eval/walltime': 1380.9604258537292, 'training/sps': 42278.621396369155, 'training/walltime': 1953.1818437576294, 'training/entropy_loss': Array(-0.02223733, dtype=float32), 'training/policy_loss': Array(0.00553694, dtype=float32), 'training/total_loss': Array(33.16984, dtype=float32), 'training/v_loss': Array(33.18654, dtype=float32), 'eval/episode_goal_distance': (Array(0.41547906, dtype=float32), Array(0.18481614, dtype=float32)), 'eval/episode_reward': (Array(-9676.852, dtype=float32), Array(4438.737, dtype=float32)), 'eval/avg_episode_length': (Array(937.91406, dtype=float32), Array(240.45813, dtype=float32)), 'eval/epoch_eval_time': 4.083823204040527, 'eval/sps': 31343.178586515947}
I0727 03:37:36.759082 140120985872192 train.py:379] starting iteration 335 3344.4708304405212
I0727 03:37:46.714422 140120985872192 train.py:394] {'eval/walltime': 1385.0570046901703, 'training/sps': 41973.42872140462, 'training/walltime': 1959.0369765758514, 'training/entropy_loss': Array(-0.02239356, dtype=float32), 'training/policy_loss': Array(0.00467297, dtype=float32), 'training/total_loss': Array(26.290302, dtype=float32), 'training/v_loss': Array(26.308022, dtype=float32), 'eval/episode_goal_distance': (Array(0.39990634, dtype=float32), Array(0.19942881, dtype=float32)), 'eval/episode_reward': (Array(-9379.799, dtype=float32), Array(4991.504, dtype=float32)), 'eval/avg_episode_length': (Array(922.3047, dtype=float32), Array(266.89294, dtype=float32)), 'eval/epoch_eval_time': 4.09657883644104, 'eval/sps': 31245.584452416344}
I0727 03:37:46.716798 140120985872192 train.py:379] starting iteration 336 3354.4285469055176
I0727 03:37:56.614252 140120985872192 train.py:394] {'eval/walltime': 1389.1380231380463, 'training/sps': 42279.710430178, 'training/walltime': 1964.849693775177, 'training/entropy_loss': Array(-0.02254215, dtype=float32), 'training/policy_loss': Array(0.0026843, dtype=float32), 'training/total_loss': Array(23.83387, dtype=float32), 'training/v_loss': Array(23.853727, dtype=float32), 'eval/episode_goal_distance': (Array(0.41311336, dtype=float32), Array(0.2039602, dtype=float32)), 'eval/episode_reward': (Array(-8931.914, dtype=float32), Array(5152.241, dtype=float32)), 'eval/avg_episode_length': (Array(867.9375, dtype=float32), Array(337.45602, dtype=float32)), 'eval/epoch_eval_time': 4.081018447875977, 'eval/sps': 31364.71977151179}
I0727 03:37:56.619085 140120985872192 train.py:379] starting iteration 337 3364.3308186531067
I0727 03:38:06.555981 140120985872192 train.py:394] {'eval/walltime': 1393.2296209335327, 'training/sps': 42073.43924290743, 'training/walltime': 1970.6909086704254, 'training/entropy_loss': Array(-0.0226237, dtype=float32), 'training/policy_loss': Array(0.00399261, dtype=float32), 'training/total_loss': Array(23.21965, dtype=float32), 'training/v_loss': Array(23.238281, dtype=float32), 'eval/episode_goal_distance': (Array(0.39802557, dtype=float32), Array(0.16497938, dtype=float32)), 'eval/episode_reward': (Array(-8810.615, dtype=float32), Array(4118.5156, dtype=float32)), 'eval/avg_episode_length': (Array(914.60156, dtype=float32), Array(278.51416, dtype=float32)), 'eval/epoch_eval_time': 4.09159779548645, 'eval/sps': 31283.62229083225}
I0727 03:38:06.558513 140120985872192 train.py:379] starting iteration 338 3374.270262479782
I0727 03:38:16.450668 140120985872192 train.py:394] {'eval/walltime': 1397.3109166622162, 'training/sps': 42321.77476572473, 'training/walltime': 1976.4978485107422, 'training/entropy_loss': Array(-0.02254969, dtype=float32), 'training/policy_loss': Array(0.00371712, dtype=float32), 'training/total_loss': Array(23.076553, dtype=float32), 'training/v_loss': Array(23.095383, dtype=float32), 'eval/episode_goal_distance': (Array(0.4114607, dtype=float32), Array(0.20915282, dtype=float32)), 'eval/episode_reward': (Array(-8848.675, dtype=float32), Array(4426.0757, dtype=float32)), 'eval/avg_episode_length': (Array(891.2578, dtype=float32), Array(310.30408, dtype=float32)), 'eval/epoch_eval_time': 4.081295728683472, 'eval/sps': 31362.588871081327}
I0727 03:38:16.453145 140120985872192 train.py:379] starting iteration 339 3384.1648938655853
I0727 03:38:26.408028 140120985872192 train.py:394] {'eval/walltime': 1401.3980929851532, 'training/sps': 41909.13718601907, 'training/walltime': 1982.3619635105133, 'training/entropy_loss': Array(-0.02401216, dtype=float32), 'training/policy_loss': Array(0.01559188, dtype=float32), 'training/total_loss': Array(22.468695, dtype=float32), 'training/v_loss': Array(22.477116, dtype=float32), 'eval/episode_goal_distance': (Array(0.42493302, dtype=float32), Array(0.19273965, dtype=float32)), 'eval/episode_reward': (Array(-9121.295, dtype=float32), Array(4671.322, dtype=float32)), 'eval/avg_episode_length': (Array(899.0469, dtype=float32), Array(300.26022, dtype=float32)), 'eval/epoch_eval_time': 4.087176322937012, 'eval/sps': 31317.464647089226}
I0727 03:38:26.410551 140120985872192 train.py:379] starting iteration 340 3394.1222999095917
I0727 03:38:36.339815 140120985872192 train.py:394] {'eval/walltime': 1405.477662563324, 'training/sps': 42038.174976304406, 'training/walltime': 1988.2080783843994, 'training/entropy_loss': Array(-0.02621678, dtype=float32), 'training/policy_loss': Array(0.00254381, dtype=float32), 'training/total_loss': Array(28.242016, dtype=float32), 'training/v_loss': Array(28.26569, dtype=float32), 'eval/episode_goal_distance': (Array(0.4422684, dtype=float32), Array(0.20251173, dtype=float32)), 'eval/episode_reward': (Array(-9508.752, dtype=float32), Array(4932.4375, dtype=float32)), 'eval/avg_episode_length': (Array(891.3906, dtype=float32), Array(309.9249, dtype=float32)), 'eval/epoch_eval_time': 4.079569578170776, 'eval/sps': 31375.85903299962}
I0727 03:38:36.342338 140120985872192 train.py:379] starting iteration 341 3404.054085969925
I0727 03:38:46.260970 140120985872192 train.py:394] {'eval/walltime': 1409.5612699985504, 'training/sps': 42144.4695057215, 'training/walltime': 1994.0394484996796, 'training/entropy_loss': Array(-0.02686826, dtype=float32), 'training/policy_loss': Array(0.00562435, dtype=float32), 'training/total_loss': Array(47.8727, dtype=float32), 'training/v_loss': Array(47.893944, dtype=float32), 'eval/episode_goal_distance': (Array(0.44374183, dtype=float32), Array(0.18048872, dtype=float32)), 'eval/episode_reward': (Array(-9746.807, dtype=float32), Array(4380.8696, dtype=float32)), 'eval/avg_episode_length': (Array(937.89844, dtype=float32), Array(240.519, dtype=float32)), 'eval/epoch_eval_time': 4.08360743522644, 'eval/sps': 31344.83469097275}
I0727 03:38:46.263494 140120985872192 train.py:379] starting iteration 342 3413.9752428531647
I0727 03:38:56.204331 140120985872192 train.py:394] {'eval/walltime': 1413.6403350830078, 'training/sps': 41951.62458333454, 'training/walltime': 1999.8976244926453, 'training/entropy_loss': Array(-0.02938339, dtype=float32), 'training/policy_loss': Array(0.01257025, dtype=float32), 'training/total_loss': Array(38.046265, dtype=float32), 'training/v_loss': Array(38.063076, dtype=float32), 'eval/episode_goal_distance': (Array(0.43467402, dtype=float32), Array(0.19374916, dtype=float32)), 'eval/episode_reward': (Array(-9011.842, dtype=float32), Array(4943.1265, dtype=float32)), 'eval/avg_episode_length': (Array(860.2422, dtype=float32), Array(345.49115, dtype=float32)), 'eval/epoch_eval_time': 4.0790650844573975, 'eval/sps': 31379.739560352402}
I0727 03:38:56.206946 140120985872192 train.py:379] starting iteration 343 3423.9186952114105
I0727 03:39:06.148663 140120985872192 train.py:394] {'eval/walltime': 1417.7250871658325, 'training/sps': 41986.57954996056, 'training/walltime': 2005.7509233951569, 'training/entropy_loss': Array(-0.03155625, dtype=float32), 'training/policy_loss': Array(0.00862815, dtype=float32), 'training/total_loss': Array(29.590973, dtype=float32), 'training/v_loss': Array(29.613903, dtype=float32), 'eval/episode_goal_distance': (Array(0.43465936, dtype=float32), Array(0.17284793, dtype=float32)), 'eval/episode_reward': (Array(-9296.752, dtype=float32), Array(4288.9824, dtype=float32)), 'eval/avg_episode_length': (Array(891.28906, dtype=float32), Array(310.21463, dtype=float32)), 'eval/epoch_eval_time': 4.084752082824707, 'eval/sps': 31336.051100434186}
I0727 03:39:06.151382 140120985872192 train.py:379] starting iteration 344 3433.863130569458
I0727 03:39:16.068589 140120985872192 train.py:394] {'eval/walltime': 1421.8053147792816, 'training/sps': 42130.02283327297, 'training/walltime': 2011.58429312706, 'training/entropy_loss': Array(-0.03286409, dtype=float32), 'training/policy_loss': Array(0.01374689, dtype=float32), 'training/total_loss': Array(26.55429, dtype=float32), 'training/v_loss': Array(26.57341, dtype=float32), 'eval/episode_goal_distance': (Array(0.48395047, dtype=float32), Array(0.2283795, dtype=float32)), 'eval/episode_reward': (Array(-10529.307, dtype=float32), Array(4920.3843, dtype=float32)), 'eval/avg_episode_length': (Array(937.78125, dtype=float32), Array(240.97235, dtype=float32)), 'eval/epoch_eval_time': 4.080227613449097, 'eval/sps': 31370.79891771015}
I0727 03:39:16.071507 140120985872192 train.py:379] starting iteration 345 3443.783255815506
I0727 03:39:26.071515 140120985872192 train.py:394] {'eval/walltime': 1425.9170026779175, 'training/sps': 41762.226687481496, 'training/walltime': 2017.4690368175507, 'training/entropy_loss': Array(-0.03394842, dtype=float32), 'training/policy_loss': Array(0.00355126, dtype=float32), 'training/total_loss': Array(26.623024, dtype=float32), 'training/v_loss': Array(26.65342, dtype=float32), 'eval/episode_goal_distance': (Array(0.45222312, dtype=float32), Array(0.2094503, dtype=float32)), 'eval/episode_reward': (Array(-9496.799, dtype=float32), Array(4897.104, dtype=float32)), 'eval/avg_episode_length': (Array(891.3047, dtype=float32), Array(310.1701, dtype=float32)), 'eval/epoch_eval_time': 4.111687898635864, 'eval/sps': 31130.767498784768}
I0727 03:39:26.073853 140120985872192 train.py:379] starting iteration 346 3453.785602092743
I0727 03:39:35.996604 140120985872192 train.py:394] {'eval/walltime': 1429.9977099895477, 'training/sps': 42093.26788599775, 'training/walltime': 2023.3075001239777, 'training/entropy_loss': Array(-0.03455019, dtype=float32), 'training/policy_loss': Array(0.00371196, dtype=float32), 'training/total_loss': Array(27.266888, dtype=float32), 'training/v_loss': Array(27.297726, dtype=float32), 'eval/episode_goal_distance': (Array(0.47855696, dtype=float32), Array(0.22023763, dtype=float32)), 'eval/episode_reward': (Array(-9977.612, dtype=float32), Array(5382.5635, dtype=float32)), 'eval/avg_episode_length': (Array(875.7656, dtype=float32), Array(328.69388, dtype=float32)), 'eval/epoch_eval_time': 4.080707311630249, 'eval/sps': 31367.111195451995}
I0727 03:39:35.999058 140120985872192 train.py:379] starting iteration 347 3463.7108066082
I0727 03:39:45.994112 140120985872192 train.py:394] {'eval/walltime': 1434.1200625896454, 'training/sps': 41873.74113933841, 'training/walltime': 2029.1765720844269, 'training/entropy_loss': Array(-0.03522557, dtype=float32), 'training/policy_loss': Array(0.00507331, dtype=float32), 'training/total_loss': Array(27.68929, dtype=float32), 'training/v_loss': Array(27.71944, dtype=float32), 'eval/episode_goal_distance': (Array(0.5088186, dtype=float32), Array(0.24555111, dtype=float32)), 'eval/episode_reward': (Array(-10996.291, dtype=float32), Array(5242.402, dtype=float32)), 'eval/avg_episode_length': (Array(914.5781, dtype=float32), Array(278.59048, dtype=float32)), 'eval/epoch_eval_time': 4.122352600097656, 'eval/sps': 31050.230879563227}
I0727 03:39:45.996685 140120985872192 train.py:379] starting iteration 348 3473.708434343338
I0727 03:39:55.949074 140120985872192 train.py:394] {'eval/walltime': 1438.2283670902252, 'training/sps': 42078.296314042986, 'training/walltime': 2035.0171127319336, 'training/entropy_loss': Array(-0.03632211, dtype=float32), 'training/policy_loss': Array(0.01035856, dtype=float32), 'training/total_loss': Array(31.055922, dtype=float32), 'training/v_loss': Array(31.081882, dtype=float32), 'eval/episode_goal_distance': (Array(0.44196987, dtype=float32), Array(0.2013553, dtype=float32)), 'eval/episode_reward': (Array(-9104.655, dtype=float32), Array(4991.734, dtype=float32)), 'eval/avg_episode_length': (Array(844.5781, dtype=float32), Array(361.16806, dtype=float32)), 'eval/epoch_eval_time': 4.108304500579834, 'eval/sps': 31156.405271793865}
I0727 03:39:55.951355 140120985872192 train.py:379] starting iteration 349 3483.6631038188934
I0727 03:40:05.852106 140120985872192 train.py:394] {'eval/walltime': 1442.311222076416, 'training/sps': 42267.93866083178, 'training/walltime': 2040.8314487934113, 'training/entropy_loss': Array(-0.0373293, dtype=float32), 'training/policy_loss': Array(0.0213605, dtype=float32), 'training/total_loss': Array(30.238293, dtype=float32), 'training/v_loss': Array(30.254261, dtype=float32), 'eval/episode_goal_distance': (Array(0.4412047, dtype=float32), Array(0.2071896, dtype=float32)), 'eval/episode_reward': (Array(-9023.426, dtype=float32), Array(4929.884, dtype=float32)), 'eval/avg_episode_length': (Array(875.6875, dtype=float32), Array(328.90027, dtype=float32)), 'eval/epoch_eval_time': 4.082854986190796, 'eval/sps': 31350.61138172357}
I0727 03:40:05.854658 140120985872192 train.py:379] starting iteration 350 3493.566406726837
I0727 03:40:15.813311 140120985872192 train.py:394] {'eval/walltime': 1446.4125382900238, 'training/sps': 41983.850229416115, 'training/walltime': 2046.685128211975, 'training/entropy_loss': Array(-0.03718337, dtype=float32), 'training/policy_loss': Array(0.01252365, dtype=float32), 'training/total_loss': Array(56.027527, dtype=float32), 'training/v_loss': Array(56.052185, dtype=float32), 'eval/episode_goal_distance': (Array(0.470092, dtype=float32), Array(0.23791565, dtype=float32)), 'eval/episode_reward': (Array(-10330.065, dtype=float32), Array(4896.844, dtype=float32)), 'eval/avg_episode_length': (Array(930.0156, dtype=float32), Array(254.47998, dtype=float32)), 'eval/epoch_eval_time': 4.101316213607788, 'eval/sps': 31209.493083051686}
I0727 03:40:15.815655 140120985872192 train.py:379] starting iteration 351 3503.527403831482
I0727 03:40:25.769649 140120985872192 train.py:394] {'eval/walltime': 1450.4972236156464, 'training/sps': 41897.96933694008, 'training/walltime': 2052.550806283951, 'training/entropy_loss': Array(-0.03838986, dtype=float32), 'training/policy_loss': Array(0.0076872, dtype=float32), 'training/total_loss': Array(32.210938, dtype=float32), 'training/v_loss': Array(32.24164, dtype=float32), 'eval/episode_goal_distance': (Array(0.43518367, dtype=float32), Array(0.20129511, dtype=float32)), 'eval/episode_reward': (Array(-9586.561, dtype=float32), Array(4598.6924, dtype=float32)), 'eval/avg_episode_length': (Array(930.0781, dtype=float32), Array(254.25262, dtype=float32)), 'eval/epoch_eval_time': 4.084685325622559, 'eval/sps': 31336.563234645535}
I0727 03:40:25.772463 140120985872192 train.py:379] starting iteration 352 3513.4842116832733
I0727 03:40:35.725132 140120985872192 train.py:394] {'eval/walltime': 1454.614832162857, 'training/sps': 42144.33510447463, 'training/walltime': 2058.38219499588, 'training/entropy_loss': Array(-0.03926403, dtype=float32), 'training/policy_loss': Array(0.00788269, dtype=float32), 'training/total_loss': Array(28.150005, dtype=float32), 'training/v_loss': Array(28.181387, dtype=float32), 'eval/episode_goal_distance': (Array(0.42335418, dtype=float32), Array(0.17615123, dtype=float32)), 'eval/episode_reward': (Array(-9084.139, dtype=float32), Array(4587.2134, dtype=float32)), 'eval/avg_episode_length': (Array(891.28906, dtype=float32), Array(310.21484, dtype=float32)), 'eval/epoch_eval_time': 4.117608547210693, 'eval/sps': 31086.005027531915}
I0727 03:40:35.727922 140120985872192 train.py:379] starting iteration 353 3523.4396708011627
I0727 03:40:45.682286 140120985872192 train.py:394] {'eval/walltime': 1458.7122650146484, 'training/sps': 41986.141740314386, 'training/walltime': 2064.235554933548, 'training/entropy_loss': Array(-0.0387656, dtype=float32), 'training/policy_loss': Array(0.00890361, dtype=float32), 'training/total_loss': Array(26.646202, dtype=float32), 'training/v_loss': Array(26.676067, dtype=float32), 'eval/episode_goal_distance': (Array(0.44460213, dtype=float32), Array(0.17841066, dtype=float32)), 'eval/episode_reward': (Array(-9529.826, dtype=float32), Array(4519.249, dtype=float32)), 'eval/avg_episode_length': (Array(899., dtype=float32), Array(300.39932, dtype=float32)), 'eval/epoch_eval_time': 4.097432851791382, 'eval/sps': 31239.07203117164}
I0727 03:40:45.685211 140120985872192 train.py:379] starting iteration 354 3533.396960258484
I0727 03:40:55.606577 140120985872192 train.py:394] {'eval/walltime': 1462.7863128185272, 'training/sps': 42056.57701410228, 'training/walltime': 2070.079111814499, 'training/entropy_loss': Array(-0.03866761, dtype=float32), 'training/policy_loss': Array(0.01124775, dtype=float32), 'training/total_loss': Array(26.551352, dtype=float32), 'training/v_loss': Array(26.57877, dtype=float32), 'eval/episode_goal_distance': (Array(0.45759022, dtype=float32), Array(0.22650659, dtype=float32)), 'eval/episode_reward': (Array(-9677.388, dtype=float32), Array(4533.5396, dtype=float32)), 'eval/avg_episode_length': (Array(922.28125, dtype=float32), Array(266.9729, dtype=float32)), 'eval/epoch_eval_time': 4.074047803878784, 'eval/sps': 31418.384408286733}
I0727 03:40:55.611464 140120985872192 train.py:379] starting iteration 355 3543.323197364807
I0727 03:41:05.511855 140120985872192 train.py:394] {'eval/walltime': 1466.8693544864655, 'training/sps': 42274.70792744575, 'training/walltime': 2075.892516851425, 'training/entropy_loss': Array(-0.03962694, dtype=float32), 'training/policy_loss': Array(0.01013251, dtype=float32), 'training/total_loss': Array(26.961786, dtype=float32), 'training/v_loss': Array(26.991282, dtype=float32), 'eval/episode_goal_distance': (Array(0.45360833, dtype=float32), Array(0.203671, dtype=float32)), 'eval/episode_reward': (Array(-9782.105, dtype=float32), Array(4858.5684, dtype=float32)), 'eval/avg_episode_length': (Array(914.5547, dtype=float32), Array(278.66702, dtype=float32)), 'eval/epoch_eval_time': 4.083041667938232, 'eval/sps': 31349.17799274743}
I0727 03:41:05.514259 140120985872192 train.py:379] starting iteration 356 3553.2260081768036
I0727 03:41:15.453101 140120985872192 train.py:394] {'eval/walltime': 1470.9506833553314, 'training/sps': 41982.37627027896, 'training/walltime': 2081.746401786804, 'training/entropy_loss': Array(-0.04012953, dtype=float32), 'training/policy_loss': Array(0.01051934, dtype=float32), 'training/total_loss': Array(28.895721, dtype=float32), 'training/v_loss': Array(28.925331, dtype=float32), 'eval/episode_goal_distance': (Array(0.45292222, dtype=float32), Array(0.21155988, dtype=float32)), 'eval/episode_reward': (Array(-9530.724, dtype=float32), Array(4836.374, dtype=float32)), 'eval/avg_episode_length': (Array(883.4531, dtype=float32), Array(319.88568, dtype=float32)), 'eval/epoch_eval_time': 4.081328868865967, 'eval/sps': 31362.334208457436}
I0727 03:41:15.455531 140120985872192 train.py:379] starting iteration 357 3563.167279958725
I0727 03:41:25.369902 140120985872192 train.py:394] {'eval/walltime': 1475.032472372055, 'training/sps': 42160.96773655232, 'training/walltime': 2087.5754899978638, 'training/entropy_loss': Array(-0.04082787, dtype=float32), 'training/policy_loss': Array(0.00892272, dtype=float32), 'training/total_loss': Array(30.859283, dtype=float32), 'training/v_loss': Array(30.89119, dtype=float32), 'eval/episode_goal_distance': (Array(0.45703742, dtype=float32), Array(0.21562324, dtype=float32)), 'eval/episode_reward': (Array(-9637.16, dtype=float32), Array(5209.411, dtype=float32)), 'eval/avg_episode_length': (Array(875.7031, dtype=float32), Array(328.85913, dtype=float32)), 'eval/epoch_eval_time': 4.081789016723633, 'eval/sps': 31358.798672730747}
I0727 03:41:25.529393 140120985872192 train.py:379] starting iteration 358 3573.241137266159
I0727 03:41:35.431174 140120985872192 train.py:394] {'eval/walltime': 1479.1200041770935, 'training/sps': 42296.68093841736, 'training/walltime': 2093.3858749866486, 'training/entropy_loss': Array(-0.04281601, dtype=float32), 'training/policy_loss': Array(0.00760027, dtype=float32), 'training/total_loss': Array(50.165443, dtype=float32), 'training/v_loss': Array(50.20066, dtype=float32), 'eval/episode_goal_distance': (Array(0.47036636, dtype=float32), Array(0.2533567, dtype=float32)), 'eval/episode_reward': (Array(-9897.818, dtype=float32), Array(5212.4736, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.30682, dtype=float32)), 'eval/epoch_eval_time': 4.087531805038452, 'eval/sps': 31314.741047940515}
I0727 03:41:35.433734 140120985872192 train.py:379] starting iteration 359 3583.145483493805
I0727 03:41:45.352023 140120985872192 train.py:394] {'eval/walltime': 1483.229986190796, 'training/sps': 42336.56018743558, 'training/walltime': 2099.1907868385315, 'training/entropy_loss': Array(-0.04422707, dtype=float32), 'training/policy_loss': Array(0.00601526, dtype=float32), 'training/total_loss': Array(34.97317, dtype=float32), 'training/v_loss': Array(35.011383, dtype=float32), 'eval/episode_goal_distance': (Array(0.43501312, dtype=float32), Array(0.18336052, dtype=float32)), 'eval/episode_reward': (Array(-9460.831, dtype=float32), Array(4576.135, dtype=float32)), 'eval/avg_episode_length': (Array(891.27344, dtype=float32), Array(310.2591, dtype=float32)), 'eval/epoch_eval_time': 4.109982013702393, 'eval/sps': 31143.68860332161}
I0727 03:41:45.354371 140120985872192 train.py:379] starting iteration 360 3593.0661199092865
I0727 03:41:55.271415 140120985872192 train.py:394] {'eval/walltime': 1487.3089849948883, 'training/sps': 42122.82472713637, 'training/walltime': 2105.025153398514, 'training/entropy_loss': Array(-0.04534183, dtype=float32), 'training/policy_loss': Array(0.00685297, dtype=float32), 'training/total_loss': Array(31.307226, dtype=float32), 'training/v_loss': Array(31.345715, dtype=float32), 'eval/episode_goal_distance': (Array(0.4516055, dtype=float32), Array(0.19920652, dtype=float32)), 'eval/episode_reward': (Array(-9858.217, dtype=float32), Array(4148.3057, dtype=float32)), 'eval/avg_episode_length': (Array(930.10156, dtype=float32), Array(254.16776, dtype=float32)), 'eval/epoch_eval_time': 4.078998804092407, 'eval/sps': 31380.249455228877}
I0727 03:41:55.273643 140120985872192 train.py:379] starting iteration 361 3602.9853920936584
I0727 03:42:05.221096 140120985872192 train.py:394] {'eval/walltime': 1491.3977773189545, 'training/sps': 41973.71244066251, 'training/walltime': 2110.8802466392517, 'training/entropy_loss': Array(-0.04541123, dtype=float32), 'training/policy_loss': Array(0.00450971, dtype=float32), 'training/total_loss': Array(28.970932, dtype=float32), 'training/v_loss': Array(29.011833, dtype=float32), 'eval/episode_goal_distance': (Array(0.44440043, dtype=float32), Array(0.20424299, dtype=float32)), 'eval/episode_reward': (Array(-9165.892, dtype=float32), Array(4644.807, dtype=float32)), 'eval/avg_episode_length': (Array(891.25, dtype=float32), Array(310.32614, dtype=float32)), 'eval/epoch_eval_time': 4.088792324066162, 'eval/sps': 31305.087139448657}
I0727 03:42:05.223248 140120985872192 train.py:379] starting iteration 362 3612.934996843338
I0727 03:42:15.189174 140120985872192 train.py:394] {'eval/walltime': 1495.4925396442413, 'training/sps': 41884.26466489672, 'training/walltime': 2116.747843980789, 'training/entropy_loss': Array(-0.04551303, dtype=float32), 'training/policy_loss': Array(0.00520876, dtype=float32), 'training/total_loss': Array(28.846788, dtype=float32), 'training/v_loss': Array(28.887093, dtype=float32), 'eval/episode_goal_distance': (Array(0.46352828, dtype=float32), Array(0.22167106, dtype=float32)), 'eval/episode_reward': (Array(-9272.697, dtype=float32), Array(5242.559, dtype=float32)), 'eval/avg_episode_length': (Array(844.65625, dtype=float32), Array(360.9868, dtype=float32)), 'eval/epoch_eval_time': 4.094762325286865, 'eval/sps': 31259.445562822195}
I0727 03:42:15.191508 140120985872192 train.py:379] starting iteration 363 3622.9032571315765
I0727 03:42:25.119094 140120985872192 train.py:394] {'eval/walltime': 1499.566888809204, 'training/sps': 42014.35293613191, 'training/walltime': 2122.5972735881805, 'training/entropy_loss': Array(-0.0465223, dtype=float32), 'training/policy_loss': Array(0.01522533, dtype=float32), 'training/total_loss': Array(28.113392, dtype=float32), 'training/v_loss': Array(28.144688, dtype=float32), 'eval/episode_goal_distance': (Array(0.49089852, dtype=float32), Array(0.24266851, dtype=float32)), 'eval/episode_reward': (Array(-9728.335, dtype=float32), Array(5305.6274, dtype=float32)), 'eval/avg_episode_length': (Array(852.3672, dtype=float32), Array(353.60608, dtype=float32)), 'eval/epoch_eval_time': 4.0743491649627686, 'eval/sps': 31416.060533233573}
I0727 03:42:25.121351 140120985872192 train.py:379] starting iteration 364 3632.83310008049
I0727 03:42:35.038572 140120985872192 train.py:394] {'eval/walltime': 1503.6483781337738, 'training/sps': 42139.18025752108, 'training/walltime': 2128.4293756484985, 'training/entropy_loss': Array(-0.04663504, dtype=float32), 'training/policy_loss': Array(0.00606269, dtype=float32), 'training/total_loss': Array(29.083622, dtype=float32), 'training/v_loss': Array(29.124193, dtype=float32), 'eval/episode_goal_distance': (Array(0.4367808, dtype=float32), Array(0.19967164, dtype=float32)), 'eval/episode_reward': (Array(-9391.005, dtype=float32), Array(4026.2346, dtype=float32)), 'eval/avg_episode_length': (Array(930.02344, dtype=float32), Array(254.45148, dtype=float32)), 'eval/epoch_eval_time': 4.081489324569702, 'eval/sps': 31361.101260137344}
I0727 03:42:35.040869 140120985872192 train.py:379] starting iteration 365 3642.7526178359985
I0727 03:42:44.975263 140120985872192 train.py:394] {'eval/walltime': 1507.7351336479187, 'training/sps': 42054.18687500375, 'training/walltime': 2134.27326464653, 'training/entropy_loss': Array(-0.04611664, dtype=float32), 'training/policy_loss': Array(0.00790986, dtype=float32), 'training/total_loss': Array(33.750496, dtype=float32), 'training/v_loss': Array(33.788704, dtype=float32), 'eval/episode_goal_distance': (Array(0.46764702, dtype=float32), Array(0.20235988, dtype=float32)), 'eval/episode_reward': (Array(-10162.492, dtype=float32), Array(4855.5327, dtype=float32)), 'eval/avg_episode_length': (Array(906.8594, dtype=float32), Array(289.58643, dtype=float32)), 'eval/epoch_eval_time': 4.0867555141448975, 'eval/sps': 31320.689372528417}
I0727 03:42:44.977521 140120985872192 train.py:379] starting iteration 366 3652.68927025795
I0727 03:42:54.920706 140120985872192 train.py:394] {'eval/walltime': 1511.831004858017, 'training/sps': 42055.1477022876, 'training/walltime': 2140.1170201301575, 'training/entropy_loss': Array(-0.04621885, dtype=float32), 'training/policy_loss': Array(0.00384168, dtype=float32), 'training/total_loss': Array(47.196373, dtype=float32), 'training/v_loss': Array(47.238747, dtype=float32), 'eval/episode_goal_distance': (Array(0.432245, dtype=float32), Array(0.19682196, dtype=float32)), 'eval/episode_reward': (Array(-9622.227, dtype=float32), Array(4623.885, dtype=float32)), 'eval/avg_episode_length': (Array(914.5781, dtype=float32), Array(278.59055, dtype=float32)), 'eval/epoch_eval_time': 4.095871210098267, 'eval/sps': 31250.98261986833}
I0727 03:42:54.922927 140120985872192 train.py:379] starting iteration 367 3662.634676218033
I0727 03:43:04.826231 140120985872192 train.py:394] {'eval/walltime': 1515.9285356998444, 'training/sps': 42356.545430062535, 'training/walltime': 2145.9191930294037, 'training/entropy_loss': Array(-0.04692551, dtype=float32), 'training/policy_loss': Array(0.00030369, dtype=float32), 'training/total_loss': Array(36.085884, dtype=float32), 'training/v_loss': Array(36.132507, dtype=float32), 'eval/episode_goal_distance': (Array(0.4670657, dtype=float32), Array(0.19174163, dtype=float32)), 'eval/episode_reward': (Array(-9525.836, dtype=float32), Array(4785.049, dtype=float32)), 'eval/avg_episode_length': (Array(875.8047, dtype=float32), Array(328.59027, dtype=float32)), 'eval/epoch_eval_time': 4.097530841827393, 'eval/sps': 31238.3249671686}
I0727 03:43:04.828402 140120985872192 train.py:379] starting iteration 368 3672.5401513576508
I0727 03:43:14.744569 140120985872192 train.py:394] {'eval/walltime': 1520.007796049118, 'training/sps': 42131.19032554563, 'training/walltime': 2151.75240111351, 'training/entropy_loss': Array(-0.04798844, dtype=float32), 'training/policy_loss': Array(0.00024313, dtype=float32), 'training/total_loss': Array(31.085442, dtype=float32), 'training/v_loss': Array(31.133186, dtype=float32), 'eval/episode_goal_distance': (Array(0.46627778, dtype=float32), Array(0.21969762, dtype=float32)), 'eval/episode_reward': (Array(-9616.199, dtype=float32), Array(5121.3613, dtype=float32)), 'eval/avg_episode_length': (Array(891.3047, dtype=float32), Array(310.1701, dtype=float32)), 'eval/epoch_eval_time': 4.079260349273682, 'eval/sps': 31378.237484349484}
I0727 03:43:14.746799 140120985872192 train.py:379] starting iteration 369 3682.4585485458374
I0727 03:43:24.695308 140120985872192 train.py:394] {'eval/walltime': 1524.122694015503, 'training/sps': 42152.78337217883, 'training/walltime': 2157.5826210975647, 'training/entropy_loss': Array(-0.04819448, dtype=float32), 'training/policy_loss': Array(0.0004288, dtype=float32), 'training/total_loss': Array(28.742033, dtype=float32), 'training/v_loss': Array(28.789799, dtype=float32), 'eval/episode_goal_distance': (Array(0.49022087, dtype=float32), Array(0.22201125, dtype=float32)), 'eval/episode_reward': (Array(-9929.35, dtype=float32), Array(5473.8354, dtype=float32)), 'eval/avg_episode_length': (Array(860.21875, dtype=float32), Array(345.54904, dtype=float32)), 'eval/epoch_eval_time': 4.114897966384888, 'eval/sps': 31106.482115874533}
I0727 03:43:24.697468 140120985872192 train.py:379] starting iteration 370 3692.409217596054
I0727 03:43:34.607815 140120985872192 train.py:394] {'eval/walltime': 1528.2093970775604, 'training/sps': 42227.73701112358, 'training/walltime': 2163.4024925231934, 'training/entropy_loss': Array(-0.04835089, dtype=float32), 'training/policy_loss': Array(0.00047438, dtype=float32), 'training/total_loss': Array(29.50414, dtype=float32), 'training/v_loss': Array(29.552017, dtype=float32), 'eval/episode_goal_distance': (Array(0.42908925, dtype=float32), Array(0.18123902, dtype=float32)), 'eval/episode_reward': (Array(-8621.431, dtype=float32), Array(4884.654, dtype=float32)), 'eval/avg_episode_length': (Array(829.25, dtype=float32), Array(374.80338, dtype=float32)), 'eval/epoch_eval_time': 4.086703062057495, 'eval/sps': 31321.091367856076}
I0727 03:43:34.610306 140120985872192 train.py:379] starting iteration 371 3702.3220551013947
I0727 03:43:44.545358 140120985872192 train.py:394] {'eval/walltime': 1532.29044008255, 'training/sps': 42006.32125128933, 'training/walltime': 2169.2530405521393, 'training/entropy_loss': Array(-0.04792418, dtype=float32), 'training/policy_loss': Array(0.00024469, dtype=float32), 'training/total_loss': Array(27.157593, dtype=float32), 'training/v_loss': Array(27.205273, dtype=float32), 'eval/episode_goal_distance': (Array(0.48185652, dtype=float32), Array(0.22687085, dtype=float32)), 'eval/episode_reward': (Array(-10172.406, dtype=float32), Array(4518.9556, dtype=float32)), 'eval/avg_episode_length': (Array(930.1172, dtype=float32), Array(254.11084, dtype=float32)), 'eval/epoch_eval_time': 4.081043004989624, 'eval/sps': 31364.53103863468}
I0727 03:43:44.547605 140120985872192 train.py:379] starting iteration 372 3712.259353160858
I0727 03:43:54.503182 140120985872192 train.py:394] {'eval/walltime': 1536.3882489204407, 'training/sps': 41981.1947838009, 'training/walltime': 2175.1070902347565, 'training/entropy_loss': Array(-0.04810509, dtype=float32), 'training/policy_loss': Array(-0.00010441, dtype=float32), 'training/total_loss': Array(26.348816, dtype=float32), 'training/v_loss': Array(26.397026, dtype=float32), 'eval/episode_goal_distance': (Array(0.43828738, dtype=float32), Array(0.17311667, dtype=float32)), 'eval/episode_reward': (Array(-9387.941, dtype=float32), Array(4476.347, dtype=float32)), 'eval/avg_episode_length': (Array(891.3281, dtype=float32), Array(310.10324, dtype=float32)), 'eval/epoch_eval_time': 4.097808837890625, 'eval/sps': 31236.205753777638}
I0727 03:43:54.505414 140120985872192 train.py:379] starting iteration 373 3722.2171630859375
I0727 03:44:04.415310 140120985872192 train.py:394] {'eval/walltime': 1540.4706583023071, 'training/sps': 42199.32154862864, 'training/walltime': 2180.93088054657, 'training/entropy_loss': Array(-0.04828607, dtype=float32), 'training/policy_loss': Array(-0.00010729, dtype=float32), 'training/total_loss': Array(30.131521, dtype=float32), 'training/v_loss': Array(30.179916, dtype=float32), 'eval/episode_goal_distance': (Array(0.4814958, dtype=float32), Array(0.21548013, dtype=float32)), 'eval/episode_reward': (Array(-9431.408, dtype=float32), Array(5056.6943, dtype=float32)), 'eval/avg_episode_length': (Array(860.1953, dtype=float32), Array(345.6068, dtype=float32)), 'eval/epoch_eval_time': 4.082409381866455, 'eval/sps': 31354.033372684222}
I0727 03:44:04.417544 140120985872192 train.py:379] starting iteration 374 3732.1292927265167
I0727 03:44:14.337918 140120985872192 train.py:394] {'eval/walltime': 1544.555831193924, 'training/sps': 42141.912582829544, 'training/walltime': 2186.7626044750214, 'training/entropy_loss': Array(-0.04881241, dtype=float32), 'training/policy_loss': Array(-1.394032e-05, dtype=float32), 'training/total_loss': Array(29.175766, dtype=float32), 'training/v_loss': Array(29.22459, dtype=float32), 'eval/episode_goal_distance': (Array(0.4728636, dtype=float32), Array(0.20956376, dtype=float32)), 'eval/episode_reward': (Array(-9756.691, dtype=float32), Array(4829.9844, dtype=float32)), 'eval/avg_episode_length': (Array(899.10156, dtype=float32), Array(300.09808, dtype=float32)), 'eval/epoch_eval_time': 4.085172891616821, 'eval/sps': 31332.823211146988}
I0727 03:44:14.340135 140120985872192 train.py:379] starting iteration 375 3742.0518839359283
I0727 03:44:24.288143 140120985872192 train.py:394] {'eval/walltime': 1548.677012681961, 'training/sps': 42203.68242789136, 'training/walltime': 2192.585793018341, 'training/entropy_loss': Array(-0.04904701, dtype=float32), 'training/policy_loss': Array(-0.00034684, dtype=float32), 'training/total_loss': Array(52.90804, dtype=float32), 'training/v_loss': Array(52.957428, dtype=float32), 'eval/episode_goal_distance': (Array(0.4963151, dtype=float32), Array(0.23608515, dtype=float32)), 'eval/episode_reward': (Array(-10047.964, dtype=float32), Array(4796.227, dtype=float32)), 'eval/avg_episode_length': (Array(906.8281, dtype=float32), Array(289.68353, dtype=float32)), 'eval/epoch_eval_time': 4.121181488037109, 'eval/sps': 31059.0543929104}
I0727 03:44:24.290317 140120985872192 train.py:379] starting iteration 376 3752.0020656585693
I0727 03:44:34.199877 140120985872192 train.py:394] {'eval/walltime': 1552.7601299285889, 'training/sps': 42207.13341363723, 'training/walltime': 2198.4085054397583, 'training/entropy_loss': Array(-0.04837821, dtype=float32), 'training/policy_loss': Array(-0.00016882, dtype=float32), 'training/total_loss': Array(31.612228, dtype=float32), 'training/v_loss': Array(31.660776, dtype=float32), 'eval/episode_goal_distance': (Array(0.48241365, dtype=float32), Array(0.25128183, dtype=float32)), 'eval/episode_reward': (Array(-9901.301, dtype=float32), Array(4989.1626, dtype=float32)), 'eval/avg_episode_length': (Array(914.6328, dtype=float32), Array(278.4123, dtype=float32)), 'eval/epoch_eval_time': 4.083117246627808, 'eval/sps': 31348.597718009078}
I0727 03:44:34.202093 140120985872192 train.py:379] starting iteration 377 3761.9138417243958
I0727 03:44:44.092894 140120985872192 train.py:394] {'eval/walltime': 1556.8608531951904, 'training/sps': 42472.41223341694, 'training/walltime': 2204.194849729538, 'training/entropy_loss': Array(-0.04768083, dtype=float32), 'training/policy_loss': Array(-0.000215, dtype=float32), 'training/total_loss': Array(28.467384, dtype=float32), 'training/v_loss': Array(28.51528, dtype=float32), 'eval/episode_goal_distance': (Array(0.47263217, dtype=float32), Array(0.20642667, dtype=float32)), 'eval/episode_reward': (Array(-9434.418, dtype=float32), Array(4786.894, dtype=float32)), 'eval/avg_episode_length': (Array(875.7031, dtype=float32), Array(328.859, dtype=float32)), 'eval/epoch_eval_time': 4.1007232666015625, 'eval/sps': 31214.005841969152}
I0727 03:44:44.095209 140120985872192 train.py:379] starting iteration 378 3771.806958436966
I0727 03:44:54.022057 140120985872192 train.py:394] {'eval/walltime': 1560.9509871006012, 'training/sps': 42131.570893794895, 'training/walltime': 2210.0280051231384, 'training/entropy_loss': Array(-0.0473672, dtype=float32), 'training/policy_loss': Array(0.000626, dtype=float32), 'training/total_loss': Array(26.05975, dtype=float32), 'training/v_loss': Array(26.106491, dtype=float32), 'eval/episode_goal_distance': (Array(0.46178663, dtype=float32), Array(0.19730693, dtype=float32)), 'eval/episode_reward': (Array(-9653.902, dtype=float32), Array(4878.9307, dtype=float32)), 'eval/avg_episode_length': (Array(883.52344, dtype=float32), Array(319.69308, dtype=float32)), 'eval/epoch_eval_time': 4.090133905410767, 'eval/sps': 31294.818937509855}
I0727 03:44:54.024293 140120985872192 train.py:379] starting iteration 379 3781.7360422611237
I0727 03:45:03.923358 140120985872192 train.py:394] {'eval/walltime': 1565.0368645191193, 'training/sps': 42303.039277366006, 'training/walltime': 2215.837516784668, 'training/entropy_loss': Array(-0.04742888, dtype=float32), 'training/policy_loss': Array(0.00165896, dtype=float32), 'training/total_loss': Array(25.953705, dtype=float32), 'training/v_loss': Array(25.999474, dtype=float32), 'eval/episode_goal_distance': (Array(0.46512446, dtype=float32), Array(0.20328365, dtype=float32)), 'eval/episode_reward': (Array(-9873.178, dtype=float32), Array(4610.1147, dtype=float32)), 'eval/avg_episode_length': (Array(899.0156, dtype=float32), Array(300.35315, dtype=float32)), 'eval/epoch_eval_time': 4.085877418518066, 'eval/sps': 31327.42049966471}
I0727 03:45:03.925577 140120985872192 train.py:379] starting iteration 380 3791.637326002121
I0727 03:45:13.833663 140120985872192 train.py:394] {'eval/walltime': 1569.143019914627, 'training/sps': 42385.27592395609, 'training/walltime': 2221.6357567310333, 'training/entropy_loss': Array(-0.04763598, dtype=float32), 'training/policy_loss': Array(0.0017337, dtype=float32), 'training/total_loss': Array(26.338064, dtype=float32), 'training/v_loss': Array(26.383965, dtype=float32), 'eval/episode_goal_distance': (Array(0.48240042, dtype=float32), Array(0.23815411, dtype=float32)), 'eval/episode_reward': (Array(-10662.002, dtype=float32), Array(4790.761, dtype=float32)), 'eval/avg_episode_length': (Array(945.6094, dtype=float32), Array(226.13509, dtype=float32)), 'eval/epoch_eval_time': 4.1061553955078125, 'eval/sps': 31172.712104377166}
I0727 03:45:13.835901 140120985872192 train.py:379] starting iteration 381 3801.5476491451263
I0727 03:45:23.797025 140120985872192 train.py:394] {'eval/walltime': 1573.2390565872192, 'training/sps': 41928.99030781312, 'training/walltime': 2227.497095108032, 'training/entropy_loss': Array(-0.04857792, dtype=float32), 'training/policy_loss': Array(7.756825e-05, dtype=float32), 'training/total_loss': Array(28.86376, dtype=float32), 'training/v_loss': Array(28.91226, dtype=float32), 'eval/episode_goal_distance': (Array(0.49596187, dtype=float32), Array(0.2004434, dtype=float32)), 'eval/episode_reward': (Array(-10377.119, dtype=float32), Array(4607.189, dtype=float32)), 'eval/avg_episode_length': (Array(914.65625, dtype=float32), Array(278.33588, dtype=float32)), 'eval/epoch_eval_time': 4.096036672592163, 'eval/sps': 31249.720212830915}
I0727 03:45:23.799290 140120985872192 train.py:379] starting iteration 382 3811.5110397338867
I0727 03:45:33.703108 140120985872192 train.py:394] {'eval/walltime': 1577.3088076114655, 'training/sps': 42151.19927944945, 'training/walltime': 2233.327534198761, 'training/entropy_loss': Array(-0.04891743, dtype=float32), 'training/policy_loss': Array(-0.00020978, dtype=float32), 'training/total_loss': Array(32.317673, dtype=float32), 'training/v_loss': Array(32.3668, dtype=float32), 'eval/episode_goal_distance': (Array(0.45557338, dtype=float32), Array(0.18336616, dtype=float32)), 'eval/episode_reward': (Array(-9774.455, dtype=float32), Array(4350.3135, dtype=float32)), 'eval/avg_episode_length': (Array(914.53906, dtype=float32), Array(278.71793, dtype=float32)), 'eval/epoch_eval_time': 4.069751024246216, 'eval/sps': 31451.555448335486}
I0727 03:45:33.705334 140120985872192 train.py:379] starting iteration 383 3821.4170830249786
I0727 03:45:43.622538 140120985872192 train.py:394] {'eval/walltime': 1581.3996024131775, 'training/sps': 42205.63163841438, 'training/walltime': 2239.1504538059235, 'training/entropy_loss': Array(-0.04844451, dtype=float32), 'training/policy_loss': Array(0.00086563, dtype=float32), 'training/total_loss': Array(47.646988, dtype=float32), 'training/v_loss': Array(47.694565, dtype=float32), 'eval/episode_goal_distance': (Array(0.48190853, dtype=float32), Array(0.2162445, dtype=float32)), 'eval/episode_reward': (Array(-9898.807, dtype=float32), Array(5350.8867, dtype=float32)), 'eval/avg_episode_length': (Array(875.7578, dtype=float32), Array(328.71457, dtype=float32)), 'eval/epoch_eval_time': 4.090794801712036, 'eval/sps': 31289.76304224079}
I0727 03:45:43.624737 140120985872192 train.py:379] starting iteration 384 3831.336485862732
I0727 03:45:53.541513 140120985872192 train.py:394] {'eval/walltime': 1585.5085151195526, 'training/sps': 42342.00694014932, 'training/walltime': 2244.9546189308167, 'training/entropy_loss': Array(-0.04832686, dtype=float32), 'training/policy_loss': Array(-0.00033426, dtype=float32), 'training/total_loss': Array(34.30585, dtype=float32), 'training/v_loss': Array(34.354515, dtype=float32), 'eval/episode_goal_distance': (Array(0.44944537, dtype=float32), Array(0.23172636, dtype=float32)), 'eval/episode_reward': (Array(-9361.928, dtype=float32), Array(5060.808, dtype=float32)), 'eval/avg_episode_length': (Array(875.71094, dtype=float32), Array(328.83832, dtype=float32)), 'eval/epoch_eval_time': 4.108912706375122, 'eval/sps': 31151.79346628696}
I0727 03:45:53.543869 140120985872192 train.py:379] starting iteration 385 3841.2556183338165
I0727 03:46:03.406536 140120985872192 train.py:394] {'eval/walltime': 1589.6014382839203, 'training/sps': 42621.46107133753, 'training/walltime': 2250.720728158951, 'training/entropy_loss': Array(-0.04815862, dtype=float32), 'training/policy_loss': Array(-0.00025628, dtype=float32), 'training/total_loss': Array(30.681099, dtype=float32), 'training/v_loss': Array(30.729513, dtype=float32), 'eval/episode_goal_distance': (Array(0.45964837, dtype=float32), Array(0.21016578, dtype=float32)), 'eval/episode_reward': (Array(-9464.737, dtype=float32), Array(5040.746, dtype=float32)), 'eval/avg_episode_length': (Array(883.46875, dtype=float32), Array(319.843, dtype=float32)), 'eval/epoch_eval_time': 4.092923164367676, 'eval/sps': 31273.49203971069}
I0727 03:46:03.408753 140120985872192 train.py:379] starting iteration 386 3851.1205015182495
I0727 03:46:13.310036 140120985872192 train.py:394] {'eval/walltime': 1593.6799697875977, 'training/sps': 42232.96545851386, 'training/walltime': 2256.5398790836334, 'training/entropy_loss': Array(-0.04783425, dtype=float32), 'training/policy_loss': Array(-0.0005455, dtype=float32), 'training/total_loss': Array(28.119204, dtype=float32), 'training/v_loss': Array(28.167583, dtype=float32), 'eval/episode_goal_distance': (Array(0.4734211, dtype=float32), Array(0.2070956, dtype=float32)), 'eval/episode_reward': (Array(-10050.936, dtype=float32), Array(4620.567, dtype=float32)), 'eval/avg_episode_length': (Array(906.71875, dtype=float32), Array(290.0235, dtype=float32)), 'eval/epoch_eval_time': 4.078531503677368, 'eval/sps': 31383.84486783786}
I0727 03:46:13.312379 140120985872192 train.py:379] starting iteration 387 3861.0241277217865
I0727 03:46:23.214521 140120985872192 train.py:394] {'eval/walltime': 1597.7864215373993, 'training/sps': 42433.30560025924, 'training/walltime': 2262.331556081772, 'training/entropy_loss': Array(-0.04790372, dtype=float32), 'training/policy_loss': Array(-0.00015312, dtype=float32), 'training/total_loss': Array(28.326635, dtype=float32), 'training/v_loss': Array(28.37469, dtype=float32), 'eval/episode_goal_distance': (Array(0.49008608, dtype=float32), Array(0.23758715, dtype=float32)), 'eval/episode_reward': (Array(-9757.417, dtype=float32), Array(5273.051, dtype=float32)), 'eval/avg_episode_length': (Array(867.9531, dtype=float32), Array(337.4165, dtype=float32)), 'eval/epoch_eval_time': 4.106451749801636, 'eval/sps': 31170.46243297102}
I0727 03:46:23.216893 140120985872192 train.py:379] starting iteration 388 3870.928641796112
I0727 03:46:33.096094 140120985872192 train.py:394] {'eval/walltime': 1601.857749223709, 'training/sps': 42342.834857948525, 'training/walltime': 2268.1356077194214, 'training/entropy_loss': Array(-0.04801852, dtype=float32), 'training/policy_loss': Array(-5.847429e-05, dtype=float32), 'training/total_loss': Array(29.797033, dtype=float32), 'training/v_loss': Array(29.845112, dtype=float32), 'eval/episode_goal_distance': (Array(0.45557845, dtype=float32), Array(0.18564774, dtype=float32)), 'eval/episode_reward': (Array(-9733.393, dtype=float32), Array(4489.708, dtype=float32)), 'eval/avg_episode_length': (Array(914.5156, dtype=float32), Array(278.7946, dtype=float32)), 'eval/epoch_eval_time': 4.0713276863098145, 'eval/sps': 31439.375521260765}
I0727 03:46:33.098392 140120985872192 train.py:379] starting iteration 389 3880.8101410865784
I0727 03:46:42.998947 140120985872192 train.py:394] {'eval/walltime': 1605.9435982704163, 'training/sps': 42292.09433128994, 'training/walltime': 2273.9466228485107, 'training/entropy_loss': Array(-0.04819762, dtype=float32), 'training/policy_loss': Array(-4.3265652e-05, dtype=float32), 'training/total_loss': Array(26.824026, dtype=float32), 'training/v_loss': Array(26.872267, dtype=float32), 'eval/episode_goal_distance': (Array(0.47593063, dtype=float32), Array(0.21897353, dtype=float32)), 'eval/episode_reward': (Array(-10137.934, dtype=float32), Array(5086.496, dtype=float32)), 'eval/avg_episode_length': (Array(899.02344, dtype=float32), Array(300.33035, dtype=float32)), 'eval/epoch_eval_time': 4.085849046707153, 'eval/sps': 31327.638034781805}
I0727 03:46:43.003836 140120985872192 train.py:379] starting iteration 390 3890.715569972992
I0727 03:46:52.974696 140120985872192 train.py:394] {'eval/walltime': 1610.0658822059631, 'training/sps': 42050.323419952576, 'training/walltime': 2279.7910487651825, 'training/entropy_loss': Array(-0.04818226, dtype=float32), 'training/policy_loss': Array(0.00351035, dtype=float32), 'training/total_loss': Array(33.36812, dtype=float32), 'training/v_loss': Array(33.412792, dtype=float32), 'eval/episode_goal_distance': (Array(0.45119286, dtype=float32), Array(0.20642388, dtype=float32)), 'eval/episode_reward': (Array(-9550.889, dtype=float32), Array(4603.1313, dtype=float32)), 'eval/avg_episode_length': (Array(891.21875, dtype=float32), Array(310.4152, dtype=float32)), 'eval/epoch_eval_time': 4.122283935546875, 'eval/sps': 31050.748080752746}
I0727 03:46:52.977184 140120985872192 train.py:379] starting iteration 391 3900.6889328956604
I0727 03:47:02.875530 140120985872192 train.py:394] {'eval/walltime': 1614.156084060669, 'training/sps': 42354.561371581636, 'training/walltime': 2285.593493461609, 'training/entropy_loss': Array(-0.04861584, dtype=float32), 'training/policy_loss': Array(0.00170214, dtype=float32), 'training/total_loss': Array(49.541054, dtype=float32), 'training/v_loss': Array(49.587967, dtype=float32), 'eval/episode_goal_distance': (Array(0.4649986, dtype=float32), Array(0.20072708, dtype=float32)), 'eval/episode_reward': (Array(-10211.362, dtype=float32), Array(4539.699, dtype=float32)), 'eval/avg_episode_length': (Array(922.3047, dtype=float32), Array(266.89233, dtype=float32)), 'eval/epoch_eval_time': 4.0902018547058105, 'eval/sps': 31294.299046081298}
I0727 03:47:02.877932 140120985872192 train.py:379] starting iteration 392 3910.5896809101105
I0727 03:47:12.783363 140120985872192 train.py:394] {'eval/walltime': 1618.2530488967896, 'training/sps': 42336.43846886977, 'training/walltime': 2291.3984220027924, 'training/entropy_loss': Array(-0.04858953, dtype=float32), 'training/policy_loss': Array(0.00038453, dtype=float32), 'training/total_loss': Array(35.39971, dtype=float32), 'training/v_loss': Array(35.44792, dtype=float32), 'eval/episode_goal_distance': (Array(0.4565277, dtype=float32), Array(0.19494104, dtype=float32)), 'eval/episode_reward': (Array(-9873.832, dtype=float32), Array(4308.1865, dtype=float32)), 'eval/avg_episode_length': (Array(914.60156, dtype=float32), Array(278.51404, dtype=float32)), 'eval/epoch_eval_time': 4.0969648361206055, 'eval/sps': 31242.6406181222}
I0727 03:47:12.785968 140120985872192 train.py:379] starting iteration 393 3920.4977169036865
I0727 03:47:22.654621 140120985872192 train.py:394] {'eval/walltime': 1622.3316495418549, 'training/sps': 42470.6920344597, 'training/walltime': 2297.1850006580353, 'training/entropy_loss': Array(-0.04883997, dtype=float32), 'training/policy_loss': Array(0.00117589, dtype=float32), 'training/total_loss': Array(31.733269, dtype=float32), 'training/v_loss': Array(31.780933, dtype=float32), 'eval/episode_goal_distance': (Array(0.46761733, dtype=float32), Array(0.22148806, dtype=float32)), 'eval/episode_reward': (Array(-8913.698, dtype=float32), Array(5601.7593, dtype=float32)), 'eval/avg_episode_length': (Array(798.0703, dtype=float32), Array(399.95816, dtype=float32)), 'eval/epoch_eval_time': 4.078600645065308, 'eval/sps': 31383.312841591145}
I0727 03:47:22.657011 140120985872192 train.py:379] starting iteration 394 3930.3687603473663
I0727 03:47:32.554485 140120985872192 train.py:394] {'eval/walltime': 1626.4090359210968, 'training/sps': 42252.7092395057, 'training/walltime': 2303.0014324188232, 'training/entropy_loss': Array(-0.04866232, dtype=float32), 'training/policy_loss': Array(-7.105166e-05, dtype=float32), 'training/total_loss': Array(30.343037, dtype=float32), 'training/v_loss': Array(30.39177, dtype=float32), 'eval/episode_goal_distance': (Array(0.45259795, dtype=float32), Array(0.23632635, dtype=float32)), 'eval/episode_reward': (Array(-9369.118, dtype=float32), Array(5421.0767, dtype=float32)), 'eval/avg_episode_length': (Array(860.21094, dtype=float32), Array(345.5687, dtype=float32)), 'eval/epoch_eval_time': 4.077386379241943, 'eval/sps': 31392.65894732238}
I0727 03:47:32.557093 140120985872192 train.py:379] starting iteration 395 3940.2688417434692
I0727 03:47:42.470719 140120985872192 train.py:394] {'eval/walltime': 1630.5106444358826, 'training/sps': 42311.54956229502, 'training/walltime': 2308.8097755908966, 'training/entropy_loss': Array(-0.04893196, dtype=float32), 'training/policy_loss': Array(-9.428608e-05, dtype=float32), 'training/total_loss': Array(28.642994, dtype=float32), 'training/v_loss': Array(28.692019, dtype=float32), 'eval/episode_goal_distance': (Array(0.46064585, dtype=float32), Array(0.19164589, dtype=float32)), 'eval/episode_reward': (Array(-9583.938, dtype=float32), Array(4480.24, dtype=float32)), 'eval/avg_episode_length': (Array(899.1875, dtype=float32), Array(299.8418, dtype=float32)), 'eval/epoch_eval_time': 4.101608514785767, 'eval/sps': 31207.268938168185}
I0727 03:47:42.473191 140120985872192 train.py:379] starting iteration 396 3950.1849398612976
I0727 03:47:52.354430 140120985872192 train.py:394] {'eval/walltime': 1634.5809834003448, 'training/sps': 42319.64106693439, 'training/walltime': 2314.6170082092285, 'training/entropy_loss': Array(-0.04899434, dtype=float32), 'training/policy_loss': Array(-0.00039967, dtype=float32), 'training/total_loss': Array(29.297588, dtype=float32), 'training/v_loss': Array(29.346983, dtype=float32), 'eval/episode_goal_distance': (Array(0.46822268, dtype=float32), Array(0.18409726, dtype=float32)), 'eval/episode_reward': (Array(-9196.959, dtype=float32), Array(4826.7007, dtype=float32)), 'eval/avg_episode_length': (Array(852.4375, dtype=float32), Array(353.43808, dtype=float32)), 'eval/epoch_eval_time': 4.07033896446228, 'eval/sps': 31447.012427602494}
I0727 03:47:52.357404 140120985872192 train.py:379] starting iteration 397 3960.0691533088684
I0727 03:48:02.298182 140120985872192 train.py:394] {'eval/walltime': 1638.6742582321167, 'training/sps': 42054.15942344075, 'training/walltime': 2320.4609010219574, 'training/entropy_loss': Array(-0.04888612, dtype=float32), 'training/policy_loss': Array(-0.00013104, dtype=float32), 'training/total_loss': Array(28.550518, dtype=float32), 'training/v_loss': Array(28.599535, dtype=float32), 'eval/episode_goal_distance': (Array(0.47726876, dtype=float32), Array(0.21393941, dtype=float32)), 'eval/episode_reward': (Array(-9711.86, dtype=float32), Array(4823.369, dtype=float32)), 'eval/avg_episode_length': (Array(891.41406, dtype=float32), Array(309.85825, dtype=float32)), 'eval/epoch_eval_time': 4.093274831771851, 'eval/sps': 31270.80522579834}
I0727 03:48:02.300735 140120985872192 train.py:379] starting iteration 398 3970.0124838352203
I0727 03:48:12.239480 140120985872192 train.py:394] {'eval/walltime': 1642.7990472316742, 'training/sps': 42296.91177013677, 'training/walltime': 2326.271254301071, 'training/entropy_loss': Array(-0.04884835, dtype=float32), 'training/policy_loss': Array(-0.00012823, dtype=float32), 'training/total_loss': Array(32.261993, dtype=float32), 'training/v_loss': Array(32.31097, dtype=float32), 'eval/episode_goal_distance': (Array(0.46936712, dtype=float32), Array(0.2039903, dtype=float32)), 'eval/episode_reward': (Array(-9939.767, dtype=float32), Array(4713.822, dtype=float32)), 'eval/avg_episode_length': (Array(930.03906, dtype=float32), Array(254.3948, dtype=float32)), 'eval/epoch_eval_time': 4.124788999557495, 'eval/sps': 31031.890361841968}
I0727 03:48:12.242120 140120985872192 train.py:379] starting iteration 399 3979.9538695812225
I0727 03:48:22.124562 140120985872192 train.py:394] {'eval/walltime': 1646.8795187473297, 'training/sps': 42383.947917181926, 'training/walltime': 2332.069675922394, 'training/entropy_loss': Array(-0.04827306, dtype=float32), 'training/policy_loss': Array(8.523258e-05, dtype=float32), 'training/total_loss': Array(29.47624, dtype=float32), 'training/v_loss': Array(29.524431, dtype=float32), 'eval/episode_goal_distance': (Array(0.4647345, dtype=float32), Array(0.18900035, dtype=float32)), 'eval/episode_reward': (Array(-9833.525, dtype=float32), Array(4294.412, dtype=float32)), 'eval/avg_episode_length': (Array(930.1172, dtype=float32), Array(254.1112, dtype=float32)), 'eval/epoch_eval_time': 4.080471515655518, 'eval/sps': 31368.92378954326}
I0727 03:48:22.126941 140120985872192 train.py:379] starting iteration 400 3989.8386902809143
I0727 03:48:31.999655 140120985872192 train.py:394] {'eval/walltime': 1650.9592208862305, 'training/sps': 42449.89967860882, 'training/walltime': 2337.859088897705, 'training/entropy_loss': Array(-0.04833585, dtype=float32), 'training/policy_loss': Array(-0.000417, dtype=float32), 'training/total_loss': Array(51.672115, dtype=float32), 'training/v_loss': Array(51.72087, dtype=float32), 'eval/episode_goal_distance': (Array(0.44226846, dtype=float32), Array(0.19830559, dtype=float32)), 'eval/episode_reward': (Array(-9586.961, dtype=float32), Array(4966.486, dtype=float32)), 'eval/avg_episode_length': (Array(875.72656, dtype=float32), Array(328.79736, dtype=float32)), 'eval/epoch_eval_time': 4.079702138900757, 'eval/sps': 31374.839545145955}
I0727 03:48:32.002134 140120985872192 train.py:379] starting iteration 401 3999.713883161545
I0727 03:48:41.910273 140120985872192 train.py:394] {'eval/walltime': 1655.049141407013, 'training/sps': 42266.43428862298, 'training/walltime': 2343.6736319065094, 'training/entropy_loss': Array(-0.0476811, dtype=float32), 'training/policy_loss': Array(-0.00037634, dtype=float32), 'training/total_loss': Array(28.203007, dtype=float32), 'training/v_loss': Array(28.251064, dtype=float32), 'eval/episode_goal_distance': (Array(0.43438965, dtype=float32), Array(0.1802495, dtype=float32)), 'eval/episode_reward': (Array(-9459.402, dtype=float32), Array(4426.129, dtype=float32)), 'eval/avg_episode_length': (Array(899.10156, dtype=float32), Array(300.0977, dtype=float32)), 'eval/epoch_eval_time': 4.089920520782471, 'eval/sps': 31296.451691318307}
I0727 03:48:41.912728 140120985872192 train.py:379] starting iteration 402 4009.6244769096375
I0727 03:48:51.821529 140120985872192 train.py:394] {'eval/walltime': 1659.1244339942932, 'training/sps': 42155.13473694206, 'training/walltime': 2349.503526687622, 'training/entropy_loss': Array(-0.04746132, dtype=float32), 'training/policy_loss': Array(-1.2018223e-05, dtype=float32), 'training/total_loss': Array(28.133411, dtype=float32), 'training/v_loss': Array(28.180885, dtype=float32), 'eval/episode_goal_distance': (Array(0.461719, dtype=float32), Array(0.19771253, dtype=float32)), 'eval/episode_reward': (Array(-9218.207, dtype=float32), Array(4902.5566, dtype=float32)), 'eval/avg_episode_length': (Array(852.4297, dtype=float32), Array(353.4568, dtype=float32)), 'eval/epoch_eval_time': 4.075292587280273, 'eval/sps': 31408.78777624733}
I0727 03:48:51.824088 140120985872192 train.py:379] starting iteration 403 4019.5358366966248
I0727 03:49:01.711478 140120985872192 train.py:394] {'eval/walltime': 1663.215585231781, 'training/sps': 42425.95460215425, 'training/walltime': 2355.29620718956, 'training/entropy_loss': Array(-0.04761287, dtype=float32), 'training/policy_loss': Array(-0.00025824, dtype=float32), 'training/total_loss': Array(26.913961, dtype=float32), 'training/v_loss': Array(26.961832, dtype=float32), 'eval/episode_goal_distance': (Array(0.47026873, dtype=float32), Array(0.2145687, dtype=float32)), 'eval/episode_reward': (Array(-9626.56, dtype=float32), Array(4761.91, dtype=float32)), 'eval/avg_episode_length': (Array(891.1953, dtype=float32), Array(310.48212, dtype=float32)), 'eval/epoch_eval_time': 4.091151237487793, 'eval/sps': 31287.036965810024}
I0727 03:49:01.713816 140120985872192 train.py:379] starting iteration 404 4029.4255652427673
I0727 03:49:11.623736 140120985872192 train.py:394] {'eval/walltime': 1667.3263096809387, 'training/sps': 42404.07038734364, 'training/walltime': 2361.091877222061, 'training/entropy_loss': Array(-0.04737236, dtype=float32), 'training/policy_loss': Array(-8.2828716e-05, dtype=float32), 'training/total_loss': Array(26.766958, dtype=float32), 'training/v_loss': Array(26.814413, dtype=float32), 'eval/episode_goal_distance': (Array(0.45910448, dtype=float32), Array(0.20721501, dtype=float32)), 'eval/episode_reward': (Array(-9253.34, dtype=float32), Array(4918.4863, dtype=float32)), 'eval/avg_episode_length': (Array(875.8125, dtype=float32), Array(328.57004, dtype=float32)), 'eval/epoch_eval_time': 4.110724449157715, 'eval/sps': 31138.06376056832}
I0727 03:49:11.626315 140120985872192 train.py:379] starting iteration 405 4039.33806347847
I0727 03:49:21.503689 140120985872192 train.py:394] {'eval/walltime': 1671.4144530296326, 'training/sps': 42476.91200882709, 'training/walltime': 2366.877608537674, 'training/entropy_loss': Array(-0.04729055, dtype=float32), 'training/policy_loss': Array(-8.764008e-05, dtype=float32), 'training/total_loss': Array(26.725609, dtype=float32), 'training/v_loss': Array(26.772987, dtype=float32), 'eval/episode_goal_distance': (Array(0.4633692, dtype=float32), Array(0.21661586, dtype=float32)), 'eval/episode_reward': (Array(-9576.037, dtype=float32), Array(4995.3047, dtype=float32)), 'eval/avg_episode_length': (Array(875.78125, dtype=float32), Array(328.6524, dtype=float32)), 'eval/epoch_eval_time': 4.088143348693848, 'eval/sps': 31310.05668891129}
I0727 03:49:21.506025 140120985872192 train.py:379] starting iteration 406 4049.2177743911743
I0727 03:49:31.382909 140120985872192 train.py:394] {'eval/walltime': 1675.4931087493896, 'training/sps': 42412.263633149065, 'training/walltime': 2372.6721589565277, 'training/entropy_loss': Array(-0.04709903, dtype=float32), 'training/policy_loss': Array(-0.00031717, dtype=float32), 'training/total_loss': Array(29.291183, dtype=float32), 'training/v_loss': Array(29.338604, dtype=float32), 'eval/episode_goal_distance': (Array(0.43874195, dtype=float32), Array(0.20541225, dtype=float32)), 'eval/episode_reward': (Array(-9192.322, dtype=float32), Array(4680.573, dtype=float32)), 'eval/avg_episode_length': (Array(891.21094, dtype=float32), Array(310.4374, dtype=float32)), 'eval/epoch_eval_time': 4.07865571975708, 'eval/sps': 31382.88906807352}
I0727 03:49:31.385372 140120985872192 train.py:379] starting iteration 407 4059.0971205234528
I0727 03:49:41.321268 140120985872192 train.py:394] {'eval/walltime': 1679.6212828159332, 'training/sps': 42342.26087788071, 'training/walltime': 2378.4762892723083, 'training/entropy_loss': Array(-0.04645304, dtype=float32), 'training/policy_loss': Array(-3.9509137e-05, dtype=float32), 'training/total_loss': Array(31.132122, dtype=float32), 'training/v_loss': Array(31.178616, dtype=float32), 'eval/episode_goal_distance': (Array(0.4259908, dtype=float32), Array(0.1756561, dtype=float32)), 'eval/episode_reward': (Array(-8893.001, dtype=float32), Array(4098.8403, dtype=float32)), 'eval/avg_episode_length': (Array(898.97656, dtype=float32), Array(300.4694, dtype=float32)), 'eval/epoch_eval_time': 4.128174066543579, 'eval/sps': 31006.44448047011}
I0727 03:49:41.323620 140120985872192 train.py:379] starting iteration 408 4069.0353689193726
I0727 03:49:51.225294 140120985872192 train.py:394] {'eval/walltime': 1683.6978750228882, 'training/sps': 42217.73182976229, 'training/walltime': 2384.297539949417, 'training/entropy_loss': Array(-0.04629058, dtype=float32), 'training/policy_loss': Array(-0.00028308, dtype=float32), 'training/total_loss': Array(49.571877, dtype=float32), 'training/v_loss': Array(49.618454, dtype=float32), 'eval/episode_goal_distance': (Array(0.42158437, dtype=float32), Array(0.17505299, dtype=float32)), 'eval/episode_reward': (Array(-8866.457, dtype=float32), Array(4771.812, dtype=float32)), 'eval/avg_episode_length': (Array(867.91406, dtype=float32), Array(337.51617, dtype=float32)), 'eval/epoch_eval_time': 4.076592206954956, 'eval/sps': 31398.774638685445}
I0727 03:49:51.411559 140120985872192 train.py:379] starting iteration 409 4079.123303413391
I0727 03:50:01.278338 140120985872192 train.py:394] {'eval/walltime': 1687.7834510803223, 'training/sps': 42538.18689873292, 'training/walltime': 2390.074937105179, 'training/entropy_loss': Array(-0.04662177, dtype=float32), 'training/policy_loss': Array(-0.00030435, dtype=float32), 'training/total_loss': Array(29.459427, dtype=float32), 'training/v_loss': Array(29.506355, dtype=float32), 'eval/episode_goal_distance': (Array(0.46917647, dtype=float32), Array(0.21727079, dtype=float32)), 'eval/episode_reward': (Array(-10376.514, dtype=float32), Array(4553.8237, dtype=float32)), 'eval/avg_episode_length': (Array(953.41406, dtype=float32), Array(210.06827, dtype=float32)), 'eval/epoch_eval_time': 4.085576057434082, 'eval/sps': 31329.731279164957}
I0727 03:50:01.281045 140120985872192 train.py:379] starting iteration 410 4088.9927945137024
I0727 03:50:11.197568 140120985872192 train.py:394] {'eval/walltime': 1691.864536523819, 'training/sps': 42140.89093422668, 'training/walltime': 2395.906802415848, 'training/entropy_loss': Array(-0.04694003, dtype=float32), 'training/policy_loss': Array(-0.00015246, dtype=float32), 'training/total_loss': Array(25.907684, dtype=float32), 'training/v_loss': Array(25.954777, dtype=float32), 'eval/episode_goal_distance': (Array(0.4503417, dtype=float32), Array(0.19251144, dtype=float32)), 'eval/episode_reward': (Array(-9525.044, dtype=float32), Array(4272.957, dtype=float32)), 'eval/avg_episode_length': (Array(906.8828, dtype=float32), Array(289.51404, dtype=float32)), 'eval/epoch_eval_time': 4.081085443496704, 'eval/sps': 31364.20488425958}
I0727 03:50:11.200231 140120985872192 train.py:379] starting iteration 411 4098.91198015213
I0727 03:50:21.061748 140120985872192 train.py:394] {'eval/walltime': 1695.946044921875, 'training/sps': 42545.86835073862, 'training/walltime': 2401.683156490326, 'training/entropy_loss': Array(-0.04715236, dtype=float32), 'training/policy_loss': Array(-0.00060245, dtype=float32), 'training/total_loss': Array(28.661057, dtype=float32), 'training/v_loss': Array(28.70881, dtype=float32), 'eval/episode_goal_distance': (Array(0.45450914, dtype=float32), Array(0.21777701, dtype=float32)), 'eval/episode_reward': (Array(-9718.684, dtype=float32), Array(4648.5503, dtype=float32)), 'eval/avg_episode_length': (Array(906.7969, dtype=float32), Array(289.7808, dtype=float32)), 'eval/epoch_eval_time': 4.08150839805603, 'eval/sps': 31360.954705119497}
I0727 03:50:21.064352 140120985872192 train.py:379] starting iteration 412 4108.776100635529
I0727 03:50:30.924855 140120985872192 train.py:394] {'eval/walltime': 1700.028743982315, 'training/sps': 42562.561146056476, 'training/walltime': 2407.4572451114655, 'training/entropy_loss': Array(-0.04744134, dtype=float32), 'training/policy_loss': Array(-0.00020095, dtype=float32), 'training/total_loss': Array(25.56735, dtype=float32), 'training/v_loss': Array(25.614994, dtype=float32), 'eval/episode_goal_distance': (Array(0.43798047, dtype=float32), Array(0.17467915, dtype=float32)), 'eval/episode_reward': (Array(-8416.758, dtype=float32), Array(4656.23, dtype=float32)), 'eval/avg_episode_length': (Array(836.8828, dtype=float32), Array(368.19925, dtype=float32)), 'eval/epoch_eval_time': 4.0826990604400635, 'eval/sps': 31351.80871896133}
I0727 03:50:30.927357 140120985872192 train.py:379] starting iteration 413 4118.6391060352325
I0727 03:50:40.841862 140120985872192 train.py:394] {'eval/walltime': 1704.1204013824463, 'training/sps': 42231.99994952443, 'training/walltime': 2413.276529073715, 'training/entropy_loss': Array(-0.04798156, dtype=float32), 'training/policy_loss': Array(-0.00021232, dtype=float32), 'training/total_loss': Array(26.339592, dtype=float32), 'training/v_loss': Array(26.387787, dtype=float32), 'eval/episode_goal_distance': (Array(0.45637786, dtype=float32), Array(0.18486999, dtype=float32)), 'eval/episode_reward': (Array(-9538.793, dtype=float32), Array(4337.4595, dtype=float32)), 'eval/avg_episode_length': (Array(899.0469, dtype=float32), Array(300.2601, dtype=float32)), 'eval/epoch_eval_time': 4.091657400131226, 'eval/sps': 31283.16657105623}
I0727 03:50:40.844263 140120985872192 train.py:379] starting iteration 414 4128.5560121536255
I0727 03:50:50.724928 140120985872192 train.py:394] {'eval/walltime': 1708.2111871242523, 'training/sps': 42474.25858367295, 'training/walltime': 2419.062621831894, 'training/entropy_loss': Array(-0.04921153, dtype=float32), 'training/policy_loss': Array(-0.00049482, dtype=float32), 'training/total_loss': Array(27.350906, dtype=float32), 'training/v_loss': Array(27.400614, dtype=float32), 'eval/episode_goal_distance': (Array(0.46043938, dtype=float32), Array(0.20412707, dtype=float32)), 'eval/episode_reward': (Array(-9687.131, dtype=float32), Array(4356.6987, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.75824, dtype=float32)), 'eval/epoch_eval_time': 4.09078574180603, 'eval/sps': 31289.83234000655}
I0727 03:50:50.727249 140120985872192 train.py:379] starting iteration 415 4138.438997507095
I0727 03:51:00.637629 140120985872192 train.py:394] {'eval/walltime': 1712.2963852882385, 'training/sps': 42216.12729119352, 'training/walltime': 2424.884093761444, 'training/entropy_loss': Array(-0.04934851, dtype=float32), 'training/policy_loss': Array(-0.00027995, dtype=float32), 'training/total_loss': Array(32.876053, dtype=float32), 'training/v_loss': Array(32.925682, dtype=float32), 'eval/episode_goal_distance': (Array(0.41989285, dtype=float32), Array(0.16571769, dtype=float32)), 'eval/episode_reward': (Array(-8882.86, dtype=float32), Array(4684.938, dtype=float32)), 'eval/avg_episode_length': (Array(875.6953, dtype=float32), Array(328.87997, dtype=float32)), 'eval/epoch_eval_time': 4.085198163986206, 'eval/sps': 31332.629376074547}
I0727 03:51:00.640153 140120985872192 train.py:379] starting iteration 416 4148.3519015312195
I0727 03:51:10.720943 140120985872192 train.py:394] {'eval/walltime': 1716.3939626216888, 'training/sps': 41102.62380285902, 'training/walltime': 2430.8632740974426, 'training/entropy_loss': Array(-0.04915405, dtype=float32), 'training/policy_loss': Array(-0.0001895, dtype=float32), 'training/total_loss': Array(48.532707, dtype=float32), 'training/v_loss': Array(48.582047, dtype=float32), 'eval/episode_goal_distance': (Array(0.44189012, dtype=float32), Array(0.1795295, dtype=float32)), 'eval/episode_reward': (Array(-9043.833, dtype=float32), Array(4788.219, dtype=float32)), 'eval/avg_episode_length': (Array(860.2344, dtype=float32), Array(345.51056, dtype=float32)), 'eval/epoch_eval_time': 4.097577333450317, 'eval/sps': 31237.97053324167}
I0727 03:51:10.723418 140120985872192 train.py:379] starting iteration 417 4158.435166835785
I0727 03:51:20.616866 140120985872192 train.py:394] {'eval/walltime': 1720.5139627456665, 'training/sps': 42594.801470680606, 'training/walltime': 2436.6329922676086, 'training/entropy_loss': Array(-0.04926609, dtype=float32), 'training/policy_loss': Array(-0.0001784, dtype=float32), 'training/total_loss': Array(35.242947, dtype=float32), 'training/v_loss': Array(35.292393, dtype=float32), 'eval/episode_goal_distance': (Array(0.44788173, dtype=float32), Array(0.21164066, dtype=float32)), 'eval/episode_reward': (Array(-9150.958, dtype=float32), Array(4911.432, dtype=float32)), 'eval/avg_episode_length': (Array(867.9531, dtype=float32), Array(337.4162, dtype=float32)), 'eval/epoch_eval_time': 4.120000123977661, 'eval/sps': 31067.960230161883}
I0727 03:51:20.619229 140120985872192 train.py:379] starting iteration 418 4168.33097743988
I0727 03:51:30.492093 140120985872192 train.py:394] {'eval/walltime': 1724.5843040943146, 'training/sps': 42381.978716502905, 'training/walltime': 2442.4316833019257, 'training/entropy_loss': Array(-0.04863565, dtype=float32), 'training/policy_loss': Array(-0.00026571, dtype=float32), 'training/total_loss': Array(29.06187, dtype=float32), 'training/v_loss': Array(29.110771, dtype=float32), 'eval/episode_goal_distance': (Array(0.44605303, dtype=float32), Array(0.18749759, dtype=float32)), 'eval/episode_reward': (Array(-9139.98, dtype=float32), Array(4130.143, dtype=float32)), 'eval/avg_episode_length': (Array(899.0781, dtype=float32), Array(300.16754, dtype=float32)), 'eval/epoch_eval_time': 4.070341348648071, 'eval/sps': 31446.994007643632}
I0727 03:51:30.494475 140120985872192 train.py:379] starting iteration 419 4178.206223964691
I0727 03:51:40.391190 140120985872192 train.py:394] {'eval/walltime': 1728.674007177353, 'training/sps': 42348.25364874395, 'training/walltime': 2448.2349922657013, 'training/entropy_loss': Array(-0.04831353, dtype=float32), 'training/policy_loss': Array(-0.00018046, dtype=float32), 'training/total_loss': Array(27.803556, dtype=float32), 'training/v_loss': Array(27.85205, dtype=float32), 'eval/episode_goal_distance': (Array(0.4350463, dtype=float32), Array(0.18796068, dtype=float32)), 'eval/episode_reward': (Array(-8742.04, dtype=float32), Array(4739.919, dtype=float32)), 'eval/avg_episode_length': (Array(852.53125, dtype=float32), Array(353.2134, dtype=float32)), 'eval/epoch_eval_time': 4.08970308303833, 'eval/sps': 31298.115633594112}
I0727 03:51:40.393345 140120985872192 train.py:379] starting iteration 420 4188.105094194412
I0727 03:51:50.300149 140120985872192 train.py:394] {'eval/walltime': 1732.777506828308, 'training/sps': 42374.004453661815, 'training/walltime': 2454.034774541855, 'training/entropy_loss': Array(-0.0481505, dtype=float32), 'training/policy_loss': Array(-0.00023855, dtype=float32), 'training/total_loss': Array(26.940132, dtype=float32), 'training/v_loss': Array(26.988522, dtype=float32), 'eval/episode_goal_distance': (Array(0.45864785, dtype=float32), Array(0.19837557, dtype=float32)), 'eval/episode_reward': (Array(-9572.836, dtype=float32), Array(4622.4336, dtype=float32)), 'eval/avg_episode_length': (Array(898.91406, dtype=float32), Array(300.6549, dtype=float32)), 'eval/epoch_eval_time': 4.1034996509552, 'eval/sps': 31192.886776584604}
I0727 03:51:50.302435 140120985872192 train.py:379] starting iteration 421 4198.014183998108
I0727 03:52:00.191393 140120985872192 train.py:394] {'eval/walltime': 1736.863960981369, 'training/sps': 42379.95916889258, 'training/walltime': 2459.833741903305, 'training/entropy_loss': Array(-0.04785559, dtype=float32), 'training/policy_loss': Array(-0.00014562, dtype=float32), 'training/total_loss': Array(25.34778, dtype=float32), 'training/v_loss': Array(25.39578, dtype=float32), 'eval/episode_goal_distance': (Array(0.4356956, dtype=float32), Array(0.17241874, dtype=float32)), 'eval/episode_reward': (Array(-8860.981, dtype=float32), Array(4808.809, dtype=float32)), 'eval/avg_episode_length': (Array(860.2578, dtype=float32), Array(345.45276, dtype=float32)), 'eval/epoch_eval_time': 4.086454153060913, 'eval/sps': 31322.999159093226}
I0727 03:52:00.193836 140120985872192 train.py:379] starting iteration 422 4207.90558552742
I0727 03:52:10.043446 140120985872192 train.py:394] {'eval/walltime': 1740.9352021217346, 'training/sps': 42557.41769465687, 'training/walltime': 2465.6085283756256, 'training/entropy_loss': Array(-0.04735467, dtype=float32), 'training/policy_loss': Array(-0.00010062, dtype=float32), 'training/total_loss': Array(24.201872, dtype=float32), 'training/v_loss': Array(24.249327, dtype=float32), 'eval/episode_goal_distance': (Array(0.43861216, dtype=float32), Array(0.18968683, dtype=float32)), 'eval/episode_reward': (Array(-9482.541, dtype=float32), Array(4590.349, dtype=float32)), 'eval/avg_episode_length': (Array(906.7969, dtype=float32), Array(289.781, dtype=float32)), 'eval/epoch_eval_time': 4.071241140365601, 'eval/sps': 31440.043855644843}
I0727 03:52:10.045601 140120985872192 train.py:379] starting iteration 423 4217.757350206375
I0727 03:52:19.915941 140120985872192 train.py:394] {'eval/walltime': 1745.0179500579834, 'training/sps': 42489.9283939501, 'training/walltime': 2471.3924872875214, 'training/entropy_loss': Array(-0.04815884, dtype=float32), 'training/policy_loss': Array(-2.5460522e-05, dtype=float32), 'training/total_loss': Array(29.815128, dtype=float32), 'training/v_loss': Array(29.863312, dtype=float32), 'eval/episode_goal_distance': (Array(0.42583996, dtype=float32), Array(0.17904972, dtype=float32)), 'eval/episode_reward': (Array(-9063.461, dtype=float32), Array(4491.478, dtype=float32)), 'eval/avg_episode_length': (Array(883.5703, dtype=float32), Array(319.5643, dtype=float32)), 'eval/epoch_eval_time': 4.082747936248779, 'eval/sps': 31351.433396989516}
I0727 03:52:19.918082 140120985872192 train.py:379] starting iteration 424 4227.629831314087
I0727 03:52:29.871588 140120985872192 train.py:394] {'eval/walltime': 1749.1349594593048, 'training/sps': 42133.82172532529, 'training/walltime': 2477.225331068039, 'training/entropy_loss': Array(-0.0480281, dtype=float32), 'training/policy_loss': Array(-0.00023274, dtype=float32), 'training/total_loss': Array(28.62397, dtype=float32), 'training/v_loss': Array(28.672232, dtype=float32), 'eval/episode_goal_distance': (Array(0.45655093, dtype=float32), Array(0.20887586, dtype=float32)), 'eval/episode_reward': (Array(-9183.055, dtype=float32), Array(5047.751, dtype=float32)), 'eval/avg_episode_length': (Array(860.1719, dtype=float32), Array(345.66492, dtype=float32)), 'eval/epoch_eval_time': 4.117009401321411, 'eval/sps': 31090.528955050875}
I0727 03:52:29.873731 140120985872192 train.py:379] starting iteration 425 4237.585480213165
I0727 03:52:39.778982 140120985872192 train.py:394] {'eval/walltime': 1753.2290363311768, 'training/sps': 42316.97077150977, 'training/walltime': 2483.032930135727, 'training/entropy_loss': Array(-0.04833473, dtype=float32), 'training/policy_loss': Array(-0.00019335, dtype=float32), 'training/total_loss': Array(50.968285, dtype=float32), 'training/v_loss': Array(51.01681, dtype=float32), 'eval/episode_goal_distance': (Array(0.43041337, dtype=float32), Array(0.18335722, dtype=float32)), 'eval/episode_reward': (Array(-9090.381, dtype=float32), Array(4779.9404, dtype=float32)), 'eval/avg_episode_length': (Array(852.4297, dtype=float32), Array(353.45654, dtype=float32)), 'eval/epoch_eval_time': 4.094076871871948, 'eval/sps': 31264.6791953064}
I0727 03:52:39.781125 140120985872192 train.py:379] starting iteration 426 4247.492873907089
I0727 03:52:49.671647 140120985872192 train.py:394] {'eval/walltime': 1757.3223628997803, 'training/sps': 42418.96399903754, 'training/walltime': 2488.8265652656555, 'training/entropy_loss': Array(-0.04925825, dtype=float32), 'training/policy_loss': Array(-0.0005856, dtype=float32), 'training/total_loss': Array(29.498611, dtype=float32), 'training/v_loss': Array(29.548454, dtype=float32), 'eval/episode_goal_distance': (Array(0.4832756, dtype=float32), Array(0.24284345, dtype=float32)), 'eval/episode_reward': (Array(-9911.18, dtype=float32), Array(5109.05, dtype=float32)), 'eval/avg_episode_length': (Array(891.3672, dtype=float32), Array(309.99182, dtype=float32)), 'eval/epoch_eval_time': 4.093326568603516, 'eval/sps': 31270.40998433424}
I0727 03:52:49.673832 140120985872192 train.py:379] starting iteration 427 4257.385580778122
I0727 03:52:59.571206 140120985872192 train.py:394] {'eval/walltime': 1761.4512767791748, 'training/sps': 42629.81435790359, 'training/walltime': 2494.5915446281433, 'training/entropy_loss': Array(-0.04913514, dtype=float32), 'training/policy_loss': Array(-0.0004531, dtype=float32), 'training/total_loss': Array(27.638237, dtype=float32), 'training/v_loss': Array(27.687824, dtype=float32), 'eval/episode_goal_distance': (Array(0.4622182, dtype=float32), Array(0.19119263, dtype=float32)), 'eval/episode_reward': (Array(-9415.699, dtype=float32), Array(4612.493, dtype=float32)), 'eval/avg_episode_length': (Array(891.14844, dtype=float32), Array(310.61575, dtype=float32)), 'eval/epoch_eval_time': 4.128913879394531, 'eval/sps': 31000.88879033972}
I0727 03:52:59.573357 140120985872192 train.py:379] starting iteration 428 4267.28510594368
I0727 03:53:09.471677 140120985872192 train.py:394] {'eval/walltime': 1765.535712480545, 'training/sps': 42296.26440635153, 'training/walltime': 2500.401986837387, 'training/entropy_loss': Array(-0.04875273, dtype=float32), 'training/policy_loss': Array(-0.0003942, dtype=float32), 'training/total_loss': Array(29.251823, dtype=float32), 'training/v_loss': Array(29.300972, dtype=float32), 'eval/episode_goal_distance': (Array(0.46304023, dtype=float32), Array(0.21247691, dtype=float32)), 'eval/episode_reward': (Array(-9026.332, dtype=float32), Array(4911.3755, dtype=float32)), 'eval/avg_episode_length': (Array(844.6172, dtype=float32), Array(361.07776, dtype=float32)), 'eval/epoch_eval_time': 4.084435701370239, 'eval/sps': 31338.47839912348}
I0727 03:53:09.473813 140120985872192 train.py:379] starting iteration 429 4277.185562133789
I0727 03:53:19.388182 140120985872192 train.py:394] {'eval/walltime': 1769.6419825553894, 'training/sps': 42338.7268950185, 'training/walltime': 2506.2066016197205, 'training/entropy_loss': Array(-0.04833149, dtype=float32), 'training/policy_loss': Array(-0.00048238, dtype=float32), 'training/total_loss': Array(27.741402, dtype=float32), 'training/v_loss': Array(27.790215, dtype=float32), 'eval/episode_goal_distance': (Array(0.46826246, dtype=float32), Array(0.2138326, dtype=float32)), 'eval/episode_reward': (Array(-9474.459, dtype=float32), Array(4901.3086, dtype=float32)), 'eval/avg_episode_length': (Array(883.4531, dtype=float32), Array(319.88568, dtype=float32)), 'eval/epoch_eval_time': 4.10627007484436, 'eval/sps': 31171.84151723181}
I0727 03:53:19.390308 140120985872192 train.py:379] starting iteration 430 4287.102056980133
I0727 03:53:29.254593 140120985872192 train.py:394] {'eval/walltime': 1773.7164838314056, 'training/sps': 42473.324012864716, 'training/walltime': 2511.9928216934204, 'training/entropy_loss': Array(-0.04834037, dtype=float32), 'training/policy_loss': Array(-0.00060798, dtype=float32), 'training/total_loss': Array(26.878405, dtype=float32), 'training/v_loss': Array(26.927353, dtype=float32), 'eval/episode_goal_distance': (Array(0.4696849, dtype=float32), Array(0.20437908, dtype=float32)), 'eval/episode_reward': (Array(-9704.178, dtype=float32), Array(4639.211, dtype=float32)), 'eval/avg_episode_length': (Array(906.8047, dtype=float32), Array(289.75647, dtype=float32)), 'eval/epoch_eval_time': 4.074501276016235, 'eval/sps': 31414.887695200214}
I0727 03:53:29.256829 140120985872192 train.py:379] starting iteration 431 4296.968577861786
I0727 03:53:39.161958 140120985872192 train.py:394] {'eval/walltime': 1777.7985756397247, 'training/sps': 42230.46180148213, 'training/walltime': 2517.812317609787, 'training/entropy_loss': Array(-0.04869278, dtype=float32), 'training/policy_loss': Array(-0.00024237, dtype=float32), 'training/total_loss': Array(29.438667, dtype=float32), 'training/v_loss': Array(29.487604, dtype=float32), 'eval/episode_goal_distance': (Array(0.4730757, dtype=float32), Array(0.18573958, dtype=float32)), 'eval/episode_reward': (Array(-9385.962, dtype=float32), Array(5032.9805, dtype=float32)), 'eval/avg_episode_length': (Array(852.41406, dtype=float32), Array(353.49423, dtype=float32)), 'eval/epoch_eval_time': 4.082091808319092, 'eval/sps': 31356.472615128994}
I0727 03:53:39.166687 140120985872192 train.py:379] starting iteration 432 4306.878420114517
I0727 03:53:49.062060 140120985872192 train.py:394] {'eval/walltime': 1781.9001860618591, 'training/sps': 42447.03463079302, 'training/walltime': 2523.6021213531494, 'training/entropy_loss': Array(-0.04860773, dtype=float32), 'training/policy_loss': Array(-0.00037429, dtype=float32), 'training/total_loss': Array(32.284035, dtype=float32), 'training/v_loss': Array(32.333015, dtype=float32), 'eval/episode_goal_distance': (Array(0.4524709, dtype=float32), Array(0.19545387, dtype=float32)), 'eval/episode_reward': (Array(-9021.531, dtype=float32), Array(5091.2705, dtype=float32)), 'eval/avg_episode_length': (Array(836.8672, dtype=float32), Array(368.23425, dtype=float32)), 'eval/epoch_eval_time': 4.101610422134399, 'eval/sps': 31207.2544260289}
I0727 03:53:49.064759 140120985872192 train.py:379] starting iteration 433 4316.776508569717
I0727 03:53:58.942789 140120985872192 train.py:394] {'eval/walltime': 1785.987066745758, 'training/sps': 42463.9018468354, 'training/walltime': 2529.389625310898, 'training/entropy_loss': Array(-0.04901502, dtype=float32), 'training/policy_loss': Array(-0.00040693, dtype=float32), 'training/total_loss': Array(52.421066, dtype=float32), 'training/v_loss': Array(52.470486, dtype=float32), 'eval/episode_goal_distance': (Array(0.475044, dtype=float32), Array(0.2138078, dtype=float32)), 'eval/episode_reward': (Array(-9212.5, dtype=float32), Array(5236.1865, dtype=float32)), 'eval/avg_episode_length': (Array(836.8594, dtype=float32), Array(368.25165, dtype=float32)), 'eval/epoch_eval_time': 4.086880683898926, 'eval/sps': 31319.730107189403}
I0727 03:53:58.944960 140120985872192 train.py:379] starting iteration 434 4326.656709194183
I0727 03:54:08.842935 140120985872192 train.py:394] {'eval/walltime': 1790.0732510089874, 'training/sps': 42313.057147814696, 'training/walltime': 2535.1977615356445, 'training/entropy_loss': Array(-0.04868716, dtype=float32), 'training/policy_loss': Array(-0.00072357, dtype=float32), 'training/total_loss': Array(32.018353, dtype=float32), 'training/v_loss': Array(32.067764, dtype=float32), 'eval/episode_goal_distance': (Array(0.43446702, dtype=float32), Array(0.19659255, dtype=float32)), 'eval/episode_reward': (Array(-8937.361, dtype=float32), Array(4630.659, dtype=float32)), 'eval/avg_episode_length': (Array(883.4844, dtype=float32), Array(319.8, dtype=float32)), 'eval/epoch_eval_time': 4.08618426322937, 'eval/sps': 31325.06802295787}
I0727 03:54:08.845827 140120985872192 train.py:379] starting iteration 435 4336.557561159134
I0727 03:54:18.723202 140120985872192 train.py:394] {'eval/walltime': 1794.1528100967407, 'training/sps': 42417.21147177857, 'training/walltime': 2540.9916360378265, 'training/entropy_loss': Array(-0.04812295, dtype=float32), 'training/policy_loss': Array(-0.00018348, dtype=float32), 'training/total_loss': Array(27.659405, dtype=float32), 'training/v_loss': Array(27.707712, dtype=float32), 'eval/episode_goal_distance': (Array(0.46590263, dtype=float32), Array(0.2043269, dtype=float32)), 'eval/episode_reward': (Array(-9440.105, dtype=float32), Array(4917.514, dtype=float32)), 'eval/avg_episode_length': (Array(883.47656, dtype=float32), Array(319.82144, dtype=float32)), 'eval/epoch_eval_time': 4.079559087753296, 'eval/sps': 31375.939714723547}
I0727 03:54:18.725330 140120985872192 train.py:379] starting iteration 436 4346.437078475952
I0727 03:54:28.591653 140120985872192 train.py:394] {'eval/walltime': 1798.251625776291, 'training/sps': 42637.90460155344, 'training/walltime': 2546.7555215358734, 'training/entropy_loss': Array(-0.04840485, dtype=float32), 'training/policy_loss': Array(-0.0004183, dtype=float32), 'training/total_loss': Array(28.209942, dtype=float32), 'training/v_loss': Array(28.258766, dtype=float32), 'eval/episode_goal_distance': (Array(0.44319114, dtype=float32), Array(0.18832532, dtype=float32)), 'eval/episode_reward': (Array(-9128.564, dtype=float32), Array(4485.109, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.30667, dtype=float32)), 'eval/epoch_eval_time': 4.098815679550171, 'eval/sps': 31228.53282684024}
I0727 03:54:28.593807 140120985872192 train.py:379] starting iteration 437 4356.305556058884
I0727 03:54:38.491895 140120985872192 train.py:394] {'eval/walltime': 1802.3414046764374, 'training/sps': 42338.067816834955, 'training/walltime': 2552.5602266788483, 'training/entropy_loss': Array(-0.04892932, dtype=float32), 'training/policy_loss': Array(-0.00021514, dtype=float32), 'training/total_loss': Array(28.860344, dtype=float32), 'training/v_loss': Array(28.909489, dtype=float32), 'eval/episode_goal_distance': (Array(0.4699024, dtype=float32), Array(0.20199713, dtype=float32)), 'eval/episode_reward': (Array(-9831.789, dtype=float32), Array(4853.221, dtype=float32)), 'eval/avg_episode_length': (Array(899.0156, dtype=float32), Array(300.35297, dtype=float32)), 'eval/epoch_eval_time': 4.089778900146484, 'eval/sps': 31297.53542310451}
I0727 03:54:38.494139 140120985872192 train.py:379] starting iteration 438 4366.205888271332
I0727 03:54:48.391234 140120985872192 train.py:394] {'eval/walltime': 1806.4225537776947, 'training/sps': 42298.31764177451, 'training/walltime': 2558.370386838913, 'training/entropy_loss': Array(-0.04840102, dtype=float32), 'training/policy_loss': Array(-0.00023645, dtype=float32), 'training/total_loss': Array(29.533604, dtype=float32), 'training/v_loss': Array(29.58224, dtype=float32), 'eval/episode_goal_distance': (Array(0.43765315, dtype=float32), Array(0.1791205, dtype=float32)), 'eval/episode_reward': (Array(-9188.047, dtype=float32), Array(4196.3916, dtype=float32)), 'eval/avg_episode_length': (Array(906.78125, dtype=float32), Array(289.82925, dtype=float32)), 'eval/epoch_eval_time': 4.081149101257324, 'eval/sps': 31363.715665415322}
I0727 03:54:48.393555 140120985872192 train.py:379] starting iteration 439 4376.105303764343
I0727 03:54:58.317991 140120985872192 train.py:394] {'eval/walltime': 1810.531576871872, 'training/sps': 42286.23366841778, 'training/walltime': 2564.1822073459625, 'training/entropy_loss': Array(-0.04860457, dtype=float32), 'training/policy_loss': Array(9.11154e-06, dtype=float32), 'training/total_loss': Array(26.906788, dtype=float32), 'training/v_loss': Array(26.955383, dtype=float32), 'eval/episode_goal_distance': (Array(0.45864967, dtype=float32), Array(0.20586812, dtype=float32)), 'eval/episode_reward': (Array(-9491.326, dtype=float32), Array(4473.7886, dtype=float32)), 'eval/avg_episode_length': (Array(899.0156, dtype=float32), Array(300.3529, dtype=float32)), 'eval/epoch_eval_time': 4.109023094177246, 'eval/sps': 31150.956581719962}
I0727 03:54:58.320320 140120985872192 train.py:379] starting iteration 440 4386.032068729401
I0727 03:55:08.210580 140120985872192 train.py:394] {'eval/walltime': 1814.6132469177246, 'training/sps': 42336.541060186035, 'training/walltime': 2569.98712182045, 'training/entropy_loss': Array(-0.04900192, dtype=float32), 'training/policy_loss': Array(-0.00023368, dtype=float32), 'training/total_loss': Array(32.261383, dtype=float32), 'training/v_loss': Array(32.31062, dtype=float32), 'eval/episode_goal_distance': (Array(0.46819362, dtype=float32), Array(0.20150435, dtype=float32)), 'eval/episode_reward': (Array(-9419.684, dtype=float32), Array(4733.563, dtype=float32)), 'eval/avg_episode_length': (Array(899., dtype=float32), Array(300.39954, dtype=float32)), 'eval/epoch_eval_time': 4.081670045852661, 'eval/sps': 31359.7127063368}
I0727 03:55:08.212748 140120985872192 train.py:379] starting iteration 441 4395.924496650696
I0727 03:55:18.095388 140120985872192 train.py:394] {'eval/walltime': 1818.7008337974548, 'training/sps': 42434.06023031315, 'training/walltime': 2575.778695821762, 'training/entropy_loss': Array(-0.04997089, dtype=float32), 'training/policy_loss': Array(-0.00015555, dtype=float32), 'training/total_loss': Array(47.792778, dtype=float32), 'training/v_loss': Array(47.842903, dtype=float32), 'eval/episode_goal_distance': (Array(0.48413667, dtype=float32), Array(0.20425783, dtype=float32)), 'eval/episode_reward': (Array(-9550.838, dtype=float32), Array(5120.1187, dtype=float32)), 'eval/avg_episode_length': (Array(867.9531, dtype=float32), Array(337.41614, dtype=float32)), 'eval/epoch_eval_time': 4.087586879730225, 'eval/sps': 31314.319124257447}
I0727 03:55:18.097544 140120985872192 train.py:379] starting iteration 442 4405.809293270111
I0727 03:55:27.997669 140120985872192 train.py:394] {'eval/walltime': 1822.806168794632, 'training/sps': 42435.744270081115, 'training/walltime': 2581.570039987564, 'training/entropy_loss': Array(-0.04977374, dtype=float32), 'training/policy_loss': Array(-0.00055333, dtype=float32), 'training/total_loss': Array(37.556816, dtype=float32), 'training/v_loss': Array(37.607143, dtype=float32), 'eval/episode_goal_distance': (Array(0.46554595, dtype=float32), Array(0.17835762, dtype=float32)), 'eval/episode_reward': (Array(-10004.174, dtype=float32), Array(4333.8364, dtype=float32)), 'eval/avg_episode_length': (Array(914.59375, dtype=float32), Array(278.53992, dtype=float32)), 'eval/epoch_eval_time': 4.105334997177124, 'eval/sps': 31178.941569449093}
I0727 03:55:28.000049 140120985872192 train.py:379] starting iteration 443 4415.711797952652
I0727 03:55:37.855393 140120985872192 train.py:394] {'eval/walltime': 1826.8799233436584, 'training/sps': 42535.09052186513, 'training/walltime': 2587.3478577136993, 'training/entropy_loss': Array(-0.04836385, dtype=float32), 'training/policy_loss': Array(-0.00074938, dtype=float32), 'training/total_loss': Array(28.817707, dtype=float32), 'training/v_loss': Array(28.866821, dtype=float32), 'eval/episode_goal_distance': (Array(0.45707244, dtype=float32), Array(0.2169379, dtype=float32)), 'eval/episode_reward': (Array(-9529.701, dtype=float32), Array(4784.38, dtype=float32)), 'eval/avg_episode_length': (Array(891.2656, dtype=float32), Array(310.28165, dtype=float32)), 'eval/epoch_eval_time': 4.073754549026489, 'eval/sps': 31420.646104117473}
I0727 03:55:37.857844 140120985872192 train.py:379] starting iteration 444 4425.569593429565
I0727 03:55:47.764516 140120985872192 train.py:394] {'eval/walltime': 1830.9712867736816, 'training/sps': 42287.85048004953, 'training/walltime': 2593.159456014633, 'training/entropy_loss': Array(-0.04833372, dtype=float32), 'training/policy_loss': Array(-0.00014096, dtype=float32), 'training/total_loss': Array(27.801453, dtype=float32), 'training/v_loss': Array(27.849928, dtype=float32), 'eval/episode_goal_distance': (Array(0.4359702, dtype=float32), Array(0.18274255, dtype=float32)), 'eval/episode_reward': (Array(-9446.057, dtype=float32), Array(4189.1787, dtype=float32)), 'eval/avg_episode_length': (Array(922.4297, dtype=float32), Array(266.46347, dtype=float32)), 'eval/epoch_eval_time': 4.091363430023193, 'eval/sps': 31285.414309741333}
I0727 03:55:47.766967 140120985872192 train.py:379] starting iteration 445 4435.478715896606
I0727 03:55:57.692202 140120985872192 train.py:394] {'eval/walltime': 1835.0817112922668, 'training/sps': 42291.24757504538, 'training/walltime': 2598.970587491989, 'training/entropy_loss': Array(-0.04864967, dtype=float32), 'training/policy_loss': Array(-0.00028064, dtype=float32), 'training/total_loss': Array(27.6143, dtype=float32), 'training/v_loss': Array(27.663229, dtype=float32), 'eval/episode_goal_distance': (Array(0.47128516, dtype=float32), Array(0.20418498, dtype=float32)), 'eval/episode_reward': (Array(-9927.854, dtype=float32), Array(4995.3887, dtype=float32)), 'eval/avg_episode_length': (Array(891.22656, dtype=float32), Array(310.3929, dtype=float32)), 'eval/epoch_eval_time': 4.110424518585205, 'eval/sps': 31140.335851260734}
I0727 03:55:57.694619 140120985872192 train.py:379] starting iteration 446 4445.406368494034
I0727 03:56:07.585892 140120985872192 train.py:394] {'eval/walltime': 1839.1771206855774, 'training/sps': 42429.63589554771, 'training/walltime': 2604.7627654075623, 'training/entropy_loss': Array(-0.04787474, dtype=float32), 'training/policy_loss': Array(-0.00010516, dtype=float32), 'training/total_loss': Array(28.983402, dtype=float32), 'training/v_loss': Array(29.03138, dtype=float32), 'eval/episode_goal_distance': (Array(0.44647968, dtype=float32), Array(0.18626516, dtype=float32)), 'eval/episode_reward': (Array(-9762.089, dtype=float32), Array(4455.166, dtype=float32)), 'eval/avg_episode_length': (Array(914.60156, dtype=float32), Array(278.51404, dtype=float32)), 'eval/epoch_eval_time': 4.095409393310547, 'eval/sps': 31254.506621261247}
I0727 03:56:07.588506 140120985872192 train.py:379] starting iteration 447 4455.300255060196
I0727 03:56:17.485074 140120985872192 train.py:394] {'eval/walltime': 1843.2579154968262, 'training/sps': 42283.409748968035, 'training/walltime': 2610.5749740600586, 'training/entropy_loss': Array(-0.04768356, dtype=float32), 'training/policy_loss': Array(-0.00010734, dtype=float32), 'training/total_loss': Array(29.524988, dtype=float32), 'training/v_loss': Array(29.572777, dtype=float32), 'eval/episode_goal_distance': (Array(0.4358083, dtype=float32), Array(0.19561051, dtype=float32)), 'eval/episode_reward': (Array(-9232.289, dtype=float32), Array(4602.4844, dtype=float32)), 'eval/avg_episode_length': (Array(891.21875, dtype=float32), Array(310.41504, dtype=float32)), 'eval/epoch_eval_time': 4.080794811248779, 'eval/sps': 31366.438627878535}
I0727 03:56:17.487484 140120985872192 train.py:379] starting iteration 448 4465.1992337703705
I0727 03:56:27.424733 140120985872192 train.py:394] {'eval/walltime': 1847.3784210681915, 'training/sps': 42277.279256721384, 'training/walltime': 2616.388025522232, 'training/entropy_loss': Array(-0.04809465, dtype=float32), 'training/policy_loss': Array(-0.00023172, dtype=float32), 'training/total_loss': Array(29.252897, dtype=float32), 'training/v_loss': Array(29.301224, dtype=float32), 'eval/episode_goal_distance': (Array(0.45546106, dtype=float32), Array(0.20850669, dtype=float32)), 'eval/episode_reward': (Array(-9136.671, dtype=float32), Array(4459.433, dtype=float32)), 'eval/avg_episode_length': (Array(891.27344, dtype=float32), Array(310.25916, dtype=float32)), 'eval/epoch_eval_time': 4.1205055713653564, 'eval/sps': 31064.14923680988}
I0727 03:56:27.427087 140120985872192 train.py:379] starting iteration 449 4475.138835668564
I0727 03:56:37.286106 140120985872192 train.py:394] {'eval/walltime': 1851.461211681366, 'training/sps': 42573.58500565752, 'training/walltime': 2622.160619020462, 'training/entropy_loss': Array(-0.04859932, dtype=float32), 'training/policy_loss': Array(-0.000196, dtype=float32), 'training/total_loss': Array(30.16311, dtype=float32), 'training/v_loss': Array(30.211906, dtype=float32), 'eval/episode_goal_distance': (Array(0.44733617, dtype=float32), Array(0.19386874, dtype=float32)), 'eval/episode_reward': (Array(-9507.538, dtype=float32), Array(4725.871, dtype=float32)), 'eval/avg_episode_length': (Array(891.2969, dtype=float32), Array(310.19272, dtype=float32)), 'eval/epoch_eval_time': 4.0827906131744385, 'eval/sps': 31351.10568417758}
I0727 03:56:37.288467 140120985872192 train.py:379] starting iteration 450 4485.000216245651
I0727 03:56:47.206107 140120985872192 train.py:394] {'eval/walltime': 1855.5578091144562, 'training/sps': 42245.49512339094, 'training/walltime': 2627.9780440330505, 'training/entropy_loss': Array(-0.04949383, dtype=float32), 'training/policy_loss': Array(-0.000204, dtype=float32), 'training/total_loss': Array(50.627216, dtype=float32), 'training/v_loss': Array(50.67691, dtype=float32), 'eval/episode_goal_distance': (Array(0.47776175, dtype=float32), Array(0.20617047, dtype=float32)), 'eval/episode_reward': (Array(-9993.891, dtype=float32), Array(5021.4688, dtype=float32)), 'eval/avg_episode_length': (Array(898.97656, dtype=float32), Array(300.46924, dtype=float32)), 'eval/epoch_eval_time': 4.09659743309021, 'eval/sps': 31245.442611978844}
I0727 03:56:47.208826 140120985872192 train.py:379] starting iteration 451 4494.920575141907
I0727 03:56:57.147509 140120985872192 train.py:394] {'eval/walltime': 1859.676103591919, 'training/sps': 42250.31234195207, 'training/walltime': 2633.794805765152, 'training/entropy_loss': Array(-0.04946797, dtype=float32), 'training/policy_loss': Array(-0.00019402, dtype=float32), 'training/total_loss': Array(31.894651, dtype=float32), 'training/v_loss': Array(31.944313, dtype=float32), 'eval/episode_goal_distance': (Array(0.47411352, dtype=float32), Array(0.21928385, dtype=float32)), 'eval/episode_reward': (Array(-9714.308, dtype=float32), Array(4577.6543, dtype=float32)), 'eval/avg_episode_length': (Array(898.9922, dtype=float32), Array(300.42288, dtype=float32)), 'eval/epoch_eval_time': 4.1182944774627686, 'eval/sps': 31080.82743972676}
I0727 03:56:57.149846 140120985872192 train.py:379] starting iteration 452 4504.861594438553
I0727 03:57:07.048948 140120985872192 train.py:394] {'eval/walltime': 1863.7647731304169, 'training/sps': 42322.96854937701, 'training/walltime': 2639.601581811905, 'training/entropy_loss': Array(-0.04948886, dtype=float32), 'training/policy_loss': Array(-0.00025375, dtype=float32), 'training/total_loss': Array(29.06213, dtype=float32), 'training/v_loss': Array(29.111874, dtype=float32), 'eval/episode_goal_distance': (Array(0.4507596, dtype=float32), Array(0.19247931, dtype=float32)), 'eval/episode_reward': (Array(-9400.6875, dtype=float32), Array(4528.248, dtype=float32)), 'eval/avg_episode_length': (Array(891.2422, dtype=float32), Array(310.34827, dtype=float32)), 'eval/epoch_eval_time': 4.088669538497925, 'eval/sps': 31306.0272528222}
I0727 03:57:07.051313 140120985872192 train.py:379] starting iteration 453 4514.763062000275
I0727 03:57:16.954272 140120985872192 train.py:394] {'eval/walltime': 1867.872370481491, 'training/sps': 42431.972835427034, 'training/walltime': 2645.393440723419, 'training/entropy_loss': Array(-0.04895875, dtype=float32), 'training/policy_loss': Array(-0.00031357, dtype=float32), 'training/total_loss': Array(29.248348, dtype=float32), 'training/v_loss': Array(29.297619, dtype=float32), 'eval/episode_goal_distance': (Array(0.46477243, dtype=float32), Array(0.20956713, dtype=float32)), 'eval/episode_reward': (Array(-9290.888, dtype=float32), Array(5301.093, dtype=float32)), 'eval/avg_episode_length': (Array(844.6328, dtype=float32), Array(361.0414, dtype=float32)), 'eval/epoch_eval_time': 4.107597351074219, 'eval/sps': 31161.76904888826}
I0727 03:57:16.956621 140120985872192 train.py:379] starting iteration 454 4524.668370246887
I0727 03:57:26.832253 140120985872192 train.py:394] {'eval/walltime': 1871.959729194641, 'training/sps': 42484.60811648488, 'training/walltime': 2651.1781239509583, 'training/entropy_loss': Array(-0.04807955, dtype=float32), 'training/policy_loss': Array(-0.0001708, dtype=float32), 'training/total_loss': Array(28.345263, dtype=float32), 'training/v_loss': Array(28.39351, dtype=float32), 'eval/episode_goal_distance': (Array(0.48480904, dtype=float32), Array(0.23609914, dtype=float32)), 'eval/episode_reward': (Array(-10236.9375, dtype=float32), Array(4879.7866, dtype=float32)), 'eval/avg_episode_length': (Array(914.5781, dtype=float32), Array(278.59055, dtype=float32)), 'eval/epoch_eval_time': 4.087358713150024, 'eval/sps': 31316.067167824775}
I0727 03:57:26.834886 140120985872192 train.py:379] starting iteration 455 4534.546634912491
I0727 03:57:36.759028 140120985872192 train.py:394] {'eval/walltime': 1876.0429286956787, 'training/sps': 42100.89780970739, 'training/walltime': 2657.015529155731, 'training/entropy_loss': Array(-0.04715747, dtype=float32), 'training/policy_loss': Array(8.449768e-05, dtype=float32), 'training/total_loss': Array(27.862858, dtype=float32), 'training/v_loss': Array(27.909931, dtype=float32), 'eval/episode_goal_distance': (Array(0.48638088, dtype=float32), Array(0.20994884, dtype=float32)), 'eval/episode_reward': (Array(-10121.252, dtype=float32), Array(4858.134, dtype=float32)), 'eval/avg_episode_length': (Array(899.0781, dtype=float32), Array(300.16733, dtype=float32)), 'eval/epoch_eval_time': 4.083199501037598, 'eval/sps': 31347.966213131007}
I0727 03:57:36.761567 140120985872192 train.py:379] starting iteration 456 4544.473315954208
I0727 03:57:46.685135 140120985872192 train.py:394] {'eval/walltime': 1880.159919500351, 'training/sps': 42350.86177305607, 'training/walltime': 2662.8184807300568, 'training/entropy_loss': Array(-0.04581738, dtype=float32), 'training/policy_loss': Array(-7.590213e-05, dtype=float32), 'training/total_loss': Array(27.45589, dtype=float32), 'training/v_loss': Array(27.501783, dtype=float32), 'eval/episode_goal_distance': (Array(0.45726186, dtype=float32), Array(0.18965836, dtype=float32)), 'eval/episode_reward': (Array(-9743.213, dtype=float32), Array(4075.255, dtype=float32)), 'eval/avg_episode_length': (Array(937.8281, dtype=float32), Array(240.79094, dtype=float32)), 'eval/epoch_eval_time': 4.116990804672241, 'eval/sps': 31090.669392493393}
I0727 03:57:46.687850 140120985872192 train.py:379] starting iteration 457 4554.399599313736
I0727 03:57:56.643972 140120985872192 train.py:394] {'eval/walltime': 1884.2554519176483, 'training/sps': 41959.903524052206, 'training/walltime': 2668.675500869751, 'training/entropy_loss': Array(-0.04503907, dtype=float32), 'training/policy_loss': Array(8.5508684e-05, dtype=float32), 'training/total_loss': Array(27.84921, dtype=float32), 'training/v_loss': Array(27.894165, dtype=float32), 'eval/episode_goal_distance': (Array(0.4378632, dtype=float32), Array(0.17978513, dtype=float32)), 'eval/episode_reward': (Array(-9521.088, dtype=float32), Array(4087.331, dtype=float32)), 'eval/avg_episode_length': (Array(922.3281, dtype=float32), Array(266.81204, dtype=float32)), 'eval/epoch_eval_time': 4.095532417297363, 'eval/sps': 31253.56778019768}
I0727 03:57:56.646484 140120985872192 train.py:379] starting iteration 458 4564.358232498169
I0727 03:58:06.554795 140120985872192 train.py:394] {'eval/walltime': 1888.337857246399, 'training/sps': 42210.114814597786, 'training/walltime': 2674.4978020191193, 'training/entropy_loss': Array(-0.0461892, dtype=float32), 'training/policy_loss': Array(4.9070277e-05, dtype=float32), 'training/total_loss': Array(44.492966, dtype=float32), 'training/v_loss': Array(44.53911, dtype=float32), 'eval/episode_goal_distance': (Array(0.46076912, dtype=float32), Array(0.21186295, dtype=float32)), 'eval/episode_reward': (Array(-9396.204, dtype=float32), Array(4647.6714, dtype=float32)), 'eval/avg_episode_length': (Array(891.35156, dtype=float32), Array(310.0365, dtype=float32)), 'eval/epoch_eval_time': 4.08240532875061, 'eval/sps': 31354.06450176603}
I0727 03:58:06.557166 140120985872192 train.py:379] starting iteration 459 4574.268915176392
I0727 03:58:16.490341 140120985872192 train.py:394] {'eval/walltime': 1892.4393339157104, 'training/sps': 42168.17543567261, 'training/walltime': 2680.3258938789368, 'training/entropy_loss': Array(-0.04583649, dtype=float32), 'training/policy_loss': Array(-7.866278e-05, dtype=float32), 'training/total_loss': Array(30.0083, dtype=float32), 'training/v_loss': Array(30.054214, dtype=float32), 'eval/episode_goal_distance': (Array(0.4711691, dtype=float32), Array(0.2014622, dtype=float32)), 'eval/episode_reward': (Array(-9563.111, dtype=float32), Array(4972.089, dtype=float32)), 'eval/avg_episode_length': (Array(867.9922, dtype=float32), Array(337.31638, dtype=float32)), 'eval/epoch_eval_time': 4.101476669311523, 'eval/sps': 31208.272122510003}
I0727 03:58:16.684300 140120985872192 train.py:379] starting iteration 460 4584.396043777466
I0727 03:58:26.578287 140120985872192 train.py:394] {'eval/walltime': 1896.5491547584534, 'training/sps': 42517.03028903634, 'training/walltime': 2686.1061658859253, 'training/entropy_loss': Array(-0.0460304, dtype=float32), 'training/policy_loss': Array(0.00015339, dtype=float32), 'training/total_loss': Array(29.090803, dtype=float32), 'training/v_loss': Array(29.13668, dtype=float32), 'eval/episode_goal_distance': (Array(0.4393729, dtype=float32), Array(0.19912465, dtype=float32)), 'eval/episode_reward': (Array(-8944.475, dtype=float32), Array(4562.968, dtype=float32)), 'eval/avg_episode_length': (Array(875.85156, dtype=float32), Array(328.46658, dtype=float32)), 'eval/epoch_eval_time': 4.10982084274292, 'eval/sps': 31144.909935921198}
I0727 03:58:26.581134 140120985872192 train.py:379] starting iteration 461 4594.292882680893
I0727 03:58:36.467530 140120985872192 train.py:394] {'eval/walltime': 1900.6763660907745, 'training/sps': 42698.93237523733, 'training/walltime': 2691.8618133068085, 'training/entropy_loss': Array(-0.04730685, dtype=float32), 'training/policy_loss': Array(-0.00024025, dtype=float32), 'training/total_loss': Array(25.866058, dtype=float32), 'training/v_loss': Array(25.913607, dtype=float32), 'eval/episode_goal_distance': (Array(0.46338707, dtype=float32), Array(0.21601781, dtype=float32)), 'eval/episode_reward': (Array(-9743.021, dtype=float32), Array(4514.7437, dtype=float32)), 'eval/avg_episode_length': (Array(922.3125, dtype=float32), Array(266.86548, dtype=float32)), 'eval/epoch_eval_time': 4.127211332321167, 'eval/sps': 31013.67720077762}
I0727 03:58:36.469949 140120985872192 train.py:379] starting iteration 462 4604.18169760704
I0727 03:58:46.362735 140120985872192 train.py:394] {'eval/walltime': 1904.76109957695, 'training/sps': 42340.99469809322, 'training/walltime': 2697.6661171913147, 'training/entropy_loss': Array(-0.04821711, dtype=float32), 'training/policy_loss': Array(-0.00038117, dtype=float32), 'training/total_loss': Array(27.17821, dtype=float32), 'training/v_loss': Array(27.226807, dtype=float32), 'eval/episode_goal_distance': (Array(0.4747495, dtype=float32), Array(0.19459322, dtype=float32)), 'eval/episode_reward': (Array(-9472.167, dtype=float32), Array(4829.944, dtype=float32)), 'eval/avg_episode_length': (Array(883.5625, dtype=float32), Array(319.58572, dtype=float32)), 'eval/epoch_eval_time': 4.084733486175537, 'eval/sps': 31336.193764710977}
I0727 03:58:46.365229 140120985872192 train.py:379] starting iteration 463 4614.076977968216
I0727 03:58:56.207370 140120985872192 train.py:394] {'eval/walltime': 1908.8442785739899, 'training/sps': 42701.67409153133, 'training/walltime': 2703.4213950634003, 'training/entropy_loss': Array(-0.04838798, dtype=float32), 'training/policy_loss': Array(-0.0001233, dtype=float32), 'training/total_loss': Array(26.351082, dtype=float32), 'training/v_loss': Array(26.399591, dtype=float32), 'eval/episode_goal_distance': (Array(0.45784774, dtype=float32), Array(0.21168847, dtype=float32)), 'eval/episode_reward': (Array(-9677.547, dtype=float32), Array(4662.518, dtype=float32)), 'eval/avg_episode_length': (Array(906.875, dtype=float32), Array(289.5379, dtype=float32)), 'eval/epoch_eval_time': 4.083178997039795, 'eval/sps': 31348.123629357633}
I0727 03:58:56.209789 140120985872192 train.py:379] starting iteration 464 4623.921537637711
I0727 03:59:06.155371 140120985872192 train.py:394] {'eval/walltime': 1912.966630935669, 'training/sps': 42230.013700687545, 'training/walltime': 2709.240952730179, 'training/entropy_loss': Array(-0.04823804, dtype=float32), 'training/policy_loss': Array(-9.445663e-05, dtype=float32), 'training/total_loss': Array(27.86679, dtype=float32), 'training/v_loss': Array(27.915123, dtype=float32), 'eval/episode_goal_distance': (Array(0.46552444, dtype=float32), Array(0.17637691, dtype=float32)), 'eval/episode_reward': (Array(-9789.478, dtype=float32), Array(4356.101, dtype=float32)), 'eval/avg_episode_length': (Array(906.8125, dtype=float32), Array(289.73224, dtype=float32)), 'eval/epoch_eval_time': 4.122352361679077, 'eval/sps': 31050.23267537088}
I0727 03:59:06.158019 140120985872192 train.py:379] starting iteration 465 4633.869768381119
I0727 03:59:16.060945 140120985872192 train.py:394] {'eval/walltime': 1917.0545768737793, 'training/sps': 42290.68540315294, 'training/walltime': 2715.0521614551544, 'training/entropy_loss': Array(-0.04869238, dtype=float32), 'training/policy_loss': Array(-7.239826e-05, dtype=float32), 'training/total_loss': Array(31.13573, dtype=float32), 'training/v_loss': Array(31.184496, dtype=float32), 'eval/episode_goal_distance': (Array(0.45668498, dtype=float32), Array(0.19394682, dtype=float32)), 'eval/episode_reward': (Array(-9643.821, dtype=float32), Array(4718.556, dtype=float32)), 'eval/avg_episode_length': (Array(891.27344, dtype=float32), Array(310.25937, dtype=float32)), 'eval/epoch_eval_time': 4.087945938110352, 'eval/sps': 31311.568679689502}
I0727 03:59:16.063499 140120985872192 train.py:379] starting iteration 466 4643.775249004364
I0727 03:59:26.005576 140120985872192 train.py:394] {'eval/walltime': 1921.1739854812622, 'training/sps': 42233.53647922858, 'training/walltime': 2720.871233701706, 'training/entropy_loss': Array(-0.04868536, dtype=float32), 'training/policy_loss': Array(-0.00019491, dtype=float32), 'training/total_loss': Array(45.17367, dtype=float32), 'training/v_loss': Array(45.22255, dtype=float32), 'eval/episode_goal_distance': (Array(0.44764817, dtype=float32), Array(0.18590245, dtype=float32)), 'eval/episode_reward': (Array(-9546.194, dtype=float32), Array(4804.3438, dtype=float32)), 'eval/avg_episode_length': (Array(875.72656, dtype=float32), Array(328.7971, dtype=float32)), 'eval/epoch_eval_time': 4.11940860748291, 'eval/sps': 31072.421358611493}
I0727 03:59:26.008073 140120985872192 train.py:379] starting iteration 467 4653.719821929932
I0727 03:59:35.909394 140120985872192 train.py:394] {'eval/walltime': 1925.2549831867218, 'training/sps': 42251.23712424938, 'training/walltime': 2726.687868118286, 'training/entropy_loss': Array(-0.04886317, dtype=float32), 'training/policy_loss': Array(-0.00041611, dtype=float32), 'training/total_loss': Array(35.6108, dtype=float32), 'training/v_loss': Array(35.660084, dtype=float32), 'eval/episode_goal_distance': (Array(0.50461406, dtype=float32), Array(0.20933934, dtype=float32)), 'eval/episode_reward': (Array(-10227.085, dtype=float32), Array(4679.668, dtype=float32)), 'eval/avg_episode_length': (Array(914.6797, dtype=float32), Array(278.25946, dtype=float32)), 'eval/epoch_eval_time': 4.080997705459595, 'eval/sps': 31364.879188429946}
I0727 03:59:35.911806 140120985872192 train.py:379] starting iteration 468 4663.6235547065735
I0727 03:59:45.804723 140120985872192 train.py:394] {'eval/walltime': 1929.3552277088165, 'training/sps': 42452.96093494751, 'training/walltime': 2732.4768636226654, 'training/entropy_loss': Array(-0.04861883, dtype=float32), 'training/policy_loss': Array(-0.00027602, dtype=float32), 'training/total_loss': Array(29.299751, dtype=float32), 'training/v_loss': Array(29.348648, dtype=float32), 'eval/episode_goal_distance': (Array(0.43841425, dtype=float32), Array(0.2010893, dtype=float32)), 'eval/episode_reward': (Array(-8772.666, dtype=float32), Array(4998.158, dtype=float32)), 'eval/avg_episode_length': (Array(836.9219, dtype=float32), Array(368.1112, dtype=float32)), 'eval/epoch_eval_time': 4.100244522094727, 'eval/sps': 31217.65038895962}
I0727 03:59:45.807216 140120985872192 train.py:379] starting iteration 469 4673.518965244293
I0727 03:59:55.701514 140120985872192 train.py:394] {'eval/walltime': 1933.4319825172424, 'training/sps': 42271.408830912296, 'training/walltime': 2738.2907223701477, 'training/entropy_loss': Array(-0.04849448, dtype=float32), 'training/policy_loss': Array(-0.00036613, dtype=float32), 'training/total_loss': Array(29.653046, dtype=float32), 'training/v_loss': Array(29.701908, dtype=float32), 'eval/episode_goal_distance': (Array(0.445512, dtype=float32), Array(0.18717492, dtype=float32)), 'eval/episode_reward': (Array(-8961.951, dtype=float32), Array(4647.117, dtype=float32)), 'eval/avg_episode_length': (Array(867.9297, dtype=float32), Array(337.47632, dtype=float32)), 'eval/epoch_eval_time': 4.076754808425903, 'eval/sps': 31397.5222977471}
I0727 03:59:55.704167 140120985872192 train.py:379] starting iteration 470 4683.415915966034
I0727 04:00:05.578889 140120985872192 train.py:394] {'eval/walltime': 1937.5070638656616, 'training/sps': 42402.26850737572, 'training/walltime': 2744.086638689041, 'training/entropy_loss': Array(-0.04795362, dtype=float32), 'training/policy_loss': Array(-0.0001499, dtype=float32), 'training/total_loss': Array(26.976753, dtype=float32), 'training/v_loss': Array(27.024858, dtype=float32), 'eval/episode_goal_distance': (Array(0.4762575, dtype=float32), Array(0.18545729, dtype=float32)), 'eval/episode_reward': (Array(-10046.784, dtype=float32), Array(4511.652, dtype=float32)), 'eval/avg_episode_length': (Array(914.5781, dtype=float32), Array(278.59045, dtype=float32)), 'eval/epoch_eval_time': 4.0750813484191895, 'eval/sps': 31410.415904863818}
I0727 04:00:05.581665 140120985872192 train.py:379] starting iteration 471 4693.293413877487
I0727 04:00:15.488401 140120985872192 train.py:394] {'eval/walltime': 1941.6032490730286, 'training/sps': 42322.15183421612, 'training/walltime': 2749.8935267925262, 'training/entropy_loss': Array(-0.04672012, dtype=float32), 'training/policy_loss': Array(-2.0397354e-05, dtype=float32), 'training/total_loss': Array(26.696726, dtype=float32), 'training/v_loss': Array(26.743465, dtype=float32), 'eval/episode_goal_distance': (Array(0.434376, dtype=float32), Array(0.17534725, dtype=float32)), 'eval/episode_reward': (Array(-9312.852, dtype=float32), Array(4178.7095, dtype=float32)), 'eval/avg_episode_length': (Array(906.8672, dtype=float32), Array(289.56256, dtype=float32)), 'eval/epoch_eval_time': 4.096185207366943, 'eval/sps': 31248.58704381663}
I0727 04:00:15.491104 140120985872192 train.py:379] starting iteration 472 4703.202853441238
I0727 04:00:25.402676 140120985872192 train.py:394] {'eval/walltime': 1945.7057256698608, 'training/sps': 42332.670756008614, 'training/walltime': 2755.6989719867706, 'training/entropy_loss': Array(-0.04603124, dtype=float32), 'training/policy_loss': Array(-0.00030499, dtype=float32), 'training/total_loss': Array(25.595499, dtype=float32), 'training/v_loss': Array(25.641834, dtype=float32), 'eval/episode_goal_distance': (Array(0.4518995, dtype=float32), Array(0.19508918, dtype=float32)), 'eval/episode_reward': (Array(-8956.07, dtype=float32), Array(4832.1733, dtype=float32)), 'eval/avg_episode_length': (Array(852.4375, dtype=float32), Array(353.4378, dtype=float32)), 'eval/epoch_eval_time': 4.102476596832275, 'eval/sps': 31200.66549528524}
I0727 04:00:25.405339 140120985872192 train.py:379] starting iteration 473 4713.11708855629
I0727 04:00:35.313031 140120985872192 train.py:394] {'eval/walltime': 1949.7974286079407, 'training/sps': 42282.61537277651, 'training/walltime': 2761.511289834976, 'training/entropy_loss': Array(-0.04637954, dtype=float32), 'training/policy_loss': Array(0.00023763, dtype=float32), 'training/total_loss': Array(27.03532, dtype=float32), 'training/v_loss': Array(27.081463, dtype=float32), 'eval/episode_goal_distance': (Array(0.45761734, dtype=float32), Array(0.20324434, dtype=float32)), 'eval/episode_reward': (Array(-9432.418, dtype=float32), Array(4788.131, dtype=float32)), 'eval/avg_episode_length': (Array(891.21094, dtype=float32), Array(310.4375, dtype=float32)), 'eval/epoch_eval_time': 4.091702938079834, 'eval/sps': 31282.81841009411}
I0727 04:00:35.315617 140120985872192 train.py:379] starting iteration 474 4723.027366638184
I0727 04:00:45.208989 140120985872192 train.py:394] {'eval/walltime': 1953.885088443756, 'training/sps': 42356.68989070263, 'training/walltime': 2767.3134429454803, 'training/entropy_loss': Array(-0.04719179, dtype=float32), 'training/policy_loss': Array(0.00016359, dtype=float32), 'training/total_loss': Array(27.993893, dtype=float32), 'training/v_loss': Array(28.04092, dtype=float32), 'eval/episode_goal_distance': (Array(0.46849117, dtype=float32), Array(0.2008183, dtype=float32)), 'eval/episode_reward': (Array(-10206.113, dtype=float32), Array(4649.985, dtype=float32)), 'eval/avg_episode_length': (Array(922.3203, dtype=float32), Array(266.83896, dtype=float32)), 'eval/epoch_eval_time': 4.08765983581543, 'eval/sps': 31313.760229871434}
I0727 04:00:45.211731 140120985872192 train.py:379] starting iteration 475 4732.923479795456
I0727 04:00:55.095272 140120985872192 train.py:394] {'eval/walltime': 1957.9668109416962, 'training/sps': 42386.23102489314, 'training/walltime': 2773.1115522384644, 'training/entropy_loss': Array(-0.04747504, dtype=float32), 'training/policy_loss': Array(5.7667014e-05, dtype=float32), 'training/total_loss': Array(45.643745, dtype=float32), 'training/v_loss': Array(45.69116, dtype=float32), 'eval/episode_goal_distance': (Array(0.4806899, dtype=float32), Array(0.21800506, dtype=float32)), 'eval/episode_reward': (Array(-9236.955, dtype=float32), Array(4885.486, dtype=float32)), 'eval/avg_episode_length': (Array(860.28906, dtype=float32), Array(345.3754, dtype=float32)), 'eval/epoch_eval_time': 4.0817224979400635, 'eval/sps': 31359.309719021367}
I0727 04:00:55.097678 140120985872192 train.py:379] starting iteration 476 4742.809426546097
I0727 04:01:05.002130 140120985872192 train.py:394] {'eval/walltime': 1962.049388885498, 'training/sps': 42238.7732862065, 'training/walltime': 2778.9299030303955, 'training/entropy_loss': Array(-0.04812242, dtype=float32), 'training/policy_loss': Array(-0.00016758, dtype=float32), 'training/total_loss': Array(28.437363, dtype=float32), 'training/v_loss': Array(28.485655, dtype=float32), 'eval/episode_goal_distance': (Array(0.45202672, dtype=float32), Array(0.20724681, dtype=float32)), 'eval/episode_reward': (Array(-9505.3125, dtype=float32), Array(5025.398, dtype=float32)), 'eval/avg_episode_length': (Array(867.9844, dtype=float32), Array(337.33646, dtype=float32)), 'eval/epoch_eval_time': 4.08257794380188, 'eval/sps': 31352.738823842432}
I0727 04:01:05.004528 140120985872192 train.py:379] starting iteration 477 4752.716276884079
I0727 04:01:14.898906 140120985872192 train.py:394] {'eval/walltime': 1966.1467082500458, 'training/sps': 42420.62938322577, 'training/walltime': 2784.7233107089996, 'training/entropy_loss': Array(-0.04854263, dtype=float32), 'training/policy_loss': Array(0.00012056, dtype=float32), 'training/total_loss': Array(30.106125, dtype=float32), 'training/v_loss': Array(30.154549, dtype=float32), 'eval/episode_goal_distance': (Array(0.45465067, dtype=float32), Array(0.21648586, dtype=float32)), 'eval/episode_reward': (Array(-9577.814, dtype=float32), Array(4453.659, dtype=float32)), 'eval/avg_episode_length': (Array(937.8594, dtype=float32), Array(240.67014, dtype=float32)), 'eval/epoch_eval_time': 4.0973193645477295, 'eval/sps': 31239.937288639667}
I0727 04:01:14.901575 140120985872192 train.py:379] starting iteration 478 4762.6133234500885
I0727 04:01:24.799871 140120985872192 train.py:394] {'eval/walltime': 1970.2227540016174, 'training/sps': 42237.46655878029, 'training/walltime': 2790.541841506958, 'training/entropy_loss': Array(-0.04881418, dtype=float32), 'training/policy_loss': Array(7.067267e-05, dtype=float32), 'training/total_loss': Array(28.68843, dtype=float32), 'training/v_loss': Array(28.737175, dtype=float32), 'eval/episode_goal_distance': (Array(0.43747675, dtype=float32), Array(0.19695193, dtype=float32)), 'eval/episode_reward': (Array(-9312.676, dtype=float32), Array(4618.171, dtype=float32)), 'eval/avg_episode_length': (Array(898.91406, dtype=float32), Array(300.6552, dtype=float32)), 'eval/epoch_eval_time': 4.076045751571655, 'eval/sps': 31402.98411779243}
I0727 04:01:24.802248 140120985872192 train.py:379] starting iteration 479 4772.513996839523
I0727 04:01:34.705343 140120985872192 train.py:394] {'eval/walltime': 1974.315062046051, 'training/sps': 42320.53239865063, 'training/walltime': 2796.348951816559, 'training/entropy_loss': Array(-0.04907988, dtype=float32), 'training/policy_loss': Array(2.5785565e-05, dtype=float32), 'training/total_loss': Array(28.494461, dtype=float32), 'training/v_loss': Array(28.543516, dtype=float32), 'eval/episode_goal_distance': (Array(0.5013694, dtype=float32), Array(0.23904367, dtype=float32)), 'eval/episode_reward': (Array(-10528.091, dtype=float32), Array(4512.35, dtype=float32)), 'eval/avg_episode_length': (Array(937.8828, dtype=float32), Array(240.57924, dtype=float32)), 'eval/epoch_eval_time': 4.092308044433594, 'eval/sps': 31278.19279736459}
I0727 04:01:34.707757 140120985872192 train.py:379] starting iteration 480 4782.419506549835
I0727 04:01:44.642782 140120985872192 train.py:394] {'eval/walltime': 1978.3975069522858, 'training/sps': 42017.8226971343, 'training/walltime': 2802.197898387909, 'training/entropy_loss': Array(-0.04948128, dtype=float32), 'training/policy_loss': Array(8.282611e-05, dtype=float32), 'training/total_loss': Array(28.190796, dtype=float32), 'training/v_loss': Array(28.240194, dtype=float32), 'eval/episode_goal_distance': (Array(0.45636103, dtype=float32), Array(0.22176391, dtype=float32)), 'eval/episode_reward': (Array(-9808.585, dtype=float32), Array(4782.766, dtype=float32)), 'eval/avg_episode_length': (Array(891.22656, dtype=float32), Array(310.3928, dtype=float32)), 'eval/epoch_eval_time': 4.082444906234741, 'eval/sps': 31353.76053808281}
I0727 04:01:44.645610 140120985872192 train.py:379] starting iteration 481 4792.357358932495
I0727 04:01:54.520014 140120985872192 train.py:394] {'eval/walltime': 1982.4770214557648, 'training/sps': 42436.47976921756, 'training/walltime': 2807.989142179489, 'training/entropy_loss': Array(-0.04963165, dtype=float32), 'training/policy_loss': Array(5.7026555e-06, dtype=float32), 'training/total_loss': Array(31.005772, dtype=float32), 'training/v_loss': Array(31.055399, dtype=float32), 'eval/episode_goal_distance': (Array(0.43905604, dtype=float32), Array(0.17792355, dtype=float32)), 'eval/episode_reward': (Array(-9042.589, dtype=float32), Array(4753.6606, dtype=float32)), 'eval/avg_episode_length': (Array(867.9531, dtype=float32), Array(337.41663, dtype=float32)), 'eval/epoch_eval_time': 4.079514503479004, 'eval/sps': 31376.28261667946}
I0727 04:01:54.522674 140120985872192 train.py:379] starting iteration 482 4802.234423398972
I0727 04:02:04.421214 140120985872192 train.py:394] {'eval/walltime': 1986.5578165054321, 'training/sps': 42269.22994127698, 'training/walltime': 2813.8033006191254, 'training/entropy_loss': Array(-0.04963033, dtype=float32), 'training/policy_loss': Array(0.00030571, dtype=float32), 'training/total_loss': Array(33.290886, dtype=float32), 'training/v_loss': Array(33.340214, dtype=float32), 'eval/episode_goal_distance': (Array(0.43136472, dtype=float32), Array(0.18547212, dtype=float32)), 'eval/episode_reward': (Array(-8846.129, dtype=float32), Array(4727.903, dtype=float32)), 'eval/avg_episode_length': (Array(867.9375, dtype=float32), Array(337.45605, dtype=float32)), 'eval/epoch_eval_time': 4.080795049667358, 'eval/sps': 31366.436795308742}
I0727 04:02:04.423622 140120985872192 train.py:379] starting iteration 483 4812.135370254517
I0727 04:02:14.341492 140120985872192 train.py:394] {'eval/walltime': 1990.6748042106628, 'training/sps': 42392.487303131296, 'training/walltime': 2819.600554227829, 'training/entropy_loss': Array(-0.04996525, dtype=float32), 'training/policy_loss': Array(0.00066212, dtype=float32), 'training/total_loss': Array(48.588234, dtype=float32), 'training/v_loss': Array(48.63754, dtype=float32), 'eval/episode_goal_distance': (Array(0.41869658, dtype=float32), Array(0.18916, dtype=float32)), 'eval/episode_reward': (Array(-8845.631, dtype=float32), Array(4791.1743, dtype=float32)), 'eval/avg_episode_length': (Array(860.1094, dtype=float32), Array(345.8195, dtype=float32)), 'eval/epoch_eval_time': 4.116987705230713, 'eval/sps': 31090.692798857162}
I0727 04:02:14.344087 140120985872192 train.py:379] starting iteration 484 4822.055835723877
I0727 04:02:24.240206 140120985872192 train.py:394] {'eval/walltime': 1994.7467246055603, 'training/sps': 42222.402630293844, 'training/walltime': 2825.4211609363556, 'training/entropy_loss': Array(-0.04959983, dtype=float32), 'training/policy_loss': Array(0.00077409, dtype=float32), 'training/total_loss': Array(34.315155, dtype=float32), 'training/v_loss': Array(34.363983, dtype=float32), 'eval/episode_goal_distance': (Array(0.42363423, dtype=float32), Array(0.17233743, dtype=float32)), 'eval/episode_reward': (Array(-8685.746, dtype=float32), Array(5199.8667, dtype=float32)), 'eval/avg_episode_length': (Array(806.0547, dtype=float32), Array(393.6667, dtype=float32)), 'eval/epoch_eval_time': 4.071920394897461, 'eval/sps': 31434.799206879707}
I0727 04:02:24.242575 140120985872192 train.py:379] starting iteration 485 4831.954323530197
I0727 04:02:34.099395 140120985872192 train.py:394] {'eval/walltime': 1998.8329000473022, 'training/sps': 42615.399541637045, 'training/walltime': 2831.188090324402, 'training/entropy_loss': Array(-0.04897444, dtype=float32), 'training/policy_loss': Array(0.00126894, dtype=float32), 'training/total_loss': Array(28.955376, dtype=float32), 'training/v_loss': Array(29.003082, dtype=float32), 'eval/episode_goal_distance': (Array(0.43210655, dtype=float32), Array(0.17491336, dtype=float32)), 'eval/episode_reward': (Array(-8964.242, dtype=float32), Array(4263.8057, dtype=float32)), 'eval/avg_episode_length': (Array(891.25, dtype=float32), Array(310.32614, dtype=float32)), 'eval/epoch_eval_time': 4.086175441741943, 'eval/sps': 31325.13564944568}
I0727 04:02:34.101837 140120985872192 train.py:379] starting iteration 486 4841.813585758209
I0727 04:02:44.035144 140120985872192 train.py:394] {'eval/walltime': 2002.9415867328644, 'training/sps': 42218.97162681651, 'training/walltime': 2837.0091700553894, 'training/entropy_loss': Array(-0.04892749, dtype=float32), 'training/policy_loss': Array(0.00170508, dtype=float32), 'training/total_loss': Array(26.54571, dtype=float32), 'training/v_loss': Array(26.592932, dtype=float32), 'eval/episode_goal_distance': (Array(0.4130972, dtype=float32), Array(0.18049324, dtype=float32)), 'eval/episode_reward': (Array(-9709.427, dtype=float32), Array(4105.0444, dtype=float32)), 'eval/avg_episode_length': (Array(930.0547, dtype=float32), Array(254.338, dtype=float32)), 'eval/epoch_eval_time': 4.108686685562134, 'eval/sps': 31153.507141294118}
I0727 04:02:44.037537 140120985872192 train.py:379] starting iteration 487 4851.749286174774
I0727 04:02:53.960773 140120985872192 train.py:394] {'eval/walltime': 2007.0299932956696, 'training/sps': 42146.180611906035, 'training/walltime': 2842.8403034210205, 'training/entropy_loss': Array(-0.0477566, dtype=float32), 'training/policy_loss': Array(0.00338749, dtype=float32), 'training/total_loss': Array(26.11673, dtype=float32), 'training/v_loss': Array(26.1611, dtype=float32), 'eval/episode_goal_distance': (Array(0.40258217, dtype=float32), Array(0.16287327, dtype=float32)), 'eval/episode_reward': (Array(-8851.211, dtype=float32), Array(4121.0884, dtype=float32)), 'eval/avg_episode_length': (Array(906.78906, dtype=float32), Array(289.80502, dtype=float32)), 'eval/epoch_eval_time': 4.088406562805176, 'eval/sps': 31308.040928340415}
I0727 04:02:53.963172 140120985872192 train.py:379] starting iteration 488 4861.674920558929
I0727 04:03:03.833056 140120985872192 train.py:394] {'eval/walltime': 2011.1174755096436, 'training/sps': 42527.9954808091, 'training/walltime': 2848.619085073471, 'training/entropy_loss': Array(-0.04696539, dtype=float32), 'training/policy_loss': Array(0.00425108, dtype=float32), 'training/total_loss': Array(27.097876, dtype=float32), 'training/v_loss': Array(27.14059, dtype=float32), 'eval/episode_goal_distance': (Array(0.3929326, dtype=float32), Array(0.1699545, dtype=float32)), 'eval/episode_reward': (Array(-8031.9526, dtype=float32), Array(4213.547, dtype=float32)), 'eval/avg_episode_length': (Array(867.97656, dtype=float32), Array(337.35638, dtype=float32)), 'eval/epoch_eval_time': 4.087482213973999, 'eval/sps': 31315.120971634453}
I0727 04:03:03.835413 140120985872192 train.py:379] starting iteration 489 4871.547161817551
I0727 04:03:13.730241 140120985872192 train.py:394] {'eval/walltime': 2015.2239680290222, 'training/sps': 42484.07581181966, 'training/walltime': 2854.403840780258, 'training/entropy_loss': Array(-0.04529411, dtype=float32), 'training/policy_loss': Array(0.00519328, dtype=float32), 'training/total_loss': Array(24.774502, dtype=float32), 'training/v_loss': Array(24.814602, dtype=float32), 'eval/episode_goal_distance': (Array(0.3553263, dtype=float32), Array(0.13937818, dtype=float32)), 'eval/episode_reward': (Array(-7717.9272, dtype=float32), Array(3848.6006, dtype=float32)), 'eval/avg_episode_length': (Array(875.7031, dtype=float32), Array(328.85916, dtype=float32)), 'eval/epoch_eval_time': 4.106492519378662, 'eval/sps': 31170.152970196374}
I0727 04:03:13.732639 140120985872192 train.py:379] starting iteration 490 4881.444388628006
I0727 04:03:23.616816 140120985872192 train.py:394] {'eval/walltime': 2019.3025588989258, 'training/sps': 42357.13894332187, 'training/walltime': 2860.205932378769, 'training/entropy_loss': Array(-0.04390831, dtype=float32), 'training/policy_loss': Array(0.00517639, dtype=float32), 'training/total_loss': Array(27.362347, dtype=float32), 'training/v_loss': Array(27.401077, dtype=float32), 'eval/episode_goal_distance': (Array(0.36417228, dtype=float32), Array(0.1512007, dtype=float32)), 'eval/episode_reward': (Array(-8439.78, dtype=float32), Array(3402.7239, dtype=float32)), 'eval/avg_episode_length': (Array(937.8906, dtype=float32), Array(240.54889, dtype=float32)), 'eval/epoch_eval_time': 4.0785908699035645, 'eval/sps': 31383.388058000157}
I0727 04:03:23.619641 140120985872192 train.py:379] starting iteration 491 4891.331390142441
I0727 04:03:33.534507 140120985872192 train.py:394] {'eval/walltime': 2023.3949909210205, 'training/sps': 42259.44764087133, 'training/walltime': 2866.021436691284, 'training/entropy_loss': Array(-0.04280491, dtype=float32), 'training/policy_loss': Array(0.0065778, dtype=float32), 'training/total_loss': Array(44.114788, dtype=float32), 'training/v_loss': Array(44.151012, dtype=float32), 'eval/episode_goal_distance': (Array(0.3640418, dtype=float32), Array(0.14360715, dtype=float32)), 'eval/episode_reward': (Array(-8203.961, dtype=float32), Array(3828.3896, dtype=float32)), 'eval/avg_episode_length': (Array(891.2344, dtype=float32), Array(310.3705, dtype=float32)), 'eval/epoch_eval_time': 4.092432022094727, 'eval/sps': 31277.245244133028}
I0727 04:03:33.537043 140120985872192 train.py:379] starting iteration 492 4901.248791694641
I0727 04:03:43.451410 140120985872192 train.py:394] {'eval/walltime': 2027.4824891090393, 'training/sps': 42206.74629449543, 'training/walltime': 2871.844202518463, 'training/entropy_loss': Array(-0.04126957, dtype=float32), 'training/policy_loss': Array(0.00494071, dtype=float32), 'training/total_loss': Array(30.53042, dtype=float32), 'training/v_loss': Array(30.566748, dtype=float32), 'eval/episode_goal_distance': (Array(0.3724137, dtype=float32), Array(0.14668684, dtype=float32)), 'eval/episode_reward': (Array(-7476.124, dtype=float32), Array(4543.225, dtype=float32)), 'eval/avg_episode_length': (Array(813.6094, dtype=float32), Array(388.0037, dtype=float32)), 'eval/epoch_eval_time': 4.087498188018799, 'eval/sps': 31314.998591361164}
I0727 04:03:43.453882 140120985872192 train.py:379] starting iteration 493 4911.1656312942505
I0727 04:03:53.366770 140120985872192 train.py:394] {'eval/walltime': 2031.57590675354, 'training/sps': 42259.50827891717, 'training/walltime': 2877.659698486328, 'training/entropy_loss': Array(-0.04000466, dtype=float32), 'training/policy_loss': Array(0.00488585, dtype=float32), 'training/total_loss': Array(21.726295, dtype=float32), 'training/v_loss': Array(21.761414, dtype=float32), 'eval/episode_goal_distance': (Array(0.38032678, dtype=float32), Array(0.15920827, dtype=float32)), 'eval/episode_reward': (Array(-8476.882, dtype=float32), Array(4250.0415, dtype=float32)), 'eval/avg_episode_length': (Array(891.28906, dtype=float32), Array(310.21487, dtype=float32)), 'eval/epoch_eval_time': 4.093417644500732, 'eval/sps': 31269.71423792061}
I0727 04:03:53.369259 140120985872192 train.py:379] starting iteration 494 4921.0810079574585
I0727 04:04:03.304364 140120985872192 train.py:394] {'eval/walltime': 2035.6948199272156, 'training/sps': 42283.31955628265, 'training/walltime': 2883.4719195365906, 'training/entropy_loss': Array(-0.03917818, dtype=float32), 'training/policy_loss': Array(0.00503533, dtype=float32), 'training/total_loss': Array(22.249084, dtype=float32), 'training/v_loss': Array(22.283226, dtype=float32), 'eval/episode_goal_distance': (Array(0.37120903, dtype=float32), Array(0.15232798, dtype=float32)), 'eval/episode_reward': (Array(-8547.443, dtype=float32), Array(4240.337, dtype=float32)), 'eval/avg_episode_length': (Array(899.1172, dtype=float32), Array(300.0508, dtype=float32)), 'eval/epoch_eval_time': 4.118913173675537, 'eval/sps': 31076.15883191304}
I0727 04:04:03.306786 140120985872192 train.py:379] starting iteration 495 4931.018535375595
I0727 04:04:13.185773 140120985872192 train.py:394] {'eval/walltime': 2039.7865579128265, 'training/sps': 42494.3583049503, 'training/walltime': 2889.2552754879, 'training/entropy_loss': Array(-0.03646848, dtype=float32), 'training/policy_loss': Array(0.00670366, dtype=float32), 'training/total_loss': Array(18.769608, dtype=float32), 'training/v_loss': Array(18.799374, dtype=float32), 'eval/episode_goal_distance': (Array(0.34149063, dtype=float32), Array(0.12360527, dtype=float32)), 'eval/episode_reward': (Array(-6904.3643, dtype=float32), Array(3840.1948, dtype=float32)), 'eval/avg_episode_length': (Array(821.5, dtype=float32), Array(381.391, dtype=float32)), 'eval/epoch_eval_time': 4.091737985610962, 'eval/sps': 31282.550459028855}
I0727 04:04:13.188204 140120985872192 train.py:379] starting iteration 496 4940.899952888489
I0727 04:04:23.102009 140120985872192 train.py:394] {'eval/walltime': 2043.886778831482, 'training/sps': 42303.711154561526, 'training/walltime': 2895.064694881439, 'training/entropy_loss': Array(-0.03478178, dtype=float32), 'training/policy_loss': Array(0.00691243, dtype=float32), 'training/total_loss': Array(19.188953, dtype=float32), 'training/v_loss': Array(19.216824, dtype=float32), 'eval/episode_goal_distance': (Array(0.35358852, dtype=float32), Array(0.14417183, dtype=float32)), 'eval/episode_reward': (Array(-7637.3154, dtype=float32), Array(4300.517, dtype=float32)), 'eval/avg_episode_length': (Array(844.8281, dtype=float32), Array(360.5876, dtype=float32)), 'eval/epoch_eval_time': 4.1002209186553955, 'eval/sps': 31217.83009730501}
I0727 04:04:23.104553 140120985872192 train.py:379] starting iteration 497 4950.816302537918
I0727 04:04:32.989179 140120985872192 train.py:394] {'eval/walltime': 2047.9616434574127, 'training/sps': 42329.31219764474, 'training/walltime': 2900.8706007003784, 'training/entropy_loss': Array(-0.03224993, dtype=float32), 'training/policy_loss': Array(0.00721185, dtype=float32), 'training/total_loss': Array(17.33736, dtype=float32), 'training/v_loss': Array(17.362396, dtype=float32), 'eval/episode_goal_distance': (Array(0.35368523, dtype=float32), Array(0.13092002, dtype=float32)), 'eval/episode_reward': (Array(-7417.366, dtype=float32), Array(4231.991, dtype=float32)), 'eval/avg_episode_length': (Array(829.0703, dtype=float32), Array(375.1976, dtype=float32)), 'eval/epoch_eval_time': 4.074864625930786, 'eval/sps': 31412.0864741027}
I0727 04:04:32.991615 140120985872192 train.py:379] starting iteration 498 4960.7033631801605
I0727 04:04:42.942546 140120985872192 train.py:394] {'eval/walltime': 2052.058936357498, 'training/sps': 42012.003551728674, 'training/walltime': 2906.7203574180603, 'training/entropy_loss': Array(-0.02646988, dtype=float32), 'training/policy_loss': Array(0.00820835, dtype=float32), 'training/total_loss': Array(21.68169, dtype=float32), 'training/v_loss': Array(21.699955, dtype=float32), 'eval/episode_goal_distance': (Array(0.32903385, dtype=float32), Array(0.1048742, dtype=float32)), 'eval/episode_reward': (Array(-7377.3535, dtype=float32), Array(3672.2246, dtype=float32)), 'eval/avg_episode_length': (Array(860.1797, dtype=float32), Array(345.64603, dtype=float32)), 'eval/epoch_eval_time': 4.097292900085449, 'eval/sps': 31240.13906775631}
I0727 04:04:42.944984 140120985872192 train.py:379] starting iteration 499 4970.656733036041
I0727 04:04:52.829878 140120985872192 train.py:394] {'eval/walltime': 2056.1553916931152, 'training/sps': 42487.15776061791, 'training/walltime': 2912.504693508148, 'training/entropy_loss': Array(-0.02087365, dtype=float32), 'training/policy_loss': Array(0.00973029, dtype=float32), 'training/total_loss': Array(17.20763, dtype=float32), 'training/v_loss': Array(17.218775, dtype=float32), 'eval/episode_goal_distance': (Array(0.33967215, dtype=float32), Array(0.10336632, dtype=float32)), 'eval/episode_reward': (Array(-6217.8477, dtype=float32), Array(4418.5063, dtype=float32)), 'eval/avg_episode_length': (Array(720.41406, dtype=float32), Array(446.95, dtype=float32)), 'eval/epoch_eval_time': 4.096455335617065, 'eval/sps': 31246.526451073547}
I0727 04:04:52.832381 140120985872192 train.py:379] starting iteration 500 4980.544130325317
I0727 04:05:02.701967 140120985872192 train.py:394] {'eval/walltime': 2060.2324600219727, 'training/sps': 42456.5402450537, 'training/walltime': 2918.293200969696, 'training/entropy_loss': Array(-0.01318383, dtype=float32), 'training/policy_loss': Array(0.01064314, dtype=float32), 'training/total_loss': Array(55.479168, dtype=float32), 'training/v_loss': Array(55.481712, dtype=float32), 'eval/episode_goal_distance': (Array(0.33788627, dtype=float32), Array(0.11498255, dtype=float32)), 'eval/episode_reward': (Array(-7110.441, dtype=float32), Array(4378.0493, dtype=float32)), 'eval/avg_episode_length': (Array(805.8906, dtype=float32), Array(393.99988, dtype=float32)), 'eval/epoch_eval_time': 4.077068328857422, 'eval/sps': 31395.107875435428}
I0727 04:05:02.704468 140120985872192 train.py:379] starting iteration 501 4990.416216611862
I0727 04:05:12.662946 140120985872192 train.py:394] {'eval/walltime': 2064.3413231372833, 'training/sps': 42042.21110269508, 'training/walltime': 2924.138754606247, 'training/entropy_loss': Array(0.0051717, dtype=float32), 'training/policy_loss': Array(0.01636838, dtype=float32), 'training/total_loss': Array(14.555338, dtype=float32), 'training/v_loss': Array(14.533796, dtype=float32), 'eval/episode_goal_distance': (Array(0.34988984, dtype=float32), Array(0.10033263, dtype=float32)), 'eval/episode_reward': (Array(-7005.421, dtype=float32), Array(4238.6523, dtype=float32)), 'eval/avg_episode_length': (Array(797.9922, dtype=float32), Array(400.1127, dtype=float32)), 'eval/epoch_eval_time': 4.108863115310669, 'eval/sps': 31152.16944634623}
I0727 04:05:12.665386 140120985872192 train.py:379] starting iteration 502 5000.377135038376
I0727 04:05:22.570543 140120985872192 train.py:394] {'eval/walltime': 2068.4298782348633, 'training/sps': 42278.8763084895, 'training/walltime': 2929.951586484909, 'training/entropy_loss': Array(0.01639559, dtype=float32), 'training/policy_loss': Array(0.01957451, dtype=float32), 'training/total_loss': Array(10.373901, dtype=float32), 'training/v_loss': Array(10.337932, dtype=float32), 'eval/episode_goal_distance': (Array(0.3265154, dtype=float32), Array(0.09962753, dtype=float32)), 'eval/episode_reward': (Array(-7227.245, dtype=float32), Array(3881.085, dtype=float32)), 'eval/avg_episode_length': (Array(852.40625, dtype=float32), Array(353.51273, dtype=float32)), 'eval/epoch_eval_time': 4.088555097579956, 'eval/sps': 31306.903525835834}
I0727 04:05:22.573052 140120985872192 train.py:379] starting iteration 503 5010.284801244736
I0727 04:05:32.513957 140120985872192 train.py:394] {'eval/walltime': 2072.529615163803, 'training/sps': 42101.554683220966, 'training/walltime': 2935.788900613785, 'training/entropy_loss': Array(0.02544812, dtype=float32), 'training/policy_loss': Array(0.01506012, dtype=float32), 'training/total_loss': Array(10.292992, dtype=float32), 'training/v_loss': Array(10.252483, dtype=float32), 'eval/episode_goal_distance': (Array(0.33081925, dtype=float32), Array(0.10503979, dtype=float32)), 'eval/episode_reward': (Array(-5897.3174, dtype=float32), Array(4434.026, dtype=float32)), 'eval/avg_episode_length': (Array(720.1953, dtype=float32), Array(447.29916, dtype=float32)), 'eval/epoch_eval_time': 4.099736928939819, 'eval/sps': 31221.515482238625}
I0727 04:05:32.516426 140120985872192 train.py:379] starting iteration 504 5020.2281749248505
I0727 04:05:42.453227 140120985872192 train.py:394] {'eval/walltime': 2076.6560413837433, 'training/sps': 42326.31914911789, 'training/walltime': 2941.595216989517, 'training/entropy_loss': Array(0.02883199, dtype=float32), 'training/policy_loss': Array(0.01637653, dtype=float32), 'training/total_loss': Array(8.310371, dtype=float32), 'training/v_loss': Array(8.265163, dtype=float32), 'eval/episode_goal_distance': (Array(0.32921052, dtype=float32), Array(0.10845837, dtype=float32)), 'eval/episode_reward': (Array(-6109.4873, dtype=float32), Array(4477.323, dtype=float32)), 'eval/avg_episode_length': (Array(728.14844, dtype=float32), Array(443.13962, dtype=float32)), 'eval/epoch_eval_time': 4.1264262199401855, 'eval/sps': 31019.578002258677}
I0727 04:05:42.456109 140120985872192 train.py:379] starting iteration 505 5030.167857885361
I0727 04:05:52.347489 140120985872192 train.py:394] {'eval/walltime': 2080.745774745941, 'training/sps': 42389.567244376274, 'training/walltime': 2947.392869949341, 'training/entropy_loss': Array(0.03320041, dtype=float32), 'training/policy_loss': Array(0.01341225, dtype=float32), 'training/total_loss': Array(7.971977, dtype=float32), 'training/v_loss': Array(7.9253645, dtype=float32), 'eval/episode_goal_distance': (Array(0.32419568, dtype=float32), Array(0.09007325, dtype=float32)), 'eval/episode_reward': (Array(-6904.8906, dtype=float32), Array(3952.4468, dtype=float32)), 'eval/avg_episode_length': (Array(805.8281, dtype=float32), Array(394.1269, dtype=float32)), 'eval/epoch_eval_time': 4.089733362197876, 'eval/sps': 31297.883911730405}
I0727 04:05:52.350070 140120985872192 train.py:379] starting iteration 506 5040.0618188381195
I0727 04:06:02.281581 140120985872192 train.py:394] {'eval/walltime': 2084.8252160549164, 'training/sps': 42024.983245325355, 'training/walltime': 2953.2408199310303, 'training/entropy_loss': Array(0.0361022, dtype=float32), 'training/policy_loss': Array(0.01530521, dtype=float32), 'training/total_loss': Array(8.123375, dtype=float32), 'training/v_loss': Array(8.071968, dtype=float32), 'eval/episode_goal_distance': (Array(0.31833178, dtype=float32), Array(0.0938142, dtype=float32)), 'eval/episode_reward': (Array(-6789.999, dtype=float32), Array(3803.9175, dtype=float32)), 'eval/avg_episode_length': (Array(829.0703, dtype=float32), Array(375.19757, dtype=float32)), 'eval/epoch_eval_time': 4.07944130897522, 'eval/sps': 31376.845578924233}
I0727 04:06:02.284071 140120985872192 train.py:379] starting iteration 507 5049.995820045471
I0727 04:06:12.206983 140120985872192 train.py:394] {'eval/walltime': 2088.9184777736664, 'training/sps': 42185.46049271667, 'training/walltime': 2959.0665237903595, 'training/entropy_loss': Array(0.03416684, dtype=float32), 'training/policy_loss': Array(0.02417906, dtype=float32), 'training/total_loss': Array(9.518145, dtype=float32), 'training/v_loss': Array(9.459798, dtype=float32), 'eval/episode_goal_distance': (Array(0.33812726, dtype=float32), Array(0.11877315, dtype=float32)), 'eval/episode_reward': (Array(-6336.575, dtype=float32), Array(4752.019, dtype=float32)), 'eval/avg_episode_length': (Array(728.0547, dtype=float32), Array(443.29248, dtype=float32)), 'eval/epoch_eval_time': 4.09326171875, 'eval/sps': 31270.905403793393}
I0727 04:06:12.209341 140120985872192 train.py:379] starting iteration 508 5059.921090126038
I0727 04:06:22.160876 140120985872192 train.py:394] {'eval/walltime': 2093.0332713127136, 'training/sps': 42133.907836892955, 'training/walltime': 2964.899355649948, 'training/entropy_loss': Array(0.01815438, dtype=float32), 'training/policy_loss': Array(0.03972528, dtype=float32), 'training/total_loss': Array(84.669235, dtype=float32), 'training/v_loss': Array(84.61136, dtype=float32), 'eval/episode_goal_distance': (Array(0.3490134, dtype=float32), Array(0.1189893, dtype=float32)), 'eval/episode_reward': (Array(-8287.173, dtype=float32), Array(3480.1616, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.08229, dtype=float32)), 'eval/epoch_eval_time': 4.114793539047241, 'eval/sps': 31107.271552107504}
I0727 04:06:22.163486 140120985872192 train.py:379] starting iteration 509 5069.875234603882
I0727 04:06:32.071251 140120985872192 train.py:394] {'eval/walltime': 2097.131457090378, 'training/sps': 42332.156159700055, 'training/walltime': 2970.704871416092, 'training/entropy_loss': Array(0.00926967, dtype=float32), 'training/policy_loss': Array(0.01607052, dtype=float32), 'training/total_loss': Array(15.801398, dtype=float32), 'training/v_loss': Array(15.776056, dtype=float32), 'eval/episode_goal_distance': (Array(0.32967117, dtype=float32), Array(0.13013877, dtype=float32)), 'eval/episode_reward': (Array(-7677.441, dtype=float32), Array(4004.1455, dtype=float32)), 'eval/avg_episode_length': (Array(891.3203, dtype=float32), Array(310.12592, dtype=float32)), 'eval/epoch_eval_time': 4.098185777664185, 'eval/sps': 31233.332734114192}
I0727 04:06:32.073626 140120985872192 train.py:379] starting iteration 510 5079.785375118256
I0727 04:06:42.018475 140120985872192 train.py:394] {'eval/walltime': 2101.25199842453, 'training/sps': 42223.67729329913, 'training/walltime': 2976.5253024101257, 'training/entropy_loss': Array(0.01009686, dtype=float32), 'training/policy_loss': Array(0.01189578, dtype=float32), 'training/total_loss': Array(11.923182, dtype=float32), 'training/v_loss': Array(11.901189, dtype=float32), 'eval/episode_goal_distance': (Array(0.3598122, dtype=float32), Array(0.11539469, dtype=float32)), 'eval/episode_reward': (Array(-8222.32, dtype=float32), Array(3826.6626, dtype=float32)), 'eval/avg_episode_length': (Array(891.2344, dtype=float32), Array(310.37082, dtype=float32)), 'eval/epoch_eval_time': 4.120541334152222, 'eval/sps': 31063.879626470312}
I0727 04:06:43.137561 140120985872192 train.py:410] total steps: 125583360
