I0728 06:11:47.990331 139845810259776 low_level_env.py:188] Initialising environment...
I0728 06:12:27.740147 139845810259776 low_level_env.py:293] Environment initialised.
I0728 06:12:27.744881 139845810259776 train.py:118] JAX is running on GPU.
I0728 06:12:27.744933 139845810259776 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 06:12:36.206935 139845810259776 train.py:367] Running initial eval
I0728 06:12:52.143990 139845810259776 train.py:373] {'eval/walltime': 15.793383836746216, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3129066 , 0.14050804], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.401024, 12.139556], dtype=float32), 'eval/episode_reward': Array([-3.1500862,  9.430392 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30799937, 0.14412071], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.793383836746216, 'eval/sps': 8104.659604497449}
I0728 06:12:52.145164 139845810259776 train.py:379] starting iteration 0, 0 steps, 24.40029764175415
I0728 06:13:35.388364 139845810259776 train.py:394] {'eval/walltime': 19.749793767929077, 'training/sps': 50053.82908529099, 'training/walltime': 39.27931261062622, 'training/entropy_loss': Array(-0.04412196, dtype=float32), 'training/policy_loss': Array(0.00740256, dtype=float32), 'training/total_loss': Array(1.1574988, dtype=float32), 'training/v_loss': Array(1.1942184, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30805248, 0.11744459], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.059345, 10.083489], dtype=float32), 'eval/episode_reward': Array([-1.8878471,  7.3408256], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30403078, 0.11985933], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9564099311828613, 'eval/sps': 32352.562607619227}
I0728 06:13:35.421152 139845810259776 train.py:379] starting iteration 1, 1966080 steps, 67.67628240585327
I0728 06:13:59.438269 139845810259776 train.py:394] {'eval/walltime': 23.806703567504883, 'training/sps': 98527.19819928418, 'training/walltime': 59.23400568962097, 'training/entropy_loss': Array(-0.04321742, dtype=float32), 'training/policy_loss': Array(0.00122245, dtype=float32), 'training/total_loss': Array(1.0272636, dtype=float32), 'training/v_loss': Array(1.0692587, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2974923 , 0.11254538], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.172485,  9.644751], dtype=float32), 'eval/episode_reward': Array([-1.4237005,  7.907718 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29335117, 0.1147334 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.056909799575806, 'eval/sps': 31551.108189140366}
I0728 06:13:59.440334 139845810259776 train.py:379] starting iteration 2, 3932160 steps, 91.695472240448
I0728 06:14:23.793512 139845810259776 train.py:394] {'eval/walltime': 27.821839332580566, 'training/sps': 96693.44124721231, 'training/walltime': 79.56713247299194, 'training/entropy_loss': Array(-0.04242108, dtype=float32), 'training/policy_loss': Array(0.002347, dtype=float32), 'training/total_loss': Array(0.9456555, dtype=float32), 'training/v_loss': Array(0.9857296, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2697838 , 0.10791171], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.713234,  9.249355], dtype=float32), 'eval/episode_reward': Array([0.30692524, 6.3348975 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26528424, 0.11026549], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.015135765075684, 'eval/sps': 31879.370334962325}
I0728 06:14:23.795438 139845810259776 train.py:379] starting iteration 3, 5898240 steps, 116.05057573318481
I0728 06:14:48.662118 139845810259776 train.py:394] {'eval/walltime': 31.842576503753662, 'training/sps': 94342.4961669637, 'training/walltime': 100.40694570541382, 'training/entropy_loss': Array(-0.03951532, dtype=float32), 'training/policy_loss': Array(0.00757151, dtype=float32), 'training/total_loss': Array(0.8408296, dtype=float32), 'training/v_loss': Array(0.8727734, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2823867 , 0.10707325], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.72967 ,  9.197315], dtype=float32), 'eval/episode_reward': Array([-0.21809563,  5.8641577 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27781743, 0.10959285], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020737171173096, 'eval/sps': 31834.95825534265}
I0728 06:14:48.664136 139845810259776 train.py:379] starting iteration 4, 7864320 steps, 140.91927337646484
I0728 06:15:13.828078 139845810259776 train.py:394] {'eval/walltime': 35.86479353904724, 'training/sps': 93016.11549769426, 'training/walltime': 121.5439281463623, 'training/entropy_loss': Array(-0.03328586, dtype=float32), 'training/policy_loss': Array(0.01457586, dtype=float32), 'training/total_loss': Array(0.719771, dtype=float32), 'training/v_loss': Array(0.73848104, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26503187, 0.10327306], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.19549 ,  8.868168], dtype=float32), 'eval/episode_reward': Array([0.34982663, 5.4930005 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2595935 , 0.10679202], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022217035293579, 'eval/sps': 31823.24545812515}
I0728 06:15:13.830011 139845810259776 train.py:379] starting iteration 5, 9830400 steps, 166.0851490497589
I0728 06:15:39.218669 139845810259776 train.py:394] {'eval/walltime': 39.894617557525635, 'training/sps': 92069.11091101964, 'training/walltime': 142.89832139015198, 'training/entropy_loss': Array(-0.02459935, dtype=float32), 'training/policy_loss': Array(0.02079454, dtype=float32), 'training/total_loss': Array(0.65183026, dtype=float32), 'training/v_loss': Array(0.65563506, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26909107, 0.09576153], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.55727 ,  8.198525], dtype=float32), 'eval/episode_reward': Array([0.45159793, 5.5681696 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2641433 , 0.09787703], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0298240184783936, 'eval/sps': 31763.17363067657}
I0728 06:15:39.220581 139845810259776 train.py:379] starting iteration 6, 11796480 steps, 191.47571897506714
I0728 06:16:04.697628 139845810259776 train.py:394] {'eval/walltime': 43.937939167022705, 'training/sps': 91748.32454694914, 'training/walltime': 164.32737755775452, 'training/entropy_loss': Array(-0.01319558, dtype=float32), 'training/policy_loss': Array(0.02601456, dtype=float32), 'training/total_loss': Array(0.5919271, dtype=float32), 'training/v_loss': Array(0.5791081, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2700225, 0.098061 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.501293,  8.360517], dtype=float32), 'eval/episode_reward': Array([2.3323064, 4.5544605], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26476645, 0.1006234 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.04332160949707, 'eval/sps': 31657.14043111236}
I0728 06:16:04.699574 139845810259776 train.py:379] starting iteration 7, 13762560 steps, 216.95471239089966
I0728 06:16:30.213913 139845810259776 train.py:394] {'eval/walltime': 47.96011281013489, 'training/sps': 91499.03237372127, 'training/walltime': 185.81481790542603, 'training/entropy_loss': Array(-0.00022022, dtype=float32), 'training/policy_loss': Array(0.03074975, dtype=float32), 'training/total_loss': Array(0.5698764, dtype=float32), 'training/v_loss': Array(0.5393469, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.24002934, 0.10528242], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.90443 ,  9.015844], dtype=float32), 'eval/episode_reward': Array([2.5926065, 5.0154004], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.23358569, 0.10905103], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022173643112183, 'eval/sps': 31823.588774988635}
I0728 06:16:30.215870 139845810259776 train.py:379] starting iteration 8, 15728640 steps, 242.47100830078125
I0728 06:16:55.767158 139845810259776 train.py:394] {'eval/walltime': 52.022939920425415, 'training/sps': 91515.22437464593, 'training/walltime': 207.29845643043518, 'training/entropy_loss': Array(0.01680457, dtype=float32), 'training/policy_loss': Array(0.03364821, dtype=float32), 'training/total_loss': Array(0.5256411, dtype=float32), 'training/v_loss': Array(0.47518831, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25295356, 0.11207457], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.988152,  9.564463], dtype=float32), 'eval/episode_reward': Array([3.2295785, 4.75803  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24647357, 0.11637148], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.062827110290527, 'eval/sps': 31505.15552970377}
I0728 06:16:55.769089 139845810259776 train.py:379] starting iteration 9, 17694720 steps, 268.024227142334
I0728 06:17:21.360463 139845810259776 train.py:394] {'eval/walltime': 56.086164236068726, 'training/sps': 91345.92672889633, 'training/walltime': 228.8219120502472, 'training/entropy_loss': Array(0.03708425, dtype=float32), 'training/policy_loss': Array(0.03325426, dtype=float32), 'training/total_loss': Array(0.4897374, dtype=float32), 'training/v_loss': Array(0.41939884, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23024797, 0.10337682], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.090847,  8.860026], dtype=float32), 'eval/episode_reward': Array([3.6337194, 5.7744627], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22311607, 0.10778052], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0632243156433105, 'eval/sps': 31502.075705543317}
I0728 06:17:21.362396 139845810259776 train.py:379] starting iteration 10, 19660800 steps, 293.61753368377686
I0728 06:17:46.856205 139845810259776 train.py:394] {'eval/walltime': 60.11600160598755, 'training/sps': 91617.6378841284, 'training/walltime': 250.28153538703918, 'training/entropy_loss': Array(0.05906678, dtype=float32), 'training/policy_loss': Array(0.03387468, dtype=float32), 'training/total_loss': Array(0.5562502, dtype=float32), 'training/v_loss': Array(0.46330875, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23676702, 0.11459953], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.614494,  9.81408 ], dtype=float32), 'eval/episode_reward': Array([2.3050146, 5.9980536], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22924766, 0.11962158], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.029837369918823, 'eval/sps': 31763.068394638074}
I0728 06:17:46.858146 139845810259776 train.py:379] starting iteration 11, 21626880 steps, 319.1132848262787
I0728 06:18:12.398179 139845810259776 train.py:394] {'eval/walltime': 64.14265394210815, 'training/sps': 91409.1160882666, 'training/walltime': 271.7901122570038, 'training/entropy_loss': Array(0.08145133, dtype=float32), 'training/policy_loss': Array(0.03441088, dtype=float32), 'training/total_loss': Array(0.5872805, dtype=float32), 'training/v_loss': Array(0.47141832, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25529507, 0.10348941], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.192053,  8.823727], dtype=float32), 'eval/episode_reward': Array([2.2530665, 5.1685734], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24928814, 0.10690439], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0266523361206055, 'eval/sps': 31788.192601529372}
I0728 06:18:12.400201 139845810259776 train.py:379] starting iteration 12, 23592960 steps, 344.655339717865
I0728 06:18:38.070204 139845810259776 train.py:394] {'eval/walltime': 68.17738604545593, 'training/sps': 90898.52324952128, 'training/walltime': 293.4195065498352, 'training/entropy_loss': Array(0.10200519, dtype=float32), 'training/policy_loss': Array(0.0369059, dtype=float32), 'training/total_loss': Array(0.5503361, dtype=float32), 'training/v_loss': Array(0.411425, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27739036, 0.09613378], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.043049,  8.263629], dtype=float32), 'eval/episode_reward': Array([1.500308, 4.678534], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27207  , 0.0990702], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.034732103347778, 'eval/sps': 31724.535042560394}
I0728 06:18:38.074231 139845810259776 train.py:379] starting iteration 13, 25559040 steps, 370.329354763031
I0728 06:19:03.686091 139845810259776 train.py:394] {'eval/walltime': 72.21922326087952, 'training/sps': 91176.11313062231, 'training/walltime': 314.9830491542816, 'training/entropy_loss': Array(0.12216598, dtype=float32), 'training/policy_loss': Array(0.03796384, dtype=float32), 'training/total_loss': Array(0.57297206, dtype=float32), 'training/v_loss': Array(0.41284224, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2743786 , 0.10164534], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.851679,  8.663034], dtype=float32), 'eval/episode_reward': Array([1.4060156, 4.5202866], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2690251, 0.1038999], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.041837215423584, 'eval/sps': 31668.76674586352}
I0728 06:19:03.688024 139845810259776 train.py:379] starting iteration 14, 27525120 steps, 395.94316267967224
I0728 06:19:29.369814 139845810259776 train.py:394] {'eval/walltime': 76.28230309486389, 'training/sps': 90970.45709395429, 'training/walltime': 336.5953402519226, 'training/entropy_loss': Array(0.1376919, dtype=float32), 'training/policy_loss': Array(0.03795923, dtype=float32), 'training/total_loss': Array(0.5589583, dtype=float32), 'training/v_loss': Array(0.3833072, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2647406 , 0.10743679], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.952847,  9.230994], dtype=float32), 'eval/episode_reward': Array([1.7436565, 4.3522654], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25836235, 0.11128425], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.063079833984375, 'eval/sps': 31503.195908035963}
I0728 06:19:29.373893 139845810259776 train.py:379] starting iteration 15, 29491200 steps, 421.62901639938354
I0728 06:19:55.033833 139845810259776 train.py:394] {'eval/walltime': 80.31239295005798, 'training/sps': 90917.81216279317, 'training/walltime': 358.22014570236206, 'training/entropy_loss': Array(0.14843962, dtype=float32), 'training/policy_loss': Array(0.03721329, dtype=float32), 'training/total_loss': Array(0.5705712, dtype=float32), 'training/v_loss': Array(0.38491833, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25487748, 0.09632433], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.224703,  8.176259], dtype=float32), 'eval/episode_reward': Array([2.2528148, 5.109256 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24950525, 0.09859984], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.030089855194092, 'eval/sps': 31761.078437253713}
I0728 06:19:55.035840 139845810259776 train.py:379] starting iteration 16, 31457280 steps, 447.2909791469574
I0728 06:20:20.748053 139845810259776 train.py:394] {'eval/walltime': 84.36585474014282, 'training/sps': 90795.27793928472, 'training/walltime': 379.8741352558136, 'training/entropy_loss': Array(0.15102993, dtype=float32), 'training/policy_loss': Array(0.03987598, dtype=float32), 'training/total_loss': Array(0.6530948, dtype=float32), 'training/v_loss': Array(0.46218893, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.24712497, 0.09956395], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.557068,  8.492155], dtype=float32), 'eval/episode_reward': Array([2.797111 , 4.7677846], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24080622, 0.10325548], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.053461790084839, 'eval/sps': 31577.946611733318}
I0728 06:20:20.750113 139845810259776 train.py:379] starting iteration 17, 33423360 steps, 473.0052514076233
I0728 06:20:46.417314 139845810259776 train.py:394] {'eval/walltime': 88.3949785232544, 'training/sps': 90881.18254352538, 'training/walltime': 401.50765657424927, 'training/entropy_loss': Array(0.15282005, dtype=float32), 'training/policy_loss': Array(0.04047748, dtype=float32), 'training/total_loss': Array(0.6775393, dtype=float32), 'training/v_loss': Array(0.48424178, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.24437411, 0.10585248], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.355705 ,  9.0790415], dtype=float32), 'eval/episode_reward': Array([2.3630285, 5.8236384], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.23790915, 0.10919596], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.029123783111572, 'eval/sps': 31768.693862552274}
I0728 06:20:46.419383 139845810259776 train.py:379] starting iteration 18, 35389440 steps, 498.67452025413513
I0728 06:21:12.103740 139845810259776 train.py:394] {'eval/walltime': 92.43811535835266, 'training/sps': 90868.49426141982, 'training/walltime': 423.14419865608215, 'training/entropy_loss': Array(0.15566373, dtype=float32), 'training/policy_loss': Array(0.04221243, dtype=float32), 'training/total_loss': Array(0.70584464, dtype=float32), 'training/v_loss': Array(0.50796854, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2541474 , 0.12309804], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.21196 , 10.528333], dtype=float32), 'eval/episode_reward': Array([2.4489667, 5.9876065], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24795285, 0.12641881], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.043136835098267, 'eval/sps': 31658.587186275385}
I0728 06:21:12.105748 139845810259776 train.py:379] starting iteration 19, 37355520 steps, 524.3608860969543
I0728 06:21:37.784224 139845810259776 train.py:394] {'eval/walltime': 96.47237539291382, 'training/sps': 90855.91563036745, 'training/walltime': 444.78373622894287, 'training/entropy_loss': Array(0.15470129, dtype=float32), 'training/policy_loss': Array(0.0429201, dtype=float32), 'training/total_loss': Array(0.72579443, dtype=float32), 'training/v_loss': Array(0.52817297, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.24721362, 0.11444217], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.647629,  9.814773], dtype=float32), 'eval/episode_reward': Array([2.1508424, 6.347589 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24034753, 0.11885236], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.034260034561157, 'eval/sps': 31728.24728783843}
I0728 06:21:37.786247 139845810259776 train.py:379] starting iteration 20, 39321600 steps, 550.0413851737976
I0728 06:22:03.485341 139845810259776 train.py:394] {'eval/walltime': 100.50871396064758, 'training/sps': 90777.45196504892, 'training/walltime': 466.4419779777527, 'training/entropy_loss': Array(0.1542272, dtype=float32), 'training/policy_loss': Array(0.04527773, dtype=float32), 'training/total_loss': Array(0.76784235, dtype=float32), 'training/v_loss': Array(0.56833744, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26183417, 0.1124702 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.928558,  9.655529], dtype=float32), 'eval/episode_reward': Array([1.4652638, 5.3448606], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25601822, 0.11555845], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.036338567733765, 'eval/sps': 31711.908664754712}
I0728 06:22:03.487300 139845810259776 train.py:379] starting iteration 21, 41287680 steps, 575.7424383163452
I0728 06:22:29.207226 139845810259776 train.py:394] {'eval/walltime': 104.58436584472656, 'training/sps': 90857.07983844628, 'training/walltime': 488.0812382698059, 'training/entropy_loss': Array(0.15860635, dtype=float32), 'training/policy_loss': Array(0.04463515, dtype=float32), 'training/total_loss': Array(0.7487096, dtype=float32), 'training/v_loss': Array(0.54546815, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2675901 , 0.10655364], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.44494 ,  9.226456], dtype=float32), 'eval/episode_reward': Array([1.3200827, 4.7749023], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26185238, 0.11058034], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0756518840789795, 'eval/sps': 31406.018875168378}
I0728 06:22:29.209222 139845810259776 train.py:379] starting iteration 22, 43253760 steps, 601.4643604755402
I0728 06:22:54.947011 139845810259776 train.py:394] {'eval/walltime': 108.63414669036865, 'training/sps': 90672.38065114687, 'training/walltime': 509.764577627182, 'training/entropy_loss': Array(0.16301745, dtype=float32), 'training/policy_loss': Array(0.04078061, dtype=float32), 'training/total_loss': Array(0.7535658, dtype=float32), 'training/v_loss': Array(0.5497678, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25523147, 0.10513987], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.302631,  9.092949], dtype=float32), 'eval/episode_reward': Array([0.2747215, 4.206321 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24866405, 0.10912321], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.04978084564209, 'eval/sps': 31606.648576487525}
I0728 06:22:54.949012 139845810259776 train.py:379] starting iteration 23, 45219840 steps, 627.2041501998901
I0728 06:23:20.653396 139845810259776 train.py:394] {'eval/walltime': 112.6637191772461, 'training/sps': 90727.41075372252, 'training/walltime': 531.4347651004791, 'training/entropy_loss': Array(0.16373149, dtype=float32), 'training/policy_loss': Array(0.03383728, dtype=float32), 'training/total_loss': Array(0.7298421, dtype=float32), 'training/v_loss': Array(0.53227335, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26287168, 0.11835036], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.035198, 10.17584 ], dtype=float32), 'eval/episode_reward': Array([0.46056396, 5.408117  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25660455, 0.12241586], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.029572486877441, 'eval/sps': 31765.15633279712}
I0728 06:23:20.655397 139845810259776 train.py:379] starting iteration 24, 47185920 steps, 652.9105353355408
I0728 06:23:46.394387 139845810259776 train.py:394] {'eval/walltime': 116.72109365463257, 'training/sps': 90700.2140675097, 'training/walltime': 553.1114504337311, 'training/entropy_loss': Array(0.16486536, dtype=float32), 'training/policy_loss': Array(0.02746645, dtype=float32), 'training/total_loss': Array(0.6821798, dtype=float32), 'training/v_loss': Array(0.48984796, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2627128 , 0.11407074], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.053423,  9.806645], dtype=float32), 'eval/episode_reward': Array([0.7038722, 4.871269 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25670594, 0.1173751 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.057374477386475, 'eval/sps': 31547.494744051866}
I0728 06:23:46.396411 139845810259776 train.py:379] starting iteration 25, 49152000 steps, 678.6515486240387
I0728 06:24:11.953055 139845810259776 train.py:394] {'eval/walltime': 120.77890062332153, 'training/sps': 91469.1948466637, 'training/walltime': 574.6059000492096, 'training/entropy_loss': Array(0.17054534, dtype=float32), 'training/policy_loss': Array(0.02262214, dtype=float32), 'training/total_loss': Array(0.8144003, dtype=float32), 'training/v_loss': Array(0.6212328, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27248502, 0.12159344], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.852953, 10.457183], dtype=float32), 'eval/episode_reward': Array([-0.03703126,  5.0935583 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2662957 , 0.12554094], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.057806968688965, 'eval/sps': 31544.132332483885}
I0728 06:24:11.983254 139845810259776 train.py:379] starting iteration 26, 51118080 steps, 704.238391160965
I0728 06:24:37.566352 139845810259776 train.py:394] {'eval/walltime': 124.84196138381958, 'training/sps': 91379.88194392632, 'training/walltime': 596.1213579177856, 'training/entropy_loss': Array(0.17278278, dtype=float32), 'training/policy_loss': Array(0.01844642, dtype=float32), 'training/total_loss': Array(0.81600213, dtype=float32), 'training/v_loss': Array(0.624773, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27618515, 0.1132477 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.124092,  9.82525 ], dtype=float32), 'eval/episode_reward': Array([0.24080226, 5.3372335 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27027243, 0.11702587], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.063060760498047, 'eval/sps': 31503.343795505993}
I0728 06:24:37.568561 139845810259776 train.py:379] starting iteration 27, 53084160 steps, 729.8236997127533
I0728 06:25:02.900209 139845810259776 train.py:394] {'eval/walltime': 128.8800687789917, 'training/sps': 92360.45006708347, 'training/walltime': 617.4083914756775, 'training/entropy_loss': Array(0.17183593, dtype=float32), 'training/policy_loss': Array(0.01853548, dtype=float32), 'training/total_loss': Array(0.83489513, dtype=float32), 'training/v_loss': Array(0.6445237, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2723354, 0.1051974], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.793812,  9.15883 ], dtype=float32), 'eval/episode_reward': Array([0.6789341, 5.205637 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2664792 , 0.10900762], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.038107395172119, 'eval/sps': 31698.01777759409}
I0728 06:25:02.904364 139845810259776 train.py:379] starting iteration 28, 55050240 steps, 755.1594874858856
I0728 06:25:28.307853 139845810259776 train.py:394] {'eval/walltime': 132.9218180179596, 'training/sps': 92060.25505140456, 'training/walltime': 638.7648389339447, 'training/entropy_loss': Array(0.16771588, dtype=float32), 'training/policy_loss': Array(0.01595151, dtype=float32), 'training/total_loss': Array(0.869213, dtype=float32), 'training/v_loss': Array(0.68554556, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28087065, 0.11530528], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.529398,  9.980098], dtype=float32), 'eval/episode_reward': Array([0.72967434, 5.1545606 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2750377 , 0.11922926], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0417492389678955, 'eval/sps': 31669.456077558683}
I0728 06:25:28.309859 139845810259776 train.py:379] starting iteration 29, 57016320 steps, 780.5649962425232
I0728 06:25:53.877304 139845810259776 train.py:394] {'eval/walltime': 136.96071982383728, 'training/sps': 91344.17625732067, 'training/walltime': 660.2887070178986, 'training/entropy_loss': Array(0.16706535, dtype=float32), 'training/policy_loss': Array(0.0152704, dtype=float32), 'training/total_loss': Array(0.8866099, dtype=float32), 'training/v_loss': Array(0.7042742, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29335216, 0.10904866], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.66909 ,  9.340441], dtype=float32), 'eval/episode_reward': Array([-0.13975915,  5.751156  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2887171 , 0.11110521], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0389018058776855, 'eval/sps': 31691.783101467252}
I0728 06:25:53.879327 139845810259776 train.py:379] starting iteration 30, 58982400 steps, 806.1344652175903
I0728 06:26:19.493411 139845810259776 train.py:394] {'eval/walltime': 141.00061178207397, 'training/sps': 91150.44218835016, 'training/walltime': 681.8583226203918, 'training/entropy_loss': Array(0.16862844, dtype=float32), 'training/policy_loss': Array(0.01420841, dtype=float32), 'training/total_loss': Array(0.9284548, dtype=float32), 'training/v_loss': Array(0.745618, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27971047, 0.11312987], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.470972,  9.739899], dtype=float32), 'eval/episode_reward': Array([-0.17057359,  5.584201  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2747112 , 0.11586183], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.039891958236694, 'eval/sps': 31684.015642801645}
I0728 06:26:19.495445 139845810259776 train.py:379] starting iteration 31, 60948480 steps, 831.7505826950073
I0728 06:26:45.188680 139845810259776 train.py:394] {'eval/walltime': 145.04658222198486, 'training/sps': 90842.94919015489, 'training/walltime': 703.5009489059448, 'training/entropy_loss': Array(0.17094338, dtype=float32), 'training/policy_loss': Array(0.01602804, dtype=float32), 'training/total_loss': Array(0.95672876, dtype=float32), 'training/v_loss': Array(0.76975733, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29012513, 0.11288991], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.3303  ,  9.736593], dtype=float32), 'eval/episode_reward': Array([0.07026812, 5.5699306 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28482845, 0.11645693], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.045970439910889, 'eval/sps': 31636.415021069497}
I0728 06:26:45.190682 139845810259776 train.py:379] starting iteration 32, 62914560 steps, 857.4458205699921
I0728 06:27:10.883191 139845810259776 train.py:394] {'eval/walltime': 149.07642078399658, 'training/sps': 90778.90696581721, 'training/walltime': 725.1588435173035, 'training/entropy_loss': Array(0.16634083, dtype=float32), 'training/policy_loss': Array(0.01539174, dtype=float32), 'training/total_loss': Array(1.0255764, dtype=float32), 'training/v_loss': Array(0.84384376, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28951514, 0.11203074], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.319624,  9.696087], dtype=float32), 'eval/episode_reward': Array([-0.37468147,  4.7627487 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2844103 , 0.11622645], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.029838562011719, 'eval/sps': 31763.05899859712}
I0728 06:27:10.885177 139845810259776 train.py:379] starting iteration 33, 64880640 steps, 883.1403148174286
I0728 06:27:36.583662 139845810259776 train.py:394] {'eval/walltime': 153.14269876480103, 'training/sps': 90907.83847098342, 'training/walltime': 746.7860214710236, 'training/entropy_loss': Array(0.08334644, dtype=float32), 'training/policy_loss': Array(0.03535769, dtype=float32), 'training/total_loss': Array(1.6162627, dtype=float32), 'training/v_loss': Array(1.4975586, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29066324, 0.12591478], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.374165, 10.866426], dtype=float32), 'eval/episode_reward': Array([-1.3128442,  6.7576623], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28451368, 0.12966196], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.066277980804443, 'eval/sps': 31478.41849579536}
I0728 06:27:36.585676 139845810259776 train.py:379] starting iteration 34, 66846720 steps, 908.8408143520355
I0728 06:28:02.233664 139845810259776 train.py:394] {'eval/walltime': 157.17875456809998, 'training/sps': 90990.93314878429, 'training/walltime': 768.3934490680695, 'training/entropy_loss': Array(-0.02702032, dtype=float32), 'training/policy_loss': Array(0.01759429, dtype=float32), 'training/total_loss': Array(2.4205325, dtype=float32), 'training/v_loss': Array(2.4299583, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2903204 , 0.13374558], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.291103, 11.607677], dtype=float32), 'eval/episode_reward': Array([-3.0294595,  8.489048 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28347385, 0.13841875], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.03605580329895, 'eval/sps': 31714.13038823117}
I0728 06:28:02.235611 139845810259776 train.py:379] starting iteration 35, 68812800 steps, 934.4907495975494
I0728 06:28:27.938708 139845810259776 train.py:394] {'eval/walltime': 161.23594212532043, 'training/sps': 90849.11316746539, 'training/walltime': 790.0346069335938, 'training/entropy_loss': Array(-0.03161984, dtype=float32), 'training/policy_loss': Array(0.01188342, dtype=float32), 'training/total_loss': Array(2.54246, dtype=float32), 'training/v_loss': Array(2.5621963, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28518382, 0.10188749], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.83206 ,  8.860464], dtype=float32), 'eval/episode_reward': Array([-1.7556896,  7.901687 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27925575, 0.10520155], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.057187557220459, 'eval/sps': 31548.948180175234}
I0728 06:28:27.940618 139845810259776 train.py:379] starting iteration 36, 70778880 steps, 960.1957573890686
I0728 06:28:53.618112 139845810259776 train.py:394] {'eval/walltime': 165.27925658226013, 'training/sps': 90898.16354549806, 'training/walltime': 811.6640868186951, 'training/entropy_loss': Array(-0.03294917, dtype=float32), 'training/policy_loss': Array(0.00941372, dtype=float32), 'training/total_loss': Array(2.8008409, dtype=float32), 'training/v_loss': Array(2.8243763, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29765192, 0.11265144], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.97817  ,  9.6959095], dtype=float32), 'eval/episode_reward': Array([-0.9717432,  8.01771  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2922997 , 0.11550342], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.043314456939697, 'eval/sps': 31657.196432077806}
I0728 06:28:53.620071 139845810259776 train.py:379] starting iteration 37, 72744960 steps, 985.8752098083496
I0728 06:29:19.303372 139845810259776 train.py:394] {'eval/walltime': 169.32612347602844, 'training/sps': 90889.38223990268, 'training/walltime': 833.2956564426422, 'training/entropy_loss': Array(-0.03252264, dtype=float32), 'training/policy_loss': Array(0.00903491, dtype=float32), 'training/total_loss': Array(3.0488882, dtype=float32), 'training/v_loss': Array(3.0723758, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3056088 , 0.12917584], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.66485 , 11.132841], dtype=float32), 'eval/episode_reward': Array([-1.7042582,  8.942413 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30020195, 0.13247997], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0468668937683105, 'eval/sps': 31629.4069857115}
I0728 06:29:19.305322 139845810259776 train.py:379] starting iteration 38, 74711040 steps, 1011.5604603290558
I0728 06:29:44.987637 139845810259776 train.py:394] {'eval/walltime': 173.39460277557373, 'training/sps': 90984.78605177956, 'training/walltime': 854.904543876648, 'training/entropy_loss': Array(-0.03184941, dtype=float32), 'training/policy_loss': Array(0.00823701, dtype=float32), 'training/total_loss': Array(3.2500446, dtype=float32), 'training/v_loss': Array(3.273657, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28780168, 0.12973802], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.148743, 11.198842], dtype=float32), 'eval/episode_reward': Array([-0.7386827,  9.299519 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28169125, 0.13399185], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.068479299545288, 'eval/sps': 31461.386571219835}
I0728 06:29:44.989582 139845810259776 train.py:379] starting iteration 39, 76677120 steps, 1037.2447209358215
I0728 06:30:10.634696 139845810259776 train.py:394] {'eval/walltime': 177.43120121955872, 'training/sps': 91006.27991947763, 'training/walltime': 876.5083277225494, 'training/entropy_loss': Array(-0.03076071, dtype=float32), 'training/policy_loss': Array(0.00686503, dtype=float32), 'training/total_loss': Array(3.389646, dtype=float32), 'training/v_loss': Array(3.4135418, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29272002, 0.12354139], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.6217   , 10.6258335], dtype=float32), 'eval/episode_reward': Array([-0.69698673,  8.598111  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2876968 , 0.12611644], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.036598443984985, 'eval/sps': 31709.867051733945}
I0728 06:30:10.636560 139845810259776 train.py:379] starting iteration 40, 78643200 steps, 1062.8916983604431
I0728 06:30:36.322691 139845810259776 train.py:394] {'eval/walltime': 181.47475171089172, 'training/sps': 90864.95579395717, 'training/walltime': 898.1457123756409, 'training/entropy_loss': Array(-0.0303703, dtype=float32), 'training/policy_loss': Array(0.00600358, dtype=float32), 'training/total_loss': Array(3.605891, dtype=float32), 'training/v_loss': Array(3.630258, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28749877, 0.12789755], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.132238, 11.047367], dtype=float32), 'eval/episode_reward': Array([-0.5024146,  9.091712 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28168076, 0.13170546], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.043550491333008, 'eval/sps': 31655.348504824327}
I0728 06:30:36.324509 139845810259776 train.py:379] starting iteration 41, 80609280 steps, 1088.5796482563019
I0728 06:31:01.983873 139845810259776 train.py:394] {'eval/walltime': 185.50183153152466, 'training/sps': 90904.77293509367, 'training/walltime': 919.7736196517944, 'training/entropy_loss': Array(-0.03063052, dtype=float32), 'training/policy_loss': Array(0.00486278, dtype=float32), 'training/total_loss': Array(3.8124514, dtype=float32), 'training/v_loss': Array(3.8382192, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31484506, 0.1353034 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.502773, 11.669605], dtype=float32), 'eval/episode_reward': Array([-1.6916878,  9.807608 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.309873  , 0.13846405], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.027079820632935, 'eval/sps': 31784.818206032553}
I0728 06:31:01.985704 139845810259776 train.py:379] starting iteration 42, 82575360 steps, 1114.2408425807953
I0728 06:31:27.712046 139845810259776 train.py:394] {'eval/walltime': 189.5513298511505, 'training/sps': 90720.32311572293, 'training/walltime': 941.4455001354218, 'training/entropy_loss': Array(-0.03085583, dtype=float32), 'training/policy_loss': Array(0.00427809, dtype=float32), 'training/total_loss': Array(3.8438654, dtype=float32), 'training/v_loss': Array(3.8704433, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.302445  , 0.11444265], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.45427 ,  9.883797], dtype=float32), 'eval/episode_reward': Array([-2.4755542,  8.092228 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29748622, 0.11721435], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0494983196258545, 'eval/sps': 31608.85371396482}
I0728 06:31:27.714032 139845810259776 train.py:379] starting iteration 43, 84541440 steps, 1139.9691700935364
I0728 06:31:53.312649 139845810259776 train.py:394] {'eval/walltime': 193.5936722755432, 'training/sps': 91227.08665401304, 'training/walltime': 962.9969940185547, 'training/entropy_loss': Array(-0.03212985, dtype=float32), 'training/policy_loss': Array(0.00368644, dtype=float32), 'training/total_loss': Array(3.9192426, dtype=float32), 'training/v_loss': Array(3.947686, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2837342 , 0.13198958], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.877344 , 11.3874655], dtype=float32), 'eval/episode_reward': Array([-1.0130848,  8.94083  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27790108, 0.13593538], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0423424243927, 'eval/sps': 31664.80880679722}
I0728 06:31:53.314658 139845810259776 train.py:379] starting iteration 44, 86507520 steps, 1165.569795370102
I0728 06:32:19.006594 139845810259776 train.py:394] {'eval/walltime': 197.63040137290955, 'training/sps': 90809.87276279711, 'training/walltime': 984.6475033760071, 'training/entropy_loss': Array(-0.03290475, dtype=float32), 'training/policy_loss': Array(0.00370154, dtype=float32), 'training/total_loss': Array(4.0208426, dtype=float32), 'training/v_loss': Array(4.050046, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28486985, 0.12787926], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.94807 , 11.024969], dtype=float32), 'eval/episode_reward': Array([-1.7685596,  8.914981 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27875173, 0.13242109], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.036729097366333, 'eval/sps': 31708.840725405757}
I0728 06:32:19.008447 139845810259776 train.py:379] starting iteration 45, 88473600 steps, 1191.2635860443115
I0728 06:32:44.699046 139845810259776 train.py:394] {'eval/walltime': 201.6689646244049, 'training/sps': 90822.37662946589, 'training/walltime': 1006.2950320243835, 'training/entropy_loss': Array(-0.03426418, dtype=float32), 'training/policy_loss': Array(0.00309104, dtype=float32), 'training/total_loss': Array(4.2133226, dtype=float32), 'training/v_loss': Array(4.2444963, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28631037, 0.12731707], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.048693 , 10.9286585], dtype=float32), 'eval/episode_reward': Array([-1.5492375,  8.307059 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28050363, 0.1310561 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.038563251495361, 'eval/sps': 31694.439836396115}
I0728 06:32:44.700963 139845810259776 train.py:379] starting iteration 46, 90439680 steps, 1216.956101179123
I0728 06:33:10.410131 139845810259776 train.py:394] {'eval/walltime': 205.72462725639343, 'training/sps': 90817.50249239578, 'training/walltime': 1027.943722486496, 'training/entropy_loss': Array(-0.03480481, dtype=float32), 'training/policy_loss': Array(0.00275783, dtype=float32), 'training/total_loss': Array(4.1511307, dtype=float32), 'training/v_loss': Array(4.1831775, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31471997, 0.13541783], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.48578 , 11.613659], dtype=float32), 'eval/episode_reward': Array([-3.354641,  9.207542], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3098627 , 0.13843456], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.055662631988525, 'eval/sps': 31560.810554215288}
I0728 06:33:10.411981 139845810259776 train.py:379] starting iteration 47, 92405760 steps, 1242.6671199798584
I0728 06:33:36.096515 139845810259776 train.py:394] {'eval/walltime': 209.76272988319397, 'training/sps': 90847.09444534266, 'training/walltime': 1049.5853612422943, 'training/entropy_loss': Array(-0.035962, dtype=float32), 'training/policy_loss': Array(0.00240219, dtype=float32), 'training/total_loss': Array(4.328499, dtype=float32), 'training/v_loss': Array(4.3620586, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3087287 , 0.13042982], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.040113, 11.256529], dtype=float32), 'eval/episode_reward': Array([-2.2364106,  9.517317 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3035672 , 0.13446213], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.038102626800537, 'eval/sps': 31698.055208026435}
I0728 06:33:36.098357 139845810259776 train.py:379] starting iteration 48, 94371840 steps, 1268.353485584259
I0728 06:34:01.819545 139845810259776 train.py:394] {'eval/walltime': 213.8250012397766, 'training/sps': 90795.11099132823, 'training/walltime': 1071.2393906116486, 'training/entropy_loss': Array(-0.03633771, dtype=float32), 'training/policy_loss': Array(0.00234617, dtype=float32), 'training/total_loss': Array(4.376934, dtype=float32), 'training/v_loss': Array(4.410925, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2969017 , 0.13860242], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.987509, 11.9369  ], dtype=float32), 'eval/episode_reward': Array([-2.069761,  8.763145], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29144564, 0.1424778 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.062271356582642, 'eval/sps': 31509.465706318333}
I0728 06:34:01.821444 139845810259776 train.py:379] starting iteration 49, 96337920 steps, 1294.0765829086304
I0728 06:34:27.514389 139845810259776 train.py:394] {'eval/walltime': 217.86733746528625, 'training/sps': 90828.63185500819, 'training/walltime': 1092.88542842865, 'training/entropy_loss': Array(-0.03776545, dtype=float32), 'training/policy_loss': Array(0.00192774, dtype=float32), 'training/total_loss': Array(4.438951, dtype=float32), 'training/v_loss': Array(4.4747887, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29456213, 0.11898711], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.78186 , 10.230881], dtype=float32), 'eval/episode_reward': Array([-2.4873495,  9.419236 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28938252, 0.12230159], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0423362255096436, 'eval/sps': 31664.857364471758}
I0728 06:34:27.516389 139845810259776 train.py:379] starting iteration 50, 98304000 steps, 1319.7715277671814
I0728 06:34:53.054424 139845810259776 train.py:394] {'eval/walltime': 221.88975405693054, 'training/sps': 91399.29669443564, 'training/walltime': 1114.3963160514832, 'training/entropy_loss': Array(-0.03890303, dtype=float32), 'training/policy_loss': Array(0.00171535, dtype=float32), 'training/total_loss': Array(4.6845517, dtype=float32), 'training/v_loss': Array(4.72174, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2713547 , 0.12231828], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.719084, 10.578694], dtype=float32), 'eval/episode_reward': Array([0.29733256, 7.6223984 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26512784, 0.126364  ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022416591644287, 'eval/sps': 31821.66667318664}
I0728 06:34:53.090310 139845810259776 train.py:379] starting iteration 51, 100270080 steps, 1345.3454475402832
I0728 06:35:18.594653 139845810259776 train.py:394] {'eval/walltime': 225.90857768058777, 'training/sps': 91528.1335424106, 'training/walltime': 1135.8769245147705, 'training/entropy_loss': Array(-0.03976625, dtype=float32), 'training/policy_loss': Array(0.00183545, dtype=float32), 'training/total_loss': Array(4.623566, dtype=float32), 'training/v_loss': Array(4.6614966, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26821887, 0.13122196], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.485498, 11.340031], dtype=float32), 'eval/episode_reward': Array([-0.4701303,  8.775    ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26159874, 0.13533872], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.018823623657227, 'eval/sps': 31850.11634910141}
I0728 06:35:18.596597 139845810259776 train.py:379] starting iteration 52, 102236160 steps, 1370.8517355918884
I0728 06:35:44.057046 139845810259776 train.py:394] {'eval/walltime': 229.93336939811707, 'training/sps': 91739.60989409615, 'training/walltime': 1157.3080163002014, 'training/entropy_loss': Array(-0.03994922, dtype=float32), 'training/policy_loss': Array(0.00186981, dtype=float32), 'training/total_loss': Array(4.5711555, dtype=float32), 'training/v_loss': Array(4.609235, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29518545, 0.12714382], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.817013, 10.942357], dtype=float32), 'eval/episode_reward': Array([-2.480051,  9.57579 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.289856  , 0.13050571], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024791717529297, 'eval/sps': 31802.887946354524}
I0728 06:35:44.058909 139845810259776 train.py:379] starting iteration 53, 104202240 steps, 1396.314048051834
I0728 06:36:09.414492 139845810259776 train.py:394] {'eval/walltime': 233.95572996139526, 'training/sps': 92188.26309758477, 'training/walltime': 1178.6348092556, 'training/entropy_loss': Array(-0.04027988, dtype=float32), 'training/policy_loss': Array(0.00196636, dtype=float32), 'training/total_loss': Array(4.664898, dtype=float32), 'training/v_loss': Array(4.7032113, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27926606, 0.12960398], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.452103, 11.115176], dtype=float32), 'eval/episode_reward': Array([-1.3665243,  7.976892 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2733478 , 0.13321038], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022360563278198, 'eval/sps': 31822.10992434771}
I0728 06:36:09.416399 139845810259776 train.py:379] starting iteration 54, 106168320 steps, 1421.671537399292
I0728 06:36:34.961792 139845810259776 train.py:394] {'eval/walltime': 238.0102882385254, 'training/sps': 91504.72014354757, 'training/walltime': 1200.1209139823914, 'training/entropy_loss': Array(-0.04053473, dtype=float32), 'training/policy_loss': Array(0.00199446, dtype=float32), 'training/total_loss': Array(4.709454, dtype=float32), 'training/v_loss': Array(4.7479944, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28456372, 0.12268616], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.888145, 10.585862], dtype=float32), 'eval/episode_reward': Array([-0.8742806,  8.545964 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27867573, 0.12612781], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.054558277130127, 'eval/sps': 31569.406887548845}
I0728 06:36:34.963633 139845810259776 train.py:379] starting iteration 55, 108134400 steps, 1447.2187712192535
I0728 06:37:00.523064 139845810259776 train.py:394] {'eval/walltime': 242.05875825881958, 'training/sps': 91418.87171664217, 'training/walltime': 1221.627195596695, 'training/entropy_loss': Array(-0.04057609, dtype=float32), 'training/policy_loss': Array(0.0024262, dtype=float32), 'training/total_loss': Array(4.7519116, dtype=float32), 'training/v_loss': Array(4.7900615, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28062797, 0.12025279], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.554905, 10.33619 ], dtype=float32), 'eval/episode_reward': Array([0.06617147, 7.821552  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2755252 , 0.12305897], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0484700202941895, 'eval/sps': 31616.882268699286}
I0728 06:37:00.524971 139845810259776 train.py:379] starting iteration 56, 110100480 steps, 1472.7801096439362
I0728 06:37:25.995835 139845810259776 train.py:394] {'eval/walltime': 246.08225011825562, 'training/sps': 91692.4245942651, 'training/walltime': 1243.0693159103394, 'training/entropy_loss': Array(-0.04082772, dtype=float32), 'training/policy_loss': Array(0.0026882, dtype=float32), 'training/total_loss': Array(4.6980386, dtype=float32), 'training/v_loss': Array(4.736178, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27618122, 0.112178  ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.161995,  9.677739], dtype=float32), 'eval/episode_reward': Array([-0.10269788,  8.091023  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27044344, 0.11565089], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023491859436035, 'eval/sps': 31813.162415082283}
I0728 06:37:25.997757 139845810259776 train.py:379] starting iteration 57, 112066560 steps, 1498.2528953552246
I0728 06:37:51.564820 139845810259776 train.py:394] {'eval/walltime': 250.14214420318604, 'training/sps': 91433.58155053548, 'training/walltime': 1264.572137594223, 'training/entropy_loss': Array(-0.04110738, dtype=float32), 'training/policy_loss': Array(0.00317596, dtype=float32), 'training/total_loss': Array(4.83209, dtype=float32), 'training/v_loss': Array(4.870021, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2963521 , 0.12125001], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.878296, 10.39454 ], dtype=float32), 'eval/episode_reward': Array([-1.262166 ,  9.7632065], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2909845 , 0.12459943], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.05989408493042, 'eval/sps': 31527.91607916878}
I0728 06:37:51.566643 139845810259776 train.py:379] starting iteration 58, 114032640 steps, 1523.8217821121216
I0728 06:38:17.213036 139845810259776 train.py:394] {'eval/walltime': 254.16228079795837, 'training/sps': 90931.08073477556, 'training/walltime': 1286.193787574768, 'training/entropy_loss': Array(-0.041048, dtype=float32), 'training/policy_loss': Array(0.00348361, dtype=float32), 'training/total_loss': Array(4.786982, dtype=float32), 'training/v_loss': Array(4.8245463, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2835397 , 0.12431534], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.794853, 10.683647], dtype=float32), 'eval/episode_reward': Array([-0.17913887,  9.031992  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27808937, 0.12700535], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020136594772339, 'eval/sps': 31839.714144650516}
I0728 06:38:17.214883 139845810259776 train.py:379] starting iteration 59, 115998720 steps, 1549.4700214862823
I0728 06:38:42.952437 139845810259776 train.py:394] {'eval/walltime': 258.22144198417664, 'training/sps': 90711.22788818607, 'training/walltime': 1307.8678410053253, 'training/entropy_loss': Array(-0.04110805, dtype=float32), 'training/policy_loss': Array(0.00369764, dtype=float32), 'training/total_loss': Array(4.717183, dtype=float32), 'training/v_loss': Array(4.754593, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2799508 , 0.11124065], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.436184,  9.589883], dtype=float32), 'eval/episode_reward': Array([1.1325729, 8.8179455], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2743572 , 0.11438853], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.059161186218262, 'eval/sps': 31533.608577700226}
I0728 06:38:42.954272 139845810259776 train.py:379] starting iteration 60, 117964800 steps, 1575.2094106674194
I0728 06:39:08.594844 139845810259776 train.py:394] {'eval/walltime': 262.23554491996765, 'training/sps': 90928.65831738425, 'training/walltime': 1329.4900670051575, 'training/entropy_loss': Array(-0.04057962, dtype=float32), 'training/policy_loss': Array(0.00416943, dtype=float32), 'training/total_loss': Array(4.6173677, dtype=float32), 'training/v_loss': Array(4.6537776, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2647456 , 0.09995141], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.177013,  8.637791], dtype=float32), 'eval/episode_reward': Array([1.6683648, 7.59072  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25911134, 0.10308225], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.014102935791016, 'eval/sps': 31887.57290170897}
I0728 06:39:08.596750 139845810259776 train.py:379] starting iteration 61, 119930880 steps, 1600.8518888950348
I0728 06:39:34.204932 139845810259776 train.py:394] {'eval/walltime': 266.30547738075256, 'training/sps': 91309.58027114293, 'training/walltime': 1351.0220901966095, 'training/entropy_loss': Array(-0.03927846, dtype=float32), 'training/policy_loss': Array(0.00500343, dtype=float32), 'training/total_loss': Array(4.603096, dtype=float32), 'training/v_loss': Array(4.637371, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2667482 , 0.11426461], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.31841 ,  9.832298], dtype=float32), 'eval/episode_reward': Array([0.36912364, 7.703217  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26085237, 0.11750201], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.069932460784912, 'eval/sps': 31450.153346110907}
I0728 06:39:34.206873 139845810259776 train.py:379] starting iteration 62, 121896960 steps, 1626.462012052536
I0728 06:39:59.709077 139845810259776 train.py:394] {'eval/walltime': 270.3465392589569, 'training/sps': 91630.76636101215, 'training/walltime': 1372.4786388874054, 'training/entropy_loss': Array(-0.0377859, dtype=float32), 'training/policy_loss': Array(0.00624874, dtype=float32), 'training/total_loss': Array(4.490536, dtype=float32), 'training/v_loss': Array(4.5220733, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2494382 , 0.10757005], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.775112,  9.252931], dtype=float32), 'eval/episode_reward': Array([3.114696, 7.907198], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24269634, 0.11130992], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.041061878204346, 'eval/sps': 31674.84286503355}
I0728 06:39:59.710895 139845810259776 train.py:379] starting iteration 63, 123863040 steps, 1651.9660329818726
I0728 06:40:25.298405 139845810259776 train.py:394] {'eval/walltime': 274.38887906074524, 'training/sps': 91273.39607866299, 'training/walltime': 1394.019198179245, 'training/entropy_loss': Array(-0.0356634, dtype=float32), 'training/policy_loss': Array(0.00764836, dtype=float32), 'training/total_loss': Array(4.4218626, dtype=float32), 'training/v_loss': Array(4.4498777, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2336489 , 0.10697119], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.39692 ,  9.258079], dtype=float32), 'eval/episode_reward': Array([2.9353242, 7.474598 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22561146, 0.11223257], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.04233980178833, 'eval/sps': 31664.82935041058}
I0728 06:40:25.300211 139845810259776 train.py:379] starting iteration 64, 125829120 steps, 1677.5553495883942
I0728 06:40:50.998044 139845810259776 train.py:394] {'eval/walltime': 278.44711422920227, 'training/sps': 90873.22866927175, 'training/walltime': 1415.654613018036, 'training/entropy_loss': Array(-0.03329882, dtype=float32), 'training/policy_loss': Array(0.00901348, dtype=float32), 'training/total_loss': Array(4.381163, dtype=float32), 'training/v_loss': Array(4.405448, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23089227, 0.11102281], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.157646,  9.5308  ], dtype=float32), 'eval/episode_reward': Array([4.582208, 8.784575], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22293407, 0.11535802], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.058235168457031, 'eval/sps': 31540.803991570177}
I0728 06:40:50.999959 139845810259776 train.py:379] starting iteration 65, 127795200 steps, 1703.2550971508026
I0728 06:41:16.703940 139845810259776 train.py:394] {'eval/walltime': 282.4906077384949, 'training/sps': 90787.90082504507, 'training/walltime': 1437.3103621006012, 'training/entropy_loss': Array(-0.03058223, dtype=float32), 'training/policy_loss': Array(0.01088125, dtype=float32), 'training/total_loss': Array(4.248087, dtype=float32), 'training/v_loss': Array(4.267788, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.22243628, 0.09989124], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.395153,  8.56861 ], dtype=float32), 'eval/episode_reward': Array([3.4602206, 7.7201624], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.21439594, 0.10401771], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0434935092926025, 'eval/sps': 31655.7946008409}
I0728 06:41:16.705760 139845810259776 train.py:379] starting iteration 66, 129761280 steps, 1728.9608991146088
I0728 06:41:42.400459 139845810259776 train.py:394] {'eval/walltime': 286.5479004383087, 'training/sps': 90884.36268012603, 'training/walltime': 1458.9431264400482, 'training/entropy_loss': Array(-0.02795149, dtype=float32), 'training/policy_loss': Array(0.01293798, dtype=float32), 'training/total_loss': Array(4.0936785, dtype=float32), 'training/v_loss': Array(4.1086917, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20718566, 0.10034381], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.11705 ,  8.597594], dtype=float32), 'eval/episode_reward': Array([5.519889, 8.494051], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1983507 , 0.10474312], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.057292699813843, 'eval/sps': 31548.13060587739}
I0728 06:41:42.402278 139845810259776 train.py:379] starting iteration 67, 131727360 steps, 1754.6574165821075
I0728 06:42:08.054731 139845810259776 train.py:394] {'eval/walltime': 290.57838582992554, 'training/sps': 90948.23887382142, 'training/walltime': 1480.5606973171234, 'training/entropy_loss': Array(-0.02643984, dtype=float32), 'training/policy_loss': Array(0.01426784, dtype=float32), 'training/total_loss': Array(4.120446, dtype=float32), 'training/v_loss': Array(4.132618, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23049821, 0.11175022], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.036037,  9.568068], dtype=float32), 'eval/episode_reward': Array([5.193726 , 7.6370363], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22245286, 0.11592746], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.030485391616821, 'eval/sps': 31757.961526478342}
I0728 06:42:08.056634 139845810259776 train.py:379] starting iteration 68, 133693440 steps, 1780.3117728233337
I0728 06:42:33.773094 139845810259776 train.py:394] {'eval/walltime': 294.6342577934265, 'training/sps': 90786.0007625448, 'training/walltime': 1502.2168996334076, 'training/entropy_loss': Array(-0.02387796, dtype=float32), 'training/policy_loss': Array(0.01634169, dtype=float32), 'training/total_loss': Array(3.8430405, dtype=float32), 'training/v_loss': Array(3.8505769, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.22043794, 0.11032206], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.14643 ,  9.486609], dtype=float32), 'eval/episode_reward': Array([4.9289694, 8.288301 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.21151818, 0.11499569], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.055871963500977, 'eval/sps': 31559.18163883853}
I0728 06:42:33.774959 139845810259776 train.py:379] starting iteration 69, 135659520 steps, 1806.0300981998444
I0728 06:42:59.185485 139845810259776 train.py:394] {'eval/walltime': 298.6595547199249, 'training/sps': 91954.6332314291, 'training/walltime': 1523.59787774086, 'training/entropy_loss': Array(-0.02089417, dtype=float32), 'training/policy_loss': Array(0.0176283, dtype=float32), 'training/total_loss': Array(3.7327704, dtype=float32), 'training/v_loss': Array(3.736036, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19837144, 0.10383102], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.307566,  8.893635], dtype=float32), 'eval/episode_reward': Array([7.2071714, 8.360118 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1884473 , 0.10941985], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.025296926498413, 'eval/sps': 31798.896413673163}
I0728 06:42:59.187323 139845810259776 train.py:379] starting iteration 70, 137625600 steps, 1831.4424612522125
I0728 06:43:24.803972 139845810259776 train.py:394] {'eval/walltime': 302.72447299957275, 'training/sps': 91244.4576549863, 'training/walltime': 1545.1452686786652, 'training/entropy_loss': Array(-0.01856632, dtype=float32), 'training/policy_loss': Array(0.01764714, dtype=float32), 'training/total_loss': Array(3.6481638, dtype=float32), 'training/v_loss': Array(3.649083, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.21480557, 0.12236597], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.587822, 10.527415], dtype=float32), 'eval/episode_reward': Array([5.959266, 8.563122], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20469673, 0.12838243], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.064918279647827, 'eval/sps': 31488.94791830589}
I0728 06:43:24.805818 139845810259776 train.py:379] starting iteration 71, 139591680 steps, 1857.060956954956
I0728 06:43:50.478143 139845810259776 train.py:394] {'eval/walltime': 306.75796699523926, 'training/sps': 90877.84739526758, 'training/walltime': 1566.7795839309692, 'training/entropy_loss': Array(-0.01557868, dtype=float32), 'training/policy_loss': Array(0.01753674, dtype=float32), 'training/total_loss': Array(3.3975475, dtype=float32), 'training/v_loss': Array(3.3955894, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18557963, 0.1048847 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.173121,  9.004033], dtype=float32), 'eval/episode_reward': Array([6.9675164, 8.476031 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.17466299, 0.11030881], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.033493995666504, 'eval/sps': 31734.273098589052}
I0728 06:43:50.479986 139845810259776 train.py:379] starting iteration 72, 141557760 steps, 1882.7351245880127
I0728 06:44:16.184569 139845810259776 train.py:394] {'eval/walltime': 310.82164764404297, 'training/sps': 90868.2128961118, 'training/walltime': 1588.4161930084229, 'training/entropy_loss': Array(-0.01282264, dtype=float32), 'training/policy_loss': Array(0.01758178, dtype=float32), 'training/total_loss': Array(3.1789122, dtype=float32), 'training/v_loss': Array(3.1741529, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.200591  , 0.12219316], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.421783, 10.496662], dtype=float32), 'eval/episode_reward': Array([6.4787736, 8.725977 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18915403, 0.12907687], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.063680648803711, 'eval/sps': 31498.538163347395}
I0728 06:44:16.186413 139845810259776 train.py:379] starting iteration 73, 143523840 steps, 1908.4415514469147
I0728 06:44:41.899415 139845810259776 train.py:394] {'eval/walltime': 314.866171836853, 'training/sps': 90761.35513467093, 'training/walltime': 1610.0782759189606, 'training/entropy_loss': Array(-0.00999574, dtype=float32), 'training/policy_loss': Array(0.0171744, dtype=float32), 'training/total_loss': Array(3.0101783, dtype=float32), 'training/v_loss': Array(3.0029998, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17093001, 0.09650308], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.995348,  8.268424], dtype=float32), 'eval/episode_reward': Array([9.0319805, 7.777898 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15996505, 0.10127868], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.044524192810059, 'eval/sps': 31647.727618379762}
I0728 06:44:41.901229 139845810259776 train.py:379] starting iteration 74, 145489920 steps, 1934.156367301941
I0728 06:45:07.620167 139845810259776 train.py:394] {'eval/walltime': 318.928204536438, 'training/sps': 90800.88654774248, 'training/walltime': 1631.7309279441833, 'training/entropy_loss': Array(-0.00826499, dtype=float32), 'training/policy_loss': Array(0.01744487, dtype=float32), 'training/total_loss': Array(2.8401413, dtype=float32), 'training/v_loss': Array(2.8309612, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1842343 , 0.10544843], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.15583 ,  8.995537], dtype=float32), 'eval/episode_reward': Array([6.9106607, 6.7822514], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.17390823, 0.11024794], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.062032699584961, 'eval/sps': 31511.316984985973}
I0728 06:45:07.621977 139845810259776 train.py:379] starting iteration 75, 147456000 steps, 1959.8771154880524
I0728 06:45:33.266071 139845810259776 train.py:394] {'eval/walltime': 322.9659357070923, 'training/sps': 91015.35003181489, 'training/walltime': 1653.3325588703156, 'training/entropy_loss': Array(-0.00669966, dtype=float32), 'training/policy_loss': Array(0.01694528, dtype=float32), 'training/total_loss': Array(3.099711, dtype=float32), 'training/v_loss': Array(3.0894656, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16765416, 0.08805396], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.677976,  7.52877 ], dtype=float32), 'eval/episode_reward': Array([9.128971, 8.531109], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15573823, 0.09432228], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.037731170654297, 'eval/sps': 31700.97131039513}
I0728 06:45:33.312695 139845810259776 train.py:379] starting iteration 76, 149422080 steps, 1985.5678324699402
I0728 06:45:58.940742 139845810259776 train.py:394] {'eval/walltime': 327.01113843917847, 'training/sps': 91115.26991822546, 'training/walltime': 1674.9105007648468, 'training/entropy_loss': Array(-0.00442639, dtype=float32), 'training/policy_loss': Array(0.01759327, dtype=float32), 'training/total_loss': Array(2.9171438, dtype=float32), 'training/v_loss': Array(2.903977, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20527597, 0.11937714], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.858788, 10.177346], dtype=float32), 'eval/episode_reward': Array([6.5999126, 8.622299 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.19526426, 0.12454062], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.045202732086182, 'eval/sps': 31642.41905225555}
I0728 06:45:58.942681 139845810259776 train.py:379] starting iteration 77, 151388160 steps, 2011.1978192329407
I0728 06:46:24.601170 139845810259776 train.py:394] {'eval/walltime': 331.04099321365356, 'training/sps': 90919.72877348193, 'training/walltime': 1696.534850358963, 'training/entropy_loss': Array(-0.00247877, dtype=float32), 'training/policy_loss': Array(0.01795295, dtype=float32), 'training/total_loss': Array(2.725443, dtype=float32), 'training/v_loss': Array(2.7099688, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16443062, 0.10870148], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.373481,  9.273764], dtype=float32), 'eval/episode_reward': Array([9.816107, 8.544276], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15169881, 0.1150055 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.029854774475098, 'eval/sps': 31762.93121299202}
I0728 06:46:24.603018 139845810259776 train.py:379] starting iteration 78, 153354240 steps, 2036.8581566810608
I0728 06:46:50.236087 139845810259776 train.py:394] {'eval/walltime': 335.07577538490295, 'training/sps': 91051.0393335999, 'training/walltime': 1718.128014087677, 'training/entropy_loss': Array(-0.00053263, dtype=float32), 'training/policy_loss': Array(0.01810166, dtype=float32), 'training/total_loss': Array(2.6274438, dtype=float32), 'training/v_loss': Array(2.6098747, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18754658, 0.11135663], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.361591,  9.487542], dtype=float32), 'eval/episode_reward': Array([8.821493, 8.930852], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1764577 , 0.11735963], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.03478217124939, 'eval/sps': 31724.141370527614}
I0728 06:46:50.238923 139845810259776 train.py:379] starting iteration 79, 155320320 steps, 2062.49405503273
I0728 06:47:15.972524 139845810259776 train.py:394] {'eval/walltime': 339.15142726898193, 'training/sps': 90798.45706543261, 'training/walltime': 1739.781245470047, 'training/entropy_loss': Array(0.00124927, dtype=float32), 'training/policy_loss': Array(0.01763687, dtype=float32), 'training/total_loss': Array(2.4733677, dtype=float32), 'training/v_loss': Array(2.4544818, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18397577, 0.10113531], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.170946,  8.602742], dtype=float32), 'eval/episode_reward': Array([9.183109 , 7.9525537], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.17329225, 0.10680247], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0756518840789795, 'eval/sps': 31406.018875168378}
I0728 06:47:15.974384 139845810259776 train.py:379] starting iteration 80, 157286400 steps, 2088.229521751404
I0728 06:47:41.654654 139845810259776 train.py:394] {'eval/walltime': 343.1789138317108, 'training/sps': 90820.85121737642, 'training/walltime': 1761.4291377067566, 'training/entropy_loss': Array(0.00416799, dtype=float32), 'training/policy_loss': Array(0.01879216, dtype=float32), 'training/total_loss': Array(2.4441457, dtype=float32), 'training/v_loss': Array(2.4211853, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17259315, 0.09871459], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.107314,  8.420975], dtype=float32), 'eval/episode_reward': Array([9.878191, 7.851758], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16107462, 0.10497505], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.027486562728882, 'eval/sps': 31781.608208090893}
I0728 06:47:41.656481 139845810259776 train.py:379] starting iteration 81, 159252480 steps, 2113.9116196632385
I0728 06:48:07.283960 139845810259776 train.py:394] {'eval/walltime': 347.2244246006012, 'training/sps': 91118.35369502909, 'training/walltime': 1783.00634932518, 'training/entropy_loss': Array(0.00762322, dtype=float32), 'training/policy_loss': Array(0.0190623, dtype=float32), 'training/total_loss': Array(2.2894678, dtype=float32), 'training/v_loss': Array(2.262782, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18160935, 0.1142654 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.872797,  9.864948], dtype=float32), 'eval/episode_reward': Array([8.441067, 8.540027], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16939414, 0.12134294], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.045510768890381, 'eval/sps': 31640.009707626697}
I0728 06:48:07.285767 139845810259776 train.py:379] starting iteration 82, 161218560 steps, 2139.540904521942
I0728 06:48:33.015680 139845810259776 train.py:394] {'eval/walltime': 351.2767810821533, 'training/sps': 90715.13060915546, 'training/walltime': 1804.6794703006744, 'training/entropy_loss': Array(0.00946253, dtype=float32), 'training/policy_loss': Array(0.01902885, dtype=float32), 'training/total_loss': Array(2.248306, dtype=float32), 'training/v_loss': Array(2.2198148, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16692781, 0.10267959], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.567671,  8.779672], dtype=float32), 'eval/episode_reward': Array([9.899706, 9.272314], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15443952, 0.10913542], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.052356481552124, 'eval/sps': 31586.55971721761}
I0728 06:48:33.017511 139845810259776 train.py:379] starting iteration 83, 163184640 steps, 2165.2726497650146
I0728 06:48:58.693919 139845810259776 train.py:394] {'eval/walltime': 355.31818199157715, 'training/sps': 90901.0582917506, 'training/walltime': 1826.3082613945007, 'training/entropy_loss': Array(0.01180825, dtype=float32), 'training/policy_loss': Array(0.01968677, dtype=float32), 'training/total_loss': Array(2.1742556, dtype=float32), 'training/v_loss': Array(2.1427608, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16563565, 0.10682661], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.508425,  9.179642], dtype=float32), 'eval/episode_reward': Array([9.570904, 8.881287], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15268865, 0.11393028], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.041400909423828, 'eval/sps': 31672.185677379042}
I0728 06:48:58.695828 139845810259776 train.py:379] starting iteration 84, 165150720 steps, 2190.9509661197662
I0728 06:49:24.413764 139845810259776 train.py:394] {'eval/walltime': 359.3756902217865, 'training/sps': 90795.03201616151, 'training/walltime': 1847.9623095989227, 'training/entropy_loss': Array(0.01434972, dtype=float32), 'training/policy_loss': Array(0.01945457, dtype=float32), 'training/total_loss': Array(2.1550026, dtype=float32), 'training/v_loss': Array(2.1211982, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19089554, 0.11191013], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.63344,  9.60804], dtype=float32), 'eval/episode_reward': Array([9.152372, 8.568291], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18018696, 0.11776425], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.057508230209351, 'eval/sps': 31546.454803714776}
I0728 06:49:24.417736 139845810259776 train.py:379] starting iteration 85, 167116800 steps, 2216.6728613376617
I0728 06:49:50.104603 139845810259776 train.py:394] {'eval/walltime': 363.4159586429596, 'training/sps': 90847.82205571754, 'training/walltime': 1869.603775024414, 'training/entropy_loss': Array(0.01422922, dtype=float32), 'training/policy_loss': Array(0.02749313, dtype=float32), 'training/total_loss': Array(2.0873823, dtype=float32), 'training/v_loss': Array(2.04566, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18617824, 0.10303368], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.329086,  8.944841], dtype=float32), 'eval/episode_reward': Array([8.324715, 9.130022], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1753365 , 0.10915814], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.040268421173096, 'eval/sps': 31681.063398959785}
I0728 06:49:50.106527 139845810259776 train.py:379] starting iteration 86, 169082880 steps, 2242.3616647720337
I0728 06:50:15.825544 139845810259776 train.py:394] {'eval/walltime': 367.4317510128021, 'training/sps': 90616.43245122844, 'training/walltime': 1891.3005020618439, 'training/entropy_loss': Array(0.01376704, dtype=float32), 'training/policy_loss': Array(0.02000682, dtype=float32), 'training/total_loss': Array(2.0382056, dtype=float32), 'training/v_loss': Array(2.0044317, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16297925, 0.10484391], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.242979,  9.148846], dtype=float32), 'eval/episode_reward': Array([8.660011, 8.608224], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.14812161, 0.11323167], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.015792369842529, 'eval/sps': 31874.157877594465}
I0728 06:50:15.829582 139845810259776 train.py:379] starting iteration 87, 171048960 steps, 2268.084706068039
I0728 06:50:41.523081 139845810259776 train.py:394] {'eval/walltime': 371.4706416130066, 'training/sps': 90814.52405504695, 'training/walltime': 1912.9499025344849, 'training/entropy_loss': Array(0.01331834, dtype=float32), 'training/policy_loss': Array(0.01737945, dtype=float32), 'training/total_loss': Array(1.9116412, dtype=float32), 'training/v_loss': Array(1.8809434, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18971774, 0.11971188], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.660227, 10.370152], dtype=float32), 'eval/episode_reward': Array([7.815036, 9.35125 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.17891797, 0.1257334 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.038890600204468, 'eval/sps': 31691.871028524525}
I0728 06:50:41.524990 139845810259776 train.py:379] starting iteration 88, 173015040 steps, 2293.7801287174225
I0728 06:51:07.256591 139845810259776 train.py:394] {'eval/walltime': 375.5092363357544, 'training/sps': 90652.34068293753, 'training/walltime': 1934.6380352973938, 'training/entropy_loss': Array(0.01436583, dtype=float32), 'training/policy_loss': Array(0.01420722, dtype=float32), 'training/total_loss': Array(1.8390293, dtype=float32), 'training/v_loss': Array(1.8104563, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19369379, 0.11670727], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.973267, 10.128009], dtype=float32), 'eval/episode_reward': Array([6.745882, 8.609561], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18258488, 0.12308317], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.038594722747803, 'eval/sps': 31694.192853525696}
I0728 06:51:07.259309 139845810259776 train.py:379] starting iteration 89, 174981120 steps, 2319.5144329071045
I0728 06:51:33.021036 139845810259776 train.py:394] {'eval/walltime': 379.5717771053314, 'training/sps': 90626.5534473795, 'training/walltime': 1956.3323392868042, 'training/entropy_loss': Array(0.01735186, dtype=float32), 'training/policy_loss': Array(0.01252733, dtype=float32), 'training/total_loss': Array(1.8320813, dtype=float32), 'training/v_loss': Array(1.8022022, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16629106, 0.10055204], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.614539,  8.7919  ], dtype=float32), 'eval/episode_reward': Array([9.244618, 8.792798], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15358257, 0.10744996], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.062540769577026, 'eval/sps': 31507.37611264066}
I0728 06:51:33.022982 139845810259776 train.py:379] starting iteration 90, 176947200 steps, 2345.2781207561493
I0728 06:51:58.690268 139845810259776 train.py:394] {'eval/walltime': 383.6055693626404, 'training/sps': 90901.00017457436, 'training/walltime': 1977.961144208908, 'training/entropy_loss': Array(0.02108167, dtype=float32), 'training/policy_loss': Array(0.01229517, dtype=float32), 'training/total_loss': Array(1.8739479, dtype=float32), 'training/v_loss': Array(1.8405712, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16879556, 0.11586022], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.817659,  9.965454], dtype=float32), 'eval/episode_reward': Array([8.773912, 9.378605], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15592305, 0.12236045], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.03379225730896, 'eval/sps': 31731.926642496925}
I0728 06:51:58.692207 139845810259776 train.py:379] starting iteration 91, 178913280 steps, 2370.9473462104797
I0728 06:52:24.393933 139845810259776 train.py:394] {'eval/walltime': 387.66444063186646, 'training/sps': 90860.85295856501, 'training/walltime': 1999.5995059013367, 'training/entropy_loss': Array(0.0248036, dtype=float32), 'training/policy_loss': Array(0.01288983, dtype=float32), 'training/total_loss': Array(1.9293723, dtype=float32), 'training/v_loss': Array(1.8916788, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17634821, 0.11441582], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.404795,  9.912796], dtype=float32), 'eval/episode_reward': Array([7.8743153, 8.634546 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1627642 , 0.12215043], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.058871269226074, 'eval/sps': 31535.86095979004}
I0728 06:52:24.395855 139845810259776 train.py:379] starting iteration 92, 180879360 steps, 2396.6509940624237
I0728 06:52:50.070839 139845810259776 train.py:394] {'eval/walltime': 391.6842608451843, 'training/sps': 90809.98876429266, 'training/walltime': 2021.249987602234, 'training/entropy_loss': Array(0.02730247, dtype=float32), 'training/policy_loss': Array(0.01266885, dtype=float32), 'training/total_loss': Array(1.9648004, dtype=float32), 'training/v_loss': Array(1.924829, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17588946, 0.12051863], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.361105, 10.515849], dtype=float32), 'eval/episode_reward': Array([9.530819, 9.206462], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16212472, 0.12839122], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019820213317871, 'eval/sps': 31842.22010126956}
I0728 06:52:50.072817 139845810259776 train.py:379] starting iteration 93, 182845440 steps, 2422.3279559612274
I0728 06:53:15.797878 139845810259776 train.py:394] {'eval/walltime': 395.7418563365936, 'training/sps': 90761.88757459153, 'training/walltime': 2042.911943435669, 'training/entropy_loss': Array(0.02877504, dtype=float32), 'training/policy_loss': Array(0.01304976, dtype=float32), 'training/total_loss': Array(1.9890151, dtype=float32), 'training/v_loss': Array(1.94719, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17161   , 0.11455438], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.993206, 10.02473 ], dtype=float32), 'eval/episode_reward': Array([8.592384, 9.137393], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15715016, 0.12311416], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.057595491409302, 'eval/sps': 31545.776376920825}
I0728 06:53:15.799834 139845810259776 train.py:379] starting iteration 94, 184811520 steps, 2448.054972887039
I0728 06:53:41.480695 139845810259776 train.py:394] {'eval/walltime': 399.76610350608826, 'training/sps': 90803.84109338946, 'training/walltime': 2064.5638909339905, 'training/entropy_loss': Array(0.02962782, dtype=float32), 'training/policy_loss': Array(0.01359286, dtype=float32), 'training/total_loss': Array(2.028256, dtype=float32), 'training/v_loss': Array(1.9850353, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1823705 , 0.10666643], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.989142,  9.2189  ], dtype=float32), 'eval/episode_reward': Array([8.186629, 8.479056], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16974463, 0.11411419], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024247169494629, 'eval/sps': 31807.191409685314}
I0728 06:53:41.482644 139845810259776 train.py:379] starting iteration 95, 186777600 steps, 2473.7377824783325
I0728 06:54:07.241786 139845810259776 train.py:394] {'eval/walltime': 403.83134055137634, 'training/sps': 90646.99553025038, 'training/walltime': 2086.2533025741577, 'training/entropy_loss': Array(0.02920048, dtype=float32), 'training/policy_loss': Array(0.01381823, dtype=float32), 'training/total_loss': Array(2.0446746, dtype=float32), 'training/v_loss': Array(2.001656, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17342137, 0.11805151], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.16421 , 10.229642], dtype=float32), 'eval/episode_reward': Array([10.135206,  9.700689], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15985654, 0.12553716], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.065237045288086, 'eval/sps': 31486.47878931478}
I0728 06:54:07.243775 139845810259776 train.py:379] starting iteration 96, 188743680 steps, 2499.4989132881165
I0728 06:54:32.943920 139845810259776 train.py:394] {'eval/walltime': 407.8722913265228, 'training/sps': 90794.46419875472, 'training/walltime': 2107.9074862003326, 'training/entropy_loss': Array(0.02899473, dtype=float32), 'training/policy_loss': Array(0.01386832, dtype=float32), 'training/total_loss': Array(2.091199, dtype=float32), 'training/v_loss': Array(2.0483358, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17242503, 0.11538181], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.022766, 10.060178], dtype=float32), 'eval/episode_reward': Array([8.302946, 9.055929], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15851092, 0.12328873], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.040950775146484, 'eval/sps': 31675.71374223434}
I0728 06:54:32.945927 139845810259776 train.py:379] starting iteration 97, 190709760 steps, 2525.2010650634766
I0728 06:54:58.647525 139845810259776 train.py:394] {'eval/walltime': 411.93074893951416, 'training/sps': 90859.75172370572, 'training/walltime': 2129.5461101531982, 'training/entropy_loss': Array(0.028436, dtype=float32), 'training/policy_loss': Array(0.01405333, dtype=float32), 'training/total_loss': Array(2.0648828, dtype=float32), 'training/v_loss': Array(2.0223932, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.15633708, 0.0990781 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([12.70533 ,  8.508421], dtype=float32), 'eval/episode_reward': Array([10.452742,  8.694691], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.14248225, 0.1062031 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.058457612991333, 'eval/sps': 31539.075236431047}
I0728 06:54:58.649345 139845810259776 train.py:379] starting iteration 98, 192675840 steps, 2550.904483318329
I0728 06:55:24.320573 139845810259776 train.py:394] {'eval/walltime': 415.9731123447418, 'training/sps': 90922.16575240115, 'training/walltime': 2151.1698801517487, 'training/entropy_loss': Array(0.02807969, dtype=float32), 'training/policy_loss': Array(0.01424082, dtype=float32), 'training/total_loss': Array(2.0415118, dtype=float32), 'training/v_loss': Array(1.9991914, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18308717, 0.12185644], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.9575205, 10.501641 ], dtype=float32), 'eval/episode_reward': Array([8.515312, 9.684912], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.17027959, 0.12880681], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.042363405227661, 'eval/sps': 31664.64445884998}
I0728 06:55:24.323145 139845810259776 train.py:379] starting iteration 99, 194641920 steps, 2576.5782833099365
I0728 06:55:50.050724 139845810259776 train.py:394] {'eval/walltime': 420.0079617500305, 'training/sps': 90653.22363048873, 'training/walltime': 2172.8578016757965, 'training/entropy_loss': Array(0.02751334, dtype=float32), 'training/policy_loss': Array(0.01538808, dtype=float32), 'training/total_loss': Array(1.9177791, dtype=float32), 'training/v_loss': Array(1.8748777, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.15513581, 0.12152749], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([12.52311 , 10.577177], dtype=float32), 'eval/episode_reward': Array([11.265493, 10.021176], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.13867907, 0.13025984], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.034849405288696, 'eval/sps': 31723.612740595338}
I0728 06:55:50.052606 139845810259776 train.py:379] starting iteration 100, 196608000 steps, 2602.307745218277
I0728 06:56:15.731490 139845810259776 train.py:394] {'eval/walltime': 424.0353820323944, 'training/sps': 90824.76537770235, 'training/walltime': 2194.504760980606, 'training/entropy_loss': Array(0.02738892, dtype=float32), 'training/policy_loss': Array(0.01492344, dtype=float32), 'training/total_loss': Array(2.2386823, dtype=float32), 'training/v_loss': Array(2.19637, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17705277, 0.11620228], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.449215, 10.071274], dtype=float32), 'eval/episode_reward': Array([9.543186, 9.510101], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16335547, 0.12382167], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.027420282363892, 'eval/sps': 31782.131246771813}
I0728 06:56:15.788655 139845810259776 train.py:379] starting iteration 101, 198574080 steps, 2628.043785572052
I0728 06:56:41.530850 139845810259776 train.py:394] {'eval/walltime': 428.07173013687134, 'training/sps': 90597.43840911514, 'training/walltime': 2216.2060368061066, 'training/entropy_loss': Array(0.02667942, dtype=float32), 'training/policy_loss': Array(0.0153285, dtype=float32), 'training/total_loss': Array(2.30547, dtype=float32), 'training/v_loss': Array(2.263462, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17155504, 0.12535916], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.814423, 10.911217], dtype=float32), 'eval/episode_reward': Array([ 8.593817 , 10.0127535], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15632068, 0.13378985], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.036348104476929, 'eval/sps': 31711.833738529236}
I0728 06:56:41.532848 139845810259776 train.py:379] starting iteration 102, 200540160 steps, 2653.7879860401154
I0728 06:57:07.252182 139845810259776 train.py:394] {'eval/walltime': 432.1300256252289, 'training/sps': 90792.41391720546, 'training/walltime': 2237.8607094287872, 'training/entropy_loss': Array(0.02425445, dtype=float32), 'training/policy_loss': Array(0.01636361, dtype=float32), 'training/total_loss': Array(2.31075, dtype=float32), 'training/v_loss': Array(2.2701318, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17684922, 0.11986006], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.354395, 10.433099], dtype=float32), 'eval/episode_reward': Array([8.795742, 9.931582], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16327515, 0.12729979], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.058295488357544, 'eval/sps': 31540.335189294856}
I0728 06:57:07.256273 139845810259776 train.py:379] starting iteration 103, 202506240 steps, 2679.5113966464996
I0728 06:57:33.036630 139845810259776 train.py:394] {'eval/walltime': 436.1880576610565, 'training/sps': 90530.83755736578, 'training/walltime': 2259.5779502391815, 'training/entropy_loss': Array(0.02313879, dtype=float32), 'training/policy_loss': Array(0.01610798, dtype=float32), 'training/total_loss': Array(2.3254964, dtype=float32), 'training/v_loss': Array(2.2862496, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17353775, 0.11081319], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.072704 ,  9.6552925], dtype=float32), 'eval/episode_reward': Array([9.171347, 9.460612], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15953824, 0.11873053], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.058032035827637, 'eval/sps': 31542.38282741757}
I0728 06:57:33.038684 139845810259776 train.py:379] starting iteration 104, 204472320 steps, 2705.293822288513
I0728 06:57:58.668909 139845810259776 train.py:394] {'eval/walltime': 440.21043610572815, 'training/sps': 91008.36598567323, 'training/walltime': 2281.181238889694, 'training/entropy_loss': Array(0.02145132, dtype=float32), 'training/policy_loss': Array(0.0159738, dtype=float32), 'training/total_loss': Array(2.4062726, dtype=float32), 'training/v_loss': Array(2.3688476, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17882285, 0.12268903], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.519846, 10.743295], dtype=float32), 'eval/episode_reward': Array([9.572107, 9.806492], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16441037, 0.1311373 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022378444671631, 'eval/sps': 31821.968459869608}
I0728 06:57:58.670901 139845810259776 train.py:379] starting iteration 105, 206438400 steps, 2730.926038980484
I0728 06:58:24.342958 139845810259776 train.py:394] {'eval/walltime': 444.25084590911865, 'training/sps': 90909.26357963882, 'training/walltime': 2302.808077812195, 'training/entropy_loss': Array(0.01966745, dtype=float32), 'training/policy_loss': Array(0.01595737, dtype=float32), 'training/total_loss': Array(2.4556696, dtype=float32), 'training/v_loss': Array(2.420045, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17752609, 0.11661049], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.374176, 10.149275], dtype=float32), 'eval/episode_reward': Array([9.315952, 9.714714], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16367033, 0.12424495], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.040409803390503, 'eval/sps': 31679.954813640197}
I0728 06:58:24.344967 139845810259776 train.py:379] starting iteration 106, 208404480 steps, 2756.6001060009003
I0728 06:58:50.073411 139845810259776 train.py:394] {'eval/walltime': 448.30370903015137, 'training/sps': 90723.8014195945, 'training/walltime': 2324.479127407074, 'training/entropy_loss': Array(0.01780507, dtype=float32), 'training/policy_loss': Array(0.0160786, dtype=float32), 'training/total_loss': Array(2.4549615, dtype=float32), 'training/v_loss': Array(2.421078, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16856602, 0.11194129], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.71848 ,  9.752001], dtype=float32), 'eval/episode_reward': Array([9.027683, 9.456458], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15437743, 0.11975708], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.052863121032715, 'eval/sps': 31582.61115104824}
I0728 06:58:50.075409 139845810259776 train.py:379] starting iteration 107, 210370560 steps, 2782.3305480480194
I0728 06:59:15.790702 139845810259776 train.py:394] {'eval/walltime': 452.3489067554474, 'training/sps': 90747.93232061344, 'training/walltime': 2346.1444144248962, 'training/entropy_loss': Array(0.01400001, dtype=float32), 'training/policy_loss': Array(0.01593668, dtype=float32), 'training/total_loss': Array(2.5225408, dtype=float32), 'training/v_loss': Array(2.4926043, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17050876, 0.11665936], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.819557, 10.147443], dtype=float32), 'eval/episode_reward': Array([8.799648, 9.488663], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15532765, 0.12541202], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0451977252960205, 'eval/sps': 31642.458216460403}
I0728 06:59:15.792715 139845810259776 train.py:379] starting iteration 108, 212336640 steps, 2808.0478534698486
I0728 06:59:41.467848 139845810259776 train.py:394] {'eval/walltime': 456.37745547294617, 'training/sps': 90846.86325383963, 'training/walltime': 2367.7861082553864, 'training/entropy_loss': Array(0.01082955, dtype=float32), 'training/policy_loss': Array(0.01574346, dtype=float32), 'training/total_loss': Array(2.440938, dtype=float32), 'training/v_loss': Array(2.4143653, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17756364, 0.11349118], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.366095,  9.968014], dtype=float32), 'eval/episode_reward': Array([8.783453, 9.490123], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16356385, 0.12172791], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.028548717498779, 'eval/sps': 31773.228766976874}
I0728 06:59:41.469809 139845810259776 train.py:379] starting iteration 109, 214302720 steps, 2833.724947452545
I0728 07:00:07.262200 139845810259776 train.py:394] {'eval/walltime': 460.44798970222473, 'training/sps': 90530.30484323035, 'training/walltime': 2389.503476858139, 'training/entropy_loss': Array(0.00812307, dtype=float32), 'training/policy_loss': Array(0.0161469, dtype=float32), 'training/total_loss': Array(2.4798963, dtype=float32), 'training/v_loss': Array(2.4556262, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19775671, 0.12504782], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.20536 , 10.859542], dtype=float32), 'eval/episode_reward': Array([7.5495176, 9.7568445], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18530595, 0.13255437], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0705342292785645, 'eval/sps': 31445.50390445578}
I0728 07:00:07.264183 139845810259776 train.py:379] starting iteration 110, 216268800 steps, 2859.5193214416504
I0728 07:00:32.902679 139845810259776 train.py:394] {'eval/walltime': 464.48275685310364, 'training/sps': 91025.60053047453, 'training/walltime': 2411.1026751995087, 'training/entropy_loss': Array(0.00550135, dtype=float32), 'training/policy_loss': Array(0.0158267, dtype=float32), 'training/total_loss': Array(2.57614, dtype=float32), 'training/v_loss': Array(2.554812, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16467798, 0.09798265], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.361167,  8.538815], dtype=float32), 'eval/episode_reward': Array([9.614107, 9.542975], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15023205, 0.10638454], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.034767150878906, 'eval/sps': 31724.259471111574}
I0728 07:00:32.904688 139845810259776 train.py:379] starting iteration 111, 218234880 steps, 2885.1598262786865
I0728 07:00:58.660797 139845810259776 train.py:394] {'eval/walltime': 468.5480616092682, 'training/sps': 90661.37327102193, 'training/walltime': 2432.788647174835, 'training/entropy_loss': Array(0.00296121, dtype=float32), 'training/policy_loss': Array(0.01581102, dtype=float32), 'training/total_loss': Array(2.5005274, dtype=float32), 'training/v_loss': Array(2.4817553, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1840289, 0.1174534], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.069223, 10.226766], dtype=float32), 'eval/episode_reward': Array([7.9066396, 8.468666 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.17083547, 0.12530988], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.065304756164551, 'eval/sps': 31485.954357026552}
I0728 07:00:58.662844 139845810259776 train.py:379] starting iteration 112, 220200960 steps, 2910.917982816696
I0728 07:01:24.317460 139845810259776 train.py:394] {'eval/walltime': 472.58109855651855, 'training/sps': 90951.92727999222, 'training/walltime': 2454.405341386795, 'training/entropy_loss': Array(-0.00053408, dtype=float32), 'training/policy_loss': Array(0.01450021, dtype=float32), 'training/total_loss': Array(2.5157053, dtype=float32), 'training/v_loss': Array(2.501739, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17748418, 0.11129281], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.472166,  9.695211], dtype=float32), 'eval/episode_reward': Array([ 8.892076, 10.111463], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16351922, 0.11984029], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.033036947250366, 'eval/sps': 31737.869420528747}
I0728 07:01:24.319483 139845810259776 train.py:379] starting iteration 113, 222167040 steps, 2936.5746207237244
I0728 07:01:50.097390 139845810259776 train.py:394] {'eval/walltime': 476.62193775177, 'training/sps': 90467.84288030461, 'training/walltime': 2476.137704372406, 'training/entropy_loss': Array(-0.00355754, dtype=float32), 'training/policy_loss': Array(0.01429135, dtype=float32), 'training/total_loss': Array(2.6665168, dtype=float32), 'training/v_loss': Array(2.6557832, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1816749 , 0.12570807], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.772498, 10.949182], dtype=float32), 'eval/episode_reward': Array([8.55592 , 9.457696], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16767879, 0.13356616], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.040839195251465, 'eval/sps': 31676.588405303875}
I0728 07:01:50.099499 139845810259776 train.py:379] starting iteration 114, 224133120 steps, 2962.3546364307404
I0728 07:02:15.750662 139845810259776 train.py:394] {'eval/walltime': 480.656845331192, 'training/sps': 90975.36472552308, 'training/walltime': 2497.748829603195, 'training/entropy_loss': Array(-0.00548208, dtype=float32), 'training/policy_loss': Array(0.01465923, dtype=float32), 'training/total_loss': Array(2.6657875, dtype=float32), 'training/v_loss': Array(2.65661, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1746709 , 0.11257564], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.191082,  9.813978], dtype=float32), 'eval/episode_reward': Array([9.443714, 9.272772], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16103317, 0.12052777], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.034907579421997, 'eval/sps': 31723.155358699958}
I0728 07:02:15.752704 139845810259776 train.py:379] starting iteration 115, 226099200 steps, 2988.007843017578
I0728 07:02:41.473034 139845810259776 train.py:394] {'eval/walltime': 484.7181375026703, 'training/sps': 90794.94904240019, 'training/walltime': 2519.4028975963593, 'training/entropy_loss': Array(-0.00819995, dtype=float32), 'training/policy_loss': Array(0.01511483, dtype=float32), 'training/total_loss': Array(2.6464205, dtype=float32), 'training/v_loss': Array(2.6395054, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17711318, 0.11174686], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.395292 ,  9.7197275], dtype=float32), 'eval/episode_reward': Array([9.372316, 9.749612], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16354372, 0.11981191], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0612921714782715, 'eval/sps': 31517.06269716843}
I0728 07:02:41.475036 139845810259776 train.py:379] starting iteration 116, 228065280 steps, 3013.7301745414734
I0728 07:03:07.129617 139845810259776 train.py:394] {'eval/walltime': 488.74035143852234, 'training/sps': 90905.37420076046, 'training/walltime': 2541.0306618213654, 'training/entropy_loss': Array(-0.00999524, dtype=float32), 'training/policy_loss': Array(0.0153309, dtype=float32), 'training/total_loss': Array(2.652343, dtype=float32), 'training/v_loss': Array(2.6470075, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16650549, 0.10521057], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.533606,  9.110689], dtype=float32), 'eval/episode_reward': Array([8.786767, 9.038062], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.153029  , 0.11261267], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022213935852051, 'eval/sps': 31823.2699805126}
I0728 07:03:07.131669 139845810259776 train.py:379] starting iteration 117, 230031360 steps, 3039.3868069648743
I0728 07:03:32.906570 139845810259776 train.py:394] {'eval/walltime': 492.7930152416229, 'training/sps': 90530.97272463613, 'training/walltime': 2562.747870206833, 'training/entropy_loss': Array(-0.01188077, dtype=float32), 'training/policy_loss': Array(0.0138861, dtype=float32), 'training/total_loss': Array(2.6902356, dtype=float32), 'training/v_loss': Array(2.68823, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1808675 , 0.12308562], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.733101, 10.682118], dtype=float32), 'eval/episode_reward': Array([ 9.86441 , 10.245941], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16772625, 0.13078383], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.052663803100586, 'eval/sps': 31584.164445634644}
I0728 07:03:32.908816 139845810259776 train.py:379] starting iteration 118, 231997440 steps, 3065.163954257965
I0728 07:03:58.612172 139845810259776 train.py:394] {'eval/walltime': 496.83548951148987, 'training/sps': 90786.12369947947, 'training/walltime': 2584.404043197632, 'training/entropy_loss': Array(-0.0134786, dtype=float32), 'training/policy_loss': Array(0.01434173, dtype=float32), 'training/total_loss': Array(2.7895422, dtype=float32), 'training/v_loss': Array(2.788679, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16506082, 0.10997501], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.381058,  9.586128], dtype=float32), 'eval/episode_reward': Array([9.229338, 9.17188 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15176383, 0.11689692], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.042474269866943, 'eval/sps': 31663.776057679912}
I0728 07:03:58.614164 139845810259776 train.py:379] starting iteration 119, 233963520 steps, 3090.869302749634
I0728 07:04:24.305491 139845810259776 train.py:394] {'eval/walltime': 500.8877100944519, 'training/sps': 90876.91099375008, 'training/walltime': 2606.0385813713074, 'training/entropy_loss': Array(-0.01470002, dtype=float32), 'training/policy_loss': Array(0.01468217, dtype=float32), 'training/total_loss': Array(2.799311, dtype=float32), 'training/v_loss': Array(2.7993288, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18077771, 0.11030743], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.824598,  9.553476], dtype=float32), 'eval/episode_reward': Array([ 8.569187, 10.53583 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1680353 , 0.11762451], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.052220582962036, 'eval/sps': 31587.61902996809}
I0728 07:04:24.307443 139845810259776 train.py:379] starting iteration 120, 235929600 steps, 3116.562582015991
I0728 07:04:49.949807 139845810259776 train.py:394] {'eval/walltime': 504.91270542144775, 'training/sps': 90969.5850160621, 'training/walltime': 2627.6510796546936, 'training/entropy_loss': Array(-0.0161571, dtype=float32), 'training/policy_loss': Array(0.01528159, dtype=float32), 'training/total_loss': Array(2.837425, dtype=float32), 'training/v_loss': Array(2.8383002, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19038919, 0.12084132], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.576622, 10.463303], dtype=float32), 'eval/episode_reward': Array([8.861977, 9.699526], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.17818254, 0.12820017], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.02499532699585, 'eval/sps': 31801.279157145214}
I0728 07:04:49.951922 139845810259776 train.py:379] starting iteration 121, 237895680 steps, 3142.2070598602295
I0728 07:05:15.702895 139845810259776 train.py:394] {'eval/walltime': 508.95649313926697, 'training/sps': 90593.8702615897, 'training/walltime': 2649.3532102108, 'training/entropy_loss': Array(-0.01720356, dtype=float32), 'training/policy_loss': Array(0.01543725, dtype=float32), 'training/total_loss': Array(2.9336655, dtype=float32), 'training/v_loss': Array(2.9354317, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.181795 , 0.1167388], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.846251, 10.096817], dtype=float32), 'eval/episode_reward': Array([8.004103 , 9.7699375], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16899145, 0.12380912], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.043787717819214, 'eval/sps': 31653.491461967616}
I0728 07:05:15.704879 139845810259776 train.py:379] starting iteration 122, 239861760 steps, 3167.9600179195404
I0728 07:05:41.420378 139845810259776 train.py:394] {'eval/walltime': 512.9948952198029, 'training/sps': 90718.72328530307, 'training/walltime': 2671.02547287941, 'training/entropy_loss': Array(-0.01807183, dtype=float32), 'training/policy_loss': Array(0.01615202, dtype=float32), 'training/total_loss': Array(2.9144263, dtype=float32), 'training/v_loss': Array(2.9163463, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17354521, 0.12254698], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.203941, 10.629664], dtype=float32), 'eval/episode_reward': Array([ 9.319273, 10.337601], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16057388, 0.12924889], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.038402080535889, 'eval/sps': 31695.704748402524}
I0728 07:05:41.422370 139845810259776 train.py:379] starting iteration 123, 241827840 steps, 3193.67750787735
I0728 07:06:07.096715 139845810259776 train.py:394] {'eval/walltime': 517.0235240459442, 'training/sps': 90849.90386642196, 'training/walltime': 2692.6664423942566, 'training/entropy_loss': Array(-0.01885859, dtype=float32), 'training/policy_loss': Array(0.01640942, dtype=float32), 'training/total_loss': Array(2.8963904, dtype=float32), 'training/v_loss': Array(2.8988395, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16764516, 0.10917479], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.633344,  9.496712], dtype=float32), 'eval/episode_reward': Array([8.620981, 9.91078 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15382232, 0.11682265], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.028628826141357, 'eval/sps': 31772.596961383286}
I0728 07:06:07.098706 139845810259776 train.py:379] starting iteration 124, 243793920 steps, 3219.3538439273834
I0728 07:06:32.782155 139845810259776 train.py:394] {'eval/walltime': 521.0596747398376, 'training/sps': 90842.38477617037, 'training/walltime': 2714.309203147888, 'training/entropy_loss': Array(-0.01938695, dtype=float32), 'training/policy_loss': Array(0.01741676, dtype=float32), 'training/total_loss': Array(2.8495727, dtype=float32), 'training/v_loss': Array(2.851543, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18316546, 0.12426705], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.004366, 10.760195], dtype=float32), 'eval/episode_reward': Array([ 7.84521 , 10.559783], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.17004333, 0.13183387], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.036150693893433, 'eval/sps': 31713.38478359094}
I0728 07:06:32.784228 139845810259776 train.py:379] starting iteration 125, 245760000 steps, 3245.039366006851
I0728 07:06:58.514128 139845810259776 train.py:394] {'eval/walltime': 525.1145541667938, 'training/sps': 90727.69424234967, 'training/walltime': 2735.979322910309, 'training/entropy_loss': Array(-0.01887825, dtype=float32), 'training/policy_loss': Array(0.0193113, dtype=float32), 'training/total_loss': Array(3.257286, dtype=float32), 'training/v_loss': Array(3.2568529, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17752421, 0.11681585], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.503269, 10.158909], dtype=float32), 'eval/episode_reward': Array([8.482946, 9.459606], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16385983, 0.12469333], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.054879426956177, 'eval/sps': 31566.90656424378}
I0728 07:06:58.583058 139845810259776 train.py:379] starting iteration 126, 247726080 steps, 3270.838194847107
I0728 07:07:24.265665 139845810259776 train.py:394] {'eval/walltime': 529.1702499389648, 'training/sps': 90928.48787068976, 'training/walltime': 2757.6015894412994, 'training/entropy_loss': Array(-0.01880516, dtype=float32), 'training/policy_loss': Array(0.02040145, dtype=float32), 'training/total_loss': Array(3.2661448, dtype=float32), 'training/v_loss': Array(3.2645485, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.177993  , 0.12564625], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.538525, 10.899701], dtype=float32), 'eval/episode_reward': Array([7.6212897, 9.815539 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16458726, 0.13292553], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0556957721710205, 'eval/sps': 31560.55266233182}
I0728 07:07:24.267781 139845810259776 train.py:379] starting iteration 127, 249692160 steps, 3296.5229189395905
I0728 07:07:49.970801 139845810259776 train.py:394] {'eval/walltime': 533.2108862400055, 'training/sps': 90780.36900876129, 'training/walltime': 2779.259135246277, 'training/entropy_loss': Array(-0.01838431, dtype=float32), 'training/policy_loss': Array(0.02119748, dtype=float32), 'training/total_loss': Array(3.2384453, dtype=float32), 'training/v_loss': Array(3.235632, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1771321 , 0.12584634], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.475477, 10.9638  ], dtype=float32), 'eval/episode_reward': Array([8.676331, 9.745775], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16341667, 0.1335603 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.040636301040649, 'eval/sps': 31678.178995480022}
I0728 07:07:49.972805 139845810259776 train.py:379] starting iteration 128, 251658240 steps, 3322.22794342041
I0728 07:08:15.688202 139845810259776 train.py:394] {'eval/walltime': 537.2666573524475, 'training/sps': 90791.13641170447, 'training/walltime': 2800.9141125679016, 'training/entropy_loss': Array(-0.01698822, dtype=float32), 'training/policy_loss': Array(0.02263896, dtype=float32), 'training/total_loss': Array(3.203591, dtype=float32), 'training/v_loss': Array(3.1979403, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16988775, 0.12134737], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.828102, 10.51473 ], dtype=float32), 'eval/episode_reward': Array([ 8.806444, 10.729018], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15573381, 0.12898946], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.055771112442017, 'eval/sps': 31559.966391429334}
I0728 07:08:15.690180 139845810259776 train.py:379] starting iteration 129, 253624320 steps, 3347.945318698883
I0728 07:08:41.419692 139845810259776 train.py:394] {'eval/walltime': 541.3103740215302, 'training/sps': 90680.90966937537, 'training/walltime': 2822.595412492752, 'training/entropy_loss': Array(-0.0163246, dtype=float32), 'training/policy_loss': Array(0.02472704, dtype=float32), 'training/total_loss': Array(3.179831, dtype=float32), 'training/v_loss': Array(3.1714287, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18171892, 0.13436137], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.880103, 11.632216], dtype=float32), 'eval/episode_reward': Array([ 8.541855, 10.779642], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.168897 , 0.1410252], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.043716669082642, 'eval/sps': 31654.047618780893}
I0728 07:08:41.421664 139845810259776 train.py:379] starting iteration 130, 255590400 steps, 3373.676802635193
I0728 07:09:07.121056 139845810259776 train.py:394] {'eval/walltime': 545.3704972267151, 'training/sps': 90875.82639450862, 'training/walltime': 2844.230208873749, 'training/entropy_loss': Array(-0.01512145, dtype=float32), 'training/policy_loss': Array(0.0278654, dtype=float32), 'training/total_loss': Array(3.216107, dtype=float32), 'training/v_loss': Array(3.203363, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17591205, 0.1152836 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.431029,  9.963803], dtype=float32), 'eval/episode_reward': Array([ 8.064571, 10.273989], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16353679, 0.12172011], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0601232051849365, 'eval/sps': 31526.13690060907}
I0728 07:09:07.123085 139845810259776 train.py:379] starting iteration 131, 257556480 steps, 3399.3782227039337
I0728 07:09:32.815125 139845810259776 train.py:394] {'eval/walltime': 549.4187352657318, 'training/sps': 90857.26002808945, 'training/walltime': 2865.8694262504578, 'training/entropy_loss': Array(-0.01316384, dtype=float32), 'training/policy_loss': Array(0.02736508, dtype=float32), 'training/total_loss': Array(3.192797, dtype=float32), 'training/v_loss': Array(3.1785958, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16261658, 0.1048079 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.264353,  9.072313], dtype=float32), 'eval/episode_reward': Array([ 9.573273, 10.369247], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.14904034, 0.11215369], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.048238039016724, 'eval/sps': 31618.694050681348}
I0728 07:09:32.817037 139845810259776 train.py:379] starting iteration 132, 259522560 steps, 3425.0721759796143
I0728 07:09:58.501445 139845810259776 train.py:394] {'eval/walltime': 553.4723348617554, 'training/sps': 90911.17882767651, 'training/walltime': 2887.4958095550537, 'training/entropy_loss': Array(-0.0108074, dtype=float32), 'training/policy_loss': Array(0.02989587, dtype=float32), 'training/total_loss': Array(3.1355062, dtype=float32), 'training/v_loss': Array(3.1164174, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18808809, 0.1235385 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.480812, 10.68369 ], dtype=float32), 'eval/episode_reward': Array([ 8.606039, 10.451814], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.17568703, 0.13098797], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.05359959602356, 'eval/sps': 31576.873089676534}
I0728 07:09:58.503427 139845810259776 train.py:379] starting iteration 133, 261488640 steps, 3450.7585666179657
I0728 07:10:24.121841 139845810259776 train.py:394] {'eval/walltime': 557.5368003845215, 'training/sps': 91234.91585754216, 'training/walltime': 2909.0454540252686, 'training/entropy_loss': Array(-0.00777098, dtype=float32), 'training/policy_loss': Array(0.03260674, dtype=float32), 'training/total_loss': Array(3.086558, dtype=float32), 'training/v_loss': Array(3.0617225, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1830667 , 0.12763825], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.0756445, 11.043022 ], dtype=float32), 'eval/episode_reward': Array([ 7.7925816, 10.5426445], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16908407, 0.13603225], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.064465522766113, 'eval/sps': 31492.45559669265}
I0728 07:10:24.123783 139845810259776 train.py:379] starting iteration 134, 263454720 steps, 3476.378921985626
I0728 07:10:49.598196 139845810259776 train.py:394] {'eval/walltime': 561.5594716072083, 'training/sps': 91671.87010527137, 'training/walltime': 2930.4923820495605, 'training/entropy_loss': Array(-0.00457283, dtype=float32), 'training/policy_loss': Array(0.03532097, dtype=float32), 'training/total_loss': Array(3.1156452, dtype=float32), 'training/v_loss': Array(3.084897, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17028451, 0.12132766], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.978691, 10.455592], dtype=float32), 'eval/episode_reward': Array([ 8.325417, 10.763646], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15659307, 0.12842585], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022671222686768, 'eval/sps': 31819.65239369177}
I0728 07:10:49.600207 139845810259776 train.py:379] starting iteration 135, 265420800 steps, 3501.8553454875946
I0728 07:11:14.969285 139845810259776 train.py:394] {'eval/walltime': 565.5834593772888, 'training/sps': 92130.43223404525, 'training/walltime': 2951.832561969757, 'training/entropy_loss': Array(-0.00194931, dtype=float32), 'training/policy_loss': Array(0.03651101, dtype=float32), 'training/total_loss': Array(3.041307, dtype=float32), 'training/v_loss': Array(3.006745, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1609189 , 0.11398647], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.148462,  9.890555], dtype=float32), 'eval/episode_reward': Array([ 9.1223135, 10.651952 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.14653827, 0.12149832], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023987770080566, 'eval/sps': 31809.241805284422}
I0728 07:11:14.971288 139845810259776 train.py:379] starting iteration 136, 267386880 steps, 3527.2264275550842
I0728 07:11:40.507994 139845810259776 train.py:394] {'eval/walltime': 569.6067142486572, 'training/sps': 91408.28624153486, 'training/walltime': 2973.341334104538, 'training/entropy_loss': Array(0.00104159, dtype=float32), 'training/policy_loss': Array(0.04524737, dtype=float32), 'training/total_loss': Array(3.0278573, dtype=float32), 'training/v_loss': Array(2.9815686, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17073871, 0.12212297], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.931042, 10.552685], dtype=float32), 'eval/episode_reward': Array([ 9.663137, 10.421202], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15690467, 0.12939458], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023254871368408, 'eval/sps': 31815.036355492946}
I0728 07:11:40.510019 139845810259776 train.py:379] starting iteration 137, 269352960 steps, 3552.7651579380035
I0728 07:12:06.038337 139845810259776 train.py:394] {'eval/walltime': 573.654844045639, 'training/sps': 91549.19582571389, 'training/walltime': 2994.8170006275177, 'training/entropy_loss': Array(0.004597, dtype=float32), 'training/policy_loss': Array(0.04232918, dtype=float32), 'training/total_loss': Array(3.048761, dtype=float32), 'training/v_loss': Array(3.0018346, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.15945245, 0.11629327], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.031556, 10.055706], dtype=float32), 'eval/episode_reward': Array([9.805716, 9.934363], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.14568377, 0.12338244], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0481297969818115, 'eval/sps': 31619.53949585157}
I0728 07:12:06.040362 139845810259776 train.py:379] starting iteration 138, 271319040 steps, 3578.295501232147
I0728 07:12:31.621472 139845810259776 train.py:394] {'eval/walltime': 577.6909916400909, 'training/sps': 91273.4082016219, 'training/walltime': 3016.3575570583344, 'training/entropy_loss': Array(0.00710416, dtype=float32), 'training/policy_loss': Array(0.04330945, dtype=float32), 'training/total_loss': Array(3.01627, dtype=float32), 'training/v_loss': Array(2.965856, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18204518, 0.12564714], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.998029, 10.918102], dtype=float32), 'eval/episode_reward': Array([ 9.282573, 11.235755], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16947633, 0.13256097], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.036147594451904, 'eval/sps': 31713.409136957485}
I0728 07:12:31.623445 139845810259776 train.py:379] starting iteration 139, 273285120 steps, 3603.878583908081
I0728 07:12:57.148612 139845810259776 train.py:394] {'eval/walltime': 581.723028421402, 'training/sps': 91493.78384542147, 'training/walltime': 3037.84623003006, 'training/entropy_loss': Array(0.00914657, dtype=float32), 'training/policy_loss': Array(0.04407302, dtype=float32), 'training/total_loss': Array(3.0695162, dtype=float32), 'training/v_loss': Array(3.0162966, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18989661, 0.11231015], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.686425,  9.617451], dtype=float32), 'eval/episode_reward': Array([8.7379875, 9.185137 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.17956412, 0.1175447 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.032036781311035, 'eval/sps': 31745.742150293634}
I0728 07:12:57.150553 139845810259776 train.py:379] starting iteration 140, 275251200 steps, 3629.405692100525
I0728 07:13:22.751801 139845810259776 train.py:394] {'eval/walltime': 585.7503798007965, 'training/sps': 91151.90312516321, 'training/walltime': 3059.4154999256134, 'training/entropy_loss': Array(0.01188855, dtype=float32), 'training/policy_loss': Array(0.04749128, dtype=float32), 'training/total_loss': Array(3.1588159, dtype=float32), 'training/v_loss': Array(3.099436, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.15913767, 0.11627148], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([12.932194, 10.084338], dtype=float32), 'eval/episode_reward': Array([8.3544655, 9.208883 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.14501598, 0.12318172], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.027351379394531, 'eval/sps': 31782.67499947904}
I0728 07:13:22.753727 139845810259776 train.py:379] starting iteration 141, 277217280 steps, 3655.0088658332825
I0728 07:13:48.440073 139845810259776 train.py:394] {'eval/walltime': 589.792676448822, 'training/sps': 90857.554339377, 'training/walltime': 3081.05464720726, 'training/entropy_loss': Array(0.01434202, dtype=float32), 'training/policy_loss': Array(0.04852168, dtype=float32), 'training/total_loss': Array(3.2447999, dtype=float32), 'training/v_loss': Array(3.181936, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17758623, 0.12163316], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.540242, 10.520631], dtype=float32), 'eval/episode_reward': Array([8.573414, 9.90346 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16401708, 0.12915571], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.042296648025513, 'eval/sps': 31665.16739005844}
I0728 07:13:48.441891 139845810259776 train.py:379] starting iteration 142, 279183360 steps, 3680.6970303058624
I0728 07:14:14.093388 139845810259776 train.py:394] {'eval/walltime': 593.8138179779053, 'training/sps': 90913.18135540608, 'training/walltime': 3102.680554151535, 'training/entropy_loss': Array(0.01501602, dtype=float32), 'training/policy_loss': Array(0.04880788, dtype=float32), 'training/total_loss': Array(3.4467483, dtype=float32), 'training/v_loss': Array(3.3829243, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.21274912, 0.10812667], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.683899,  9.322941], dtype=float32), 'eval/episode_reward': Array([6.6029906, 9.452379 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20396024, 0.11263382], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021141529083252, 'eval/sps': 31831.756995924912}
I0728 07:14:14.095242 139845810259776 train.py:379] starting iteration 143, 281149440 steps, 3706.3503801822662
I0728 07:14:39.790620 139845810259776 train.py:394] {'eval/walltime': 597.8460898399353, 'training/sps': 90776.64254113128, 'training/walltime': 3124.338989019394, 'training/entropy_loss': Array(0.01446433, dtype=float32), 'training/policy_loss': Array(0.04378843, dtype=float32), 'training/total_loss': Array(3.639524, dtype=float32), 'training/v_loss': Array(3.5812712, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.22588658, 0.12178192], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.70219, 10.5931 ], dtype=float32), 'eval/episode_reward': Array([4.3364806, 8.30696  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.21622324, 0.1279882 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.032271862030029, 'eval/sps': 31743.891379277928}
I0728 07:14:39.792543 139845810259776 train.py:379] starting iteration 144, 283115520 steps, 3732.0476820468903
I0728 07:15:05.508194 139845810259776 train.py:394] {'eval/walltime': 601.8962426185608, 'training/sps': 90767.66589847884, 'training/walltime': 3145.9995658397675, 'training/entropy_loss': Array(0.01334007, dtype=float32), 'training/policy_loss': Array(0.04501292, dtype=float32), 'training/total_loss': Array(3.7721424, dtype=float32), 'training/v_loss': Array(3.7137895, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2264713 , 0.12280802], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.828032, 10.667022], dtype=float32), 'eval/episode_reward': Array([5.6192055, 8.826865 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.21707374, 0.12853971], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.050152778625488, 'eval/sps': 31603.746079781147}
I0728 07:15:05.510162 139845810259776 train.py:379] starting iteration 145, 285081600 steps, 3757.7653000354767
I0728 07:15:31.303411 139845810259776 train.py:394] {'eval/walltime': 605.9555168151855, 'training/sps': 90480.24081462594, 'training/walltime': 3167.7289509773254, 'training/entropy_loss': Array(0.01278664, dtype=float32), 'training/policy_loss': Array(0.04192069, dtype=float32), 'training/total_loss': Array(3.9759908, dtype=float32), 'training/v_loss': Array(3.9212832, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23188879, 0.11413506], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.301018,  9.937661], dtype=float32), 'eval/episode_reward': Array([3.9689817, 8.015247 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22272025, 0.11975794], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.059274196624756, 'eval/sps': 31532.730680383866}
I0728 07:15:31.305259 139845810259776 train.py:379] starting iteration 146, 287047680 steps, 3783.5603981018066
I0728 07:15:56.948832 139845810259776 train.py:394] {'eval/walltime': 609.9912893772125, 'training/sps': 91007.3425257438, 'training/walltime': 3189.3324825763702, 'training/entropy_loss': Array(0.01209268, dtype=float32), 'training/policy_loss': Array(0.04369503, dtype=float32), 'training/total_loss': Array(4.095177, dtype=float32), 'training/v_loss': Array(4.039389, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23670478, 0.10325572], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.732595,  8.963782], dtype=float32), 'eval/episode_reward': Array([3.781592, 8.458032], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22880118, 0.10747798], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0357725620269775, 'eval/sps': 31716.35617040611}
I0728 07:15:56.950716 139845810259776 train.py:379] starting iteration 147, 289013760 steps, 3809.2058544158936
I0728 07:16:22.560040 139845810259776 train.py:394] {'eval/walltime': 614.0238811969757, 'training/sps': 91139.54407003077, 'training/walltime': 3210.9046773910522, 'training/entropy_loss': Array(0.01112836, dtype=float32), 'training/policy_loss': Array(0.0391243, dtype=float32), 'training/total_loss': Array(4.1523256, dtype=float32), 'training/v_loss': Array(4.102073, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.24814138, 0.11240927], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.698013,  9.727247], dtype=float32), 'eval/episode_reward': Array([2.7564478, 7.8054056], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24039769, 0.11665912], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.032591819763184, 'eval/sps': 31741.372725275447}
I0728 07:16:22.562689 139845810259776 train.py:379] starting iteration 148, 290979840 steps, 3834.8178272247314
I0728 07:16:48.273380 139845810259776 train.py:394] {'eval/walltime': 618.0774505138397, 'training/sps': 90808.25177290465, 'training/walltime': 3232.5555732250214, 'training/entropy_loss': Array(0.0080419, dtype=float32), 'training/policy_loss': Array(0.03457104, dtype=float32), 'training/total_loss': Array(4.258775, dtype=float32), 'training/v_loss': Array(4.2161627, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26740634, 0.12110521], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.421246, 10.465648], dtype=float32), 'eval/episode_reward': Array([1.1937239, 8.921838 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2604273 , 0.12514944], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.053569316864014, 'eval/sps': 31577.10896110329}
I0728 07:16:48.277114 139845810259776 train.py:379] starting iteration 149, 292945920 steps, 3860.5322394371033
I0728 07:17:13.950223 139845810259776 train.py:394] {'eval/walltime': 622.1028327941895, 'training/sps': 90843.36149700213, 'training/walltime': 3254.1981012821198, 'training/entropy_loss': Array(0.00451981, dtype=float32), 'training/policy_loss': Array(0.02810247, dtype=float32), 'training/total_loss': Array(4.365345, dtype=float32), 'training/v_loss': Array(4.3327227, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27668864, 0.12452822], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.218311, 10.770621], dtype=float32), 'eval/episode_reward': Array([0.9852867, 8.405716 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26955974, 0.12930909], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0253822803497314, 'eval/sps': 31798.222152674443}
I0728 07:17:13.952233 139845810259776 train.py:379] starting iteration 150, 294912000 steps, 3886.207371711731
I0728 07:17:39.657184 139845810259776 train.py:394] {'eval/walltime': 626.1485605239868, 'training/sps': 90791.88111906998, 'training/walltime': 3275.852900981903, 'training/entropy_loss': Array(0.0014984, dtype=float32), 'training/policy_loss': Array(0.01605498, dtype=float32), 'training/total_loss': Array(4.6871724, dtype=float32), 'training/v_loss': Array(4.669619, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27281877, 0.12042244], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.897785, 10.467338], dtype=float32), 'eval/episode_reward': Array([-0.11071593,  8.463191  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2657395 , 0.12497745], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.045727729797363, 'eval/sps': 31638.31294361746}
I0728 07:17:39.725058 139845810259776 train.py:379] starting iteration 151, 296878080 steps, 3911.980194091797
I0728 07:18:05.312647 139845810259776 train.py:394] {'eval/walltime': 630.2186303138733, 'training/sps': 91398.7841014027, 'training/walltime': 3297.3639092445374, 'training/entropy_loss': Array(-0.00203089, dtype=float32), 'training/policy_loss': Array(0.01012066, dtype=float32), 'training/total_loss': Array(4.770534, dtype=float32), 'training/v_loss': Array(4.762444, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2713822 , 0.11288802], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.729507,  9.784467], dtype=float32), 'eval/episode_reward': Array([-1.0418278,  8.720542 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26429784, 0.11753836], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.070069789886475, 'eval/sps': 31449.092179711806}
I0728 07:18:05.315925 139845810259776 train.py:379] starting iteration 152, 298844160 steps, 3937.571063041687
I0728 07:18:30.814720 139845810259776 train.py:394] {'eval/walltime': 634.2455310821533, 'training/sps': 91584.90739690697, 'training/walltime': 3318.8312017917633, 'training/entropy_loss': Array(-0.00567622, dtype=float32), 'training/policy_loss': Array(0.00845625, dtype=float32), 'training/total_loss': Array(4.792218, dtype=float32), 'training/v_loss': Array(4.7894382, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29524902, 0.12207642], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.812782, 10.579419], dtype=float32), 'eval/episode_reward': Array([-0.8461597,  7.8056283], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28912959, 0.12552296], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.026900768280029, 'eval/sps': 31786.231488061076}
I0728 07:18:30.817720 139845810259776 train.py:379] starting iteration 153, 300810240 steps, 3963.0728418827057
I0728 07:18:56.453096 139845810259776 train.py:394] {'eval/walltime': 638.2952463626862, 'training/sps': 91104.4868983273, 'training/walltime': 3340.411697626114, 'training/entropy_loss': Array(-0.00934965, dtype=float32), 'training/policy_loss': Array(0.00718856, dtype=float32), 'training/total_loss': Array(4.827945, dtype=float32), 'training/v_loss': Array(4.8301063, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29784295, 0.13143058], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.029   , 11.427894], dtype=float32), 'eval/episode_reward': Array([-0.295241,  9.305655], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29152474, 0.13530086], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.049715280532837, 'eval/sps': 31607.160289836112}
I0728 07:18:56.455006 139845810259776 train.py:379] starting iteration 154, 302776320 steps, 3988.710144519806
I0728 07:19:22.109256 139845810259776 train.py:394] {'eval/walltime': 642.3481175899506, 'training/sps': 91036.11162270985, 'training/walltime': 3362.008402109146, 'training/entropy_loss': Array(-0.01273447, dtype=float32), 'training/policy_loss': Array(0.00584445, dtype=float32), 'training/total_loss': Array(4.8071337, dtype=float32), 'training/v_loss': Array(4.814024, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2871312 , 0.12326012], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.114433, 10.673602], dtype=float32), 'eval/episode_reward': Array([-1.1187661,  8.121487 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28075784, 0.1269115 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.052871227264404, 'eval/sps': 31582.547982013504}
I0728 07:19:22.111184 139845810259776 train.py:379] starting iteration 155, 304742400 steps, 4014.3663222789764
I0728 07:19:47.816705 139845810259776 train.py:394] {'eval/walltime': 646.3816306591034, 'training/sps': 90739.41365523476, 'training/walltime': 3383.6757230758667, 'training/entropy_loss': Array(-0.0173781, dtype=float32), 'training/policy_loss': Array(0.0046053, dtype=float32), 'training/total_loss': Array(4.8230877, dtype=float32), 'training/v_loss': Array(4.8358607, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27798522, 0.12902297], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.338684, 11.13804 ], dtype=float32), 'eval/episode_reward': Array([0.28214926, 8.463358  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27103794, 0.13358317], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.033513069152832, 'eval/sps': 31734.123035055425}
I0728 07:19:47.818634 139845810259776 train.py:379] starting iteration 156, 306708480 steps, 4040.0737733840942
I0728 07:20:13.452902 139845810259776 train.py:394] {'eval/walltime': 650.4267904758453, 'training/sps': 91089.18949654912, 'training/walltime': 3405.259843111038, 'training/entropy_loss': Array(-0.02063182, dtype=float32), 'training/policy_loss': Array(0.00386701, dtype=float32), 'training/total_loss': Array(4.7934513, dtype=float32), 'training/v_loss': Array(4.810216, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25966913, 0.12090644], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.739525, 10.472893], dtype=float32), 'eval/episode_reward': Array([0.18687278, 8.615126  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2519998 , 0.12574933], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.045159816741943, 'eval/sps': 31642.7547485859}
I0728 07:20:13.454827 139845810259776 train.py:379] starting iteration 157, 308674560 steps, 4065.70996594429
I0728 07:20:39.174674 139845810259776 train.py:394] {'eval/walltime': 654.4666492938995, 'training/sps': 90707.6088661291, 'training/walltime': 3426.934761285782, 'training/entropy_loss': Array(-0.02259192, dtype=float32), 'training/policy_loss': Array(0.00407425, dtype=float32), 'training/total_loss': Array(5.0917854, dtype=float32), 'training/v_loss': Array(5.110303, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25133246, 0.1147542 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.067266,  9.938109], dtype=float32), 'eval/episode_reward': Array([1.8195951, 8.225665 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.244289  , 0.11841752], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.039858818054199, 'eval/sps': 31684.275556355034}
I0728 07:20:39.176657 139845810259776 train.py:379] starting iteration 158, 310640640 steps, 4091.431795358658
I0728 07:21:04.926780 139845810259776 train.py:394] {'eval/walltime': 658.5272998809814, 'training/sps': 90665.11320763077, 'training/walltime': 3448.6198387145996, 'training/entropy_loss': Array(-0.02396033, dtype=float32), 'training/policy_loss': Array(0.00447517, dtype=float32), 'training/total_loss': Array(5.099551, dtype=float32), 'training/v_loss': Array(5.119036, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.22124915, 0.11171729], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.381252,  9.661953], dtype=float32), 'eval/episode_reward': Array([5.21924 , 9.665174], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2125496 , 0.11669354], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.060650587081909, 'eval/sps': 31522.04240552108}
I0728 07:21:04.928696 139845810259776 train.py:379] starting iteration 159, 312606720 steps, 4117.183834075928
I0728 07:21:30.586988 139845810259776 train.py:394] {'eval/walltime': 662.5772788524628, 'training/sps': 91005.90630577932, 'training/walltime': 3470.2237112522125, 'training/entropy_loss': Array(-0.02361789, dtype=float32), 'training/policy_loss': Array(0.00531522, dtype=float32), 'training/total_loss': Array(5.0228705, dtype=float32), 'training/v_loss': Array(5.041173, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2183229, 0.1296433], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.249376, 11.137862], dtype=float32), 'eval/episode_reward': Array([4.739337, 9.906315], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.209514  , 0.13443556], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.049978971481323, 'eval/sps': 31605.10237246556}
I0728 07:21:30.588904 139845810259776 train.py:379] starting iteration 160, 314572800 steps, 4142.844042301178
I0728 07:21:56.251901 139845810259776 train.py:394] {'eval/walltime': 666.6130163669586, 'training/sps': 90927.0280710558, 'training/walltime': 3491.8463249206543, 'training/entropy_loss': Array(-0.0235616, dtype=float32), 'training/policy_loss': Array(0.00615789, dtype=float32), 'training/total_loss': Array(5.004779, dtype=float32), 'training/v_loss': Array(5.0221825, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.21690801, 0.12036436], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.041225, 10.316728], dtype=float32), 'eval/episode_reward': Array([5.6056757, 9.9838915], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20841095, 0.12514351], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.03573751449585, 'eval/sps': 31716.631604568058}
I0728 07:21:56.253738 139845810259776 train.py:379] starting iteration 161, 316538880 steps, 4168.5088765621185
I0728 07:22:21.930169 139845810259776 train.py:394] {'eval/walltime': 670.6491017341614, 'training/sps': 90871.51329065507, 'training/walltime': 3513.482148170471, 'training/entropy_loss': Array(-0.02311306, dtype=float32), 'training/policy_loss': Array(0.00665658, dtype=float32), 'training/total_loss': Array(5.051283, dtype=float32), 'training/v_loss': Array(5.0677395, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20861977, 0.12672931], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.35262 , 10.901395], dtype=float32), 'eval/episode_reward': Array([ 6.873641, 10.322675], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.19999367, 0.13114454], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.036085367202759, 'eval/sps': 31713.898085538123}
I0728 07:22:21.932071 139845810259776 train.py:379] starting iteration 162, 318504960 steps, 4194.187209844589
I0728 07:22:47.594019 139845810259776 train.py:394] {'eval/walltime': 674.6770701408386, 'training/sps': 90899.56330112505, 'training/walltime': 3535.1112949848175, 'training/entropy_loss': Array(-0.02224573, dtype=float32), 'training/policy_loss': Array(0.00691034, dtype=float32), 'training/total_loss': Array(4.8862047, dtype=float32), 'training/v_loss': Array(4.90154, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19816971, 0.11728659], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.466692, 10.043962], dtype=float32), 'eval/episode_reward': Array([6.7948  , 9.563407], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18878382, 0.12233689], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.027968406677246, 'eval/sps': 31777.806347192734}
I0728 07:22:47.595941 139845810259776 train.py:379] starting iteration 163, 320471040 steps, 4219.851079463959
I0728 07:23:13.291887 139845810259776 train.py:394] {'eval/walltime': 678.7353882789612, 'training/sps': 90883.06355491639, 'training/walltime': 3556.7443685531616, 'training/entropy_loss': Array(-0.02139387, dtype=float32), 'training/policy_loss': Array(0.00699747, dtype=float32), 'training/total_loss': Array(4.7840376, dtype=float32), 'training/v_loss': Array(4.7984333, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.187071  , 0.12120736], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.487449, 10.377455], dtype=float32), 'eval/episode_reward': Array([7.402604, 9.713649], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1773332 , 0.12606326], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.058318138122559, 'eval/sps': 31540.159160418803}
I0728 07:23:13.293738 139845810259776 train.py:379] starting iteration 164, 322437120 steps, 4245.54887676239
I0728 07:23:38.960295 139845810259776 train.py:394] {'eval/walltime': 682.7585997581482, 'training/sps': 90859.14805810561, 'training/walltime': 3578.3831362724304, 'training/entropy_loss': Array(-0.02157468, dtype=float32), 'training/policy_loss': Array(0.0071591, dtype=float32), 'training/total_loss': Array(4.918891, dtype=float32), 'training/v_loss': Array(4.933306, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17864412, 0.10972963], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.77121 ,  9.413449], dtype=float32), 'eval/episode_reward': Array([ 8.769241, 10.601302], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1694023, 0.1137557], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023211479187012, 'eval/sps': 31815.379495254754}
I0728 07:23:38.962114 139845810259776 train.py:379] starting iteration 165, 324403200 steps, 4271.217252731323
I0728 07:24:04.590313 139845810259776 train.py:394] {'eval/walltime': 686.789110660553, 'training/sps': 91051.05843487754, 'training/walltime': 3599.9762954711914, 'training/entropy_loss': Array(-0.0218391, dtype=float32), 'training/policy_loss': Array(0.00772203, dtype=float32), 'training/total_loss': Array(4.8415184, dtype=float32), 'training/v_loss': Array(4.8556356, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19712946, 0.12820913], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.359737, 10.974865], dtype=float32), 'eval/episode_reward': Array([ 8.058739, 10.831679], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18729681, 0.13308878], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.030510902404785, 'eval/sps': 31757.760517067305}
I0728 07:24:04.592211 139845810259776 train.py:379] starting iteration 166, 326369280 steps, 4296.847349405289
I0728 07:24:30.341773 139845810259776 train.py:394] {'eval/walltime': 690.8432414531708, 'training/sps': 90640.12070141197, 'training/walltime': 3621.6673521995544, 'training/entropy_loss': Array(-0.02171418, dtype=float32), 'training/policy_loss': Array(0.00810167, dtype=float32), 'training/total_loss': Array(4.7510815, dtype=float32), 'training/v_loss': Array(4.764694, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19857827, 0.12283756], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.499786, 10.49859 ], dtype=float32), 'eval/episode_reward': Array([7.82801 , 9.874938], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18947816, 0.12740828], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.054130792617798, 'eval/sps': 31572.73569789024}
I0728 07:24:30.343772 139845810259776 train.py:379] starting iteration 167, 328335360 steps, 4322.598909854889
I0728 07:24:56.034615 139845810259776 train.py:394] {'eval/walltime': 694.8845772743225, 'training/sps': 90832.52467479298, 'training/walltime': 3643.3124623298645, 'training/entropy_loss': Array(-0.02206988, dtype=float32), 'training/policy_loss': Array(0.00867458, dtype=float32), 'training/total_loss': Array(4.699251, dtype=float32), 'training/v_loss': Array(4.7126465, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17308041, 0.11091748], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.287178,  9.481993], dtype=float32), 'eval/episode_reward': Array([9.526823, 9.89774 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1618841 , 0.11633576], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.041335821151733, 'eval/sps': 31672.69577798202}
I0728 07:24:56.037443 139845810259776 train.py:379] starting iteration 168, 330301440 steps, 4348.292565822601
I0728 07:25:21.748185 139845810259776 train.py:394] {'eval/walltime': 698.9406645298004, 'training/sps': 90820.05302216038, 'training/walltime': 3664.9605448246, 'training/entropy_loss': Array(-0.02214357, dtype=float32), 'training/policy_loss': Array(0.0094141, dtype=float32), 'training/total_loss': Array(4.605571, dtype=float32), 'training/v_loss': Array(4.6183, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18505907, 0.1135686 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.236278,  9.789567], dtype=float32), 'eval/episode_reward': Array([ 8.337516, 10.331063], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.17473567, 0.11871919], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.056087255477905, 'eval/sps': 31557.506517427842}
I0728 07:25:21.751904 139845810259776 train.py:379] starting iteration 169, 332267520 steps, 4374.007029533386
I0728 07:25:47.398854 139845810259776 train.py:394] {'eval/walltime': 702.969321012497, 'training/sps': 90965.2489598633, 'training/walltime': 3686.5740733146667, 'training/entropy_loss': Array(-0.02203678, dtype=float32), 'training/policy_loss': Array(0.01017419, dtype=float32), 'training/total_loss': Array(4.6244054, dtype=float32), 'training/v_loss': Array(4.636268, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17126144, 0.1147234 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.020597,  9.848474], dtype=float32), 'eval/episode_reward': Array([ 8.024307, 10.035193], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15966444, 0.12042939], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.028656482696533, 'eval/sps': 31772.378843858318}
I0728 07:25:47.400670 139845810259776 train.py:379] starting iteration 170, 334233600 steps, 4399.6558084487915
I0728 07:26:13.104616 139845810259776 train.py:394] {'eval/walltime': 707.0202140808105, 'training/sps': 90820.86121991173, 'training/walltime': 3708.2219631671906, 'training/entropy_loss': Array(-0.02216772, dtype=float32), 'training/policy_loss': Array(0.01137708, dtype=float32), 'training/total_loss': Array(4.4677725, dtype=float32), 'training/v_loss': Array(4.4785624, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17646986, 0.10554232], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.449047,  9.087527], dtype=float32), 'eval/episode_reward': Array([7.6898246, 8.662389 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1647717 , 0.11170775], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.050893068313599, 'eval/sps': 31597.97058116048}
I0728 07:26:13.106528 139845810259776 train.py:379] starting iteration 171, 336199680 steps, 4425.361665964127
I0728 07:26:38.765331 139845810259776 train.py:394] {'eval/walltime': 711.0407478809357, 'training/sps': 90880.75987755823, 'training/walltime': 3729.8555850982666, 'training/entropy_loss': Array(-0.02204391, dtype=float32), 'training/policy_loss': Array(0.01220567, dtype=float32), 'training/total_loss': Array(4.445499, dtype=float32), 'training/v_loss': Array(4.455337, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18104577, 0.11174479], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.749832,  9.729208], dtype=float32), 'eval/episode_reward': Array([9.25971 , 9.210972], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16884936, 0.11836047], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020533800125122, 'eval/sps': 31836.568566098496}
I0728 07:26:38.767143 139845810259776 train.py:379] starting iteration 172, 338165760 steps, 4451.02228140831
I0728 07:27:04.396959 139845810259776 train.py:394] {'eval/walltime': 715.0705771446228, 'training/sps': 91040.17904174955, 'training/walltime': 3751.451324701309, 'training/entropy_loss': Array(-0.02136213, dtype=float32), 'training/policy_loss': Array(0.01352433, dtype=float32), 'training/total_loss': Array(4.3016415, dtype=float32), 'training/v_loss': Array(4.3094797, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17750435, 0.11220496], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.502607,  9.699584], dtype=float32), 'eval/episode_reward': Array([8.082302, 9.41953 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16578576, 0.11794215], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.029829263687134, 'eval/sps': 31763.132287864}
I0728 07:27:04.398792 139845810259776 train.py:379] starting iteration 173, 340131840 steps, 4476.653930902481
I0728 07:27:29.994290 139845810259776 train.py:394] {'eval/walltime': 719.0983836650848, 'training/sps': 91176.39539783463, 'training/walltime': 3773.0148005485535, 'training/entropy_loss': Array(-0.02081318, dtype=float32), 'training/policy_loss': Array(0.01436121, dtype=float32), 'training/total_loss': Array(4.1622696, dtype=float32), 'training/v_loss': Array(4.1687217, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.15292647, 0.10161   ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([12.344633,  8.74835 ], dtype=float32), 'eval/episode_reward': Array([10.069433, 10.120408], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.13915509, 0.10813807], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.027806520462036, 'eval/sps': 31779.083565641806}
I0728 07:27:29.996157 139845810259776 train.py:379] starting iteration 174, 342097920 steps, 4502.251295804977
I0728 07:27:55.595379 139845810259776 train.py:394] {'eval/walltime': 723.1276633739471, 'training/sps': 91168.82418518238, 'training/walltime': 3794.5800671577454, 'training/entropy_loss': Array(-0.01972827, dtype=float32), 'training/policy_loss': Array(0.01562731, dtype=float32), 'training/total_loss': Array(3.9612246, dtype=float32), 'training/v_loss': Array(3.9653254, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18773782, 0.11169079], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.277437,  9.658452], dtype=float32), 'eval/episode_reward': Array([8.514885, 9.941398], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.17591527, 0.11814027], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.029279708862305, 'eval/sps': 31767.464472239804}
I0728 07:27:55.597239 139845810259776 train.py:379] starting iteration 175, 344064000 steps, 4527.852377653122
I0728 07:28:21.296009 139845810259776 train.py:394] {'eval/walltime': 727.1756224632263, 'training/sps': 90826.64105326487, 'training/walltime': 3816.226579427719, 'training/entropy_loss': Array(-0.01788775, dtype=float32), 'training/policy_loss': Array(0.01622313, dtype=float32), 'training/total_loss': Array(4.462289, dtype=float32), 'training/v_loss': Array(4.463953, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17929657, 0.10194203], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.5378   ,  8.8136015], dtype=float32), 'eval/episode_reward': Array([8.821842, 9.096122], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16718033, 0.1083599 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.047959089279175, 'eval/sps': 31620.872932980437}
I0728 07:28:21.375104 139845810259776 train.py:379] starting iteration 176, 346030080 steps, 4553.6302399635315
I0728 07:28:47.038072 139845810259776 train.py:394] {'eval/walltime': 731.2016451358795, 'training/sps': 90886.34799949318, 'training/walltime': 3837.8588712215424, 'training/entropy_loss': Array(-0.0153642, dtype=float32), 'training/policy_loss': Array(0.01738989, dtype=float32), 'training/total_loss': Array(4.4045825, dtype=float32), 'training/v_loss': Array(4.4025564, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18028623, 0.10410079], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.574884,  9.052965], dtype=float32), 'eval/episode_reward': Array([8.528878, 8.900644], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16816753, 0.11044901], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.026022672653198, 'eval/sps': 31793.164223699325}
I0728 07:28:47.040019 139845810259776 train.py:379] starting iteration 177, 347996160 steps, 4579.295157194138
I0728 07:29:12.737549 139845810259776 train.py:394] {'eval/walltime': 735.2586212158203, 'training/sps': 90869.68282378015, 'training/walltime': 3859.495130300522, 'training/entropy_loss': Array(-0.01265274, dtype=float32), 'training/policy_loss': Array(0.01861765, dtype=float32), 'training/total_loss': Array(4.397822, dtype=float32), 'training/v_loss': Array(4.391857, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19809666, 0.09793509], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.086763,  8.567591], dtype=float32), 'eval/episode_reward': Array([7.5053225, 7.9470468], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18694395, 0.10465516], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.056976079940796, 'eval/sps': 31550.59272665663}
I0728 07:29:12.739417 139845810259776 train.py:379] starting iteration 178, 349962240 steps, 4604.994555234909
I0728 07:29:38.422803 139845810259776 train.py:394] {'eval/walltime': 739.2968926429749, 'training/sps': 90854.97167271927, 'training/walltime': 3881.1348927021027, 'training/entropy_loss': Array(-0.01041699, dtype=float32), 'training/policy_loss': Array(0.02045307, dtype=float32), 'training/total_loss': Array(4.3007584, dtype=float32), 'training/v_loss': Array(4.290722, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1965232, 0.1177913], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.991035, 10.189741], dtype=float32), 'eval/episode_reward': Array([7.691469, 8.790337], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18497851, 0.12458459], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.038271427154541, 'eval/sps': 31696.73022454356}
I0728 07:29:38.424906 139845810259776 train.py:379] starting iteration 179, 351928320 steps, 4630.680043935776
I0728 07:30:04.148971 139845810259776 train.py:394] {'eval/walltime': 743.3552739620209, 'training/sps': 90765.7087385433, 'training/walltime': 3902.7959365844727, 'training/entropy_loss': Array(-0.00693106, dtype=float32), 'training/policy_loss': Array(0.0220872, dtype=float32), 'training/total_loss': Array(4.2891865, dtype=float32), 'training/v_loss': Array(4.27403, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20401993, 0.1139527 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.607143, 10.00464 ], dtype=float32), 'eval/episode_reward': Array([6.756215, 8.469799], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.19263491, 0.12087888], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0583813190460205, 'eval/sps': 31539.668142886138}
I0728 07:30:04.150816 139845810259776 train.py:379] starting iteration 180, 353894400 steps, 4656.405953645706
I0728 07:30:29.769461 139845810259776 train.py:394] {'eval/walltime': 747.3753294944763, 'training/sps': 91046.08232031135, 'training/walltime': 3924.3902759552, 'training/entropy_loss': Array(-0.00158083, dtype=float32), 'training/policy_loss': Array(0.02578379, dtype=float32), 'training/total_loss': Array(4.261137, dtype=float32), 'training/v_loss': Array(4.2369337, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1915718 , 0.10498954], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.494806,  9.111822], dtype=float32), 'eval/episode_reward': Array([7.13106 , 8.604702], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18026769, 0.11108545], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020055532455444, 'eval/sps': 31840.35617583068}
I0728 07:30:29.771378 139845810259776 train.py:379] starting iteration 181, 355860480 steps, 4682.026515483856
I0728 07:30:55.270677 139845810259776 train.py:394] {'eval/walltime': 751.4123840332031, 'training/sps': 91626.53299354753, 'training/walltime': 3945.847815990448, 'training/entropy_loss': Array(0.00331999, dtype=float32), 'training/policy_loss': Array(0.02690615, dtype=float32), 'training/total_loss': Array(4.2323027, dtype=float32), 'training/v_loss': Array(4.202076, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19080044, 0.09961416], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.419727,  8.693329], dtype=float32), 'eval/episode_reward': Array([7.263759, 8.090461], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.17921765, 0.10641228], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.037054538726807, 'eval/sps': 31706.284562697096}
I0728 07:30:55.272466 139845810259776 train.py:379] starting iteration 182, 357826560 steps, 4707.527604341507
I0728 07:31:20.809161 139845810259776 train.py:394] {'eval/walltime': 755.4357891082764, 'training/sps': 91407.02072894773, 'training/walltime': 3967.356885910034, 'training/entropy_loss': Array(0.00921934, dtype=float32), 'training/policy_loss': Array(0.02858373, dtype=float32), 'training/total_loss': Array(4.287326, dtype=float32), 'training/v_loss': Array(4.249523, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20032622, 0.09993826], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.26966 ,  8.802682], dtype=float32), 'eval/episode_reward': Array([5.600271, 8.804252], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18921766, 0.10637321], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023405075073242, 'eval/sps': 31813.84862116323}
I0728 07:31:20.810992 139845810259776 train.py:379] starting iteration 183, 359792640 steps, 4733.066129922867
I0728 07:31:46.355253 139845810259776 train.py:394] {'eval/walltime': 759.4634847640991, 'training/sps': 91394.05048529021, 'training/walltime': 3988.8690083026886, 'training/entropy_loss': Array(0.01443396, dtype=float32), 'training/policy_loss': Array(0.03614739, dtype=float32), 'training/total_loss': Array(4.1540813, dtype=float32), 'training/v_loss': Array(4.1035, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18643942, 0.09830862], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.072727,  8.578039], dtype=float32), 'eval/episode_reward': Array([8.053886, 8.071237], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.17464966, 0.10476456], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.027695655822754, 'eval/sps': 31779.95830319332}
I0728 07:31:46.357053 139845810259776 train.py:379] starting iteration 184, 361758720 steps, 4758.612192153931
I0728 07:32:12.027418 139845810259776 train.py:394] {'eval/walltime': 763.4942736625671, 'training/sps': 90875.05026597553, 'training/walltime': 4010.503989458084, 'training/entropy_loss': Array(0.01376581, dtype=float32), 'training/policy_loss': Array(0.03394435, dtype=float32), 'training/total_loss': Array(4.208855, dtype=float32), 'training/v_loss': Array(4.161145, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.21287555, 0.10884408], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.444664,  9.403852], dtype=float32), 'eval/episode_reward': Array([6.558219, 8.079776], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2033326 , 0.11386344], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.030788898468018, 'eval/sps': 31755.57024299858}
I0728 07:32:12.029244 139845810259776 train.py:379] starting iteration 185, 363724800 steps, 4784.284382581711
I0728 07:32:37.706112 139845810259776 train.py:394] {'eval/walltime': 767.5521154403687, 'training/sps': 90960.28422657971, 'training/walltime': 4032.11869764328, 'training/entropy_loss': Array(0.00410778, dtype=float32), 'training/policy_loss': Array(0.03428214, dtype=float32), 'training/total_loss': Array(4.102804, dtype=float32), 'training/v_loss': Array(4.0644145, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20632537, 0.10366215], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.939095 ,  9.0024605], dtype=float32), 'eval/episode_reward': Array([7.33451 , 8.264694], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.19605923, 0.10929583], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.057841777801514, 'eval/sps': 31543.86173956461}
I0728 07:32:37.708009 139845810259776 train.py:379] starting iteration 186, 365690880 steps, 4809.9631469249725
I0728 07:33:03.384039 139845810259776 train.py:394] {'eval/walltime': 771.5758912563324, 'training/sps': 90821.02726232001, 'training/walltime': 4053.7665479183197, 'training/entropy_loss': Array(-0.01596869, dtype=float32), 'training/policy_loss': Array(0.02570813, dtype=float32), 'training/total_loss': Array(4.116776, dtype=float32), 'training/v_loss': Array(4.1070366, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.21277502, 0.09976925], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.536694,  8.644085], dtype=float32), 'eval/episode_reward': Array([5.5358753, 8.498918 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20344841, 0.10471857], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023775815963745, 'eval/sps': 31810.91737073885}
I0728 07:33:03.385869 139845810259776 train.py:379] starting iteration 187, 367656960 steps, 4835.6410076618195
I0728 07:33:29.038463 139845810259776 train.py:394] {'eval/walltime': 775.6158797740936, 'training/sps': 90987.1130765456, 'training/walltime': 4075.374882698059, 'training/entropy_loss': Array(-0.01936539, dtype=float32), 'training/policy_loss': Array(0.01549937, dtype=float32), 'training/total_loss': Array(4.1135316, dtype=float32), 'training/v_loss': Array(4.1173973, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23471336, 0.11735877], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.406738, 10.18562 ], dtype=float32), 'eval/episode_reward': Array([4.197081, 9.637097], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22556874, 0.12288625], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0399885177612305, 'eval/sps': 31683.258365034046}
I0728 07:33:29.040297 139845810259776 train.py:379] starting iteration 188, 369623040 steps, 4861.295436143875
I0728 07:33:54.712147 139845810259776 train.py:394] {'eval/walltime': 779.6645841598511, 'training/sps': 90943.85971882226, 'training/walltime': 4096.993494510651, 'training/entropy_loss': Array(-0.0191999, dtype=float32), 'training/policy_loss': Array(0.01278201, dtype=float32), 'training/total_loss': Array(4.1009636, dtype=float32), 'training/v_loss': Array(4.107382, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20886205, 0.10157667], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.12822,  8.86331], dtype=float32), 'eval/episode_reward': Array([5.17164  , 8.5422125], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.19907425, 0.10695636], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.048704385757446, 'eval/sps': 31615.05207697531}
I0728 07:33:54.714071 139845810259776 train.py:379] starting iteration 189, 371589120 steps, 4886.969209432602
I0728 07:34:20.374492 139845810259776 train.py:394] {'eval/walltime': 783.6925880908966, 'training/sps': 90904.63664931494, 'training/walltime': 4118.621434211731, 'training/entropy_loss': Array(-0.01922986, dtype=float32), 'training/policy_loss': Array(0.01232844, dtype=float32), 'training/total_loss': Array(4.0481396, dtype=float32), 'training/v_loss': Array(4.055041, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20597401, 0.10683174], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.92007 ,  9.291358], dtype=float32), 'eval/episode_reward': Array([5.754443 , 8.3579445], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.19572294, 0.11244211], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.028003931045532, 'eval/sps': 31777.526087660885}
I0728 07:34:20.376476 139845810259776 train.py:379] starting iteration 190, 373555200 steps, 4912.631614923477
I0728 07:34:46.084542 139845810259776 train.py:394] {'eval/walltime': 787.7360460758209, 'training/sps': 90770.23361250955, 'training/walltime': 4140.281398296356, 'training/entropy_loss': Array(-0.01962826, dtype=float32), 'training/policy_loss': Array(0.01217982, dtype=float32), 'training/total_loss': Array(4.0321407, dtype=float32), 'training/v_loss': Array(4.0395894, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20565236, 0.1070748 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.906927,  9.361543], dtype=float32), 'eval/episode_reward': Array([5.94312 , 8.625082], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1949063 , 0.11368801], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.043457984924316, 'eval/sps': 31656.07271727243}
I0728 07:34:46.086461 139845810259776 train.py:379] starting iteration 191, 375521280 steps, 4938.341599702835
I0728 07:35:11.787971 139845810259776 train.py:394] {'eval/walltime': 791.7680549621582, 'training/sps': 90750.11042392974, 'training/walltime': 4161.946165323257, 'training/entropy_loss': Array(-0.01904768, dtype=float32), 'training/policy_loss': Array(0.0120253, dtype=float32), 'training/total_loss': Array(3.8884487, dtype=float32), 'training/v_loss': Array(3.895471, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2263696 , 0.09527566], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.681892,  8.298879], dtype=float32), 'eval/episode_reward': Array([4.7517495, 8.254143 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.21762584, 0.10028882], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.03200888633728, 'eval/sps': 31745.961779433615}
I0728 07:35:11.789879 139845810259776 train.py:379] starting iteration 192, 377487360 steps, 4964.04501748085
I0728 07:35:37.510594 139845810259776 train.py:394] {'eval/walltime': 795.8166785240173, 'training/sps': 90740.72565117701, 'training/walltime': 4183.613173007965, 'training/entropy_loss': Array(-0.01788695, dtype=float32), 'training/policy_loss': Array(0.01191133, dtype=float32), 'training/total_loss': Array(3.8359332, dtype=float32), 'training/v_loss': Array(3.841909, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2327369 , 0.10883429], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.193962,  9.490094], dtype=float32), 'eval/episode_reward': Array([3.9180133, 7.5388637], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22375384, 0.11430015], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.048623561859131, 'eval/sps': 31615.683217834732}
I0728 07:35:37.512503 139845810259776 train.py:379] starting iteration 193, 379453440 steps, 4989.7676413059235
I0728 07:36:03.174621 139845810259776 train.py:394] {'eval/walltime': 799.8487310409546, 'training/sps': 90914.64872902638, 'training/walltime': 4205.23873090744, 'training/entropy_loss': Array(-0.01747074, dtype=float32), 'training/policy_loss': Array(0.01297613, dtype=float32), 'training/total_loss': Array(3.8536267, dtype=float32), 'training/v_loss': Array(3.858121, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.21748157, 0.09624064], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.859404,  8.336962], dtype=float32), 'eval/episode_reward': Array([5.161989, 7.68416 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20826247, 0.10117615], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.032052516937256, 'eval/sps': 31745.618258273258}
I0728 07:36:03.176555 139845810259776 train.py:379] starting iteration 194, 381419520 steps, 5015.431693315506
I0728 07:36:28.840710 139845810259776 train.py:394] {'eval/walltime': 803.8850991725922, 'training/sps': 90924.97280075119, 'training/walltime': 4226.861833333969, 'training/entropy_loss': Array(-0.01751774, dtype=float32), 'training/policy_loss': Array(0.01369177, dtype=float32), 'training/total_loss': Array(3.775346, dtype=float32), 'training/v_loss': Array(3.779172, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19975255, 0.09770774], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.234097,  8.531887], dtype=float32), 'eval/episode_reward': Array([5.810631 , 7.9394794], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18891606, 0.10362407], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.036368131637573, 'eval/sps': 31711.676394608192}
I0728 07:36:28.842555 139845810259776 train.py:379] starting iteration 195, 383385600 steps, 5041.0976939201355
I0728 07:36:54.507876 139845810259776 train.py:394] {'eval/walltime': 807.941150188446, 'training/sps': 91003.04906589731, 'training/walltime': 4248.46638417244, 'training/entropy_loss': Array(-0.01773502, dtype=float32), 'training/policy_loss': Array(0.0136211, dtype=float32), 'training/total_loss': Array(3.7605383, dtype=float32), 'training/v_loss': Array(3.7646523, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.21941395, 0.10159989], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.955814,  8.830163], dtype=float32), 'eval/episode_reward': Array([6.221341, 7.381149], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20996584, 0.10705439], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.056051015853882, 'eval/sps': 31557.788474475925}
I0728 07:36:54.509710 139845810259776 train.py:379] starting iteration 196, 385351680 steps, 5066.764848947525
I0728 07:37:20.151520 139845810259776 train.py:394] {'eval/walltime': 811.9545919895172, 'training/sps': 90920.90564456373, 'training/walltime': 4270.090453863144, 'training/entropy_loss': Array(-0.0165218, dtype=float32), 'training/policy_loss': Array(0.01435278, dtype=float32), 'training/total_loss': Array(3.713643, dtype=float32), 'training/v_loss': Array(3.7158122, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19835988, 0.09896271], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.05427 ,  8.650802], dtype=float32), 'eval/episode_reward': Array([7.22951 , 7.252577], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18720935, 0.10539602], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.013441801071167, 'eval/sps': 31892.82574518396}
I0728 07:37:20.153334 139845810259776 train.py:379] starting iteration 197, 387317760 steps, 5092.4084730148315
I0728 07:37:45.868842 139845810259776 train.py:394] {'eval/walltime': 815.9878206253052, 'training/sps': 90694.4493155615, 'training/walltime': 4291.7685170173645, 'training/entropy_loss': Array(-0.01520385, dtype=float32), 'training/policy_loss': Array(0.01523164, dtype=float32), 'training/total_loss': Array(3.6448712, dtype=float32), 'training/v_loss': Array(3.644843, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20084633, 0.09630833], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.369324,  8.416626], dtype=float32), 'eval/episode_reward': Array([6.4744635, 7.796239 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.19042757, 0.10221267], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.033228635787964, 'eval/sps': 31736.361004734583}
I0728 07:37:45.870668 139845810259776 train.py:379] starting iteration 198, 389283840 steps, 5118.125807523727
I0728 07:38:11.589964 139845810259776 train.py:394] {'eval/walltime': 820.0443751811981, 'training/sps': 90777.40299948816, 'training/walltime': 4313.426770448685, 'training/entropy_loss': Array(-0.01503154, dtype=float32), 'training/policy_loss': Array(0.01464892, dtype=float32), 'training/total_loss': Array(3.7023027, dtype=float32), 'training/v_loss': Array(3.7026854, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19865665, 0.09893722], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.193   ,  8.579035], dtype=float32), 'eval/episode_reward': Array([7.0580535, 8.1250725], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1878285 , 0.10459037], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.056554555892944, 'eval/sps': 31553.87120679908}
I0728 07:38:11.591823 139845810259776 train.py:379] starting iteration 199, 391249920 steps, 5143.8469614982605
I0728 07:38:37.216750 139845810259776 train.py:394] {'eval/walltime': 824.0664193630219, 'training/sps': 91031.0537358534, 'training/walltime': 4335.0246748924255, 'training/entropy_loss': Array(-0.01510742, dtype=float32), 'training/policy_loss': Array(0.01450755, dtype=float32), 'training/total_loss': Array(3.6041777, dtype=float32), 'training/v_loss': Array(3.6047773, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.21329662, 0.09863365], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.406944,  8.573433], dtype=float32), 'eval/episode_reward': Array([6.6240826, 8.028431 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20337984, 0.10444507], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0220441818237305, 'eval/sps': 31824.613110530398}
I0728 07:38:37.218583 139845810259776 train.py:379] starting iteration 200, 393216000 steps, 5169.473721504211
I0728 07:39:02.980741 139845810259776 train.py:394] {'eval/walltime': 828.1202936172485, 'training/sps': 90587.19156432431, 'training/walltime': 4356.7284054756165, 'training/entropy_loss': Array(-0.01428767, dtype=float32), 'training/policy_loss': Array(0.01516598, dtype=float32), 'training/total_loss': Array(4.064723, dtype=float32), 'training/v_loss': Array(4.0638447, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19988808, 0.1109581 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.247185,  9.570595], dtype=float32), 'eval/episode_reward': Array([6.793969 , 8.6180525], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18853223, 0.11742876], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.053874254226685, 'eval/sps': 31574.733692477897}
I0728 07:39:03.067145 139845810259776 train.py:379] starting iteration 201, 395182080 steps, 5195.322281122208
I0728 07:39:28.751739 139845810259776 train.py:394] {'eval/walltime': 832.181004524231, 'training/sps': 90947.86874568861, 'training/walltime': 4378.346064329147, 'training/entropy_loss': Array(-0.0144261, dtype=float32), 'training/policy_loss': Array(0.01583827, dtype=float32), 'training/total_loss': Array(4.107748, dtype=float32), 'training/v_loss': Array(4.106336, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19152735, 0.10050701], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.49201,  8.74612], dtype=float32), 'eval/episode_reward': Array([7.306421, 8.345141], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1797667 , 0.10733758], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.060710906982422, 'eval/sps': 31521.5741607961}
I0728 07:39:28.755821 139845810259776 train.py:379] starting iteration 202, 397148160 steps, 5221.010946035385
I0728 07:39:54.451203 139845810259776 train.py:394] {'eval/walltime': 836.2067637443542, 'training/sps': 90750.08345915047, 'training/walltime': 4400.01083779335, 'training/entropy_loss': Array(-0.01361675, dtype=float32), 'training/policy_loss': Array(0.01649037, dtype=float32), 'training/total_loss': Array(4.023178, dtype=float32), 'training/v_loss': Array(4.0203047, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19655669, 0.10247061], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.028048,  8.896769], dtype=float32), 'eval/episode_reward': Array([6.6977005, 8.33118  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18581909, 0.10869769], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.025759220123291, 'eval/sps': 31795.244822436234}
I0728 07:39:54.453036 139845810259776 train.py:379] starting iteration 203, 399114240 steps, 5246.708174943924
I0728 07:40:20.194728 139845810259776 train.py:394] {'eval/walltime': 840.2752068042755, 'training/sps': 90732.85324626035, 'training/walltime': 4421.679725408554, 'training/entropy_loss': Array(-0.01316673, dtype=float32), 'training/policy_loss': Array(0.01670429, dtype=float32), 'training/total_loss': Array(4.14021, dtype=float32), 'training/v_loss': Array(4.1366725, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2002928 , 0.09589102], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.298496,  8.386565], dtype=float32), 'eval/episode_reward': Array([7.166237, 8.745951], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18957578, 0.10212992], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.068443059921265, 'eval/sps': 31461.666813269137}
I0728 07:40:20.196630 139845810259776 train.py:379] starting iteration 204, 401080320 steps, 5272.451767921448
I0728 07:40:45.886707 139845810259776 train.py:394] {'eval/walltime': 844.3158850669861, 'training/sps': 90834.42068189847, 'training/walltime': 4443.324383735657, 'training/entropy_loss': Array(-0.01165857, dtype=float32), 'training/policy_loss': Array(0.01891363, dtype=float32), 'training/total_loss': Array(4.173871, dtype=float32), 'training/v_loss': Array(4.1666164, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20031768, 0.10601404], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.238102,  9.22986 ], dtype=float32), 'eval/episode_reward': Array([7.491892 , 7.9766564], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18860549, 0.11315268], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.040678262710571, 'eval/sps': 31677.85002365789}
I0728 07:40:45.888520 139845810259776 train.py:379] starting iteration 205, 403046400 steps, 5298.1436586380005
I0728 07:41:11.597131 139845810259776 train.py:394] {'eval/walltime': 848.3547718524933, 'training/sps': 90748.76520059444, 'training/walltime': 4464.989471912384, 'training/entropy_loss': Array(-0.01233102, dtype=float32), 'training/policy_loss': Array(0.0182362, dtype=float32), 'training/total_loss': Array(4.3266983, dtype=float32), 'training/v_loss': Array(4.320793, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20772031, 0.11080099], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.93472 ,  9.616288], dtype=float32), 'eval/episode_reward': Array([6.222558, 8.125039], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.19731045, 0.11656098], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.038886785507202, 'eval/sps': 31691.900961251085}
I0728 07:41:11.598979 139845810259776 train.py:379] starting iteration 206, 405012480 steps, 5323.854117631912
I0728 07:41:37.298833 139845810259776 train.py:394] {'eval/walltime': 852.3920769691467, 'training/sps': 90777.42398472202, 'training/walltime': 4486.647720336914, 'training/entropy_loss': Array(-0.01171472, dtype=float32), 'training/policy_loss': Array(0.0179373, dtype=float32), 'training/total_loss': Array(4.1122923, dtype=float32), 'training/v_loss': Array(4.1060696, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.21707523, 0.11372915], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.808407,  9.845685], dtype=float32), 'eval/episode_reward': Array([5.336853, 8.190459], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20736131, 0.11915047], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.037305116653442, 'eval/sps': 31704.31669184823}
I0728 07:41:37.300780 139845810259776 train.py:379] starting iteration 207, 406978560 steps, 5349.5559186935425
I0728 07:42:03.020409 139845810259776 train.py:394] {'eval/walltime': 856.4607615470886, 'training/sps': 90826.52500922189, 'training/walltime': 4508.294260263443, 'training/entropy_loss': Array(-0.01095021, dtype=float32), 'training/policy_loss': Array(0.01780142, dtype=float32), 'training/total_loss': Array(4.225092, dtype=float32), 'training/v_loss': Array(4.2182407, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19839585, 0.10291442], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.167171,  8.95374 ], dtype=float32), 'eval/episode_reward': Array([7.0344687, 8.108874 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18752517, 0.10890408], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0686845779418945, 'eval/sps': 31459.799241736153}
I0728 07:42:03.022450 139845810259776 train.py:379] starting iteration 208, 408944640 steps, 5375.277588367462
I0728 07:42:28.714651 139845810259776 train.py:394] {'eval/walltime': 860.5010409355164, 'training/sps': 90825.3405766389, 'training/walltime': 4529.94108247757, 'training/entropy_loss': Array(-0.00983214, dtype=float32), 'training/policy_loss': Array(0.01877153, dtype=float32), 'training/total_loss': Array(4.158901, dtype=float32), 'training/v_loss': Array(4.1499615, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19155727, 0.10428458], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.545035,  9.046641], dtype=float32), 'eval/episode_reward': Array([7.1289244, 8.077177 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18063134, 0.10986824], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.040279388427734, 'eval/sps': 31680.977401369986}
I0728 07:42:28.716567 139845810259776 train.py:379] starting iteration 209, 410910720 steps, 5400.9717054367065
I0728 07:42:54.396530 139845810259776 train.py:394] {'eval/walltime': 864.5639591217041, 'training/sps': 90968.85445044549, 'training/walltime': 4551.553754329681, 'training/entropy_loss': Array(-0.0081059, dtype=float32), 'training/policy_loss': Array(0.02080072, dtype=float32), 'training/total_loss': Array(4.2515182, dtype=float32), 'training/v_loss': Array(4.238823, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19870278, 0.09623439], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.165295,  8.338938], dtype=float32), 'eval/episode_reward': Array([7.446625, 7.797618], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1881097 , 0.10207717], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.062918186187744, 'eval/sps': 31504.449298326388}
I0728 07:42:54.398368 139845810259776 train.py:379] starting iteration 210, 412876800 steps, 5426.653506755829
I0728 07:43:20.088651 139845810259776 train.py:394] {'eval/walltime': 868.6131613254547, 'training/sps': 90876.65461338413, 'training/walltime': 4573.188353538513, 'training/entropy_loss': Array(-0.00642364, dtype=float32), 'training/policy_loss': Array(0.02017185, dtype=float32), 'training/total_loss': Array(4.2017345, dtype=float32), 'training/v_loss': Array(4.1879864, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19943854, 0.10195093], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.267975,  8.854602], dtype=float32), 'eval/episode_reward': Array([6.8355465, 8.487183 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.188801  , 0.10784903], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.04920220375061, 'eval/sps': 31611.165251623846}
I0728 07:43:20.092077 139845810259776 train.py:379] starting iteration 211, 414842880 steps, 5452.347215652466
I0728 07:43:45.807016 139845810259776 train.py:394] {'eval/walltime': 872.6454749107361, 'training/sps': 90695.43881769924, 'training/walltime': 4594.866180181503, 'training/entropy_loss': Array(-0.00595512, dtype=float32), 'training/policy_loss': Array(0.02004181, dtype=float32), 'training/total_loss': Array(4.3794603, dtype=float32), 'training/v_loss': Array(4.3653736, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.21635704, 0.11184721], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.650616,  9.705908], dtype=float32), 'eval/episode_reward': Array([5.4172277, 8.782698 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20582548, 0.11840657], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.032313585281372, 'eval/sps': 31743.56291812762}
I0728 07:43:45.808840 139845810259776 train.py:379] starting iteration 212, 416808960 steps, 5478.063978433609
I0728 07:44:11.481371 139845810259776 train.py:394] {'eval/walltime': 876.6715426445007, 'training/sps': 90853.66738175473, 'training/walltime': 4616.506253242493, 'training/entropy_loss': Array(-0.00551255, dtype=float32), 'training/policy_loss': Array(0.02097066, dtype=float32), 'training/total_loss': Array(4.3930235, dtype=float32), 'training/v_loss': Array(4.3775654, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.21256252, 0.10495453], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.413467,  9.060456], dtype=float32), 'eval/episode_reward': Array([6.386633, 8.577331], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20300584, 0.10988116], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.026067733764648, 'eval/sps': 31792.808383854797}
I0728 07:44:11.484732 139845810259776 train.py:379] starting iteration 213, 418775040 steps, 5503.739870071411
I0728 07:44:37.197856 139845810259776 train.py:394] {'eval/walltime': 880.7004110813141, 'training/sps': 90687.77074083677, 'training/walltime': 4638.185912847519, 'training/entropy_loss': Array(-0.00642527, dtype=float32), 'training/policy_loss': Array(0.0204526, dtype=float32), 'training/total_loss': Array(4.291152, dtype=float32), 'training/v_loss': Array(4.2771254, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19540128, 0.09843014], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.895854,  8.570119], dtype=float32), 'eval/episode_reward': Array([6.4236574, 8.488255 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18469693, 0.10381471], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0288684368133545, 'eval/sps': 31770.707335691008}
I0728 07:44:37.199697 139845810259776 train.py:379] starting iteration 214, 420741120 steps, 5529.454835891724
I0728 07:45:02.882202 139845810259776 train.py:394] {'eval/walltime': 884.727114200592, 'training/sps': 90805.99188358006, 'training/walltime': 4659.837347507477, 'training/entropy_loss': Array(-0.0034905, dtype=float32), 'training/policy_loss': Array(0.02176719, dtype=float32), 'training/total_loss': Array(4.2991295, dtype=float32), 'training/v_loss': Array(4.280853, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2130339 , 0.10744215], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.434113,  9.372969], dtype=float32), 'eval/episode_reward': Array([7.376194, 7.732506], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20251706, 0.11419589], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.026703119277954, 'eval/sps': 31787.791701652008}
I0728 07:45:02.884021 139845810259776 train.py:379] starting iteration 215, 422707200 steps, 5555.13916015625
I0728 07:45:28.604338 139845810259776 train.py:394] {'eval/walltime': 888.7557594776154, 'training/sps': 90656.21641249499, 'training/walltime': 4681.524553060532, 'training/entropy_loss': Array(-0.00292351, dtype=float32), 'training/policy_loss': Array(0.02251363, dtype=float32), 'training/total_loss': Array(4.2772837, dtype=float32), 'training/v_loss': Array(4.2576933, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19451383, 0.10260276], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.896231,  8.908845], dtype=float32), 'eval/episode_reward': Array([7.4744325, 8.546459 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18385772, 0.10827769], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.028645277023315, 'eval/sps': 31772.46721870152}
I0728 07:45:28.606156 139845810259776 train.py:379] starting iteration 216, 424673280 steps, 5580.861294269562
I0728 07:45:54.264859 139845810259776 train.py:394] {'eval/walltime': 892.7779650688171, 'training/sps': 90886.8278147156, 'training/walltime': 4703.1567306518555, 'training/entropy_loss': Array(-0.00396033, dtype=float32), 'training/policy_loss': Array(0.02308122, dtype=float32), 'training/total_loss': Array(4.4185767, dtype=float32), 'training/v_loss': Array(4.3994555, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.21087115, 0.10206088], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.242336,  8.887427], dtype=float32), 'eval/episode_reward': Array([5.5243845, 7.9148   ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20115173, 0.10750388], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022205591201782, 'eval/sps': 31823.33600251281}
I0728 07:45:54.266688 139845810259776 train.py:379] starting iteration 217, 426639360 steps, 5606.521825790405
I0728 07:46:19.971420 139845810259776 train.py:394] {'eval/walltime': 896.8361420631409, 'training/sps': 90846.21572372863, 'training/walltime': 4724.798578739166, 'training/entropy_loss': Array(-0.00512333, dtype=float32), 'training/policy_loss': Array(0.02247104, dtype=float32), 'training/total_loss': Array(4.468629, dtype=float32), 'training/v_loss': Array(4.451281, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.21074976, 0.10514353], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.255413,  9.086043], dtype=float32), 'eval/episode_reward': Array([5.662517, 8.818117], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20096797, 0.11035873], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0581769943237305, 'eval/sps': 31541.256130286252}
I0728 07:46:19.973238 139845810259776 train.py:379] starting iteration 218, 428605440 steps, 5632.2283771038055
I0728 07:46:45.651507 139845810259776 train.py:394] {'eval/walltime': 900.8668255805969, 'training/sps': 90842.58892509983, 'training/walltime': 4746.441290855408, 'training/entropy_loss': Array(-0.01148688, dtype=float32), 'training/policy_loss': Array(0.02104299, dtype=float32), 'training/total_loss': Array(4.5022335, dtype=float32), 'training/v_loss': Array(4.4926777, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.22775945, 0.10777041], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.883318,  9.241151], dtype=float32), 'eval/episode_reward': Array([4.917305, 7.883034], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.21957225, 0.11204836], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.030683517456055, 'eval/sps': 31756.40048286067}
I0728 07:46:45.653416 139845810259776 train.py:379] starting iteration 219, 430571520 steps, 5657.908554315567
I0728 07:47:11.406547 139845810259776 train.py:394] {'eval/walltime': 904.9253039360046, 'training/sps': 90645.80680774055, 'training/walltime': 4768.13098692894, 'training/entropy_loss': Array(-0.01744222, dtype=float32), 'training/policy_loss': Array(0.01777576, dtype=float32), 'training/total_loss': Array(4.426276, dtype=float32), 'training/v_loss': Array(4.425943, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.21050367, 0.09777938], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.303596,  8.499195], dtype=float32), 'eval/episode_reward': Array([5.3075266, 9.0582075], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20076007, 0.10349718], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.058478355407715, 'eval/sps': 31538.9140438427}
I0728 07:47:11.408456 139845810259776 train.py:379] starting iteration 220, 432537600 steps, 5683.663595199585
I0728 07:47:37.163268 139845810259776 train.py:394] {'eval/walltime': 908.9710879325867, 'training/sps': 90583.76452400628, 'training/walltime': 4789.835538625717, 'training/entropy_loss': Array(-0.01999864, dtype=float32), 'training/policy_loss': Array(0.01467454, dtype=float32), 'training/total_loss': Array(4.3753004, dtype=float32), 'training/v_loss': Array(4.380625, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2377268 , 0.11703067], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.678337, 10.089928], dtype=float32), 'eval/episode_reward': Array([3.8755004, 8.5841465], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22930215, 0.12201577], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.045783996582031, 'eval/sps': 31637.872933438182}
I0728 07:47:37.165190 139845810259776 train.py:379] starting iteration 221, 434503680 steps, 5709.420329332352
I0728 07:48:02.822216 139845810259776 train.py:394] {'eval/walltime': 913.0338826179504, 'training/sps': 91067.0912541631, 'training/walltime': 4811.424896240234, 'training/entropy_loss': Array(-0.02094645, dtype=float32), 'training/policy_loss': Array(0.01285932, dtype=float32), 'training/total_loss': Array(4.298458, dtype=float32), 'training/v_loss': Array(4.3065453, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.22055694, 0.11078753], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.181246,  9.577196], dtype=float32), 'eval/episode_reward': Array([5.0168276, 8.983551 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.21194404, 0.11510214], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0627946853637695, 'eval/sps': 31505.40697050737}
I0728 07:48:02.824226 139845810259776 train.py:379] starting iteration 222, 436469760 steps, 5735.07936501503
I0728 07:48:28.563913 139845810259776 train.py:394] {'eval/walltime': 917.0768368244171, 'training/sps': 90635.87377394467, 'training/walltime': 4833.116969347, 'training/entropy_loss': Array(-0.02210788, dtype=float32), 'training/policy_loss': Array(0.01203855, dtype=float32), 'training/total_loss': Array(4.138626, dtype=float32), 'training/v_loss': Array(4.148695, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.22758532, 0.09547799], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.830627,  8.291874], dtype=float32), 'eval/episode_reward': Array([5.171669 , 7.7932186], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.21946445, 0.09980771], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.042954206466675, 'eval/sps': 31660.017270357643}
I0728 07:48:28.565831 139845810259776 train.py:379] starting iteration 223, 438435840 steps, 5760.820970058441
I0728 07:48:54.287302 139845810259776 train.py:394] {'eval/walltime': 921.120195388794, 'training/sps': 90716.92890759018, 'training/walltime': 4854.789660692215, 'training/entropy_loss': Array(-0.02280569, dtype=float32), 'training/policy_loss': Array(0.01244802, dtype=float32), 'training/total_loss': Array(4.1383805, dtype=float32), 'training/v_loss': Array(4.148738, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.21206203, 0.10086382], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.418264,  8.800861], dtype=float32), 'eval/episode_reward': Array([5.685854, 8.653477], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20245418, 0.10667686], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.043358564376831, 'eval/sps': 31656.851095947157}
I0728 07:48:54.289225 139845810259776 train.py:379] starting iteration 224, 440401920 steps, 5786.544363737106
I0728 07:49:19.998965 139845810259776 train.py:394] {'eval/walltime': 925.1736946105957, 'training/sps': 90807.72378869173, 'training/walltime': 4876.440682411194, 'training/entropy_loss': Array(-0.02340871, dtype=float32), 'training/policy_loss': Array(0.01207097, dtype=float32), 'training/total_loss': Array(4.0648613, dtype=float32), 'training/v_loss': Array(4.0761986, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.22377425, 0.10827478], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.442865 ,  9.4220915], dtype=float32), 'eval/episode_reward': Array([4.6758814, 8.448221 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.21472344, 0.11372732], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.053499221801758, 'eval/sps': 31577.65500769104}
I0728 07:49:20.000905 139845810259776 train.py:379] starting iteration 225, 442368000 steps, 5812.25604391098
I0728 07:49:45.666629 139845810259776 train.py:394] {'eval/walltime': 929.1994495391846, 'training/sps': 90874.45040281158, 'training/walltime': 4898.075806379318, 'training/entropy_loss': Array(-0.02388804, dtype=float32), 'training/policy_loss': Array(0.012588, dtype=float32), 'training/total_loss': Array(4.3591704, dtype=float32), 'training/v_loss': Array(4.3704705, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.21283504, 0.10876808], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.499088,  9.415557], dtype=float32), 'eval/episode_reward': Array([6.075121, 8.449584], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20368978, 0.11329137], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.025754928588867, 'eval/sps': 31795.278716796445}
I0728 07:49:45.762081 139845810259776 train.py:379] starting iteration 226, 444334080 steps, 5838.017217874527
I0728 07:50:11.397959 139845810259776 train.py:394] {'eval/walltime': 933.2509441375732, 'training/sps': 91109.70293595696, 'training/walltime': 4919.655066728592, 'training/entropy_loss': Array(-0.02350294, dtype=float32), 'training/policy_loss': Array(0.01385933, dtype=float32), 'training/total_loss': Array(4.3405504, dtype=float32), 'training/v_loss': Array(4.350194, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18545285, 0.10073188], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.037708,  8.82305 ], dtype=float32), 'eval/episode_reward': Array([7.354144, 9.229744], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.17366794, 0.10756371], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.051494598388672, 'eval/sps': 31593.279194030554}
I0728 07:50:11.400010 139845810259776 train.py:379] starting iteration 227, 446300160 steps, 5863.655147790909
I0728 07:50:36.836394 139845810259776 train.py:394] {'eval/walltime': 937.2726483345032, 'training/sps': 91831.279233358, 'training/walltime': 4941.06476521492, 'training/entropy_loss': Array(-0.02234431, dtype=float32), 'training/policy_loss': Array(0.01606892, dtype=float32), 'training/total_loss': Array(4.3259034, dtype=float32), 'training/v_loss': Array(4.3321786, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20629247, 0.11764371], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.930546 , 10.1085615], dtype=float32), 'eval/episode_reward': Array([7.692468, 9.757603], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1971432 , 0.12199296], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021704196929932, 'eval/sps': 31827.303484356704}
I0728 07:50:36.838393 139845810259776 train.py:379] starting iteration 228, 448266240 steps, 5889.093531131744
I0728 07:51:02.245605 139845810259776 train.py:394] {'eval/walltime': 941.2933588027954, 'training/sps': 91951.85965388197, 'training/walltime': 4962.446388244629, 'training/entropy_loss': Array(-0.02039932, dtype=float32), 'training/policy_loss': Array(0.01804173, dtype=float32), 'training/total_loss': Array(4.1949816, dtype=float32), 'training/v_loss': Array(4.197339, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18128017, 0.0983292 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.692866,  8.595483], dtype=float32), 'eval/episode_reward': Array([7.719032, 8.147462], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16893573, 0.10551769], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020710468292236, 'eval/sps': 31835.1696819311}
I0728 07:51:02.247648 139845810259776 train.py:379] starting iteration 229, 450232320 steps, 5914.502786159515
I0728 07:51:27.691752 139845810259776 train.py:394] {'eval/walltime': 945.3191492557526, 'training/sps': 91815.35140582528, 'training/walltime': 4983.859800815582, 'training/entropy_loss': Array(-0.01818467, dtype=float32), 'training/policy_loss': Array(0.01963011, dtype=float32), 'training/total_loss': Array(4.157503, dtype=float32), 'training/v_loss': Array(4.156058, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16722843, 0.10208652], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.367559,  8.866148], dtype=float32), 'eval/episode_reward': Array([8.375144, 8.819896], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1537709 , 0.10879108], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.025790452957153, 'eval/sps': 31794.998148991413}
I0728 07:51:27.693698 139845810259776 train.py:379] starting iteration 230, 452198400 steps, 5939.948836803436
I0728 07:51:53.297419 139845810259776 train.py:394] {'eval/walltime': 949.3542366027832, 'training/sps': 91173.26535383565, 'training/walltime': 5005.424016952515, 'training/entropy_loss': Array(-0.01575381, dtype=float32), 'training/policy_loss': Array(0.02069408, dtype=float32), 'training/total_loss': Array(4.204919, dtype=float32), 'training/v_loss': Array(4.1999784, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1820815 , 0.10196095], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.797668,  8.846229], dtype=float32), 'eval/episode_reward': Array([8.766917, 9.246198], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.17068604, 0.1080561 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.03508734703064, 'eval/sps': 31721.742057007337}
I0728 07:51:53.299419 139845810259776 train.py:379] starting iteration 231, 454164480 steps, 5965.554557323456
I0728 07:52:18.867719 139845810259776 train.py:394] {'eval/walltime': 953.3803889751434, 'training/sps': 91286.61504707871, 'training/walltime': 5026.961457014084, 'training/entropy_loss': Array(-0.01447734, dtype=float32), 'training/policy_loss': Array(0.02148673, dtype=float32), 'training/total_loss': Array(4.141284, dtype=float32), 'training/v_loss': Array(4.1342745, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17703938, 0.10021296], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.278595,  8.715242], dtype=float32), 'eval/episode_reward': Array([8.526157, 8.049733], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1643159 , 0.10730091], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0261523723602295, 'eval/sps': 31792.14002895853}
I0728 07:52:18.869591 139845810259776 train.py:379] starting iteration 232, 456130560 steps, 5991.1247301101685
I0728 07:52:44.562747 139845810259776 train.py:394] {'eval/walltime': 957.4224309921265, 'training/sps': 90827.10423201091, 'training/walltime': 5048.6078588962555, 'training/entropy_loss': Array(-0.01270183, dtype=float32), 'training/policy_loss': Array(0.02297561, dtype=float32), 'training/total_loss': Array(4.0023956, dtype=float32), 'training/v_loss': Array(3.9921212, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17511958, 0.1147462 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.098322,  9.938168], dtype=float32), 'eval/episode_reward': Array([10.14339,  9.36068], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16172086, 0.12151756], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.042042016983032, 'eval/sps': 31667.162157690484}
I0728 07:52:44.564712 139845810259776 train.py:379] starting iteration 233, 458096640 steps, 6016.819850921631
I0728 07:53:10.211978 139845810259776 train.py:394] {'eval/walltime': 961.4524669647217, 'training/sps': 90976.87425487873, 'training/walltime': 5070.218625545502, 'training/entropy_loss': Array(-0.01127356, dtype=float32), 'training/policy_loss': Array(0.02537328, dtype=float32), 'training/total_loss': Array(4.156313, dtype=float32), 'training/v_loss': Array(4.1422133, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16998124, 0.09297545], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.738887,  8.061424], dtype=float32), 'eval/episode_reward': Array([8.789415, 8.488867], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15684208, 0.10039312], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.030035972595215, 'eval/sps': 31761.503090894763}
I0728 07:53:10.216103 139845810259776 train.py:379] starting iteration 234, 460062720 steps, 6042.4712266922
I0728 07:53:35.936053 139845810259776 train.py:394] {'eval/walltime': 965.5023965835571, 'training/sps': 90750.48393770563, 'training/walltime': 5091.883303403854, 'training/entropy_loss': Array(-0.0096966, dtype=float32), 'training/policy_loss': Array(0.02532026, dtype=float32), 'training/total_loss': Array(4.1328526, dtype=float32), 'training/v_loss': Array(4.1172285, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1816888 , 0.10213393], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.691759,  8.874802], dtype=float32), 'eval/episode_reward': Array([8.570358, 8.153977], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16953507, 0.10824961], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.049929618835449, 'eval/sps': 31605.48751383146}
I0728 07:53:35.938036 139845810259776 train.py:379] starting iteration 235, 462028800 steps, 6068.193175077438
I0728 07:54:01.571090 139845810259776 train.py:394] {'eval/walltime': 969.550591468811, 'training/sps': 91105.89603525474, 'training/walltime': 5113.463465452194, 'training/entropy_loss': Array(-0.00842954, dtype=float32), 'training/policy_loss': Array(0.02760846, dtype=float32), 'training/total_loss': Array(4.1443014, dtype=float32), 'training/v_loss': Array(4.125123, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19638151, 0.11115039], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.080368,  9.514757], dtype=float32), 'eval/episode_reward': Array([7.6452856, 8.430044 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18559542, 0.11683407], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.048194885253906, 'eval/sps': 31619.03110600164}
I0728 07:54:01.573127 139845810259776 train.py:379] starting iteration 236, 463994880 steps, 6093.82826590538
I0728 07:54:27.235184 139845810259776 train.py:394] {'eval/walltime': 973.6080083847046, 'training/sps': 91022.94500477487, 'training/walltime': 5135.063293933868, 'training/entropy_loss': Array(-0.00684069, dtype=float32), 'training/policy_loss': Array(0.02878096, dtype=float32), 'training/total_loss': Array(4.0445375, dtype=float32), 'training/v_loss': Array(4.0225973, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18025032, 0.09930201], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.677092,  8.599844], dtype=float32), 'eval/episode_reward': Array([8.109335, 8.902706], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16832373, 0.10502697], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.057416915893555, 'eval/sps': 31547.164773381657}
I0728 07:54:27.237225 139845810259776 train.py:379] starting iteration 237, 465960960 steps, 6119.49236369133
I0728 07:54:52.883723 139845810259776 train.py:394] {'eval/walltime': 977.6419975757599, 'training/sps': 90988.81876693103, 'training/walltime': 5156.671223640442, 'training/entropy_loss': Array(-0.00565066, dtype=float32), 'training/policy_loss': Array(0.02781891, dtype=float32), 'training/total_loss': Array(4.062036, dtype=float32), 'training/v_loss': Array(4.039868, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16886288, 0.1026013 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.587608,  8.961591], dtype=float32), 'eval/episode_reward': Array([9.556435, 9.768987], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15434773, 0.1101597 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.033989191055298, 'eval/sps': 31730.37753393558}
I0728 07:54:52.885720 139845810259776 train.py:379] starting iteration 238, 467927040 steps, 6145.140858411789
I0728 07:55:18.608748 139845810259776 train.py:394] {'eval/walltime': 981.6964435577393, 'training/sps': 90754.37405671201, 'training/walltime': 5178.334972858429, 'training/entropy_loss': Array(-0.00427864, dtype=float32), 'training/policy_loss': Array(0.02790069, dtype=float32), 'training/total_loss': Array(3.9719188, dtype=float32), 'training/v_loss': Array(3.9482968, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19532281, 0.10693442], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.055256,  9.235199], dtype=float32), 'eval/episode_reward': Array([7.5238423, 8.710314 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18424937, 0.11276615], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.05444598197937, 'eval/sps': 31570.281258873925}
I0728 07:55:18.610763 139845810259776 train.py:379] starting iteration 239, 469893120 steps, 6170.865901470184
I0728 07:55:44.250009 139845810259776 train.py:394] {'eval/walltime': 985.7234816551208, 'training/sps': 91000.7413103253, 'training/walltime': 5199.940071582794, 'training/entropy_loss': Array(-0.00213108, dtype=float32), 'training/policy_loss': Array(0.02956307, dtype=float32), 'training/total_loss': Array(3.80837, dtype=float32), 'training/v_loss': Array(3.7809381, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17803232, 0.09791623], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([14.422382,  8.586214], dtype=float32), 'eval/episode_reward': Array([7.2310658, 8.122568 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16565952, 0.10417534], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.027038097381592, 'eval/sps': 31785.14752150631}
I0728 07:55:44.252073 139845810259776 train.py:379] starting iteration 240, 471859200 steps, 6196.507211923599
I0728 07:56:09.898518 139845810259776 train.py:394] {'eval/walltime': 989.745847940445, 'training/sps': 90947.21977490657, 'training/walltime': 5221.557884693146, 'training/entropy_loss': Array(7.717703e-05, dtype=float32), 'training/policy_loss': Array(0.03055318, dtype=float32), 'training/total_loss': Array(3.8903766, dtype=float32), 'training/v_loss': Array(3.8597457, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18844104, 0.10574736], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.362934,  9.216879], dtype=float32), 'eval/episode_reward': Array([6.837406, 8.284923], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.17648041, 0.11241214], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022366285324097, 'eval/sps': 31822.064655577873}
I0728 07:56:09.900504 139845810259776 train.py:379] starting iteration 241, 473825280 steps, 6222.155642986298
I0728 07:56:35.624861 139845810259776 train.py:394] {'eval/walltime': 993.8012142181396, 'training/sps': 90751.7652930886, 'training/walltime': 5243.222256660461, 'training/entropy_loss': Array(0.00069833, dtype=float32), 'training/policy_loss': Array(0.02909454, dtype=float32), 'training/total_loss': Array(3.8018777, dtype=float32), 'training/v_loss': Array(3.7720852, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19940709, 0.11438466], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.299427,  9.964278], dtype=float32), 'eval/episode_reward': Array([6.948582, 8.908153], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18771866, 0.12126804], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.055366277694702, 'eval/sps': 31563.11692584335}
I0728 07:56:35.626901 139845810259776 train.py:379] starting iteration 242, 475791360 steps, 6247.882038831711
I0728 07:57:01.261015 139845810259776 train.py:394] {'eval/walltime': 997.8207292556763, 'training/sps': 90981.52460264457, 'training/walltime': 5264.831918716431, 'training/entropy_loss': Array(0.00333912, dtype=float32), 'training/policy_loss': Array(0.03140025, dtype=float32), 'training/total_loss': Array(3.613013, dtype=float32), 'training/v_loss': Array(3.5782735, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20263237, 0.10739098], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.586437 ,  9.4045105], dtype=float32), 'eval/episode_reward': Array([6.7802258, 7.2521524], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.19191572, 0.11382078], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019515037536621, 'eval/sps': 31844.637675107544}
I0728 07:57:01.263022 139845810259776 train.py:379] starting iteration 243, 477757440 steps, 6273.518160820007
I0728 07:57:26.879455 139845810259776 train.py:394] {'eval/walltime': 1001.8428704738617, 'training/sps': 91067.20590236252, 'training/walltime': 5286.42124915123, 'training/entropy_loss': Array(0.00533283, dtype=float32), 'training/policy_loss': Array(0.0310024, dtype=float32), 'training/total_loss': Array(3.3934984, dtype=float32), 'training/v_loss': Array(3.3571634, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20655602, 0.10845558], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.91682 ,  9.503666], dtype=float32), 'eval/episode_reward': Array([6.1040716, 8.502182 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.19589344, 0.11497919], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022141218185425, 'eval/sps': 31823.8453242939}
I0728 07:57:26.881491 139845810259776 train.py:379] starting iteration 244, 479723520 steps, 6299.136629581451
I0728 07:57:52.614703 139845810259776 train.py:394] {'eval/walltime': 1005.893239736557, 'training/sps': 90695.09468396263, 'training/walltime': 5308.09915804863, 'training/entropy_loss': Array(0.00655518, dtype=float32), 'training/policy_loss': Array(0.03095607, dtype=float32), 'training/total_loss': Array(3.247223, dtype=float32), 'training/v_loss': Array(3.2097116, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19622597, 0.09776852], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.104702,  8.503624], dtype=float32), 'eval/episode_reward': Array([6.995791, 7.673753], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18569922, 0.10399655], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0503692626953125, 'eval/sps': 31602.056923279775}
I0728 07:57:52.616725 139845810259776 train.py:379] starting iteration 245, 481689600 steps, 6324.871863603592
I0728 07:58:18.243305 139845810259776 train.py:394] {'eval/walltime': 1009.9204051494598, 'training/sps': 91045.79482809047, 'training/walltime': 5329.693565607071, 'training/entropy_loss': Array(0.00844556, dtype=float32), 'training/policy_loss': Array(0.03344448, dtype=float32), 'training/total_loss': Array(2.9370325, dtype=float32), 'training/v_loss': Array(2.8951426, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.22887847, 0.11012807], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.92226 ,  9.513834], dtype=float32), 'eval/episode_reward': Array([4.853445, 8.409643], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2202827 , 0.11511441], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.027165412902832, 'eval/sps': 31784.142660218164}
I0728 07:58:18.245348 139845810259776 train.py:379] starting iteration 246, 483655680 steps, 6350.500485658646
I0728 07:58:43.951426 139845810259776 train.py:394] {'eval/walltime': 1013.9569065570831, 'training/sps': 90748.71726464134, 'training/walltime': 5351.35866522789, 'training/entropy_loss': Array(0.00998556, dtype=float32), 'training/policy_loss': Array(0.02858449, dtype=float32), 'training/total_loss': Array(2.7239583, dtype=float32), 'training/v_loss': Array(2.685388, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23925006, 0.11856335], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.851677, 10.319284], dtype=float32), 'eval/episode_reward': Array([5.027547, 8.619045], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.23059975, 0.12431667], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.036501407623291, 'eval/sps': 31710.62934804399}
I0728 07:58:43.953424 139845810259776 train.py:379] starting iteration 247, 485621760 steps, 6376.208562612534
I0728 07:59:09.631666 139845810259776 train.py:394] {'eval/walltime': 1017.982479095459, 'training/sps': 90820.04001909631, 'training/walltime': 5373.006750822067, 'training/entropy_loss': Array(0.01052384, dtype=float32), 'training/policy_loss': Array(0.02869961, dtype=float32), 'training/total_loss': Array(2.6655495, dtype=float32), 'training/v_loss': Array(2.626326, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23256335, 0.11489328], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.265951,  9.973829], dtype=float32), 'eval/episode_reward': Array([3.9617462, 6.7666388], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22409233, 0.1199503 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0255725383758545, 'eval/sps': 31796.71929390757}
I0728 07:59:09.633673 139845810259776 train.py:379] starting iteration 248, 487587840 steps, 6401.888811588287
I0728 07:59:35.316742 139845810259776 train.py:394] {'eval/walltime': 1022.0057709217072, 'training/sps': 90792.6338357075, 'training/walltime': 5394.6613709926605, 'training/entropy_loss': Array(0.01145321, dtype=float32), 'training/policy_loss': Array(0.02721412, dtype=float32), 'training/total_loss': Array(2.3998234, dtype=float32), 'training/v_loss': Array(2.361156, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23154087, 0.1139487 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.210781,  9.907479], dtype=float32), 'eval/episode_reward': Array([4.0359116, 7.4297714], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22312625, 0.11875398], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023291826248169, 'eval/sps': 31814.744126916477}
I0728 07:59:35.318763 139845810259776 train.py:379] starting iteration 249, 489553920 steps, 6427.573901414871
I0728 08:00:01.042454 139845810259776 train.py:394] {'eval/walltime': 1026.0626809597015, 'training/sps': 90761.85760594979, 'training/walltime': 5416.323333978653, 'training/entropy_loss': Array(0.01147465, dtype=float32), 'training/policy_loss': Array(0.02770878, dtype=float32), 'training/total_loss': Array(2.3569689, dtype=float32), 'training/v_loss': Array(2.3177855, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.22954568, 0.10077681], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.062542,  8.793731], dtype=float32), 'eval/episode_reward': Array([3.8472295, 7.311085 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22121808, 0.1057034 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.056910037994385, 'eval/sps': 31551.106334928587}
I0728 08:00:01.044537 139845810259776 train.py:379] starting iteration 250, 491520000 steps, 6453.299674749374
I0728 08:00:26.743321 139845810259776 train.py:394] {'eval/walltime': 1030.0960819721222, 'training/sps': 90768.68197650618, 'training/walltime': 5437.9836683273315, 'training/entropy_loss': Array(0.01323943, dtype=float32), 'training/policy_loss': Array(0.02704434, dtype=float32), 'training/total_loss': Array(2.6845636, dtype=float32), 'training/v_loss': Array(2.64428, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.22531545, 0.11197422], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.706264,  9.756967], dtype=float32), 'eval/episode_reward': Array([4.1329145, 7.8079762], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.21691184, 0.11680987], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.033401012420654, 'eval/sps': 31735.00467863981}
I0728 08:00:26.845191 139845810259776 train.py:379] starting iteration 251, 493486080 steps, 6479.100327253342
I0728 08:00:52.497627 139845810259776 train.py:394] {'eval/walltime': 1034.1244916915894, 'training/sps': 90941.80268462766, 'training/walltime': 5459.602769136429, 'training/entropy_loss': Array(0.01479428, dtype=float32), 'training/policy_loss': Array(0.02669596, dtype=float32), 'training/total_loss': Array(2.4460964, dtype=float32), 'training/v_loss': Array(2.4046059, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23056805, 0.1056883 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.107481,  9.275615], dtype=float32), 'eval/episode_reward': Array([4.380172, 7.28184 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22216131, 0.11109623], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.028409719467163, 'eval/sps': 31774.325084522567}
I0728 08:00:52.499809 139845810259776 train.py:379] starting iteration 252, 495452160 steps, 6504.754946947098
I0728 08:01:18.201945 139845810259776 train.py:394] {'eval/walltime': 1038.180329322815, 'training/sps': 90848.06225977572, 'training/walltime': 5481.244177341461, 'training/entropy_loss': Array(0.01514369, dtype=float32), 'training/policy_loss': Array(0.02657141, dtype=float32), 'training/total_loss': Array(2.413408, dtype=float32), 'training/v_loss': Array(2.3716931, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23012784, 0.10598807], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.140238,  9.269964], dtype=float32), 'eval/episode_reward': Array([3.4853177, 6.786439 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22225723, 0.11072491], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.055837631225586, 'eval/sps': 31559.448784275217}
I0728 08:01:18.203896 139845810259776 train.py:379] starting iteration 253, 497418240 steps, 6530.459032773972
I0728 08:01:43.878577 139845810259776 train.py:394] {'eval/walltime': 1042.2430119514465, 'training/sps': 90991.60784401653, 'training/walltime': 5502.851444721222, 'training/entropy_loss': Array(0.0145269, dtype=float32), 'training/policy_loss': Array(0.02627362, dtype=float32), 'training/total_loss': Array(2.481841, dtype=float32), 'training/v_loss': Array(2.4410405, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23220128, 0.10549624], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.287624,  9.170507], dtype=float32), 'eval/episode_reward': Array([3.432136, 7.427323], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22401273, 0.11066271], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.062682628631592, 'eval/sps': 31506.2759512459}
I0728 08:01:43.880629 139845810259776 train.py:379] starting iteration 254, 499384320 steps, 6556.135766029358
I0728 08:02:09.585879 139845810259776 train.py:394] {'eval/walltime': 1046.2858681678772, 'training/sps': 90779.3276855841, 'training/walltime': 5524.509238958359, 'training/entropy_loss': Array(0.01493948, dtype=float32), 'training/policy_loss': Array(0.02662788, dtype=float32), 'training/total_loss': Array(2.4492235, dtype=float32), 'training/v_loss': Array(2.4076562, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.24240802, 0.11658925], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.200962, 10.053031], dtype=float32), 'eval/episode_reward': Array([2.1036024, 7.4185824], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.23495507, 0.12063587], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.042856216430664, 'eval/sps': 31660.784640272956}
I0728 08:02:10.373709 139845810259776 train.py:410] total steps: 501350400
