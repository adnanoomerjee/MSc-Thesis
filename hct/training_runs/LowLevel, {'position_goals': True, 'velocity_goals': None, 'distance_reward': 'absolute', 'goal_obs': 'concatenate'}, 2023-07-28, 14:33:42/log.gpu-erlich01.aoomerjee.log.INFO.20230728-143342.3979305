I0728 14:33:42.696380 139933189490496 low_level_env.py:191] Initialising environment...
I0728 14:33:43.049912 139933189490496 low_level_env.py:299] Environment initialised.
I0728 14:33:43.057923 139933189490496 train.py:118] JAX is running on GPU.
I0728 14:33:43.057982 139933189490496 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 14:33:46.936571 139933189490496 train.py:367] Running initial eval
I0728 14:33:59.040885 139933189490496 train.py:373] {'eval/walltime': 11.96234393119812, 'eval/episode_distance_from_origin': Array([1.5793378, 0.780511 ], dtype=float32), 'eval/episode_forward_reward': Array([-3.0407548, 21.55749  ], dtype=float32), 'eval/episode_reward': Array([516.56775, 385.28357], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-122.28995,   91.19473], dtype=float32), 'eval/episode_reward_forward': Array([-3.0407548, 21.55749  ], dtype=float32), 'eval/episode_reward_survive': Array([641.89844, 478.11844], dtype=float32), 'eval/episode_x_position': Array([-0.15369336,  1.0790795 ], dtype=float32), 'eval/episode_x_velocity': Array([0.0175406, 0.4263863], dtype=float32), 'eval/episode_y_position': Array([0.06668564, 1.1500347 ], dtype=float32), 'eval/episode_y_velocity': Array([0.04709449, 0.3858922 ], dtype=float32), 'eval/avg_episode_length': Array([641.89844, 478.11844], dtype=float32), 'eval/epoch_eval_time': 11.96234393119812, 'eval/sps': 10700.244093983329}
I0728 14:33:59.042969 139933189490496 train.py:379] starting iteration 0, 0 steps, 15.98505973815918
I0728 14:34:19.177255 139933189490496 train.py:394] {'eval/walltime': 15.324855327606201, 'training/sps': 117272.09870146868, 'training/walltime': 16.76511311531067, 'training/entropy_loss': Array(-0.00260131, dtype=float32), 'training/policy_loss': Array(-0.00507583, dtype=float32), 'training/total_loss': Array(77.169846, dtype=float32), 'training/v_loss': Array(77.17752, dtype=float32), 'eval/episode_distance_from_origin': Array([1.2819465, 0.5083858], dtype=float32), 'eval/episode_forward_reward': Array([-0.4937305, 16.365229 ], dtype=float32), 'eval/episode_reward': Array([637.282 , 408.2476], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-67.864914,  43.94297 ], dtype=float32), 'eval/episode_reward_forward': Array([-0.4937305, 16.365229 ], dtype=float32), 'eval/episode_reward_survive': Array([705.6406 , 453.07193], dtype=float32), 'eval/episode_x_position': Array([-0.02164631,  0.8185537 ], dtype=float32), 'eval/episode_x_velocity': Array([0.12203719, 0.36469886], dtype=float32), 'eval/episode_y_position': Array([0.17876053, 0.8419784 ], dtype=float32), 'eval/episode_y_velocity': Array([0.01221869, 0.3955173 ], dtype=float32), 'eval/avg_episode_length': Array([705.6406 , 453.07193], dtype=float32), 'eval/epoch_eval_time': 3.362511396408081, 'eval/sps': 38066.78547966642}
I0728 14:34:19.198433 139933189490496 train.py:379] starting iteration 1, 1966080 steps, 36.140525341033936
I0728 14:34:28.283746 139933189490496 train.py:394] {'eval/walltime': 18.688493728637695, 'training/sps': 344005.2305479171, 'training/walltime': 22.480375051498413, 'training/entropy_loss': Array(-4.110666e-05, dtype=float32), 'training/policy_loss': Array(-0.01323502, dtype=float32), 'training/total_loss': Array(38.399338, dtype=float32), 'training/v_loss': Array(38.412613, dtype=float32), 'eval/episode_distance_from_origin': Array([1.8068326, 0.9728115], dtype=float32), 'eval/episode_forward_reward': Array([23.519485, 24.96287 ], dtype=float32), 'eval/episode_reward': Array([688.33606, 460.7524 ], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-24.581863,  18.00445 ], dtype=float32), 'eval/episode_reward_forward': Array([23.519485, 24.96287 ], dtype=float32), 'eval/episode_reward_survive': Array([689.39844, 460.72546], dtype=float32), 'eval/episode_x_position': Array([1.1780746, 1.2537355], dtype=float32), 'eval/episode_x_velocity': Array([0.08256053, 0.38955578], dtype=float32), 'eval/episode_y_position': Array([0.47335482, 0.7195704 ], dtype=float32), 'eval/episode_y_velocity': Array([0.04621903, 0.35430738], dtype=float32), 'eval/avg_episode_length': Array([689.39844, 460.72546], dtype=float32), 'eval/epoch_eval_time': 3.363638401031494, 'eval/sps': 38054.03100426832}
I0728 14:34:28.286190 139933189490496 train.py:379] starting iteration 2, 3932160 steps, 45.228283643722534
I0728 14:34:37.389230 139933189490496 train.py:394] {'eval/walltime': 22.05205202102661, 'training/sps': 342938.47027667216, 'training/walltime': 28.213415145874023, 'training/entropy_loss': Array(0.00291284, dtype=float32), 'training/policy_loss': Array(-0.00624498, dtype=float32), 'training/total_loss': Array(36.738777, dtype=float32), 'training/v_loss': Array(36.742107, dtype=float32), 'eval/episode_distance_from_origin': Array([4.065005, 2.339742], dtype=float32), 'eval/episode_forward_reward': Array([32.001453, 38.042397], dtype=float32), 'eval/episode_reward': Array([721.8956 , 467.01654], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-14.371444,  10.759019], dtype=float32), 'eval/episode_reward_forward': Array([32.001453, 38.042397], dtype=float32), 'eval/episode_reward_survive': Array([704.2656, 455.1292], dtype=float32), 'eval/episode_x_position': Array([1.6075537, 1.8944219], dtype=float32), 'eval/episode_x_velocity': Array([0.02280577, 0.34254077], dtype=float32), 'eval/episode_y_position': Array([3.1814654, 2.2927554], dtype=float32), 'eval/episode_y_velocity': Array([0.01981581, 0.3501769 ], dtype=float32), 'eval/avg_episode_length': Array([704.2656, 455.1292], dtype=float32), 'eval/epoch_eval_time': 3.363558292388916, 'eval/sps': 38054.937323262486}
I0728 14:34:37.391658 139933189490496 train.py:379] starting iteration 3, 5898240 steps, 54.33375096321106
I0728 14:34:46.524721 139933189490496 train.py:394] {'eval/walltime': 25.421215534210205, 'training/sps': 341462.80012311373, 'training/walltime': 33.97123122215271, 'training/entropy_loss': Array(0.0058742, dtype=float32), 'training/policy_loss': Array(0.00032018, dtype=float32), 'training/total_loss': Array(29.74247, dtype=float32), 'training/v_loss': Array(29.736275, dtype=float32), 'eval/episode_distance_from_origin': Array([4.863163, 2.46628 ], dtype=float32), 'eval/episode_forward_reward': Array([53.347416, 40.9549  ], dtype=float32), 'eval/episode_reward': Array([804.32214, 439.66098], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-16.650284,  10.966704], dtype=float32), 'eval/episode_reward_forward': Array([53.347416, 40.9549  ], dtype=float32), 'eval/episode_reward_survive': Array([767.625  , 420.03293], dtype=float32), 'eval/episode_x_position': Array([2.6630228, 2.0453851], dtype=float32), 'eval/episode_x_velocity': Array([0.03589081, 0.30238557], dtype=float32), 'eval/episode_y_position': Array([3.5894227, 2.2839143], dtype=float32), 'eval/episode_y_velocity': Array([0.078206  , 0.32420942], dtype=float32), 'eval/avg_episode_length': Array([767.625  , 420.03293], dtype=float32), 'eval/epoch_eval_time': 3.3691635131835938, 'eval/sps': 37991.62596268594}
I0728 14:34:46.527149 139933189490496 train.py:379] starting iteration 4, 7864320 steps, 63.46924185752869
I0728 14:34:55.678129 139933189490496 train.py:394] {'eval/walltime': 28.790945053100586, 'training/sps': 340442.77004716586, 'training/walltime': 39.74629878997803, 'training/entropy_loss': Array(0.00758625, dtype=float32), 'training/policy_loss': Array(-0.00193663, dtype=float32), 'training/total_loss': Array(30.64831, dtype=float32), 'training/v_loss': Array(30.642662, dtype=float32), 'eval/episode_distance_from_origin': Array([5.047764, 3.018278], dtype=float32), 'eval/episode_forward_reward': Array([50.955154, 43.758614], dtype=float32), 'eval/episode_reward': Array([701.9144, 493.7962], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-15.5407095,  12.369405 ], dtype=float32), 'eval/episode_reward_forward': Array([50.955154, 43.758614], dtype=float32), 'eval/episode_reward_survive': Array([666.5    , 468.94516], dtype=float32), 'eval/episode_x_position': Array([2.5484767, 2.1960871], dtype=float32), 'eval/episode_x_velocity': Array([0.04019425, 0.3263368 ], dtype=float32), 'eval/episode_y_position': Array([3.728156, 2.98481 ], dtype=float32), 'eval/episode_y_velocity': Array([0.04515793, 0.35946855], dtype=float32), 'eval/avg_episode_length': Array([666.5    , 468.94516], dtype=float32), 'eval/epoch_eval_time': 3.369729518890381, 'eval/sps': 37985.24459676786}
I0728 14:34:55.680568 139933189490496 train.py:379] starting iteration 5, 9830400 steps, 72.62266087532043
I0728 14:35:04.838272 139933189490496 train.py:394] {'eval/walltime': 32.163689374923706, 'training/sps': 340241.55289037945, 'training/walltime': 45.524781703948975, 'training/entropy_loss': Array(0.00900641, dtype=float32), 'training/policy_loss': Array(-0.00319522, dtype=float32), 'training/total_loss': Array(26.529835, dtype=float32), 'training/v_loss': Array(26.524023, dtype=float32), 'eval/episode_distance_from_origin': Array([7.8085027, 4.087314 ], dtype=float32), 'eval/episode_forward_reward': Array([107.45833 ,  66.626015], dtype=float32), 'eval/episode_reward': Array([841.1699 , 473.93997], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-24.874454,  14.776508], dtype=float32), 'eval/episode_reward_forward': Array([107.45833 ,  66.626015], dtype=float32), 'eval/episode_reward_survive': Array([758.58594, 427.03928], dtype=float32), 'eval/episode_x_position': Array([5.363024 , 3.3285224], dtype=float32), 'eval/episode_x_velocity': Array([0.16677958, 0.30942926], dtype=float32), 'eval/episode_y_position': Array([5.2388034, 3.1526887], dtype=float32), 'eval/episode_y_velocity': Array([0.1410662 , 0.34982726], dtype=float32), 'eval/avg_episode_length': Array([758.58594, 427.03928], dtype=float32), 'eval/epoch_eval_time': 3.37274432182312, 'eval/sps': 37951.2906364661}
I0728 14:35:04.840695 139933189490496 train.py:379] starting iteration 6, 11796480 steps, 81.78278803825378
I0728 14:35:14.011090 139933189490496 train.py:394] {'eval/walltime': 35.53664422035217, 'training/sps': 339487.0303151662, 'training/walltime': 51.316107511520386, 'training/entropy_loss': Array(0.0104206, dtype=float32), 'training/policy_loss': Array(-0.00339791, dtype=float32), 'training/total_loss': Array(28.274818, dtype=float32), 'training/v_loss': Array(28.267792, dtype=float32), 'eval/episode_distance_from_origin': Array([10.906056 ,  5.0925975], dtype=float32), 'eval/episode_forward_reward': Array([155.45473,  82.23151], dtype=float32), 'eval/episode_reward': Array([926.5992 , 453.56717], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-34.957127,  17.631962], dtype=float32), 'eval/episode_reward_forward': Array([155.45473,  82.23151], dtype=float32), 'eval/episode_reward_survive': Array([806.10156, 393.59857], dtype=float32), 'eval/episode_x_position': Array([7.781136 , 4.1105886], dtype=float32), 'eval/episode_x_velocity': Array([0.17277166, 0.29258618], dtype=float32), 'eval/episode_y_position': Array([7.2797594, 3.7463024], dtype=float32), 'eval/episode_y_velocity': Array([0.17353615, 0.35225317], dtype=float32), 'eval/avg_episode_length': Array([806.10156, 393.59857], dtype=float32), 'eval/epoch_eval_time': 3.372954845428467, 'eval/sps': 37948.92189958747}
I0728 14:35:14.013497 139933189490496 train.py:379] starting iteration 7, 13762560 steps, 90.95559024810791
I0728 14:35:23.223467 139933189490496 train.py:394] {'eval/walltime': 38.942331314086914, 'training/sps': 339091.4656405135, 'training/walltime': 57.11418914794922, 'training/entropy_loss': Array(0.01156553, dtype=float32), 'training/policy_loss': Array(-0.00239744, dtype=float32), 'training/total_loss': Array(33.513165, dtype=float32), 'training/v_loss': Array(33.503998, dtype=float32), 'eval/episode_distance_from_origin': Array([13.747161 ,  7.9269414], dtype=float32), 'eval/episode_forward_reward': Array([198.13937, 126.56326], dtype=float32), 'eval/episode_reward': Array([891.60114, 532.1503 ], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-43.21012 ,  26.400827], dtype=float32), 'eval/episode_reward_forward': Array([198.13937, 126.56326], dtype=float32), 'eval/episode_reward_survive': Array([736.6719 , 437.89664], dtype=float32), 'eval/episode_x_position': Array([9.9021015, 6.327114 ], dtype=float32), 'eval/episode_x_velocity': Array([0.19959943, 0.42935365], dtype=float32), 'eval/episode_y_position': Array([8.976309, 5.718378], dtype=float32), 'eval/episode_y_velocity': Array([0.15178165, 0.4788726 ], dtype=float32), 'eval/avg_episode_length': Array([736.6719 , 437.89664], dtype=float32), 'eval/epoch_eval_time': 3.405687093734741, 'eval/sps': 37584.19269799469}
I0728 14:35:23.225908 139933189490496 train.py:379] starting iteration 8, 15728640 steps, 100.16800117492676
I0728 14:35:32.486880 139933189490496 train.py:394] {'eval/walltime': 42.363367795944214, 'training/sps': 337037.4000708216, 'training/walltime': 62.94760704040527, 'training/entropy_loss': Array(0.01294894, dtype=float32), 'training/policy_loss': Array(-0.00206948, dtype=float32), 'training/total_loss': Array(47.655056, dtype=float32), 'training/v_loss': Array(47.644173, dtype=float32), 'eval/episode_distance_from_origin': Array([19.934464, 10.952921], dtype=float32), 'eval/episode_forward_reward': Array([307.13422, 181.2706 ], dtype=float32), 'eval/episode_reward': Array([1016.23553,  561.1878 ], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-58.015846,  32.76597 ], dtype=float32), 'eval/episode_reward_forward': Array([307.13422, 181.2706 ], dtype=float32), 'eval/episode_reward_survive': Array([767.1172 , 420.94064], dtype=float32), 'eval/episode_x_position': Array([15.359078,  9.048581], dtype=float32), 'eval/episode_x_velocity': Array([0.27145016, 0.5068438 ], dtype=float32), 'eval/episode_y_position': Array([12.054354 ,  7.3355117], dtype=float32), 'eval/episode_y_velocity': Array([0.21244577, 0.4584047 ], dtype=float32), 'eval/avg_episode_length': Array([767.1172 , 420.94064], dtype=float32), 'eval/epoch_eval_time': 3.4210364818573, 'eval/sps': 37415.561242570584}
I0728 14:35:32.489291 139933189490496 train.py:379] starting iteration 9, 17694720 steps, 109.43138456344604
I0728 14:35:41.762085 139933189490496 train.py:394] {'eval/walltime': 45.80179715156555, 'training/sps': 337308.58830511494, 'training/walltime': 68.77633500099182, 'training/entropy_loss': Array(0.01434096, dtype=float32), 'training/policy_loss': Array(-0.00148143, dtype=float32), 'training/total_loss': Array(62.61549, dtype=float32), 'training/v_loss': Array(62.60263, dtype=float32), 'eval/episode_distance_from_origin': Array([23.666002, 14.861264], dtype=float32), 'eval/episode_forward_reward': Array([364.02377, 244.71391], dtype=float32), 'eval/episode_reward': Array([1022.14374,  640.0525 ], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-62.739433,  39.927715], dtype=float32), 'eval/episode_reward_forward': Array([364.02377, 244.71391], dtype=float32), 'eval/episode_reward_survive': Array([720.8594 , 446.34418], dtype=float32), 'eval/episode_x_position': Array([18.1995  , 12.234288], dtype=float32), 'eval/episode_x_velocity': Array([0.41359234, 0.57786965], dtype=float32), 'eval/episode_y_position': Array([14.27183 ,  9.790401], dtype=float32), 'eval/episode_y_velocity': Array([0.30665648, 0.5190966 ], dtype=float32), 'eval/avg_episode_length': Array([720.8594 , 446.34418], dtype=float32), 'eval/epoch_eval_time': 3.438429355621338, 'eval/sps': 37226.29920860186}
I0728 14:35:41.764395 139933189490496 train.py:379] starting iteration 10, 19660800 steps, 118.70648860931396
I0728 14:35:51.049579 139933189490496 train.py:394] {'eval/walltime': 49.24752497673035, 'training/sps': 337053.3937334257, 'training/walltime': 74.60947608947754, 'training/entropy_loss': Array(0.0153971, dtype=float32), 'training/policy_loss': Array(-0.000949, dtype=float32), 'training/total_loss': Array(68.52568, dtype=float32), 'training/v_loss': Array(68.51123, dtype=float32), 'eval/episode_distance_from_origin': Array([30.028421, 18.314026], dtype=float32), 'eval/episode_forward_reward': Array([479.90845, 313.1181 ], dtype=float32), 'eval/episode_reward': Array([1135.2465 ,  700.11273], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-72.646454,  45.062565], dtype=float32), 'eval/episode_reward_forward': Array([479.90845, 313.1181 ], dtype=float32), 'eval/episode_reward_survive': Array([727.9844 , 443.42792], dtype=float32), 'eval/episode_x_position': Array([23.992004, 15.656457], dtype=float32), 'eval/episode_x_velocity': Array([0.47646943, 0.58735627], dtype=float32), 'eval/episode_y_position': Array([16.87822, 11.44578], dtype=float32), 'eval/episode_y_velocity': Array([0.34092617, 0.56628394], dtype=float32), 'eval/avg_episode_length': Array([727.9844 , 443.42792], dtype=float32), 'eval/epoch_eval_time': 3.445727825164795, 'eval/sps': 37147.44939086368}
I0728 14:35:51.051718 139933189490496 train.py:379] starting iteration 11, 21626880 steps, 127.99381232261658
I0728 14:36:00.339699 139933189490496 train.py:394] {'eval/walltime': 52.69331741333008, 'training/sps': 336860.5378551464, 'training/walltime': 80.44595670700073, 'training/entropy_loss': Array(0.01666324, dtype=float32), 'training/policy_loss': Array(-0.00023588, dtype=float32), 'training/total_loss': Array(66.84732, dtype=float32), 'training/v_loss': Array(66.83089, dtype=float32), 'eval/episode_distance_from_origin': Array([36.943146, 20.389584], dtype=float32), 'eval/episode_forward_reward': Array([612.953  , 360.44574], dtype=float32), 'eval/episode_reward': Array([1297.3993 ,  722.44794], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-82.99894,  46.37921], dtype=float32), 'eval/episode_reward_forward': Array([612.953  , 360.44574], dtype=float32), 'eval/episode_reward_survive': Array([767.4453 , 420.36545], dtype=float32), 'eval/episode_x_position': Array([30.639763, 18.018559], dtype=float32), 'eval/episode_x_velocity': Array([0.7252743 , 0.62909645], dtype=float32), 'eval/episode_y_position': Array([19.48603 , 11.701292], dtype=float32), 'eval/episode_y_velocity': Array([0.4360386 , 0.52495784], dtype=float32), 'eval/avg_episode_length': Array([767.4453 , 420.36545], dtype=float32), 'eval/epoch_eval_time': 3.4457924365997314, 'eval/sps': 37146.75284571375}
I0728 14:36:00.341813 139933189490496 train.py:379] starting iteration 12, 23592960 steps, 137.28390669822693
I0728 14:36:09.649190 139933189490496 train.py:394] {'eval/walltime': 56.15882587432861, 'training/sps': 336873.8311748581, 'training/walltime': 86.28220701217651, 'training/entropy_loss': Array(0.01762982, dtype=float32), 'training/policy_loss': Array(-0.00045456, dtype=float32), 'training/total_loss': Array(58.25597, dtype=float32), 'training/v_loss': Array(58.238792, dtype=float32), 'eval/episode_distance_from_origin': Array([42.220932, 23.147545], dtype=float32), 'eval/episode_forward_reward': Array([719.1741, 408.7731], dtype=float32), 'eval/episode_reward': Array([1398.2854,  775.1487], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-87.9591  ,  48.786686], dtype=float32), 'eval/episode_reward_forward': Array([719.1741, 408.7731], dtype=float32), 'eval/episode_reward_survive': Array([767.0703 , 421.02548], dtype=float32), 'eval/episode_x_position': Array([35.95394, 20.43699], dtype=float32), 'eval/episode_x_velocity': Array([0.74596226, 0.63744885], dtype=float32), 'eval/episode_y_position': Array([21.275883, 12.447043], dtype=float32), 'eval/episode_y_velocity': Array([0.45260507, 0.50612384], dtype=float32), 'eval/avg_episode_length': Array([767.0703 , 421.02548], dtype=float32), 'eval/epoch_eval_time': 3.465508460998535, 'eval/sps': 36935.41696421618}
I0728 14:36:09.651328 139933189490496 train.py:379] starting iteration 13, 25559040 steps, 146.59342122077942
I0728 14:36:18.952807 139933189490496 train.py:394] {'eval/walltime': 59.61925148963928, 'training/sps': 336922.3207760745, 'training/walltime': 92.11761736869812, 'training/entropy_loss': Array(0.01856516, dtype=float32), 'training/policy_loss': Array(-0.0001169, dtype=float32), 'training/total_loss': Array(48.76794, dtype=float32), 'training/v_loss': Array(48.74949, dtype=float32), 'eval/episode_distance_from_origin': Array([41.086586, 27.211088], dtype=float32), 'eval/episode_forward_reward': Array([736.41  , 502.0964], dtype=float32), 'eval/episode_reward': Array([1343.4691,  901.5296], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-83.46438 ,  56.026627], dtype=float32), 'eval/episode_reward_forward': Array([736.41  , 502.0964], dtype=float32), 'eval/episode_reward_survive': Array([690.52344, 459.08334], dtype=float32), 'eval/episode_x_position': Array([36.81796 , 25.102852], dtype=float32), 'eval/episode_x_velocity': Array([0.84177136, 0.7967682 ], dtype=float32), 'eval/episode_y_position': Array([17.059683, 12.301304], dtype=float32), 'eval/episode_y_velocity': Array([0.43524617, 0.5772577 ], dtype=float32), 'eval/avg_episode_length': Array([690.52344, 459.08334], dtype=float32), 'eval/epoch_eval_time': 3.460425615310669, 'eval/sps': 36989.66954633078}
I0728 14:36:18.954922 139933189490496 train.py:379] starting iteration 14, 27525120 steps, 155.8970148563385
I0728 14:36:28.260181 139933189490496 train.py:394] {'eval/walltime': 63.0759596824646, 'training/sps': 336490.61771128327, 'training/walltime': 97.9605143070221, 'training/entropy_loss': Array(0.01948629, dtype=float32), 'training/policy_loss': Array(-0.00065784, dtype=float32), 'training/total_loss': Array(39.164528, dtype=float32), 'training/v_loss': Array(39.145702, dtype=float32), 'eval/episode_distance_from_origin': Array([49.064655, 26.7514  ], dtype=float32), 'eval/episode_forward_reward': Array([894.62445, 500.3632 ], dtype=float32), 'eval/episode_reward': Array([1566.5269,  864.2158], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-95.90239 ,  53.000412], dtype=float32), 'eval/episode_reward_forward': Array([894.62445, 500.3632 ], dtype=float32), 'eval/episode_reward_survive': Array([767.8047, 419.7272], dtype=float32), 'eval/episode_x_position': Array([44.73467 , 25.013021], dtype=float32), 'eval/episode_x_velocity': Array([0.96560454, 0.7269239 ], dtype=float32), 'eval/episode_y_position': Array([19.216906, 11.242796], dtype=float32), 'eval/episode_y_velocity': Array([0.41463125, 0.51534265], dtype=float32), 'eval/avg_episode_length': Array([767.8047, 419.7272], dtype=float32), 'eval/epoch_eval_time': 3.4567081928253174, 'eval/sps': 37029.4490769208}
I0728 14:36:28.262287 139933189490496 train.py:379] starting iteration 15, 29491200 steps, 165.20438051223755
I0728 14:36:37.572638 139933189490496 train.py:394] {'eval/walltime': 66.54348158836365, 'training/sps': 336883.93259753525, 'training/walltime': 103.79658961296082, 'training/entropy_loss': Array(0.02047899, dtype=float32), 'training/policy_loss': Array(0.00062774, dtype=float32), 'training/total_loss': Array(36.95568, dtype=float32), 'training/v_loss': Array(36.934578, dtype=float32), 'eval/episode_distance_from_origin': Array([51.469456, 28.061172], dtype=float32), 'eval/episode_forward_reward': Array([933.18915, 522.0232 ], dtype=float32), 'eval/episode_reward': Array([1602.8752,  886.6978], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-97.24349 ,  53.768196], dtype=float32), 'eval/episode_reward_forward': Array([933.18915, 522.0232 ], dtype=float32), 'eval/episode_reward_survive': Array([766.9297 , 421.27036], dtype=float32), 'eval/episode_x_position': Array([46.657898, 26.100668], dtype=float32), 'eval/episode_x_velocity': Array([1.0075524, 0.6525092], dtype=float32), 'eval/episode_y_position': Array([20.755783, 12.127004], dtype=float32), 'eval/episode_y_velocity': Array([0.42488122, 0.5376957 ], dtype=float32), 'eval/avg_episode_length': Array([766.9297 , 421.27036], dtype=float32), 'eval/epoch_eval_time': 3.467521905899048, 'eval/sps': 36913.970112847084}
I0728 14:36:37.574911 139933189490496 train.py:379] starting iteration 16, 31457280 steps, 174.51700448989868
I0728 14:36:46.910422 139933189490496 train.py:394] {'eval/walltime': 70.0065987110138, 'training/sps': 335177.66487538617, 'training/walltime': 109.66237425804138, 'training/entropy_loss': Array(0.0213679, dtype=float32), 'training/policy_loss': Array(0.0002901, dtype=float32), 'training/total_loss': Array(40.745346, dtype=float32), 'training/v_loss': Array(40.723686, dtype=float32), 'eval/episode_distance_from_origin': Array([50.62814 , 28.801832], dtype=float32), 'eval/episode_forward_reward': Array([915.1167, 532.8257], dtype=float32), 'eval/episode_reward': Array([1578.3893,  898.1493], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-95.3212 ,  54.96359], dtype=float32), 'eval/episode_reward_forward': Array([915.1167, 532.8257], dtype=float32), 'eval/episode_reward_survive': Array([758.59375, 427.02557], dtype=float32), 'eval/episode_x_position': Array([45.75672 , 26.639433], dtype=float32), 'eval/episode_x_velocity': Array([0.9421859, 0.7202337], dtype=float32), 'eval/episode_y_position': Array([20.876013, 12.37419 ], dtype=float32), 'eval/episode_y_velocity': Array([0.49014485, 0.5004089 ], dtype=float32), 'eval/avg_episode_length': Array([758.59375, 427.02557], dtype=float32), 'eval/epoch_eval_time': 3.4631171226501465, 'eval/sps': 36960.92146662604}
I0728 14:36:46.912696 139933189490496 train.py:379] starting iteration 17, 33423360 steps, 183.85478973388672
I0728 14:36:56.219973 139933189490496 train.py:394] {'eval/walltime': 73.47806334495544, 'training/sps': 337224.170039418, 'training/walltime': 115.49256134033203, 'training/entropy_loss': Array(0.02191802, dtype=float32), 'training/policy_loss': Array(0.00114912, dtype=float32), 'training/total_loss': Array(38.737427, dtype=float32), 'training/v_loss': Array(38.71436, dtype=float32), 'eval/episode_distance_from_origin': Array([45.93309 , 32.008347], dtype=float32), 'eval/episode_forward_reward': Array([832.5144 , 593.36816], dtype=float32), 'eval/episode_reward': Array([1412.7665,  999.5857], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-86.255806,  61.29136 ], dtype=float32), 'eval/episode_reward_forward': Array([832.5144 , 593.36816], dtype=float32), 'eval/episode_reward_survive': Array([666.5078 , 468.93893], dtype=float32), 'eval/episode_x_position': Array([41.632393, 29.670153], dtype=float32), 'eval/episode_x_velocity': Array([0.94343626, 0.7760476 ], dtype=float32), 'eval/episode_y_position': Array([18.41162 , 13.464097], dtype=float32), 'eval/episode_y_velocity': Array([0.44543532, 0.45641428], dtype=float32), 'eval/avg_episode_length': Array([666.5078 , 468.93893], dtype=float32), 'eval/epoch_eval_time': 3.4714646339416504, 'eval/sps': 36872.04494278926}
I0728 14:36:56.222248 139933189490496 train.py:379] starting iteration 18, 35389440 steps, 193.1643421649933
I0728 14:37:05.539533 139933189490496 train.py:394] {'eval/walltime': 76.94815587997437, 'training/sps': 336569.5039161848, 'training/walltime': 121.33408880233765, 'training/entropy_loss': Array(0.02252517, dtype=float32), 'training/policy_loss': Array(0.00062801, dtype=float32), 'training/total_loss': Array(30.797373, dtype=float32), 'training/v_loss': Array(30.77422, dtype=float32), 'eval/episode_distance_from_origin': Array([50.269302, 30.377802], dtype=float32), 'eval/episode_forward_reward': Array([909.80493, 563.5772 ], dtype=float32), 'eval/episode_reward': Array([1543.3324,  946.4356], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-94.464745,  57.956146], dtype=float32), 'eval/episode_reward_forward': Array([909.80493, 563.5772 ], dtype=float32), 'eval/episode_reward_survive': Array([727.9922 , 443.41364], dtype=float32), 'eval/episode_x_position': Array([45.488453, 28.181467], dtype=float32), 'eval/episode_x_velocity': Array([1.0255742, 0.7369974], dtype=float32), 'eval/episode_y_position': Array([20.45106 , 12.948503], dtype=float32), 'eval/episode_y_velocity': Array([0.47891837, 0.5124054 ], dtype=float32), 'eval/avg_episode_length': Array([727.9922 , 443.41364], dtype=float32), 'eval/epoch_eval_time': 3.470092535018921, 'eval/sps': 36886.62440792867}
I0728 14:37:05.541794 139933189490496 train.py:379] starting iteration 19, 37355520 steps, 202.48388719558716
I0728 14:37:14.860665 139933189490496 train.py:394] {'eval/walltime': 80.42995095252991, 'training/sps': 337159.795036815, 'training/walltime': 127.16538906097412, 'training/entropy_loss': Array(0.02310504, dtype=float32), 'training/policy_loss': Array(0.00135983, dtype=float32), 'training/total_loss': Array(31.18215, dtype=float32), 'training/v_loss': Array(31.157686, dtype=float32), 'eval/episode_distance_from_origin': Array([51.420113, 31.060133], dtype=float32), 'eval/episode_forward_reward': Array([932.16174, 575.9267 ], dtype=float32), 'eval/episode_reward': Array([1565.3243 ,  960.42084], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-94.34514,  57.838  ], dtype=float32), 'eval/episode_reward_forward': Array([932.16174, 575.9267 ], dtype=float32), 'eval/episode_reward_survive': Array([727.5078 , 444.18283], dtype=float32), 'eval/episode_x_position': Array([46.60172 , 28.801264], dtype=float32), 'eval/episode_x_velocity': Array([1.0357466, 0.7758922], dtype=float32), 'eval/episode_y_position': Array([20.871529, 13.091518], dtype=float32), 'eval/episode_y_velocity': Array([0.44919705, 0.48025233], dtype=float32), 'eval/avg_episode_length': Array([727.5078 , 444.18283], dtype=float32), 'eval/epoch_eval_time': 3.481795072555542, 'eval/sps': 36762.64608705173}
I0728 14:37:14.862924 139933189490496 train.py:379] starting iteration 20, 39321600 steps, 211.8050172328949
I0728 14:37:24.194283 139933189490496 train.py:394] {'eval/walltime': 83.91294622421265, 'training/sps': 336506.95774239587, 'training/walltime': 133.00800228118896, 'training/entropy_loss': Array(0.02368362, dtype=float32), 'training/policy_loss': Array(0.00161083, dtype=float32), 'training/total_loss': Array(38.636406, dtype=float32), 'training/v_loss': Array(38.611115, dtype=float32), 'eval/episode_distance_from_origin': Array([54.38483 , 30.932959], dtype=float32), 'eval/episode_forward_reward': Array([986.083 , 572.8745], dtype=float32), 'eval/episode_reward': Array([1638.4348 ,  945.82434], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-98.51538,  56.86129], dtype=float32), 'eval/episode_reward_forward': Array([986.083 , 572.8745], dtype=float32), 'eval/episode_reward_survive': Array([750.8672 , 431.51105], dtype=float32), 'eval/episode_x_position': Array([49.297928, 28.637259], dtype=float32), 'eval/episode_x_velocity': Array([1.1570055 , 0.77754116], dtype=float32), 'eval/episode_y_position': Array([22.153791 , 13.1484995], dtype=float32), 'eval/episode_y_velocity': Array([0.51969784, 0.52054256], dtype=float32), 'eval/avg_episode_length': Array([750.8672 , 431.51105], dtype=float32), 'eval/epoch_eval_time': 3.4829952716827393, 'eval/sps': 36749.978112419136}
I0728 14:37:24.196556 139933189490496 train.py:379] starting iteration 21, 41287680 steps, 221.13864946365356
I0728 14:37:33.536803 139933189490496 train.py:394] {'eval/walltime': 87.41285991668701, 'training/sps': 336967.62980548455, 'training/walltime': 138.84262800216675, 'training/entropy_loss': Array(0.02405565, dtype=float32), 'training/policy_loss': Array(0.00125202, dtype=float32), 'training/total_loss': Array(30.418015, dtype=float32), 'training/v_loss': Array(30.392708, dtype=float32), 'eval/episode_distance_from_origin': Array([55.138195, 30.074453], dtype=float32), 'eval/episode_forward_reward': Array([999.86865, 557.8197 ], dtype=float32), 'eval/episode_reward': Array([1665.4783,  920.5456], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-101.390366,   56.08685 ], dtype=float32), 'eval/episode_reward_forward': Array([999.86865, 557.8197 ], dtype=float32), 'eval/episode_reward_survive': Array([767.     , 421.14285], dtype=float32), 'eval/episode_x_position': Array([50.006195, 27.887257], dtype=float32), 'eval/episode_x_velocity': Array([1.0583954 , 0.67437637], dtype=float32), 'eval/episode_y_position': Array([22.482162, 12.667379], dtype=float32), 'eval/episode_y_velocity': Array([0.54086596, 0.47096372], dtype=float32), 'eval/avg_episode_length': Array([767.     , 421.14285], dtype=float32), 'eval/epoch_eval_time': 3.4999136924743652, 'eval/sps': 36572.33041924148}
I0728 14:37:33.539103 139933189490496 train.py:379] starting iteration 22, 43253760 steps, 230.48119568824768
I0728 14:37:42.917144 139933189490496 train.py:394] {'eval/walltime': 90.91266512870789, 'training/sps': 334834.66289200616, 'training/walltime': 144.7144215106964, 'training/entropy_loss': Array(0.02454742, dtype=float32), 'training/policy_loss': Array(0.00182589, dtype=float32), 'training/total_loss': Array(27.912766, dtype=float32), 'training/v_loss': Array(27.88639, dtype=float32), 'eval/episode_distance_from_origin': Array([48.69234, 33.39515], dtype=float32), 'eval/episode_forward_reward': Array([889.3644 , 623.79736], dtype=float32), 'eval/episode_reward': Array([1474.427 , 1027.3237], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-88.718575,  61.846474], dtype=float32), 'eval/episode_reward_forward': Array([889.3644 , 623.79736], dtype=float32), 'eval/episode_reward_survive': Array([673.78125, 466.83063], dtype=float32), 'eval/episode_x_position': Array([44.467   , 31.182623], dtype=float32), 'eval/episode_x_velocity': Array([0.9981477, 0.7889409], dtype=float32), 'eval/episode_y_position': Array([18.863594, 13.421687], dtype=float32), 'eval/episode_y_velocity': Array([0.4387425, 0.5417089], dtype=float32), 'eval/avg_episode_length': Array([673.78125, 466.83063], dtype=float32), 'eval/epoch_eval_time': 3.499805212020874, 'eval/sps': 36573.464020327476}
I0728 14:37:42.919421 139933189490496 train.py:379] starting iteration 23, 45219840 steps, 239.86151456832886
I0728 14:37:52.259114 139933189490496 train.py:394] {'eval/walltime': 94.40424799919128, 'training/sps': 336617.86471846123, 'training/walltime': 150.5551097393036, 'training/entropy_loss': Array(0.02493204, dtype=float32), 'training/policy_loss': Array(0.00272203, dtype=float32), 'training/total_loss': Array(28.90787, dtype=float32), 'training/v_loss': Array(28.880215, dtype=float32), 'eval/episode_distance_from_origin': Array([55.716843, 30.99656 ], dtype=float32), 'eval/episode_forward_reward': Array([1018.8717,  578.6864], dtype=float32), 'eval/episode_reward': Array([1679.342 ,  945.4043], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-99.63905,  56.11409], dtype=float32), 'eval/episode_reward_forward': Array([1018.8717,  578.6864], dtype=float32), 'eval/episode_reward_survive': Array([760.1094 , 424.39752], dtype=float32), 'eval/episode_x_position': Array([50.953693, 28.932346], dtype=float32), 'eval/episode_x_velocity': Array([1.1126208 , 0.71749175], dtype=float32), 'eval/episode_y_position': Array([21.782497, 12.525321], dtype=float32), 'eval/episode_y_velocity': Array([0.5224767 , 0.51381934], dtype=float32), 'eval/avg_episode_length': Array([760.1094 , 424.39752], dtype=float32), 'eval/epoch_eval_time': 3.4915828704833984, 'eval/sps': 36659.59100729544}
I0728 14:37:52.261479 139933189490496 train.py:379] starting iteration 24, 47185920 steps, 249.2035722732544
I0728 14:38:01.640370 139933189490496 train.py:394] {'eval/walltime': 97.9018714427948, 'training/sps': 334637.5470179803, 'training/walltime': 156.43036198616028, 'training/entropy_loss': Array(0.02530234, dtype=float32), 'training/policy_loss': Array(0.00191415, dtype=float32), 'training/total_loss': Array(24.084219, dtype=float32), 'training/v_loss': Array(24.057003, dtype=float32), 'eval/episode_distance_from_origin': Array([55.798515, 31.139404], dtype=float32), 'eval/episode_forward_reward': Array([1018.7245,  580.3811], dtype=float32), 'eval/episode_reward': Array([1679.4808,  948.9226], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-98.26715,  55.57049], dtype=float32), 'eval/episode_reward_forward': Array([1018.7245,  580.3811], dtype=float32), 'eval/episode_reward_survive': Array([759.02344, 426.28464], dtype=float32), 'eval/episode_x_position': Array([50.94792 , 29.023352], dtype=float32), 'eval/episode_x_velocity': Array([1.1505301, 0.7485843], dtype=float32), 'eval/episode_y_position': Array([21.91278 , 12.823361], dtype=float32), 'eval/episode_y_velocity': Array([0.45983586, 0.49910673], dtype=float32), 'eval/avg_episode_length': Array([759.02344, 426.28464], dtype=float32), 'eval/epoch_eval_time': 3.4976234436035156, 'eval/sps': 36596.27803389971}
I0728 14:38:01.642802 139933189490496 train.py:379] starting iteration 25, 49152000 steps, 258.584894657135
I0728 14:38:10.998824 139933189490496 train.py:394] {'eval/walltime': 101.41376113891602, 'training/sps': 336771.860007302, 'training/walltime': 162.26837944984436, 'training/entropy_loss': Array(0.02552079, dtype=float32), 'training/policy_loss': Array(0.00194589, dtype=float32), 'training/total_loss': Array(30.777157, dtype=float32), 'training/v_loss': Array(30.749691, dtype=float32), 'eval/episode_distance_from_origin': Array([53.42137, 32.19756], dtype=float32), 'eval/episode_forward_reward': Array([966.8655, 594.6946], dtype=float32), 'eval/episode_reward': Array([1600.7452,  978.5912], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-94.49529 ,  57.834892], dtype=float32), 'eval/episode_reward_forward': Array([966.8655, 594.6946], dtype=float32), 'eval/episode_reward_survive': Array([728.375  , 442.80154], dtype=float32), 'eval/episode_x_position': Array([48.347996, 29.74109 ], dtype=float32), 'eval/episode_x_velocity': Array([1.0349709 , 0.78924525], dtype=float32), 'eval/episode_y_position': Array([21.896969, 13.72959 ], dtype=float32), 'eval/episode_y_velocity': Array([0.49251115, 0.4309043 ], dtype=float32), 'eval/avg_episode_length': Array([728.375  , 442.80154], dtype=float32), 'eval/epoch_eval_time': 3.511889696121216, 'eval/sps': 36447.613984394906}
I0728 14:38:11.037877 139933189490496 train.py:379] starting iteration 26, 51118080 steps, 267.9799687862396
I0728 14:38:20.419260 139933189490496 train.py:394] {'eval/walltime': 104.91753768920898, 'training/sps': 334873.89091549383, 'training/walltime': 168.13948512077332, 'training/entropy_loss': Array(0.02603422, dtype=float32), 'training/policy_loss': Array(0.00256322, dtype=float32), 'training/total_loss': Array(27.5211, dtype=float32), 'training/v_loss': Array(27.492502, dtype=float32), 'eval/episode_distance_from_origin': Array([52.592743, 32.99187 ], dtype=float32), 'eval/episode_forward_reward': Array([962.55194, 616.7061 ], dtype=float32), 'eval/episode_reward': Array([1598.0616,  990.3138], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-92.4823  ,  58.772007], dtype=float32), 'eval/episode_reward_forward': Array([962.55194, 616.7061 ], dtype=float32), 'eval/episode_reward_survive': Array([727.9922 , 443.41165], dtype=float32), 'eval/episode_x_position': Array([48.13034 , 30.832523], dtype=float32), 'eval/episode_x_velocity': Array([1.0974371 , 0.79590553], dtype=float32), 'eval/episode_y_position': Array([20.242973, 13.304423], dtype=float32), 'eval/episode_y_velocity': Array([0.48012787, 0.43842214], dtype=float32), 'eval/avg_episode_length': Array([727.9922 , 443.41165], dtype=float32), 'eval/epoch_eval_time': 3.5037765502929688, 'eval/sps': 36532.0100076865}
I0728 14:38:20.421801 139933189490496 train.py:379] starting iteration 27, 53084160 steps, 277.363894701004
I0728 14:38:29.777971 139933189490496 train.py:394] {'eval/walltime': 108.42023062705994, 'training/sps': 336216.56159357034, 'training/walltime': 173.98714470863342, 'training/entropy_loss': Array(0.02632667, dtype=float32), 'training/policy_loss': Array(0.00387097, dtype=float32), 'training/total_loss': Array(28.740799, dtype=float32), 'training/v_loss': Array(28.710602, dtype=float32), 'eval/episode_distance_from_origin': Array([49.20291, 34.69274], dtype=float32), 'eval/episode_forward_reward': Array([899.72174, 648.5414 ], dtype=float32), 'eval/episode_reward': Array([1488.2214, 1046.749 ], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-84.90641 ,  60.850563], dtype=float32), 'eval/episode_reward_forward': Array([899.72174, 648.5414 ], dtype=float32), 'eval/episode_reward_survive': Array([673.40625, 467.352  ], dtype=float32), 'eval/episode_x_position': Array([44.986282, 32.426155], dtype=float32), 'eval/episode_x_velocity': Array([1.056415  , 0.81279844], dtype=float32), 'eval/episode_y_position': Array([18.843887, 13.915532], dtype=float32), 'eval/episode_y_velocity': Array([0.38770735, 0.52468324], dtype=float32), 'eval/avg_episode_length': Array([673.40625, 467.352  ], dtype=float32), 'eval/epoch_eval_time': 3.502692937850952, 'eval/sps': 36543.3117521667}
I0728 14:38:29.780308 139933189490496 train.py:379] starting iteration 28, 55050240 steps, 286.7224016189575
I0728 14:38:39.159505 139933189490496 train.py:394] {'eval/walltime': 111.92095589637756, 'training/sps': 334831.67187740514, 'training/walltime': 179.8589906692505, 'training/entropy_loss': Array(0.02658437, dtype=float32), 'training/policy_loss': Array(0.00297963, dtype=float32), 'training/total_loss': Array(30.260765, dtype=float32), 'training/v_loss': Array(30.231201, dtype=float32), 'eval/episode_distance_from_origin': Array([46.731285, 34.866085], dtype=float32), 'eval/episode_forward_reward': Array([854.1215, 652.9357], dtype=float32), 'eval/episode_reward': Array([1408.4001, 1069.3264], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-80.73674 ,  61.393818], dtype=float32), 'eval/episode_reward_forward': Array([854.1215, 652.9357], dtype=float32), 'eval/episode_reward_survive': Array([635.0156, 479.1831], dtype=float32), 'eval/episode_x_position': Array([42.719322, 32.642212], dtype=float32), 'eval/episode_x_velocity': Array([0.9695492, 0.9201806], dtype=float32), 'eval/episode_y_position': Array([17.783737, 13.861811], dtype=float32), 'eval/episode_y_velocity': Array([0.35940188, 0.540223  ], dtype=float32), 'eval/avg_episode_length': Array([635.0156, 479.1831], dtype=float32), 'eval/epoch_eval_time': 3.500725269317627, 'eval/sps': 36563.85181718364}
I0728 14:38:39.161887 139933189490496 train.py:379] starting iteration 29, 57016320 steps, 296.10398030281067
I0728 14:38:48.510797 139933189490496 train.py:394] {'eval/walltime': 115.42114114761353, 'training/sps': 336505.9141299446, 'training/walltime': 185.70162200927734, 'training/entropy_loss': Array(0.02684112, dtype=float32), 'training/policy_loss': Array(0.00301174, dtype=float32), 'training/total_loss': Array(29.664625, dtype=float32), 'training/v_loss': Array(29.634771, dtype=float32), 'eval/episode_distance_from_origin': Array([50.14425 , 34.945526], dtype=float32), 'eval/episode_forward_reward': Array([920.45764, 656.7682 ], dtype=float32), 'eval/episode_reward': Array([1502.2876, 1062.5851], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-85.678055,  60.697243], dtype=float32), 'eval/episode_reward_forward': Array([920.45764, 656.7682 ], dtype=float32), 'eval/episode_reward_survive': Array([667.5078, 467.5825], dtype=float32), 'eval/episode_x_position': Array([46.01558 , 32.837177], dtype=float32), 'eval/episode_x_velocity': Array([1.0230103, 0.8700076], dtype=float32), 'eval/episode_y_position': Array([18.781422, 13.661901], dtype=float32), 'eval/episode_y_velocity': Array([0.48065636, 0.4749217 ], dtype=float32), 'eval/avg_episode_length': Array([667.5078, 467.5825], dtype=float32), 'eval/epoch_eval_time': 3.500185251235962, 'eval/sps': 36569.49298749302}
I0728 14:38:48.513176 139933189490496 train.py:379] starting iteration 30, 58982400 steps, 305.455269575119
I0728 14:38:57.907981 139933189490496 train.py:394] {'eval/walltime': 118.92877078056335, 'training/sps': 334355.952335425, 'training/walltime': 191.5818223953247, 'training/entropy_loss': Array(0.02698261, dtype=float32), 'training/policy_loss': Array(0.00286153, dtype=float32), 'training/total_loss': Array(25.824593, dtype=float32), 'training/v_loss': Array(25.794746, dtype=float32), 'eval/episode_distance_from_origin': Array([54.441956, 32.878464], dtype=float32), 'eval/episode_forward_reward': Array([1000.89215,  618.1747 ], dtype=float32), 'eval/episode_reward': Array([1634.1704, 1003.0626], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-94.104576,  57.70567 ], dtype=float32), 'eval/episode_reward_forward': Array([1000.89215,  618.1747 ], dtype=float32), 'eval/episode_reward_survive': Array([727.3828 , 444.38666], dtype=float32), 'eval/episode_x_position': Array([50.047302, 30.912117], dtype=float32), 'eval/episode_x_velocity': Array([1.0428417, 0.7451759], dtype=float32), 'eval/episode_y_position': Array([20.44667 , 12.885053], dtype=float32), 'eval/episode_y_velocity': Array([0.4804994 , 0.50359774], dtype=float32), 'eval/avg_episode_length': Array([727.3828 , 444.38666], dtype=float32), 'eval/epoch_eval_time': 3.507629632949829, 'eval/sps': 36491.88009976845}
I0728 14:38:57.910359 139933189490496 train.py:379] starting iteration 31, 60948480 steps, 314.85245156288147
I0728 14:39:07.264792 139933189490496 train.py:394] {'eval/walltime': 122.43099522590637, 'training/sps': 336291.8907428003, 'training/walltime': 197.42817211151123, 'training/entropy_loss': Array(0.02713287, dtype=float32), 'training/policy_loss': Array(0.00272426, dtype=float32), 'training/total_loss': Array(25.194588, dtype=float32), 'training/v_loss': Array(25.16473, dtype=float32), 'eval/episode_distance_from_origin': Array([56.332817, 32.604343], dtype=float32), 'eval/episode_forward_reward': Array([1035.0667 ,  609.13324], dtype=float32), 'eval/episode_reward': Array([1681.828 ,  983.1106], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-98.18405,  57.68133], dtype=float32), 'eval/episode_reward_forward': Array([1035.0667 ,  609.13324], dtype=float32), 'eval/episode_reward_survive': Array([744.9453 , 432.80283], dtype=float32), 'eval/episode_x_position': Array([51.74906 , 30.452353], dtype=float32), 'eval/episode_x_velocity': Array([1.2228972 , 0.71240896], dtype=float32), 'eval/episode_y_position': Array([21.505697, 12.96852 ], dtype=float32), 'eval/episode_y_velocity': Array([0.44335142, 0.4778764 ], dtype=float32), 'eval/avg_episode_length': Array([744.9453 , 432.80283], dtype=float32), 'eval/epoch_eval_time': 3.5022244453430176, 'eval/sps': 36548.20015039422}
I0728 14:39:07.267186 139933189490496 train.py:379] starting iteration 32, 62914560 steps, 324.2092800140381
I0728 14:39:16.656185 139933189490496 train.py:394] {'eval/walltime': 125.94056177139282, 'training/sps': 334759.87637530203, 'training/walltime': 203.3012773990631, 'training/entropy_loss': Array(0.02729762, dtype=float32), 'training/policy_loss': Array(0.00347158, dtype=float32), 'training/total_loss': Array(25.050385, dtype=float32), 'training/v_loss': Array(25.019615, dtype=float32), 'eval/episode_distance_from_origin': Array([60.273544, 30.053703], dtype=float32), 'eval/episode_forward_reward': Array([1107.5378 ,  562.99023], dtype=float32), 'eval/episode_reward': Array([1800.3237,  907.7698], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-105.36227 ,   53.187183], dtype=float32), 'eval/episode_reward_forward': Array([1107.5378 ,  562.99023], dtype=float32), 'eval/episode_reward_survive': Array([798.14844, 399.83224], dtype=float32), 'eval/episode_x_position': Array([55.377106, 28.154139], dtype=float32), 'eval/episode_x_velocity': Array([1.2407522 , 0.80439794], dtype=float32), 'eval/episode_y_position': Array([23.051476, 12.043338], dtype=float32), 'eval/episode_y_velocity': Array([0.4661303 , 0.44301552], dtype=float32), 'eval/avg_episode_length': Array([798.14844, 399.83224], dtype=float32), 'eval/epoch_eval_time': 3.50956654548645, 'eval/sps': 36471.74041039826}
I0728 14:39:16.658593 139933189490496 train.py:379] starting iteration 33, 64880640 steps, 333.60068678855896
