I0727 18:19:55.842249 140376670381888 low_level_env.py:187] Initialising environment...
I0727 18:20:35.925308 140376670381888 low_level_env.py:289] Environment initialised.
I0727 18:20:35.931380 140376670381888 train.py:118] JAX is running on GPU.
I0727 18:20:35.931424 140376670381888 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0727 18:20:43.830694 140376670381888 train.py:367] Running initial eval
I0727 18:20:59.270759 140376670381888 train.py:373] {'eval/walltime': 15.302613258361816, 'eval/episode_goal_distance': (Array(0.5944773, dtype=float32), Array(0.30694872, dtype=float32)), 'eval/episode_reward': (Array(-12128.729, dtype=float32), Array(6117.8647, dtype=float32)), 'eval/avg_episode_length': (Array(930.0547, dtype=float32), Array(254.338, dtype=float32)), 'eval/epoch_eval_time': 15.302613258361816, 'eval/sps': 8364.584390842974}
I0727 18:20:59.273633 140376670381888 train.py:379] starting iteration 0 23.342262029647827
I0727 18:21:32.356870 140376670381888 train.py:394] {'eval/walltime': 19.126550912857056, 'training/sps': 14000.950445214463, 'training/walltime': 29.255156755447388, 'training/entropy_loss': Array(-0.0430539, dtype=float32), 'training/policy_loss': Array(0.09249962, dtype=float32), 'training/total_loss': Array(178.70682, dtype=float32), 'training/v_loss': Array(178.65738, dtype=float32), 'eval/episode_goal_distance': (Array(0.5347246, dtype=float32), Array(0.2804418, dtype=float32)), 'eval/episode_reward': (Array(-11476.672, dtype=float32), Array(5343.674, dtype=float32)), 'eval/avg_episode_length': (Array(945.6875, dtype=float32), Array(225.81038, dtype=float32)), 'eval/epoch_eval_time': 3.8239376544952393, 'eval/sps': 33473.34908808706}
I0727 18:21:32.393449 140376670381888 train.py:379] starting iteration 1 56.462064027786255
I0727 18:21:46.496519 140376670381888 train.py:394] {'eval/walltime': 22.970494508743286, 'training/sps': 39939.52947388848, 'training/walltime': 39.51066064834595, 'training/entropy_loss': Array(-0.04407097, dtype=float32), 'training/policy_loss': Array(0.0312585, dtype=float32), 'training/total_loss': Array(230.58282, dtype=float32), 'training/v_loss': Array(230.59561, dtype=float32), 'eval/episode_goal_distance': (Array(0.5243301, dtype=float32), Array(0.23878463, dtype=float32)), 'eval/episode_reward': (Array(-10727.424, dtype=float32), Array(5467.3384, dtype=float32)), 'eval/avg_episode_length': (Array(891.28906, dtype=float32), Array(310.21484, dtype=float32)), 'eval/epoch_eval_time': 3.8439435958862305, 'eval/sps': 33299.13585022032}
I0727 18:21:46.498702 140376670381888 train.py:379] starting iteration 2 70.56733560562134
I0727 18:22:00.758379 140376670381888 train.py:394] {'eval/walltime': 26.820583820343018, 'training/sps': 39360.161078218196, 'training/walltime': 49.91712212562561, 'training/entropy_loss': Array(-0.03925946, dtype=float32), 'training/policy_loss': Array(0.03124801, dtype=float32), 'training/total_loss': Array(244.2561, dtype=float32), 'training/v_loss': Array(244.2641, dtype=float32), 'eval/episode_goal_distance': (Array(0.47424346, dtype=float32), Array(0.24276704, dtype=float32)), 'eval/episode_reward': (Array(-10417.639, dtype=float32), Array(5797.378, dtype=float32)), 'eval/avg_episode_length': (Array(891.3203, dtype=float32), Array(310.12576, dtype=float32)), 'eval/epoch_eval_time': 3.8500893115997314, 'eval/sps': 33245.982012509565}
I0727 18:22:00.760451 140376670381888 train.py:379] starting iteration 3 84.82908535003662
I0727 18:22:14.963862 140376670381888 train.py:394] {'eval/walltime': 30.683958292007446, 'training/sps': 39625.028096388545, 'training/walltime': 60.25402331352234, 'training/entropy_loss': Array(-0.02189676, dtype=float32), 'training/policy_loss': Array(0.04283492, dtype=float32), 'training/total_loss': Array(249.01898, dtype=float32), 'training/v_loss': Array(248.99805, dtype=float32), 'eval/episode_goal_distance': (Array(0.5552151, dtype=float32), Array(0.31174904, dtype=float32)), 'eval/episode_reward': (Array(-12238.027, dtype=float32), Array(6709.773, dtype=float32)), 'eval/avg_episode_length': (Array(914.5547, dtype=float32), Array(278.66693, dtype=float32)), 'eval/epoch_eval_time': 3.8633744716644287, 'eval/sps': 33131.657554504345}
I0727 18:22:14.965819 140376670381888 train.py:379] starting iteration 4 99.03445315361023
I0727 18:22:29.248240 140376670381888 train.py:394] {'eval/walltime': 34.55811095237732, 'training/sps': 39365.46688303326, 'training/walltime': 70.65908217430115, 'training/entropy_loss': Array(0.00879439, dtype=float32), 'training/policy_loss': Array(0.04964541, dtype=float32), 'training/total_loss': Array(246.84521, dtype=float32), 'training/v_loss': Array(246.78677, dtype=float32), 'eval/episode_goal_distance': (Array(0.5935311, dtype=float32), Array(0.32852456, dtype=float32)), 'eval/episode_reward': (Array(-12470.6455, dtype=float32), Array(7619.01, dtype=float32)), 'eval/avg_episode_length': (Array(844.71875, dtype=float32), Array(360.84164, dtype=float32)), 'eval/epoch_eval_time': 3.874152660369873, 'eval/sps': 33039.482751766314}
I0727 18:22:29.250374 140376670381888 train.py:379] starting iteration 5 113.319007396698
I0727 18:22:43.675824 140376670381888 train.py:394] {'eval/walltime': 38.51606869697571, 'training/sps': 39142.796272839216, 'training/walltime': 81.1233320236206, 'training/entropy_loss': Array(0.04503006, dtype=float32), 'training/policy_loss': Array(0.0435399, dtype=float32), 'training/total_loss': Array(199.35132, dtype=float32), 'training/v_loss': Array(199.26274, dtype=float32), 'eval/episode_goal_distance': (Array(0.71002245, dtype=float32), Array(0.4973959, dtype=float32)), 'eval/episode_reward': (Array(-14008.44, dtype=float32), Array(9748.57, dtype=float32)), 'eval/avg_episode_length': (Array(852.46875, dtype=float32), Array(353.36343, dtype=float32)), 'eval/epoch_eval_time': 3.9579577445983887, 'eval/sps': 32339.910696289677}
I0727 18:22:43.678510 140376670381888 train.py:379] starting iteration 6 127.74714374542236
I0727 18:22:58.200906 140376670381888 train.py:394] {'eval/walltime': 42.55720400810242, 'training/sps': 39091.47263545361, 'training/walltime': 91.60132050514221, 'training/entropy_loss': Array(0.06453225, dtype=float32), 'training/policy_loss': Array(0.07100583, dtype=float32), 'training/total_loss': Array(367.01703, dtype=float32), 'training/v_loss': Array(366.88153, dtype=float32), 'eval/episode_goal_distance': (Array(0.5524465, dtype=float32), Array(0.3175985, dtype=float32)), 'eval/episode_reward': (Array(-11304.8545, dtype=float32), Array(7200.874, dtype=float32)), 'eval/avg_episode_length': (Array(844.6094, dtype=float32), Array(361.096, dtype=float32)), 'eval/epoch_eval_time': 4.041135311126709, 'eval/sps': 31674.267290078024}
I0727 18:22:58.203154 140376670381888 train.py:379] starting iteration 7 142.27178764343262
I0727 18:23:12.776721 140376670381888 train.py:394] {'eval/walltime': 46.63901662826538, 'training/sps': 39052.24610658303, 'training/walltime': 102.08983373641968, 'training/entropy_loss': Array(0.07122605, dtype=float32), 'training/policy_loss': Array(0.11963207, dtype=float32), 'training/total_loss': Array(292.06104, dtype=float32), 'training/v_loss': Array(291.8702, dtype=float32), 'eval/episode_goal_distance': (Array(0.572533, dtype=float32), Array(0.31080472, dtype=float32)), 'eval/episode_reward': (Array(-12531.949, dtype=float32), Array(7330.582, dtype=float32)), 'eval/avg_episode_length': (Array(883.5625, dtype=float32), Array(319.58563, dtype=float32)), 'eval/epoch_eval_time': 4.081812620162964, 'eval/sps': 31358.617337728178}
I0727 18:23:12.778898 140376670381888 train.py:379] starting iteration 8 156.84753251075745
I0727 18:23:27.415776 140376670381888 train.py:394] {'eval/walltime': 50.74286937713623, 'training/sps': 38903.15453317931, 'training/walltime': 112.61854290962219, 'training/entropy_loss': Array(0.08006771, dtype=float32), 'training/policy_loss': Array(0.20061979, dtype=float32), 'training/total_loss': Array(248.50928, dtype=float32), 'training/v_loss': Array(248.2286, dtype=float32), 'eval/episode_goal_distance': (Array(0.5974946, dtype=float32), Array(0.32152405, dtype=float32)), 'eval/episode_reward': (Array(-12551.051, dtype=float32), Array(7134.9854, dtype=float32)), 'eval/avg_episode_length': (Array(891.34375, dtype=float32), Array(310.05884, dtype=float32)), 'eval/epoch_eval_time': 4.10385274887085, 'eval/sps': 31190.20292217318}
I0727 18:23:27.417840 140376670381888 train.py:379] starting iteration 9 171.486474275589
I0727 18:23:42.203628 140376670381888 train.py:394] {'eval/walltime': 54.831345319747925, 'training/sps': 38301.52101893345, 'training/walltime': 123.31263518333435, 'training/entropy_loss': Array(0.09250359, dtype=float32), 'training/policy_loss': Array(0.17948988, dtype=float32), 'training/total_loss': Array(261.84323, dtype=float32), 'training/v_loss': Array(261.57123, dtype=float32), 'eval/episode_goal_distance': (Array(0.55500084, dtype=float32), Array(0.33348033, dtype=float32)), 'eval/episode_reward': (Array(-11473.521, dtype=float32), Array(7466.7817, dtype=float32)), 'eval/avg_episode_length': (Array(852.47656, dtype=float32), Array(353.34476, dtype=float32)), 'eval/epoch_eval_time': 4.088475942611694, 'eval/sps': 31307.509643369543}
I0727 18:23:42.205702 140376670381888 train.py:379] starting iteration 10 186.27433609962463
I0727 18:23:57.171074 140376670381888 train.py:394] {'eval/walltime': 58.91703796386719, 'training/sps': 37659.00376972128, 'training/walltime': 134.18918418884277, 'training/entropy_loss': Array(0.00836876, dtype=float32), 'training/policy_loss': Array(0.1415514, dtype=float32), 'training/total_loss': Array(276.69318, dtype=float32), 'training/v_loss': Array(276.54327, dtype=float32), 'eval/episode_goal_distance': (Array(0.49573094, dtype=float32), Array(0.25890028, dtype=float32)), 'eval/episode_reward': (Array(-10524.678, dtype=float32), Array(6260.993, dtype=float32)), 'eval/avg_episode_length': (Array(868.0625, dtype=float32), Array(337.13684, dtype=float32)), 'eval/epoch_eval_time': 4.085692644119263, 'eval/sps': 31328.837274197966}
I0727 18:23:57.173122 140376670381888 train.py:379] starting iteration 11 201.24175596237183
I0727 18:24:12.289319 140376670381888 train.py:394] {'eval/walltime': 63.00366425514221, 'training/sps': 37146.63493682753, 'training/walltime': 145.21575498580933, 'training/entropy_loss': Array(-0.00936298, dtype=float32), 'training/policy_loss': Array(0.07889238, dtype=float32), 'training/total_loss': Array(270.57904, dtype=float32), 'training/v_loss': Array(270.5095, dtype=float32), 'eval/episode_goal_distance': (Array(0.51069385, dtype=float32), Array(0.2799436, dtype=float32)), 'eval/episode_reward': (Array(-10767.079, dtype=float32), Array(6112.655, dtype=float32)), 'eval/avg_episode_length': (Array(875.8125, dtype=float32), Array(328.5702, dtype=float32)), 'eval/epoch_eval_time': 4.086626291275024, 'eval/sps': 31321.679761440602}
I0727 18:24:12.291365 140376670381888 train.py:379] starting iteration 12 216.35999941825867
I0727 18:24:27.552375 140376670381888 train.py:394] {'eval/walltime': 67.10665106773376, 'training/sps': 36719.26470801027, 'training/walltime': 156.3706624507904, 'training/entropy_loss': Array(-0.00259687, dtype=float32), 'training/policy_loss': Array(0.07586984, dtype=float32), 'training/total_loss': Array(311.2857, dtype=float32), 'training/v_loss': Array(311.2124, dtype=float32), 'eval/episode_goal_distance': (Array(0.5062312, dtype=float32), Array(0.27857596, dtype=float32)), 'eval/episode_reward': (Array(-9942.418, dtype=float32), Array(6895.958, dtype=float32)), 'eval/avg_episode_length': (Array(782.5078, dtype=float32), Array(411.02216, dtype=float32)), 'eval/epoch_eval_time': 4.102986812591553, 'eval/sps': 31196.785621436567}
I0727 18:24:27.554628 140376670381888 train.py:379] starting iteration 13 231.6232614517212
I0727 18:24:42.873658 140376670381888 train.py:394] {'eval/walltime': 71.19852876663208, 'training/sps': 36492.673071484016, 'training/walltime': 167.59483337402344, 'training/entropy_loss': Array(-0.00063768, dtype=float32), 'training/policy_loss': Array(0.07244214, dtype=float32), 'training/total_loss': Array(309.27216, dtype=float32), 'training/v_loss': Array(309.20038, dtype=float32), 'eval/episode_goal_distance': (Array(0.54045093, dtype=float32), Array(0.27692845, dtype=float32)), 'eval/episode_reward': (Array(-10455.902, dtype=float32), Array(6450.9526, dtype=float32)), 'eval/avg_episode_length': (Array(821.35156, dtype=float32), Array(381.70786, dtype=float32)), 'eval/epoch_eval_time': 4.091877698898315, 'eval/sps': 31281.482345980752}
I0727 18:24:42.875697 140376670381888 train.py:379] starting iteration 14 246.94433093070984
I0727 18:24:58.159460 140376670381888 train.py:394] {'eval/walltime': 75.2813172340393, 'training/sps': 36578.10457589398, 'training/walltime': 178.79278922080994, 'training/entropy_loss': Array(-0.0014131, dtype=float32), 'training/policy_loss': Array(0.0638781, dtype=float32), 'training/total_loss': Array(300.0509, dtype=float32), 'training/v_loss': Array(299.98843, dtype=float32), 'eval/episode_goal_distance': (Array(0.5719023, dtype=float32), Array(0.31932527, dtype=float32)), 'eval/episode_reward': (Array(-11570.76, dtype=float32), Array(6743.6367, dtype=float32)), 'eval/avg_episode_length': (Array(875.7578, dtype=float32), Array(328.71472, dtype=float32)), 'eval/epoch_eval_time': 4.082788467407227, 'eval/sps': 31351.122161194493}
I0727 18:24:58.161531 140376670381888 train.py:379] starting iteration 15 262.23016571998596
I0727 18:25:13.639208 140376670381888 train.py:394] {'eval/walltime': 79.39672946929932, 'training/sps': 36058.21440293586, 'training/walltime': 190.15219807624817, 'training/entropy_loss': Array(-0.00961829, dtype=float32), 'training/policy_loss': Array(0.04043408, dtype=float32), 'training/total_loss': Array(215.38083, dtype=float32), 'training/v_loss': Array(215.35, dtype=float32), 'eval/episode_goal_distance': (Array(0.62044775, dtype=float32), Array(0.36618152, dtype=float32)), 'eval/episode_reward': (Array(-11788.837, dtype=float32), Array(6955.173, dtype=float32)), 'eval/avg_episode_length': (Array(860.1797, dtype=float32), Array(345.64572, dtype=float32)), 'eval/epoch_eval_time': 4.11541223526001, 'eval/sps': 31102.59499724528}
I0727 18:25:13.641248 140376670381888 train.py:379] starting iteration 16 277.70988178253174
I0727 18:25:29.133817 140376670381888 train.py:394] {'eval/walltime': 83.47783279418945, 'training/sps': 35903.23870577494, 'training/walltime': 201.56063961982727, 'training/entropy_loss': Array(-0.00222145, dtype=float32), 'training/policy_loss': Array(0.0261262, dtype=float32), 'training/total_loss': Array(210.22786, dtype=float32), 'training/v_loss': Array(210.20395, dtype=float32), 'eval/episode_goal_distance': (Array(0.6244019, dtype=float32), Array(0.38117573, dtype=float32)), 'eval/episode_reward': (Array(-11789.932, dtype=float32), Array(6800.6016, dtype=float32)), 'eval/avg_episode_length': (Array(891.2969, dtype=float32), Array(310.19238, dtype=float32)), 'eval/epoch_eval_time': 4.081103324890137, 'eval/sps': 31364.067461694507}
I0727 18:25:29.135952 140376670381888 train.py:379] starting iteration 17 293.20458602905273
I0727 18:25:44.622774 140376670381888 train.py:394] {'eval/walltime': 87.59830498695374, 'training/sps': 36045.46812488411, 'training/walltime': 212.9240653514862, 'training/entropy_loss': Array(0.006322, dtype=float32), 'training/policy_loss': Array(0.02391322, dtype=float32), 'training/total_loss': Array(231.51846, dtype=float32), 'training/v_loss': Array(231.48822, dtype=float32), 'eval/episode_goal_distance': (Array(0.63731265, dtype=float32), Array(0.3816799, dtype=float32)), 'eval/episode_reward': (Array(-11594.58, dtype=float32), Array(7565.106, dtype=float32)), 'eval/avg_episode_length': (Array(829.125, dtype=float32), Array(375.07755, dtype=float32)), 'eval/epoch_eval_time': 4.120472192764282, 'eval/sps': 31064.40087734926}
I0727 18:25:44.624921 140376670381888 train.py:379] starting iteration 18 308.69355511665344
I0727 18:26:00.061016 140376670381888 train.py:394] {'eval/walltime': 91.69214844703674, 'training/sps': 36123.24331834761, 'training/walltime': 224.2630250453949, 'training/entropy_loss': Array(0.01765153, dtype=float32), 'training/policy_loss': Array(0.02173944, dtype=float32), 'training/total_loss': Array(255.0597, dtype=float32), 'training/v_loss': Array(255.02031, dtype=float32), 'eval/episode_goal_distance': (Array(0.61033386, dtype=float32), Array(0.38017854, dtype=float32)), 'eval/episode_reward': (Array(-11091.724, dtype=float32), Array(7541.723, dtype=float32)), 'eval/avg_episode_length': (Array(805.83594, dtype=float32), Array(394.11072, dtype=float32)), 'eval/epoch_eval_time': 4.093843460083008, 'eval/sps': 31266.461760949853}
I0727 18:26:00.063258 140376670381888 train.py:379] starting iteration 19 324.1318919658661
I0727 18:26:15.604213 140376670381888 train.py:394] {'eval/walltime': 95.777179479599, 'training/sps': 35763.81054936775, 'training/walltime': 235.71594333648682, 'training/entropy_loss': Array(0.02525542, dtype=float32), 'training/policy_loss': Array(0.02203402, dtype=float32), 'training/total_loss': Array(269.21594, dtype=float32), 'training/v_loss': Array(269.16867, dtype=float32), 'eval/episode_goal_distance': (Array(0.58867836, dtype=float32), Array(0.36448303, dtype=float32)), 'eval/episode_reward': (Array(-11352.098, dtype=float32), Array(6244.1855, dtype=float32)), 'eval/avg_episode_length': (Array(883.5078, dtype=float32), Array(319.7357, dtype=float32)), 'eval/epoch_eval_time': 4.085031032562256, 'eval/sps': 31333.911292153516}
I0727 18:26:15.606131 140376670381888 train.py:379] starting iteration 20 339.67476439476013
I0727 18:26:31.139870 140376670381888 train.py:394] {'eval/walltime': 99.86872172355652, 'training/sps': 35806.996285985864, 'training/walltime': 247.1550486087799, 'training/entropy_loss': Array(0.03258535, dtype=float32), 'training/policy_loss': Array(0.01676301, dtype=float32), 'training/total_loss': Array(165.14474, dtype=float32), 'training/v_loss': Array(165.0954, dtype=float32), 'eval/episode_goal_distance': (Array(0.5862219, dtype=float32), Array(0.38031998, dtype=float32)), 'eval/episode_reward': (Array(-11553.875, dtype=float32), Array(6549.8833, dtype=float32)), 'eval/avg_episode_length': (Array(867.9219, dtype=float32), Array(337.496, dtype=float32)), 'eval/epoch_eval_time': 4.0915422439575195, 'eval/sps': 31284.047033617517}
I0727 18:26:31.141894 140376670381888 train.py:379] starting iteration 21 355.21052861213684
I0727 18:26:46.660655 140376670381888 train.py:394] {'eval/walltime': 103.95052552223206, 'training/sps': 35823.31494590732, 'training/walltime': 258.58894300460815, 'training/entropy_loss': Array(0.04799826, dtype=float32), 'training/policy_loss': Array(0.01639283, dtype=float32), 'training/total_loss': Array(133.52362, dtype=float32), 'training/v_loss': Array(133.45923, dtype=float32), 'eval/episode_goal_distance': (Array(0.59681374, dtype=float32), Array(0.4026271, dtype=float32)), 'eval/episode_reward': (Array(-11643.441, dtype=float32), Array(6474.8735, dtype=float32)), 'eval/avg_episode_length': (Array(891.2344, dtype=float32), Array(310.37054, dtype=float32)), 'eval/epoch_eval_time': 4.081803798675537, 'eval/sps': 31358.685109150374}
I0727 18:26:46.662687 140376670381888 train.py:379] starting iteration 22 370.7313208580017
I0727 18:27:02.257152 140376670381888 train.py:394] {'eval/walltime': 108.03778052330017, 'training/sps': 35604.434505983365, 'training/walltime': 270.0931279659271, 'training/entropy_loss': Array(0.06383931, dtype=float32), 'training/policy_loss': Array(0.01757735, dtype=float32), 'training/total_loss': Array(152.89957, dtype=float32), 'training/v_loss': Array(152.81815, dtype=float32), 'eval/episode_goal_distance': (Array(0.56765455, dtype=float32), Array(0.37453854, dtype=float32)), 'eval/episode_reward': (Array(-11035.777, dtype=float32), Array(6769.8604, dtype=float32)), 'eval/avg_episode_length': (Array(852.4219, dtype=float32), Array(353.47528, dtype=float32)), 'eval/epoch_eval_time': 4.087255001068115, 'eval/sps': 31316.861797600097}
I0727 18:27:02.260681 140376670381888 train.py:379] starting iteration 23 386.3293044567108
I0727 18:27:17.871651 140376670381888 train.py:394] {'eval/walltime': 112.12209582328796, 'training/sps': 35544.732204713975, 'training/walltime': 281.61663579940796, 'training/entropy_loss': Array(0.08394961, dtype=float32), 'training/policy_loss': Array(0.02205688, dtype=float32), 'training/total_loss': Array(168.9191, dtype=float32), 'training/v_loss': Array(168.81308, dtype=float32), 'eval/episode_goal_distance': (Array(0.6045193, dtype=float32), Array(0.3545763, dtype=float32)), 'eval/episode_reward': (Array(-10144.627, dtype=float32), Array(6965.1113, dtype=float32)), 'eval/avg_episode_length': (Array(767.0781, dtype=float32), Array(420.9821, dtype=float32)), 'eval/epoch_eval_time': 4.084315299987793, 'eval/sps': 31339.402224990456}
I0727 18:27:17.873573 140376670381888 train.py:379] starting iteration 24 401.94220662117004
I0727 18:27:33.495986 140376670381888 train.py:394] {'eval/walltime': 116.22145056724548, 'training/sps': 35555.58136953651, 'training/walltime': 293.1366274356842, 'training/entropy_loss': Array(0.11250955, dtype=float32), 'training/policy_loss': Array(0.02491209, dtype=float32), 'training/total_loss': Array(184.00337, dtype=float32), 'training/v_loss': Array(183.86594, dtype=float32), 'eval/episode_goal_distance': (Array(0.5864656, dtype=float32), Array(0.3854668, dtype=float32)), 'eval/episode_reward': (Array(-11469.013, dtype=float32), Array(7604.8047, dtype=float32)), 'eval/avg_episode_length': (Array(836.91406, dtype=float32), Array(368.12842, dtype=float32)), 'eval/epoch_eval_time': 4.0993547439575195, 'eval/sps': 31224.426280422056}
I0727 18:27:33.497933 140376670381888 train.py:379] starting iteration 25 417.56656670570374
I0727 18:27:49.128163 140376670381888 train.py:394] {'eval/walltime': 120.34887361526489, 'training/sps': 35617.862423528306, 'training/walltime': 304.63647532463074, 'training/entropy_loss': Array(0.14785078, dtype=float32), 'training/policy_loss': Array(0.02102891, dtype=float32), 'training/total_loss': Array(154.0762, dtype=float32), 'training/v_loss': Array(153.90732, dtype=float32), 'eval/episode_goal_distance': (Array(0.5871727, dtype=float32), Array(0.38303262, dtype=float32)), 'eval/episode_reward': (Array(-10642.822, dtype=float32), Array(6702.749, dtype=float32)), 'eval/avg_episode_length': (Array(836.84375, dtype=float32), Array(368.28723, dtype=float32)), 'eval/epoch_eval_time': 4.127423048019409, 'eval/sps': 31012.086357714714}
I0727 18:27:49.167320 140376670381888 train.py:379] starting iteration 26 433.2359366416931
I0727 18:28:04.661923 140376670381888 train.py:394] {'eval/walltime': 124.4582166671753, 'training/sps': 35987.11153890062, 'training/walltime': 316.0183279514313, 'training/entropy_loss': Array(0.19399945, dtype=float32), 'training/policy_loss': Array(0.0256805, dtype=float32), 'training/total_loss': Array(111.68144, dtype=float32), 'training/v_loss': Array(111.46176, dtype=float32), 'eval/episode_goal_distance': (Array(0.56903005, dtype=float32), Array(0.35470524, dtype=float32)), 'eval/episode_reward': (Array(-10889.475, dtype=float32), Array(6692.9683, dtype=float32)), 'eval/avg_episode_length': (Array(844.66406, dtype=float32), Array(360.96893, dtype=float32)), 'eval/epoch_eval_time': 4.1093430519104, 'eval/sps': 31148.531135772137}
I0727 18:28:04.663993 140376670381888 train.py:379] starting iteration 27 448.73262643814087
I0727 18:28:20.277193 140376670381888 train.py:394] {'eval/walltime': 128.5514054298401, 'training/sps': 35564.414629912186, 'training/walltime': 327.5354583263397, 'training/entropy_loss': Array(0.25123435, dtype=float32), 'training/policy_loss': Array(0.02851252, dtype=float32), 'training/total_loss': Array(134.13736, dtype=float32), 'training/v_loss': Array(133.8576, dtype=float32), 'eval/episode_goal_distance': (Array(0.58283114, dtype=float32), Array(0.36397356, dtype=float32)), 'eval/episode_reward': (Array(-11131.603, dtype=float32), Array(7127.941, dtype=float32)), 'eval/avg_episode_length': (Array(821.3047, dtype=float32), Array(381.8075, dtype=float32)), 'eval/epoch_eval_time': 4.093188762664795, 'eval/sps': 31271.46276944921}
I0727 18:28:20.279376 140376670381888 train.py:379] starting iteration 28 464.34800958633423
I0727 18:28:35.886503 140376670381888 train.py:394] {'eval/walltime': 132.64548921585083, 'training/sps': 35586.236514308635, 'training/walltime': 339.045526266098, 'training/entropy_loss': Array(0.3175449, dtype=float32), 'training/policy_loss': Array(0.03330646, dtype=float32), 'training/total_loss': Array(153.60419, dtype=float32), 'training/v_loss': Array(153.25334, dtype=float32), 'eval/episode_goal_distance': (Array(0.5674416, dtype=float32), Array(0.35241777, dtype=float32)), 'eval/episode_reward': (Array(-11028.924, dtype=float32), Array(6633.76, dtype=float32)), 'eval/avg_episode_length': (Array(821.4453, dtype=float32), Array(381.50726, dtype=float32)), 'eval/epoch_eval_time': 4.094083786010742, 'eval/sps': 31264.62639513361}
I0727 18:28:35.888478 140376670381888 train.py:379] starting iteration 29 479.95711183547974
I0727 18:28:51.540177 140376670381888 train.py:394] {'eval/walltime': 136.75389051437378, 'training/sps': 35493.36448564341, 'training/walltime': 350.585711479187, 'training/entropy_loss': Array(0.3858323, dtype=float32), 'training/policy_loss': Array(0.03691665, dtype=float32), 'training/total_loss': Array(174.8357, dtype=float32), 'training/v_loss': Array(174.41296, dtype=float32), 'eval/episode_goal_distance': (Array(0.6004627, dtype=float32), Array(0.36312005, dtype=float32)), 'eval/episode_reward': (Array(-11206.7295, dtype=float32), Array(7273.349, dtype=float32)), 'eval/avg_episode_length': (Array(821.3047, dtype=float32), Array(381.8076, dtype=float32)), 'eval/epoch_eval_time': 4.108401298522949, 'eval/sps': 31155.6711964867}
I0727 18:28:51.542089 140376670381888 train.py:379] starting iteration 30 495.6107225418091
I0727 18:29:07.153488 140376670381888 train.py:394] {'eval/walltime': 140.87185168266296, 'training/sps': 35647.34839703257, 'training/walltime': 362.07604718208313, 'training/entropy_loss': Array(0.45056787, dtype=float32), 'training/policy_loss': Array(0.03879494, dtype=float32), 'training/total_loss': Array(149.85391, dtype=float32), 'training/v_loss': Array(149.36456, dtype=float32), 'eval/episode_goal_distance': (Array(0.6603588, dtype=float32), Array(0.4102329, dtype=float32)), 'eval/episode_reward': (Array(-12541.646, dtype=float32), Array(7463.1016, dtype=float32)), 'eval/avg_episode_length': (Array(867.9531, dtype=float32), Array(337.4162, dtype=float32)), 'eval/epoch_eval_time': 4.117961168289185, 'eval/sps': 31083.343132440918}
I0727 18:29:07.155724 140376670381888 train.py:379] starting iteration 31 511.22435784339905
I0727 18:29:22.736128 140376670381888 train.py:394] {'eval/walltime': 144.96126699447632, 'training/sps': 35654.82205150899, 'training/walltime': 373.56397438049316, 'training/entropy_loss': Array(0.5171312, dtype=float32), 'training/policy_loss': Array(0.05130639, dtype=float32), 'training/total_loss': Array(104.03953, dtype=float32), 'training/v_loss': Array(103.4711, dtype=float32), 'eval/episode_goal_distance': (Array(0.590837, dtype=float32), Array(0.36036134, dtype=float32)), 'eval/episode_reward': (Array(-10917.941, dtype=float32), Array(6845.042, dtype=float32)), 'eval/avg_episode_length': (Array(829.14844, dtype=float32), Array(375.0259, dtype=float32)), 'eval/epoch_eval_time': 4.0894153118133545, 'eval/sps': 31300.31807486959}
I0727 18:29:22.738318 140376670381888 train.py:379] starting iteration 32 526.8069519996643
I0727 18:29:38.317472 140376670381888 train.py:394] {'eval/walltime': 149.05318307876587, 'training/sps': 35666.18279730507, 'training/walltime': 385.04824233055115, 'training/entropy_loss': Array(0.46393028, dtype=float32), 'training/policy_loss': Array(0.08373189, dtype=float32), 'training/total_loss': Array(164.23277, dtype=float32), 'training/v_loss': Array(163.68512, dtype=float32), 'eval/episode_goal_distance': (Array(0.5787389, dtype=float32), Array(0.29412192, dtype=float32)), 'eval/episode_reward': (Array(-12694.337, dtype=float32), Array(6665.2334, dtype=float32)), 'eval/avg_episode_length': (Array(906.875, dtype=float32), Array(289.5378, dtype=float32)), 'eval/epoch_eval_time': 4.091916084289551, 'eval/sps': 31281.188901072906}
I0727 18:29:38.319450 140376670381888 train.py:379] starting iteration 33 542.3880834579468
I0727 18:29:53.907413 140376670381888 train.py:394] {'eval/walltime': 153.135582447052, 'training/sps': 35609.227710391715, 'training/walltime': 396.55087876319885, 'training/entropy_loss': Array(0.13306776, dtype=float32), 'training/policy_loss': Array(0.01039596, dtype=float32), 'training/total_loss': Array(266.84125, dtype=float32), 'training/v_loss': Array(266.69778, dtype=float32), 'eval/episode_goal_distance': (Array(0.5392386, dtype=float32), Array(0.2673632, dtype=float32)), 'eval/episode_reward': (Array(-11800.404, dtype=float32), Array(6496.6514, dtype=float32)), 'eval/avg_episode_length': (Array(883.58594, dtype=float32), Array(319.52103, dtype=float32)), 'eval/epoch_eval_time': 4.082399368286133, 'eval/sps': 31354.110279939803}
I0727 18:29:53.909337 140376670381888 train.py:379] starting iteration 34 557.9779713153839
I0727 18:30:09.477589 140376670381888 train.py:394] {'eval/walltime': 157.21451830863953, 'training/sps': 35659.87457259717, 'training/walltime': 408.03717827796936, 'training/entropy_loss': Array(0.14318109, dtype=float32), 'training/policy_loss': Array(0.00705797, dtype=float32), 'training/total_loss': Array(257.80154, dtype=float32), 'training/v_loss': Array(257.65134, dtype=float32), 'eval/episode_goal_distance': (Array(0.57096756, dtype=float32), Array(0.2817524, dtype=float32)), 'eval/episode_reward': (Array(-12386.872, dtype=float32), Array(6545.0493, dtype=float32)), 'eval/avg_episode_length': (Array(906.89844, dtype=float32), Array(289.46515, dtype=float32)), 'eval/epoch_eval_time': 4.078935861587524, 'eval/sps': 31380.73368728635}
I0727 18:30:09.479754 140376670381888 train.py:379] starting iteration 35 573.5483884811401
I0727 18:30:25.066524 140376670381888 train.py:394] {'eval/walltime': 161.30928492546082, 'training/sps': 35651.80913038163, 'training/walltime': 419.5260763168335, 'training/entropy_loss': Array(0.14674428, dtype=float32), 'training/policy_loss': Array(0.00653974, dtype=float32), 'training/total_loss': Array(183.75687, dtype=float32), 'training/v_loss': Array(183.60358, dtype=float32), 'eval/episode_goal_distance': (Array(0.54150385, dtype=float32), Array(0.23317389, dtype=float32)), 'eval/episode_reward': (Array(-11830.656, dtype=float32), Array(5813.0596, dtype=float32)), 'eval/avg_episode_length': (Array(899.02344, dtype=float32), Array(300.32993, dtype=float32)), 'eval/epoch_eval_time': 4.094766616821289, 'eval/sps': 31259.41280125133}
I0727 18:30:25.068590 140376670381888 train.py:379] starting iteration 36 589.1372237205505
I0727 18:30:40.640147 140376670381888 train.py:394] {'eval/walltime': 165.3999285697937, 'training/sps': 35691.24227680198, 'training/walltime': 431.00228095054626, 'training/entropy_loss': Array(0.14915851, dtype=float32), 'training/policy_loss': Array(0.00609959, dtype=float32), 'training/total_loss': Array(162.09148, dtype=float32), 'training/v_loss': Array(161.93622, dtype=float32), 'eval/episode_goal_distance': (Array(0.55759, dtype=float32), Array(0.3014677, dtype=float32)), 'eval/episode_reward': (Array(-12072.735, dtype=float32), Array(7462.2197, dtype=float32)), 'eval/avg_episode_length': (Array(867.9922, dtype=float32), Array(337.31638, dtype=float32)), 'eval/epoch_eval_time': 4.090643644332886, 'eval/sps': 31290.91926091612}
I0727 18:30:40.642312 140376670381888 train.py:379] starting iteration 37 604.7109463214874
I0727 18:30:56.306279 140376670381888 train.py:394] {'eval/walltime': 169.52520942687988, 'training/sps': 35507.496166949, 'training/walltime': 442.53787326812744, 'training/entropy_loss': Array(0.14742959, dtype=float32), 'training/policy_loss': Array(0.00589368, dtype=float32), 'training/total_loss': Array(167.20311, dtype=float32), 'training/v_loss': Array(167.04979, dtype=float32), 'eval/episode_goal_distance': (Array(0.5554399, dtype=float32), Array(0.2877343, dtype=float32)), 'eval/episode_reward': (Array(-12427.943, dtype=float32), Array(6332.32, dtype=float32)), 'eval/avg_episode_length': (Array(922.375, dtype=float32), Array(266.65103, dtype=float32)), 'eval/epoch_eval_time': 4.125280857086182, 'eval/sps': 31028.19042735687}
I0727 18:30:56.308233 140376670381888 train.py:379] starting iteration 38 620.376867055893
I0727 18:31:11.955254 140376670381888 train.py:394] {'eval/walltime': 173.6118667125702, 'training/sps': 35440.60669994712, 'training/walltime': 454.095237493515, 'training/entropy_loss': Array(0.14274827, dtype=float32), 'training/policy_loss': Array(0.00566329, dtype=float32), 'training/total_loss': Array(174.09796, dtype=float32), 'training/v_loss': Array(173.94955, dtype=float32), 'eval/episode_goal_distance': (Array(0.56299484, dtype=float32), Array(0.27550602, dtype=float32)), 'eval/episode_reward': (Array(-12499.988, dtype=float32), Array(6797.905, dtype=float32)), 'eval/avg_episode_length': (Array(891.3203, dtype=float32), Array(310.12585, dtype=float32)), 'eval/epoch_eval_time': 4.086657285690308, 'eval/sps': 31321.44220857477}
I0727 18:31:11.957216 140376670381888 train.py:379] starting iteration 39 636.0258502960205
I0727 18:31:27.576079 140376670381888 train.py:394] {'eval/walltime': 177.71293091773987, 'training/sps': 35571.39616411956, 'training/walltime': 465.610107421875, 'training/entropy_loss': Array(0.13979857, dtype=float32), 'training/policy_loss': Array(0.00559166, dtype=float32), 'training/total_loss': Array(170.61409, dtype=float32), 'training/v_loss': Array(170.46869, dtype=float32), 'eval/episode_goal_distance': (Array(0.59055877, dtype=float32), Array(0.33460233, dtype=float32)), 'eval/episode_reward': (Array(-12688.037, dtype=float32), Array(7484.514, dtype=float32)), 'eval/avg_episode_length': (Array(883.5469, dtype=float32), Array(319.6286, dtype=float32)), 'eval/epoch_eval_time': 4.101064205169678, 'eval/sps': 31211.41089150642}
I0727 18:31:27.578073 140376670381888 train.py:379] starting iteration 40 651.6467077732086
I0727 18:31:43.150322 140376670381888 train.py:394] {'eval/walltime': 181.82076287269592, 'training/sps': 35737.187520587264, 'training/walltime': 477.07155776023865, 'training/entropy_loss': Array(0.13535121, dtype=float32), 'training/policy_loss': Array(0.00546298, dtype=float32), 'training/total_loss': Array(182.80542, dtype=float32), 'training/v_loss': Array(182.66458, dtype=float32), 'eval/episode_goal_distance': (Array(0.5401243, dtype=float32), Array(0.25876638, dtype=float32)), 'eval/episode_reward': (Array(-12337.887, dtype=float32), Array(6356.85, dtype=float32)), 'eval/avg_episode_length': (Array(922.3281, dtype=float32), Array(266.81186, dtype=float32)), 'eval/epoch_eval_time': 4.107831954956055, 'eval/sps': 31159.989357785045}
I0727 18:31:43.152509 140376670381888 train.py:379] starting iteration 41 667.2211430072784
I0727 18:31:58.719389 140376670381888 train.py:394] {'eval/walltime': 185.9203929901123, 'training/sps': 35728.36780384536, 'training/walltime': 488.5358374118805, 'training/entropy_loss': Array(0.13084322, dtype=float32), 'training/policy_loss': Array(0.00520835, dtype=float32), 'training/total_loss': Array(149.0463, dtype=float32), 'training/v_loss': Array(148.91023, dtype=float32), 'eval/episode_goal_distance': (Array(0.5264582, dtype=float32), Array(0.26865104, dtype=float32)), 'eval/episode_reward': (Array(-12289.295, dtype=float32), Array(6700.473, dtype=float32)), 'eval/avg_episode_length': (Array(906.7969, dtype=float32), Array(289.78082, dtype=float32)), 'eval/epoch_eval_time': 4.099630117416382, 'eval/sps': 31222.32892577796}
I0727 18:31:58.721424 140376670381888 train.py:379] starting iteration 42 682.7900578975677
I0727 18:32:14.354634 140376670381888 train.py:394] {'eval/walltime': 190.03889179229736, 'training/sps': 35581.06555954448, 'training/walltime': 500.04757809638977, 'training/entropy_loss': Array(0.12411533, dtype=float32), 'training/policy_loss': Array(0.00483269, dtype=float32), 'training/total_loss': Array(156.45331, dtype=float32), 'training/v_loss': Array(156.32436, dtype=float32), 'eval/episode_goal_distance': (Array(0.57400656, dtype=float32), Array(0.3211126, dtype=float32)), 'eval/episode_reward': (Array(-12007.706, dtype=float32), Array(7837.108, dtype=float32)), 'eval/avg_episode_length': (Array(829.1406, dtype=float32), Array(375.04327, dtype=float32)), 'eval/epoch_eval_time': 4.118498802185059, 'eval/sps': 31079.285474622437}
I0727 18:32:14.356631 140376670381888 train.py:379] starting iteration 43 698.4252653121948
I0727 18:32:29.935428 140376670381888 train.py:394] {'eval/walltime': 194.10837626457214, 'training/sps': 35598.24251920445, 'training/walltime': 511.55376410484314, 'training/entropy_loss': Array(0.11468033, dtype=float32), 'training/policy_loss': Array(0.00482258, dtype=float32), 'training/total_loss': Array(161.80666, dtype=float32), 'training/v_loss': Array(161.68713, dtype=float32), 'eval/episode_goal_distance': (Array(0.56490004, dtype=float32), Array(0.283581, dtype=float32)), 'eval/episode_reward': (Array(-12509.473, dtype=float32), Array(6999.705, dtype=float32)), 'eval/avg_episode_length': (Array(891.21094, dtype=float32), Array(310.43735, dtype=float32)), 'eval/epoch_eval_time': 4.06948447227478, 'eval/sps': 31453.615530924963}
I0727 18:32:29.937350 140376670381888 train.py:379] starting iteration 44 714.0059840679169
I0727 18:32:45.581903 140376670381888 train.py:394] {'eval/walltime': 198.20188069343567, 'training/sps': 35469.0799533448, 'training/walltime': 523.1018505096436, 'training/entropy_loss': Array(0.10606293, dtype=float32), 'training/policy_loss': Array(0.00474556, dtype=float32), 'training/total_loss': Array(163.09326, dtype=float32), 'training/v_loss': Array(162.98244, dtype=float32), 'eval/episode_goal_distance': (Array(0.52293265, dtype=float32), Array(0.25497156, dtype=float32)), 'eval/episode_reward': (Array(-11267.77, dtype=float32), Array(6406.1387, dtype=float32)), 'eval/avg_episode_length': (Array(868., dtype=float32), Array(337.29657, dtype=float32)), 'eval/epoch_eval_time': 4.093504428863525, 'eval/sps': 31269.051304174718}
I0727 18:32:45.583906 140376670381888 train.py:379] starting iteration 45 729.6525406837463
I0727 18:33:01.234463 140376670381888 train.py:394] {'eval/walltime': 202.32120418548584, 'training/sps': 35530.398191253196, 'training/walltime': 534.6300072669983, 'training/entropy_loss': Array(0.09287865, dtype=float32), 'training/policy_loss': Array(0.0046267, dtype=float32), 'training/total_loss': Array(163.83542, dtype=float32), 'training/v_loss': Array(163.73792, dtype=float32), 'eval/episode_goal_distance': (Array(0.5891712, dtype=float32), Array(0.32384092, dtype=float32)), 'eval/episode_reward': (Array(-12650.378, dtype=float32), Array(7507.1562, dtype=float32)), 'eval/avg_episode_length': (Array(875.8281, dtype=float32), Array(328.5285, dtype=float32)), 'eval/epoch_eval_time': 4.119323492050171, 'eval/sps': 31073.063391847118}
I0727 18:33:01.236560 140376670381888 train.py:379] starting iteration 46 745.305193901062
I0727 18:33:16.880170 140376670381888 train.py:394] {'eval/walltime': 206.41025686264038, 'training/sps': 35458.308387631776, 'training/walltime': 546.1816017627716, 'training/entropy_loss': Array(0.0872919, dtype=float32), 'training/policy_loss': Array(0.0047051, dtype=float32), 'training/total_loss': Array(137.39458, dtype=float32), 'training/v_loss': Array(137.30258, dtype=float32), 'eval/episode_goal_distance': (Array(0.58682513, dtype=float32), Array(0.2771683, dtype=float32)), 'eval/episode_reward': (Array(-12781.835, dtype=float32), Array(6567.9263, dtype=float32)), 'eval/avg_episode_length': (Array(906.85156, dtype=float32), Array(289.61102, dtype=float32)), 'eval/epoch_eval_time': 4.089052677154541, 'eval/sps': 31303.093920783547}
I0727 18:33:16.882070 140376670381888 train.py:379] starting iteration 47 760.9507038593292
I0727 18:33:32.520070 140376670381888 train.py:394] {'eval/walltime': 210.51700377464294, 'training/sps': 35530.20934380891, 'training/walltime': 557.7098197937012, 'training/entropy_loss': Array(0.07914697, dtype=float32), 'training/policy_loss': Array(0.00491247, dtype=float32), 'training/total_loss': Array(147.8901, dtype=float32), 'training/v_loss': Array(147.80606, dtype=float32), 'eval/episode_goal_distance': (Array(0.5557261, dtype=float32), Array(0.3150761, dtype=float32)), 'eval/episode_reward': (Array(-12634.38, dtype=float32), Array(6313.6846, dtype=float32)), 'eval/avg_episode_length': (Array(953.375, dtype=float32), Array(210.24405, dtype=float32)), 'eval/epoch_eval_time': 4.1067469120025635, 'eval/sps': 31168.222133655578}
I0727 18:33:32.522162 140376670381888 train.py:379] starting iteration 48 776.5907959938049
I0727 18:33:48.165245 140376670381888 train.py:394] {'eval/walltime': 214.64150190353394, 'training/sps': 35569.90478249086, 'training/walltime': 569.2251725196838, 'training/entropy_loss': Array(0.07235985, dtype=float32), 'training/policy_loss': Array(0.00499869, dtype=float32), 'training/total_loss': Array(151.6201, dtype=float32), 'training/v_loss': Array(151.54276, dtype=float32), 'eval/episode_goal_distance': (Array(0.55437326, dtype=float32), Array(0.32445878, dtype=float32)), 'eval/episode_reward': (Array(-12210.527, dtype=float32), Array(6652.11, dtype=float32)), 'eval/avg_episode_length': (Array(914.6406, dtype=float32), Array(278.38702, dtype=float32)), 'eval/epoch_eval_time': 4.124498128890991, 'eval/sps': 31034.07881395186}
I0727 18:33:48.167268 140376670381888 train.py:379] starting iteration 49 792.2359020709991
I0727 18:34:03.776128 140376670381888 train.py:394] {'eval/walltime': 218.73135542869568, 'training/sps': 35567.710285336776, 'training/walltime': 580.7412357330322, 'training/entropy_loss': Array(0.06661726, dtype=float32), 'training/policy_loss': Array(0.00497872, dtype=float32), 'training/total_loss': Array(152.09372, dtype=float32), 'training/v_loss': Array(152.02211, dtype=float32), 'eval/episode_goal_distance': (Array(0.58985627, dtype=float32), Array(0.2893365, dtype=float32)), 'eval/episode_reward': (Array(-13243.046, dtype=float32), Array(6367.7554, dtype=float32)), 'eval/avg_episode_length': (Array(930.14844, dtype=float32), Array(253.99736, dtype=float32)), 'eval/epoch_eval_time': 4.089853525161743, 'eval/sps': 31296.964356428372}
I0727 18:34:03.778082 140376670381888 train.py:379] starting iteration 50 807.8467161655426
I0727 18:34:19.443561 140376670381888 train.py:394] {'eval/walltime': 222.85454201698303, 'training/sps': 35496.590512056886, 'training/walltime': 592.2803721427917, 'training/entropy_loss': Array(0.04573719, dtype=float32), 'training/policy_loss': Array(0.00434992, dtype=float32), 'training/total_loss': Array(159.89844, dtype=float32), 'training/v_loss': Array(159.84834, dtype=float32), 'eval/episode_goal_distance': (Array(0.52124023, dtype=float32), Array(0.28882697, dtype=float32)), 'eval/episode_reward': (Array(-10882.624, dtype=float32), Array(6558.45, dtype=float32)), 'eval/avg_episode_length': (Array(868.09375, dtype=float32), Array(337.0577, dtype=float32)), 'eval/epoch_eval_time': 4.1231865882873535, 'eval/sps': 31043.95041534303}
I0727 18:34:19.490891 140376670381888 train.py:379] starting iteration 51 823.5595202445984
I0727 18:34:35.178651 140376670381888 train.py:394] {'eval/walltime': 226.9659869670868, 'training/sps': 35391.768714225174, 'training/walltime': 603.8536846637726, 'training/entropy_loss': Array(0.04509961, dtype=float32), 'training/policy_loss': Array(0.00412321, dtype=float32), 'training/total_loss': Array(132.78639, dtype=float32), 'training/v_loss': Array(132.73718, dtype=float32), 'eval/episode_goal_distance': (Array(0.51229537, dtype=float32), Array(0.24814038, dtype=float32)), 'eval/episode_reward': (Array(-11053.951, dtype=float32), Array(6993.1377, dtype=float32)), 'eval/avg_episode_length': (Array(836.9219, dtype=float32), Array(368.11084, dtype=float32)), 'eval/epoch_eval_time': 4.11144495010376, 'eval/sps': 31132.60704044443}
I0727 18:34:35.180783 140376670381888 train.py:379] starting iteration 52 839.2494165897369
I0727 18:34:50.796698 140376670381888 train.py:394] {'eval/walltime': 231.0811631679535, 'training/sps': 35624.69726863905, 'training/walltime': 615.3513262271881, 'training/entropy_loss': Array(0.04309316, dtype=float32), 'training/policy_loss': Array(0.00455534, dtype=float32), 'training/total_loss': Array(139.56885, dtype=float32), 'training/v_loss': Array(139.5212, dtype=float32), 'eval/episode_goal_distance': (Array(0.50402814, dtype=float32), Array(0.25211304, dtype=float32)), 'eval/episode_reward': (Array(-10966.23, dtype=float32), Array(6243.656, dtype=float32)), 'eval/avg_episode_length': (Array(899.09375, dtype=float32), Array(300.1211, dtype=float32)), 'eval/epoch_eval_time': 4.115176200866699, 'eval/sps': 31104.378950539678}
I0727 18:34:50.798625 140376670381888 train.py:379] starting iteration 53 854.8672590255737
I0727 18:35:06.359985 140376670381888 train.py:394] {'eval/walltime': 235.1596553325653, 'training/sps': 35679.5995414202, 'training/walltime': 626.8312757015228, 'training/entropy_loss': Array(0.03558474, dtype=float32), 'training/policy_loss': Array(0.00462519, dtype=float32), 'training/total_loss': Array(141.43555, dtype=float32), 'training/v_loss': Array(141.39534, dtype=float32), 'eval/episode_goal_distance': (Array(0.55389965, dtype=float32), Array(0.29535976, dtype=float32)), 'eval/episode_reward': (Array(-12318.543, dtype=float32), Array(6922.3545, dtype=float32)), 'eval/avg_episode_length': (Array(883.52344, dtype=float32), Array(319.69275, dtype=float32)), 'eval/epoch_eval_time': 4.078492164611816, 'eval/sps': 31384.147580477897}
I0727 18:35:06.362023 140376670381888 train.py:379] starting iteration 54 870.4306571483612
