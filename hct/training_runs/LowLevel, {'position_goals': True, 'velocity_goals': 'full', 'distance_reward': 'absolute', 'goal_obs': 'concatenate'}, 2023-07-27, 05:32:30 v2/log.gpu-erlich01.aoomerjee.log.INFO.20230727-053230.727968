I0727 05:32:30.624631 140120985872192 low_level_env.py:187] Initialising environment...
I0727 05:33:02.627284 140120985872192 low_level_env.py:289] Environment initialised.
I0727 05:33:02.641192 140120985872192 train.py:118] JAX is running on GPU.
I0727 05:33:02.641257 140120985872192 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0727 05:33:06.676152 140120985872192 train.py:367] Running initial eval
I0727 05:33:22.913110 140120985872192 train.py:373] {'eval/walltime': 16.234812021255493, 'eval/episode_goal_distance': (Array(0.30125147, dtype=float32), Array(0.04198639, dtype=float32)), 'eval/episode_reward': (Array(-47445.336, dtype=float32), Array(17638.959, dtype=float32)), 'eval/avg_episode_length': (Array(898.9922, dtype=float32), Array(300.42285, dtype=float32)), 'eval/epoch_eval_time': 16.234812021255493, 'eval/sps': 7884.2920898877965}
I0727 05:33:22.914540 140120985872192 train.py:379] starting iteration 0 20.27336883544922
I0727 05:33:52.730033 140120985872192 train.py:394] {'eval/walltime': 20.11527442932129, 'training/sps': 9477.760401404046, 'training/walltime': 25.930176496505737, 'training/entropy_loss': Array(-0.04060638, dtype=float32), 'training/policy_loss': Array(0.02772022, dtype=float32), 'training/total_loss': Array(5554.8457, dtype=float32), 'training/v_loss': Array(5554.8584, dtype=float32), 'eval/episode_goal_distance': (Array(0.30096036, dtype=float32), Array(0.03863143, dtype=float32)), 'eval/episode_reward': (Array(-48160.266, dtype=float32), Array(15561.221, dtype=float32)), 'eval/avg_episode_length': (Array(922.375, dtype=float32), Array(266.65106, dtype=float32)), 'eval/epoch_eval_time': 3.880462408065796, 'eval/sps': 32985.759566680405}
I0727 05:33:52.755859 140120985872192 train.py:379] starting iteration 1 50.11468553543091
I0727 05:34:02.165441 140120985872192 train.py:394] {'eval/walltime': 23.990848302841187, 'training/sps': 44442.41976935295, 'training/walltime': 31.460028409957886, 'training/entropy_loss': Array(-0.04138349, dtype=float32), 'training/policy_loss': Array(0.00441925, dtype=float32), 'training/total_loss': Array(5009.4614, dtype=float32), 'training/v_loss': Array(5009.498, dtype=float32), 'eval/episode_goal_distance': (Array(0.3032921, dtype=float32), Array(0.0392188, dtype=float32)), 'eval/episode_reward': (Array(-46036.023, dtype=float32), Array(18622.838, dtype=float32)), 'eval/avg_episode_length': (Array(875.75, dtype=float32), Array(328.73523, dtype=float32)), 'eval/epoch_eval_time': 3.8755738735198975, 'eval/sps': 33027.3668306436}
I0727 05:34:02.168234 140120985872192 train.py:379] starting iteration 2 59.5270619392395
I0727 05:34:11.674615 140120985872192 train.py:394] {'eval/walltime': 27.947733640670776, 'training/sps': 44317.07825613807, 'training/walltime': 37.00552034378052, 'training/entropy_loss': Array(-0.0423636, dtype=float32), 'training/policy_loss': Array(0.00252031, dtype=float32), 'training/total_loss': Array(4415.2285, dtype=float32), 'training/v_loss': Array(4415.2686, dtype=float32), 'eval/episode_goal_distance': (Array(0.30650368, dtype=float32), Array(0.04190129, dtype=float32)), 'eval/episode_reward': (Array(-47456.79, dtype=float32), Array(18453.56, dtype=float32)), 'eval/avg_episode_length': (Array(891.28906, dtype=float32), Array(310.21478, dtype=float32)), 'eval/epoch_eval_time': 3.95688533782959, 'eval/sps': 32348.67555454864}
I0727 05:34:11.677068 140120985872192 train.py:379] starting iteration 3 69.03589630126953
I0727 05:34:21.268546 140120985872192 train.py:394] {'eval/walltime': 31.97677493095398, 'training/sps': 44215.21370270276, 'training/walltime': 42.563788175582886, 'training/entropy_loss': Array(-0.04323502, dtype=float32), 'training/policy_loss': Array(0.00277456, dtype=float32), 'training/total_loss': Array(3855.8906, dtype=float32), 'training/v_loss': Array(3855.9312, dtype=float32), 'eval/episode_goal_distance': (Array(0.30295768, dtype=float32), Array(0.04316343, dtype=float32)), 'eval/episode_reward': (Array(-46643.93, dtype=float32), Array(18236.877, dtype=float32)), 'eval/avg_episode_length': (Array(883.5, dtype=float32), Array(319.75708, dtype=float32)), 'eval/epoch_eval_time': 4.029041290283203, 'eval/sps': 31769.34431242893}
I0727 05:34:21.271181 140120985872192 train.py:379] starting iteration 4 78.63000917434692
I0727 05:34:30.886703 140120985872192 train.py:394] {'eval/walltime': 36.02086114883423, 'training/sps': 44142.66527460048, 'training/walltime': 48.13119101524353, 'training/entropy_loss': Array(-0.04409107, dtype=float32), 'training/policy_loss': Array(0.00283002, dtype=float32), 'training/total_loss': Array(2869.8325, dtype=float32), 'training/v_loss': Array(2869.8738, dtype=float32), 'eval/episode_goal_distance': (Array(0.30291107, dtype=float32), Array(0.03918587, dtype=float32)), 'eval/episode_reward': (Array(-44766.445, dtype=float32), Array(18724.146, dtype=float32)), 'eval/avg_episode_length': (Array(868.0156, dtype=float32), Array(337.25635, dtype=float32)), 'eval/epoch_eval_time': 4.044086217880249, 'eval/sps': 31651.15507035173}
I0727 05:34:30.889220 140120985872192 train.py:379] starting iteration 5 88.24804735183716
I0727 05:34:40.506662 140120985872192 train.py:394] {'eval/walltime': 40.0688681602478, 'training/sps': 44160.0559330695, 'training/walltime': 53.69640135765076, 'training/entropy_loss': Array(-0.04468179, dtype=float32), 'training/policy_loss': Array(0.00253904, dtype=float32), 'training/total_loss': Array(2255.2625, dtype=float32), 'training/v_loss': Array(2255.3044, dtype=float32), 'eval/episode_goal_distance': (Array(0.3093005, dtype=float32), Array(0.04202451, dtype=float32)), 'eval/episode_reward': (Array(-46331.08, dtype=float32), Array(19457.178, dtype=float32)), 'eval/avg_episode_length': (Array(868.10156, dtype=float32), Array(337.03693, dtype=float32)), 'eval/epoch_eval_time': 4.048007011413574, 'eval/sps': 31620.498590811996}
I0727 05:34:40.509196 140120985872192 train.py:379] starting iteration 6 97.86802363395691
I0727 05:34:50.151245 140120985872192 train.py:394] {'eval/walltime': 44.13425636291504, 'training/sps': 44102.208846941365, 'training/walltime': 59.268911361694336, 'training/entropy_loss': Array(-0.0453995, dtype=float32), 'training/policy_loss': Array(0.00344143, dtype=float32), 'training/total_loss': Array(1891.5557, dtype=float32), 'training/v_loss': Array(1891.5978, dtype=float32), 'eval/episode_goal_distance': (Array(0.30127913, dtype=float32), Array(0.04446567, dtype=float32)), 'eval/episode_reward': (Array(-47673.145, dtype=float32), Array(15895.631, dtype=float32)), 'eval/avg_episode_length': (Array(922.41406, dtype=float32), Array(266.51718, dtype=float32)), 'eval/epoch_eval_time': 4.065388202667236, 'eval/sps': 31485.30807365979}
I0727 05:34:50.153839 140120985872192 train.py:379] starting iteration 7 107.51266670227051
I0727 05:34:59.827091 140120985872192 train.py:394] {'eval/walltime': 48.23867845535278, 'training/sps': 44163.91944144984, 'training/walltime': 64.83363485336304, 'training/entropy_loss': Array(-0.04565428, dtype=float32), 'training/policy_loss': Array(0.00425927, dtype=float32), 'training/total_loss': Array(1889.6736, dtype=float32), 'training/v_loss': Array(1889.715, dtype=float32), 'eval/episode_goal_distance': (Array(0.30735964, dtype=float32), Array(0.03845354, dtype=float32)), 'eval/episode_reward': (Array(-47802.32, dtype=float32), Array(17362.89, dtype=float32)), 'eval/avg_episode_length': (Array(899.0547, dtype=float32), Array(300.23706, dtype=float32)), 'eval/epoch_eval_time': 4.104422092437744, 'eval/sps': 31185.876383385516}
I0727 05:34:59.829545 140120985872192 train.py:379] starting iteration 8 117.18837237358093
I0727 05:35:09.537961 140120985872192 train.py:394] {'eval/walltime': 52.378246784210205, 'training/sps': 44163.85132281339, 'training/walltime': 70.39836692810059, 'training/entropy_loss': Array(-0.04443361, dtype=float32), 'training/policy_loss': Array(0.00948652, dtype=float32), 'training/total_loss': Array(2282.1714, dtype=float32), 'training/v_loss': Array(2282.206, dtype=float32), 'eval/episode_goal_distance': (Array(0.29718757, dtype=float32), Array(0.04184429, dtype=float32)), 'eval/episode_reward': (Array(-46910.492, dtype=float32), Array(16656.29, dtype=float32)), 'eval/avg_episode_length': (Array(906.9531, dtype=float32), Array(289.295, dtype=float32)), 'eval/epoch_eval_time': 4.139568328857422, 'eval/sps': 30921.09848935137}
I0727 05:35:09.540637 140120985872192 train.py:379] starting iteration 9 126.89946436882019
I0727 05:35:19.249949 140120985872192 train.py:394] {'eval/walltime': 56.514127254486084, 'training/sps': 44127.61930708493, 'training/walltime': 75.96766805648804, 'training/entropy_loss': Array(-0.04309211, dtype=float32), 'training/policy_loss': Array(0.01266449, dtype=float32), 'training/total_loss': Array(495.94543, dtype=float32), 'training/v_loss': Array(495.9759, dtype=float32), 'eval/episode_goal_distance': (Array(0.30070287, dtype=float32), Array(0.04429995, dtype=float32)), 'eval/episode_reward': (Array(-46217.62, dtype=float32), Array(18685.318, dtype=float32)), 'eval/avg_episode_length': (Array(875.78906, dtype=float32), Array(328.632, dtype=float32)), 'eval/epoch_eval_time': 4.135880470275879, 'eval/sps': 30948.670040133416}
I0727 05:35:19.252473 140120985872192 train.py:379] starting iteration 10 136.61130094528198
I0727 05:35:28.933681 140120985872192 train.py:394] {'eval/walltime': 60.62541055679321, 'training/sps': 44153.90069100438, 'training/walltime': 81.53365421295166, 'training/entropy_loss': Array(-0.04274461, dtype=float32), 'training/policy_loss': Array(0.01168132, dtype=float32), 'training/total_loss': Array(330.84897, dtype=float32), 'training/v_loss': Array(330.88, dtype=float32), 'eval/episode_goal_distance': (Array(0.30026188, dtype=float32), Array(0.03923557, dtype=float32)), 'eval/episode_reward': (Array(-46636.47, dtype=float32), Array(17070.617, dtype=float32)), 'eval/avg_episode_length': (Array(899.1328, dtype=float32), Array(300.0048, dtype=float32)), 'eval/epoch_eval_time': 4.111283302307129, 'eval/sps': 31133.83111501225}
I0727 05:35:28.936161 140120985872192 train.py:379] starting iteration 11 146.2949891090393
I0727 05:35:38.644114 140120985872192 train.py:394] {'eval/walltime': 64.73861503601074, 'training/sps': 43956.980127353396, 'training/walltime': 87.12457513809204, 'training/entropy_loss': Array(-0.04025201, dtype=float32), 'training/policy_loss': Array(0.01345838, dtype=float32), 'training/total_loss': Array(417.5141, dtype=float32), 'training/v_loss': Array(417.5409, dtype=float32), 'eval/episode_goal_distance': (Array(0.3025489, dtype=float32), Array(0.03646667, dtype=float32)), 'eval/episode_reward': (Array(-47897.367, dtype=float32), Array(16028.764, dtype=float32)), 'eval/avg_episode_length': (Array(914.53906, dtype=float32), Array(278.718, dtype=float32)), 'eval/epoch_eval_time': 4.113204479217529, 'eval/sps': 31119.289266248667}
I0727 05:35:38.646601 140120985872192 train.py:379] starting iteration 12 156.00542902946472
I0727 05:35:48.376585 140120985872192 train.py:394] {'eval/walltime': 68.84537672996521, 'training/sps': 43735.94687900401, 'training/walltime': 92.7437515258789, 'training/entropy_loss': Array(-0.03701984, dtype=float32), 'training/policy_loss': Array(0.01679419, dtype=float32), 'training/total_loss': Array(605.4997, dtype=float32), 'training/v_loss': Array(605.5199, dtype=float32), 'eval/episode_goal_distance': (Array(0.29740268, dtype=float32), Array(0.04484034, dtype=float32)), 'eval/episode_reward': (Array(-42380.812, dtype=float32), Array(20404.516, dtype=float32)), 'eval/avg_episode_length': (Array(829.1172, dtype=float32), Array(375.09427, dtype=float32)), 'eval/epoch_eval_time': 4.106761693954468, 'eval/sps': 31168.109946196248}
I0727 05:35:48.379074 140120985872192 train.py:379] starting iteration 13 165.73790192604065
I0727 05:35:58.106027 140120985872192 train.py:394] {'eval/walltime': 72.9491662979126, 'training/sps': 43736.884023691586, 'training/walltime': 98.36280751228333, 'training/entropy_loss': Array(-0.03367165, dtype=float32), 'training/policy_loss': Array(0.0220477, dtype=float32), 'training/total_loss': Array(680.5647, dtype=float32), 'training/v_loss': Array(680.5764, dtype=float32), 'eval/episode_goal_distance': (Array(0.2914974, dtype=float32), Array(0.03942143, dtype=float32)), 'eval/episode_reward': (Array(-43331.156, dtype=float32), Array(18727.27, dtype=float32)), 'eval/avg_episode_length': (Array(860.2031, dtype=float32), Array(345.58777, dtype=float32)), 'eval/epoch_eval_time': 4.103789567947388, 'eval/sps': 31190.6831187795}
I0727 05:35:58.108531 140120985872192 train.py:379] starting iteration 14 175.4673593044281
I0727 05:36:07.892657 140120985872192 train.py:394] {'eval/walltime': 77.06506562232971, 'training/sps': 43386.57395192194, 'training/walltime': 104.02723264694214, 'training/entropy_loss': Array(-0.02983389, dtype=float32), 'training/policy_loss': Array(0.023745, dtype=float32), 'training/total_loss': Array(700.1, dtype=float32), 'training/v_loss': Array(700.1061, dtype=float32), 'eval/episode_goal_distance': (Array(0.2917766, dtype=float32), Array(0.04274632, dtype=float32)), 'eval/episode_reward': (Array(-44972.82, dtype=float32), Array(18404.254, dtype=float32)), 'eval/avg_episode_length': (Array(875.8281, dtype=float32), Array(328.52863, dtype=float32)), 'eval/epoch_eval_time': 4.115899324417114, 'eval/sps': 31098.9142131476}
I0727 05:36:07.895148 140120985872192 train.py:379] starting iteration 15 185.25397539138794
I0727 05:36:17.680648 140120985872192 train.py:394] {'eval/walltime': 81.17200469970703, 'training/sps': 43307.62309628083, 'training/walltime': 109.701984167099, 'training/entropy_loss': Array(-0.02520379, dtype=float32), 'training/policy_loss': Array(0.02610541, dtype=float32), 'training/total_loss': Array(629.8036, dtype=float32), 'training/v_loss': Array(629.8026, dtype=float32), 'eval/episode_goal_distance': (Array(0.29377824, dtype=float32), Array(0.04573772, dtype=float32)), 'eval/episode_reward': (Array(-44689.414, dtype=float32), Array(18048.596, dtype=float32)), 'eval/avg_episode_length': (Array(883.4922, dtype=float32), Array(319.77853, dtype=float32)), 'eval/epoch_eval_time': 4.106939077377319, 'eval/sps': 31166.763759675847}
I0727 05:36:17.683129 140120985872192 train.py:379] starting iteration 16 195.0419569015503
I0727 05:36:27.484981 140120985872192 train.py:394] {'eval/walltime': 85.27616763114929, 'training/sps': 43161.766282699326, 'training/walltime': 115.39591240882874, 'training/entropy_loss': Array(-0.01817419, dtype=float32), 'training/policy_loss': Array(0.02856083, dtype=float32), 'training/total_loss': Array(1605.3667, dtype=float32), 'training/v_loss': Array(1605.3564, dtype=float32), 'eval/episode_goal_distance': (Array(0.29103804, dtype=float32), Array(0.03693379, dtype=float32)), 'eval/episode_reward': (Array(-47150.07, dtype=float32), Array(15383.791, dtype=float32)), 'eval/avg_episode_length': (Array(922.35156, dtype=float32), Array(266.73138, dtype=float32)), 'eval/epoch_eval_time': 4.104162931442261, 'eval/sps': 31187.8456431112}
I0727 05:36:27.487547 140120985872192 train.py:379] starting iteration 17 204.8463749885559
I0727 05:36:37.275533 140120985872192 train.py:394] {'eval/walltime': 89.36956763267517, 'training/sps': 43185.55244746807, 'training/walltime': 121.08670449256897, 'training/entropy_loss': Array(-0.00977814, dtype=float32), 'training/policy_loss': Array(0.02607054, dtype=float32), 'training/total_loss': Array(994.2778, dtype=float32), 'training/v_loss': Array(994.2614, dtype=float32), 'eval/episode_goal_distance': (Array(0.2896046, dtype=float32), Array(0.03521475, dtype=float32)), 'eval/episode_reward': (Array(-44183.36, dtype=float32), Array(18923.256, dtype=float32)), 'eval/avg_episode_length': (Array(860.28125, dtype=float32), Array(345.395, dtype=float32)), 'eval/epoch_eval_time': 4.093400001525879, 'eval/sps': 31269.84901360385}
I0727 05:36:37.278080 140120985872192 train.py:379] starting iteration 18 214.63690781593323
I0727 05:36:47.101458 140120985872192 train.py:394] {'eval/walltime': 93.46575927734375, 'training/sps': 42939.26018940407, 'training/walltime': 126.81013798713684, 'training/entropy_loss': Array(-0.00218111, dtype=float32), 'training/policy_loss': Array(0.02794317, dtype=float32), 'training/total_loss': Array(692.7585, dtype=float32), 'training/v_loss': Array(692.73267, dtype=float32), 'eval/episode_goal_distance': (Array(0.28886038, dtype=float32), Array(0.03528178, dtype=float32)), 'eval/episode_reward': (Array(-43754.375, dtype=float32), Array(19533.436, dtype=float32)), 'eval/avg_episode_length': (Array(852.46875, dtype=float32), Array(353.3632, dtype=float32)), 'eval/epoch_eval_time': 4.096191644668579, 'eval/sps': 31248.537935621032}
I0727 05:36:47.103882 140120985872192 train.py:379] starting iteration 19 224.46270942687988
I0727 05:36:56.970124 140120985872192 train.py:394] {'eval/walltime': 97.57123637199402, 'training/sps': 42687.401564785956, 'training/walltime': 132.56734013557434, 'training/entropy_loss': Array(0.00521888, dtype=float32), 'training/policy_loss': Array(0.03484655, dtype=float32), 'training/total_loss': Array(641.2738, dtype=float32), 'training/v_loss': Array(641.2337, dtype=float32), 'eval/episode_goal_distance': (Array(0.28391385, dtype=float32), Array(0.04218959, dtype=float32)), 'eval/episode_reward': (Array(-44603.062, dtype=float32), Array(17409.674, dtype=float32)), 'eval/avg_episode_length': (Array(899.0547, dtype=float32), Array(300.2368, dtype=float32)), 'eval/epoch_eval_time': 4.1054770946502686, 'eval/sps': 31177.862413796727}
I0727 05:36:56.972627 140120985872192 train.py:379] starting iteration 20 234.3314552307129
I0727 05:37:06.795294 140120985872192 train.py:394] {'eval/walltime': 101.66974353790283, 'training/sps': 42961.92247814658, 'training/walltime': 138.28775453567505, 'training/entropy_loss': Array(0.01440354, dtype=float32), 'training/policy_loss': Array(0.0411697, dtype=float32), 'training/total_loss': Array(518.6841, dtype=float32), 'training/v_loss': Array(518.6285, dtype=float32), 'eval/episode_goal_distance': (Array(0.28035778, dtype=float32), Array(0.03541245, dtype=float32)), 'eval/episode_reward': (Array(-42678.273, dtype=float32), Array(17059.89, dtype=float32)), 'eval/avg_episode_length': (Array(883.46094, dtype=float32), Array(319.86426, dtype=float32)), 'eval/epoch_eval_time': 4.0985071659088135, 'eval/sps': 31230.8835433296}
I0727 05:37:06.797715 140120985872192 train.py:379] starting iteration 21 244.15654277801514
I0727 05:37:16.660463 140120985872192 train.py:394] {'eval/walltime': 105.77743458747864, 'training/sps': 42731.245078905, 'training/walltime': 144.03904962539673, 'training/entropy_loss': Array(0.02387849, dtype=float32), 'training/policy_loss': Array(0.05015057, dtype=float32), 'training/total_loss': Array(457.50842, dtype=float32), 'training/v_loss': Array(457.43442, dtype=float32), 'eval/episode_goal_distance': (Array(0.2920807, dtype=float32), Array(0.03823697, dtype=float32)), 'eval/episode_reward': (Array(-44413.098, dtype=float32), Array(18128.082, dtype=float32)), 'eval/avg_episode_length': (Array(875.6797, dtype=float32), Array(328.92108, dtype=float32)), 'eval/epoch_eval_time': 4.107691049575806, 'eval/sps': 31161.058233242333}
I0727 05:37:16.663039 140120985872192 train.py:379] starting iteration 22 254.02186703681946
I0727 05:37:26.581006 140120985872192 train.py:394] {'eval/walltime': 109.91606211662292, 'training/sps': 42551.240870175374, 'training/walltime': 149.8146743774414, 'training/entropy_loss': Array(-0.0087186, dtype=float32), 'training/policy_loss': Array(0.14398794, dtype=float32), 'training/total_loss': Array(544.3223, dtype=float32), 'training/v_loss': Array(544.187, dtype=float32), 'eval/episode_goal_distance': (Array(0.29800075, dtype=float32), Array(0.04379354, dtype=float32)), 'eval/episode_reward': (Array(-46514.8, dtype=float32), Array(16995.02, dtype=float32)), 'eval/avg_episode_length': (Array(899.0625, dtype=float32), Array(300.21387, dtype=float32)), 'eval/epoch_eval_time': 4.138627529144287, 'eval/sps': 30928.127525036205}
I0727 05:37:26.583631 140120985872192 train.py:379] starting iteration 23 263.9424583911896
I0727 05:37:36.465961 140120985872192 train.py:394] {'eval/walltime': 114.02456331253052, 'training/sps': 42591.286802562885, 'training/walltime': 155.5848686695099, 'training/entropy_loss': Array(0.00078312, dtype=float32), 'training/policy_loss': Array(0.03967583, dtype=float32), 'training/total_loss': Array(410.2388, dtype=float32), 'training/v_loss': Array(410.19833, dtype=float32), 'eval/episode_goal_distance': (Array(0.30365103, dtype=float32), Array(0.04739134, dtype=float32)), 'eval/episode_reward': (Array(-41124.785, dtype=float32), Array(22307.074, dtype=float32)), 'eval/avg_episode_length': (Array(790.2969, dtype=float32), Array(405.5876, dtype=float32)), 'eval/epoch_eval_time': 4.108501195907593, 'eval/sps': 31154.91365257447}
I0727 05:37:36.468464 140120985872192 train.py:379] starting iteration 24 273.8272924423218
I0727 05:37:46.329036 140120985872192 train.py:394] {'eval/walltime': 118.13187384605408, 'training/sps': 42744.6571172739, 'training/walltime': 161.33435916900635, 'training/entropy_loss': Array(0.01037779, dtype=float32), 'training/policy_loss': Array(0.02875243, dtype=float32), 'training/total_loss': Array(414.76752, dtype=float32), 'training/v_loss': Array(414.7284, dtype=float32), 'eval/episode_goal_distance': (Array(0.29912704, dtype=float32), Array(0.04781602, dtype=float32)), 'eval/episode_reward': (Array(-45780.67, dtype=float32), Array(18139.922, dtype=float32)), 'eval/avg_episode_length': (Array(883.58594, dtype=float32), Array(319.52112, dtype=float32)), 'eval/epoch_eval_time': 4.10731053352356, 'eval/sps': 31163.94510599421}
I0727 05:37:46.331624 140120985872192 train.py:379] starting iteration 25 283.69045209884644
I0727 05:37:56.228800 140120985872192 train.py:394] {'eval/walltime': 122.25896286964417, 'training/sps': 42620.01953969946, 'training/walltime': 167.1006634235382, 'training/entropy_loss': Array(0.01722666, dtype=float32), 'training/policy_loss': Array(0.02804938, dtype=float32), 'training/total_loss': Array(2113.9663, dtype=float32), 'training/v_loss': Array(2113.921, dtype=float32), 'eval/episode_goal_distance': (Array(0.30178455, dtype=float32), Array(0.04474016, dtype=float32)), 'eval/episode_reward': (Array(-44045.395, dtype=float32), Array(20655.576, dtype=float32)), 'eval/avg_episode_length': (Array(836.96875, dtype=float32), Array(368.00558, dtype=float32)), 'eval/epoch_eval_time': 4.127089023590088, 'eval/sps': 31014.59630949634}
I0727 05:37:56.231271 140120985872192 train.py:379] starting iteration 26 293.5900990962982
I0727 05:38:06.078594 140120985872192 train.py:394] {'eval/walltime': 126.3439736366272, 'training/sps': 42676.29395545808, 'training/walltime': 172.85936403274536, 'training/entropy_loss': Array(0.02751521, dtype=float32), 'training/policy_loss': Array(0.02658083, dtype=float32), 'training/total_loss': Array(518.2235, dtype=float32), 'training/v_loss': Array(518.1694, dtype=float32), 'eval/episode_goal_distance': (Array(0.29850945, dtype=float32), Array(0.03965787, dtype=float32)), 'eval/episode_reward': (Array(-43857.207, dtype=float32), Array(20081.516, dtype=float32)), 'eval/avg_episode_length': (Array(844.64844, dtype=float32), Array(361.0051, dtype=float32)), 'eval/epoch_eval_time': 4.085010766983032, 'eval/sps': 31334.066738466365}
I0727 05:38:06.081067 140120985872192 train.py:379] starting iteration 27 303.4398944377899
I0727 05:38:15.950307 140120985872192 train.py:394] {'eval/walltime': 130.4408779144287, 'training/sps': 42602.728762752035, 'training/walltime': 178.62800860404968, 'training/entropy_loss': Array(0.04172531, dtype=float32), 'training/policy_loss': Array(0.03028954, dtype=float32), 'training/total_loss': Array(372.63425, dtype=float32), 'training/v_loss': Array(372.5622, dtype=float32), 'eval/episode_goal_distance': (Array(0.29705155, dtype=float32), Array(0.04640003, dtype=float32)), 'eval/episode_reward': (Array(-43574.664, dtype=float32), Array(20689.39, dtype=float32)), 'eval/avg_episode_length': (Array(836.875, dtype=float32), Array(368.21667, dtype=float32)), 'eval/epoch_eval_time': 4.096904277801514, 'eval/sps': 31243.102430669318}
I0727 05:38:15.953001 140120985872192 train.py:379] starting iteration 28 313.31182885169983
I0727 05:38:25.847544 140120985872192 train.py:394] {'eval/walltime': 134.54008674621582, 'training/sps': 42433.49600206982, 'training/walltime': 184.419659614563, 'training/entropy_loss': Array(0.05744395, dtype=float32), 'training/policy_loss': Array(0.03281236, dtype=float32), 'training/total_loss': Array(360.12787, dtype=float32), 'training/v_loss': Array(360.0376, dtype=float32), 'eval/episode_goal_distance': (Array(0.2953359, dtype=float32), Array(0.04563994, dtype=float32)), 'eval/episode_reward': (Array(-42622.758, dtype=float32), Array(21083.06, dtype=float32)), 'eval/avg_episode_length': (Array(821.39844, dtype=float32), Array(381.60757, dtype=float32)), 'eval/epoch_eval_time': 4.099208831787109, 'eval/sps': 31225.537720213328}
I0727 05:38:25.849995 140120985872192 train.py:379] starting iteration 29 323.2088234424591
I0727 05:38:35.785176 140120985872192 train.py:394] {'eval/walltime': 138.6523277759552, 'training/sps': 42231.06562801817, 'training/walltime': 190.23907232284546, 'training/entropy_loss': Array(0.07413419, dtype=float32), 'training/policy_loss': Array(0.03950507, dtype=float32), 'training/total_loss': Array(316.2212, dtype=float32), 'training/v_loss': Array(316.10754, dtype=float32), 'eval/episode_goal_distance': (Array(0.29623103, dtype=float32), Array(0.04352367, dtype=float32)), 'eval/episode_reward': (Array(-40643.37, dtype=float32), Array(21771.414, dtype=float32)), 'eval/avg_episode_length': (Array(798.03906, dtype=float32), Array(400.02002, dtype=float32)), 'eval/epoch_eval_time': 4.11224102973938, 'eval/sps': 31126.58014798131}
I0727 05:38:35.787635 140120985872192 train.py:379] starting iteration 30 333.146461725235
I0727 05:38:45.675662 140120985872192 train.py:394] {'eval/walltime': 142.7434959411621, 'training/sps': 42421.930010031865, 'training/walltime': 196.03230237960815, 'training/entropy_loss': Array(0.08965975, dtype=float32), 'training/policy_loss': Array(0.01673332, dtype=float32), 'training/total_loss': Array(335.83337, dtype=float32), 'training/v_loss': Array(335.727, dtype=float32), 'eval/episode_goal_distance': (Array(0.2904605, dtype=float32), Array(0.04313112, dtype=float32)), 'eval/episode_reward': (Array(-43631.28, dtype=float32), Array(19380.92, dtype=float32)), 'eval/avg_episode_length': (Array(852.41406, dtype=float32), Array(353.4941, dtype=float32)), 'eval/epoch_eval_time': 4.091168165206909, 'eval/sps': 31286.907511788006}
I0727 05:38:45.678044 140120985872192 train.py:379] starting iteration 31 343.03687143325806
I0727 05:38:55.580578 140120985872192 train.py:394] {'eval/walltime': 146.8551254272461, 'training/sps': 42466.522486479145, 'training/walltime': 201.81944918632507, 'training/entropy_loss': Array(0.09753077, dtype=float32), 'training/policy_loss': Array(0.02386064, dtype=float32), 'training/total_loss': Array(387.94812, dtype=float32), 'training/v_loss': Array(387.82675, dtype=float32), 'eval/episode_goal_distance': (Array(0.29429984, dtype=float32), Array(0.0440927, dtype=float32)), 'eval/episode_reward': (Array(-42927.297, dtype=float32), Array(20090.693, dtype=float32)), 'eval/avg_episode_length': (Array(836.7969, dtype=float32), Array(368.39304, dtype=float32)), 'eval/epoch_eval_time': 4.111629486083984, 'eval/sps': 31131.20976323922}
I0727 05:38:55.583102 140120985872192 train.py:379] starting iteration 32 352.94193029403687
I0727 05:39:05.458706 140120985872192 train.py:394] {'eval/walltime': 150.93309354782104, 'training/sps': 42415.60046142336, 'training/walltime': 207.6135437488556, 'training/entropy_loss': Array(0.10506003, dtype=float32), 'training/policy_loss': Array(0.0178173, dtype=float32), 'training/total_loss': Array(319.46326, dtype=float32), 'training/v_loss': Array(319.3404, dtype=float32), 'eval/episode_goal_distance': (Array(0.2993664, dtype=float32), Array(0.04267472, dtype=float32)), 'eval/episode_reward': (Array(-45524.18, dtype=float32), Array(18678.293, dtype=float32)), 'eval/avg_episode_length': (Array(875.7031, dtype=float32), Array(328.85892, dtype=float32)), 'eval/epoch_eval_time': 4.077968120574951, 'eval/sps': 31388.18063686906}
I0727 05:39:05.461097 140120985872192 train.py:379] starting iteration 33 362.8199243545532
I0727 05:39:15.346237 140120985872192 train.py:394] {'eval/walltime': 155.03339052200317, 'training/sps': 42509.92899946503, 'training/walltime': 213.39478135108948, 'training/entropy_loss': Array(0.10986512, dtype=float32), 'training/policy_loss': Array(0.02727892, dtype=float32), 'training/total_loss': Array(2163.2705, dtype=float32), 'training/v_loss': Array(2163.1333, dtype=float32), 'eval/episode_goal_distance': (Array(0.2950912, dtype=float32), Array(0.044163, dtype=float32)), 'eval/episode_reward': (Array(-39781.93, dtype=float32), Array(21832.242, dtype=float32)), 'eval/avg_episode_length': (Array(782.66406, dtype=float32), Array(410.7269, dtype=float32)), 'eval/epoch_eval_time': 4.100296974182129, 'eval/sps': 31217.251044488476}
I0727 05:39:15.348673 140120985872192 train.py:379] starting iteration 34 372.7075011730194
I0727 05:39:25.286669 140120985872192 train.py:394] {'eval/walltime': 159.150493144989, 'training/sps': 42246.36775152979, 'training/walltime': 219.2120862007141, 'training/entropy_loss': Array(0.07862335, dtype=float32), 'training/policy_loss': Array(0.0134505, dtype=float32), 'training/total_loss': Array(707.43726, dtype=float32), 'training/v_loss': Array(707.3452, dtype=float32), 'eval/episode_goal_distance': (Array(0.29985854, dtype=float32), Array(0.04450956, dtype=float32)), 'eval/episode_reward': (Array(-45271.094, dtype=float32), Array(18649.893, dtype=float32)), 'eval/avg_episode_length': (Array(875.8203, dtype=float32), Array(328.5494, dtype=float32)), 'eval/epoch_eval_time': 4.11710262298584, 'eval/sps': 31089.824986478176}
I0727 05:39:25.289098 140120985872192 train.py:379] starting iteration 35 382.6479260921478
I0727 05:39:35.174851 140120985872192 train.py:394] {'eval/walltime': 163.2682399749756, 'training/sps': 42635.54316039901, 'training/walltime': 224.9762909412384, 'training/entropy_loss': Array(0.06629007, dtype=float32), 'training/policy_loss': Array(0.01122435, dtype=float32), 'training/total_loss': Array(418.1145, dtype=float32), 'training/v_loss': Array(418.037, dtype=float32), 'eval/episode_goal_distance': (Array(0.30583978, dtype=float32), Array(0.04170472, dtype=float32)), 'eval/episode_reward': (Array(-46133.008, dtype=float32), Array(19630.555, dtype=float32)), 'eval/avg_episode_length': (Array(860.1172, dtype=float32), Array(345.79996, dtype=float32)), 'eval/epoch_eval_time': 4.117746829986572, 'eval/sps': 31084.96109276767}
I0727 05:39:35.177297 140120985872192 train.py:379] starting iteration 36 392.53612518310547
I0727 05:39:45.096962 140120985872192 train.py:394] {'eval/walltime': 167.3587577342987, 'training/sps': 42187.313056966406, 'training/walltime': 230.80173897743225, 'training/entropy_loss': Array(0.04825046, dtype=float32), 'training/policy_loss': Array(0.00897756, dtype=float32), 'training/total_loss': Array(317.7193, dtype=float32), 'training/v_loss': Array(317.66205, dtype=float32), 'eval/episode_goal_distance': (Array(0.30435023, dtype=float32), Array(0.04427942, dtype=float32)), 'eval/episode_reward': (Array(-45121.895, dtype=float32), Array(20005.744, dtype=float32)), 'eval/avg_episode_length': (Array(852.4297, dtype=float32), Array(353.45654, dtype=float32)), 'eval/epoch_eval_time': 4.09051775932312, 'eval/sps': 31291.882233798406}
I0727 05:39:45.099616 140120985872192 train.py:379] starting iteration 37 402.45844411849976
I0727 05:39:55.009064 140120985872192 train.py:394] {'eval/walltime': 171.48604202270508, 'training/sps': 42530.134448730496, 'training/walltime': 236.5802299976349, 'training/entropy_loss': Array(0.03260463, dtype=float32), 'training/policy_loss': Array(0.00467439, dtype=float32), 'training/total_loss': Array(363.6104, dtype=float32), 'training/v_loss': Array(363.57312, dtype=float32), 'eval/episode_goal_distance': (Array(0.31407577, dtype=float32), Array(0.04423872, dtype=float32)), 'eval/episode_reward': (Array(-44063.14, dtype=float32), Array(19222.492, dtype=float32)), 'eval/avg_episode_length': (Array(852.46875, dtype=float32), Array(353.3632, dtype=float32)), 'eval/epoch_eval_time': 4.127284288406372, 'eval/sps': 31013.128986426906}
I0727 05:39:55.011486 140120985872192 train.py:379] starting iteration 38 412.37031388282776
I0727 05:40:04.911438 140120985872192 train.py:394] {'eval/walltime': 175.58019304275513, 'training/sps': 42357.36869485549, 'training/walltime': 242.3822901248932, 'training/entropy_loss': Array(0.0217219, dtype=float32), 'training/policy_loss': Array(0.00890007, dtype=float32), 'training/total_loss': Array(301.0968, dtype=float32), 'training/v_loss': Array(301.0662, dtype=float32), 'eval/episode_goal_distance': (Array(0.31053707, dtype=float32), Array(0.04250759, dtype=float32)), 'eval/episode_reward': (Array(-46220.133, dtype=float32), Array(19660.908, dtype=float32)), 'eval/avg_episode_length': (Array(860.22656, dtype=float32), Array(345.5301, dtype=float32)), 'eval/epoch_eval_time': 4.094151020050049, 'eval/sps': 31264.112968269368}
I0727 05:40:04.913860 140120985872192 train.py:379] starting iteration 39 422.27268743515015
I0727 05:40:14.789905 140120985872192 train.py:394] {'eval/walltime': 179.6773784160614, 'training/sps': 42554.160408443786, 'training/walltime': 248.1575186252594, 'training/entropy_loss': Array(0.01290962, dtype=float32), 'training/policy_loss': Array(0.00244441, dtype=float32), 'training/total_loss': Array(367.05865, dtype=float32), 'training/v_loss': Array(367.04327, dtype=float32), 'eval/episode_goal_distance': (Array(0.30783358, dtype=float32), Array(0.04267792, dtype=float32)), 'eval/episode_reward': (Array(-43746.965, dtype=float32), Array(20743.404, dtype=float32)), 'eval/avg_episode_length': (Array(829.3125, dtype=float32), Array(374.66623, dtype=float32)), 'eval/epoch_eval_time': 4.097185373306274, 'eval/sps': 31240.958935843515}
I0727 05:40:14.792286 140120985872192 train.py:379] starting iteration 40 432.15111351013184
I0727 05:40:24.752233 140120985872192 train.py:394] {'eval/walltime': 183.81526136398315, 'training/sps': 42238.8096334231, 'training/walltime': 253.9758644104004, 'training/entropy_loss': Array(0.00863698, dtype=float32), 'training/policy_loss': Array(0.00186559, dtype=float32), 'training/total_loss': Array(400.6614, dtype=float32), 'training/v_loss': Array(400.6509, dtype=float32), 'eval/episode_goal_distance': (Array(0.31436905, dtype=float32), Array(0.04308518, dtype=float32)), 'eval/episode_reward': (Array(-45960.312, dtype=float32), Array(19092.348, dtype=float32)), 'eval/avg_episode_length': (Array(867.97656, dtype=float32), Array(337.35645, dtype=float32)), 'eval/epoch_eval_time': 4.137882947921753, 'eval/sps': 30933.69281126907}
I0727 05:40:24.754688 140120985872192 train.py:379] starting iteration 41 442.1135160923004
I0727 05:40:34.659028 140120985872192 train.py:394] {'eval/walltime': 187.91656470298767, 'training/sps': 42377.134913043374, 'training/walltime': 259.7752182483673, 'training/entropy_loss': Array(0.00268704, dtype=float32), 'training/policy_loss': Array(0.00196275, dtype=float32), 'training/total_loss': Array(1207.2003, dtype=float32), 'training/v_loss': Array(1207.1956, dtype=float32), 'eval/episode_goal_distance': (Array(0.3112818, dtype=float32), Array(0.04587471, dtype=float32)), 'eval/episode_reward': (Array(-46089.203, dtype=float32), Array(18715.854, dtype=float32)), 'eval/avg_episode_length': (Array(875.7031, dtype=float32), Array(328.85944, dtype=float32)), 'eval/epoch_eval_time': 4.101303339004517, 'eval/sps': 31209.591054308272}
I0727 05:40:34.661335 140120985872192 train.py:379] starting iteration 42 452.0201630592346
I0727 05:40:44.605121 140120985872192 train.py:394] {'eval/walltime': 192.0478413105011, 'training/sps': 42308.49303659207, 'training/walltime': 265.5839810371399, 'training/entropy_loss': Array(-0.00302105, dtype=float32), 'training/policy_loss': Array(0.0017641, dtype=float32), 'training/total_loss': Array(736.3502, dtype=float32), 'training/v_loss': Array(736.3515, dtype=float32), 'eval/episode_goal_distance': (Array(0.3146645, dtype=float32), Array(0.03934512, dtype=float32)), 'eval/episode_reward': (Array(-43435.812, dtype=float32), Array(21756.244, dtype=float32)), 'eval/avg_episode_length': (Array(813.5078, dtype=float32), Array(388.21536, dtype=float32)), 'eval/epoch_eval_time': 4.131276607513428, 'eval/sps': 30983.15899913607}
I0727 05:40:44.607647 140120985872192 train.py:379] starting iteration 43 461.9664752483368
I0727 05:40:54.504954 140120985872192 train.py:394] {'eval/walltime': 196.1393084526062, 'training/sps': 42356.952707165314, 'training/walltime': 271.3860981464386, 'training/entropy_loss': Array(-0.00753384, dtype=float32), 'training/policy_loss': Array(0.0013741, dtype=float32), 'training/total_loss': Array(492.99045, dtype=float32), 'training/v_loss': Array(492.99658, dtype=float32), 'eval/episode_goal_distance': (Array(0.3032742, dtype=float32), Array(0.04109297, dtype=float32)), 'eval/episode_reward': (Array(-44762.84, dtype=float32), Array(19906.83, dtype=float32)), 'eval/avg_episode_length': (Array(852.40625, dtype=float32), Array(353.51276, dtype=float32)), 'eval/epoch_eval_time': 4.0914671421051025, 'eval/sps': 31284.62127503306}
I0727 05:40:54.507331 140120985872192 train.py:379] starting iteration 44 471.86615896224976
I0727 05:41:04.409801 140120985872192 train.py:394] {'eval/walltime': 200.24317717552185, 'training/sps': 42408.48590225954, 'training/walltime': 277.1811647415161, 'training/entropy_loss': Array(-0.01194044, dtype=float32), 'training/policy_loss': Array(0.00154742, dtype=float32), 'training/total_loss': Array(354.8348, dtype=float32), 'training/v_loss': Array(354.8452, dtype=float32), 'eval/episode_goal_distance': (Array(0.30727562, dtype=float32), Array(0.04407015, dtype=float32)), 'eval/episode_reward': (Array(-43857.016, dtype=float32), Array(20445.537, dtype=float32)), 'eval/avg_episode_length': (Array(836.8906, dtype=float32), Array(368.18155, dtype=float32)), 'eval/epoch_eval_time': 4.103868722915649, 'eval/sps': 31190.081516316306}
I0727 05:41:04.412133 140120985872192 train.py:379] starting iteration 45 481.7709605693817
I0727 05:41:14.314216 140120985872192 train.py:394] {'eval/walltime': 204.3293640613556, 'training/sps': 42282.91716283568, 'training/walltime': 282.9934411048889, 'training/entropy_loss': Array(-0.01906141, dtype=float32), 'training/policy_loss': Array(0.00138507, dtype=float32), 'training/total_loss': Array(363.58533, dtype=float32), 'training/v_loss': Array(363.60297, dtype=float32), 'eval/episode_goal_distance': (Array(0.3113612, dtype=float32), Array(0.0425462, dtype=float32)), 'eval/episode_reward': (Array(-42685.168, dtype=float32), Array(20891.592, dtype=float32)), 'eval/avg_episode_length': (Array(821.52344, dtype=float32), Array(381.34048, dtype=float32)), 'eval/epoch_eval_time': 4.08618688583374, 'eval/sps': 31325.047917842123}
I0727 05:41:14.316621 140120985872192 train.py:379] starting iteration 46 491.6754493713379
