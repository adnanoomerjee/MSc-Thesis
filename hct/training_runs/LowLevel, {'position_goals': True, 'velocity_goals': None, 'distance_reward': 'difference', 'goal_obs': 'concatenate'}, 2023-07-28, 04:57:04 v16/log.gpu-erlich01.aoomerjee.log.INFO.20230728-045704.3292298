I0728 04:57:04.057088 140199540242240 low_level_env.py:188] Initialising environment...
I0728 04:57:43.247062 140199540242240 low_level_env.py:293] Environment initialised.
I0728 04:57:43.251867 140199540242240 train.py:118] JAX is running on GPU.
I0728 04:57:43.251962 140199540242240 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 04:57:51.669779 140199540242240 train.py:367] Running initial eval
I0728 04:58:07.578748 140199540242240 train.py:373] {'eval/walltime': 15.77217960357666, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3002417, 0.141522 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.327242, 12.159487], dtype=float32), 'eval/episode_reward': Array([-2.0763042,  9.202915 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29527953, 0.1447369 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.77217960357666, 'eval/sps': 8115.555567917411}
I0728 04:58:07.579993 140199540242240 train.py:379] starting iteration 0, 0 steps, 24.328165531158447
I0728 04:58:34.199875 140199540242240 train.py:394] {'eval/walltime': 19.546820163726807, 'training/sps': 17933.680414976418, 'training/walltime': 22.839706659317017, 'training/entropy_loss': Array(-0.04392709, dtype=float32), 'training/policy_loss': Array(0.00986696, dtype=float32), 'training/total_loss': Array(160.59494, dtype=float32), 'training/v_loss': Array(160.629, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28939134, 0.12004282], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.441383, 10.273154], dtype=float32), 'eval/episode_reward': Array([-0.2698858,  7.856965 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28479242, 0.12265883], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.7746405601501465, 'eval/sps': 33910.51358673167}
I0728 04:58:34.219079 140199540242240 train.py:379] starting iteration 1, 409600 steps, 50.96725153923035
I0728 04:58:42.353330 140199540242240 train.py:394] {'eval/walltime': 23.331140995025635, 'training/sps': 94262.78824098427, 'training/walltime': 27.18500566482544, 'training/entropy_loss': Array(-0.04307833, dtype=float32), 'training/policy_loss': Array(0.00089927, dtype=float32), 'training/total_loss': Array(103.40019, dtype=float32), 'training/v_loss': Array(103.44237, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30781275, 0.11735104], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.035286, 10.056465], dtype=float32), 'eval/episode_reward': Array([-2.2865012,  6.5670476], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30396712, 0.1192643 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.784320831298828, 'eval/sps': 33823.7706859724}
I0728 04:58:42.355443 140199540242240 train.py:379] starting iteration 2, 819200 steps, 59.10361647605896
I0728 04:58:50.512686 140199540242240 train.py:394] {'eval/walltime': 27.11652970314026, 'training/sps': 93796.26958803687, 'training/walltime': 31.55191707611084, 'training/entropy_loss': Array(-0.04301794, dtype=float32), 'training/policy_loss': Array(0.00058161, dtype=float32), 'training/total_loss': Array(96.07422, dtype=float32), 'training/v_loss': Array(96.11666, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28870806, 0.11121898], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.379011,  9.574283], dtype=float32), 'eval/episode_reward': Array([-1.3588498,  6.8648314], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2842415 , 0.11373284], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.785388708114624, 'eval/sps': 33814.22883351721}
I0728 04:58:50.514696 140199540242240 train.py:379] starting iteration 3, 1228800 steps, 67.26286911964417
I0728 04:58:58.689826 140199540242240 train.py:394] {'eval/walltime': 30.917002201080322, 'training/sps': 93736.95009396956, 'training/walltime': 35.921591997146606, 'training/entropy_loss': Array(-0.0438013, dtype=float32), 'training/policy_loss': Array(0.00042763, dtype=float32), 'training/total_loss': Array(92.95941, dtype=float32), 'training/v_loss': Array(93.00278, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29230946, 0.12221137], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.648054, 10.54264 ], dtype=float32), 'eval/episode_reward': Array([-1.1364806,  6.531272 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28754875, 0.12508607], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.8004724979400635, 'eval/sps': 33680.02269964556}
I0728 04:58:58.691862 140199540242240 train.py:379] starting iteration 4, 1638400 steps, 75.44003582000732
I0728 04:59:06.950365 140199540242240 train.py:394] {'eval/walltime': 34.77925515174866, 'training/sps': 93307.94040325414, 'training/walltime': 40.311357736587524, 'training/entropy_loss': Array(-0.04327568, dtype=float32), 'training/policy_loss': Array(0.00118127, dtype=float32), 'training/total_loss': Array(88.678345, dtype=float32), 'training/v_loss': Array(88.720436, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28698134, 0.11968215], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.212078, 10.265012], dtype=float32), 'eval/episode_reward': Array([-1.6667606,  6.8707833], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28232223, 0.12285021], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.862252950668335, 'eval/sps': 33141.278325090156}
I0728 04:59:06.952397 140199540242240 train.py:379] starting iteration 5, 2048000 steps, 83.70057129859924
I0728 04:59:15.258900 140199540242240 train.py:394] {'eval/walltime': 38.68252730369568, 'training/sps': 93124.88225277202, 'training/walltime': 44.709752559661865, 'training/entropy_loss': Array(-0.04187967, dtype=float32), 'training/policy_loss': Array(0.00188886, dtype=float32), 'training/total_loss': Array(143.29303, dtype=float32), 'training/v_loss': Array(143.33301, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29169038, 0.11378807], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.602562,  9.812898], dtype=float32), 'eval/episode_reward': Array([-1.5936946,  6.0495524], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28686315, 0.11699214], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9032721519470215, 'eval/sps': 32792.99905750905}
I0728 04:59:15.260966 140199540242240 train.py:379] starting iteration 6, 2457600 steps, 92.0091392993927
I0728 04:59:23.600368 140199540242240 train.py:394] {'eval/walltime': 42.608652114868164, 'training/sps': 92907.27174745686, 'training/walltime': 49.118449449539185, 'training/entropy_loss': Array(-0.03953107, dtype=float32), 'training/policy_loss': Array(0.00600887, dtype=float32), 'training/total_loss': Array(79.39316, dtype=float32), 'training/v_loss': Array(79.42669, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31621158, 0.1045348 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.620323,  8.95334 ], dtype=float32), 'eval/episode_reward': Array([-1.786725,  5.128225], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3125019 , 0.10642343], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9261248111724854, 'eval/sps': 32602.121979350548}
I0728 04:59:23.602372 140199540242240 train.py:379] starting iteration 7, 2867200 steps, 100.35054588317871
I0728 04:59:31.960778 140199540242240 train.py:394] {'eval/walltime': 46.56111192703247, 'training/sps': 93066.99349067805, 'training/walltime': 53.519580125808716, 'training/entropy_loss': Array(-0.03421777, dtype=float32), 'training/policy_loss': Array(0.01357767, dtype=float32), 'training/total_loss': Array(63.099297, dtype=float32), 'training/v_loss': Array(63.119938, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30116814, 0.11193225], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.269445,  9.599468], dtype=float32), 'eval/episode_reward': Array([-2.7724094,  5.538714 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29667333, 0.11436539], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9524598121643066, 'eval/sps': 32384.89600983676}
I0728 04:59:31.962803 140199540242240 train.py:379] starting iteration 8, 3276800 steps, 108.7109763622284
I0728 04:59:40.331858 140199540242240 train.py:394] {'eval/walltime': 50.519245862960815, 'training/sps': 92955.54561425038, 'training/walltime': 57.92598748207092, 'training/entropy_loss': Array(-0.02600961, dtype=float32), 'training/policy_loss': Array(0.02080986, dtype=float32), 'training/total_loss': Array(56.874622, dtype=float32), 'training/v_loss': Array(56.879818, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31563297, 0.11266857], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.366144,  9.719645], dtype=float32), 'eval/episode_reward': Array([-2.1484149,  5.494636 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31117898, 0.11557667], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9581339359283447, 'eval/sps': 32338.471126035492}
I0728 04:59:40.333892 140199540242240 train.py:379] starting iteration 9, 3686400 steps, 117.08206558227539
I0728 04:59:48.717618 140199540242240 train.py:394] {'eval/walltime': 54.4933660030365, 'training/sps': 92984.5249623675, 'training/walltime': 62.331021547317505, 'training/entropy_loss': Array(-0.012605, dtype=float32), 'training/policy_loss': Array(0.02263186, dtype=float32), 'training/total_loss': Array(50.62832, dtype=float32), 'training/v_loss': Array(50.61829, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28896356, 0.09719021], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.09264 ,  8.342757], dtype=float32), 'eval/episode_reward': Array([-1.3680729,  4.4176593], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.284122  , 0.09965635], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9741201400756836, 'eval/sps': 32208.387136872603}
I0728 04:59:48.719634 140199540242240 train.py:379] starting iteration 10, 4096000 steps, 125.46780705451965
I0728 04:59:57.096907 140199540242240 train.py:394] {'eval/walltime': 58.46220302581787, 'training/sps': 93009.81111555832, 'training/walltime': 66.73485803604126, 'training/entropy_loss': Array(-0.0053304, dtype=float32), 'training/policy_loss': Array(0.02743951, dtype=float32), 'training/total_loss': Array(118.521774, dtype=float32), 'training/v_loss': Array(118.499664, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27186137, 0.10079015], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.618567,  8.701339], dtype=float32), 'eval/episode_reward': Array([-0.69905686,  4.6638722 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26667333, 0.10402447], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.968837022781372, 'eval/sps': 32251.26133052882}
I0728 04:59:57.098773 140199540242240 train.py:379] starting iteration 11, 4505600 steps, 133.84694719314575
I0728 05:00:05.478132 140199540242240 train.py:394] {'eval/walltime': 62.432271003723145, 'training/sps': 92984.45953724453, 'training/walltime': 71.13989520072937, 'training/entropy_loss': Array(0.01782212, dtype=float32), 'training/policy_loss': Array(0.02291811, dtype=float32), 'training/total_loss': Array(46.63753, dtype=float32), 'training/v_loss': Array(46.596794, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29080608, 0.10841876], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.157997,  9.301908], dtype=float32), 'eval/episode_reward': Array([-0.7128796,  6.030489 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28580981, 0.1109939 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9700679779052734, 'eval/sps': 32241.26153818067}
I0728 05:00:05.479966 140199540242240 train.py:379] starting iteration 12, 4915200 steps, 142.22813987731934
I0728 05:00:13.866358 140199540242240 train.py:394] {'eval/walltime': 66.40791845321655, 'training/sps': 92992.27597459678, 'training/walltime': 75.54456210136414, 'training/entropy_loss': Array(0.03900418, dtype=float32), 'training/policy_loss': Array(0.02135874, dtype=float32), 'training/total_loss': Array(27.529533, dtype=float32), 'training/v_loss': Array(27.46917, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28914788, 0.10968943], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.041224,  9.435355], dtype=float32), 'eval/episode_reward': Array([0.5021314, 5.7662616], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28401154, 0.11288701], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.975647449493408, 'eval/sps': 32196.013762817485}
I0728 05:00:13.868195 140199540242240 train.py:379] starting iteration 13, 5324800 steps, 150.6163685321808
I0728 05:00:22.257407 140199540242240 train.py:394] {'eval/walltime': 70.38554787635803, 'training/sps': 92938.91077796627, 'training/walltime': 79.95175814628601, 'training/entropy_loss': Array(0.06131603, dtype=float32), 'training/policy_loss': Array(0.01834658, dtype=float32), 'training/total_loss': Array(16.47189, dtype=float32), 'training/v_loss': Array(16.392227, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3013394 , 0.09892259], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.148573,  8.528003], dtype=float32), 'eval/episode_reward': Array([-0.8908793,  6.046249 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2969051 , 0.10106907], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9776294231414795, 'eval/sps': 32179.9711293636}
I0728 05:00:22.259402 140199540242240 train.py:379] starting iteration 14, 5734400 steps, 159.00757479667664
I0728 05:00:30.653501 140199540242240 train.py:394] {'eval/walltime': 74.36964964866638, 'training/sps': 92987.67050959093, 'training/walltime': 84.35664319992065, 'training/entropy_loss': Array(0.08361353, dtype=float32), 'training/policy_loss': Array(0.02326684, dtype=float32), 'training/total_loss': Array(9.513399, dtype=float32), 'training/v_loss': Array(9.406519, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2865296 , 0.10173146], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.871944,  8.782647], dtype=float32), 'eval/episode_reward': Array([-0.17543939,  4.064035  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28125334, 0.10442074], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9841017723083496, 'eval/sps': 32127.69334600558}
I0728 05:00:30.655497 140199540242240 train.py:379] starting iteration 15, 6144000 steps, 167.40367102622986
I0728 05:00:39.055888 140199540242240 train.py:394] {'eval/walltime': 78.36063027381897, 'training/sps': 92998.15049196559, 'training/walltime': 88.76103186607361, 'training/entropy_loss': Array(0.07910024, dtype=float32), 'training/policy_loss': Array(0.03134276, dtype=float32), 'training/total_loss': Array(90.34512, dtype=float32), 'training/v_loss': Array(90.23467, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28092065, 0.10033069], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.390175,  8.621423], dtype=float32), 'eval/episode_reward': Array([0.08734281, 4.7335443 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2760217 , 0.10288196], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.990980625152588, 'eval/sps': 32072.318064712766}
I0728 05:00:39.057862 140199540242240 train.py:379] starting iteration 16, 6553600 steps, 175.80603623390198
I0728 05:00:47.480684 140199540242240 train.py:394] {'eval/walltime': 82.37068390846252, 'training/sps': 92916.86421476053, 'training/walltime': 93.16927361488342, 'training/entropy_loss': Array(0.1076335, dtype=float32), 'training/policy_loss': Array(0.01985364, dtype=float32), 'training/total_loss': Array(10.867173, dtype=float32), 'training/v_loss': Array(10.739686, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2757156 , 0.09699693], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.932882,  8.408685], dtype=float32), 'eval/episode_reward': Array([0.42129493, 3.1128156 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27051196, 0.09954734], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.010053634643555, 'eval/sps': 31919.772567176064}
I0728 05:00:47.482689 140199540242240 train.py:379] starting iteration 17, 6963200 steps, 184.2308623790741
I0728 05:00:55.913958 140199540242240 train.py:394] {'eval/walltime': 86.39204955101013, 'training/sps': 92980.80594189759, 'training/walltime': 97.57448387145996, 'training/entropy_loss': Array(0.13543871, dtype=float32), 'training/policy_loss': Array(0.02080562, dtype=float32), 'training/total_loss': Array(4.9651766, dtype=float32), 'training/v_loss': Array(4.8089323, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27686524, 0.09714036], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.063179,  8.413417], dtype=float32), 'eval/episode_reward': Array([-0.34444487,  3.85886   ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27138394, 0.10036168], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021365642547607, 'eval/sps': 31829.982990283297}
I0728 05:00:55.915948 140199540242240 train.py:379] starting iteration 18, 7372800 steps, 192.6641218662262
I0728 05:01:04.337364 140199540242240 train.py:394] {'eval/walltime': 90.40875911712646, 'training/sps': 93087.5023441378, 'training/walltime': 101.97464489936829, 'training/entropy_loss': Array(0.16962194, dtype=float32), 'training/policy_loss': Array(0.02047691, dtype=float32), 'training/total_loss': Array(2.440443, dtype=float32), 'training/v_loss': Array(2.250344, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29971159, 0.14219823], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.013145, 12.267664], dtype=float32), 'eval/episode_reward': Array([-1.3522184,  6.988141 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2941025 , 0.14575528], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016709566116333, 'eval/sps': 31866.879567237505}
I0728 05:01:04.339369 140199540242240 train.py:379] starting iteration 19, 7782400 steps, 201.0875425338745
I0728 05:01:12.773766 140199540242240 train.py:394] {'eval/walltime': 94.43819427490234, 'training/sps': 93077.19383884965, 'training/walltime': 106.3752932548523, 'training/entropy_loss': Array(0.20656516, dtype=float32), 'training/policy_loss': Array(0.02157852, dtype=float32), 'training/total_loss': Array(1.4641658, dtype=float32), 'training/v_loss': Array(1.2360221, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27745622, 0.10214148], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.100203,  8.811453], dtype=float32), 'eval/episode_reward': Array([-0.30172968,  2.446588  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2715024 , 0.10690027], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.029435157775879, 'eval/sps': 31766.238936241367}
I0728 05:01:12.775763 140199540242240 train.py:379] starting iteration 20, 8192000 steps, 209.5239372253418
I0728 05:01:21.227485 140199540242240 train.py:394] {'eval/walltime': 98.48120498657227, 'training/sps': 93002.73182176579, 'training/walltime': 110.77946496009827, 'training/entropy_loss': Array(0.23630676, dtype=float32), 'training/policy_loss': Array(0.03339742, dtype=float32), 'training/total_loss': Array(101.08044, dtype=float32), 'training/v_loss': Array(100.81073, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28990704, 0.11535307], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.167286 ,  9.9097185], dtype=float32), 'eval/episode_reward': Array([-0.77346504,  4.8730636 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28499085, 0.11779168], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.043010711669922, 'eval/sps': 31659.57478928642}
I0728 05:01:21.229467 140199540242240 train.py:379] starting iteration 21, 8601600 steps, 217.977641582489
I0728 05:01:29.685826 140199540242240 train.py:394] {'eval/walltime': 102.5270323753357, 'training/sps': 92966.0837555034, 'training/walltime': 115.18537282943726, 'training/entropy_loss': Array(0.2689889, dtype=float32), 'training/policy_loss': Array(0.02673085, dtype=float32), 'training/total_loss': Array(7.686967, dtype=float32), 'training/v_loss': Array(7.391247, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29185385, 0.11403401], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.384674,  9.810879], dtype=float32), 'eval/episode_reward': Array([-0.6196491,  5.3335834], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28686613, 0.11715026], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.045827388763428, 'eval/sps': 31637.533611912717}
I0728 05:01:29.687827 140199540242240 train.py:379] starting iteration 22, 9011200 steps, 226.43600130081177
I0728 05:01:38.155292 140199540242240 train.py:394] {'eval/walltime': 106.58871412277222, 'training/sps': 93094.55419263296, 'training/walltime': 119.585200548172, 'training/entropy_loss': Array(0.30427617, dtype=float32), 'training/policy_loss': Array(0.02556919, dtype=float32), 'training/total_loss': Array(3.3447313, dtype=float32), 'training/v_loss': Array(3.0148857, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26199847, 0.11060871], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.776701,  9.524567], dtype=float32), 'eval/episode_reward': Array([-0.19934818,  6.781505  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25600812, 0.11383115], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.061681747436523, 'eval/sps': 31514.039740012988}
I0728 05:01:38.159387 140199540242240 train.py:379] starting iteration 23, 9420800 steps, 234.907546043396
I0728 05:01:46.626895 140199540242240 train.py:394] {'eval/walltime': 110.64638900756836, 'training/sps': 92999.25299053338, 'training/walltime': 123.98953700065613, 'training/entropy_loss': Array(0.32194674, dtype=float32), 'training/policy_loss': Array(0.02768326, dtype=float32), 'training/total_loss': Array(8.303148, dtype=float32), 'training/v_loss': Array(7.9535184, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27472985, 0.11298589], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.868694,  9.681574], dtype=float32), 'eval/episode_reward': Array([-0.37293446,  5.6304307 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2691664 , 0.11598565], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.057674884796143, 'eval/sps': 31545.159145107486}
I0728 05:01:46.628900 140199540242240 train.py:379] starting iteration 24, 9830400 steps, 243.37707424163818
I0728 05:01:55.111884 140199540242240 train.py:394] {'eval/walltime': 114.72038674354553, 'training/sps': 93028.55681123404, 'training/walltime': 128.39248609542847, 'training/entropy_loss': Array(0.32266963, dtype=float32), 'training/policy_loss': Array(0.03233304, dtype=float32), 'training/total_loss': Array(13.873144, dtype=float32), 'training/v_loss': Array(13.518141, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26809302, 0.12298465], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.339928, 10.563814], dtype=float32), 'eval/episode_reward': Array([0.41736627, 6.2640734 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2626489, 0.1254601], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.073997735977173, 'eval/sps': 31418.77052842751}
I0728 05:01:55.113924 140199540242240 train.py:379] starting iteration 25, 10240000 steps, 251.86209726333618
I0728 05:02:03.581326 140199540242240 train.py:394] {'eval/walltime': 118.77743768692017, 'training/sps': 92970.38521095076, 'training/walltime': 132.79819011688232, 'training/entropy_loss': Array(0.32696453, dtype=float32), 'training/policy_loss': Array(0.05018793, dtype=float32), 'training/total_loss': Array(129.21529, dtype=float32), 'training/v_loss': Array(128.83812, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2693044 , 0.11489986], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.40231 ,  9.924073], dtype=float32), 'eval/episode_reward': Array([0.41361046, 5.500282  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2632441 , 0.11860222], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.057050943374634, 'eval/sps': 31550.01053389048}
I0728 05:02:03.615191 140199540242240 train.py:379] starting iteration 26, 10649600 steps, 260.3633644580841
I0728 05:02:12.093566 140199540242240 train.py:394] {'eval/walltime': 122.84060001373291, 'training/sps': 92870.2219258425, 'training/walltime': 137.20864582061768, 'training/entropy_loss': Array(0.33163923, dtype=float32), 'training/policy_loss': Array(0.06760076, dtype=float32), 'training/total_loss': Array(40.122375, dtype=float32), 'training/v_loss': Array(39.723137, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2753973 , 0.11298805], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.886364,  9.766887], dtype=float32), 'eval/episode_reward': Array([0.47853094, 5.82623   ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26893437, 0.11716218], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.063162326812744, 'eval/sps': 31502.556310716413}
I0728 05:02:12.095669 140199540242240 train.py:379] starting iteration 27, 11059200 steps, 268.84384393692017
I0728 05:02:20.552271 140199540242240 train.py:394] {'eval/walltime': 126.89319968223572, 'training/sps': 93102.32356013423, 'training/walltime': 141.6081063747406, 'training/entropy_loss': Array(0.33970016, dtype=float32), 'training/policy_loss': Array(0.06376724, dtype=float32), 'training/total_loss': Array(37.814884, dtype=float32), 'training/v_loss': Array(37.411423, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26964527, 0.10031521], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.405987,  8.634175], dtype=float32), 'eval/episode_reward': Array([1.0667572, 5.174303 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26395476, 0.10262586], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.052599668502808, 'eval/sps': 31584.664282245358}
I0728 05:02:20.554281 140199540242240 train.py:379] starting iteration 28, 11468800 steps, 277.30245542526245
I0728 05:02:29.017716 140199540242240 train.py:394] {'eval/walltime': 130.94651436805725, 'training/sps': 92976.93626214717, 'training/walltime': 146.01349997520447, 'training/entropy_loss': Array(0.3187875, dtype=float32), 'training/policy_loss': Array(0.09408525, dtype=float32), 'training/total_loss': Array(43.69642, dtype=float32), 'training/v_loss': Array(43.283546, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2968359 , 0.13464107], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.796457, 11.678121], dtype=float32), 'eval/episode_reward': Array([-0.5373842,  8.409531 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2901206 , 0.13872509], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.053314685821533, 'eval/sps': 31579.092649219445}
I0728 05:02:29.019817 140199540242240 train.py:379] starting iteration 29, 11878400 steps, 285.7679901123047
I0728 05:02:37.493146 140199540242240 train.py:394] {'eval/walltime': 135.00750303268433, 'training/sps': 92923.18155169806, 'training/walltime': 150.42144203186035, 'training/entropy_loss': Array(0.24860574, dtype=float32), 'training/policy_loss': Array(0.0726216, dtype=float32), 'training/total_loss': Array(159.33853, dtype=float32), 'training/v_loss': Array(159.01732, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31599513, 0.11566009], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.546644, 10.024123], dtype=float32), 'eval/episode_reward': Array([-2.0173147,  7.9937887], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31028807, 0.11849118], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.060988664627075, 'eval/sps': 31519.418193636935}
I0728 05:02:37.495188 140199540242240 train.py:379] starting iteration 30, 12288000 steps, 294.24336218833923
I0728 05:02:45.968424 140199540242240 train.py:394] {'eval/walltime': 139.0712857246399, 'training/sps': 93025.45886862637, 'training/walltime': 154.82453775405884, 'training/entropy_loss': Array(0.21710238, dtype=float32), 'training/policy_loss': Array(0.03411558, dtype=float32), 'training/total_loss': Array(318.32962, dtype=float32), 'training/v_loss': Array(318.07843, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30133796, 0.11736036], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.334856, 10.174132], dtype=float32), 'eval/episode_reward': Array([-2.0344565,  6.902051 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29484493, 0.12147314], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.063782691955566, 'eval/sps': 31497.747222896916}
I0728 05:02:45.972556 140199540242240 train.py:379] starting iteration 31, 12697600 steps, 302.72071409225464
I0728 05:02:54.416728 140199540242240 train.py:394] {'eval/walltime': 143.0961492061615, 'training/sps': 92793.36895048658, 'training/walltime': 159.2386462688446, 'training/entropy_loss': Array(0.17783126, dtype=float32), 'training/policy_loss': Array(0.0210529, dtype=float32), 'training/total_loss': Array(171.66962, dtype=float32), 'training/v_loss': Array(171.47073, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31253344, 0.1213577 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.29322 , 10.509392], dtype=float32), 'eval/episode_reward': Array([-1.892654,  8.111801], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30656284, 0.12457947], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0248634815216064, 'eval/sps': 31802.320895517525}
I0728 05:02:54.418748 140199540242240 train.py:379] starting iteration 32, 13107200 steps, 311.16692185401917
I0728 05:03:02.840547 140199540242240 train.py:394] {'eval/walltime': 147.11814761161804, 'training/sps': 93188.34741900214, 'training/walltime': 163.6340456008911, 'training/entropy_loss': Array(0.14425221, dtype=float32), 'training/policy_loss': Array(0.01385946, dtype=float32), 'training/total_loss': Array(128.3604, dtype=float32), 'training/v_loss': Array(128.20229, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30560192, 0.12563396], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.627148, 10.897391], dtype=float32), 'eval/episode_reward': Array([-1.6822054,  8.843076 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29909003, 0.1299448 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021998405456543, 'eval/sps': 31824.97532230387}
I0728 05:03:02.842553 140199540242240 train.py:379] starting iteration 33, 13516800 steps, 319.590726852417
I0728 05:03:11.486018 140199540242240 train.py:394] {'eval/walltime': 151.15848779678345, 'training/sps': 89079.21558672073, 'training/walltime': 168.23220014572144, 'training/entropy_loss': Array(0.11603522, dtype=float32), 'training/policy_loss': Array(0.00928396, dtype=float32), 'training/total_loss': Array(112.564896, dtype=float32), 'training/v_loss': Array(112.439575, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30910137, 0.12977903], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.00307 , 11.248827], dtype=float32), 'eval/episode_reward': Array([-2.94175 ,  7.614802], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30291378, 0.13352787], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.040340185165405, 'eval/sps': 31680.50068406799}
I0728 05:03:11.488067 140199540242240 train.py:379] starting iteration 34, 13926400 steps, 328.2362413406372
I0728 05:03:19.887556 140199540242240 train.py:394] {'eval/walltime': 155.16645169258118, 'training/sps': 93367.47912028035, 'training/walltime': 172.61916661262512, 'training/entropy_loss': Array(0.09330029, dtype=float32), 'training/policy_loss': Array(0.00722665, dtype=float32), 'training/total_loss': Array(108.291374, dtype=float32), 'training/v_loss': Array(108.190834, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28508997, 0.13920708], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.88024, 12.0695 ], dtype=float32), 'eval/episode_reward': Array([-2.6185973,  8.971504 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2776611 , 0.14404228], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0079638957977295, 'eval/sps': 31936.415428842924}
I0728 05:03:19.889555 140199540242240 train.py:379] starting iteration 35, 14336000 steps, 336.63772916793823
I0728 05:03:28.313527 140199540242240 train.py:394] {'eval/walltime': 159.18850660324097, 'training/sps': 93151.2751574068, 'training/walltime': 177.0163152217865, 'training/entropy_loss': Array(0.07670549, dtype=float32), 'training/policy_loss': Array(0.00795995, dtype=float32), 'training/total_loss': Array(427.8786, dtype=float32), 'training/v_loss': Array(427.79395, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2870412, 0.1173131], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.080257, 10.159554], dtype=float32), 'eval/episode_reward': Array([-2.0038862,  8.007684 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28053913, 0.12122829], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.02205491065979, 'eval/sps': 31824.528218338644}
I0728 05:03:28.315531 140199540242240 train.py:379] starting iteration 36, 14745600 steps, 345.0637049674988
I0728 05:03:36.729963 140199540242240 train.py:394] {'eval/walltime': 163.1985731124878, 'training/sps': 93098.25709306553, 'training/walltime': 181.41596794128418, 'training/entropy_loss': Array(0.05354135, dtype=float32), 'training/policy_loss': Array(0.00488659, dtype=float32), 'training/total_loss': Array(227.00418, dtype=float32), 'training/v_loss': Array(226.94576, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31545392, 0.12648374], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.547577, 10.903583], dtype=float32), 'eval/episode_reward': Array([-2.5411503,  9.87973  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31001234, 0.12922664], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.010066509246826, 'eval/sps': 31919.670086479702}
I0728 05:03:36.731945 140199540242240 train.py:379] starting iteration 37, 15155200 steps, 353.4801185131073
I0728 05:03:45.164014 140199540242240 train.py:394] {'eval/walltime': 167.2187852859497, 'training/sps': 92946.03064407437, 'training/walltime': 185.82282638549805, 'training/entropy_loss': Array(0.03697222, dtype=float32), 'training/policy_loss': Array(0.0047979, dtype=float32), 'training/total_loss': Array(156.23004, dtype=float32), 'training/v_loss': Array(156.18828, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31817022, 0.12722236], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.71178 , 10.944825], dtype=float32), 'eval/episode_reward': Array([-2.7511878,  8.602996 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31261945, 0.13016894], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020212173461914, 'eval/sps': 31839.115568314824}
I0728 05:03:45.166009 140199540242240 train.py:379] starting iteration 38, 15564800 steps, 361.914183139801
I0728 05:03:53.600743 140199540242240 train.py:394] {'eval/walltime': 171.23571586608887, 'training/sps': 92807.24934937194, 'training/walltime': 190.23627471923828, 'training/entropy_loss': Array(0.0280994, dtype=float32), 'training/policy_loss': Array(0.00601045, dtype=float32), 'training/total_loss': Array(128.77994, dtype=float32), 'training/v_loss': Array(128.74583, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32922235, 0.15268265], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.611637, 13.149224], dtype=float32), 'eval/episode_reward': Array([-4.2015767,  9.9305525], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3231993 , 0.15670249], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.01693058013916, 'eval/sps': 31865.126231672555}
I0728 05:03:53.602765 140199540242240 train.py:379] starting iteration 39, 15974400 steps, 370.35093927383423
I0728 05:04:02.044260 140199540242240 train.py:394] {'eval/walltime': 175.25874042510986, 'training/sps': 92798.33614233191, 'training/walltime': 194.65014696121216, 'training/entropy_loss': Array(0.02587987, dtype=float32), 'training/policy_loss': Array(0.00837446, dtype=float32), 'training/total_loss': Array(106.8195, dtype=float32), 'training/v_loss': Array(106.78525, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.322545  , 0.13600576], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.95653 , 11.736696], dtype=float32), 'eval/episode_reward': Array([-3.0318177,  9.566081 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31689024, 0.13934837], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023024559020996, 'eval/sps': 31816.85772038857}
I0728 05:04:02.046224 140199540242240 train.py:379] starting iteration 40, 16384000 steps, 378.7943983078003
I0728 05:04:10.480779 140199540242240 train.py:394] {'eval/walltime': 179.2800908088684, 'training/sps': 92910.13068844985, 'training/walltime': 199.05870819091797, 'training/entropy_loss': Array(0.02070138, dtype=float32), 'training/policy_loss': Array(0.01023984, dtype=float32), 'training/total_loss': Array(425.45557, dtype=float32), 'training/v_loss': Array(425.42462, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3123038, 0.141663 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.080235, 12.246287], dtype=float32), 'eval/episode_reward': Array([-2.4504108,  8.745112 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30573738, 0.14605816], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021350383758545, 'eval/sps': 31830.103767373068}
I0728 05:04:10.482805 140199540242240 train.py:379] starting iteration 41, 16793600 steps, 387.2309784889221
I0728 05:04:18.925798 140199540242240 train.py:394] {'eval/walltime': 183.30609393119812, 'training/sps': 92834.68650195535, 'training/walltime': 203.47085213661194, 'training/entropy_loss': Array(0.01681489, dtype=float32), 'training/policy_loss': Array(0.0077839, dtype=float32), 'training/total_loss': Array(230.18616, dtype=float32), 'training/v_loss': Array(230.16154, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32030326, 0.12534395], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.766409, 10.810728], dtype=float32), 'eval/episode_reward': Array([-1.9553229,  9.692383 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31463444, 0.12882909], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.026003122329712, 'eval/sps': 31793.318611717998}
I0728 05:04:18.927809 140199540242240 train.py:379] starting iteration 42, 17203200 steps, 395.6759829521179
I0728 05:04:27.358525 140199540242240 train.py:394] {'eval/walltime': 187.32197403907776, 'training/sps': 92878.74222411681, 'training/walltime': 207.88090324401855, 'training/entropy_loss': Array(0.01734949, dtype=float32), 'training/policy_loss': Array(0.00790249, dtype=float32), 'training/total_loss': Array(162.26419, dtype=float32), 'training/v_loss': Array(162.23894, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30601507, 0.12073856], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.477692, 10.406014], dtype=float32), 'eval/episode_reward': Array([-2.4989772,  8.713938 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30028218, 0.12422688], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.015880107879639, 'eval/sps': 31873.461498227658}
I0728 05:04:27.360454 140199540242240 train.py:379] starting iteration 43, 17612800 steps, 404.108628988266
I0728 05:04:35.829249 140199540242240 train.py:394] {'eval/walltime': 191.35192561149597, 'training/sps': 92373.93675875131, 'training/walltime': 212.3150544166565, 'training/entropy_loss': Array(0.02256493, dtype=float32), 'training/policy_loss': Array(0.00839644, dtype=float32), 'training/total_loss': Array(104.44976, dtype=float32), 'training/v_loss': Array(104.418785, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30850068, 0.13870804], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.707222, 12.000221], dtype=float32), 'eval/episode_reward': Array([-2.842964,  9.098576], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30223167, 0.1432046 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.029951572418213, 'eval/sps': 31762.168279156842}
I0728 05:04:35.831245 140199540242240 train.py:379] starting iteration 44, 18022400 steps, 412.5794184207916
I0728 05:04:44.286494 140199540242240 train.py:394] {'eval/walltime': 195.3670380115509, 'training/sps': 92346.41880226105, 'training/walltime': 216.75052690505981, 'training/entropy_loss': Array(0.03097458, dtype=float32), 'training/policy_loss': Array(0.00887653, dtype=float32), 'training/total_loss': Array(72.19654, dtype=float32), 'training/v_loss': Array(72.156685, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2852936 , 0.11824541], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.693403, 10.187735], dtype=float32), 'eval/episode_reward': Array([-1.5138936,  7.5331335], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2793703 , 0.12171973], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.015112400054932, 'eval/sps': 31879.5558496068}
I0728 05:04:44.288506 140199540242240 train.py:379] starting iteration 45, 18432000 steps, 421.0366804599762
I0728 05:04:52.743055 140199540242240 train.py:394] {'eval/walltime': 199.38608312606812, 'training/sps': 92445.06678588118, 'training/walltime': 221.1812663078308, 'training/entropy_loss': Array(0.0370646, dtype=float32), 'training/policy_loss': Array(0.01048408, dtype=float32), 'training/total_loss': Array(304.5904, dtype=float32), 'training/v_loss': Array(304.54285, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2848193 , 0.11918125], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.63895 , 10.254659], dtype=float32), 'eval/episode_reward': Array([-1.1394967,  6.924553 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27921328, 0.12216999], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019045114517212, 'eval/sps': 31848.361079016155}
I0728 05:04:52.745077 140199540242240 train.py:379] starting iteration 46, 18841600 steps, 429.4932506084442
I0728 05:05:01.203511 140199540242240 train.py:394] {'eval/walltime': 203.40604329109192, 'training/sps': 92379.93707577094, 'training/walltime': 225.6151294708252, 'training/entropy_loss': Array(0.03768943, dtype=float32), 'training/policy_loss': Array(0.00897924, dtype=float32), 'training/total_loss': Array(143.3117, dtype=float32), 'training/v_loss': Array(143.26505, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2974294 , 0.10895643], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.769875,  9.349091], dtype=float32), 'eval/episode_reward': Array([-1.6387337,  7.7555513], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29264283, 0.11145461], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019960165023804, 'eval/sps': 31841.111539781155}
I0728 05:05:01.205530 140199540242240 train.py:379] starting iteration 47, 19251200 steps, 437.9537031650543
I0728 05:05:09.676096 140199540242240 train.py:394] {'eval/walltime': 207.43927907943726, 'training/sps': 92399.9404505279, 'training/walltime': 230.04803276062012, 'training/entropy_loss': Array(0.03787758, dtype=float32), 'training/policy_loss': Array(0.00848696, dtype=float32), 'training/total_loss': Array(101.81288, dtype=float32), 'training/v_loss': Array(101.766525, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29283988, 0.11662064], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.420753, 10.052201], dtype=float32), 'eval/episode_reward': Array([-0.61704934,  8.3671875 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28755534, 0.11986625], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.033235788345337, 'eval/sps': 31736.304723338006}
I0728 05:05:09.678126 140199540242240 train.py:379] starting iteration 48, 19660800 steps, 446.42629957199097
I0728 05:05:18.113106 140199540242240 train.py:394] {'eval/walltime': 211.45728373527527, 'training/sps': 92835.13798902754, 'training/walltime': 234.46015524864197, 'training/entropy_loss': Array(0.03661637, dtype=float32), 'training/policy_loss': Array(0.00664616, dtype=float32), 'training/total_loss': Array(78.229904, dtype=float32), 'training/v_loss': Array(78.18664, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2619103 , 0.12400478], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.681496, 10.701688], dtype=float32), 'eval/episode_reward': Array([1.2362512, 6.5424814], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2552737, 0.128143 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.018004655838013, 'eval/sps': 31856.608183372988}
I0728 05:05:18.115139 140199540242240 train.py:379] starting iteration 49, 20070400 steps, 454.86331248283386
I0728 05:05:26.557934 140199540242240 train.py:394] {'eval/walltime': 215.48301935195923, 'training/sps': 92829.91104092679, 'training/walltime': 238.87252616882324, 'training/entropy_loss': Array(0.03633437, dtype=float32), 'training/policy_loss': Array(0.00754375, dtype=float32), 'training/total_loss': Array(60.47399, dtype=float32), 'training/v_loss': Array(60.430107, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27366793, 0.12035106], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.673914, 10.375894], dtype=float32), 'eval/episode_reward': Array([-0.37940305,  6.89735   ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26751736, 0.12405924], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.02573561668396, 'eval/sps': 31795.431242311664}
I0728 05:05:26.560002 140199540242240 train.py:379] starting iteration 50, 20480000 steps, 463.3081753253937
I0728 05:05:35.017528 140199540242240 train.py:394] {'eval/walltime': 219.50469326972961, 'training/sps': 92444.36041521716, 'training/walltime': 243.30329942703247, 'training/entropy_loss': Array(0.03998233, dtype=float32), 'training/policy_loss': Array(0.00983617, dtype=float32), 'training/total_loss': Array(226.4147, dtype=float32), 'training/v_loss': Array(226.3649, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26675656, 0.10471926], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.127693,  9.075277], dtype=float32), 'eval/episode_reward': Array([0.88872343, 6.3832088 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26036882, 0.10876955], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021673917770386, 'eval/sps': 31827.5431119396}
I0728 05:05:35.059883 140199540242240 train.py:379] starting iteration 51, 20889600 steps, 471.80805563926697
I0728 05:05:43.529429 140199540242240 train.py:394] {'eval/walltime': 223.53047847747803, 'training/sps': 92273.00203528508, 'training/walltime': 247.74230098724365, 'training/entropy_loss': Array(0.03750873, dtype=float32), 'training/policy_loss': Array(0.00600832, dtype=float32), 'training/total_loss': Array(105.89996, dtype=float32), 'training/v_loss': Array(105.85645, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2623466 , 0.11558297], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.775589,  9.952341], dtype=float32), 'eval/episode_reward': Array([0.23977783, 7.271011  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25626206, 0.11877882], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.025785207748413, 'eval/sps': 31795.0395747987}
I0728 05:05:43.531560 140199540242240 train.py:379] starting iteration 52, 21299200 steps, 480.2797338962555
I0728 05:05:52.002070 140199540242240 train.py:394] {'eval/walltime': 227.5524182319641, 'training/sps': 92171.31346141166, 'training/walltime': 252.18619990348816, 'training/entropy_loss': Array(0.03676223, dtype=float32), 'training/policy_loss': Array(0.00613114, dtype=float32), 'training/total_loss': Array(89.241585, dtype=float32), 'training/v_loss': Array(89.198685, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25129926, 0.10710449], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.859215,  9.235088], dtype=float32), 'eval/episode_reward': Array([1.4777485, 6.533267 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24462925, 0.11160467], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021939754486084, 'eval/sps': 31825.439418188304}
I0728 05:05:52.004080 140199540242240 train.py:379] starting iteration 53, 21708800 steps, 488.75225377082825
I0728 05:06:00.476599 140199540242240 train.py:394] {'eval/walltime': 231.5863208770752, 'training/sps': 92375.23311884874, 'training/walltime': 256.62028884887695, 'training/entropy_loss': Array(0.03822958, dtype=float32), 'training/policy_loss': Array(0.00755882, dtype=float32), 'training/total_loss': Array(82.401985, dtype=float32), 'training/v_loss': Array(82.3562, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26085877, 0.12299985], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.632013, 10.58496 ], dtype=float32), 'eval/episode_reward': Array([0.4535659, 6.7380733], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25417507, 0.1269877 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.033902645111084, 'eval/sps': 31731.058297881948}
I0728 05:06:00.478634 140199540242240 train.py:379] starting iteration 54, 22118400 steps, 497.22680830955505
I0728 05:06:08.916934 140199540242240 train.py:394] {'eval/walltime': 235.6077344417572, 'training/sps': 92833.9741646162, 'training/walltime': 261.03246665000916, 'training/entropy_loss': Array(0.04421184, dtype=float32), 'training/policy_loss': Array(0.00585981, dtype=float32), 'training/total_loss': Array(70.5128, dtype=float32), 'training/v_loss': Array(70.46272, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26486415, 0.12035065], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.97565 , 10.378319], dtype=float32), 'eval/episode_reward': Array([1.0382165, 7.421363 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2584446 , 0.12437683], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021413564682007, 'eval/sps': 31829.603680695196}
I0728 05:06:08.918952 140199540242240 train.py:379] starting iteration 55, 22528000 steps, 505.6671259403229
I0728 05:06:17.407202 140199540242240 train.py:394] {'eval/walltime': 239.63016033172607, 'training/sps': 91818.85713940609, 'training/walltime': 265.4934239387512, 'training/entropy_loss': Array(0.04594845, dtype=float32), 'training/policy_loss': Array(0.0050167, dtype=float32), 'training/total_loss': Array(243.39532, dtype=float32), 'training/v_loss': Array(243.34438, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28485066, 0.12226424], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.675713, 10.479243], dtype=float32), 'eval/episode_reward': Array([-0.05463788,  6.296749  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27901882, 0.12564734], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022425889968872, 'eval/sps': 31821.593113550327}
I0728 05:06:17.409229 140199540242240 train.py:379] starting iteration 56, 22937600 steps, 514.1574029922485
I0728 05:06:25.877632 140199540242240 train.py:394] {'eval/walltime': 243.67585468292236, 'training/sps': 92711.49449422823, 'training/walltime': 269.9114305973053, 'training/entropy_loss': Array(0.05083457, dtype=float32), 'training/policy_loss': Array(0.00509219, dtype=float32), 'training/total_loss': Array(111.23158, dtype=float32), 'training/v_loss': Array(111.17565, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2786687 , 0.13194202], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.049759, 11.405779], dtype=float32), 'eval/episode_reward': Array([0.00953735, 8.699176  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27200288, 0.13584487], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.045694351196289, 'eval/sps': 31638.57397238897}
I0728 05:06:25.879608 140199540242240 train.py:379] starting iteration 57, 23347200 steps, 522.6277816295624
I0728 05:06:34.337381 140199540242240 train.py:394] {'eval/walltime': 247.69500637054443, 'training/sps': 92384.661383708, 'training/walltime': 274.34506702423096, 'training/entropy_loss': Array(0.05620836, dtype=float32), 'training/policy_loss': Array(0.00653286, dtype=float32), 'training/total_loss': Array(87.41818, dtype=float32), 'training/v_loss': Array(87.35544, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2750857 , 0.12069158], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.784615, 10.460102], dtype=float32), 'eval/episode_reward': Array([0.8315131, 7.420638 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26867673, 0.12470503], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.01915168762207, 'eval/sps': 31847.5165777411}
I0728 05:06:34.339333 140199540242240 train.py:379] starting iteration 58, 23756800 steps, 531.0875072479248
I0728 05:06:42.806687 140199540242240 train.py:394] {'eval/walltime': 251.72111988067627, 'training/sps': 92325.80349587492, 'training/walltime': 278.78152990341187, 'training/entropy_loss': Array(0.06400892, dtype=float32), 'training/policy_loss': Array(0.00513906, dtype=float32), 'training/total_loss': Array(68.351746, dtype=float32), 'training/v_loss': Array(68.2826, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27821195, 0.12111475], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.156828, 10.42928 ], dtype=float32), 'eval/episode_reward': Array([0.4588871, 7.204827 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2726269 , 0.12437573], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.026113510131836, 'eval/sps': 31792.446903914693}
I0728 05:06:42.808681 140199540242240 train.py:379] starting iteration 59, 24166400 steps, 539.556854724884
I0728 05:06:51.272115 140199540242240 train.py:394] {'eval/walltime': 255.7405345439911, 'training/sps': 92272.63529430468, 'training/walltime': 283.2205491065979, 'training/entropy_loss': Array(0.07423158, dtype=float32), 'training/policy_loss': Array(0.00480233, dtype=float32), 'training/total_loss': Array(50.643646, dtype=float32), 'training/v_loss': Array(50.564613, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2821542 , 0.11293581], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.396336,  9.718692], dtype=float32), 'eval/episode_reward': Array([1.1724222, 7.249641 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27652597, 0.11636844], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019414663314819, 'eval/sps': 31845.43291048208}
I0728 05:06:51.274078 140199540242240 train.py:379] starting iteration 60, 24576000 steps, 548.0222518444061
I0728 05:06:59.717451 140199540242240 train.py:394] {'eval/walltime': 259.75655150413513, 'training/sps': 92620.11626123493, 'training/walltime': 287.6429145336151, 'training/entropy_loss': Array(0.07360768, dtype=float32), 'training/policy_loss': Array(0.00539433, dtype=float32), 'training/total_loss': Array(232.29387, dtype=float32), 'training/v_loss': Array(232.21486, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29093266, 0.11647627], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.201527,  9.982668], dtype=float32), 'eval/episode_reward': Array([-0.3561503,  7.041001 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28553668, 0.11910851], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016016960144043, 'eval/sps': 31872.375358546546}
I0728 05:06:59.719418 140199540242240 train.py:379] starting iteration 61, 24985600 steps, 556.4675915241241
I0728 05:07:08.198630 140199540242240 train.py:394] {'eval/walltime': 263.78268337249756, 'training/sps': 92084.2687032989, 'training/walltime': 292.0910141468048, 'training/entropy_loss': Array(0.08957212, dtype=float32), 'training/policy_loss': Array(0.00515275, dtype=float32), 'training/total_loss': Array(88.25499, dtype=float32), 'training/v_loss': Array(88.16026, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27046055, 0.11027553], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.432964,  9.537707], dtype=float32), 'eval/episode_reward': Array([0.25457194, 6.333679  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2645381 , 0.11374696], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.026131868362427, 'eval/sps': 31792.301937706332}
I0728 05:07:08.200598 140199540242240 train.py:379] starting iteration 62, 25395200 steps, 564.9487721920013
I0728 05:07:16.682301 140199540242240 train.py:394] {'eval/walltime': 267.8337142467499, 'training/sps': 92546.29873396992, 'training/walltime': 296.51690697669983, 'training/entropy_loss': Array(0.10651974, dtype=float32), 'training/policy_loss': Array(0.00534769, dtype=float32), 'training/total_loss': Array(65.07011, dtype=float32), 'training/v_loss': Array(64.958244, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29029033, 0.12213312], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.134802, 10.512508], dtype=float32), 'eval/episode_reward': Array([-0.24499395,  7.769091  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28442833, 0.12581845], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.051030874252319, 'eval/sps': 31596.89569722778}
I0728 05:07:16.684255 140199540242240 train.py:379] starting iteration 63, 25804800 steps, 573.4324283599854
I0728 05:07:25.134490 140199540242240 train.py:394] {'eval/walltime': 271.85298895835876, 'training/sps': 92538.01875885966, 'training/walltime': 300.94319581985474, 'training/entropy_loss': Array(0.12163645, dtype=float32), 'training/policy_loss': Array(0.00548145, dtype=float32), 'training/total_loss': Array(57.249905, dtype=float32), 'training/v_loss': Array(57.122787, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26455733, 0.11922324], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.928188, 10.255568], dtype=float32), 'eval/episode_reward': Array([0.4040562, 7.528398 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25797004, 0.12320653], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019274711608887, 'eval/sps': 31846.541772896762}
I0728 05:07:25.136420 140199540242240 train.py:379] starting iteration 64, 26214400 steps, 581.8845944404602
I0728 05:07:33.602264 140199540242240 train.py:394] {'eval/walltime': 275.8740313053131, 'training/sps': 92251.01264245856, 'training/walltime': 305.38325548171997, 'training/entropy_loss': Array(0.14067382, dtype=float32), 'training/policy_loss': Array(0.00518425, dtype=float32), 'training/total_loss': Array(44.52053, dtype=float32), 'training/v_loss': Array(44.374672, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2818845 , 0.11533568], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.389942,  9.875591], dtype=float32), 'eval/episode_reward': Array([0.349914 , 7.1068134], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27649152, 0.11796674], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021042346954346, 'eval/sps': 31832.542150905454}
I0728 05:07:33.604170 140199540242240 train.py:379] starting iteration 65, 26624000 steps, 590.3523440361023
I0728 05:07:42.060549 140199540242240 train.py:394] {'eval/walltime': 279.8906855583191, 'training/sps': 92354.60494694381, 'training/walltime': 309.81833481788635, 'training/entropy_loss': Array(0.12614998, dtype=float32), 'training/policy_loss': Array(0.00602984, dtype=float32), 'training/total_loss': Array(232.3713, dtype=float32), 'training/v_loss': Array(232.23914, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27061862, 0.12087632], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.397625, 10.438494], dtype=float32), 'eval/episode_reward': Array([-0.542249 ,  6.3751483], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26408565, 0.12518495], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0166542530059814, 'eval/sps': 31867.3184041687}
I0728 05:07:42.062462 140199540242240 train.py:379] starting iteration 66, 27033600 steps, 598.8106355667114
I0728 05:07:50.518236 140199540242240 train.py:394] {'eval/walltime': 283.9078736305237, 'training/sps': 92379.17209134057, 'training/walltime': 314.2522346973419, 'training/entropy_loss': Array(0.15252209, dtype=float32), 'training/policy_loss': Array(0.00547328, dtype=float32), 'training/total_loss': Array(95.33966, dtype=float32), 'training/v_loss': Array(95.18167, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28351432, 0.12106767], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.519144, 10.423375], dtype=float32), 'eval/episode_reward': Array([-0.88220346,  7.420534  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27745357, 0.12499507], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.01718807220459, 'eval/sps': 31863.08375394408}
I0728 05:07:50.520201 140199540242240 train.py:379] starting iteration 67, 27443200 steps, 607.2683751583099
I0728 05:07:58.992494 140199540242240 train.py:394] {'eval/walltime': 287.9314947128296, 'training/sps': 92170.91785818693, 'training/walltime': 318.69615268707275, 'training/entropy_loss': Array(0.1787306, dtype=float32), 'training/policy_loss': Array(0.00605728, dtype=float32), 'training/total_loss': Array(77.037766, dtype=float32), 'training/v_loss': Array(76.85298, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29687274, 0.11768214], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.671711, 10.117044], dtype=float32), 'eval/episode_reward': Array([-0.44194847,  6.970056  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29129893, 0.12063811], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023621082305908, 'eval/sps': 31812.140701540444}
I0728 05:07:58.994450 140199540242240 train.py:379] starting iteration 68, 27852800 steps, 615.7426228523254
I0728 05:08:07.444900 140199540242240 train.py:394] {'eval/walltime': 291.95119976997375, 'training/sps': 92548.7266776645, 'training/walltime': 323.12192940711975, 'training/entropy_loss': Array(0.20716304, dtype=float32), 'training/policy_loss': Array(0.00692627, dtype=float32), 'training/total_loss': Array(61.831173, dtype=float32), 'training/v_loss': Array(61.617085, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25824887, 0.12061875], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.37297 , 10.381499], dtype=float32), 'eval/episode_reward': Array([1.7024288, 7.0713463], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25152427, 0.12445395], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019705057144165, 'eval/sps': 31843.13231452328}
I0728 05:08:07.446879 140199540242240 train.py:379] starting iteration 69, 28262400 steps, 624.1950526237488
I0728 05:08:15.922548 140199540242240 train.py:394] {'eval/walltime': 295.97397780418396, 'training/sps': 92081.58374669417, 'training/walltime': 327.5701587200165, 'training/entropy_loss': Array(0.24052885, dtype=float32), 'training/policy_loss': Array(0.00770309, dtype=float32), 'training/total_loss': Array(52.928654, dtype=float32), 'training/v_loss': Array(52.680428, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27960253, 0.11811561], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.152975, 10.192783], dtype=float32), 'eval/episode_reward': Array([0.3617625, 6.8994656], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2735712 , 0.12162226], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022778034210205, 'eval/sps': 31818.807528397556}
I0728 05:08:15.924514 140199540242240 train.py:379] starting iteration 70, 28672000 steps, 632.6726884841919
I0728 05:08:24.403563 140199540242240 train.py:394] {'eval/walltime': 300.0066750049591, 'training/sps': 92263.43303248315, 'training/walltime': 332.0096206665039, 'training/entropy_loss': Array(0.21009661, dtype=float32), 'training/policy_loss': Array(0.0090724, dtype=float32), 'training/total_loss': Array(253.96875, dtype=float32), 'training/v_loss': Array(253.74956, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29332134, 0.11981639], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.365011, 10.325894], dtype=float32), 'eval/episode_reward': Array([-0.81792283,  7.038332  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28774926, 0.12294193], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0326972007751465, 'eval/sps': 31740.543270989063}
I0728 05:08:24.407684 140199540242240 train.py:379] starting iteration 71, 29081600 steps, 641.1558430194855
I0728 05:08:32.865180 140199540242240 train.py:394] {'eval/walltime': 304.02383279800415, 'training/sps': 92357.21151349464, 'training/walltime': 336.44457483291626, 'training/entropy_loss': Array(0.26404983, dtype=float32), 'training/policy_loss': Array(0.00972268, dtype=float32), 'training/total_loss': Array(94.85387, dtype=float32), 'training/v_loss': Array(94.58009, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26700783, 0.11905338], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.101404, 10.226165], dtype=float32), 'eval/episode_reward': Array([0.03923237, 7.807008  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2609349 , 0.12221473], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.017157793045044, 'eval/sps': 31863.32392061074}
I0728 05:08:32.867208 140199540242240 train.py:379] starting iteration 72, 29491200 steps, 649.6153819561005
I0728 05:08:41.317736 140199540242240 train.py:394] {'eval/walltime': 308.0416433811188, 'training/sps': 92504.39565605097, 'training/walltime': 340.87247252464294, 'training/entropy_loss': Array(0.30940175, dtype=float32), 'training/policy_loss': Array(0.01232324, dtype=float32), 'training/total_loss': Array(63.943703, dtype=float32), 'training/v_loss': Array(63.621983, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28155118, 0.12575077], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.356724, 10.773272], dtype=float32), 'eval/episode_reward': Array([-0.4561673,  7.9968963], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.275486  , 0.12959571], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.017810583114624, 'eval/sps': 31858.14695643861}
I0728 05:08:41.319747 140199540242240 train.py:379] starting iteration 73, 29900800 steps, 658.0679206848145
I0728 05:08:49.801145 140199540242240 train.py:394] {'eval/walltime': 312.074431180954, 'training/sps': 92175.80874157387, 'training/walltime': 345.31615471839905, 'training/entropy_loss': Array(0.3535904, dtype=float32), 'training/policy_loss': Array(0.01415134, dtype=float32), 'training/total_loss': Array(57.686142, dtype=float32), 'training/v_loss': Array(57.318398, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27669346, 0.11796661], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.956928, 10.135076], dtype=float32), 'eval/episode_reward': Array([0.0704022, 7.275555 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2707274 , 0.12166731], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.032787799835205, 'eval/sps': 31739.83020014853}
I0728 05:08:49.803185 140199540242240 train.py:379] starting iteration 74, 30310400 steps, 666.5513589382172
I0728 05:08:58.255817 140199540242240 train.py:394] {'eval/walltime': 316.091055393219, 'training/sps': 92431.48347903778, 'training/walltime': 349.74754524230957, 'training/entropy_loss': Array(0.39131695, dtype=float32), 'training/policy_loss': Array(0.01611188, dtype=float32), 'training/total_loss': Array(45.583046, dtype=float32), 'training/v_loss': Array(45.175613, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27164942, 0.12360382], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.46534 , 10.600936], dtype=float32), 'eval/episode_reward': Array([-0.39884835,  7.2739353 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26526803, 0.1271251 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016624212265015, 'eval/sps': 31867.556743083893}
I0728 05:08:58.257822 140199540242240 train.py:379] starting iteration 75, 30720000 steps, 675.0059957504272
I0728 05:09:06.720297 140199540242240 train.py:394] {'eval/walltime': 320.11091685295105, 'training/sps': 92290.8717992135, 'training/walltime': 354.1856873035431, 'training/entropy_loss': Array(0.3003152, dtype=float32), 'training/policy_loss': Array(0.0212577, dtype=float32), 'training/total_loss': Array(274.39673, dtype=float32), 'training/v_loss': Array(274.07516, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27285546, 0.12368953], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.682518, 10.586832], dtype=float32), 'eval/episode_reward': Array([0.12443192, 8.736171  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2668987 , 0.12709317], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019861459732056, 'eval/sps': 31841.89337921408}
I0728 05:09:06.776203 140199540242240 train.py:379] starting iteration 76, 31129600 steps, 683.5243752002716
I0728 05:09:15.255466 140199540242240 train.py:394] {'eval/walltime': 324.1505355834961, 'training/sps': 92357.48459088575, 'training/walltime': 358.6206283569336, 'training/entropy_loss': Array(0.15482002, dtype=float32), 'training/policy_loss': Array(0.02100355, dtype=float32), 'training/total_loss': Array(199.63632, dtype=float32), 'training/v_loss': Array(199.46051, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25674403, 0.12679854], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.473446, 10.901673], dtype=float32), 'eval/episode_reward': Array([1.9852549, 9.024989 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25074068, 0.13053283], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.039618730545044, 'eval/sps': 31686.158654564326}
I0728 05:09:15.257494 140199540242240 train.py:379] starting iteration 77, 31539200 steps, 692.0056681632996
I0728 05:09:23.718173 140199540242240 train.py:394] {'eval/walltime': 328.1717882156372, 'training/sps': 92357.57892708706, 'training/walltime': 363.0555648803711, 'training/entropy_loss': Array(0.13915347, dtype=float32), 'training/policy_loss': Array(0.0102602, dtype=float32), 'training/total_loss': Array(200.35545, dtype=float32), 'training/v_loss': Array(200.20604, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.24110655, 0.12291399], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.14566, 10.58675], dtype=float32), 'eval/episode_reward': Array([3.0439265, 9.969516 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.23451543, 0.12711331], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021252632141113, 'eval/sps': 31830.877517348737}
I0728 05:09:23.720181 140199540242240 train.py:379] starting iteration 78, 31948800 steps, 700.4683547019958
I0728 05:09:32.178072 140199540242240 train.py:394] {'eval/walltime': 332.19253277778625, 'training/sps': 92412.94778730896, 'training/walltime': 367.4878442287445, 'training/entropy_loss': Array(0.14466485, dtype=float32), 'training/policy_loss': Array(0.01008755, dtype=float32), 'training/total_loss': Array(202.57706, dtype=float32), 'training/v_loss': Array(202.4223, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2428529 , 0.12373655], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.259136, 10.653066], dtype=float32), 'eval/episode_reward': Array([3.9239495, 8.569004 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.23552687, 0.12826028], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020744562149048, 'eval/sps': 31834.899735979565}
I0728 05:09:32.180021 140199540242240 train.py:379] starting iteration 79, 32358400 steps, 708.9281947612762
I0728 05:09:40.643608 140199540242240 train.py:394] {'eval/walltime': 336.20864939689636, 'training/sps': 92197.76724072467, 'training/walltime': 371.930468082428, 'training/entropy_loss': Array(0.1436024, dtype=float32), 'training/policy_loss': Array(0.00939939, dtype=float32), 'training/total_loss': Array(212.07681, dtype=float32), 'training/v_loss': Array(211.92383, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2613554 , 0.12044773], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.887999, 10.350641], dtype=float32), 'eval/episode_reward': Array([2.4660556, 8.155219 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25543493, 0.1237192 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016116619110107, 'eval/sps': 31871.584453232907}
I0728 05:09:40.645608 140199540242240 train.py:379] starting iteration 80, 32768000 steps, 717.3937819004059
I0728 05:09:49.100232 140199540242240 train.py:394] {'eval/walltime': 340.22922253608704, 'training/sps': 92475.27174103196, 'training/walltime': 376.3597602844238, 'training/entropy_loss': Array(0.13714863, dtype=float32), 'training/policy_loss': Array(0.01061747, dtype=float32), 'training/total_loss': Array(437.17398, dtype=float32), 'training/v_loss': Array(437.02625, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27636737, 0.12277199], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.206154, 10.49698 ], dtype=float32), 'eval/episode_reward': Array([0.77935195, 8.9283695 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27074176, 0.1258383 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020573139190674, 'eval/sps': 31836.257063033037}
I0728 05:09:49.102232 140199540242240 train.py:379] starting iteration 81, 33177600 steps, 725.8504066467285
I0728 05:09:57.575460 140199540242240 train.py:394] {'eval/walltime': 344.2539789676666, 'training/sps': 92180.51219380934, 'training/walltime': 380.8032157421112, 'training/entropy_loss': Array(0.131144, dtype=float32), 'training/policy_loss': Array(0.00853105, dtype=float32), 'training/total_loss': Array(339.6399, dtype=float32), 'training/v_loss': Array(339.50024, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27194852, 0.1310013 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.82114 , 11.243446], dtype=float32), 'eval/episode_reward': Array([0.4927235, 8.202359 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26577148, 0.13470829], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.02475643157959, 'eval/sps': 31803.16676946437}
I0728 05:09:57.577492 140199540242240 train.py:379] starting iteration 82, 33587200 steps, 734.325665473938
I0728 05:10:06.023831 140199540242240 train.py:394] {'eval/walltime': 348.26920771598816, 'training/sps': 92532.46637256774, 'training/walltime': 385.22977018356323, 'training/entropy_loss': Array(0.12047067, dtype=float32), 'training/policy_loss': Array(0.00835619, dtype=float32), 'training/total_loss': Array(355.60254, dtype=float32), 'training/v_loss': Array(355.47372, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.269157  , 0.11932541], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.534288, 10.25563 ], dtype=float32), 'eval/episode_reward': Array([0.93308735, 9.15469   ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2632751 , 0.12315071], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.015228748321533, 'eval/sps': 31878.6320837903}
I0728 05:10:06.025841 140199540242240 train.py:379] starting iteration 83, 33996800 steps, 742.7740151882172
I0728 05:10:14.489799 140199540242240 train.py:394] {'eval/walltime': 352.2892954349518, 'training/sps': 92266.5597125472, 'training/walltime': 389.66908168792725, 'training/entropy_loss': Array(0.10456248, dtype=float32), 'training/policy_loss': Array(0.00781317, dtype=float32), 'training/total_loss': Array(376.53577, dtype=float32), 'training/v_loss': Array(376.4234, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27476794, 0.13546447], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.017536, 11.643882], dtype=float32), 'eval/episode_reward': Array([0.06179261, 9.981541  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2685917 , 0.13916674], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020087718963623, 'eval/sps': 31840.101248586274}
I0728 05:10:14.491822 140199540242240 train.py:379] starting iteration 84, 34406400 steps, 751.2399966716766
I0728 05:10:22.956115 140199540242240 train.py:394] {'eval/walltime': 356.30933356285095, 'training/sps': 92262.40736901914, 'training/walltime': 394.10859298706055, 'training/entropy_loss': Array(0.08707045, dtype=float32), 'training/policy_loss': Array(0.00757573, dtype=float32), 'training/total_loss': Array(367.29156, dtype=float32), 'training/v_loss': Array(367.1969, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2991122 , 0.13410969], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.06932 , 11.507569], dtype=float32), 'eval/episode_reward': Array([-0.2835074,  9.48409  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29349622, 0.13720077], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.02003812789917, 'eval/sps': 31840.494027078163}
I0728 05:10:22.958107 140199540242240 train.py:379] starting iteration 85, 34816000 steps, 759.706280708313
I0728 05:10:31.418557 140199540242240 train.py:394] {'eval/walltime': 360.3246605396271, 'training/sps': 92238.31331671528, 'training/walltime': 398.5492639541626, 'training/entropy_loss': Array(0.07620502, dtype=float32), 'training/policy_loss': Array(0.00704847, dtype=float32), 'training/total_loss': Array(632.20264, dtype=float32), 'training/v_loss': Array(632.1194, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30717295, 0.14958555], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.774883, 12.887346], dtype=float32), 'eval/episode_reward': Array([-2.1210823, 10.759962 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3012885 , 0.15351479], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.015326976776123, 'eval/sps': 31877.852224819377}
I0728 05:10:31.420552 140199540242240 train.py:379] starting iteration 86, 35225600 steps, 768.1687254905701
I0728 05:10:39.874896 140199540242240 train.py:394] {'eval/walltime': 364.3403329849243, 'training/sps': 92372.00470337311, 'training/walltime': 402.9835078716278, 'training/entropy_loss': Array(0.06203624, dtype=float32), 'training/policy_loss': Array(0.00593147, dtype=float32), 'training/total_loss': Array(480.30554, dtype=float32), 'training/v_loss': Array(480.2375, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29712477, 0.15784296], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.82515 , 13.614083], dtype=float32), 'eval/episode_reward': Array([-2.9221587, 10.643698 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29076448, 0.16201921], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.015672445297241, 'eval/sps': 31875.109771440882}
I0728 05:10:39.876822 140199540242240 train.py:379] starting iteration 87, 35635200 steps, 776.624997138977
I0728 05:10:48.361189 140199540242240 train.py:394] {'eval/walltime': 368.36506485939026, 'training/sps': 91942.69317076799, 'training/walltime': 407.43845677375793, 'training/entropy_loss': Array(0.04715295, dtype=float32), 'training/policy_loss': Array(0.0056335, dtype=float32), 'training/total_loss': Array(398.3782, dtype=float32), 'training/v_loss': Array(398.3254, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29906723, 0.14103067], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.079374, 12.101574], dtype=float32), 'eval/episode_reward': Array([-1.6041124, 10.411647 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29376113, 0.1437307 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024731874465942, 'eval/sps': 31803.360818162535}
I0728 05:10:48.363154 140199540242240 train.py:379] starting iteration 88, 36044800 steps, 785.111328125
I0728 05:10:56.827667 140199540242240 train.py:394] {'eval/walltime': 372.3843867778778, 'training/sps': 92235.54013822766, 'training/walltime': 411.8792612552643, 'training/entropy_loss': Array(0.03767106, dtype=float32), 'training/policy_loss': Array(0.00499238, dtype=float32), 'training/total_loss': Array(336.12292, dtype=float32), 'training/v_loss': Array(336.08026, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2985754 , 0.14671004], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.022186, 12.637848], dtype=float32), 'eval/episode_reward': Array([-2.3030384, 10.531465 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29244912, 0.15063144], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019321918487549, 'eval/sps': 31846.167735717416}
I0728 05:10:56.829504 140199540242240 train.py:379] starting iteration 89, 36454400 steps, 793.5776779651642
I0728 05:11:05.296958 140199540242240 train.py:394] {'eval/walltime': 376.40895676612854, 'training/sps': 92285.15569612705, 'training/walltime': 416.3176782131195, 'training/entropy_loss': Array(0.02950805, dtype=float32), 'training/policy_loss': Array(0.00438911, dtype=float32), 'training/total_loss': Array(281.60718, dtype=float32), 'training/v_loss': Array(281.5733, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2676133, 0.1393602], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.32507 , 11.997851], dtype=float32), 'eval/episode_reward': Array([0.53409225, 9.604245  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2603733 , 0.14366682], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024569988250732, 'eval/sps': 31804.6400916573}
I0728 05:11:05.298817 140199540242240 train.py:379] starting iteration 90, 36864000 steps, 802.0469915866852
I0728 05:11:13.763312 140199540242240 train.py:394] {'eval/walltime': 380.42857694625854, 'training/sps': 92247.48580194067, 'training/walltime': 420.75790762901306, 'training/entropy_loss': Array(0.0252192, dtype=float32), 'training/policy_loss': Array(0.0046742, dtype=float32), 'training/total_loss': Array(654.5675, dtype=float32), 'training/v_loss': Array(654.53766, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28784156, 0.13895808], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.047266, 11.913183], dtype=float32), 'eval/episode_reward': Array([-1.4556942,  9.719737 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28173923, 0.14250365], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019620180130005, 'eval/sps': 31843.804703921094}
I0728 05:11:13.765140 140199540242240 train.py:379] starting iteration 91, 37273600 steps, 810.5133140087128
I0728 05:11:22.235893 140199540242240 train.py:394] {'eval/walltime': 384.45190143585205, 'training/sps': 92198.68261139999, 'training/walltime': 425.2004873752594, 'training/entropy_loss': Array(0.01424575, dtype=float32), 'training/policy_loss': Array(0.00443468, dtype=float32), 'training/total_loss': Array(451.81952, dtype=float32), 'training/v_loss': Array(451.80084, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25950605, 0.11800358], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.609863, 10.184277], dtype=float32), 'eval/episode_reward': Array([0.66924465, 8.2245455 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2525816 , 0.12202334], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023324489593506, 'eval/sps': 31814.485839031193}
I0728 05:11:22.237792 140199540242240 train.py:379] starting iteration 92, 37683200 steps, 818.9859662055969
I0728 05:11:30.689607 140199540242240 train.py:394] {'eval/walltime': 388.4706394672394, 'training/sps': 92487.16506861303, 'training/walltime': 429.6292099952698, 'training/entropy_loss': Array(0.00739973, dtype=float32), 'training/policy_loss': Array(0.00517559, dtype=float32), 'training/total_loss': Array(361.33374, dtype=float32), 'training/v_loss': Array(361.32117, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28110975, 0.13811646], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.495682, 11.919312], dtype=float32), 'eval/episode_reward': Array([ 0.39624298, 10.260036  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27414942, 0.14299756], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.018738031387329, 'eval/sps': 31850.79470228928}
I0728 05:11:30.691533 140199540242240 train.py:379] starting iteration 93, 38092800 steps, 827.4397065639496
I0728 05:11:39.207994 140199540242240 train.py:394] {'eval/walltime': 392.50601959228516, 'training/sps': 91504.98718760636, 'training/walltime': 434.10546875, 'training/entropy_loss': Array(0.00427759, dtype=float32), 'training/policy_loss': Array(0.00365055, dtype=float32), 'training/total_loss': Array(285.8512, dtype=float32), 'training/v_loss': Array(285.84326, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30188465, 0.13636126], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.242575, 11.746844], dtype=float32), 'eval/episode_reward': Array([-2.6569848,  9.410888 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29630506, 0.13944429], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.035380125045776, 'eval/sps': 31719.44055668064}
I0728 05:11:39.209924 140199540242240 train.py:379] starting iteration 94, 38502400 steps, 835.9580979347229
I0728 05:11:47.656994 140199540242240 train.py:394] {'eval/walltime': 396.52184557914734, 'training/sps': 92533.22393000642, 'training/walltime': 438.531986951828, 'training/entropy_loss': Array(0.00033214, dtype=float32), 'training/policy_loss': Array(0.00274786, dtype=float32), 'training/total_loss': Array(215.4163, dtype=float32), 'training/v_loss': Array(215.41324, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2621358 , 0.12606852], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.7753  , 10.871332], dtype=float32), 'eval/episode_reward': Array([1.4004745, 9.169134 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25527683, 0.13029891], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.015825986862183, 'eval/sps': 31873.8910547303}
I0728 05:11:47.658962 140199540242240 train.py:379] starting iteration 95, 38912000 steps, 844.4071359634399
I0728 05:11:56.149437 140199540242240 train.py:394] {'eval/walltime': 400.5469374656677, 'training/sps': 91827.16597507136, 'training/walltime': 442.99254059791565, 'training/entropy_loss': Array(0.00040704, dtype=float32), 'training/policy_loss': Array(0.00358946, dtype=float32), 'training/total_loss': Array(602.84863, dtype=float32), 'training/v_loss': Array(602.84467, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27119887, 0.11157857], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.605038,  9.648282], dtype=float32), 'eval/episode_reward': Array([1.6943792, 9.685105 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26484585, 0.11538117], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.025091886520386, 'eval/sps': 31800.516263655667}
I0728 05:11:56.151388 140199540242240 train.py:379] starting iteration 96, 39321600 steps, 852.8995621204376
I0728 05:12:04.608964 140199540242240 train.py:394] {'eval/walltime': 404.5640273094177, 'training/sps': 92339.82725548463, 'training/walltime': 447.428329706192, 'training/entropy_loss': Array(-0.00711536, dtype=float32), 'training/policy_loss': Array(0.00298017, dtype=float32), 'training/total_loss': Array(376.4711, dtype=float32), 'training/v_loss': Array(376.47522, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25286216, 0.12722732], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.04335 , 10.892272], dtype=float32), 'eval/episode_reward': Array([1.2823627, 8.858286 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24573717, 0.1311799 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.01708984375, 'eval/sps': 31863.862890482556}
I0728 05:12:04.610908 140199540242240 train.py:379] starting iteration 97, 39731200 steps, 861.3590822219849
I0728 05:12:13.081663 140199540242240 train.py:394] {'eval/walltime': 408.5891819000244, 'training/sps': 92233.1087921424, 'training/walltime': 451.8692512512207, 'training/entropy_loss': Array(-0.01020953, dtype=float32), 'training/policy_loss': Array(0.00330144, dtype=float32), 'training/total_loss': Array(323.75027, dtype=float32), 'training/v_loss': Array(323.7572, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27964967, 0.1269108 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.360256, 10.9117  ], dtype=float32), 'eval/episode_reward': Array([-0.20218445,  9.068419  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27353406, 0.13047285], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0251545906066895, 'eval/sps': 31800.02087341129}
I0728 05:12:13.083611 140199540242240 train.py:379] starting iteration 98, 40140800 steps, 869.8317856788635
I0728 05:12:21.565221 140199540242240 train.py:394] {'eval/walltime': 412.6111648082733, 'training/sps': 91936.47892270604, 'training/walltime': 456.32450127601624, 'training/entropy_loss': Array(-0.0123557, dtype=float32), 'training/policy_loss': Array(0.00301469, dtype=float32), 'training/total_loss': Array(272.9856, dtype=float32), 'training/v_loss': Array(272.99493, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28146175, 0.11969624], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.486477, 10.317431], dtype=float32), 'eval/episode_reward': Array([-0.01364431,  8.685362  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27545798, 0.12325366], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021982908248901, 'eval/sps': 31825.09794794948}
I0728 05:12:21.567167 140199540242240 train.py:379] starting iteration 99, 40550400 steps, 878.3153409957886
I0728 05:12:30.032694 140199540242240 train.py:394] {'eval/walltime': 416.63165259361267, 'training/sps': 92241.21542847552, 'training/walltime': 460.76503252983093, 'training/entropy_loss': Array(-0.01457443, dtype=float32), 'training/policy_loss': Array(0.00249949, dtype=float32), 'training/total_loss': Array(230.64922, dtype=float32), 'training/v_loss': Array(230.6613, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2695225 , 0.12495879], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.411789, 10.821639], dtype=float32), 'eval/episode_reward': Array([1.3768134, 8.871738 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26287878, 0.12956357], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0204877853393555, 'eval/sps': 31836.93293802557}
I0728 05:12:30.034628 140199540242240 train.py:379] starting iteration 100, 40960000 steps, 886.7828025817871
I0728 05:12:38.511988 140199540242240 train.py:394] {'eval/walltime': 420.65584802627563, 'training/sps': 92075.96262587853, 'training/walltime': 465.21353340148926, 'training/entropy_loss': Array(-0.01288246, dtype=float32), 'training/policy_loss': Array(0.00288342, dtype=float32), 'training/total_loss': Array(620.04785, dtype=float32), 'training/v_loss': Array(620.0578, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27039132, 0.13489382], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.520813, 11.57014 ], dtype=float32), 'eval/episode_reward': Array([1.4715863, 9.15292  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26389825, 0.13854477], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024195432662964, 'eval/sps': 31807.6003369691}
I0728 05:12:38.589496 140199540242240 train.py:379] starting iteration 101, 41369600 steps, 895.3376519680023
I0728 05:12:47.064688 140199540242240 train.py:394] {'eval/walltime': 424.676322221756, 'training/sps': 92059.29576888932, 'training/walltime': 469.6628396511078, 'training/entropy_loss': Array(-0.01775839, dtype=float32), 'training/policy_loss': Array(0.00273565, dtype=float32), 'training/total_loss': Array(376.27112, dtype=float32), 'training/v_loss': Array(376.28613, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27044055, 0.13075843], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.546482, 11.273154], dtype=float32), 'eval/episode_reward': Array([-0.13824254,  9.291008  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2634897 , 0.13509695], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020474195480347, 'eval/sps': 31837.040552055372}
I0728 05:12:47.066779 140199540242240 train.py:379] starting iteration 102, 41779200 steps, 903.8149530887604
I0728 05:12:55.532026 140199540242240 train.py:394] {'eval/walltime': 428.692355632782, 'training/sps': 92157.70171747215, 'training/walltime': 474.10739493370056, 'training/entropy_loss': Array(-0.0186397, dtype=float32), 'training/policy_loss': Array(0.0023187, dtype=float32), 'training/total_loss': Array(323.86203, dtype=float32), 'training/v_loss': Array(323.87836, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27297068, 0.11897276], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.692524, 10.305247], dtype=float32), 'eval/episode_reward': Array([0.45761177, 8.387009  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2667023 , 0.12243223], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016033411026001, 'eval/sps': 31872.244799701268}
I0728 05:12:55.533954 140199540242240 train.py:379] starting iteration 103, 42188800 steps, 912.2821276187897
I0728 05:13:04.001008 140199540242240 train.py:394] {'eval/walltime': 432.70942091941833, 'training/sps': 92143.99515055011, 'training/walltime': 478.5526113510132, 'training/entropy_loss': Array(-0.01850672, dtype=float32), 'training/policy_loss': Array(0.00285971, dtype=float32), 'training/total_loss': Array(266.0722, dtype=float32), 'training/v_loss': Array(266.08786, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26470083, 0.11658641], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.015503, 10.025432], dtype=float32), 'eval/episode_reward': Array([1.2285502, 8.629574 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25780255, 0.12112766], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0170652866363525, 'eval/sps': 31864.057680570946}
I0728 05:13:04.002990 140199540242240 train.py:379] starting iteration 104, 42598400 steps, 920.7511630058289
I0728 05:13:12.461530 140199540242240 train.py:394] {'eval/walltime': 436.7260534763336, 'training/sps': 92341.27155759143, 'training/walltime': 482.98833107948303, 'training/entropy_loss': Array(-0.0180705, dtype=float32), 'training/policy_loss': Array(0.00292408, dtype=float32), 'training/total_loss': Array(222.58841, dtype=float32), 'training/v_loss': Array(222.60355, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27241385, 0.12379709], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.708927, 10.638837], dtype=float32), 'eval/episode_reward': Array([1.383028, 9.124167], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26587802, 0.12761353], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016632556915283, 'eval/sps': 31867.490537472062}
I0728 05:13:12.465597 140199540242240 train.py:379] starting iteration 105, 43008000 steps, 929.2137560844421
I0728 05:13:20.959813 140199540242240 train.py:394] {'eval/walltime': 440.75969433784485, 'training/sps': 91934.34865730413, 'training/walltime': 487.4436843395233, 'training/entropy_loss': Array(-0.01589236, dtype=float32), 'training/policy_loss': Array(0.00270117, dtype=float32), 'training/total_loss': Array(595.3273, dtype=float32), 'training/v_loss': Array(595.34045, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26077878, 0.11426739], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.696404,  9.852544], dtype=float32), 'eval/episode_reward': Array([1.9937253, 9.03781  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25454772, 0.11781988], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0336408615112305, 'eval/sps': 31733.117645987935}
I0728 05:13:20.961823 140199540242240 train.py:379] starting iteration 106, 43417600 steps, 937.7099969387054
I0728 05:13:29.422789 140199540242240 train.py:394] {'eval/walltime': 444.77943539619446, 'training/sps': 92321.75496501903, 'training/walltime': 491.88034176826477, 'training/entropy_loss': Array(-0.02025577, dtype=float32), 'training/policy_loss': Array(0.00249849, dtype=float32), 'training/total_loss': Array(342.36774, dtype=float32), 'training/v_loss': Array(342.3855, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23639448, 0.11325021], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.5932  ,  9.787471], dtype=float32), 'eval/episode_reward': Array([3.1529608, 8.476222 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22887106, 0.11724789], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019741058349609, 'eval/sps': 31842.847124225744}
I0728 05:13:29.424791 140199540242240 train.py:379] starting iteration 107, 43827200 steps, 946.1729648113251
I0728 05:13:37.875902 140199540242240 train.py:394] {'eval/walltime': 448.7954490184784, 'training/sps': 92449.92213267989, 'training/walltime': 496.31084847450256, 'training/entropy_loss': Array(-0.02034808, dtype=float32), 'training/policy_loss': Array(0.00280812, dtype=float32), 'training/total_loss': Array(303.69647, dtype=float32), 'training/v_loss': Array(303.714, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25552356, 0.13026963], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.241772, 11.161484], dtype=float32), 'eval/episode_reward': Array([1.3774328, 9.580235 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.248454  , 0.13434394], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0160136222839355, 'eval/sps': 31872.401848877565}
I0728 05:13:37.877917 140199540242240 train.py:379] starting iteration 108, 44236800 steps, 954.626091003418
I0728 05:13:46.374514 140199540242240 train.py:394] {'eval/walltime': 452.82733964920044, 'training/sps': 91836.68396698366, 'training/walltime': 500.77093982696533, 'training/entropy_loss': Array(-0.02084201, dtype=float32), 'training/policy_loss': Array(0.00277034, dtype=float32), 'training/total_loss': Array(262.64478, dtype=float32), 'training/v_loss': Array(262.66284, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25366938, 0.12292645], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.06551 , 10.590974], dtype=float32), 'eval/episode_reward': Array([2.084038, 8.591489], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2463902 , 0.12729742], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.031890630722046, 'eval/sps': 31746.89289056367}
I0728 05:13:46.376566 140199540242240 train.py:379] starting iteration 109, 44646400 steps, 963.1247406005859
I0728 05:13:54.855825 140199540242240 train.py:394] {'eval/walltime': 456.8440811634064, 'training/sps': 91884.08730915989, 'training/walltime': 505.2287302017212, 'training/entropy_loss': Array(-0.02024729, dtype=float32), 'training/policy_loss': Array(0.00427605, dtype=float32), 'training/total_loss': Array(243.81833, dtype=float32), 'training/v_loss': Array(243.83429, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26674628, 0.11474803], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.160784,  9.860152], dtype=float32), 'eval/episode_reward': Array([1.5940955, 8.022632 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25996333, 0.11881006], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016741514205933, 'eval/sps': 31866.626106585365}
I0728 05:13:54.857856 140199540242240 train.py:379] starting iteration 110, 45056000 steps, 971.6060304641724
I0728 05:14:03.357913 140199540242240 train.py:394] {'eval/walltime': 460.8736720085144, 'training/sps': 91723.22005411604, 'training/walltime': 509.69433879852295, 'training/entropy_loss': Array(-0.0167919, dtype=float32), 'training/policy_loss': Array(0.00337949, dtype=float32), 'training/total_loss': Array(625.334, dtype=float32), 'training/v_loss': Array(625.3474, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23289356, 0.12274045], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.364681, 10.496143], dtype=float32), 'eval/episode_reward': Array([3.6105943, 8.342346 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22503737, 0.12743273], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.029590845108032, 'eval/sps': 31765.01161535877}
I0728 05:14:03.359846 140199540242240 train.py:379] starting iteration 111, 45465600 steps, 980.1080198287964
I0728 05:14:11.829610 140199540242240 train.py:394] {'eval/walltime': 464.9039087295532, 'training/sps': 92358.81524564272, 'training/walltime': 514.1292159557343, 'training/entropy_loss': Array(-0.0190178, dtype=float32), 'training/policy_loss': Array(0.00247154, dtype=float32), 'training/total_loss': Array(386.55313, dtype=float32), 'training/v_loss': Array(386.56967, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26899505, 0.11753524], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.37624 , 10.113558], dtype=float32), 'eval/episode_reward': Array([0.5996203, 8.056953 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26261637, 0.12119251], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.030236721038818, 'eval/sps': 31759.92103188599}
I0728 05:14:11.831648 140199540242240 train.py:379] starting iteration 112, 45875200 steps, 988.5798223018646
I0728 05:14:20.303247 140199540242240 train.py:394] {'eval/walltime': 468.9217231273651, 'training/sps': 92064.26855105118, 'training/walltime': 518.578281879425, 'training/entropy_loss': Array(-0.01740975, dtype=float32), 'training/policy_loss': Array(0.00274178, dtype=float32), 'training/total_loss': Array(318.0468, dtype=float32), 'training/v_loss': Array(318.06146, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26987255, 0.12211537], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.443466, 10.496986], dtype=float32), 'eval/episode_reward': Array([0.9207761, 8.815462 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26353756, 0.1256123 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.01781439781189, 'eval/sps': 31858.11670885272}
I0728 05:14:20.305295 140199540242240 train.py:379] starting iteration 113, 46284800 steps, 997.0534691810608
I0728 05:14:28.785824 140199540242240 train.py:394] {'eval/walltime': 472.94865226745605, 'training/sps': 92065.6697096978, 'training/walltime': 523.0272800922394, 'training/entropy_loss': Array(-0.01582361, dtype=float32), 'training/policy_loss': Array(0.00427923, dtype=float32), 'training/total_loss': Array(271.8637, dtype=float32), 'training/v_loss': Array(271.87524, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27390298, 0.12280069], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.868357, 10.563226], dtype=float32), 'eval/episode_reward': Array([0.46006083, 9.118868  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26812142, 0.1257611 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.026929140090942, 'eval/sps': 31786.007537522575}
I0728 05:14:28.787846 140199540242240 train.py:379] starting iteration 114, 46694400 steps, 1005.5360198020935
I0728 05:14:37.264160 140199540242240 train.py:394] {'eval/walltime': 476.96512722969055, 'training/sps': 91939.3227102827, 'training/walltime': 527.4823923110962, 'training/entropy_loss': Array(-0.01311552, dtype=float32), 'training/policy_loss': Array(0.00382819, dtype=float32), 'training/total_loss': Array(234.16745, dtype=float32), 'training/v_loss': Array(234.17673, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.264368  , 0.11997586], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.95568 , 10.317109], dtype=float32), 'eval/episode_reward': Array([1.6791155, 9.430945 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25786442, 0.12383266], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016474962234497, 'eval/sps': 31868.740924203194}
I0728 05:14:37.266171 140199540242240 train.py:379] starting iteration 115, 47104000 steps, 1014.0143444538116
I0728 05:14:45.763790 140199540242240 train.py:394] {'eval/walltime': 481.0046229362488, 'training/sps': 91982.85263806474, 'training/walltime': 531.935396194458, 'training/entropy_loss': Array(-0.00817203, dtype=float32), 'training/policy_loss': Array(0.00404203, dtype=float32), 'training/total_loss': Array(612.6172, dtype=float32), 'training/v_loss': Array(612.6213, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2555222 , 0.11204691], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.182846,  9.714412], dtype=float32), 'eval/episode_reward': Array([2.5847628, 7.850569 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24907978, 0.1157176 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0394957065582275, 'eval/sps': 31687.123665508207}
I0728 05:14:45.765770 140199540242240 train.py:379] starting iteration 116, 47513600 steps, 1022.5139439105988
I0728 05:14:54.236138 140199540242240 train.py:394] {'eval/walltime': 485.021137714386, 'training/sps': 92057.71722022373, 'training/walltime': 536.3847787380219, 'training/entropy_loss': Array(-0.00743936, dtype=float32), 'training/policy_loss': Array(0.00397864, dtype=float32), 'training/total_loss': Array(344.91446, dtype=float32), 'training/v_loss': Array(344.9179, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23747261, 0.1163885 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.674622, 10.022304], dtype=float32), 'eval/episode_reward': Array([2.6457515, 8.633973 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.23034903, 0.11991605], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016514778137207, 'eval/sps': 31868.42500785327}
I0728 05:14:54.238149 140199540242240 train.py:379] starting iteration 117, 47923200 steps, 1030.9863221645355
I0728 05:15:02.737330 140199540242240 train.py:394] {'eval/walltime': 489.0504925251007, 'training/sps': 91729.78753179799, 'training/walltime': 540.850067615509, 'training/entropy_loss': Array(-0.00485134, dtype=float32), 'training/policy_loss': Array(0.00526051, dtype=float32), 'training/total_loss': Array(298.0177, dtype=float32), 'training/v_loss': Array(298.0173, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28759736, 0.11231557], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.986572,  9.669684], dtype=float32), 'eval/episode_reward': Array([0.61093867, 7.2415047 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28201705, 0.1152321 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.029354810714722, 'eval/sps': 31766.872368654855}
I0728 05:15:02.739319 140199540242240 train.py:379] starting iteration 118, 48332800 steps, 1039.487492799759
I0728 05:15:11.215744 140199540242240 train.py:394] {'eval/walltime': 493.0702769756317, 'training/sps': 92002.20648461979, 'training/walltime': 545.3021347522736, 'training/entropy_loss': Array(-0.00138782, dtype=float32), 'training/policy_loss': Array(0.00546104, dtype=float32), 'training/total_loss': Array(269.14468, dtype=float32), 'training/v_loss': Array(269.14062, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25474003, 0.112983  ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.190495,  9.797958], dtype=float32), 'eval/episode_reward': Array([1.419901, 8.95504 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24823244, 0.11625078], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019784450531006, 'eval/sps': 31842.503391715803}
I0728 05:15:11.217740 140199540242240 train.py:379] starting iteration 119, 48742400 steps, 1047.9659142494202
I0728 05:15:19.705846 140199540242240 train.py:394] {'eval/walltime': 497.1033320426941, 'training/sps': 92039.29166921813, 'training/walltime': 549.7524080276489, 'training/entropy_loss': Array(0.00240947, dtype=float32), 'training/policy_loss': Array(0.00753773, dtype=float32), 'training/total_loss': Array(236.97124, dtype=float32), 'training/v_loss': Array(236.96127, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2725576 , 0.12477638], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.709572, 10.780213], dtype=float32), 'eval/episode_reward': Array([0.6842122, 8.209579 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26608068, 0.1287245 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.033055067062378, 'eval/sps': 31737.726827824707}
I0728 05:15:19.707828 140199540242240 train.py:379] starting iteration 120, 49152000 steps, 1056.4560015201569
I0728 05:15:28.180476 140199540242240 train.py:394] {'eval/walltime': 501.12083101272583, 'training/sps': 92038.5914859583, 'training/walltime': 554.2027151584625, 'training/entropy_loss': Array(0.0083076, dtype=float32), 'training/policy_loss': Array(0.00853076, dtype=float32), 'training/total_loss': Array(617.05884, dtype=float32), 'training/v_loss': Array(617.042, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2898178 , 0.12582758], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.235004, 10.838297], dtype=float32), 'eval/episode_reward': Array([0.20359497, 8.409073  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28387725, 0.12981062], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.017498970031738, 'eval/sps': 31860.61800010587}
I0728 05:15:28.182513 140199540242240 train.py:379] starting iteration 121, 49561600 steps, 1064.9306864738464
I0728 05:15:36.673123 140199540242240 train.py:394] {'eval/walltime': 505.1457507610321, 'training/sps': 91821.12437479787, 'training/walltime': 558.663562297821, 'training/entropy_loss': Array(0.01232867, dtype=float32), 'training/policy_loss': Array(0.0107645, dtype=float32), 'training/total_loss': Array(335.79236, dtype=float32), 'training/v_loss': Array(335.76926, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2723918 , 0.12423413], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.782444, 10.761142], dtype=float32), 'eval/episode_reward': Array([0.06790945, 7.863149  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.266211  , 0.12766609], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024919748306274, 'eval/sps': 31801.87631166153}
I0728 05:15:36.675135 140199540242240 train.py:379] starting iteration 122, 49971200 steps, 1073.4233086109161
I0728 05:15:45.175653 140199540242240 train.py:394] {'eval/walltime': 509.1542708873749, 'training/sps': 91274.31702861264, 'training/walltime': 563.1511335372925, 'training/entropy_loss': Array(0.01764338, dtype=float32), 'training/policy_loss': Array(0.01349655, dtype=float32), 'training/total_loss': Array(297.55994, dtype=float32), 'training/v_loss': Array(297.52878, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30994296, 0.13288958], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.081781, 11.397682], dtype=float32), 'eval/episode_reward': Array([-2.558568,  8.543427], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30462754, 0.13598198], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.008520126342773, 'eval/sps': 31931.983865772054}
I0728 05:15:45.177675 140199540242240 train.py:379] starting iteration 123, 50380800 steps, 1081.9258487224579
I0728 05:15:53.657697 140199540242240 train.py:394] {'eval/walltime': 513.1799204349518, 'training/sps': 92072.69094617362, 'training/walltime': 567.5997924804688, 'training/entropy_loss': Array(0.02255158, dtype=float32), 'training/policy_loss': Array(0.01748535, dtype=float32), 'training/total_loss': Array(280.11255, dtype=float32), 'training/v_loss': Array(280.0725, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.293512  , 0.12774415], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.61959 , 11.081181], dtype=float32), 'eval/episode_reward': Array([-2.3652673,  8.973398 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28695762, 0.13273312], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.025649547576904, 'eval/sps': 31796.111034316193}
I0728 05:15:53.661800 140199540242240 train.py:379] starting iteration 124, 50790400 steps, 1090.4099588394165
I0728 05:16:02.120371 140199540242240 train.py:394] {'eval/walltime': 517.1957550048828, 'training/sps': 92299.51422578438, 'training/walltime': 572.0375189781189, 'training/entropy_loss': Array(0.02767665, dtype=float32), 'training/policy_loss': Array(0.02331236, dtype=float32), 'training/total_loss': Array(265.85977, dtype=float32), 'training/v_loss': Array(265.80878, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2839065 , 0.11294511], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.880806,  9.742974], dtype=float32), 'eval/episode_reward': Array([-1.0312312,  7.933824 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27800477, 0.11588115], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.01583456993103, 'eval/sps': 31873.82293045959}
I0728 05:16:02.122392 140199540242240 train.py:379] starting iteration 125, 51200000 steps, 1098.8705627918243
I0728 05:16:10.607958 140199540242240 train.py:394] {'eval/walltime': 521.219911813736, 'training/sps': 91904.03874250138, 'training/walltime': 576.4943416118622, 'training/entropy_loss': Array(0.03355246, dtype=float32), 'training/policy_loss': Array(0.0260895, dtype=float32), 'training/total_loss': Array(679.89185, dtype=float32), 'training/v_loss': Array(679.8322, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3055211 , 0.13371634], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.689291, 11.509376], dtype=float32), 'eval/episode_reward': Array([-2.7030764,  8.9929285], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2999023 , 0.13654174], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024156808853149, 'eval/sps': 31807.905625943764}
I0728 05:16:10.670775 140199540242240 train.py:379] starting iteration 126, 51609600 steps, 1107.4189472198486
I0728 05:16:19.154932 140199540242240 train.py:394] {'eval/walltime': 525.2503809928894, 'training/sps': 92071.62510978473, 'training/walltime': 580.9430520534515, 'training/entropy_loss': Array(0.03839486, dtype=float32), 'training/policy_loss': Array(0.03309725, dtype=float32), 'training/total_loss': Array(458.76794, dtype=float32), 'training/v_loss': Array(458.69644, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32637295, 0.15181544], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.521418, 13.109911], dtype=float32), 'eval/episode_reward': Array([-5.3616023, 10.29977  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32100356, 0.15554696], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.030469179153442, 'eval/sps': 31758.089272099347}
I0728 05:16:19.157039 140199540242240 train.py:379] starting iteration 127, 52019200 steps, 1115.905212879181
I0728 05:16:27.650634 140199540242240 train.py:394] {'eval/walltime': 529.2736661434174, 'training/sps': 91725.90374195417, 'training/walltime': 585.408529996872, 'training/entropy_loss': Array(0.04462406, dtype=float32), 'training/policy_loss': Array(0.03313505, dtype=float32), 'training/total_loss': Array(402.38956, dtype=float32), 'training/v_loss': Array(402.3118, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30545038, 0.14796105], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.733059, 12.766078], dtype=float32), 'eval/episode_reward': Array([-2.581253,  9.864162], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29942048, 0.15165918], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023285150527954, 'eval/sps': 31814.796916197512}
I0728 05:16:27.652598 140199540242240 train.py:379] starting iteration 128, 52428800 steps, 1124.4007720947266
I0728 05:16:36.143084 140199540242240 train.py:394] {'eval/walltime': 533.3130283355713, 'training/sps': 92114.32227229212, 'training/walltime': 589.8551783561707, 'training/entropy_loss': Array(0.04967697, dtype=float32), 'training/policy_loss': Array(0.03493597, dtype=float32), 'training/total_loss': Array(355.6424, dtype=float32), 'training/v_loss': Array(355.5578, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3077523 , 0.13957708], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.909151, 12.061205], dtype=float32), 'eval/episode_reward': Array([-3.2746043,  9.267512 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30146998, 0.14369151], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.039362192153931, 'eval/sps': 31688.17103072054}
I0728 05:16:36.145890 140199540242240 train.py:379] starting iteration 129, 52838400 steps, 1132.8940570354462
I0728 05:16:44.623577 140199540242240 train.py:394] {'eval/walltime': 537.3327016830444, 'training/sps': 92031.69870113785, 'training/walltime': 594.3058187961578, 'training/entropy_loss': Array(0.05483089, dtype=float32), 'training/policy_loss': Array(0.03756651, dtype=float32), 'training/total_loss': Array(315.06543, dtype=float32), 'training/v_loss': Array(314.97302, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30691025, 0.11398138], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.848093,  9.824188], dtype=float32), 'eval/episode_reward': Array([-2.4261339,  9.045867 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3011831, 0.1175684], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0196733474731445, 'eval/sps': 31843.383512857738}
I0728 05:16:44.625537 140199540242240 train.py:379] starting iteration 130, 53248000 steps, 1141.3737106323242
I0728 05:16:53.123897 140199540242240 train.py:394] {'eval/walltime': 541.3593103885651, 'training/sps': 91701.40377876713, 'training/walltime': 598.7724897861481, 'training/entropy_loss': Array(0.0609356, dtype=float32), 'training/policy_loss': Array(0.03830102, dtype=float32), 'training/total_loss': Array(850.64856, dtype=float32), 'training/v_loss': Array(850.5494, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31691134, 0.13549218], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.72097 , 11.646321], dtype=float32), 'eval/episode_reward': Array([-4.2253704,  9.242676 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3113677 , 0.13866875], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.02660870552063, 'eval/sps': 31788.53704471141}
I0728 05:16:53.125841 140199540242240 train.py:379] starting iteration 131, 53657600 steps, 1149.8740150928497
I0728 05:17:01.583073 140199540242240 train.py:394] {'eval/walltime': 545.3761994838715, 'training/sps': 92350.8765713073, 'training/walltime': 603.2077481746674, 'training/entropy_loss': Array(0.06511327, dtype=float32), 'training/policy_loss': Array(0.03913353, dtype=float32), 'training/total_loss': Array(522.338, dtype=float32), 'training/v_loss': Array(522.2338, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33529872, 0.15411724], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.282156, 13.28778 ], dtype=float32), 'eval/episode_reward': Array([-5.444528, 10.800796], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3297808 , 0.15774643], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0168890953063965, 'eval/sps': 31865.455322021167}
I0728 05:17:01.584982 140199540242240 train.py:379] starting iteration 132, 54067200 steps, 1158.333155632019
I0728 05:17:10.049602 140199540242240 train.py:394] {'eval/walltime': 549.3939180374146, 'training/sps': 92208.89146302865, 'training/walltime': 607.649836063385, 'training/entropy_loss': Array(0.06956124, dtype=float32), 'training/policy_loss': Array(0.03645567, dtype=float32), 'training/total_loss': Array(400.85156, dtype=float32), 'training/v_loss': Array(400.74554, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32168362, 0.12447482], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.130276, 10.737994], dtype=float32), 'eval/episode_reward': Array([-3.0434232,  9.774754 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31638187, 0.1275573 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.017718553543091, 'eval/sps': 31858.87669685601}
I0728 05:17:10.051662 140199540242240 train.py:379] starting iteration 133, 54476800 steps, 1166.799836397171
I0728 05:17:18.546026 140199540242240 train.py:394] {'eval/walltime': 553.4290840625763, 'training/sps': 91953.39171436896, 'training/walltime': 612.1042666435242, 'training/entropy_loss': Array(0.07392299, dtype=float32), 'training/policy_loss': Array(0.03648281, dtype=float32), 'training/total_loss': Array(319.82092, dtype=float32), 'training/v_loss': Array(319.7105, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.336995  , 0.13741621], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.462019, 11.827083], dtype=float32), 'eval/episode_reward': Array([-5.593793, 10.369905], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.33167282, 0.1405776 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.035166025161743, 'eval/sps': 31721.123542833488}
I0728 05:17:18.548046 140199540242240 train.py:379] starting iteration 134, 54886400 steps, 1175.2962203025818
I0728 05:17:27.015525 140199540242240 train.py:394] {'eval/walltime': 557.4522461891174, 'training/sps': 92264.42403333618, 'training/walltime': 616.5436809062958, 'training/entropy_loss': Array(0.07808207, dtype=float32), 'training/policy_loss': Array(0.03744384, dtype=float32), 'training/total_loss': Array(255.5813, dtype=float32), 'training/v_loss': Array(255.46579, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3143341 , 0.14349557], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.474016, 12.387946], dtype=float32), 'eval/episode_reward': Array([-4.1699076,  9.609847 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30829215, 0.14721401], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023162126541138, 'eval/sps': 31815.769778596114}
I0728 05:17:27.017525 140199540242240 train.py:379] starting iteration 135, 55296000 steps, 1183.7656993865967
I0728 05:17:35.501239 140199540242240 train.py:394] {'eval/walltime': 561.4734718799591, 'training/sps': 91884.58856944875, 'training/walltime': 621.0014469623566, 'training/entropy_loss': Array(0.08537586, dtype=float32), 'training/policy_loss': Array(0.03678051, dtype=float32), 'training/total_loss': Array(918.5165, dtype=float32), 'training/v_loss': Array(918.3943, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29067782, 0.11216313], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.458641,  9.696069], dtype=float32), 'eval/episode_reward': Array([-2.1878667,  8.225129 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28434867, 0.11589081], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021225690841675, 'eval/sps': 31831.09077700351}
I0728 05:17:35.503230 140199540242240 train.py:379] starting iteration 136, 55705600 steps, 1192.2514040470123
I0728 05:17:43.971509 140199540242240 train.py:394] {'eval/walltime': 565.4925475120544, 'training/sps': 92163.44654001946, 'training/walltime': 625.4457252025604, 'training/entropy_loss': Array(0.08889078, dtype=float32), 'training/policy_loss': Array(0.03421253, dtype=float32), 'training/total_loss': Array(569.3148, dtype=float32), 'training/v_loss': Array(569.1917, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3311574 , 0.14238788], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.931751, 12.268433], dtype=float32), 'eval/episode_reward': Array([-4.3375697,  8.56533  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3257517 , 0.14562958], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019075632095337, 'eval/sps': 31848.11924857146}
I0728 05:17:43.973577 140199540242240 train.py:379] starting iteration 137, 56115200 steps, 1200.7217509746552
I0728 05:17:52.433910 140199540242240 train.py:394] {'eval/walltime': 569.5122609138489, 'training/sps': 92361.83418666534, 'training/walltime': 629.8804574012756, 'training/entropy_loss': Array(0.09325212, dtype=float32), 'training/policy_loss': Array(0.03443896, dtype=float32), 'training/total_loss': Array(401.7384, dtype=float32), 'training/v_loss': Array(401.6107, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31450337, 0.13134362], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.549557, 11.314163], dtype=float32), 'eval/episode_reward': Array([-3.7122843,  8.4086075], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30882275, 0.13448012], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019713401794434, 'eval/sps': 31843.06621035712}
I0728 05:17:52.435937 140199540242240 train.py:379] starting iteration 138, 56524800 steps, 1209.1841106414795
I0728 05:18:00.917227 140199540242240 train.py:394] {'eval/walltime': 573.5412268638611, 'training/sps': 92094.87186332289, 'training/walltime': 634.3280448913574, 'training/entropy_loss': Array(0.09835098, dtype=float32), 'training/policy_loss': Array(0.03274877, dtype=float32), 'training/total_loss': Array(312.60242, dtype=float32), 'training/v_loss': Array(312.4713, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31794032, 0.13421929], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.819422, 11.594325], dtype=float32), 'eval/episode_reward': Array([-2.5388203,  9.859294 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3120838 , 0.13799207], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.028965950012207, 'eval/sps': 31769.938388189203}
I0728 05:18:00.919234 140199540242240 train.py:379] starting iteration 139, 56934400 steps, 1217.667408466339
I0728 05:18:09.409796 140199540242240 train.py:394] {'eval/walltime': 577.5631413459778, 'training/sps': 91756.94406672008, 'training/walltime': 638.7920122146606, 'training/entropy_loss': Array(0.10065867, dtype=float32), 'training/policy_loss': Array(0.03315036, dtype=float32), 'training/total_loss': Array(249.99985, dtype=float32), 'training/v_loss': Array(249.86603, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31417245, 0.1200546 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.509228, 10.392792], dtype=float32), 'eval/episode_reward': Array([-2.084817,  9.003698], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3083073 , 0.12385793], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021914482116699, 'eval/sps': 31825.639398636515}
I0728 05:18:09.411819 140199540242240 train.py:379] starting iteration 140, 57344000 steps, 1226.1599929332733
I0728 05:18:17.925363 140199540242240 train.py:394] {'eval/walltime': 581.597797870636, 'training/sps': 91539.71659876531, 'training/walltime': 643.2665727138519, 'training/entropy_loss': Array(0.10460888, dtype=float32), 'training/policy_loss': Array(0.03400404, dtype=float32), 'training/total_loss': Array(950.6002, dtype=float32), 'training/v_loss': Array(950.4617, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28492916, 0.12316562], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.964607, 10.632758], dtype=float32), 'eval/episode_reward': Array([-2.677947, 10.007555], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27832353, 0.12678118], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.034656524658203, 'eval/sps': 31725.129318373278}
I0728 05:18:17.927424 140199540242240 train.py:379] starting iteration 141, 57753600 steps, 1234.6755974292755
I0728 05:18:26.412493 140199540242240 train.py:394] {'eval/walltime': 585.6171231269836, 'training/sps': 91822.79788226582, 'training/walltime': 647.727338552475, 'training/entropy_loss': Array(0.10854715, dtype=float32), 'training/policy_loss': Array(0.03285942, dtype=float32), 'training/total_loss': Array(543.9531, dtype=float32), 'training/v_loss': Array(543.81165, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28015578, 0.12414209], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.538095, 10.714708], dtype=float32), 'eval/episode_reward': Array([-0.42443955,  9.190983  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27359593, 0.12761912], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019325256347656, 'eval/sps': 31846.141288976713}
I0728 05:18:26.414437 140199540242240 train.py:379] starting iteration 142, 58163200 steps, 1243.1626105308533
I0728 05:18:34.901577 140199540242240 train.py:394] {'eval/walltime': 589.639547586441, 'training/sps': 91828.40776720305, 'training/walltime': 652.1878318786621, 'training/entropy_loss': Array(0.11342373, dtype=float32), 'training/policy_loss': Array(0.03579616, dtype=float32), 'training/total_loss': Array(372.84406, dtype=float32), 'training/v_loss': Array(372.69482, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.317841  , 0.14344105], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.804615, 12.328594], dtype=float32), 'eval/episode_reward': Array([-2.517726, 10.548779], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31185395, 0.14705002], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0224244594573975, 'eval/sps': 31821.604430395317}
I0728 05:18:34.903468 140199540242240 train.py:379] starting iteration 143, 58572800 steps, 1251.6516423225403
I0728 05:18:43.368929 140199540242240 train.py:394] {'eval/walltime': 593.6529266834259, 'training/sps': 92093.65741113001, 'training/walltime': 656.6354780197144, 'training/entropy_loss': Array(0.1166303, dtype=float32), 'training/policy_loss': Array(0.03603284, dtype=float32), 'training/total_loss': Array(286.445, dtype=float32), 'training/v_loss': Array(286.29236, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31142205, 0.13008767], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.222294, 11.258607], dtype=float32), 'eval/episode_reward': Array([-3.18362 , 10.394112], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30525932, 0.13421333], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.013379096984863, 'eval/sps': 31893.324031154378}
I0728 05:18:43.370879 140199540242240 train.py:379] starting iteration 144, 58982400 steps, 1260.119052886963
I0728 05:18:51.844152 140199540242240 train.py:394] {'eval/walltime': 597.6762111186981, 'training/sps': 92137.1013940589, 'training/walltime': 661.0810270309448, 'training/entropy_loss': Array(0.11892775, dtype=float32), 'training/policy_loss': Array(0.03249258, dtype=float32), 'training/total_loss': Array(247.07379, dtype=float32), 'training/v_loss': Array(246.92236, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31336725, 0.12382402], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.419832, 10.738044], dtype=float32), 'eval/episode_reward': Array([-1.9725957,  8.283148 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30770737, 0.12690659], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023284435272217, 'eval/sps': 31814.8025722023}
I0728 05:18:51.846064 140199540242240 train.py:379] starting iteration 145, 59392000 steps, 1268.5942380428314
I0728 05:19:00.343527 140199540242240 train.py:394] {'eval/walltime': 601.7154984474182, 'training/sps': 91973.49143186524, 'training/walltime': 665.5344841480255, 'training/entropy_loss': Array(0.12071911, dtype=float32), 'training/policy_loss': Array(0.03431962, dtype=float32), 'training/total_loss': Array(967.6712, dtype=float32), 'training/v_loss': Array(967.51624, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3192584 , 0.14027439], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.949688, 12.113009], dtype=float32), 'eval/episode_reward': Array([-3.6796877,  9.390696 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3131647 , 0.14429669], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.039287328720093, 'eval/sps': 31688.758333653543}
I0728 05:19:00.345439 140199540242240 train.py:379] starting iteration 146, 59801600 steps, 1277.093612909317
I0728 05:19:08.819676 140199540242240 train.py:394] {'eval/walltime': 605.7358856201172, 'training/sps': 92061.23943129728, 'training/walltime': 669.9836964607239, 'training/entropy_loss': Array(0.12367865, dtype=float32), 'training/policy_loss': Array(0.03183945, dtype=float32), 'training/total_loss': Array(544.59595, dtype=float32), 'training/v_loss': Array(544.4403, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30955404, 0.12731381], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.13203 , 11.027421], dtype=float32), 'eval/episode_reward': Array([-2.617844,  8.79795 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30361927, 0.13080172], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020387172698975, 'eval/sps': 31837.72967668454}
I0728 05:19:08.821576 140199540242240 train.py:379] starting iteration 147, 60211200 steps, 1285.569750070572
I0728 05:19:17.307601 140199540242240 train.py:394] {'eval/walltime': 609.7597546577454, 'training/sps': 91901.61009316315, 'training/walltime': 674.4406368732452, 'training/entropy_loss': Array(0.12595233, dtype=float32), 'training/policy_loss': Array(0.03452345, dtype=float32), 'training/total_loss': Array(372.96478, dtype=float32), 'training/v_loss': Array(372.8043, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2991919 , 0.12866178], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.216515, 11.118044], dtype=float32), 'eval/episode_reward': Array([-1.7620549,  9.605498 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29308194, 0.13227564], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023869037628174, 'eval/sps': 31810.18040175786}
I0728 05:19:17.309602 140199540242240 train.py:379] starting iteration 148, 60620800 steps, 1294.0577754974365
I0728 05:19:25.767589 140199540242240 train.py:394] {'eval/walltime': 613.774861574173, 'training/sps': 92294.0647964065, 'training/walltime': 678.8786253929138, 'training/entropy_loss': Array(0.12821648, dtype=float32), 'training/policy_loss': Array(0.03067052, dtype=float32), 'training/total_loss': Array(287.27795, dtype=float32), 'training/v_loss': Array(287.11905, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31458426, 0.1451957 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.528233, 12.589991], dtype=float32), 'eval/episode_reward': Array([-2.9132643,  9.677727 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30832493, 0.14914013], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.015106916427612, 'eval/sps': 31879.599389070885}
I0728 05:19:25.769562 140199540242240 train.py:379] starting iteration 149, 61030400 steps, 1302.517736196518
I0728 05:19:34.243356 140199540242240 train.py:394] {'eval/walltime': 617.793506860733, 'training/sps': 92036.02752551755, 'training/walltime': 683.3290565013885, 'training/entropy_loss': Array(0.13112202, dtype=float32), 'training/policy_loss': Array(0.03025895, dtype=float32), 'training/total_loss': Array(249.31818, dtype=float32), 'training/v_loss': Array(249.15677, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31992027, 0.14457217], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.98682  , 12.5034275], dtype=float32), 'eval/episode_reward': Array([-2.783224,  9.029141], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31388116, 0.1484294 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.018645286560059, 'eval/sps': 31851.529774992257}
I0728 05:19:34.245352 140199540242240 train.py:379] starting iteration 150, 61440000 steps, 1310.993525981903
I0728 05:19:42.734251 140199540242240 train.py:394] {'eval/walltime': 621.8325529098511, 'training/sps': 92142.00845685578, 'training/walltime': 687.77436876297, 'training/entropy_loss': Array(0.13188154, dtype=float32), 'training/policy_loss': Array(0.02938173, dtype=float32), 'training/total_loss': Array(945.597, dtype=float32), 'training/v_loss': Array(945.4358, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31101242, 0.15031354], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.193459, 12.985858], dtype=float32), 'eval/episode_reward': Array([-3.406391, 10.470649], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30471936, 0.15412734], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.039046049118042, 'eval/sps': 31690.651318013526}
I0728 05:19:42.804292 140199540242240 train.py:379] starting iteration 151, 61849600 steps, 1319.5524644851685
I0728 05:19:51.302936 140199540242240 train.py:394] {'eval/walltime': 625.862913608551, 'training/sps': 91769.35921690548, 'training/walltime': 692.2377321720123, 'training/entropy_loss': Array(0.13168584, dtype=float32), 'training/policy_loss': Array(0.03468619, dtype=float32), 'training/total_loss': Array(546.5884, dtype=float32), 'training/v_loss': Array(546.42206, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31302324, 0.13711703], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.378246, 11.81689 ], dtype=float32), 'eval/episode_reward': Array([-4.6905656, 10.712483 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30697614, 0.14080498], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.030360698699951, 'eval/sps': 31758.944067038014}
I0728 05:19:51.305051 140199540242240 train.py:379] starting iteration 152, 62259200 steps, 1328.05322432518
I0728 05:19:59.768858 140199540242240 train.py:394] {'eval/walltime': 629.8795983791351, 'training/sps': 92197.74250122896, 'training/walltime': 696.6803572177887, 'training/entropy_loss': Array(0.13188633, dtype=float32), 'training/policy_loss': Array(0.03046341, dtype=float32), 'training/total_loss': Array(355.99042, dtype=float32), 'training/v_loss': Array(355.82806, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3210016 , 0.12543899], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.112625, 10.795657], dtype=float32), 'eval/episode_reward': Array([-3.1460009,  9.292984 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31565788, 0.12813665], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0166847705841064, 'eval/sps': 31867.076285746523}
I0728 05:19:59.770824 140199540242240 train.py:379] starting iteration 153, 62668800 steps, 1336.5189979076385
I0728 05:20:08.265085 140199540242240 train.py:394] {'eval/walltime': 633.9007937908173, 'training/sps': 91665.12832415341, 'training/walltime': 701.1487958431244, 'training/entropy_loss': Array(0.13386397, dtype=float32), 'training/policy_loss': Array(0.0278713, dtype=float32), 'training/total_loss': Array(283.3863, dtype=float32), 'training/v_loss': Array(283.22455, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32499236, 0.1352879 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.456871, 11.691358], dtype=float32), 'eval/episode_reward': Array([-2.723114,  8.597697], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3192653 , 0.13887241], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021195411682129, 'eval/sps': 31831.330461619025}
I0728 05:20:08.267079 140199540242240 train.py:379] starting iteration 154, 63078400 steps, 1345.0152533054352
I0728 05:20:16.741504 140199540242240 train.py:394] {'eval/walltime': 637.9137761592865, 'training/sps': 91904.96795605647, 'training/walltime': 705.6055734157562, 'training/entropy_loss': Array(0.13491437, dtype=float32), 'training/policy_loss': Array(0.03132236, dtype=float32), 'training/total_loss': Array(238.84232, dtype=float32), 'training/v_loss': Array(238.67609, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31086564, 0.13576038], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.229197, 11.64401 ], dtype=float32), 'eval/episode_reward': Array([-3.2335286, 10.008001 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30511558, 0.13879219], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.012982368469238, 'eval/sps': 31896.47704553108}
I0728 05:20:16.743478 140199540242240 train.py:379] starting iteration 155, 63488000 steps, 1353.4916517734528
I0728 05:20:25.220987 140199540242240 train.py:394] {'eval/walltime': 641.9387490749359, 'training/sps': 92086.82054430801, 'training/walltime': 710.0535497665405, 'training/entropy_loss': Array(0.13772205, dtype=float32), 'training/policy_loss': Array(0.03067467, dtype=float32), 'training/total_loss': Array(962.1705, dtype=float32), 'training/v_loss': Array(962.0021, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3079301 , 0.12947191], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.929752, 11.170453], dtype=float32), 'eval/episode_reward': Array([-2.308919,  9.578917], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30175233, 0.13342008], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024972915649414, 'eval/sps': 31801.45622901606}
I0728 05:20:25.222999 140199540242240 train.py:379] starting iteration 156, 63897600 steps, 1361.9711730480194
I0728 05:20:33.708127 140199540242240 train.py:394] {'eval/walltime': 645.9697785377502, 'training/sps': 92060.07026412438, 'training/walltime': 714.5028185844421, 'training/entropy_loss': Array(0.13954943, dtype=float32), 'training/policy_loss': Array(0.03744007, dtype=float32), 'training/total_loss': Array(529.66235, dtype=float32), 'training/v_loss': Array(529.4854, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2978912, 0.1297379], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.067856, 11.234874], dtype=float32), 'eval/episode_reward': Array([-3.141458,  9.893773], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29156867, 0.13352314], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.031029462814331, 'eval/sps': 31753.67512958703}
I0728 05:20:33.710115 140199540242240 train.py:379] starting iteration 157, 64307200 steps, 1370.4582891464233
I0728 05:20:42.194504 140199540242240 train.py:394] {'eval/walltime': 649.9966175556183, 'training/sps': 91989.50660533058, 'training/walltime': 718.9555003643036, 'training/entropy_loss': Array(0.14004022, dtype=float32), 'training/policy_loss': Array(0.03129719, dtype=float32), 'training/total_loss': Array(340.34753, dtype=float32), 'training/v_loss': Array(340.17618, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32106537, 0.14150666], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.134087, 12.231588], dtype=float32), 'eval/episode_reward': Array([-4.247226,  8.87227 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31529087, 0.14482793], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.026839017868042, 'eval/sps': 31786.718920730025}
I0728 05:20:42.196500 140199540242240 train.py:379] starting iteration 158, 64716800 steps, 1378.9446740150452
I0728 05:20:50.674433 140199540242240 train.py:394] {'eval/walltime': 654.0187726020813, 'training/sps': 92020.17851836314, 'training/walltime': 723.4066979885101, 'training/entropy_loss': Array(0.14152896, dtype=float32), 'training/policy_loss': Array(0.02822348, dtype=float32), 'training/total_loss': Array(259.17828, dtype=float32), 'training/v_loss': Array(259.0085, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31678712, 0.13588922], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.670507, 11.723422], dtype=float32), 'eval/episode_reward': Array([-3.070017,  9.741689], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31031954, 0.14049016], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022155046463013, 'eval/sps': 31823.735913054905}
I0728 05:20:50.676407 140199540242240 train.py:379] starting iteration 159, 65126400 steps, 1387.4245810508728
I0728 05:20:59.145406 140199540242240 train.py:394] {'eval/walltime': 658.0366728305817, 'training/sps': 92122.87238514904, 'training/walltime': 727.8529336452484, 'training/entropy_loss': Array(0.14461386, dtype=float32), 'training/policy_loss': Array(0.02882147, dtype=float32), 'training/total_loss': Array(228.59265, dtype=float32), 'training/v_loss': Array(228.41922, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3067558 , 0.13350366], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.904594, 11.587367], dtype=float32), 'eval/episode_reward': Array([-2.91588 ,  8.977478], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30034488, 0.13788448], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.017900228500366, 'eval/sps': 31857.43615335478}
I0728 05:20:59.147476 140199540242240 train.py:379] starting iteration 160, 65536000 steps, 1395.8956496715546
I0728 05:21:07.595012 140199540242240 train.py:394] {'eval/walltime': 662.0549547672272, 'training/sps': 92579.03933157209, 'training/walltime': 732.2772612571716, 'training/entropy_loss': Array(0.14699376, dtype=float32), 'training/policy_loss': Array(0.02967269, dtype=float32), 'training/total_loss': Array(968.5091, dtype=float32), 'training/v_loss': Array(968.33234, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3079285 , 0.11891854], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.968376, 10.268075], dtype=float32), 'eval/episode_reward': Array([-2.3214748,  8.15641  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30231628, 0.12189905], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.018281936645508, 'eval/sps': 31854.409923972475}
I0728 05:21:07.596985 140199540242240 train.py:379] starting iteration 161, 65945600 steps, 1404.345158815384
I0728 05:21:16.104671 140199540242240 train.py:394] {'eval/walltime': 666.0897541046143, 'training/sps': 91673.00334358215, 'training/walltime': 736.745316028595, 'training/entropy_loss': Array(0.14865597, dtype=float32), 'training/policy_loss': Array(0.02952623, dtype=float32), 'training/total_loss': Array(526.11816, dtype=float32), 'training/v_loss': Array(525.94, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3041756 , 0.12157138], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.634405, 10.509909], dtype=float32), 'eval/episode_reward': Array([-1.4081448,  8.980083 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29818544, 0.12514071], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.034799337387085, 'eval/sps': 31724.006399508366}
I0728 05:21:16.106703 140199540242240 train.py:379] starting iteration 162, 66355200 steps, 1412.8548765182495
I0728 05:21:24.593969 140199540242240 train.py:394] {'eval/walltime': 670.1092846393585, 'training/sps': 91776.26669039976, 'training/walltime': 741.2083435058594, 'training/entropy_loss': Array(0.14791554, dtype=float32), 'training/policy_loss': Array(0.03041452, dtype=float32), 'training/total_loss': Array(327.28015, dtype=float32), 'training/v_loss': Array(327.1018, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31069994, 0.12471151], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.235668, 10.814916], dtype=float32), 'eval/episode_reward': Array([-2.9741764, 10.618917 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30492234, 0.12796989], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019530534744263, 'eval/sps': 31844.51489883851}
I0728 05:21:24.595977 140199540242240 train.py:379] starting iteration 163, 66764800 steps, 1421.3441512584686
I0728 05:21:33.082379 140199540242240 train.py:394] {'eval/walltime': 674.1401393413544, 'training/sps': 92023.96896615319, 'training/walltime': 745.6593577861786, 'training/entropy_loss': Array(0.14761192, dtype=float32), 'training/policy_loss': Array(0.02983348, dtype=float32), 'training/total_loss': Array(262.3794, dtype=float32), 'training/v_loss': Array(262.20193, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29772866, 0.13327499], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.12371 , 11.528501], dtype=float32), 'eval/episode_reward': Array([-2.2336583,  9.347159 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29155627, 0.1367255 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.03085470199585, 'eval/sps': 31755.051834694437}
I0728 05:21:33.084353 140199540242240 train.py:379] starting iteration 164, 67174400 steps, 1429.832526922226
I0728 05:21:41.541954 140199540242240 train.py:394] {'eval/walltime': 678.1578674316406, 'training/sps': 92346.96483101936, 'training/walltime': 750.0948040485382, 'training/entropy_loss': Array(0.1490832, dtype=float32), 'training/policy_loss': Array(0.02586048, dtype=float32), 'training/total_loss': Array(238.66246, dtype=float32), 'training/v_loss': Array(238.48752, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31511986, 0.13382167], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.58462 , 11.577057], dtype=float32), 'eval/episode_reward': Array([-3.0441685,  7.9936347], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30929494, 0.1370947 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.017728090286255, 'eval/sps': 31858.801074534702}
I0728 05:21:41.543949 140199540242240 train.py:379] starting iteration 165, 67584000 steps, 1438.2921221256256
I0728 05:21:50.018128 140199540242240 train.py:394] {'eval/walltime': 682.1783175468445, 'training/sps': 92065.41809003853, 'training/walltime': 754.5438144207001, 'training/entropy_loss': Array(0.1493209, dtype=float32), 'training/policy_loss': Array(0.02805287, dtype=float32), 'training/total_loss': Array(969.81067, dtype=float32), 'training/v_loss': Array(969.63324, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32604966, 0.13542141], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.534729, 11.694006], dtype=float32), 'eval/episode_reward': Array([-3.116251,  9.357425], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32045448, 0.13895454], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020450115203857, 'eval/sps': 31837.231238350967}
I0728 05:21:50.020044 140199540242240 train.py:379] starting iteration 166, 67993600 steps, 1446.7682178020477
I0728 05:21:58.515592 140199540242240 train.py:394] {'eval/walltime': 686.1939630508423, 'training/sps': 91521.84884492207, 'training/walltime': 759.0192484855652, 'training/entropy_loss': Array(0.14815168, dtype=float32), 'training/policy_loss': Array(0.02764688, dtype=float32), 'training/total_loss': Array(532.07434, dtype=float32), 'training/v_loss': Array(531.89856, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32495698, 0.12043361], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.448603, 10.407956], dtype=float32), 'eval/episode_reward': Array([-3.1208067,  8.592942 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.319421  , 0.12413956], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.015645503997803, 'eval/sps': 31875.323624201574}
I0728 05:21:58.517502 140199540242240 train.py:379] starting iteration 167, 68403200 steps, 1455.2656757831573
I0728 05:22:06.979819 140199540242240 train.py:394] {'eval/walltime': 690.2108500003815, 'training/sps': 92238.52131182431, 'training/walltime': 763.4599094390869, 'training/entropy_loss': Array(0.14440463, dtype=float32), 'training/policy_loss': Array(0.02644044, dtype=float32), 'training/total_loss': Array(331.8258, dtype=float32), 'training/v_loss': Array(331.65494, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31732354, 0.1386414 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.76894 , 11.960628], dtype=float32), 'eval/episode_reward': Array([-2.9549408,  9.256521 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31156564, 0.14195886], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016886949539185, 'eval/sps': 31865.472344120637}
I0728 05:22:06.981737 140199540242240 train.py:379] starting iteration 168, 68812800 steps, 1463.7299106121063
I0728 05:22:15.476030 140199540242240 train.py:394] {'eval/walltime': 694.2395539283752, 'training/sps': 91886.90819807546, 'training/walltime': 767.9175629615784, 'training/entropy_loss': Array(0.14560065, dtype=float32), 'training/policy_loss': Array(0.02519821, dtype=float32), 'training/total_loss': Array(266.90982, dtype=float32), 'training/v_loss': Array(266.739, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2976238 , 0.13586037], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.046862, 11.769347], dtype=float32), 'eval/episode_reward': Array([-1.4730664,  9.042759 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29099604, 0.14024153], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.028703927993774, 'eval/sps': 31772.004666459023}
I0728 05:22:15.477939 140199540242240 train.py:379] starting iteration 169, 69222400 steps, 1472.2261130809784
I0728 05:22:23.938869 140199540242240 train.py:394] {'eval/walltime': 698.2574894428253, 'training/sps': 92291.83364157501, 'training/walltime': 772.3556587696075, 'training/entropy_loss': Array(0.14709425, dtype=float32), 'training/policy_loss': Array(0.02613571, dtype=float32), 'training/total_loss': Array(241.09613, dtype=float32), 'training/v_loss': Array(240.92291, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30300042, 0.12629919], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.545723, 10.880572], dtype=float32), 'eval/episode_reward': Array([-3.5008185, 10.006374 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29694968, 0.12982967], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.017935514450073, 'eval/sps': 31857.156377861655}
I0728 05:22:23.940887 140199540242240 train.py:379] starting iteration 170, 69632000 steps, 1480.6890606880188
I0728 05:22:32.412165 140199540242240 train.py:394] {'eval/walltime': 702.277027130127, 'training/sps': 92110.0799111962, 'training/walltime': 776.8025119304657, 'training/entropy_loss': Array(0.14790732, dtype=float32), 'training/policy_loss': Array(0.02347003, dtype=float32), 'training/total_loss': Array(959.27325, dtype=float32), 'training/v_loss': Array(959.10187, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31035876, 0.12575877], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.176271, 10.894312], dtype=float32), 'eval/episode_reward': Array([-4.0373993,  8.689992 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30425942, 0.12963642], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019537687301636, 'eval/sps': 31844.458233187495}
I0728 05:22:32.414145 140199540242240 train.py:379] starting iteration 171, 70041600 steps, 1489.162318944931
I0728 05:22:40.903661 140199540242240 train.py:394] {'eval/walltime': 706.310094833374, 'training/sps': 92039.61711013438, 'training/walltime': 781.2527694702148, 'training/entropy_loss': Array(0.14901902, dtype=float32), 'training/policy_loss': Array(0.02665359, dtype=float32), 'training/total_loss': Array(529.667, dtype=float32), 'training/v_loss': Array(529.49133, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33302838, 0.12724003], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.13945, 10.89435], dtype=float32), 'eval/episode_reward': Array([-4.1299105,  9.798318 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32813638, 0.12938587], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.03306770324707, 'eval/sps': 31737.627388934157}
I0728 05:22:40.905598 140199540242240 train.py:379] starting iteration 172, 70451200 steps, 1497.6537716388702
I0728 05:22:49.379927 140199540242240 train.py:394] {'eval/walltime': 710.332138299942, 'training/sps': 92102.47032179014, 'training/walltime': 785.6999900341034, 'training/entropy_loss': Array(0.14781548, dtype=float32), 'training/policy_loss': Array(0.02255058, dtype=float32), 'training/total_loss': Array(352.91058, dtype=float32), 'training/v_loss': Array(352.74023, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31015536, 0.1408012 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.151424, 12.154085], dtype=float32), 'eval/episode_reward': Array([-3.566514,  9.924357], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30430534, 0.14401215], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022043466567993, 'eval/sps': 31824.61877002595}
I0728 05:22:49.381880 140199540242240 train.py:379] starting iteration 173, 70860800 steps, 1506.1300535202026
I0728 05:22:57.866556 140199540242240 train.py:394] {'eval/walltime': 714.3568868637085, 'training/sps': 91942.03382036937, 'training/walltime': 790.1549708843231, 'training/entropy_loss': Array(0.14723441, dtype=float32), 'training/policy_loss': Array(0.02304749, dtype=float32), 'training/total_loss': Array(275.86218, dtype=float32), 'training/v_loss': Array(275.6919, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30958176, 0.1299503 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.100695, 11.228353], dtype=float32), 'eval/episode_reward': Array([-3.6866286,  9.713638 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3036127 , 0.13336663], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0247485637664795, 'eval/sps': 31803.228940148696}
I0728 05:22:57.868498 140199540242240 train.py:379] starting iteration 174, 71270400 steps, 1514.6166715621948
I0728 05:23:06.321193 140199540242240 train.py:394] {'eval/walltime': 718.3735406398773, 'training/sps': 92430.67288428184, 'training/walltime': 794.586400270462, 'training/entropy_loss': Array(0.14887878, dtype=float32), 'training/policy_loss': Array(0.02551545, dtype=float32), 'training/total_loss': Array(235.91176, dtype=float32), 'training/v_loss': Array(235.73735, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3027801 , 0.13587247], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.497223, 11.686187], dtype=float32), 'eval/episode_reward': Array([-1.7049012,  9.163747 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29662505, 0.13936324], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016653776168823, 'eval/sps': 31867.32218729824}
I0728 05:23:06.323127 140199540242240 train.py:379] starting iteration 175, 71680000 steps, 1523.0713005065918
I0728 05:23:14.801500 140199540242240 train.py:394] {'eval/walltime': 722.3973858356476, 'training/sps': 92055.06833988833, 'training/walltime': 799.0359108448029, 'training/entropy_loss': Array(0.14890023, dtype=float32), 'training/policy_loss': Array(0.02692518, dtype=float32), 'training/total_loss': Array(1006.3977, dtype=float32), 'training/v_loss': Array(1006.2218, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30085853, 0.13054416], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.37413 , 11.291879], dtype=float32), 'eval/episode_reward': Array([-2.0144873,  9.483083 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2945625 , 0.13431294], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023845195770264, 'eval/sps': 31810.368881623348}
I0728 05:23:14.878598 140199540242240 train.py:379] starting iteration 176, 72089600 steps, 1531.626769065857
I0728 05:23:23.348966 140199540242240 train.py:394] {'eval/walltime': 726.4157252311707, 'training/sps': 92139.78464762513, 'training/walltime': 803.4813303947449, 'training/entropy_loss': Array(0.14618728, dtype=float32), 'training/policy_loss': Array(0.0270389, dtype=float32), 'training/total_loss': Array(553.85925, dtype=float32), 'training/v_loss': Array(553.68604, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.323546  , 0.13279489], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.28275 , 11.438364], dtype=float32), 'eval/episode_reward': Array([-4.1789865,  9.448401 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31785998, 0.13639535], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.018339395523071, 'eval/sps': 31853.954432671337}
I0728 05:23:23.353236 140199540242240 train.py:379] starting iteration 177, 72499200 steps, 1540.1013946533203
I0728 05:23:31.841549 140199540242240 train.py:394] {'eval/walltime': 730.4430384635925, 'training/sps': 91929.80310926298, 'training/walltime': 807.9369039535522, 'training/entropy_loss': Array(0.14518118, dtype=float32), 'training/policy_loss': Array(0.02457942, dtype=float32), 'training/total_loss': Array(359.05978, dtype=float32), 'training/v_loss': Array(358.89, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3080748 , 0.13330705], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.963005, 11.545657], dtype=float32), 'eval/episode_reward': Array([-2.3709185,  9.774505 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30206203, 0.13684379], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.027313232421875, 'eval/sps': 31782.976047042062}
I0728 05:23:31.843563 140199540242240 train.py:379] starting iteration 178, 72908800 steps, 1548.591736793518
I0728 05:23:40.302034 140199540242240 train.py:394] {'eval/walltime': 734.456648349762, 'training/sps': 92251.76559839955, 'training/walltime': 812.3769273757935, 'training/entropy_loss': Array(0.14639434, dtype=float32), 'training/policy_loss': Array(0.02808161, dtype=float32), 'training/total_loss': Array(272.1188, dtype=float32), 'training/v_loss': Array(271.9443, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3291208, 0.1451795], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.738237, 12.45612 ], dtype=float32), 'eval/episode_reward': Array([-4.0557313, 10.050053 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32375103, 0.14800519], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.013609886169434, 'eval/sps': 31891.490112448988}
I0728 05:23:40.304061 140199540242240 train.py:379] starting iteration 179, 73318400 steps, 1557.0522348880768
I0728 05:23:48.768240 140199540242240 train.py:394] {'eval/walltime': 738.4750154018402, 'training/sps': 92230.1180656502, 'training/walltime': 816.8179929256439, 'training/entropy_loss': Array(0.14847143, dtype=float32), 'training/policy_loss': Array(0.02773083, dtype=float32), 'training/total_loss': Array(232.79858, dtype=float32), 'training/v_loss': Array(232.62238, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2990281 , 0.13651417], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.197178, 11.813063], dtype=float32), 'eval/episode_reward': Array([-1.8338203,  8.739344 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2926568 , 0.14011024], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.018367052078247, 'eval/sps': 31853.73519668893}
I0728 05:23:48.770256 140199540242240 train.py:379] starting iteration 180, 73728000 steps, 1565.518429517746
I0728 05:23:57.255644 140199540242240 train.py:394] {'eval/walltime': 742.5010306835175, 'training/sps': 91959.06186911481, 'training/walltime': 821.27214884758, 'training/entropy_loss': Array(0.14888988, dtype=float32), 'training/policy_loss': Array(0.0289655, dtype=float32), 'training/total_loss': Array(993.953, dtype=float32), 'training/v_loss': Array(993.77515, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31271148, 0.12613493], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.366646, 10.923631], dtype=float32), 'eval/episode_reward': Array([-3.7407832,  8.234469 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3066187 , 0.13004027], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.026015281677246, 'eval/sps': 31793.22258972523}
I0728 05:23:57.257680 140199540242240 train.py:379] starting iteration 181, 74137600 steps, 1574.0058543682098
I0728 05:24:05.745337 140199540242240 train.py:394] {'eval/walltime': 746.5215337276459, 'training/sps': 91789.45701901904, 'training/walltime': 825.7345349788666, 'training/entropy_loss': Array(0.1468968, dtype=float32), 'training/policy_loss': Array(0.02694702, dtype=float32), 'training/total_loss': Array(551.2029, dtype=float32), 'training/v_loss': Array(551.02905, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30577487, 0.13154773], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.779549, 11.396227], dtype=float32), 'eval/episode_reward': Array([-3.0960603,  9.195848 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29959947, 0.13543713], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020503044128418, 'eval/sps': 31836.812109104718}
I0728 05:24:05.747413 140199540242240 train.py:379] starting iteration 182, 74547200 steps, 1582.4955866336823
I0728 05:24:14.233555 140199540242240 train.py:394] {'eval/walltime': 750.5450932979584, 'training/sps': 91890.14702802911, 'training/walltime': 830.1920313835144, 'training/entropy_loss': Array(0.14393845, dtype=float32), 'training/policy_loss': Array(0.02197697, dtype=float32), 'training/total_loss': Array(358.6117, dtype=float32), 'training/v_loss': Array(358.4458, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3222807 , 0.14309853], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.174664, 12.368789], dtype=float32), 'eval/episode_reward': Array([-5.3047295, 10.650404 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31654662, 0.14636499], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0235595703125, 'eval/sps': 31812.6270440824}
I0728 05:24:14.235598 140199540242240 train.py:379] starting iteration 183, 74956800 steps, 1590.983772277832
I0728 05:24:22.703469 140199540242240 train.py:394] {'eval/walltime': 754.5610616207123, 'training/sps': 92114.47537994025, 'training/walltime': 834.6386723518372, 'training/entropy_loss': Array(0.14412071, dtype=float32), 'training/policy_loss': Array(0.0223984, dtype=float32), 'training/total_loss': Array(269.38373, dtype=float32), 'training/v_loss': Array(269.21722, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32844722, 0.12748262], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.724054, 11.001238], dtype=float32), 'eval/episode_reward': Array([-4.5974417,  8.684867 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32317206, 0.13050152], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.015968322753906, 'eval/sps': 31872.761364866892}
I0728 05:24:22.705494 140199540242240 train.py:379] starting iteration 184, 75366400 steps, 1599.453667640686
I0728 05:24:31.183612 140199540242240 train.py:394] {'eval/walltime': 758.5838606357574, 'training/sps': 92037.82228282646, 'training/walltime': 839.0890166759491, 'training/entropy_loss': Array(0.14493144, dtype=float32), 'training/policy_loss': Array(0.02456621, dtype=float32), 'training/total_loss': Array(234.01294, dtype=float32), 'training/v_loss': Array(233.84343, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32434317, 0.13470376], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.34105 , 11.553678], dtype=float32), 'eval/episode_reward': Array([-3.3381665,  8.923676 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31884617, 0.13768783], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022799015045166, 'eval/sps': 31818.641577986684}
I0728 05:24:31.185611 140199540242240 train.py:379] starting iteration 185, 75776000 steps, 1607.9337847232819
I0728 05:24:39.680041 140199540242240 train.py:394] {'eval/walltime': 762.6191446781158, 'training/sps': 91955.56223787715, 'training/walltime': 843.5433421134949, 'training/entropy_loss': Array(0.14202088, dtype=float32), 'training/policy_loss': Array(0.02701687, dtype=float32), 'training/total_loss': Array(1015.3583, dtype=float32), 'training/v_loss': Array(1015.1892, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32423937, 0.1351998 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.341564, 11.661806], dtype=float32), 'eval/episode_reward': Array([-3.0679607,  9.240705 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31850895, 0.13869052], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.035284042358398, 'eval/sps': 31720.19581679587}
I0728 05:24:39.682051 140199540242240 train.py:379] starting iteration 186, 76185600 steps, 1616.4302251338959
I0728 05:24:48.166127 140199540242240 train.py:394] {'eval/walltime': 766.6384716033936, 'training/sps': 91846.97484608699, 'training/walltime': 848.0029337406158, 'training/entropy_loss': Array(0.1410546, dtype=float32), 'training/policy_loss': Array(0.02404228, dtype=float32), 'training/total_loss': Array(562.079, dtype=float32), 'training/v_loss': Array(561.9139, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32843572, 0.13441393], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.764471, 11.567368], dtype=float32), 'eval/episode_reward': Array([-4.691903,  9.399632], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32306138, 0.13736385], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.01932692527771, 'eval/sps': 31846.12806562283}
I0728 05:24:48.168218 140199540242240 train.py:379] starting iteration 187, 76595200 steps, 1624.916392326355
I0728 05:24:56.640586 140199540242240 train.py:394] {'eval/walltime': 770.6654260158539, 'training/sps': 92247.05982541225, 'training/walltime': 852.4431836605072, 'training/entropy_loss': Array(0.13824794, dtype=float32), 'training/policy_loss': Array(0.02452692, dtype=float32), 'training/total_loss': Array(369.5078, dtype=float32), 'training/v_loss': Array(369.34503, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31387383, 0.14296429], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.417776, 12.399344], dtype=float32), 'eval/episode_reward': Array([-2.8139555,  9.199701 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30722994, 0.14777285], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.026954412460327, 'eval/sps': 31785.808054826357}
I0728 05:24:56.642656 140199540242240 train.py:379] starting iteration 188, 77004800 steps, 1633.3908281326294
I0728 05:25:05.106257 140199540242240 train.py:394] {'eval/walltime': 774.6821029186249, 'training/sps': 92210.36136753623, 'training/walltime': 856.8852007389069, 'training/entropy_loss': Array(0.13935043, dtype=float32), 'training/policy_loss': Array(0.02488564, dtype=float32), 'training/total_loss': Array(278.83777, dtype=float32), 'training/v_loss': Array(278.67352, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2995725 , 0.12963523], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.232979, 11.193029], dtype=float32), 'eval/episode_reward': Array([-2.933122,  9.002973], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.293436  , 0.13318412], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016676902770996, 'eval/sps': 31867.138706550253}
I0728 05:25:05.108301 140199540242240 train.py:379] starting iteration 189, 77414400 steps, 1641.856474161148
I0728 05:25:13.567604 140199540242240 train.py:394] {'eval/walltime': 778.7036836147308, 'training/sps': 92437.37688060109, 'training/walltime': 861.3163087368011, 'training/entropy_loss': Array(0.1404812, dtype=float32), 'training/policy_loss': Array(0.02558421, dtype=float32), 'training/total_loss': Array(249.42128, dtype=float32), 'training/v_loss': Array(249.25522, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30496442, 0.12577394], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.74538 , 10.846608], dtype=float32), 'eval/episode_reward': Array([-3.0708659,  9.065159 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29924572, 0.1286703 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021580696105957, 'eval/sps': 31828.280885657892}
I0728 05:25:13.571793 140199540242240 train.py:379] starting iteration 190, 77824000 steps, 1650.3199515342712
I0728 05:25:22.049827 140199540242240 train.py:394] {'eval/walltime': 782.7287273406982, 'training/sps': 92095.56796621579, 'training/walltime': 865.7638626098633, 'training/entropy_loss': Array(0.13907708, dtype=float32), 'training/policy_loss': Array(0.02473664, dtype=float32), 'training/total_loss': Array(977.2462, dtype=float32), 'training/v_loss': Array(977.0824, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3034431, 0.1286499], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.555264, 11.127741], dtype=float32), 'eval/episode_reward': Array([-2.705215,  9.313334], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29734725, 0.1322171 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.025043725967407, 'eval/sps': 31800.89676398126}
I0728 05:25:22.051851 140199540242240 train.py:379] starting iteration 191, 78233600 steps, 1658.8000252246857
I0728 05:25:30.516273 140199540242240 train.py:394] {'eval/walltime': 786.7477967739105, 'training/sps': 92249.76930133047, 'training/walltime': 870.2039821147919, 'training/entropy_loss': Array(0.13883385, dtype=float32), 'training/policy_loss': Array(0.02106475, dtype=float32), 'training/total_loss': Array(523.18835, dtype=float32), 'training/v_loss': Array(523.02844, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30947846, 0.12526786], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.062244, 10.802821], dtype=float32), 'eval/episode_reward': Array([-2.6286025,  8.605173 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30375946, 0.12813656], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.01906943321228, 'eval/sps': 31848.16837008331}
I0728 05:25:30.518307 140199540242240 train.py:379] starting iteration 192, 78643200 steps, 1667.2664813995361
I0728 05:25:38.984923 140199540242240 train.py:394] {'eval/walltime': 790.7718598842621, 'training/sps': 92303.61039885994, 'training/walltime': 874.6415116786957, 'training/entropy_loss': Array(0.13932288, dtype=float32), 'training/policy_loss': Array(0.02161777, dtype=float32), 'training/total_loss': Array(352.96353, dtype=float32), 'training/v_loss': Array(352.8026, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30415407, 0.13891098], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.619965, 12.028419], dtype=float32), 'eval/episode_reward': Array([-2.5079906,  9.521999 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29786703, 0.1427538 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0240631103515625, 'eval/sps': 31808.646258735556}
I0728 05:25:38.986978 140199540242240 train.py:379] starting iteration 193, 79052800 steps, 1675.7351517677307
I0728 05:25:47.476258 140199540242240 train.py:394] {'eval/walltime': 794.8113017082214, 'training/sps': 92150.75648140415, 'training/walltime': 879.0864019393921, 'training/entropy_loss': Array(0.13925257, dtype=float32), 'training/policy_loss': Array(0.02317336, dtype=float32), 'training/total_loss': Array(272.91028, dtype=float32), 'training/v_loss': Array(272.74783, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3221054 , 0.12497894], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.19581 , 10.773435], dtype=float32), 'eval/episode_reward': Array([-4.1744194, 10.130574 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31680787, 0.12750116], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.039441823959351, 'eval/sps': 31687.54634385052}
I0728 05:25:47.478278 140199540242240 train.py:379] starting iteration 194, 79462400 steps, 1684.2264523506165
I0728 05:25:55.944518 140199540242240 train.py:394] {'eval/walltime': 798.8305175304413, 'training/sps': 92214.18234865407, 'training/walltime': 883.5282349586487, 'training/entropy_loss': Array(0.13661219, dtype=float32), 'training/policy_loss': Array(0.02199502, dtype=float32), 'training/total_loss': Array(236.30197, dtype=float32), 'training/v_loss': Array(236.14337, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29945996, 0.12860222], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.169935, 11.138906], dtype=float32), 'eval/episode_reward': Array([-3.125207,  9.209522], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2933703 , 0.13183823], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019215822219849, 'eval/sps': 31847.008387149624}
I0728 05:25:55.946538 140199540242240 train.py:379] starting iteration 195, 79872000 steps, 1692.6947116851807
I0728 05:26:04.423042 140199540242240 train.py:394] {'eval/walltime': 802.8568871021271, 'training/sps': 92145.23564204854, 'training/walltime': 887.973391532898, 'training/entropy_loss': Array(0.13371298, dtype=float32), 'training/policy_loss': Array(0.02441441, dtype=float32), 'training/total_loss': Array(975.72253, dtype=float32), 'training/v_loss': Array(975.56433, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30976933, 0.13144043], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.122545, 11.345481], dtype=float32), 'eval/episode_reward': Array([-1.9453889,  9.488493 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30396247, 0.13448828], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.026369571685791, 'eval/sps': 31790.425027081652}
I0728 05:26:04.425125 140199540242240 train.py:379] starting iteration 196, 80281600 steps, 1701.1732995510101
I0728 05:26:12.888735 140199540242240 train.py:394] {'eval/walltime': 806.8703215122223, 'training/sps': 92141.77618731589, 'training/walltime': 892.4187150001526, 'training/entropy_loss': Array(0.13334234, dtype=float32), 'training/policy_loss': Array(0.02116409, dtype=float32), 'training/total_loss': Array(521.61523, dtype=float32), 'training/v_loss': Array(521.4607, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32251894, 0.13229693], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.213898, 11.376753], dtype=float32), 'eval/episode_reward': Array([-3.9301176,  8.831571 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3169549, 0.1355104], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.013434410095215, 'eval/sps': 31892.88447770181}
I0728 05:26:12.890741 140199540242240 train.py:379] starting iteration 197, 80691200 steps, 1709.6389150619507
I0728 05:26:21.350886 140199540242240 train.py:394] {'eval/walltime': 810.8933885097504, 'training/sps': 92413.30570236934, 'training/walltime': 896.8509771823883, 'training/entropy_loss': Array(0.12942237, dtype=float32), 'training/policy_loss': Array(0.02136159, dtype=float32), 'training/total_loss': Array(355.70386, dtype=float32), 'training/v_loss': Array(355.55307, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3233862 , 0.11795288], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.320396, 10.176472], dtype=float32), 'eval/episode_reward': Array([-4.4766097,  8.491168 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3178087 , 0.12158884], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023066997528076, 'eval/sps': 31816.52209089434}
I0728 05:26:21.352880 140199540242240 train.py:379] starting iteration 198, 81100800 steps, 1718.1010539531708
I0728 05:26:29.826212 140199540242240 train.py:394] {'eval/walltime': 814.9159820079803, 'training/sps': 92127.23449257054, 'training/walltime': 901.2970023155212, 'training/entropy_loss': Array(0.13013509, dtype=float32), 'training/policy_loss': Array(0.02576552, dtype=float32), 'training/total_loss': Array(273.40344, dtype=float32), 'training/v_loss': Array(273.2475, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30655944, 0.14150622], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.8238  , 12.216236], dtype=float32), 'eval/episode_reward': Array([-2.572745,  9.041828], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30014873, 0.14552683], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0225934982299805, 'eval/sps': 31820.267212265542}
I0728 05:26:29.828235 140199540242240 train.py:379] starting iteration 199, 81510400 steps, 1726.5764091014862
I0728 05:26:38.283916 140199540242240 train.py:394] {'eval/walltime': 818.9363152980804, 'training/sps': 92446.72829183082, 'training/walltime': 905.7276620864868, 'training/entropy_loss': Array(0.13096644, dtype=float32), 'training/policy_loss': Array(0.02562405, dtype=float32), 'training/total_loss': Array(247.22664, dtype=float32), 'training/v_loss': Array(247.07004, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32794714, 0.12709592], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.681072, 10.952863], dtype=float32), 'eval/episode_reward': Array([-3.6500444,  9.3217325], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32262802, 0.1302224 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020333290100098, 'eval/sps': 31838.15638250556}
I0728 05:26:38.285871 140199540242240 train.py:379] starting iteration 200, 81920000 steps, 1735.0340445041656
I0728 05:26:46.746384 140199540242240 train.py:394] {'eval/walltime': 822.9550142288208, 'training/sps': 92314.83954612994, 'training/walltime': 910.1646518707275, 'training/entropy_loss': Array(0.12937102, dtype=float32), 'training/policy_loss': Array(0.02070903, dtype=float32), 'training/total_loss': Array(1010.57666, dtype=float32), 'training/v_loss': Array(1010.4266, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30949843, 0.13325682], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.073303, 11.491091], dtype=float32), 'eval/episode_reward': Array([-3.0321512,  9.061849 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30365434, 0.13647528], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0186989307403564, 'eval/sps': 31851.10460026893}
I0728 05:26:46.830889 140199540242240 train.py:379] starting iteration 201, 82329600 steps, 1743.5790617465973
I0728 05:26:55.281943 140199540242240 train.py:394] {'eval/walltime': 826.9768102169037, 'training/sps': 92572.85349911077, 'training/walltime': 914.5892751216888, 'training/entropy_loss': Array(0.13142367, dtype=float32), 'training/policy_loss': Array(0.02211328, dtype=float32), 'training/total_loss': Array(546.46, dtype=float32), 'training/v_loss': Array(546.30646, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31404397, 0.13780783], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.46903 , 11.894603], dtype=float32), 'eval/episode_reward': Array([-3.670598,  9.460484], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.307934  , 0.14178102], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021795988082886, 'eval/sps': 31826.57707633131}
I0728 05:26:55.284058 140199540242240 train.py:379] starting iteration 202, 82739200 steps, 1752.0322318077087
I0728 05:27:03.745024 140199540242240 train.py:394] {'eval/walltime': 830.9967269897461, 'training/sps': 92331.36584335806, 'training/walltime': 919.0254707336426, 'training/entropy_loss': Array(0.13059424, dtype=float32), 'training/policy_loss': Array(0.01987293, dtype=float32), 'training/total_loss': Array(351.8645, dtype=float32), 'training/v_loss': Array(351.71402, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29868394, 0.13055257], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.15603 , 11.268997], dtype=float32), 'eval/episode_reward': Array([-2.430282,  8.035252], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29237157, 0.13432875], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019916772842407, 'eval/sps': 31841.455242242148}
I0728 05:27:03.747004 140199540242240 train.py:379] starting iteration 203, 83148800 steps, 1760.495177745819
I0728 05:27:12.224815 140199540242240 train.py:394] {'eval/walltime': 835.0300884246826, 'training/sps': 92256.01111290649, 'training/walltime': 923.4652898311615, 'training/entropy_loss': Array(0.12831618, dtype=float32), 'training/policy_loss': Array(0.02031513, dtype=float32), 'training/total_loss': Array(277.05023, dtype=float32), 'training/v_loss': Array(276.90155, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2966036 , 0.12966463], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.939236, 11.190268], dtype=float32), 'eval/episode_reward': Array([-1.4745028,  8.313623 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28989577, 0.13425335], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.033361434936523, 'eval/sps': 31735.31607935713}
I0728 05:27:12.226832 140199540242240 train.py:379] starting iteration 204, 83558400 steps, 1768.9750061035156
I0728 05:27:20.691877 140199540242240 train.py:394] {'eval/walltime': 839.0510604381561, 'training/sps': 92297.84808990889, 'training/walltime': 927.9030964374542, 'training/entropy_loss': Array(0.12544435, dtype=float32), 'training/policy_loss': Array(0.0204532, dtype=float32), 'training/total_loss': Array(241.79353, dtype=float32), 'training/v_loss': Array(241.64763, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31655374, 0.13909408], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.700829, 12.037547], dtype=float32), 'eval/episode_reward': Array([-2.9708357,  9.812723 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3106262 , 0.14267823], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020972013473511, 'eval/sps': 31833.09895495328}
I0728 05:27:20.693871 140199540242240 train.py:379] starting iteration 205, 83968000 steps, 1777.4420456886292
I0728 05:27:29.147708 140199540242240 train.py:394] {'eval/walltime': 843.0747034549713, 'training/sps': 92548.21315944336, 'training/walltime': 932.3288977146149, 'training/entropy_loss': Array(0.12356438, dtype=float32), 'training/policy_loss': Array(0.01971538, dtype=float32), 'training/total_loss': Array(990.9269, dtype=float32), 'training/v_loss': Array(990.78357, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3178317, 0.1326481], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.789528, 11.450272], dtype=float32), 'eval/episode_reward': Array([-3.6322532,  8.943668 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31200695, 0.1362487 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0236430168151855, 'eval/sps': 31811.967280664776}
I0728 05:27:29.149698 140199540242240 train.py:379] starting iteration 206, 84377600 steps, 1785.8978717327118
I0728 05:27:37.624300 140199540242240 train.py:394] {'eval/walltime': 847.1076893806458, 'training/sps': 92322.29077959836, 'training/walltime': 936.7655293941498, 'training/entropy_loss': Array(0.12275293, dtype=float32), 'training/policy_loss': Array(0.01881151, dtype=float32), 'training/total_loss': Array(559.95764, dtype=float32), 'training/v_loss': Array(559.81616, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31941837, 0.12524697], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.918642, 10.786344], dtype=float32), 'eval/episode_reward': Array([-3.7733798, 10.233152 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31396165, 0.12803416], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0329859256744385, 'eval/sps': 31738.2709384473}
I0728 05:27:37.626274 140199540242240 train.py:379] starting iteration 207, 84787200 steps, 1794.374446630478
I0728 05:27:46.066238 140199540242240 train.py:394] {'eval/walltime': 851.1247568130493, 'training/sps': 92703.87025417188, 'training/walltime': 941.1838994026184, 'training/entropy_loss': Array(0.12127574, dtype=float32), 'training/policy_loss': Array(0.01874029, dtype=float32), 'training/total_loss': Array(352.90802, dtype=float32), 'training/v_loss': Array(352.768, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.311598  , 0.11862215], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.348272, 10.215102], dtype=float32), 'eval/episode_reward': Array([-3.1466336,  9.095017 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30602574, 0.12159841], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0170674324035645, 'eval/sps': 31864.040659982827}
I0728 05:27:46.068253 140199540242240 train.py:379] starting iteration 208, 85196800 steps, 1802.8164269924164
I0728 05:27:54.530643 140199540242240 train.py:394] {'eval/walltime': 855.146842956543, 'training/sps': 92344.81550058769, 'training/walltime': 945.6194489002228, 'training/entropy_loss': Array(0.11909099, dtype=float32), 'training/policy_loss': Array(0.02157482, dtype=float32), 'training/total_loss': Array(270.4486, dtype=float32), 'training/v_loss': Array(270.30795, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28749195, 0.13334851], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.140556, 11.572089], dtype=float32), 'eval/episode_reward': Array([-1.4665953,  9.347845 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2805554 , 0.13764016], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022086143493652, 'eval/sps': 31824.28109031425}
I0728 05:27:54.532642 140199540242240 train.py:379] starting iteration 209, 85606400 steps, 1811.2808158397675
I0728 05:28:02.989055 140199540242240 train.py:394] {'eval/walltime': 859.1758244037628, 'training/sps': 92614.71877957237, 'training/walltime': 950.042072057724, 'training/entropy_loss': Array(0.1154177, dtype=float32), 'training/policy_loss': Array(0.02511795, dtype=float32), 'training/total_loss': Array(247.80928, dtype=float32), 'training/v_loss': Array(247.66878, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32803148, 0.12464824], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.69857 , 10.712427], dtype=float32), 'eval/episode_reward': Array([-4.08665 ,  8.946633], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32282704, 0.12726276], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.028981447219849, 'eval/sps': 31769.81618724626}
I0728 05:28:02.991042 140199540242240 train.py:379] starting iteration 210, 86016000 steps, 1819.7392160892487
I0728 05:28:11.451655 140199540242240 train.py:394] {'eval/walltime': 863.1919560432434, 'training/sps': 92259.9002876619, 'training/walltime': 954.4817039966583, 'training/entropy_loss': Array(0.11060397, dtype=float32), 'training/policy_loss': Array(0.02077525, dtype=float32), 'training/total_loss': Array(1007.97473, dtype=float32), 'training/v_loss': Array(1007.84326, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30230504, 0.12925099], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.462366, 11.214491], dtype=float32), 'eval/episode_reward': Array([-2.3588443,  8.164228 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29616132, 0.13279833], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016131639480591, 'eval/sps': 31871.465253204282}
I0728 05:28:11.453638 140199540242240 train.py:379] starting iteration 211, 86425600 steps, 1828.2018117904663
I0728 05:28:19.895216 140199540242240 train.py:394] {'eval/walltime': 867.2168214321136, 'training/sps': 92832.7752553272, 'training/walltime': 958.8939387798309, 'training/entropy_loss': Array(0.10540439, dtype=float32), 'training/policy_loss': Array(0.02187137, dtype=float32), 'training/total_loss': Array(540.4159, dtype=float32), 'training/v_loss': Array(540.28864, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29887173, 0.11483829], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.163754,  9.94741 ], dtype=float32), 'eval/episode_reward': Array([-2.0959117,  9.107133 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29276332, 0.11847078], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024865388870239, 'eval/sps': 31802.305824674797}
I0728 05:28:19.897183 140199540242240 train.py:379] starting iteration 212, 86835200 steps, 1836.645357131958
I0728 05:28:28.378341 140199540242240 train.py:394] {'eval/walltime': 871.2427208423615, 'training/sps': 92034.13423103905, 'training/walltime': 963.34446144104, 'training/entropy_loss': Array(0.1032462, dtype=float32), 'training/policy_loss': Array(0.02241779, dtype=float32), 'training/total_loss': Array(353.73145, dtype=float32), 'training/v_loss': Array(353.60577, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33297163, 0.14283782], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.085052, 12.313365], dtype=float32), 'eval/episode_reward': Array([-4.2849507,  9.790545 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3275158 , 0.14598782], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.025899410247803, 'eval/sps': 31794.13764640516}
I0728 05:28:28.380314 140199540242240 train.py:379] starting iteration 213, 87244800 steps, 1845.1284880638123
I0728 05:28:36.864158 140199540242240 train.py:394] {'eval/walltime': 875.2998995780945, 'training/sps': 92622.9226010979, 'training/walltime': 967.7666928768158, 'training/entropy_loss': Array(0.09865555, dtype=float32), 'training/policy_loss': Array(0.02227476, dtype=float32), 'training/total_loss': Array(277.7912, dtype=float32), 'training/v_loss': Array(277.67023, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2933747 , 0.12766866], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.633526, 11.051153], dtype=float32), 'eval/episode_reward': Array([-2.3140144, 10.043276 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.287097  , 0.13117398], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.057178735733032, 'eval/sps': 31549.016776771}
I0728 05:28:36.866164 140199540242240 train.py:379] starting iteration 214, 87654400 steps, 1853.6143381595612
I0728 05:28:45.310437 140199540242240 train.py:394] {'eval/walltime': 879.321950674057, 'training/sps': 92723.40362464913, 'training/walltime': 972.1841320991516, 'training/entropy_loss': Array(0.08980361, dtype=float32), 'training/policy_loss': Array(0.02007872, dtype=float32), 'training/total_loss': Array(263.16568, dtype=float32), 'training/v_loss': Array(263.05582, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.337309 , 0.1302784], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.424673, 11.19749 ], dtype=float32), 'eval/episode_reward': Array([-3.6143675,  8.297949 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.33203557, 0.133466  ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022051095962524, 'eval/sps': 31824.558402177157}
I0728 05:28:45.312411 140199540242240 train.py:379] starting iteration 215, 88064000 steps, 1862.060584783554
I0728 05:28:53.782116 140199540242240 train.py:394] {'eval/walltime': 883.3503756523132, 'training/sps': 92358.58188177473, 'training/walltime': 976.6190204620361, 'training/entropy_loss': Array(0.08540271, dtype=float32), 'training/policy_loss': Array(0.01172602, dtype=float32), 'training/total_loss': Array(1082.7975, dtype=float32), 'training/v_loss': Array(1082.7004, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32186848, 0.13036555], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.06985 , 11.254513], dtype=float32), 'eval/episode_reward': Array([-3.6991887,  8.7000675], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3159517 , 0.13436815], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.028424978256226, 'eval/sps': 31774.20473035768}
I0728 05:28:53.786228 140199540242240 train.py:379] starting iteration 216, 88473600 steps, 1870.5343866348267
I0728 05:29:02.250308 140199540242240 train.py:394] {'eval/walltime': 887.3683741092682, 'training/sps': 92232.47002797703, 'training/walltime': 981.0599727630615, 'training/entropy_loss': Array(0.08377713, dtype=float32), 'training/policy_loss': Array(0.00630712, dtype=float32), 'training/total_loss': Array(569.8286, dtype=float32), 'training/v_loss': Array(569.7385, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29884785, 0.1274043 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.135887, 11.006853], dtype=float32), 'eval/episode_reward': Array([-2.369168,  8.929638], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2926532 , 0.13112983], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.017998456954956, 'eval/sps': 31856.657331074468}
I0728 05:29:02.252281 140199540242240 train.py:379] starting iteration 217, 88883200 steps, 1879.0004546642303
I0728 05:29:10.705441 140199540242240 train.py:394] {'eval/walltime': 891.3935499191284, 'training/sps': 92605.10872531655, 'training/walltime': 985.4830548763275, 'training/entropy_loss': Array(0.08235675, dtype=float32), 'training/policy_loss': Array(0.00545375, dtype=float32), 'training/total_loss': Array(363.40076, dtype=float32), 'training/v_loss': Array(363.31293, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30223942, 0.14381407], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.358742 , 12.4052925], dtype=float32), 'eval/episode_reward': Array([-2.4408135,  9.0103245], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2957871 , 0.14773099], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0251758098602295, 'eval/sps': 31799.853235340965}
I0728 05:29:10.707453 140199540242240 train.py:379] starting iteration 218, 89292800 steps, 1887.4556274414062
I0728 05:29:19.173283 140199540242240 train.py:394] {'eval/walltime': 895.4161899089813, 'training/sps': 92275.97571845021, 'training/walltime': 989.9219133853912, 'training/entropy_loss': Array(0.08050834, dtype=float32), 'training/policy_loss': Array(0.00617532, dtype=float32), 'training/total_loss': Array(303.06708, dtype=float32), 'training/v_loss': Array(302.9804, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32956833, 0.13705376], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.740541 , 11.7710085], dtype=float32), 'eval/episode_reward': Array([-3.939919,  9.613961], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3241217 , 0.14022593], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022639989852905, 'eval/sps': 31819.899449833825}
I0728 05:29:19.175272 140199540242240 train.py:379] starting iteration 219, 89702400 steps, 1895.9234459400177
I0728 05:29:27.632441 140199540242240 train.py:394] {'eval/walltime': 899.4255299568176, 'training/sps': 92189.97000905432, 'training/walltime': 994.3649129867554, 'training/entropy_loss': Array(0.07883938, dtype=float32), 'training/policy_loss': Array(0.00645528, dtype=float32), 'training/total_loss': Array(275.92035, dtype=float32), 'training/v_loss': Array(275.83502, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30382836, 0.15348205], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.476225, 13.287111], dtype=float32), 'eval/episode_reward': Array([-2.8651023, 10.597235 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2971232 , 0.15777795], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.009340047836304, 'eval/sps': 31925.453683849286}
I0728 05:29:27.634446 140199540242240 train.py:379] starting iteration 220, 90112000 steps, 1904.3826196193695
I0728 05:29:36.088924 140199540242240 train.py:394] {'eval/walltime': 903.4416792392731, 'training/sps': 92384.08510155471, 'training/walltime': 998.7985770702362, 'training/entropy_loss': Array(0.07659818, dtype=float32), 'training/policy_loss': Array(0.00520557, dtype=float32), 'training/total_loss': Array(1039.7261, dtype=float32), 'training/v_loss': Array(1039.644, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3120457 , 0.13431503], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.193054, 11.564643], dtype=float32), 'eval/episode_reward': Array([-2.6392193,  9.385123 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3060767 , 0.13804513], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016149282455444, 'eval/sps': 31871.32524161097}
I0728 05:29:36.090914 140199540242240 train.py:379] starting iteration 221, 90521600 steps, 1912.8390882015228
I0728 05:29:44.561258 140199540242240 train.py:394] {'eval/walltime': 907.4597311019897, 'training/sps': 92095.25694021846, 'training/walltime': 1003.2461459636688, 'training/entropy_loss': Array(0.07215938, dtype=float32), 'training/policy_loss': Array(0.0050428, dtype=float32), 'training/total_loss': Array(550.6411, dtype=float32), 'training/v_loss': Array(550.56396, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3127743 , 0.13764869], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.301048, 11.854315], dtype=float32), 'eval/episode_reward': Array([-3.1029747, 10.478077 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3071015 , 0.14053744], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.018051862716675, 'eval/sps': 31856.233909697963}
I0728 05:29:44.563255 140199540242240 train.py:379] starting iteration 222, 90931200 steps, 1921.3114290237427
I0728 05:29:53.051811 140199540242240 train.py:394] {'eval/walltime': 911.5082306861877, 'training/sps': 92344.77579103605, 'training/walltime': 1007.6816973686218, 'training/entropy_loss': Array(0.06747009, dtype=float32), 'training/policy_loss': Array(0.00514117, dtype=float32), 'training/total_loss': Array(349.4504, dtype=float32), 'training/v_loss': Array(349.37778, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3123259 , 0.12490021], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.234358, 10.766578], dtype=float32), 'eval/episode_reward': Array([-2.2320616,  8.754339 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30664423, 0.12812701], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.048499584197998, 'eval/sps': 31616.651388481398}
I0728 05:29:53.053791 140199540242240 train.py:379] starting iteration 223, 91340800 steps, 1929.801964521408
I0728 05:30:01.488201 140199540242240 train.py:394] {'eval/walltime': 915.5238955020905, 'training/sps': 92794.7773534944, 'training/walltime': 1012.0957388877869, 'training/entropy_loss': Array(0.06366722, dtype=float32), 'training/policy_loss': Array(0.00539507, dtype=float32), 'training/total_loss': Array(287.40826, dtype=float32), 'training/v_loss': Array(287.3392, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31454533, 0.11877237], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.472775, 10.226631], dtype=float32), 'eval/episode_reward': Array([-3.368657,  8.306259], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30922815, 0.12147226], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.01566481590271, 'eval/sps': 31875.170331223464}
I0728 05:30:01.490216 140199540242240 train.py:379] starting iteration 224, 91750400 steps, 1938.2383906841278
I0728 05:30:09.954810 140199540242240 train.py:394] {'eval/walltime': 919.5470633506775, 'training/sps': 92312.4288186767, 'training/walltime': 1016.532844543457, 'training/entropy_loss': Array(0.06154856, dtype=float32), 'training/policy_loss': Array(0.00596925, dtype=float32), 'training/total_loss': Array(259.80267, dtype=float32), 'training/v_loss': Array(259.73514, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31612402, 0.13349923], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.548744, 11.508354], dtype=float32), 'eval/episode_reward': Array([-3.4299967,  9.144668 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31031448, 0.13703182], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023167848587036, 'eval/sps': 31815.724527862905}
I0728 05:30:09.956743 140199540242240 train.py:379] starting iteration 225, 92160000 steps, 1946.704916715622
I0728 05:30:18.435349 140199540242240 train.py:394] {'eval/walltime': 923.5945198535919, 'training/sps': 92528.49438297136, 'training/walltime': 1020.9595890045166, 'training/entropy_loss': Array(0.05865552, dtype=float32), 'training/policy_loss': Array(0.00493462, dtype=float32), 'training/total_loss': Array(1010.4716, dtype=float32), 'training/v_loss': Array(1010.408, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3203107 , 0.12764817], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.932941, 11.002012], dtype=float32), 'eval/episode_reward': Array([-3.3587337, 10.676726 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3148186 , 0.13082787], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.047456502914429, 'eval/sps': 31624.79940373214}
I0728 05:30:18.524912 140199540242240 train.py:379] starting iteration 226, 92569600 steps, 1955.2730836868286
I0728 05:30:27.011310 140199540242240 train.py:394] {'eval/walltime': 927.6305561065674, 'training/sps': 92138.11439039366, 'training/walltime': 1025.4050891399384, 'training/entropy_loss': Array(0.05268227, dtype=float32), 'training/policy_loss': Array(0.00420034, dtype=float32), 'training/total_loss': Array(551.8523, dtype=float32), 'training/v_loss': Array(551.7954, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31085742, 0.12882464], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.12335 , 11.137897], dtype=float32), 'eval/episode_reward': Array([-3.7314985,  8.310166 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30479017, 0.13267711], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.036036252975464, 'eval/sps': 31714.284009623378}
I0728 05:30:27.013415 140199540242240 train.py:379] starting iteration 227, 92979200 steps, 1963.7615885734558
I0728 05:30:35.450773 140199540242240 train.py:394] {'eval/walltime': 931.6450488567352, 'training/sps': 92707.19695011506, 'training/walltime': 1029.8233006000519, 'training/entropy_loss': Array(0.04883202, dtype=float32), 'training/policy_loss': Array(0.0035621, dtype=float32), 'training/total_loss': Array(355.12656, dtype=float32), 'training/v_loss': Array(355.0742, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30631775, 0.13597088], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.753553, 11.727857], dtype=float32), 'eval/episode_reward': Array([-1.1305369,  9.705179 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30038226, 0.13945037], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.014492750167847, 'eval/sps': 31884.476561739535}
I0728 05:30:35.452760 140199540242240 train.py:379] starting iteration 228, 93388800 steps, 1972.200933456421
I0728 05:30:43.920681 140199540242240 train.py:394] {'eval/walltime': 935.665682554245, 'training/sps': 92192.15666303494, 'training/walltime': 1034.266194820404, 'training/entropy_loss': Array(0.04696343, dtype=float32), 'training/policy_loss': Array(0.00385458, dtype=float32), 'training/total_loss': Array(285.3977, dtype=float32), 'training/v_loss': Array(285.34686, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2940533 , 0.12087976], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.623913, 10.533881], dtype=float32), 'eval/episode_reward': Array([-2.2120132,  8.8005705], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2874642 , 0.12530802], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020633697509766, 'eval/sps': 31835.77754901635}
I0728 05:30:43.922655 140199540242240 train.py:379] starting iteration 229, 93798400 steps, 1980.670829296112
I0728 05:30:52.377736 140199540242240 train.py:394] {'eval/walltime': 939.6873979568481, 'training/sps': 92490.2770551733, 'training/walltime': 1038.6947684288025, 'training/entropy_loss': Array(0.04684915, dtype=float32), 'training/policy_loss': Array(0.00382622, dtype=float32), 'training/total_loss': Array(241.97931, dtype=float32), 'training/v_loss': Array(241.92862, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28370053, 0.12205218], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.739153, 10.529339], dtype=float32), 'eval/episode_reward': Array([-1.9964495,  8.681717 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27728295, 0.12555927], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021715402603149, 'eval/sps': 31827.214804197483}
I0728 05:30:52.379810 140199540242240 train.py:379] starting iteration 230, 94208000 steps, 1989.1279821395874
I0728 05:31:00.838701 140199540242240 train.py:394] {'eval/walltime': 943.7051923274994, 'training/sps': 92335.66335214225, 'training/walltime': 1043.1307575702667, 'training/entropy_loss': Array(0.04481268, dtype=float32), 'training/policy_loss': Array(0.00407196, dtype=float32), 'training/total_loss': Array(981.4397, dtype=float32), 'training/v_loss': Array(981.39075, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30579826, 0.12819932], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.666424, 11.05426 ], dtype=float32), 'eval/episode_reward': Array([-2.1018362,  8.328742 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29970154, 0.13211137], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.017794370651245, 'eval/sps': 31858.27550931942}
I0728 05:31:00.840716 140199540242240 train.py:379] starting iteration 231, 94617600 steps, 1997.5888893604279
I0728 05:31:09.280593 140199540242240 train.py:394] {'eval/walltime': 947.7263510227203, 'training/sps': 92799.39380684108, 'training/walltime': 1047.5445795059204, 'training/entropy_loss': Array(0.03777333, dtype=float32), 'training/policy_loss': Array(0.00335717, dtype=float32), 'training/total_loss': Array(555.4092, dtype=float32), 'training/v_loss': Array(555.3681, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3132844, 0.1288222], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.33313 , 11.076485], dtype=float32), 'eval/episode_reward': Array([-3.528378,  8.770023], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30749243, 0.13214757], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021158695220947, 'eval/sps': 31831.621107648647}
I0728 05:31:09.282579 140199540242240 train.py:379] starting iteration 232, 95027200 steps, 2006.0307528972626
I0728 05:31:17.756393 140199540242240 train.py:394] {'eval/walltime': 951.7482120990753, 'training/sps': 92101.58648603181, 'training/walltime': 1051.9918427467346, 'training/entropy_loss': Array(0.03228787, dtype=float32), 'training/policy_loss': Array(0.00296774, dtype=float32), 'training/total_loss': Array(354.62256, dtype=float32), 'training/v_loss': Array(354.58728, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32519072, 0.12329894], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.307056, 10.659976], dtype=float32), 'eval/episode_reward': Array([-3.0653453,  8.161435 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31973356, 0.12687741], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0218610763549805, 'eval/sps': 31826.06200709613}
I0728 05:31:17.758405 140199540242240 train.py:379] starting iteration 233, 95436800 steps, 2014.5065786838531
I0728 05:31:26.215740 140199540242240 train.py:394] {'eval/walltime': 955.7642710208893, 'training/sps': 92319.16032128817, 'training/walltime': 1056.428624868393, 'training/entropy_loss': Array(0.03168986, dtype=float32), 'training/policy_loss': Array(0.00353543, dtype=float32), 'training/total_loss': Array(265.43753, dtype=float32), 'training/v_loss': Array(265.4023, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29045808, 0.13085459], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.301693, 11.297657], dtype=float32), 'eval/episode_reward': Array([-1.7733893,  9.178549 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28380343, 0.13506268], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016058921813965, 'eval/sps': 31872.042340998632}
I0728 05:31:26.217696 140199540242240 train.py:379] starting iteration 234, 95846400 steps, 2022.9658699035645
I0728 05:31:34.666073 140199540242240 train.py:394] {'eval/walltime': 959.7833399772644, 'training/sps': 92570.95800624993, 'training/walltime': 1060.8533387184143, 'training/entropy_loss': Array(0.0361072, dtype=float32), 'training/policy_loss': Array(0.0044529, dtype=float32), 'training/total_loss': Array(220.44142, dtype=float32), 'training/v_loss': Array(220.40085, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31055933, 0.12897252], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.049574, 11.107891], dtype=float32), 'eval/episode_reward': Array([-2.7874584,  8.592106 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30476302, 0.13221008], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019068956375122, 'eval/sps': 31848.17214866742}
I0728 05:31:34.668138 140199540242240 train.py:379] starting iteration 235, 96256000 steps, 2031.4163115024567
I0728 05:31:43.133689 140199540242240 train.py:394] {'eval/walltime': 963.8021891117096, 'training/sps': 92204.91751139286, 'training/walltime': 1065.295618057251, 'training/entropy_loss': Array(0.03985818, dtype=float32), 'training/policy_loss': Array(0.00576242, dtype=float32), 'training/total_loss': Array(930.97687, dtype=float32), 'training/v_loss': Array(930.93134, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3191085, 0.1287304], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.767164, 11.152214], dtype=float32), 'eval/episode_reward': Array([-3.0415096,  8.4342   ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31346825, 0.13204244], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.01884913444519, 'eval/sps': 31849.91417142874}
I0728 05:31:43.135741 140199540242240 train.py:379] starting iteration 236, 96665600 steps, 2039.8839147090912
I0728 05:31:51.630779 140199540242240 train.py:394] {'eval/walltime': 967.853481054306, 'training/sps': 92270.22181489819, 'training/walltime': 1069.734753370285, 'training/entropy_loss': Array(0.038012, dtype=float32), 'training/policy_loss': Array(0.00503179, dtype=float32), 'training/total_loss': Array(521.61084, dtype=float32), 'training/v_loss': Array(521.5678, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2958311 , 0.12575744], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.766884, 10.903187], dtype=float32), 'eval/episode_reward': Array([-1.9804568,  8.556908 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.289572  , 0.12952234], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0512919425964355, 'eval/sps': 31594.859569159064}
I0728 05:31:51.632780 140199540242240 train.py:379] starting iteration 237, 97075200 steps, 2048.38095331192
I0728 05:32:00.081171 140199540242240 train.py:394] {'eval/walltime': 971.8713779449463, 'training/sps': 92553.28377834041, 'training/walltime': 1074.1603121757507, 'training/entropy_loss': Array(0.0396454, dtype=float32), 'training/policy_loss': Array(0.00553962, dtype=float32), 'training/total_loss': Array(328.66467, dtype=float32), 'training/v_loss': Array(328.6195, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29854763, 0.13667302], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.928022, 11.806973], dtype=float32), 'eval/episode_reward': Array([-1.7839781,  9.316925 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2920634 , 0.14045472], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.017896890640259, 'eval/sps': 31857.46261885854}
I0728 05:32:00.083260 140199540242240 train.py:379] starting iteration 238, 97484800 steps, 2056.8314332962036
I0728 05:32:08.541716 140199540242240 train.py:394] {'eval/walltime': 975.8964402675629, 'training/sps': 92500.4958021254, 'training/walltime': 1078.5883965492249, 'training/entropy_loss': Array(0.04475848, dtype=float32), 'training/policy_loss': Array(0.00686678, dtype=float32), 'training/total_loss': Array(232.0426, dtype=float32), 'training/v_loss': Array(231.99097, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29911566, 0.12529585], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.939919, 10.834132], dtype=float32), 'eval/episode_reward': Array([-1.3608208,  8.77089  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29286206, 0.12919702], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.025062322616577, 'eval/sps': 31800.7498370338}
I0728 05:32:08.545840 140199540242240 train.py:379] starting iteration 239, 97894400 steps, 2065.2939982414246
I0728 05:32:17.002031 140199540242240 train.py:394] {'eval/walltime': 979.9217298030853, 'training/sps': 92550.6511752864, 'training/walltime': 1083.0140812397003, 'training/entropy_loss': Array(0.05394102, dtype=float32), 'training/policy_loss': Array(0.00768385, dtype=float32), 'training/total_loss': Array(182.91058, dtype=float32), 'training/v_loss': Array(182.84897, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2833224 , 0.12655233], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.563492, 10.974012], dtype=float32), 'eval/episode_reward': Array([-1.910046 ,  8.8572855], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2761792 , 0.13132529], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.025289535522461, 'eval/sps': 31798.954800747342}
I0728 05:32:17.004059 140199540242240 train.py:379] starting iteration 240, 98304000 steps, 2073.7522332668304
I0728 05:32:25.464428 140199540242240 train.py:394] {'eval/walltime': 983.9422740936279, 'training/sps': 92356.2632027321, 'training/walltime': 1087.4490809440613, 'training/entropy_loss': Array(0.05565718, dtype=float32), 'training/policy_loss': Array(0.00887377, dtype=float32), 'training/total_loss': Array(831.63403, dtype=float32), 'training/v_loss': Array(831.56946, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29706162, 0.12590523], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.794561, 10.840855], dtype=float32), 'eval/episode_reward': Array([-2.594221,  8.351248], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2910628 , 0.12899333], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0205442905426025, 'eval/sps': 31836.48549801834}
I0728 05:32:25.466442 140199540242240 train.py:379] starting iteration 241, 98713600 steps, 2082.214614868164
I0728 05:32:33.943494 140199540242240 train.py:394] {'eval/walltime': 987.9670000076294, 'training/sps': 92091.95426993922, 'training/walltime': 1091.8968093395233, 'training/entropy_loss': Array(0.05050178, dtype=float32), 'training/policy_loss': Array(0.0078124, dtype=float32), 'training/total_loss': Array(508.3587, dtype=float32), 'training/v_loss': Array(508.3004, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29982144, 0.12875201], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.040682, 11.134901], dtype=float32), 'eval/episode_reward': Array([-1.7926711,  8.294154 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29333323, 0.13285841], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024725914001465, 'eval/sps': 31803.407917718247}
I0728 05:32:33.945536 140199540242240 train.py:379] starting iteration 242, 99123200 steps, 2090.6937091350555
I0728 05:32:42.395808 140199540242240 train.py:394] {'eval/walltime': 991.9866268634796, 'training/sps': 92540.27678695128, 'training/walltime': 1096.322990179062, 'training/entropy_loss': Array(0.04892702, dtype=float32), 'training/policy_loss': Array(0.00703618, dtype=float32), 'training/total_loss': Array(309.88568, dtype=float32), 'training/v_loss': Array(309.8297, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29880595, 0.12109377], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.948376, 10.458091], dtype=float32), 'eval/episode_reward': Array([-1.581713 ,  7.8579516], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29277903, 0.1243646 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.01962685585022, 'eval/sps': 31843.751818332854}
I0728 05:32:42.397777 140199540242240 train.py:379] starting iteration 243, 99532800 steps, 2099.1459505558014
I0728 05:32:50.849133 140199540242240 train.py:394] {'eval/walltime': 996.0073235034943, 'training/sps': 92541.57782006043, 'training/walltime': 1100.7491087913513, 'training/entropy_loss': Array(0.04836772, dtype=float32), 'training/policy_loss': Array(0.00670429, dtype=float32), 'training/total_loss': Array(234.38321, dtype=float32), 'training/v_loss': Array(234.32816, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2935354 , 0.12470479], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.437397, 10.831569], dtype=float32), 'eval/episode_reward': Array([-1.416507,  8.736371], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2867096, 0.129341 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020696640014648, 'eval/sps': 31835.279171803835}
I0728 05:32:50.851118 140199540242240 train.py:379] starting iteration 244, 99942400 steps, 2107.599292039871
I0728 05:32:59.320185 140199540242240 train.py:394] {'eval/walltime': 1000.0281882286072, 'training/sps': 92180.170917718, 'training/walltime': 1105.1925806999207, 'training/entropy_loss': Array(0.04995237, dtype=float32), 'training/policy_loss': Array(0.00727198, dtype=float32), 'training/total_loss': Array(171.95335, dtype=float32), 'training/v_loss': Array(171.89613, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29780346, 0.12467921], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.819454, 10.753143], dtype=float32), 'eval/episode_reward': Array([-1.718962 ,  8.2401085], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29188505, 0.1276555 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020864725112915, 'eval/sps': 31833.948354580734}
I0728 05:32:59.322185 140199540242240 train.py:379] starting iteration 245, 100352000 steps, 2116.0703587532043
I0728 05:33:07.796932 140199540242240 train.py:394] {'eval/walltime': 1004.0690813064575, 'training/sps': 92476.55601333654, 'training/walltime': 1109.621811389923, 'training/entropy_loss': Array(0.04966533, dtype=float32), 'training/policy_loss': Array(0.00766159, dtype=float32), 'training/total_loss': Array(768.57117, dtype=float32), 'training/v_loss': Array(768.51385, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29644686, 0.13635018], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.66991 , 11.774603], dtype=float32), 'eval/episode_reward': Array([-0.894206 ,  9.1753235], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28956962, 0.14094485], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.040893077850342, 'eval/sps': 31676.16601924368}
I0728 05:33:07.798928 140199540242240 train.py:379] starting iteration 246, 100761600 steps, 2124.5471012592316
I0728 05:33:16.249312 140199540242240 train.py:394] {'eval/walltime': 1008.0863044261932, 'training/sps': 92482.330701676, 'training/walltime': 1114.0507655143738, 'training/entropy_loss': Array(0.04167555, dtype=float32), 'training/policy_loss': Array(0.00655744, dtype=float32), 'training/total_loss': Array(479.7594, dtype=float32), 'training/v_loss': Array(479.71118, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31524816, 0.1453988 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.249937, 12.564392], dtype=float32), 'eval/episode_reward': Array([-1.3707138,  9.485046 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30885553, 0.1496953 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.017223119735718, 'eval/sps': 31862.805770275656}
I0728 05:33:16.251252 140199540242240 train.py:379] starting iteration 247, 101171200 steps, 2132.999424934387
I0728 05:33:24.712765 140199540242240 train.py:394] {'eval/walltime': 1012.1078040599823, 'training/sps': 92353.21483827502, 'training/walltime': 1118.4859116077423, 'training/entropy_loss': Array(0.0380939, dtype=float32), 'training/policy_loss': Array(0.00607067, dtype=float32), 'training/total_loss': Array(353.22745, dtype=float32), 'training/v_loss': Array(353.18326, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31497842, 0.13752255], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.339712, 11.856367], dtype=float32), 'eval/episode_reward': Array([-3.112016,  9.594702], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30927303, 0.14080743], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0214996337890625, 'eval/sps': 31828.922455824824}
I0728 05:33:24.714754 140199540242240 train.py:379] starting iteration 248, 101580800 steps, 2141.462927341461
I0728 05:33:33.177361 140199540242240 train.py:394] {'eval/walltime': 1016.1327464580536, 'training/sps': 92396.25810591069, 'training/walltime': 1122.9189915657043, 'training/entropy_loss': Array(0.03544693, dtype=float32), 'training/policy_loss': Array(0.00507525, dtype=float32), 'training/total_loss': Array(250.20572, dtype=float32), 'training/v_loss': Array(250.16519, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29205945, 0.13410062], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.315197, 11.56901 ], dtype=float32), 'eval/episode_reward': Array([-1.0685053,  8.580272 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28517962, 0.13845067], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024942398071289, 'eval/sps': 31801.697351330116}
I0728 05:33:33.179373 140199540242240 train.py:379] starting iteration 249, 101990400 steps, 2149.9275467395782
I0728 05:33:41.609934 140199540242240 train.py:394] {'eval/walltime': 1020.1467769145966, 'training/sps': 92841.48937256778, 'training/walltime': 1127.330812215805, 'training/entropy_loss': Array(0.03590456, dtype=float32), 'training/policy_loss': Array(0.00488713, dtype=float32), 'training/total_loss': Array(183.39856, dtype=float32), 'training/v_loss': Array(183.35776, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28415468, 0.12645046], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.635885, 10.946282], dtype=float32), 'eval/episode_reward': Array([-0.72611547,  7.4674697 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27724248, 0.13081221], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.014030456542969, 'eval/sps': 31888.14867893113}
I0728 05:33:41.611943 140199540242240 train.py:379] starting iteration 250, 102400000 steps, 2158.3601171970367
I0728 05:33:50.060916 140199540242240 train.py:394] {'eval/walltime': 1024.1604731082916, 'training/sps': 92447.8625277978, 'training/walltime': 1131.7614176273346, 'training/entropy_loss': Array(0.03628512, dtype=float32), 'training/policy_loss': Array(0.00611202, dtype=float32), 'training/total_loss': Array(774.7089, dtype=float32), 'training/v_loss': Array(774.6665, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2835155 , 0.13443984], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.576805, 11.607115], dtype=float32), 'eval/episode_reward': Array([-0.7376236,  8.762456 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2763986 , 0.13873139], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.013696193695068, 'eval/sps': 31890.80434166127}
I0728 05:33:50.159413 140199540242240 train.py:379] starting iteration 251, 102809600 steps, 2166.90758395195
I0728 05:33:58.587640 140199540242240 train.py:394] {'eval/walltime': 1028.1734416484833, 'training/sps': 92864.7199593857, 'training/walltime': 1136.1721346378326, 'training/entropy_loss': Array(0.02793407, dtype=float32), 'training/policy_loss': Array(0.00528185, dtype=float32), 'training/total_loss': Array(481.82526, dtype=float32), 'training/v_loss': Array(481.792, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28550583, 0.11960644], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.749744, 10.404163], dtype=float32), 'eval/episode_reward': Array([-0.26208922,  8.8672285 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2786582 , 0.12418012], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.01296854019165, 'eval/sps': 31896.586957516243}
I0728 05:33:58.589750 140199540242240 train.py:379] starting iteration 252, 103219200 steps, 2175.3379237651825
I0728 05:34:07.040817 140199540242240 train.py:394] {'eval/walltime': 1032.1870710849762, 'training/sps': 92401.7047007278, 'training/walltime': 1140.604953289032, 'training/entropy_loss': Array(0.02228113, dtype=float32), 'training/policy_loss': Array(0.00465071, dtype=float32), 'training/total_loss': Array(334.54584, dtype=float32), 'training/v_loss': Array(334.51886, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28644228, 0.12831746], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.796482, 11.078688], dtype=float32), 'eval/episode_reward': Array([-1.1709251,  8.413022 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27945557, 0.132864  ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.01362943649292, 'eval/sps': 31891.33476952109}
I0728 05:34:07.042811 140199540242240 train.py:379] starting iteration 253, 103628800 steps, 2183.790984392166
I0728 05:34:15.513799 140199540242240 train.py:394] {'eval/walltime': 1036.2090857028961, 'training/sps': 92159.09583406601, 'training/walltime': 1145.0494413375854, 'training/entropy_loss': Array(0.01879673, dtype=float32), 'training/policy_loss': Array(0.00367631, dtype=float32), 'training/total_loss': Array(244.15645, dtype=float32), 'training/v_loss': Array(244.13397, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27991134, 0.13367468], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.193008 , 11.5315275], dtype=float32), 'eval/episode_reward': Array([-0.47324783,  8.546821  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27242327, 0.13868551], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022014617919922, 'eval/sps': 31824.847038024483}
I0728 05:34:15.515754 140199540242240 train.py:379] starting iteration 254, 104038400 steps, 2192.263927936554
I0728 05:34:23.994928 140199540242240 train.py:394] {'eval/walltime': 1040.2568972110748, 'training/sps': 92527.45783451785, 'training/walltime': 1149.4762353897095, 'training/entropy_loss': Array(0.01962499, dtype=float32), 'training/policy_loss': Array(0.00435932, dtype=float32), 'training/total_loss': Array(177.60252, dtype=float32), 'training/v_loss': Array(177.57854, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.252461  , 0.11547019], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.891748, 10.023129], dtype=float32), 'eval/episode_reward': Array([1.4128174, 8.023127 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2444283 , 0.12044858], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.047811508178711, 'eval/sps': 31622.025813547047}
I0728 05:34:24.768747 140199540242240 train.py:410] total steps: 104448000
