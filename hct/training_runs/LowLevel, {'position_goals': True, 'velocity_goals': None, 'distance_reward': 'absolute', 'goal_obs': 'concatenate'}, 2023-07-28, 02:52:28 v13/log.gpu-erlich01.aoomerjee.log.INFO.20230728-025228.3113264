I0728 02:52:28.631665 139877494024000 low_level_env.py:188] Initialising environment...
I0728 02:53:08.701709 139877494024000 low_level_env.py:293] Environment initialised.
I0728 02:53:08.706097 139877494024000 train.py:118] JAX is running on GPU.
I0728 02:53:08.706207 139877494024000 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 02:53:17.464624 139877494024000 train.py:367] Running initial eval
I0728 02:53:34.452742 139877494024000 train.py:373] {'eval/walltime': 16.849515438079834, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27951884, 0.10875069], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.556236,  9.389715], dtype=float32), 'eval/episode_reward': Array([-23539.594,   9373.456], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27480876, 0.11140298], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 16.849515438079834, 'eval/sps': 7596.657629139918}
I0728 02:53:34.454708 139877494024000 train.py:379] starting iteration 0, 0 steps, 25.748637914657593
I0728 02:54:45.699437 139877494024000 train.py:394] {'eval/walltime': 21.01778221130371, 'training/sps': 6107.403722766069, 'training/walltime': 67.06614112854004, 'training/entropy_loss': Array(-0.49456656, dtype=float32), 'training/policy_loss': Array(0.02201213, dtype=float32), 'training/total_loss': Array(136096.84, dtype=float32), 'training/v_loss': Array(136097.31, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2859724 , 0.09421283], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.07997 ,  8.110438], dtype=float32), 'eval/episode_reward': Array([-24090.598,   8127.894], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2809691 , 0.09778643], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.168266773223877, 'eval/sps': 30708.20726308756}
I0728 02:54:45.736740 139877494024000 train.py:379] starting iteration 1, 409600 steps, 97.03068232536316
I0728 02:55:12.289515 139877494024000 train.py:394] {'eval/walltime': 25.410552978515625, 'training/sps': 18490.630204330017, 'training/walltime': 89.21790099143982, 'training/entropy_loss': Array(-0.5312508, dtype=float32), 'training/policy_loss': Array(0.00227224, dtype=float32), 'training/total_loss': Array(142742.84, dtype=float32), 'training/v_loss': Array(142743.38, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28592426, 0.09445452], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.137295,  8.0881  ], dtype=float32), 'eval/episode_reward': Array([-24117.496,   8061.705], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28171343, 0.09635349], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.392770767211914, 'eval/sps': 29138.784330701925}
I0728 02:55:12.293198 139877494024000 train.py:379] starting iteration 2, 819200 steps, 123.5871434211731
I0728 02:55:39.072170 139877494024000 train.py:394] {'eval/walltime': 29.841098308563232, 'training/sps': 18334.321061314695, 'training/walltime': 111.55851554870605, 'training/entropy_loss': Array(-0.54306424, dtype=float32), 'training/policy_loss': Array(0.00028926, dtype=float32), 'training/total_loss': Array(148241.44, dtype=float32), 'training/v_loss': Array(148242., dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27501407, 0.1052541 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.274982 ,  8.9878235], dtype=float32), 'eval/episode_reward': Array([-23255.445,   8945.57 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2706638 , 0.10733148], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.430545330047607, 'eval/sps': 28890.348809189276}
I0728 02:55:39.075529 139877494024000 train.py:379] starting iteration 3, 1228800 steps, 150.36947512626648
I0728 02:56:06.564654 139877494024000 train.py:394] {'eval/walltime': 34.23651146888733, 'training/sps': 17742.670232302688, 'training/walltime': 134.64410495758057, 'training/entropy_loss': Array(-0.54582727, dtype=float32), 'training/policy_loss': Array(4.749886e-05, dtype=float32), 'training/total_loss': Array(153259.84, dtype=float32), 'training/v_loss': Array(153260.4, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27676278, 0.09732103], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.434256,  8.386512], dtype=float32), 'eval/episode_reward': Array([-23445.848,   8374.358], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2727838, 0.0995381], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.395413160324097, 'eval/sps': 29121.26695060491}
I0728 02:56:06.568168 139877494024000 train.py:379] starting iteration 4, 1638400 steps, 177.86211323738098
I0728 02:56:34.573935 139877494024000 train.py:394] {'eval/walltime': 38.667510747909546, 'training/sps': 17380.423806145733, 'training/walltime': 158.21084904670715, 'training/entropy_loss': Array(-0.54599255, dtype=float32), 'training/policy_loss': Array(8.2786384e-05, dtype=float32), 'training/total_loss': Array(156468.44, dtype=float32), 'training/v_loss': Array(156468.97, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26719773, 0.10094577], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.577251,  8.664007], dtype=float32), 'eval/episode_reward': Array([-22571.48 ,   8666.087], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2627908 , 0.10383228], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.430999279022217, 'eval/sps': 28887.38903795209}
I0728 02:56:34.577453 139877494024000 train.py:379] starting iteration 5, 2048000 steps, 205.87139892578125
I0728 02:57:02.900421 139877494024000 train.py:394] {'eval/walltime': 43.06851840019226, 'training/sps': 17128.354458979808, 'training/walltime': 182.12441301345825, 'training/entropy_loss': Array(-0.5463086, dtype=float32), 'training/policy_loss': Array(-0.00012778, dtype=float32), 'training/total_loss': Array(127744.09, dtype=float32), 'training/v_loss': Array(127744.64, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.275843  , 0.09781188], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.343204,  8.39808 ], dtype=float32), 'eval/episode_reward': Array([-23339.184,   8388.267], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27129024, 0.10024191], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.401007652282715, 'eval/sps': 29084.24845242179}
I0728 02:57:02.903907 139877494024000 train.py:379] starting iteration 6, 2457600 steps, 234.19785165786743
I0728 02:57:31.357716 139877494024000 train.py:394] {'eval/walltime': 47.459362268447876, 'training/sps': 17027.882898773925, 'training/walltime': 206.17907691001892, 'training/entropy_loss': Array(-0.5463133, dtype=float32), 'training/policy_loss': Array(-0.0002584, dtype=float32), 'training/total_loss': Array(133138.69, dtype=float32), 'training/v_loss': Array(133139.23, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2941221, 0.0968316], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.868952,  8.239254], dtype=float32), 'eval/episode_reward': Array([-24882.14 ,   8242.774], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2905734 , 0.09850705], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.390843868255615, 'eval/sps': 29151.571734399102}
I0728 02:57:31.361245 139877494024000 train.py:379] starting iteration 7, 2867200 steps, 262.65519070625305
I0728 02:57:59.893424 139877494024000 train.py:394] {'eval/walltime': 51.86565041542053, 'training/sps': 16983.231554760256, 'training/walltime': 230.29698395729065, 'training/entropy_loss': Array(-0.5463263, dtype=float32), 'training/policy_loss': Array(7.231205e-05, dtype=float32), 'training/total_loss': Array(138647.48, dtype=float32), 'training/v_loss': Array(138648.03, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27051896, 0.09819899], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.930527,  8.388388], dtype=float32), 'eval/episode_reward': Array([-22903.617,   8346.295], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2661393 , 0.10086247], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.406288146972656, 'eval/sps': 29049.393895844623}
I0728 02:57:59.896872 139877494024000 train.py:379] starting iteration 8, 3276800 steps, 291.1908178329468
I0728 02:58:28.493867 139877494024000 train.py:394] {'eval/walltime': 56.253358364105225, 'training/sps': 16924.55649492485, 'training/walltime': 254.4985044002533, 'training/entropy_loss': Array(-0.5467367, dtype=float32), 'training/policy_loss': Array(-0.00022039, dtype=float32), 'training/total_loss': Array(143384.44, dtype=float32), 'training/v_loss': Array(143384.98, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28916848, 0.10322733], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.500702,  8.842285], dtype=float32), 'eval/episode_reward': Array([-24485.586,   8846.105], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2856692 , 0.10534768], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.387707948684692, 'eval/sps': 29172.406526822437}
I0728 02:58:28.497383 139877494024000 train.py:379] starting iteration 9, 3686400 steps, 319.7913281917572
I0728 02:58:57.199380 139877494024000 train.py:394] {'eval/walltime': 60.65489935874939, 'training/sps': 16861.55655081809, 'training/walltime': 278.79044914245605, 'training/entropy_loss': Array(-0.5462735, dtype=float32), 'training/policy_loss': Array(5.6501904e-07, dtype=float32), 'training/total_loss': Array(143474.9, dtype=float32), 'training/v_loss': Array(143475.45, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27026248, 0.09575229], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.87659 ,  8.246714], dtype=float32), 'eval/episode_reward': Array([-22854.203,   8227.036], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26578078, 0.09868942], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.401540994644165, 'eval/sps': 29080.724263559412}
I0728 02:58:57.202787 139877494024000 train.py:379] starting iteration 10, 4096000 steps, 348.4967317581177
I0728 02:59:25.899799 139877494024000 train.py:394] {'eval/walltime': 65.05059146881104, 'training/sps': 16860.498630569666, 'training/walltime': 303.083918094635, 'training/entropy_loss': Array(-0.54657894, dtype=float32), 'training/policy_loss': Array(-0.00032983, dtype=float32), 'training/total_loss': Array(112702.83, dtype=float32), 'training/v_loss': Array(112703.375, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26081637, 0.1065181 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.102486,  9.135633], dtype=float32), 'eval/episode_reward': Array([-22099.295,   9111.743], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25630516, 0.11009702], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.3956921100616455, 'eval/sps': 29119.41892085907}
I0728 02:59:25.903212 139877494024000 train.py:379] starting iteration 11, 4505600 steps, 377.19715785980225
I0728 02:59:54.648222 139877494024000 train.py:394] {'eval/walltime': 69.47606682777405, 'training/sps': 16848.096964027285, 'training/walltime': 327.3952691555023, 'training/entropy_loss': Array(-0.5464804, dtype=float32), 'training/policy_loss': Array(-0.00023024, dtype=float32), 'training/total_loss': Array(117421.79, dtype=float32), 'training/v_loss': Array(117422.336, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2817629 , 0.10431884], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.824177,  8.919121], dtype=float32), 'eval/episode_reward': Array([-23804.645,   8858.208], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2772804 , 0.10694522], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.425475358963013, 'eval/sps': 28923.446549252338}
I0728 02:59:54.651634 139877494024000 train.py:379] starting iteration 12, 4915200 steps, 405.94558000564575
I0728 03:00:23.382547 139877494024000 train.py:394] {'eval/walltime': 73.86353826522827, 'training/sps': 16831.712779428275, 'training/walltime': 351.7302851676941, 'training/entropy_loss': Array(-0.54640836, dtype=float32), 'training/policy_loss': Array(-0.0004361, dtype=float32), 'training/total_loss': Array(119634.86, dtype=float32), 'training/v_loss': Array(119635.41, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2937302 , 0.09682063], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.798815,  8.389303], dtype=float32), 'eval/episode_reward': Array([-24780.664 ,   8370.8955], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28922528, 0.09974693], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.387471437454224, 'eval/sps': 29173.979095866303}
I0728 03:00:23.385928 139877494024000 train.py:379] starting iteration 13, 5324800 steps, 434.6798732280731
I0728 03:00:52.102926 139877494024000 train.py:394] {'eval/walltime': 78.25946140289307, 'training/sps': 16847.087485600805, 'training/walltime': 376.0430929660797, 'training/entropy_loss': Array(-0.5465131, dtype=float32), 'training/policy_loss': Array(-8.215162e-05, dtype=float32), 'training/total_loss': Array(121064.766, dtype=float32), 'training/v_loss': Array(121065.31, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28537303, 0.09202314], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.133615 ,  7.9471397], dtype=float32), 'eval/episode_reward': Array([-24124.86 ,   7919.833], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28124672, 0.09415263], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.395923137664795, 'eval/sps': 29117.888550707517}
I0728 03:00:52.106421 139877494024000 train.py:379] starting iteration 14, 5734400 steps, 463.4003667831421
I0728 03:01:20.853394 139877494024000 train.py:394] {'eval/walltime': 82.64995312690735, 'training/sps': 16823.232634225424, 'training/walltime': 400.39037561416626, 'training/entropy_loss': Array(-0.54656684, dtype=float32), 'training/policy_loss': Array(-0.00035974, dtype=float32), 'training/total_loss': Array(120492.73, dtype=float32), 'training/v_loss': Array(120493.27, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2824939 , 0.09704612], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.85511 ,  8.400011], dtype=float32), 'eval/episode_reward': Array([-23863.176,   8375.02 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27749917, 0.10041094], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.390491724014282, 'eval/sps': 29153.90986842995}
I0728 03:01:20.859744 139877494024000 train.py:379] starting iteration 15, 6144000 steps, 492.1536898612976
I0728 03:01:49.572846 139877494024000 train.py:394] {'eval/walltime': 87.03714847564697, 'training/sps': 16843.576566398613, 'training/walltime': 424.70825123786926, 'training/entropy_loss': Array(-0.5466869, dtype=float32), 'training/policy_loss': Array(-8.291402e-05, dtype=float32), 'training/total_loss': Array(92889.4, dtype=float32), 'training/v_loss': Array(92889.94, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27896196, 0.08965389], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.588924,  7.646533], dtype=float32), 'eval/episode_reward': Array([-23578.56 ,   7617.777], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27484602, 0.09164426], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.387195348739624, 'eval/sps': 29175.815031070477}
I0728 03:01:49.576315 139877494024000 train.py:379] starting iteration 16, 6553600 steps, 520.8702590465546
I0728 03:02:18.309668 139877494024000 train.py:394] {'eval/walltime': 91.43661642074585, 'training/sps': 16837.803481421735, 'training/walltime': 449.034464597702, 'training/entropy_loss': Array(-0.5468975, dtype=float32), 'training/policy_loss': Array(-0.00017346, dtype=float32), 'training/total_loss': Array(96416.59, dtype=float32), 'training/v_loss': Array(96417.14, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27780628, 0.0958251 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.451324,  8.179456], dtype=float32), 'eval/episode_reward': Array([-23470.049,   8159.915], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2734583 , 0.09797187], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.399467945098877, 'eval/sps': 29094.427234683088}
I0728 03:02:18.313063 139877494024000 train.py:379] starting iteration 17, 6963200 steps, 549.6070082187653
I0728 03:02:47.081741 139877494024000 train.py:394] {'eval/walltime': 95.85287499427795, 'training/sps': 16825.562050770353, 'training/walltime': 473.37837648391724, 'training/entropy_loss': Array(-0.5468691, dtype=float32), 'training/policy_loss': Array(-0.00026934, dtype=float32), 'training/total_loss': Array(97887.17, dtype=float32), 'training/v_loss': Array(97887.72, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27052534, 0.09023125], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.869818,  7.774314], dtype=float32), 'eval/episode_reward': Array([-22878.047 ,   7768.4604], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2661728 , 0.09219521], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4162585735321045, 'eval/sps': 28983.810134474566}
I0728 03:02:47.085202 139877494024000 train.py:379] starting iteration 18, 7372800 steps, 578.3791472911835
I0728 03:03:15.800238 139877494024000 train.py:394] {'eval/walltime': 100.24579215049744, 'training/sps': 16846.55355182133, 'training/walltime': 497.6919548511505, 'training/entropy_loss': Array(-0.54640454, dtype=float32), 'training/policy_loss': Array(-0.00023813, dtype=float32), 'training/total_loss': Array(98005.484, dtype=float32), 'training/v_loss': Array(98006.03, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28158334, 0.10590489], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.79108 ,  9.089874], dtype=float32), 'eval/episode_reward': Array([-23798.88 ,   9061.286], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2770439 , 0.10848317], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.392917156219482, 'eval/sps': 29137.813313592287}
I0728 03:03:15.803606 139877494024000 train.py:379] starting iteration 19, 7782400 steps, 607.0975518226624
I0728 03:03:44.505869 139877494024000 train.py:394] {'eval/walltime': 104.63795614242554, 'training/sps': 16854.413493239965, 'training/walltime': 521.9941947460175, 'training/entropy_loss': Array(-0.5466255, dtype=float32), 'training/policy_loss': Array(-0.00016703, dtype=float32), 'training/total_loss': Array(97908.14, dtype=float32), 'training/v_loss': Array(97908.69, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2729696 , 0.10281353], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.03012 ,  8.811139], dtype=float32), 'eval/episode_reward': Array([-23021.848,   8796.983], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26767957, 0.10687177], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.392163991928101, 'eval/sps': 29142.809839349768}
I0728 03:03:44.509268 139877494024000 train.py:379] starting iteration 20, 8192000 steps, 635.8032119274139
I0728 03:04:13.296355 139877494024000 train.py:394] {'eval/walltime': 109.05950379371643, 'training/sps': 16816.076244317876, 'training/walltime': 546.3518388271332, 'training/entropy_loss': Array(-0.5464129, dtype=float32), 'training/policy_loss': Array(-0.00011602, dtype=float32), 'training/total_loss': Array(71025.59, dtype=float32), 'training/v_loss': Array(71026.14, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27815056, 0.09436445], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.536003,  8.116618], dtype=float32), 'eval/episode_reward': Array([-23528.422 ,   8094.4185], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27378404, 0.09643365], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4215476512908936, 'eval/sps': 28949.139553574583}
I0728 03:04:13.299762 139877494024000 train.py:379] starting iteration 21, 8601600 steps, 664.5937073230743
I0728 03:04:42.025627 139877494024000 train.py:394] {'eval/walltime': 113.45457458496094, 'training/sps': 16839.99497501702, 'training/walltime': 570.6748864650726, 'training/entropy_loss': Array(-0.5466749, dtype=float32), 'training/policy_loss': Array(-0.00022913, dtype=float32), 'training/total_loss': Array(73155.12, dtype=float32), 'training/v_loss': Array(73155.67, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28271562, 0.09875304], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.922485,  8.504392], dtype=float32), 'eval/episode_reward': Array([-23911.781,   8480.697], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27791536, 0.10222097], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.395070791244507, 'eval/sps': 29123.53545134948}
I0728 03:04:42.028975 139877494024000 train.py:379] starting iteration 22, 9011200 steps, 693.3229215145111
I0728 03:05:10.798123 139877494024000 train.py:394] {'eval/walltime': 117.86207604408264, 'training/sps': 16818.67501939204, 'training/walltime': 595.0287668704987, 'training/entropy_loss': Array(-0.54690045, dtype=float32), 'training/policy_loss': Array(-0.00026731, dtype=float32), 'training/total_loss': Array(74968.5, dtype=float32), 'training/v_loss': Array(74969.05, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26185608, 0.09792631], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.077538,  8.447704], dtype=float32), 'eval/episode_reward': Array([-22056.865,   8434.187], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2565292 , 0.10100956], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.407501459121704, 'eval/sps': 29041.39707885813}
I0728 03:05:10.801487 139877494024000 train.py:379] starting iteration 23, 9420800 steps, 722.0954322814941
I0728 03:05:39.574018 139877494024000 train.py:394] {'eval/walltime': 122.26425957679749, 'training/sps': 16812.566226528663, 'training/walltime': 619.391496181488, 'training/entropy_loss': Array(-0.54644823, dtype=float32), 'training/policy_loss': Array(-0.00021885, dtype=float32), 'training/total_loss': Array(75827.28, dtype=float32), 'training/v_loss': Array(75827.82, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26992422, 0.10595654], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.844229,  9.072507], dtype=float32), 'eval/episode_reward': Array([-22824.393,   9073.389], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.265177 , 0.1085998], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.402183532714844, 'eval/sps': 29076.479671682817}
I0728 03:05:39.577423 139877494024000 train.py:379] starting iteration 24, 9830400 steps, 750.8713688850403
I0728 03:06:08.358374 139877494024000 train.py:394] {'eval/walltime': 126.68065333366394, 'training/sps': 16816.716562185902, 'training/walltime': 643.748212814331, 'training/entropy_loss': Array(-0.54669553, dtype=float32), 'training/policy_loss': Array(-0.00024399, dtype=float32), 'training/total_loss': Array(74724.67, dtype=float32), 'training/v_loss': Array(74725.22, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27513278, 0.10161056], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.323463,  8.720742], dtype=float32), 'eval/episode_reward': Array([-23298.582,   8704.306], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2711354, 0.1033036], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.416393756866455, 'eval/sps': 28982.922956312505}
I0728 03:06:08.361805 139877494024000 train.py:379] starting iteration 25, 10240000 steps, 779.6557500362396
I0728 03:06:37.114825 139877494024000 train.py:394] {'eval/walltime': 131.06986165046692, 'training/sps': 16817.00941275153, 'training/walltime': 668.1045053005219, 'training/entropy_loss': Array(-0.547082, dtype=float32), 'training/policy_loss': Array(-0.00029642, dtype=float32), 'training/total_loss': Array(51011.28, dtype=float32), 'training/v_loss': Array(51011.83, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26938313, 0.10649734], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.759998,  9.129998], dtype=float32), 'eval/episode_reward': Array([-22773.521,   9115.901], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26495832, 0.10936048], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.3892083168029785, 'eval/sps': 29162.434489605846}
I0728 03:06:37.185182 139877494024000 train.py:379] starting iteration 26, 10649600 steps, 808.4791095256805
I0728 03:07:05.960308 139877494024000 train.py:394] {'eval/walltime': 135.46393942832947, 'training/sps': 16805.45244241819, 'training/walltime': 692.4775474071503, 'training/entropy_loss': Array(-0.54665685, dtype=float32), 'training/policy_loss': Array(-0.000271, dtype=float32), 'training/total_loss': Array(53273.156, dtype=float32), 'training/v_loss': Array(53273.707, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28275314, 0.10466966], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.891632,  9.023065], dtype=float32), 'eval/episode_reward': Array([-23876.262,   9015.548], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.277838  , 0.10847894], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.394077777862549, 'eval/sps': 29130.117050923982}
I0728 03:07:05.964051 139877494024000 train.py:379] starting iteration 27, 11059200 steps, 837.2579960823059
I0728 03:07:34.677343 139877494024000 train.py:394] {'eval/walltime': 139.85543394088745, 'training/sps': 16846.152628128664, 'training/walltime': 716.791704416275, 'training/entropy_loss': Array(-0.5467641, dtype=float32), 'training/policy_loss': Array(-0.00028483, dtype=float32), 'training/total_loss': Array(52668.76, dtype=float32), 'training/v_loss': Array(52669.305, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27863264, 0.09107829], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.55955  ,  7.8757215], dtype=float32), 'eval/episode_reward': Array([-23561.95 ,   7867.354], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27425957, 0.09376492], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.391494512557983, 'eval/sps': 29147.252634375218}
I0728 03:07:34.680879 139877494024000 train.py:379] starting iteration 28, 11468800 steps, 865.9748237133026
I0728 03:08:03.445051 139877494024000 train.py:394] {'eval/walltime': 144.2701382637024, 'training/sps': 16827.108541151745, 'training/walltime': 741.133378982544, 'training/entropy_loss': Array(-0.54654795, dtype=float32), 'training/policy_loss': Array(-0.00019925, dtype=float32), 'training/total_loss': Array(52850.58, dtype=float32), 'training/v_loss': Array(52851.117, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28445232, 0.10104414], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.074154,  8.685223], dtype=float32), 'eval/episode_reward': Array([-24066.352,   8665.17 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28020436, 0.10331637], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.414704322814941, 'eval/sps': 28994.01423975401}
I0728 03:08:03.448518 139877494024000 train.py:379] starting iteration 29, 11878400 steps, 894.7424626350403
I0728 03:08:32.226905 139877494024000 train.py:394] {'eval/walltime': 148.6654965877533, 'training/sps': 16804.34369182289, 'training/walltime': 765.5080292224884, 'training/entropy_loss': Array(-0.5469153, dtype=float32), 'training/policy_loss': Array(-0.00036593, dtype=float32), 'training/total_loss': Array(52809.914, dtype=float32), 'training/v_loss': Array(52810.46, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28865093, 0.09510688], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.41094 ,  8.142266], dtype=float32), 'eval/episode_reward': Array([-24424.371,   8145.941], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2850212 , 0.09665132], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.395358324050903, 'eval/sps': 29121.630266091957}
I0728 03:08:32.230352 139877494024000 train.py:379] starting iteration 30, 12288000 steps, 923.5242924690247
I0728 03:09:01.039904 139877494024000 train.py:394] {'eval/walltime': 153.08637833595276, 'training/sps': 16800.33927164655, 'training/walltime': 789.8884892463684, 'training/entropy_loss': Array(-0.5469129, dtype=float32), 'training/policy_loss': Array(-0.00020196, dtype=float32), 'training/total_loss': Array(32857.582, dtype=float32), 'training/v_loss': Array(32858.133, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2802686 , 0.10368339], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.71714 ,  8.930105], dtype=float32), 'eval/episode_reward': Array([-23724.98 ,   8907.633], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27617157, 0.10584639], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.420881748199463, 'eval/sps': 28953.50006865301}
I0728 03:09:01.043275 139877494024000 train.py:379] starting iteration 31, 12697600 steps, 952.3372201919556
I0728 03:09:29.807872 139877494024000 train.py:394] {'eval/walltime': 157.47631096839905, 'training/sps': 16811.126044119053, 'training/walltime': 814.2533056735992, 'training/entropy_loss': Array(-0.5468477, dtype=float32), 'training/policy_loss': Array(-0.00011524, dtype=float32), 'training/total_loss': Array(34342.37, dtype=float32), 'training/v_loss': Array(34342.914, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29310727, 0.09800991], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.770363 ,  8.4591675], dtype=float32), 'eval/episode_reward': Array([-24777.094,   8417.431], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28909144, 0.10035919], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.389932632446289, 'eval/sps': 29157.622842305904}
I0728 03:09:29.811373 139877494024000 train.py:379] starting iteration 32, 13107200 steps, 981.1053185462952
I0728 03:09:58.605884 139877494024000 train.py:394] {'eval/walltime': 161.89852905273438, 'training/sps': 16811.70939190948, 'training/walltime': 838.6172766685486, 'training/entropy_loss': Array(-0.5463933, dtype=float32), 'training/policy_loss': Array(-0.00016167, dtype=float32), 'training/total_loss': Array(34600.066, dtype=float32), 'training/v_loss': Array(34600.61, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2826166 , 0.10083184], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.946613,  8.691516], dtype=float32), 'eval/episode_reward': Array([-23943.965,   8697.705], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27835393, 0.10390586], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.422218084335327, 'eval/sps': 28944.75070178245}
I0728 03:09:58.609332 139877494024000 train.py:379] starting iteration 33, 13516800 steps, 1009.9032773971558
I0728 03:10:27.407265 139877494024000 train.py:394] {'eval/walltime': 166.31157803535461, 'training/sps': 16802.906232447836, 'training/walltime': 862.9940121173859, 'training/entropy_loss': Array(-0.5467037, dtype=float32), 'training/policy_loss': Array(-0.00011573, dtype=float32), 'training/total_loss': Array(34922.67, dtype=float32), 'training/v_loss': Array(34923.22, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2766858 , 0.10486016], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.359932,  9.019114], dtype=float32), 'eval/episode_reward': Array([-23352.867,   9025.337], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2721836 , 0.10737158], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.413048982620239, 'eval/sps': 29004.889930770776}
I0728 03:10:27.410657 139877494024000 train.py:379] starting iteration 34, 13926400 steps, 1038.7046015262604
I0728 03:10:56.191078 139877494024000 train.py:394] {'eval/walltime': 170.6927945613861, 'training/sps': 16792.797058997778, 'training/walltime': 887.3854222297668, 'training/entropy_loss': Array(-0.5469124, dtype=float32), 'training/policy_loss': Array(-0.00013161, dtype=float32), 'training/total_loss': Array(34805.89, dtype=float32), 'training/v_loss': Array(34806.438, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25322178, 0.11166546], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.366936,  9.591515], dtype=float32), 'eval/episode_reward': Array([-21361.775,   9571.003], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24816954, 0.11474176], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.381216526031494, 'eval/sps': 29215.62977759111}
I0728 03:10:56.194369 139877494024000 train.py:379] starting iteration 35, 14336000 steps, 1067.4883139133453
I0728 03:11:25.005323 139877494024000 train.py:394] {'eval/walltime': 175.12808346748352, 'training/sps': 16809.26259649549, 'training/walltime': 911.7529397010803, 'training/entropy_loss': Array(-0.54685235, dtype=float32), 'training/policy_loss': Array(-0.00017295, dtype=float32), 'training/total_loss': Array(21104.3, dtype=float32), 'training/v_loss': Array(21104.848, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26273686, 0.09009278], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.21464  ,  7.7307124], dtype=float32), 'eval/episode_reward': Array([-22216.367 ,   7723.9517], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25829136, 0.09264148], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.435288906097412, 'eval/sps': 28859.450355991925}
I0728 03:11:25.008650 139877494024000 train.py:379] starting iteration 36, 14745600 steps, 1096.302594423294
I0728 03:11:53.770751 139877494024000 train.py:394] {'eval/walltime': 179.5136694908142, 'training/sps': 16808.862130089823, 'training/walltime': 936.1210377216339, 'training/entropy_loss': Array(-0.5468555, dtype=float32), 'training/policy_loss': Array(-6.8236346e-05, dtype=float32), 'training/total_loss': Array(22954.625, dtype=float32), 'training/v_loss': Array(22955.172, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28887787, 0.10032133], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.441341,  8.558755], dtype=float32), 'eval/episode_reward': Array([-24412.691,   8520.983], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28494626, 0.10250486], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.3855860233306885, 'eval/sps': 29186.52132669576}
I0728 03:11:53.774196 139877494024000 train.py:379] starting iteration 37, 15155200 steps, 1125.0681412220001
I0728 03:12:22.566534 139877494024000 train.py:394] {'eval/walltime': 183.94360089302063, 'training/sps': 16818.440889289177, 'training/walltime': 960.4752571582794, 'training/entropy_loss': Array(-0.5468023, dtype=float32), 'training/policy_loss': Array(-0.00014434, dtype=float32), 'training/total_loss': Array(24333.777, dtype=float32), 'training/v_loss': Array(24334.324, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28609574, 0.09846154], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.242935,  8.432718], dtype=float32), 'eval/episode_reward': Array([-24217.012,   8405.308], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28213894, 0.10143213], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.429931402206421, 'eval/sps': 28894.35261599015}
I0728 03:12:22.569835 139877494024000 train.py:379] starting iteration 38, 15564800 steps, 1153.8637804985046
I0728 03:12:51.326633 139877494024000 train.py:394] {'eval/walltime': 188.33866596221924, 'training/sps': 16819.145769053415, 'training/walltime': 984.8284559249878, 'training/entropy_loss': Array(-0.5466422, dtype=float32), 'training/policy_loss': Array(-0.00023185, dtype=float32), 'training/total_loss': Array(25454.363, dtype=float32), 'training/v_loss': Array(25454.91, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27740973, 0.10034546], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.445896,  8.606532], dtype=float32), 'eval/episode_reward': Array([-23471.   ,   8581.543], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27248865, 0.10327576], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.395065069198608, 'eval/sps': 29123.573368013727}
I0728 03:12:51.330068 139877494024000 train.py:379] starting iteration 39, 15974400 steps, 1182.6240129470825
I0728 03:13:20.105372 139877494024000 train.py:394] {'eval/walltime': 192.72828483581543, 'training/sps': 16802.313306906555, 'training/walltime': 1009.2060515880585, 'training/entropy_loss': Array(-0.54629946, dtype=float32), 'training/policy_loss': Array(-0.00018231, dtype=float32), 'training/total_loss': Array(25626.54, dtype=float32), 'training/v_loss': Array(25627.086, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28289676, 0.10204232], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.879438,  8.799611], dtype=float32), 'eval/episode_reward': Array([-23900.395,   8785.975], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27815256, 0.10531135], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.389618873596191, 'eval/sps': 29159.706955409572}
I0728 03:13:20.108717 139877494024000 train.py:379] starting iteration 40, 16384000 steps, 1211.4026627540588
I0728 03:13:48.842774 139877494024000 train.py:394] {'eval/walltime': 197.1189329624176, 'training/sps': 16831.349169291156, 'training/walltime': 1033.5415933132172, 'training/entropy_loss': Array(-0.54676133, dtype=float32), 'training/policy_loss': Array(-0.0001051, dtype=float32), 'training/total_loss': Array(18589.084, dtype=float32), 'training/v_loss': Array(18589.63, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28142226, 0.09728813], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.786774,  8.379106], dtype=float32), 'eval/episode_reward': Array([-23785.057,   8363.135], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27666867, 0.10105848], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.390648126602173, 'eval/sps': 29152.87135502166}
I0728 03:13:48.846119 139877494024000 train.py:379] starting iteration 41, 16793600 steps, 1240.1400649547577
I0728 03:14:17.653303 139877494024000 train.py:394] {'eval/walltime': 201.51030158996582, 'training/sps': 16781.387092424575, 'training/walltime': 1057.9495875835419, 'training/entropy_loss': Array(-0.5468646, dtype=float32), 'training/policy_loss': Array(-0.00013151, dtype=float32), 'training/total_loss': Array(19139.754, dtype=float32), 'training/v_loss': Array(19140.3, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2963792 , 0.09184987], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.105335,  7.920439], dtype=float32), 'eval/episode_reward': Array([-25093.736,   7916.394], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29306045, 0.09357724], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.391368627548218, 'eval/sps': 29148.088183037544}
I0728 03:14:17.656664 139877494024000 train.py:379] starting iteration 42, 17203200 steps, 1268.9506087303162
I0728 03:14:46.453456 139877494024000 train.py:394] {'eval/walltime': 205.90982151031494, 'training/sps': 16794.251012345216, 'training/walltime': 1082.3388860225677, 'training/entropy_loss': Array(-0.5469308, dtype=float32), 'training/policy_loss': Array(-7.695084e-05, dtype=float32), 'training/total_loss': Array(20383.816, dtype=float32), 'training/v_loss': Array(20384.363, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26951456, 0.09774758], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.761354,  8.415661], dtype=float32), 'eval/episode_reward': Array([-22792.75 ,   8390.661], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2645759 , 0.10058837], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.399519920349121, 'eval/sps': 29094.08351760404}
I0728 03:14:46.456797 139877494024000 train.py:379] starting iteration 43, 17612800 steps, 1297.7507421970367
I0728 03:15:15.245878 139877494024000 train.py:394] {'eval/walltime': 210.30553269386292, 'training/sps': 16796.950284158556, 'training/walltime': 1106.7242650985718, 'training/entropy_loss': Array(-0.54665065, dtype=float32), 'training/policy_loss': Array(-4.582709e-05, dtype=float32), 'training/total_loss': Array(21731.625, dtype=float32), 'training/v_loss': Array(21732.172, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27135086, 0.09790254], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.944206,  8.402205], dtype=float32), 'eval/episode_reward': Array([-22951.902,   8399.808], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26632625, 0.10124902], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.395711183547974, 'eval/sps': 29119.29256841791}
I0728 03:15:15.249186 139877494024000 train.py:379] starting iteration 44, 18022400 steps, 1326.5431311130524
I0728 03:15:44.071697 139877494024000 train.py:394] {'eval/walltime': 214.7266867160797, 'training/sps': 16791.613004364688, 'training/walltime': 1131.1173951625824, 'training/entropy_loss': Array(-0.5466248, dtype=float32), 'training/policy_loss': Array(-8.674376e-05, dtype=float32), 'training/total_loss': Array(22848.947, dtype=float32), 'training/v_loss': Array(22849.496, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2639379 , 0.09537277], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.27464 ,  8.244944], dtype=float32), 'eval/episode_reward': Array([-22274.2  ,   8235.002], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25902382, 0.09807168], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.421154022216797, 'eval/sps': 28951.7169853811}
I0728 03:15:44.075080 139877494024000 train.py:379] starting iteration 45, 18432000 steps, 1355.3690254688263
I0728 03:16:12.857979 139877494024000 train.py:394] {'eval/walltime': 219.1269154548645, 'training/sps': 16804.473216590915, 'training/walltime': 1155.4918575286865, 'training/entropy_loss': Array(-0.5466102, dtype=float32), 'training/policy_loss': Array(-8.5512926e-05, dtype=float32), 'training/total_loss': Array(23717.191, dtype=float32), 'training/v_loss': Array(23717.738, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2650876, 0.1012482], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.393898,  8.661894], dtype=float32), 'eval/episode_reward': Array([-22398.748,   8639.965], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25976497, 0.10440788], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.40022873878479, 'eval/sps': 29089.39684698067}
I0728 03:16:12.861303 139877494024000 train.py:379] starting iteration 46, 18841600 steps, 1384.1552486419678
I0728 03:16:41.640212 139877494024000 train.py:394] {'eval/walltime': 223.51207876205444, 'training/sps': 16797.744845495014, 'training/walltime': 1179.8760831356049, 'training/entropy_loss': Array(-0.5469657, dtype=float32), 'training/policy_loss': Array(5.1715742e-05, dtype=float32), 'training/total_loss': Array(22582.66, dtype=float32), 'training/v_loss': Array(22583.207, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28023583, 0.09862   ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.694782,  8.460916], dtype=float32), 'eval/episode_reward': Array([-23681.738,   8437.488], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2762025 , 0.10089898], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.385163307189941, 'eval/sps': 29189.334816819795}
I0728 03:16:41.643492 139877494024000 train.py:379] starting iteration 47, 19251200 steps, 1412.9374375343323
I0728 03:17:10.415065 139877494024000 train.py:394] {'eval/walltime': 227.92424058914185, 'training/sps': 16820.260920263063, 'training/walltime': 1204.2276673316956, 'training/entropy_loss': Array(-0.5468137, dtype=float32), 'training/policy_loss': Array(-3.6631827e-05, dtype=float32), 'training/total_loss': Array(23462.074, dtype=float32), 'training/v_loss': Array(23462.621, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2816533 , 0.10039276], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.863888,  8.639826], dtype=float32), 'eval/episode_reward': Array([-23853.36 ,   8640.485], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2776625 , 0.10261308], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.412161827087402, 'eval/sps': 29010.721958150967}
I0728 03:17:10.418373 139877494024000 train.py:379] starting iteration 48, 19660800 steps, 1441.7123177051544
I0728 03:17:39.216728 139877494024000 train.py:394] {'eval/walltime': 232.3238639831543, 'training/sps': 16792.88569747343, 'training/walltime': 1228.6189486980438, 'training/entropy_loss': Array(-0.5468848, dtype=float32), 'training/policy_loss': Array(-6.7305475e-05, dtype=float32), 'training/total_loss': Array(24367.959, dtype=float32), 'training/v_loss': Array(24368.504, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2717725, 0.0916476], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.991611,  7.885851], dtype=float32), 'eval/episode_reward': Array([-22997.947,   7880.179], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26768106, 0.09391022], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.399623394012451, 'eval/sps': 29093.39926099087}
I0728 03:17:39.220201 139877494024000 train.py:379] starting iteration 49, 20070400 steps, 1470.5141468048096
I0728 03:18:07.945262 139877494024000 train.py:394] {'eval/walltime': 236.7199957370758, 'training/sps': 16841.106289128667, 'training/walltime': 1252.9403913021088, 'training/entropy_loss': Array(-0.54680455, dtype=float32), 'training/policy_loss': Array(-4.970825e-05, dtype=float32), 'training/total_loss': Array(25049.857, dtype=float32), 'training/v_loss': Array(25050.404, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26883456, 0.09981396], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.754612,  8.573718], dtype=float32), 'eval/episode_reward': Array([-22728.34 ,   8539.521], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26461732, 0.10229256], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.396131753921509, 'eval/sps': 29116.506775716938}
I0728 03:18:07.948673 139877494024000 train.py:379] starting iteration 50, 20480000 steps, 1499.242618560791
I0728 03:18:36.713546 139877494024000 train.py:394] {'eval/walltime': 241.1073739528656, 'training/sps': 16807.612994856998, 'training/walltime': 1277.3103003501892, 'training/entropy_loss': Array(-0.546893, dtype=float32), 'training/policy_loss': Array(-1.6005963e-05, dtype=float32), 'training/total_loss': Array(26056.574, dtype=float32), 'training/v_loss': Array(26057.125, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27087474, 0.09391321], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.910465,  8.071989], dtype=float32), 'eval/episode_reward': Array([-22921.078,   8059.182], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26665494, 0.09645869], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.387378215789795, 'eval/sps': 29174.598975611236}
I0728 03:18:36.767481 139877494024000 train.py:379] starting iteration 51, 20889600 steps, 1528.0614070892334
I0728 03:19:05.573755 139877494024000 train.py:394] {'eval/walltime': 245.53066873550415, 'training/sps': 16804.164694509796, 'training/walltime': 1301.6852102279663, 'training/entropy_loss': Array(-0.5465408, dtype=float32), 'training/policy_loss': Array(3.79335e-05, dtype=float32), 'training/total_loss': Array(23860.596, dtype=float32), 'training/v_loss': Array(23861.143, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26325214, 0.09637053], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.331968,  8.285129], dtype=float32), 'eval/episode_reward': Array([-22331.121,   8263.485], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2585872 , 0.09921877], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.42329478263855, 'eval/sps': 28937.705102178705}
I0728 03:19:05.577350 139877494024000 train.py:379] starting iteration 52, 21299200 steps, 1556.871295928955
I0728 03:19:34.335453 139877494024000 train.py:394] {'eval/walltime': 249.9161171913147, 'training/sps': 16811.08689250221, 'training/walltime': 1326.050083398819, 'training/entropy_loss': Array(-0.54683363, dtype=float32), 'training/policy_loss': Array(9.1151705e-06, dtype=float32), 'training/total_loss': Array(23604.56, dtype=float32), 'training/v_loss': Array(23605.107, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26453775, 0.10361608], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.303396,  8.959315], dtype=float32), 'eval/episode_reward': Array([-22315.13 ,   8933.216], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25977916, 0.10645299], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.385448455810547, 'eval/sps': 29187.4368812624}
I0728 03:19:34.338741 139877494024000 train.py:379] starting iteration 53, 21708800 steps, 1585.6326849460602
I0728 03:20:03.144027 139877494024000 train.py:394] {'eval/walltime': 254.30339694023132, 'training/sps': 16779.679860023196, 'training/walltime': 1350.4605610370636, 'training/entropy_loss': Array(-0.54685605, dtype=float32), 'training/policy_loss': Array(-1.7261766e-05, dtype=float32), 'training/total_loss': Array(23823.875, dtype=float32), 'training/v_loss': Array(23824.422, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.263301  , 0.11195093], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.298866,  9.605298], dtype=float32), 'eval/episode_reward': Array([-22296.89 ,   9587.716], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25878125, 0.11503034], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.387279748916626, 'eval/sps': 29175.253762107994}
I0728 03:20:03.147325 139877494024000 train.py:379] starting iteration 54, 22118400 steps, 1614.441269159317
I0728 03:20:31.950624 139877494024000 train.py:394] {'eval/walltime': 258.70754837989807, 'training/sps': 16793.04163780361, 'training/walltime': 1374.8516159057617, 'training/entropy_loss': Array(-0.54679215, dtype=float32), 'training/policy_loss': Array(-9.4019575e-05, dtype=float32), 'training/total_loss': Array(23960.396, dtype=float32), 'training/v_loss': Array(23960.943, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27498895, 0.10471977], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.193638,  9.094708], dtype=float32), 'eval/episode_reward': Array([-23200.736,   9064.286], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26986378, 0.10856143], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.404151439666748, 'eval/sps': 29063.487428508015}
I0728 03:20:31.953869 139877494024000 train.py:379] starting iteration 55, 22528000 steps, 1643.2478148937225
I0728 03:21:00.677568 139877494024000 train.py:394] {'eval/walltime': 263.09525203704834, 'training/sps': 16836.500211016464, 'training/walltime': 1399.1797122955322, 'training/entropy_loss': Array(-0.5467261, dtype=float32), 'training/policy_loss': Array(-9.5599455e-05, dtype=float32), 'training/total_loss': Array(23793.98, dtype=float32), 'training/v_loss': Array(23794.527, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2820884 , 0.10142937], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.884846,  8.70517 ], dtype=float32), 'eval/episode_reward': Array([-23881.95 ,   8692.067], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2782813, 0.1034231], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.3877036571502686, 'eval/sps': 29172.435059831183}
I0728 03:21:00.680935 139877494024000 train.py:379] starting iteration 56, 22937600 steps, 1671.9748809337616
I0728 03:21:29.475375 139877494024000 train.py:394] {'eval/walltime': 267.4964292049408, 'training/sps': 16796.8524062344, 'training/walltime': 1423.5652334690094, 'training/entropy_loss': Array(-0.54684275, dtype=float32), 'training/policy_loss': Array(-2.5063848e-05, dtype=float32), 'training/total_loss': Array(22705.758, dtype=float32), 'training/v_loss': Array(22706.303, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27672875, 0.1071739 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.375107,  9.212615], dtype=float32), 'eval/episode_reward': Array([-23363.758,   9195.956], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27181333, 0.11081861], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.401177167892456, 'eval/sps': 29083.12824436785}
I0728 03:21:29.478695 139877494024000 train.py:379] starting iteration 57, 23347200 steps, 1700.7726407051086
I0728 03:21:58.262825 139877494024000 train.py:394] {'eval/walltime': 271.88783025741577, 'training/sps': 16797.262647176256, 'training/walltime': 1447.950159072876, 'training/entropy_loss': Array(-0.54710966, dtype=float32), 'training/policy_loss': Array(-6.979878e-05, dtype=float32), 'training/total_loss': Array(22257.938, dtype=float32), 'training/v_loss': Array(22258.484, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28230977, 0.10203747], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.86567 ,  8.765118], dtype=float32), 'eval/episode_reward': Array([-23864.344,   8747.028], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.277623  , 0.10531099], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.391401052474976, 'eval/sps': 29147.872961377492}
I0728 03:21:58.266145 139877494024000 train.py:379] starting iteration 58, 23756800 steps, 1729.5600907802582
I0728 03:22:27.029281 139877494024000 train.py:394] {'eval/walltime': 276.2809798717499, 'training/sps': 16814.100455071515, 'training/walltime': 1472.3106653690338, 'training/entropy_loss': Array(-0.54665405, dtype=float32), 'training/policy_loss': Array(7.862658e-06, dtype=float32), 'training/total_loss': Array(22215.453, dtype=float32), 'training/v_loss': Array(22216., dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28465158, 0.10856371], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.160583,  9.285178], dtype=float32), 'eval/episode_reward': Array([-24137.668,   9294.183], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28060615, 0.11165289], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.3931496143341064, 'eval/sps': 29136.271521998155}
I0728 03:22:27.035575 139877494024000 train.py:379] starting iteration 59, 24166400 steps, 1758.3295211791992
I0728 03:22:55.850502 139877494024000 train.py:394] {'eval/walltime': 280.6813657283783, 'training/sps': 16782.29575836944, 'training/walltime': 1496.7173380851746, 'training/entropy_loss': Array(-0.54684275, dtype=float32), 'training/policy_loss': Array(5.8731434e-06, dtype=float32), 'training/total_loss': Array(22392.45, dtype=float32), 'training/v_loss': Array(22392.996, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29476985, 0.09935468], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.956287,  8.447661], dtype=float32), 'eval/episode_reward': Array([-24947.293,   8444.977], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29090917, 0.1016935 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.400385856628418, 'eval/sps': 29088.358196404573}
I0728 03:22:55.853803 139877494024000 train.py:379] starting iteration 60, 24576000 steps, 1787.1477484703064
I0728 03:23:24.649062 139877494024000 train.py:394] {'eval/walltime': 285.09995102882385, 'training/sps': 16808.185738793483, 'training/walltime': 1521.086416721344, 'training/entropy_loss': Array(-0.5469139, dtype=float32), 'training/policy_loss': Array(-4.1921627e-05, dtype=float32), 'training/total_loss': Array(21402.902, dtype=float32), 'training/v_loss': Array(21403.453, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2855013 , 0.09710531], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.209295,  8.305926], dtype=float32), 'eval/episode_reward': Array([-24197.783,   8298.904], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28165638, 0.09948814], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.418585300445557, 'eval/sps': 28968.547916703763}
I0728 03:23:24.652408 139877494024000 train.py:379] starting iteration 61, 24985600 steps, 1815.9463529586792
I0728 03:23:53.444695 139877494024000 train.py:394] {'eval/walltime': 289.49342346191406, 'training/sps': 16793.329888817243, 'training/walltime': 1545.4770529270172, 'training/entropy_loss': Array(-0.54706264, dtype=float32), 'training/policy_loss': Array(-0.00012329, dtype=float32), 'training/total_loss': Array(20083.877, dtype=float32), 'training/v_loss': Array(20084.424, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26836133, 0.10185477], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.725868,  8.696346], dtype=float32), 'eval/episode_reward': Array([-22733.129,   8680.417], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26364058, 0.10493595], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.39347243309021, 'eval/sps': 29134.13067894668}
I0728 03:23:53.448150 139877494024000 train.py:379] starting iteration 62, 25395200 steps, 1844.7420954704285
I0728 03:24:22.218316 139877494024000 train.py:394] {'eval/walltime': 293.88457918167114, 'training/sps': 16807.0795871465, 'training/walltime': 1569.8477354049683, 'training/entropy_loss': Array(-0.5467619, dtype=float32), 'training/policy_loss': Array(-4.3557513e-05, dtype=float32), 'training/total_loss': Array(19995.258, dtype=float32), 'training/v_loss': Array(19995.803, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28488111, 0.09159245], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.1696   ,  7.8723903], dtype=float32), 'eval/episode_reward': Array([-24159.027 ,   7850.1772], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28152218, 0.09401456], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.39115571975708, 'eval/sps': 29149.50144539192}
I0728 03:24:22.221843 139877494024000 train.py:379] starting iteration 63, 25804800 steps, 1873.5157883167267
I0728 03:24:51.008017 139877494024000 train.py:394] {'eval/walltime': 298.2961175441742, 'training/sps': 16810.28810773336, 'training/walltime': 1594.213766336441, 'training/entropy_loss': Array(-0.54667026, dtype=float32), 'training/policy_loss': Array(-2.6020994e-05, dtype=float32), 'training/total_loss': Array(19842.277, dtype=float32), 'training/v_loss': Array(19842.824, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26676077, 0.10807244], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.53138 ,  9.289871], dtype=float32), 'eval/episode_reward': Array([-22499.992,   9241.205], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26183322, 0.11111564], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.411538362503052, 'eval/sps': 29014.821924244676}
I0728 03:24:51.011583 139877494024000 train.py:379] starting iteration 64, 26214400 steps, 1902.3055276870728
I0728 03:25:19.770511 139877494024000 train.py:394] {'eval/walltime': 302.6908917427063, 'training/sps': 16817.576047470095, 'training/walltime': 1618.5692381858826, 'training/entropy_loss': Array(-0.5470235, dtype=float32), 'training/policy_loss': Array(9.935835e-06, dtype=float32), 'training/total_loss': Array(19820.701, dtype=float32), 'training/v_loss': Array(19821.25, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2865147 , 0.09841504], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.2425 ,  8.43709], dtype=float32), 'eval/episode_reward': Array([-24223.953,   8414.669], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28200948, 0.10078273], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.3947741985321045, 'eval/sps': 29125.500928523972}
I0728 03:25:19.773993 139877494024000 train.py:379] starting iteration 65, 26624000 steps, 1931.067939043045
I0728 03:25:48.546654 139877494024000 train.py:394] {'eval/walltime': 307.0945932865143, 'training/sps': 16814.02541550313, 'training/walltime': 1642.9298532009125, 'training/entropy_loss': Array(-0.5469425, dtype=float32), 'training/policy_loss': Array(-6.9616035e-05, dtype=float32), 'training/total_loss': Array(19402.227, dtype=float32), 'training/v_loss': Array(19402.771, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26487654, 0.09552583], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.444145,  8.141474], dtype=float32), 'eval/episode_reward': Array([-22427.621 ,   8115.6797], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26070106, 0.09772941], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.403701543807983, 'eval/sps': 29066.4566448605}
I0728 03:25:48.550075 139877494024000 train.py:379] starting iteration 66, 27033600 steps, 1959.8440201282501
I0728 03:26:17.333435 139877494024000 train.py:394] {'eval/walltime': 311.4810881614685, 'training/sps': 16794.667365100744, 'training/walltime': 1667.3185470104218, 'training/entropy_loss': Array(-0.5466825, dtype=float32), 'training/policy_loss': Array(3.0918625e-05, dtype=float32), 'training/total_loss': Array(18149.04, dtype=float32), 'training/v_loss': Array(18149.586, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27060014, 0.09811045], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.81644 ,  8.426775], dtype=float32), 'eval/episode_reward': Array([-22843.023,   8410.66 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26619822, 0.10016456], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.386494874954224, 'eval/sps': 29180.474079850777}
I0728 03:26:17.336909 139877494024000 train.py:379] starting iteration 67, 27443200 steps, 1988.6308546066284
I0728 03:26:46.051748 139877494024000 train.py:394] {'eval/walltime': 315.87308716773987, 'training/sps': 16846.1864919796, 'training/walltime': 1691.6326551437378, 'training/entropy_loss': Array(-0.5468694, dtype=float32), 'training/policy_loss': Array(8.934669e-06, dtype=float32), 'training/total_loss': Array(18285.945, dtype=float32), 'training/v_loss': Array(18286.492, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2879816 , 0.10707326], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.333363 ,  9.2319975], dtype=float32), 'eval/episode_reward': Array([-24342.781,   9217.642], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2830037 , 0.11096928], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.391999006271362, 'eval/sps': 29143.904590421815}
I0728 03:26:46.055295 139877494024000 train.py:379] starting iteration 68, 27852800 steps, 2017.3492393493652
I0728 03:27:14.877973 139877494024000 train.py:394] {'eval/walltime': 320.2603681087494, 'training/sps': 16768.545646617662, 'training/walltime': 1716.0593411922455, 'training/entropy_loss': Array(-0.5466505, dtype=float32), 'training/policy_loss': Array(-1.5284955e-05, dtype=float32), 'training/total_loss': Array(18387.477, dtype=float32), 'training/v_loss': Array(18388.023, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27293858, 0.10625602], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.18518 ,  9.138155], dtype=float32), 'eval/episode_reward': Array([-23171.664,   9105.395], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2686124 , 0.10967451], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.3872809410095215, 'eval/sps': 29175.245834734935}
I0728 03:27:14.881454 139877494024000 train.py:379] starting iteration 69, 28262400 steps, 2046.1753997802734
I0728 03:27:43.704695 139877494024000 train.py:394] {'eval/walltime': 324.68063950538635, 'training/sps': 16790.700703559174, 'training/walltime': 1740.4537966251373, 'training/entropy_loss': Array(-0.54666424, dtype=float32), 'training/policy_loss': Array(-4.5417382e-05, dtype=float32), 'training/total_loss': Array(18208.725, dtype=float32), 'training/v_loss': Array(18209.271, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28221118, 0.10700844], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.866447,  9.142675], dtype=float32), 'eval/episode_reward': Array([-23847.078,   9132.918], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27741683, 0.11008671], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.420271396636963, 'eval/sps': 28957.497971139313}
I0728 03:27:43.708189 139877494024000 train.py:379] starting iteration 70, 28672000 steps, 2075.0021345615387
I0728 03:28:12.449814 139877494024000 train.py:394] {'eval/walltime': 329.0713155269623, 'training/sps': 16826.559722063077, 'training/walltime': 1764.7962651252747, 'training/entropy_loss': Array(-0.54676723, dtype=float32), 'training/policy_loss': Array(-3.885922e-05, dtype=float32), 'training/total_loss': Array(17165.113, dtype=float32), 'training/v_loss': Array(17165.66, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28293496, 0.10617547], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.006165,  9.106251], dtype=float32), 'eval/episode_reward': Array([-24010.658,   9104.66 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27882463, 0.10878666], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.390676021575928, 'eval/sps': 29152.68614013053}
I0728 03:28:12.453416 139877494024000 train.py:379] starting iteration 71, 29081600 steps, 2103.7473611831665
I0728 03:28:41.269475 139877494024000 train.py:394] {'eval/walltime': 333.465030670166, 'training/sps': 16777.588744281184, 'training/walltime': 1789.2097852230072, 'training/entropy_loss': Array(-0.5468612, dtype=float32), 'training/policy_loss': Array(2.1040141e-06, dtype=float32), 'training/total_loss': Array(16237.039, dtype=float32), 'training/v_loss': Array(16237.586, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26704463, 0.09897592], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.57806,  8.48704], dtype=float32), 'eval/episode_reward': Array([-22556.61 ,   8453.124], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26276442, 0.10144281], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.393715143203735, 'eval/sps': 29132.521301020693}
I0728 03:28:41.272977 139877494024000 train.py:379] starting iteration 72, 29491200 steps, 2132.566922187805
I0728 03:29:10.043886 139877494024000 train.py:394] {'eval/walltime': 337.85307145118713, 'training/sps': 16804.420453087794, 'training/walltime': 1813.5843241214752, 'training/entropy_loss': Array(-0.54668665, dtype=float32), 'training/policy_loss': Array(-5.5256714e-05, dtype=float32), 'training/total_loss': Array(16801.305, dtype=float32), 'training/v_loss': Array(16801.852, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27448708, 0.10657403], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.24979 ,  9.099386], dtype=float32), 'eval/episode_reward': Array([-23231.54 ,   9083.631], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2702434 , 0.10883918], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.388040781021118, 'eval/sps': 29170.193803489172}
I0728 03:29:10.047367 139877494024000 train.py:379] starting iteration 73, 29900800 steps, 2161.341312646866
I0728 03:29:38.850607 139877494024000 train.py:394] {'eval/walltime': 342.2434515953064, 'training/sps': 16783.907435752582, 'training/walltime': 1837.9886531829834, 'training/entropy_loss': Array(-0.546608, dtype=float32), 'training/policy_loss': Array(-6.937762e-05, dtype=float32), 'training/total_loss': Array(16728.514, dtype=float32), 'training/v_loss': Array(16729.06, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27533725, 0.09730594], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.33822 ,  8.341355], dtype=float32), 'eval/episode_reward': Array([-23325.969,   8310.7  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27131587, 0.09953247], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.390380144119263, 'eval/sps': 29154.65080431608}
I0728 03:29:38.854107 139877494024000 train.py:379] starting iteration 74, 30310400 steps, 2190.148051738739
I0728 03:30:07.643000 139877494024000 train.py:394] {'eval/walltime': 346.6437385082245, 'training/sps': 16800.564518912954, 'training/walltime': 1862.3687863349915, 'training/entropy_loss': Array(-0.5465247, dtype=float32), 'training/policy_loss': Array(-5.1416806e-05, dtype=float32), 'training/total_loss': Array(16785.348, dtype=float32), 'training/v_loss': Array(16785.895, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2671194 , 0.10772888], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.558323,  9.267982], dtype=float32), 'eval/episode_reward': Array([-22527.63 ,   9248.472], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26203018, 0.11105318], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.400286912918091, 'eval/sps': 29089.01226968303}
I0728 03:30:07.646404 139877494024000 train.py:379] starting iteration 75, 30720000 steps, 2218.940349340439
I0728 03:30:36.405352 139877494024000 train.py:394] {'eval/walltime': 351.0367503166199, 'training/sps': 16816.093198121387, 'training/walltime': 1886.7264058589935, 'training/entropy_loss': Array(-0.54640263, dtype=float32), 'training/policy_loss': Array(-9.783403e-06, dtype=float32), 'training/total_loss': Array(16239.773, dtype=float32), 'training/v_loss': Array(16240.32, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2736261 , 0.09563114], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.097599,  8.26477 ], dtype=float32), 'eval/episode_reward': Array([-23087.63 ,   8248.745], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26898193, 0.09836965], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.393011808395386, 'eval/sps': 29137.18550798841}
I0728 03:30:36.472405 139877494024000 train.py:379] starting iteration 76, 31129600 steps, 2247.7663333415985
I0728 03:31:05.269943 139877494024000 train.py:394] {'eval/walltime': 355.4158799648285, 'training/sps': 16780.126630949402, 'training/walltime': 1911.1362335681915, 'training/entropy_loss': Array(-0.5469761, dtype=float32), 'training/policy_loss': Array(2.7303096e-05, dtype=float32), 'training/total_loss': Array(15096.505, dtype=float32), 'training/v_loss': Array(15097.053, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28239438, 0.10547774], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.948645,  9.039968], dtype=float32), 'eval/episode_reward': Array([-23926.047,   9017.855], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27822304, 0.10828547], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.379129648208618, 'eval/sps': 29229.552509906}
I0728 03:31:05.273702 139877494024000 train.py:379] starting iteration 77, 31539200 steps, 2276.567647457123
I0728 03:31:34.076624 139877494024000 train.py:394] {'eval/walltime': 359.8257234096527, 'training/sps': 16797.467939397757, 'training/walltime': 1935.5208611488342, 'training/entropy_loss': Array(-0.54684544, dtype=float32), 'training/policy_loss': Array(-0.00016986, dtype=float32), 'training/total_loss': Array(15592.512, dtype=float32), 'training/v_loss': Array(15593.059, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2715321 , 0.09734695], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.007252,  8.359897], dtype=float32), 'eval/episode_reward': Array([-23019.691,   8332.919], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26685187, 0.10051177], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.409843444824219, 'eval/sps': 29025.973733882114}
I0728 03:31:34.080017 139877494024000 train.py:379] starting iteration 78, 31948800 steps, 2305.3739624023438
I0728 03:32:02.844024 139877494024000 train.py:394] {'eval/walltime': 364.2106215953827, 'training/sps': 16806.95561252159, 'training/walltime': 1959.891723394394, 'training/entropy_loss': Array(-0.54689074, dtype=float32), 'training/policy_loss': Array(-2.4413494e-05, dtype=float32), 'training/total_loss': Array(15658.803, dtype=float32), 'training/v_loss': Array(15659.35, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29143596, 0.10451505], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.636719,  8.989417], dtype=float32), 'eval/episode_reward': Array([-24626.018,   8965.815], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28688854, 0.10743732], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.3848981857299805, 'eval/sps': 29191.09967400328}
I0728 03:32:02.847303 139877494024000 train.py:379] starting iteration 79, 32358400 steps, 2334.1412489414215
I0728 03:32:31.648406 139877494024000 train.py:394] {'eval/walltime': 368.6334865093231, 'training/sps': 16807.027464965984, 'training/walltime': 1984.2624814510345, 'training/entropy_loss': Array(-0.5464104, dtype=float32), 'training/policy_loss': Array(-4.0663177e-05, dtype=float32), 'training/total_loss': Array(15593.068, dtype=float32), 'training/v_loss': Array(15593.615, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28940392, 0.09514167], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.487228,  8.16861 ], dtype=float32), 'eval/episode_reward': Array([-24482.078,   8127.641], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28526533, 0.09762763], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.42286491394043, 'eval/sps': 28940.517626156012}
I0728 03:32:31.651726 139877494024000 train.py:379] starting iteration 80, 32768000 steps, 2362.9456713199615
I0728 03:33:00.440556 139877494024000 train.py:394] {'eval/walltime': 373.0322732925415, 'training/sps': 16799.182078949816, 'training/walltime': 2008.6446208953857, 'training/entropy_loss': Array(-0.5469281, dtype=float32), 'training/policy_loss': Array(-5.468755e-05, dtype=float32), 'training/total_loss': Array(15459.721, dtype=float32), 'training/v_loss': Array(15460.267, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28391144, 0.09367763], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.023663,  8.01891 ], dtype=float32), 'eval/episode_reward': Array([-24018.662,   7979.963], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28003228, 0.09573124], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.398786783218384, 'eval/sps': 29098.93257120966}
I0728 03:33:00.443915 139877494024000 train.py:379] starting iteration 81, 33177600 steps, 2391.7378599643707
I0728 03:33:29.240063 139877494024000 train.py:394] {'eval/walltime': 377.4368214607239, 'training/sps': 16798.51960696484, 'training/walltime': 2033.0277218818665, 'training/entropy_loss': Array(-0.5469718, dtype=float32), 'training/policy_loss': Array(2.7602287e-05, dtype=float32), 'training/total_loss': Array(14004.562, dtype=float32), 'training/v_loss': Array(14005.108, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27973297, 0.10646547], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.698597,  9.115045], dtype=float32), 'eval/episode_reward': Array([-23662.8  ,   9090.563], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27556965, 0.10887271], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.404548168182373, 'eval/sps': 29060.86960852146}
I0728 03:33:29.243505 139877494024000 train.py:379] starting iteration 82, 33587200 steps, 2420.5374512672424
I0728 03:33:57.992973 139877494024000 train.py:394] {'eval/walltime': 381.8269855976105, 'training/sps': 16820.10595611732, 'training/walltime': 2057.37953042984, 'training/entropy_loss': Array(-0.54659903, dtype=float32), 'training/policy_loss': Array(-2.7088834e-05, dtype=float32), 'training/total_loss': Array(14271.282, dtype=float32), 'training/v_loss': Array(14271.829, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2766598 , 0.09774414], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.415005,  8.387248], dtype=float32), 'eval/episode_reward': Array([-23415.021,   8356.631], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2728256 , 0.09977906], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.390164136886597, 'eval/sps': 29156.085287229067}
I0728 03:33:57.996382 139877494024000 train.py:379] starting iteration 83, 33996800 steps, 2449.290327310562
I0728 03:34:26.795370 139877494024000 train.py:394] {'eval/walltime': 386.2606928348541, 'training/sps': 16815.921686462247, 'training/walltime': 2081.7373983860016, 'training/entropy_loss': Array(-0.5468335, dtype=float32), 'training/policy_loss': Array(-7.1899136e-05, dtype=float32), 'training/total_loss': Array(14560.996, dtype=float32), 'training/v_loss': Array(14561.543, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27767056, 0.09521509], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.494774 ,  8.1579485], dtype=float32), 'eval/episode_reward': Array([-23463.719 ,   8112.1387], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2735973 , 0.09737913], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.433707237243652, 'eval/sps': 28869.74559907457}
I0728 03:34:26.798793 139877494024000 train.py:379] starting iteration 84, 34406400 steps, 2478.0927381515503
I0728 03:34:55.643617 139877494024000 train.py:394] {'eval/walltime': 390.6789507865906, 'training/sps': 16773.876131245703, 'training/walltime': 2106.156322002411, 'training/entropy_loss': Array(-0.54651093, dtype=float32), 'training/policy_loss': Array(-2.1500906e-05, dtype=float32), 'training/total_loss': Array(14473.199, dtype=float32), 'training/v_loss': Array(14473.746, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29288256, 0.09640212], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.796509,  8.301107], dtype=float32), 'eval/episode_reward': Array([-24795.652,   8274.685], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28892994, 0.09864529], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.41825795173645, 'eval/sps': 28970.694196271143}
I0728 03:34:55.646980 139877494024000 train.py:379] starting iteration 85, 34816000 steps, 2506.9409255981445
I0728 03:35:24.419275 139877494024000 train.py:394] {'eval/walltime': 395.07928681373596, 'training/sps': 16811.352732275143, 'training/walltime': 2130.5208098888397, 'training/entropy_loss': Array(-0.54682565, dtype=float32), 'training/policy_loss': Array(-0.00014863, dtype=float32), 'training/total_loss': Array(14178.641, dtype=float32), 'training/v_loss': Array(14179.1875, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28075695, 0.10318496], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.717628,  8.929714], dtype=float32), 'eval/episode_reward': Array([-23725.938,   8915.249], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27638972, 0.10597626], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.400336027145386, 'eval/sps': 29088.687593487488}
I0728 03:35:24.422739 139877494024000 train.py:379] starting iteration 86, 35225600 steps, 2535.716684818268
I0728 03:35:53.245835 139877494024000 train.py:394] {'eval/walltime': 399.4673755168915, 'training/sps': 16767.912592246794, 'training/walltime': 2154.9484181404114, 'training/entropy_loss': Array(-0.5465603, dtype=float32), 'training/policy_loss': Array(-2.0219792e-05, dtype=float32), 'training/total_loss': Array(13061.422, dtype=float32), 'training/v_loss': Array(13061.968, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26225132, 0.1065221 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.169394,  9.090019], dtype=float32), 'eval/episode_reward': Array([-22162.93 ,   9086.701], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25670648, 0.110172  ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.388088703155518, 'eval/sps': 29169.875237014683}
I0728 03:35:53.249198 139877494024000 train.py:379] starting iteration 87, 35635200 steps, 2564.5431435108185
I0728 03:36:22.004934 139877494024000 train.py:394] {'eval/walltime': 403.8666970729828, 'training/sps': 16822.363675638077, 'training/walltime': 2179.2969584465027, 'training/entropy_loss': Array(-0.5470139, dtype=float32), 'training/policy_loss': Array(-6.4923486e-05, dtype=float32), 'training/total_loss': Array(13531.231, dtype=float32), 'training/v_loss': Array(13531.777, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2834324 , 0.10467295], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.933144,  8.99423 ], dtype=float32), 'eval/episode_reward': Array([-23942.936,   8966.622], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27896494, 0.10751916], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.399321556091309, 'eval/sps': 29095.395362216925}
I0728 03:36:22.008334 139877494024000 train.py:379] starting iteration 88, 36044800 steps, 2593.302280187607
I0728 03:36:50.792652 139877494024000 train.py:394] {'eval/walltime': 408.26442694664, 'training/sps': 16801.479699394007, 'training/walltime': 2203.675763607025, 'training/entropy_loss': Array(-0.5468244, dtype=float32), 'training/policy_loss': Array(-4.5319e-05, dtype=float32), 'training/total_loss': Array(13646.279, dtype=float32), 'training/v_loss': Array(13646.826, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27212   , 0.10790171], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.009089,  9.304695], dtype=float32), 'eval/episode_reward': Array([-23008.68 ,   9282.556], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26748133, 0.11154527], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.397729873657227, 'eval/sps': 29105.925938455388}
I0728 03:36:50.796037 139877494024000 train.py:379] starting iteration 89, 36454400 steps, 2622.089982509613
I0728 03:37:19.622850 139877494024000 train.py:394] {'eval/walltime': 412.66527009010315, 'training/sps': 16774.496041676994, 'training/walltime': 2228.0937848091125, 'training/entropy_loss': Array(-0.54655766, dtype=float32), 'training/policy_loss': Array(-7.172264e-06, dtype=float32), 'training/total_loss': Array(13817.365, dtype=float32), 'training/v_loss': Array(13817.911, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2724208 , 0.09897678], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.075571,  8.482642], dtype=float32), 'eval/episode_reward': Array([-23077.773,   8458.747], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26822537, 0.10130371], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.400843143463135, 'eval/sps': 29085.335656674542}
I0728 03:37:19.626316 139877494024000 train.py:379] starting iteration 90, 36864000 steps, 2650.920261144638
I0728 03:37:48.381487 139877494024000 train.py:394] {'eval/walltime': 417.0612018108368, 'training/sps': 16820.46463411637, 'training/walltime': 2252.445074081421, 'training/entropy_loss': Array(-0.54679143, dtype=float32), 'training/policy_loss': Array(-4.2868684e-05, dtype=float32), 'training/total_loss': Array(13501.185, dtype=float32), 'training/v_loss': Array(13501.73, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26890004, 0.10002089], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.6924  ,  8.610911], dtype=float32), 'eval/episode_reward': Array([-22697.465,   8592.208], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2643013 , 0.10286542], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.395931720733643, 'eval/sps': 29117.831697949558}
I0728 03:37:48.384967 139877494024000 train.py:379] starting iteration 91, 37273600 steps, 2679.6789121627808
I0728 03:38:17.154237 139877494024000 train.py:394] {'eval/walltime': 421.48998260498047, 'training/sps': 16833.29553227021, 'training/walltime': 2276.777801990509, 'training/entropy_loss': Array(-0.546729, dtype=float32), 'training/policy_loss': Array(-0.00013222, dtype=float32), 'training/total_loss': Array(12300.502, dtype=float32), 'training/v_loss': Array(12301.049, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26805627, 0.10456558], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.655704,  8.970475], dtype=float32), 'eval/episode_reward': Array([-22639.309,   8946.933], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.263242  , 0.10761631], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.428780794143677, 'eval/sps': 28901.859439342454}
I0728 03:38:17.157709 139877494024000 train.py:379] starting iteration 92, 37683200 steps, 2708.45165514946
I0728 03:38:45.909728 139877494024000 train.py:394] {'eval/walltime': 425.88232588768005, 'training/sps': 16820.01768876237, 'training/walltime': 2301.129738330841, 'training/entropy_loss': Array(-0.54689133, dtype=float32), 'training/policy_loss': Array(-3.2196513e-05, dtype=float32), 'training/total_loss': Array(12824.424, dtype=float32), 'training/v_loss': Array(12824.973, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2820256 , 0.10293717], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.865177,  8.892607], dtype=float32), 'eval/episode_reward': Array([-23864.578,   8869.509], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27829123, 0.10521903], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.392343282699585, 'eval/sps': 29141.620260912237}
I0728 03:38:45.913307 139877494024000 train.py:379] starting iteration 93, 38092800 steps, 2737.2072529792786
I0728 03:39:14.685900 139877494024000 train.py:394] {'eval/walltime': 430.2940993309021, 'training/sps': 16819.52383682929, 'training/walltime': 2325.482389688492, 'training/entropy_loss': Array(-0.5467529, dtype=float32), 'training/policy_loss': Array(-5.8503407e-05, dtype=float32), 'training/total_loss': Array(12991.652, dtype=float32), 'training/v_loss': Array(12992.199, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26876363, 0.10399862], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.710693,  8.945939], dtype=float32), 'eval/episode_reward': Array([-22729.797,   8926.33 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2643339 , 0.10686918], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.411773443222046, 'eval/sps': 29013.2758735947}
I0728 03:39:14.689384 139877494024000 train.py:379] starting iteration 94, 38502400 steps, 2765.9833290576935
I0728 03:39:43.467478 139877494024000 train.py:394] {'eval/walltime': 434.68679904937744, 'training/sps': 16802.354553975463, 'training/walltime': 2349.859925508499, 'training/entropy_loss': Array(-0.5464784, dtype=float32), 'training/policy_loss': Array(-0.00018452, dtype=float32), 'training/total_loss': Array(13192.283, dtype=float32), 'training/v_loss': Array(13192.83, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2737465 , 0.09703235], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.168465,  8.282972], dtype=float32), 'eval/episode_reward': Array([-23191.121,   8285.53 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26949778, 0.09981117], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.392699718475342, 'eval/sps': 29139.255629434967}
I0728 03:39:43.470973 139877494024000 train.py:379] starting iteration 95, 38912000 steps, 2794.7649183273315
I0728 03:40:12.287783 139877494024000 train.py:394] {'eval/walltime': 439.1204249858856, 'training/sps': 16804.032709034636, 'training/walltime': 2374.2350268363953, 'training/entropy_loss': Array(-0.54652286, dtype=float32), 'training/policy_loss': Array(-8.152818e-05, dtype=float32), 'training/total_loss': Array(12823.402, dtype=float32), 'training/v_loss': Array(12823.949, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28765854, 0.08948216], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.320032 ,  7.6901274], dtype=float32), 'eval/episode_reward': Array([-24322.434,   7679.544], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2836185 , 0.09162638], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.433625936508179, 'eval/sps': 28870.274992303443}
I0728 03:40:12.291278 139877494024000 train.py:379] starting iteration 96, 39321600 steps, 2823.585223674774
I0728 03:40:41.091510 139877494024000 train.py:394] {'eval/walltime': 443.52581667900085, 'training/sps': 16796.00784596579, 'training/walltime': 2398.6217741966248, 'training/entropy_loss': Array(-0.54682875, dtype=float32), 'training/policy_loss': Array(-4.573849e-05, dtype=float32), 'training/total_loss': Array(11230.91, dtype=float32), 'training/v_loss': Array(11231.455, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2679831 , 0.09694801], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.637344,  8.330015], dtype=float32), 'eval/episode_reward': Array([-22644.875,   8320.234], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2638294 , 0.09889358], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.405391693115234, 'eval/sps': 29055.30516163613}
I0728 03:40:41.094946 139877494024000 train.py:379] starting iteration 97, 39731200 steps, 2852.388890028
I0728 03:41:09.842972 139877494024000 train.py:394] {'eval/walltime': 447.92148184776306, 'training/sps': 16825.25687333871, 'training/walltime': 2422.9661276340485, 'training/entropy_loss': Array(-0.54681784, dtype=float32), 'training/policy_loss': Array(-8.138573e-06, dtype=float32), 'training/total_loss': Array(11803.004, dtype=float32), 'training/v_loss': Array(11803.551, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27380845, 0.09634277], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.110188,  8.300913], dtype=float32), 'eval/episode_reward': Array([-23120.07 ,   8293.782], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26928538, 0.09919733], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.395665168762207, 'eval/sps': 29119.597395550496}
I0728 03:41:09.846438 139877494024000 train.py:379] starting iteration 98, 40140800 steps, 2881.140383005142
I0728 03:41:38.639518 139877494024000 train.py:394] {'eval/walltime': 452.3312005996704, 'training/sps': 16803.89464425576, 'training/walltime': 2447.341429233551, 'training/entropy_loss': Array(-0.54667133, dtype=float32), 'training/policy_loss': Array(-5.570521e-05, dtype=float32), 'training/total_loss': Array(12109.0625, dtype=float32), 'training/v_loss': Array(12109.609, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28678542, 0.09883939], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.203573,  8.477739], dtype=float32), 'eval/episode_reward': Array([-24164.082,   8411.338], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2825055 , 0.10080796], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.409718751907349, 'eval/sps': 29026.79449673197}
I0728 03:41:38.642987 139877494024000 train.py:379] starting iteration 99, 40550400 steps, 2909.9369332790375
I0728 03:42:07.440062 139877494024000 train.py:394] {'eval/walltime': 456.73301887512207, 'training/sps': 16795.878123253002, 'training/walltime': 2471.728364944458, 'training/entropy_loss': Array(-0.54663754, dtype=float32), 'training/policy_loss': Array(-7.399918e-05, dtype=float32), 'training/total_loss': Array(12150.535, dtype=float32), 'training/v_loss': Array(12151.08, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28390568, 0.09679706], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.019896,  8.328315], dtype=float32), 'eval/episode_reward': Array([-24002.96 ,   8310.114], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27993435, 0.09938816], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.40181827545166, 'eval/sps': 29078.892400860466}
I0728 03:42:07.443564 139877494024000 train.py:379] starting iteration 100, 40960000 steps, 2938.737509250641
I0728 03:42:36.229404 139877494024000 train.py:394] {'eval/walltime': 461.1194398403168, 'training/sps': 16793.09662797917, 'training/walltime': 2496.119339942932, 'training/entropy_loss': Array(-0.5465218, dtype=float32), 'training/policy_loss': Array(-6.088646e-05, dtype=float32), 'training/total_loss': Array(12262.995, dtype=float32), 'training/v_loss': Array(12263.541, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2872324 , 0.09791043], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.345665,  8.346773], dtype=float32), 'eval/episode_reward': Array([-24344.828,   8328.514], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28361344, 0.10038994], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.386420965194702, 'eval/sps': 29180.965761301115}
I0728 03:42:36.303416 139877494024000 train.py:379] starting iteration 101, 41369600 steps, 2967.59734416008
I0728 03:43:05.133929 139877494024000 train.py:394] {'eval/walltime': 465.5103569030762, 'training/sps': 16765.585529944285, 'training/walltime': 2520.550338745117, 'training/entropy_loss': Array(-0.54693305, dtype=float32), 'training/policy_loss': Array(-6.4656466e-05, dtype=float32), 'training/total_loss': Array(11127.365, dtype=float32), 'training/v_loss': Array(11127.912, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2710866 , 0.09998465], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.87362 ,  8.598205], dtype=float32), 'eval/episode_reward': Array([-22873.354,   8608.226], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26585987, 0.10340173], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.390917062759399, 'eval/sps': 29151.085791531783}
I0728 03:43:05.137857 139877494024000 train.py:379] starting iteration 102, 41779200 steps, 2996.431801557541
I0728 03:43:33.896220 139877494024000 train.py:394] {'eval/walltime': 469.91084837913513, 'training/sps': 16821.64485528301, 'training/walltime': 2544.8999195098877, 'training/entropy_loss': Array(-0.5464607, dtype=float32), 'training/policy_loss': Array(-5.1026436e-06, dtype=float32), 'training/total_loss': Array(11444.092, dtype=float32), 'training/v_loss': Array(11444.638, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2765522 , 0.09251948], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.402435,  8.015649], dtype=float32), 'eval/episode_reward': Array([-23383.66  ,   7985.7627], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2719109 , 0.09567361], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.40049147605896, 'eval/sps': 29087.660025337813}
I0728 03:43:33.899688 139877494024000 train.py:379] starting iteration 103, 42188800 steps, 3025.193633079529
I0728 03:44:02.656855 139877494024000 train.py:394] {'eval/walltime': 474.2964425086975, 'training/sps': 16812.11476514476, 'training/walltime': 2569.263303041458, 'training/entropy_loss': Array(-0.5465144, dtype=float32), 'training/policy_loss': Array(-7.1523915e-05, dtype=float32), 'training/total_loss': Array(11559.945, dtype=float32), 'training/v_loss': Array(11560.492, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27379608, 0.09844977], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.144037 ,  8.4590225], dtype=float32), 'eval/episode_reward': Array([-23138.617,   8454.561], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2694956, 0.1009502], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.385594129562378, 'eval/sps': 29186.467378998575}
I0728 03:44:02.660366 139877494024000 train.py:379] starting iteration 104, 42598400 steps, 3053.9543113708496
I0728 03:44:31.652106 139877494024000 train.py:394] {'eval/walltime': 478.69853711128235, 'training/sps': 16663.301989953852, 'training/walltime': 2593.844265460968, 'training/entropy_loss': Array(-0.54647434, dtype=float32), 'training/policy_loss': Array(-0.0001298, dtype=float32), 'training/total_loss': Array(11832.865, dtype=float32), 'training/v_loss': Array(11833.412, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28576672, 0.09175283], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.199503,  7.875591], dtype=float32), 'eval/episode_reward': Array([-24171.854 ,   7866.0146], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2820422 , 0.09378138], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.402094602584839, 'eval/sps': 29077.06706821804}
I0728 03:44:31.655679 139877494024000 train.py:379] starting iteration 105, 43008000 steps, 3082.9496245384216
