I0728 15:17:40.799110 139654118168384 low_level_env.py:191] Initialising environment...
I0728 15:17:41.149747 139654118168384 low_level_env.py:299] Environment initialised.
I0728 15:17:41.158151 139654118168384 train.py:118] JAX is running on GPU.
I0728 15:17:41.158209 139654118168384 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 15:17:45.048663 139654118168384 train.py:367] Running initial eval
I0728 15:17:57.058242 139654118168384 train.py:373] {'eval/walltime': 11.869630575180054, 'eval/episode_distance_from_origin': Array([2.8402524, 1.9528004], dtype=float32), 'eval/episode_forward_reward': Array([ 2.7797909, 48.330593 ], dtype=float32), 'eval/episode_reward': Array([-82.92013,  82.87259], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-662.08276,  542.79865], dtype=float32), 'eval/episode_reward_forward': Array([ 2.7797909, 48.330593 ], dtype=float32), 'eval/episode_reward_survive': Array([576.3828, 472.7114], dtype=float32), 'eval/episode_x_position': Array([0.13733426, 2.417406  ], dtype=float32), 'eval/episode_x_velocity': Array([-0.02167145,  0.68225306], dtype=float32), 'eval/episode_y_position': Array([-0.33886763,  2.2875504 ], dtype=float32), 'eval/episode_y_velocity': Array([0.1074294, 0.6268799], dtype=float32), 'eval/avg_episode_length': Array([576.3828, 472.7114], dtype=float32), 'eval/epoch_eval_time': 11.869630575180054, 'eval/sps': 10783.823404550933}
I0728 15:17:57.060405 139654118168384 train.py:379] starting iteration 0, 0 steps, 15.902263402938843
I0728 15:18:27.417452 139654118168384 train.py:394] {'eval/walltime': 15.173481225967407, 'training/sps': 205960.4688802714, 'training/walltime': 27.046743631362915, 'training/entropy_loss': Array(-0.03404686, dtype=float32), 'training/policy_loss': Array(-0.01105537, dtype=float32), 'training/total_loss': Array(118.12919, dtype=float32), 'training/v_loss': Array(118.174286, dtype=float32), 'eval/episode_distance_from_origin': Array([23.781923, 16.392696], dtype=float32), 'eval/episode_forward_reward': Array([456.60046, 330.6553 ], dtype=float32), 'eval/episode_reward': Array([777.6516, 509.593 ], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-384.08954,  244.5311 ], dtype=float32), 'eval/episode_reward_forward': Array([456.60046, 330.6553 ], dtype=float32), 'eval/episode_reward_survive': Array([705.1406 , 446.43954], dtype=float32), 'eval/episode_x_position': Array([22.833065, 16.530893], dtype=float32), 'eval/episode_x_velocity': Array([0.5985005 , 0.80197376], dtype=float32), 'eval/episode_y_position': Array([-1.0416003,  6.172854 ], dtype=float32), 'eval/episode_y_velocity': Array([-0.02412186,  0.6137966 ], dtype=float32), 'eval/avg_episode_length': Array([705.1406 , 446.43954], dtype=float32), 'eval/epoch_eval_time': 3.3038506507873535, 'eval/sps': 38742.671364244576}
I0728 15:18:27.437553 139654118168384 train.py:379] starting iteration 1, 5570560 steps, 46.27941608428955
I0728 15:18:46.732162 139654118168384 train.py:394] {'eval/walltime': 18.507022380828857, 'training/sps': 349146.74776147585, 'training/walltime': 43.001524925231934, 'training/entropy_loss': Array(0.00333719, dtype=float32), 'training/policy_loss': Array(-0.01012171, dtype=float32), 'training/total_loss': Array(429.66998, dtype=float32), 'training/v_loss': Array(429.67676, dtype=float32), 'eval/episode_distance_from_origin': Array([73.917114, 60.6752  ], dtype=float32), 'eval/episode_forward_reward': Array([1467.9146, 1218.066 ], dtype=float32), 'eval/episode_reward': Array([1594.0145, 1316.1381], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-405.7049,  331.8671], dtype=float32), 'eval/episode_reward_forward': Array([1467.9146, 1218.066 ], dtype=float32), 'eval/episode_reward_survive': Array([531.8047 , 431.80292], dtype=float32), 'eval/episode_x_position': Array([73.397835, 60.9066  ], dtype=float32), 'eval/episode_x_velocity': Array([2.0938897, 1.645178 ], dtype=float32), 'eval/episode_y_position': Array([-3.0342565,  6.1967254], dtype=float32), 'eval/episode_y_velocity': Array([0.16709739, 0.9209498 ], dtype=float32), 'eval/avg_episode_length': Array([531.8047 , 431.80292], dtype=float32), 'eval/epoch_eval_time': 3.33354115486145, 'eval/sps': 38397.60604525069}
I0728 15:18:46.734597 139654118168384 train.py:379] starting iteration 2, 11141120 steps, 65.5764594078064
I0728 15:19:06.119528 139654118168384 train.py:394] {'eval/walltime': 21.841121196746826, 'training/sps': 347188.98738008173, 'training/walltime': 59.04627346992493, 'training/entropy_loss': Array(0.02690585, dtype=float32), 'training/policy_loss': Array(-0.00555239, dtype=float32), 'training/total_loss': Array(571.5327, dtype=float32), 'training/v_loss': Array(571.5113, dtype=float32), 'eval/episode_distance_from_origin': Array([127.51862,  95.55324], dtype=float32), 'eval/episode_forward_reward': Array([2540.4224, 1916.8921], dtype=float32), 'eval/episode_reward': Array([2643.0986, 1989.027 ], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-496.80826,  371.52142], dtype=float32), 'eval/episode_reward_forward': Array([2540.4224, 1916.8921], dtype=float32), 'eval/episode_reward_survive': Array([599.4844 , 444.20117], dtype=float32), 'eval/episode_x_position': Array([127.0286 ,  95.83927], dtype=float32), 'eval/episode_x_velocity': Array([2.9823074, 1.9317976], dtype=float32), 'eval/episode_y_position': Array([5.32744  , 6.3918123], dtype=float32), 'eval/episode_y_velocity': Array([0.15946352, 0.98225707], dtype=float32), 'eval/avg_episode_length': Array([599.4844 , 444.20117], dtype=float32), 'eval/epoch_eval_time': 3.3340988159179688, 'eval/sps': 38391.18366525022}
I0728 15:19:06.121967 139654118168384 train.py:379] starting iteration 3, 16711680 steps, 84.96382975578308
I0728 15:19:25.539316 139654118168384 train.py:394] {'eval/walltime': 25.186877965927124, 'training/sps': 346735.44655135414, 'training/walltime': 75.11200904846191, 'training/entropy_loss': Array(0.04281644, dtype=float32), 'training/policy_loss': Array(-0.00409946, dtype=float32), 'training/total_loss': Array(571.3799, dtype=float32), 'training/v_loss': Array(571.3412, dtype=float32), 'eval/episode_distance_from_origin': Array([180.75342, 122.72657], dtype=float32), 'eval/episode_forward_reward': Array([3603.9282, 2458.4766], dtype=float32), 'eval/episode_reward': Array([3698.5483, 2518.354 ], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-574.8712,  388.4947], dtype=float32), 'eval/episode_reward_forward': Array([3603.9282, 2458.4766], dtype=float32), 'eval/episode_reward_survive': Array([669.4922 , 448.59152], dtype=float32), 'eval/episode_x_position': Array([180.19203, 122.91275], dtype=float32), 'eval/episode_x_velocity': Array([4.097312 , 2.5685906], dtype=float32), 'eval/episode_y_position': Array([8.909259, 8.769412], dtype=float32), 'eval/episode_y_velocity': Array([0.04269569, 1.0549405 ], dtype=float32), 'eval/avg_episode_length': Array([669.4922 , 448.59152], dtype=float32), 'eval/epoch_eval_time': 3.345756769180298, 'eval/sps': 38257.41344352408}
I0728 15:19:25.541690 139654118168384 train.py:379] starting iteration 4, 22282240 steps, 104.38355255126953
I0728 15:19:45.000710 139654118168384 train.py:394] {'eval/walltime': 28.531982898712158, 'training/sps': 345819.7741160169, 'training/walltime': 91.22028398513794, 'training/entropy_loss': Array(0.05774499, dtype=float32), 'training/policy_loss': Array(-0.00273386, dtype=float32), 'training/total_loss': Array(452.67047, dtype=float32), 'training/v_loss': Array(452.61545, dtype=float32), 'eval/episode_distance_from_origin': Array([219.78455, 139.71358], dtype=float32), 'eval/episode_forward_reward': Array([4385.8037, 2800.1353], dtype=float32), 'eval/episode_reward': Array([4484.45  , 2860.2676], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-608.2206 ,  386.55954], dtype=float32), 'eval/episode_reward_forward': Array([4385.8037, 2800.1353], dtype=float32), 'eval/episode_reward_survive': Array([706.8672, 446.8153], dtype=float32), 'eval/episode_x_position': Array([219.2909, 140.0158], dtype=float32), 'eval/episode_x_velocity': Array([4.6854334, 2.8153675], dtype=float32), 'eval/episode_y_position': Array([7.70916 , 8.496697], dtype=float32), 'eval/episode_y_velocity': Array([0.04093337, 1.110774  ], dtype=float32), 'eval/avg_episode_length': Array([706.8672, 446.8153], dtype=float32), 'eval/epoch_eval_time': 3.345104932785034, 'eval/sps': 38264.86838887623}
I0728 15:19:45.002981 139654118168384 train.py:379] starting iteration 5, 27852800 steps, 123.84484386444092
I0728 15:20:04.442247 139654118168384 train.py:394] {'eval/walltime': 31.88585376739502, 'training/sps': 346436.2247558597, 'training/walltime': 107.29989576339722, 'training/entropy_loss': Array(0.06668063, dtype=float32), 'training/policy_loss': Array(-0.00154636, dtype=float32), 'training/total_loss': Array(472.07092, dtype=float32), 'training/v_loss': Array(472.00577, dtype=float32), 'eval/episode_distance_from_origin': Array([248.23132, 151.51582], dtype=float32), 'eval/episode_forward_reward': Array([4955.595, 3036.241], dtype=float32), 'eval/episode_reward': Array([5062.8555, 3098.9731], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-618.48285,  376.9792 ], dtype=float32), 'eval/episode_reward_forward': Array([4955.595, 3036.241], dtype=float32), 'eval/episode_reward_survive': Array([725.7422 , 439.84763], dtype=float32), 'eval/episode_x_position': Array([247.76988, 151.80528], dtype=float32), 'eval/episode_x_velocity': Array([5.398224 , 3.0378084], dtype=float32), 'eval/episode_y_position': Array([7.7000294, 9.011704 ], dtype=float32), 'eval/episode_y_velocity': Array([0.20487002, 1.0485337 ], dtype=float32), 'eval/avg_episode_length': Array([725.7422 , 439.84763], dtype=float32), 'eval/epoch_eval_time': 3.3538708686828613, 'eval/sps': 38164.85637393321}
I0728 15:20:04.444638 139654118168384 train.py:379] starting iteration 6, 33423360 steps, 143.28650069236755
I0728 15:20:23.917932 139654118168384 train.py:394] {'eval/walltime': 35.2476441860199, 'training/sps': 345870.6848787021, 'training/walltime': 123.40579962730408, 'training/entropy_loss': Array(0.07401809, dtype=float32), 'training/policy_loss': Array(-0.00086511, dtype=float32), 'training/total_loss': Array(406.5225, dtype=float32), 'training/v_loss': Array(406.4493, dtype=float32), 'eval/episode_distance_from_origin': Array([266.92487, 166.46684], dtype=float32), 'eval/episode_forward_reward': Array([5329.361 , 3335.4255], dtype=float32), 'eval/episode_reward': Array([5428.8164, 3395.3574], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-616.1847 ,  382.42245], dtype=float32), 'eval/episode_reward_forward': Array([5329.361 , 3335.4255], dtype=float32), 'eval/episode_reward_survive': Array([715.6406 , 442.31866], dtype=float32), 'eval/episode_x_position': Array([266.4764, 166.7715], dtype=float32), 'eval/episode_x_velocity': Array([5.758954 , 3.3429437], dtype=float32), 'eval/episode_y_position': Array([ 5.159809, 10.508564], dtype=float32), 'eval/episode_y_velocity': Array([-0.06156819,  1.034447  ], dtype=float32), 'eval/avg_episode_length': Array([715.6406 , 442.31866], dtype=float32), 'eval/epoch_eval_time': 3.361790418624878, 'eval/sps': 38074.94937544551}
I0728 15:20:23.920237 139654118168384 train.py:379] starting iteration 7, 38993920 steps, 162.76209950447083
I0728 15:20:43.419744 139654118168384 train.py:394] {'eval/walltime': 38.610710859298706, 'training/sps': 345343.4911570978, 'training/walltime': 139.5362904071808, 'training/entropy_loss': Array(0.07965283, dtype=float32), 'training/policy_loss': Array(-0.00068001, dtype=float32), 'training/total_loss': Array(280.86618, dtype=float32), 'training/v_loss': Array(280.7872, dtype=float32), 'eval/episode_distance_from_origin': Array([269.7324 , 177.93349], dtype=float32), 'eval/episode_forward_reward': Array([5386.696 , 3566.9324], dtype=float32), 'eval/episode_reward': Array([5497.7627, 3638.4377], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-585.62885,  386.65628], dtype=float32), 'eval/episode_reward_forward': Array([5386.696 , 3566.9324], dtype=float32), 'eval/episode_reward_survive': Array([696.6953, 458.199 ], dtype=float32), 'eval/episode_x_position': Array([269.32996, 178.34589], dtype=float32), 'eval/episode_x_velocity': Array([5.7030334, 3.6944213], dtype=float32), 'eval/episode_y_position': Array([-1.8889425,  8.113192 ], dtype=float32), 'eval/episode_y_velocity': Array([-0.14876159,  0.82149845], dtype=float32), 'eval/avg_episode_length': Array([696.6953, 458.199 ], dtype=float32), 'eval/epoch_eval_time': 3.3630666732788086, 'eval/sps': 38060.50026216308}
I0728 15:20:43.422070 139654118168384 train.py:379] starting iteration 8, 44564480 steps, 182.26393294334412
I0728 15:21:02.910643 139654118168384 train.py:394] {'eval/walltime': 41.97259306907654, 'training/sps': 345555.44310818636, 'training/walltime': 155.65688729286194, 'training/entropy_loss': Array(0.08785099, dtype=float32), 'training/policy_loss': Array(-2.7111531e-05, dtype=float32), 'training/total_loss': Array(200.24268, dtype=float32), 'training/v_loss': Array(200.15483, dtype=float32), 'eval/episode_distance_from_origin': Array([326.58075, 156.42621], dtype=float32), 'eval/episode_forward_reward': Array([6525.334 , 3135.9849], dtype=float32), 'eval/episode_reward': Array([6664.8115, 3201.4844], dtype=float32), 'eval/episode_reward_contact': Array([0., 0.], dtype=float32), 'eval/episode_reward_ctrl': Array([-673.82666,  323.1452 ], dtype=float32), 'eval/episode_reward_forward': Array([6525.334 , 3135.9849], dtype=float32), 'eval/episode_reward_survive': Array([813.3047, 388.6413], dtype=float32), 'eval/episode_x_position': Array([326.26904, 156.78734], dtype=float32), 'eval/episode_x_velocity': Array([6.9536223, 3.258884 ], dtype=float32), 'eval/episode_y_position': Array([4.6535473, 8.257217 ], dtype=float32), 'eval/episode_y_velocity': Array([0.0923765 , 0.80712456], dtype=float32), 'eval/avg_episode_length': Array([813.3047, 388.6413], dtype=float32), 'eval/epoch_eval_time': 3.361882209777832, 'eval/sps': 38073.90979604214}
I0728 15:21:03.255217 139654118168384 train.py:410] total steps: 50135040
