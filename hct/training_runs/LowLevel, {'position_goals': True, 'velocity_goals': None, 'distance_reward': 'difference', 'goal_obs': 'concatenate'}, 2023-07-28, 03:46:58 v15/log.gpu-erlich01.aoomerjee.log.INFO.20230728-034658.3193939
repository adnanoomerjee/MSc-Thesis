I0728 03:46:58.649525 140337775261504 low_level_env.py:188] Initialising environment...
I0728 03:47:38.563120 140337775261504 low_level_env.py:293] Environment initialised.
I0728 03:47:38.568725 140337775261504 train.py:118] JAX is running on GPU.
I0728 03:47:38.568847 140337775261504 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 03:47:47.229950 140337775261504 train.py:367] Running initial eval
I0728 03:48:04.341886 140337775261504 train.py:373] {'eval/walltime': 16.969745635986328, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3156141 , 0.14746188], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.615604, 12.721508], dtype=float32), 'eval/episode_reward': Array([-3.3646665,  9.287771 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31060338, 0.15087026], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 16.969745635986328, 'eval/sps': 7542.835511250154}
I0728 03:48:04.343040 140337775261504 train.py:379] starting iteration 0, 0 steps, 25.774363040924072
I0728 03:49:15.362962 140337775261504 train.py:394] {'eval/walltime': 21.175379514694214, 'training/sps': 6131.358578291717, 'training/walltime': 66.80411767959595, 'training/entropy_loss': Array(-0.04473465, dtype=float32), 'training/policy_loss': Array(0.01610655, dtype=float32), 'training/total_loss': Array(2.0024714, dtype=float32), 'training/v_loss': Array(2.0310996, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32021385, 0.13392118], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.099413, 11.468218], dtype=float32), 'eval/episode_reward': Array([-2.9279163,  9.558052 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3158755 , 0.13676004], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.205633878707886, 'eval/sps': 30435.364487630093}
I0728 03:49:15.400191 140337775261504 train.py:379] starting iteration 1, 409600 steps, 96.8314962387085
I0728 03:49:41.880370 140337775261504 train.py:394] {'eval/walltime': 25.508736848831177, 'training/sps': 18501.785693304504, 'training/walltime': 88.94252133369446, 'training/entropy_loss': Array(-0.04342173, dtype=float32), 'training/policy_loss': Array(0.00491647, dtype=float32), 'training/total_loss': Array(1.8002982, dtype=float32), 'training/v_loss': Array(1.8388034, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31272906, 0.14131561], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.4166  , 12.088617], dtype=float32), 'eval/episode_reward': Array([-2.6678152,  9.844878 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30850726, 0.14418457], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.333357334136963, 'eval/sps': 29538.297936256542}
I0728 03:49:41.884087 140337775261504 train.py:379] starting iteration 2, 819200 steps, 123.31541109085083
I0728 03:50:08.565607 140337775261504 train.py:394] {'eval/walltime': 29.999945402145386, 'training/sps': 18464.968189067236, 'training/walltime': 111.12506699562073, 'training/entropy_loss': Array(-0.04296681, dtype=float32), 'training/policy_loss': Array(0.00731467, dtype=float32), 'training/total_loss': Array(1.8724747, dtype=float32), 'training/v_loss': Array(1.9081267, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30705053, 0.11726654], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.927437, 10.078974], dtype=float32), 'eval/episode_reward': Array([-2.9072745,  7.5664587], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30300534, 0.11939745], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.491208553314209, 'eval/sps': 28500.123848745487}
I0728 03:50:08.568985 140337775261504 train.py:379] starting iteration 3, 1228800 steps, 150.00030899047852
I0728 03:50:35.511016 140337775261504 train.py:394] {'eval/walltime': 34.447460651397705, 'training/sps': 18215.51927194878, 'training/walltime': 133.61138725280762, 'training/entropy_loss': Array(-0.03989075, dtype=float32), 'training/policy_loss': Array(0.00984897, dtype=float32), 'training/total_loss': Array(1.7406237, dtype=float32), 'training/v_loss': Array(1.7706656, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28079748, 0.12085417], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.684868, 10.395545], dtype=float32), 'eval/episode_reward': Array([-0.1732923,  7.0112705], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27575982, 0.12388706], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.447515249252319, 'eval/sps': 28780.114924062}
I0728 03:50:35.514506 140337775261504 train.py:379] starting iteration 4, 1638400 steps, 176.94582891464233
I0728 03:51:03.160182 140337775261504 train.py:394] {'eval/walltime': 38.896594762802124, 'training/sps': 17664.01492688487, 'training/walltime': 156.79977345466614, 'training/entropy_loss': Array(-0.0360987, dtype=float32), 'training/policy_loss': Array(0.01232376, dtype=float32), 'training/total_loss': Array(1.2362652, dtype=float32), 'training/v_loss': Array(1.2600402, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27308097, 0.10992189], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.006804,  9.432556], dtype=float32), 'eval/episode_reward': Array([-0.46148604,  5.236672  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26825944, 0.11272959], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.449134111404419, 'eval/sps': 28769.642990059332}
I0728 03:51:03.163575 140337775261504 train.py:379] starting iteration 5, 2048000 steps, 204.59489798545837
I0728 03:51:31.247959 140337775261504 train.py:394] {'eval/walltime': 43.37426972389221, 'training/sps': 17356.746932095848, 'training/walltime': 180.3986656665802, 'training/entropy_loss': Array(-0.02791274, dtype=float32), 'training/policy_loss': Array(0.01398009, dtype=float32), 'training/total_loss': Array(1.832287, dtype=float32), 'training/v_loss': Array(1.8462197, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2864133 , 0.10638916], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.219187,  9.112214], dtype=float32), 'eval/episode_reward': Array([-1.2103186,  5.305605 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2819311 , 0.10884665], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.477674961090088, 'eval/sps': 28586.264325188637}
I0728 03:51:31.251347 140337775261504 train.py:379] starting iteration 6, 2457600 steps, 232.68267059326172
I0728 03:51:59.609206 140337775261504 train.py:394] {'eval/walltime': 47.833099603652954, 'training/sps': 17144.269793544496, 'training/walltime': 204.29003024101257, 'training/entropy_loss': Array(-0.02399467, dtype=float32), 'training/policy_loss': Array(0.02606921, dtype=float32), 'training/total_loss': Array(0.74744254, dtype=float32), 'training/v_loss': Array(0.745368, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2929427 , 0.09531668], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.684755,  8.242754], dtype=float32), 'eval/episode_reward': Array([0.14884259, 2.931835  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2880788 , 0.09782338], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.458829879760742, 'eval/sps': 28707.08312533072}
I0728 03:51:59.612594 140337775261504 train.py:379] starting iteration 7, 2867200 steps, 261.04391741752625
I0728 03:52:28.084677 140337775261504 train.py:394] {'eval/walltime': 52.292460680007935, 'training/sps': 17063.242264387754, 'training/walltime': 228.29484677314758, 'training/entropy_loss': Array(-0.00948879, dtype=float32), 'training/policy_loss': Array(0.00695328, dtype=float32), 'training/total_loss': Array(0.1789769, dtype=float32), 'training/v_loss': Array(0.18151242, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27389792, 0.09890982], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.066847,  8.503959], dtype=float32), 'eval/episode_reward': Array([-0.5698111,  2.4034817], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2690528, 0.1015418], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4593610763549805, 'eval/sps': 28703.663553664377}
I0728 03:52:28.088084 140337775261504 train.py:379] starting iteration 8, 3276800 steps, 289.5194082260132
I0728 03:52:56.751015 140337775261504 train.py:394] {'eval/walltime': 56.74887752532959, 'training/sps': 16926.597854431962, 'training/walltime': 252.49344849586487, 'training/entropy_loss': Array(-0.00233005, dtype=float32), 'training/policy_loss': Array(0.00092094, dtype=float32), 'training/total_loss': Array(0.03995656, dtype=float32), 'training/v_loss': Array(0.04136566, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2878557 , 0.10557242], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.254238,  9.118056], dtype=float32), 'eval/episode_reward': Array([-0.03650871,  2.2225182 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28299734, 0.10832236], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.456416845321655, 'eval/sps': 28722.62726822208}
I0728 03:52:56.754401 140337775261504 train.py:379] starting iteration 9, 3686400 steps, 318.18572425842285
I0728 03:53:25.503590 140337775261504 train.py:394] {'eval/walltime': 61.23465824127197, 'training/sps': 16886.72144126493, 'training/walltime': 276.74919295310974, 'training/entropy_loss': Array(0.00601324, dtype=float32), 'training/policy_loss': Array(0.00122925, dtype=float32), 'training/total_loss': Array(0.02677061, dtype=float32), 'training/v_loss': Array(0.01952812, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26943123, 0.09483662], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.695202,  8.197726], dtype=float32), 'eval/episode_reward': Array([0.02936494, 2.169871  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26434687, 0.09777384], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.485780715942383, 'eval/sps': 28534.609269929388}
I0728 03:53:25.506933 140337775261504 train.py:379] starting iteration 10, 4096000 steps, 346.93825674057007
I0728 03:53:54.173273 140337775261504 train.py:394] {'eval/walltime': 65.66589617729187, 'training/sps': 16906.53364659535, 'training/walltime': 300.97651290893555, 'training/entropy_loss': Array(0.01187979, dtype=float32), 'training/policy_loss': Array(0.00101793, dtype=float32), 'training/total_loss': Array(0.8033632, dtype=float32), 'training/v_loss': Array(0.79046553, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26147953, 0.10251034], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.988232,  8.891627], dtype=float32), 'eval/episode_reward': Array([-0.06872205,  2.3326235 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25549877, 0.10713423], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4312379360198975, 'eval/sps': 28885.833224962997}
I0728 03:53:54.176600 140337775261504 train.py:379] starting iteration 11, 4505600 steps, 375.6079239845276
I0728 03:54:22.949822 140337775261504 train.py:394] {'eval/walltime': 70.12497901916504, 'training/sps': 16851.436539503105, 'training/walltime': 325.2830460071564, 'training/entropy_loss': Array(0.01606043, dtype=float32), 'training/policy_loss': Array(0.00104675, dtype=float32), 'training/total_loss': Array(0.04677566, dtype=float32), 'training/v_loss': Array(0.02966848, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2801418 , 0.10562696], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.561117,  9.138911], dtype=float32), 'eval/episode_reward': Array([-0.11599827,  2.931016  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27455294, 0.1096441 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.459082841873169, 'eval/sps': 28705.454583173843}
I0728 03:54:22.953145 140337775261504 train.py:379] starting iteration 12, 4915200 steps, 404.38446855545044
I0728 03:54:51.754863 140337775261504 train.py:394] {'eval/walltime': 74.56824088096619, 'training/sps': 16820.976493123195, 'training/walltime': 349.6335942745209, 'training/entropy_loss': Array(0.01890356, dtype=float32), 'training/policy_loss': Array(0.00095604, dtype=float32), 'training/total_loss': Array(0.02910144, dtype=float32), 'training/v_loss': Array(0.00924185, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29303113, 0.09829547], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.670565,  8.520173], dtype=float32), 'eval/episode_reward': Array([-0.12720914,  2.8938718 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2882426 , 0.10151298], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4432618618011475, 'eval/sps': 28807.665175086742}
I0728 03:54:51.758205 140337775261504 train.py:379] starting iteration 13, 5324800 steps, 433.18952894210815
I0728 03:55:20.531328 140337775261504 train.py:394] {'eval/walltime': 79.02506732940674, 'training/sps': 16850.36484897677, 'training/walltime': 373.9416732788086, 'training/entropy_loss': Array(0.01971558, dtype=float32), 'training/policy_loss': Array(0.00101425, dtype=float32), 'training/total_loss': Array(0.02772655, dtype=float32), 'training/v_loss': Array(0.00699671, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28790003, 0.094928  ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.271534,  8.18837 ], dtype=float32), 'eval/episode_reward': Array([-0.01383875,  2.354658  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28320897, 0.09728171], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.456826448440552, 'eval/sps': 28719.987524932083}
I0728 03:55:20.534685 140337775261504 train.py:379] starting iteration 14, 5734400 steps, 461.96600890159607
I0728 03:55:49.280807 140337775261504 train.py:394] {'eval/walltime': 83.46873593330383, 'training/sps': 16859.828335610688, 'training/walltime': 398.2361080646515, 'training/entropy_loss': Array(0.0222644, dtype=float32), 'training/policy_loss': Array(0.00088991, dtype=float32), 'training/total_loss': Array(0.02923831, dtype=float32), 'training/v_loss': Array(0.00608401, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2829944 , 0.09723717], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.852482,  8.360437], dtype=float32), 'eval/episode_reward': Array([-0.15597689,  2.4965487 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27801645, 0.10011597], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.443668603897095, 'eval/sps': 28805.028324511884}
I0728 03:55:49.284095 140337775261504 train.py:379] starting iteration 15, 6144000 steps, 490.71541929244995
I0728 03:56:18.137245 140337775261504 train.py:394] {'eval/walltime': 87.94801568984985, 'training/sps': 16810.349461450227, 'training/walltime': 422.60205006599426, 'training/entropy_loss': Array(0.0233655, dtype=float32), 'training/policy_loss': Array(0.00090686, dtype=float32), 'training/total_loss': Array(0.6668816, dtype=float32), 'training/v_loss': Array(0.6426093, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2816469 , 0.09066404], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.699297,  7.803661], dtype=float32), 'eval/episode_reward': Array([-0.2217789,  2.52085  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2768823 , 0.09318902], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4792797565460205, 'eval/sps': 28576.022699395093}
I0728 03:56:18.140519 140337775261504 train.py:379] starting iteration 16, 6553600 steps, 519.5718429088593
I0728 03:56:46.999945 140337775261504 train.py:394] {'eval/walltime': 92.4249336719513, 'training/sps': 16805.26848983176, 'training/walltime': 446.9753589630127, 'training/entropy_loss': Array(0.02185225, dtype=float32), 'training/policy_loss': Array(0.0011555, dtype=float32), 'training/total_loss': Array(0.05103742, dtype=float32), 'training/v_loss': Array(0.02802967, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27687243, 0.09520219], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.3209  ,  8.183782], dtype=float32), 'eval/episode_reward': Array([0.03327835, 2.094244  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2717361 , 0.09776196], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.47691798210144, 'eval/sps': 28591.097829296734}
I0728 03:56:47.003228 140337775261504 train.py:379] starting iteration 17, 6963200 steps, 548.4345524311066
I0728 03:57:15.866135 140337775261504 train.py:394] {'eval/walltime': 96.90017914772034, 'training/sps': 16800.75362613241, 'training/walltime': 471.3552176952362, 'training/entropy_loss': Array(0.02613608, dtype=float32), 'training/policy_loss': Array(0.00095787, dtype=float32), 'training/total_loss': Array(0.0343866, dtype=float32), 'training/v_loss': Array(0.00729265, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27474266, 0.09413365], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.145245,  8.116961], dtype=float32), 'eval/episode_reward': Array([-0.4265107,  2.572306 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2700562 , 0.09633819], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.475245475769043, 'eval/sps': 28601.78300677551}
I0728 03:57:15.869337 140337775261504 train.py:379] starting iteration 18, 7372800 steps, 577.3006608486176
I0728 03:57:44.754931 140337775261504 train.py:394] {'eval/walltime': 101.37304639816284, 'training/sps': 16783.488664987282, 'training/walltime': 495.7601556777954, 'training/entropy_loss': Array(0.02958067, dtype=float32), 'training/policy_loss': Array(0.00143684, dtype=float32), 'training/total_loss': Array(0.03538115, dtype=float32), 'training/v_loss': Array(0.00436363, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28079218, 0.10375622], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.652042,  8.954728], dtype=float32), 'eval/episode_reward': Array([0.00888582, 2.5761282 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.275935  , 0.10619816], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.472867250442505, 'eval/sps': 28616.990586370932}
I0728 03:57:44.758176 140337775261504 train.py:379] starting iteration 19, 7782400 steps, 606.1894993782043
I0728 03:58:13.633395 140337775261504 train.py:394] {'eval/walltime': 105.84837460517883, 'training/sps': 16792.167260330014, 'training/walltime': 520.1524806022644, 'training/entropy_loss': Array(0.03231847, dtype=float32), 'training/policy_loss': Array(0.00143981, dtype=float32), 'training/total_loss': Array(0.03700369, dtype=float32), 'training/v_loss': Array(0.00324541, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27681512, 0.1097846 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.30296 ,  9.508079], dtype=float32), 'eval/episode_reward': Array([-0.50448924,  4.0861177 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27088523, 0.11474554], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.475328207015991, 'eval/sps': 28601.254272107653}
I0728 03:58:13.636575 140337775261504 train.py:379] starting iteration 20, 8192000 steps, 635.0678989887238
I0728 03:58:42.489718 140337775261504 train.py:394] {'eval/walltime': 110.31193161010742, 'training/sps': 16799.154810289307, 'training/walltime': 544.5346596240997, 'training/entropy_loss': Array(0.02885652, dtype=float32), 'training/policy_loss': Array(0.00134652, dtype=float32), 'training/total_loss': Array(0.894768, dtype=float32), 'training/v_loss': Array(0.86456496, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27992415, 0.09836152], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.609787,  8.477768], dtype=float32), 'eval/episode_reward': Array([-0.21596554,  2.8633866 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27525416, 0.10048708], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.463557004928589, 'eval/sps': 28676.680920320818}
I0728 03:58:42.492990 140337775261504 train.py:379] starting iteration 21, 8601600 steps, 663.9243144989014
I0728 03:59:11.418373 140337775261504 train.py:394] {'eval/walltime': 114.7991988658905, 'training/sps': 16766.050858006747, 'training/walltime': 568.9649803638458, 'training/entropy_loss': Array(0.024633, dtype=float32), 'training/policy_loss': Array(0.00112033, dtype=float32), 'training/total_loss': Array(0.05100013, dtype=float32), 'training/v_loss': Array(0.0252468, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28413343, 0.10072768], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.98584,  8.69055], dtype=float32), 'eval/episode_reward': Array([-0.22081366,  1.923427  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2792997 , 0.10410776], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.487267255783081, 'eval/sps': 28525.156337643297}
I0728 03:59:11.421652 140337775261504 train.py:379] starting iteration 22, 9011200 steps, 692.8529758453369
I0728 03:59:40.286975 140337775261504 train.py:394] {'eval/walltime': 119.28044319152832, 'training/sps': 16802.99152645484, 'training/walltime': 593.3415920734406, 'training/entropy_loss': Array(0.02353758, dtype=float32), 'training/policy_loss': Array(0.00109388, dtype=float32), 'training/total_loss': Array(0.03162621, dtype=float32), 'training/v_loss': Array(0.00699475, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26472104, 0.10671192], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.232973,  9.211661], dtype=float32), 'eval/episode_reward': Array([-0.6556201,  4.586574 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2589935 , 0.10978559], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.481244325637817, 'eval/sps': 28563.495024739965}
I0728 03:59:40.290296 140337775261504 train.py:379] starting iteration 23, 9420800 steps, 721.7216203212738
I0728 04:00:09.179718 140337775261504 train.py:394] {'eval/walltime': 123.72830319404602, 'training/sps': 16764.30175766065, 'training/walltime': 617.7744617462158, 'training/entropy_loss': Array(0.02922562, dtype=float32), 'training/policy_loss': Array(0.00179359, dtype=float32), 'training/total_loss': Array(0.03529339, dtype=float32), 'training/v_loss': Array(0.00427418, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2697388 , 0.10855673], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.700367,  9.364699], dtype=float32), 'eval/episode_reward': Array([-0.2046048,  2.9739916], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26436543, 0.11172879], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4478600025177, 'eval/sps': 28777.88417970573}
I0728 04:00:09.183168 140337775261504 train.py:379] starting iteration 24, 9830400 steps, 750.6144914627075
I0728 04:00:38.057764 140337775261504 train.py:394] {'eval/walltime': 128.16866993904114, 'training/sps': 16768.783136168164, 'training/walltime': 642.2008018493652, 'training/entropy_loss': Array(0.03352366, dtype=float32), 'training/policy_loss': Array(0.00181605, dtype=float32), 'training/total_loss': Array(0.03862339, dtype=float32), 'training/v_loss': Array(0.00328367, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27419305, 0.09937687], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.103128,  8.590434], dtype=float32), 'eval/episode_reward': Array([-0.34583294,  2.1503394 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26925516, 0.10184127], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.440366744995117, 'eval/sps': 28826.447757783295}
I0728 04:00:38.061067 140337775261504 train.py:379] starting iteration 25, 10240000 steps, 779.492390871048
I0728 04:01:06.955492 140337775261504 train.py:394] {'eval/walltime': 132.62616658210754, 'training/sps': 16766.756262140803, 'training/walltime': 666.6300947666168, 'training/entropy_loss': Array(0.03633886, dtype=float32), 'training/policy_loss': Array(0.00192959, dtype=float32), 'training/total_loss': Array(1.2787759, dtype=float32), 'training/v_loss': Array(1.2405075, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27209938, 0.10560831], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.903233,  9.128617], dtype=float32), 'eval/episode_reward': Array([-0.08731154,  2.7028415 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2667126, 0.1089448], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.457496643066406, 'eval/sps': 28715.669410341066}
I0728 04:01:07.005977 140337775261504 train.py:379] starting iteration 26, 10649600 steps, 808.437283039093
I0728 04:01:35.924724 140337775261504 train.py:394] {'eval/walltime': 137.08880758285522, 'training/sps': 16754.01365444956, 'training/walltime': 691.0779678821564, 'training/entropy_loss': Array(0.04012733, dtype=float32), 'training/policy_loss': Array(0.00198868, dtype=float32), 'training/total_loss': Array(0.06546295, dtype=float32), 'training/v_loss': Array(0.02334693, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28229833, 0.10230848], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.809488,  8.888359], dtype=float32), 'eval/episode_reward': Array([-0.44459373,  2.7290292 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27699825, 0.10655102], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.462641000747681, 'eval/sps': 28682.567111841305}
I0728 04:01:35.928305 140337775261504 train.py:379] starting iteration 27, 11059200 steps, 837.3596293926239
I0728 04:02:04.862195 140337775261504 train.py:394] {'eval/walltime': 141.5723638534546, 'training/sps': 16757.68462400205, 'training/walltime': 715.5204854011536, 'training/entropy_loss': Array(0.04216006, dtype=float32), 'training/policy_loss': Array(0.00167876, dtype=float32), 'training/total_loss': Array(0.04901662, dtype=float32), 'training/v_loss': Array(0.00517782, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2801751 , 0.09555804], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.616095,  8.316883], dtype=float32), 'eval/episode_reward': Array([-0.14334887,  2.3623943 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27515376, 0.09927712], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.483556270599365, 'eval/sps': 28548.76626381425}
I0728 04:02:04.865393 140337775261504 train.py:379] starting iteration 28, 11468800 steps, 866.2967174053192
I0728 04:02:33.737646 140337775261504 train.py:394] {'eval/walltime': 146.0397868156433, 'training/sps': 16789.0519537162, 'training/walltime': 739.9173364639282, 'training/entropy_loss': Array(0.0437681, dtype=float32), 'training/policy_loss': Array(0.00197095, dtype=float32), 'training/total_loss': Array(0.04933127, dtype=float32), 'training/v_loss': Array(0.00359222, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28419375, 0.10433571], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.933157,  8.995058], dtype=float32), 'eval/episode_reward': Array([0.32591543, 2.5869045 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27925265, 0.10728585], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.467422962188721, 'eval/sps': 28651.865087179718}
I0728 04:02:33.740876 140337775261504 train.py:379] starting iteration 29, 11878400 steps, 895.1721999645233
I0728 04:03:02.626278 140337775261504 train.py:394] {'eval/walltime': 150.5077052116394, 'training/sps': 16780.674228205447, 'training/walltime': 764.3263676166534, 'training/entropy_loss': Array(0.04544329, dtype=float32), 'training/policy_loss': Array(0.00158195, dtype=float32), 'training/total_loss': Array(0.05164349, dtype=float32), 'training/v_loss': Array(0.00461825, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29363465, 0.10030369], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.766897,  8.619352], dtype=float32), 'eval/episode_reward': Array([-0.23756725,  3.470765  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28937888, 0.10203236], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.467918395996094, 'eval/sps': 28648.687969481864}
I0728 04:03:02.629715 140337775261504 train.py:379] starting iteration 30, 12288000 steps, 924.0610392093658
I0728 04:03:31.517465 140337775261504 train.py:394] {'eval/walltime': 154.96363019943237, 'training/sps': 16770.45017150893, 'training/walltime': 788.7502796649933, 'training/entropy_loss': Array(0.04625685, dtype=float32), 'training/policy_loss': Array(0.00248963, dtype=float32), 'training/total_loss': Array(1.367892, dtype=float32), 'training/v_loss': Array(1.3191457, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2814867 , 0.10227821], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.733406,  8.80552 ], dtype=float32), 'eval/episode_reward': Array([-0.4330069,  2.326484 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27655077, 0.10474032], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.455924987792969, 'eval/sps': 28725.79775257813}
I0728 04:03:31.521134 140337775261504 train.py:379] starting iteration 31, 12697600 steps, 952.9524571895599
I0728 04:04:00.422557 140337775261504 train.py:394] {'eval/walltime': 159.4274263381958, 'training/sps': 16766.70471703111, 'training/walltime': 813.1796476840973, 'training/entropy_loss': Array(0.05102318, dtype=float32), 'training/policy_loss': Array(0.00239412, dtype=float32), 'training/total_loss': Array(0.07918784, dtype=float32), 'training/v_loss': Array(0.02577055, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29504165, 0.10036845], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.872286,  8.701574], dtype=float32), 'eval/episode_reward': Array([-0.47171903,  2.8073015 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.290445  , 0.10301577], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.463796138763428, 'eval/sps': 28675.14465735859}
I0728 04:04:00.425974 140337775261504 train.py:379] starting iteration 32, 13107200 steps, 981.8572978973389
I0728 04:04:29.293436 140337775261504 train.py:394] {'eval/walltime': 163.87884974479675, 'training/sps': 16781.496592685555, 'training/walltime': 837.5874826908112, 'training/entropy_loss': Array(0.05403787, dtype=float32), 'training/policy_loss': Array(0.00268343, dtype=float32), 'training/total_loss': Array(0.06403716, dtype=float32), 'training/v_loss': Array(0.00731586, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.283908  , 0.10315305], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.935617,  8.9332  ], dtype=float32), 'eval/episode_reward': Array([0.00932474, 2.4666178 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27867377, 0.10690445], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.451423406600952, 'eval/sps': 28754.84722711181}
I0728 04:04:29.296851 140337775261504 train.py:379] starting iteration 33, 13516800 steps, 1010.7281744480133
I0728 04:04:58.164277 140337775261504 train.py:394] {'eval/walltime': 168.32049894332886, 'training/sps': 16774.487852336984, 'training/walltime': 862.0055158138275, 'training/entropy_loss': Array(0.05512259, dtype=float32), 'training/policy_loss': Array(0.00241514, dtype=float32), 'training/total_loss': Array(0.06416973, dtype=float32), 'training/v_loss': Array(0.00663201, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27625597, 0.10281494], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.257755,  8.898885], dtype=float32), 'eval/episode_reward': Array([-0.19643557,  2.2679918 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2710101 , 0.10603178], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4416491985321045, 'eval/sps': 28818.124592617984}
I0728 04:04:58.167669 140337775261504 train.py:379] starting iteration 34, 13926400 steps, 1039.598993062973
I0728 04:05:27.031107 140337775261504 train.py:394] {'eval/walltime': 172.76505517959595, 'training/sps': 16779.67363227673, 'training/walltime': 886.4160025119781, 'training/entropy_loss': Array(0.05903376, dtype=float32), 'training/policy_loss': Array(0.00265751, dtype=float32), 'training/total_loss': Array(0.06854661, dtype=float32), 'training/v_loss': Array(0.00685534, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25653377, 0.11215404], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.507109,  9.743383], dtype=float32), 'eval/episode_reward': Array([-0.24546796,  3.3725657 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2504871 , 0.11615483], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.44455623626709, 'eval/sps': 28799.275607210027}
I0728 04:05:27.034507 140337775261504 train.py:379] starting iteration 35, 14336000 steps, 1068.4658317565918
I0728 04:05:55.919877 140337775261504 train.py:394] {'eval/walltime': 177.2102768421173, 'training/sps': 16764.84488697076, 'training/walltime': 910.8480806350708, 'training/entropy_loss': Array(0.06254733, dtype=float32), 'training/policy_loss': Array(0.00241416, dtype=float32), 'training/total_loss': Array(1.2817357, dtype=float32), 'training/v_loss': Array(1.2167742, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26149684, 0.09018376], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.962505,  7.798951], dtype=float32), 'eval/episode_reward': Array([0.11386327, 2.6586196 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25570112, 0.09364295], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.445221662521362, 'eval/sps': 28794.96450743864}
I0728 04:05:55.923276 140337775261504 train.py:379] starting iteration 36, 14745600 steps, 1097.3545997142792
I0728 04:06:24.755486 140337775261504 train.py:394] {'eval/walltime': 181.64864993095398, 'training/sps': 16796.71757961954, 'training/walltime': 935.2337975502014, 'training/entropy_loss': Array(0.06879962, dtype=float32), 'training/policy_loss': Array(0.00257983, dtype=float32), 'training/total_loss': Array(0.0929326, dtype=float32), 'training/v_loss': Array(0.02155314, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28780076, 0.10048401], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.27917 ,  8.636655], dtype=float32), 'eval/episode_reward': Array([-0.2727426,  2.6049178], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28323936, 0.10296758], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.43837308883667, 'eval/sps': 28839.39620171718}
I0728 04:06:24.758888 140337775261504 train.py:379] starting iteration 37, 15155200 steps, 1126.1902112960815
I0728 04:06:53.661842 140337775261504 train.py:394] {'eval/walltime': 186.10805201530457, 'training/sps': 16762.333045330583, 'training/walltime': 959.6695368289948, 'training/entropy_loss': Array(0.07400848, dtype=float32), 'training/policy_loss': Array(0.00342475, dtype=float32), 'training/total_loss': Array(0.08499867, dtype=float32), 'training/v_loss': Array(0.00756543, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28641474, 0.09789117], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.061283,  8.472806], dtype=float32), 'eval/episode_reward': Array([-0.1006898,  2.3592613], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28088033, 0.10148412], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.459402084350586, 'eval/sps': 28703.39959906091}
I0728 04:06:53.665067 140337775261504 train.py:379] starting iteration 38, 15564800 steps, 1155.0963904857635
I0728 04:07:22.529266 140337775261504 train.py:394] {'eval/walltime': 190.5379831790924, 'training/sps': 16768.689187051077, 'training/walltime': 984.0960137844086, 'training/entropy_loss': Array(0.07887354, dtype=float32), 'training/policy_loss': Array(0.00299146, dtype=float32), 'training/total_loss': Array(0.090313, dtype=float32), 'training/v_loss': Array(0.008448, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27970776, 0.10121673], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.534555,  8.721087], dtype=float32), 'eval/episode_reward': Array([-0.12449565,  2.3233225 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27362373, 0.10450917], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.429931163787842, 'eval/sps': 28894.354171082145}
I0728 04:07:22.532610 140337775261504 train.py:379] starting iteration 39, 15974400 steps, 1183.9639346599579
I0728 04:07:51.417121 140337775261504 train.py:394] {'eval/walltime': 195.00149631500244, 'training/sps': 16777.542211782653, 'training/walltime': 1008.5096015930176, 'training/entropy_loss': Array(0.0816249, dtype=float32), 'training/policy_loss': Array(0.00427417, dtype=float32), 'training/total_loss': Array(0.09740995, dtype=float32), 'training/v_loss': Array(0.01151088, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28844026, 0.10471004], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.241724,  9.039043], dtype=float32), 'eval/episode_reward': Array([-0.31701207,  3.8217518 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28271323, 0.10799091], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.463513135910034, 'eval/sps': 28676.962765094}
I0728 04:07:51.420450 140337775261504 train.py:379] starting iteration 40, 16384000 steps, 1212.8517744541168
I0728 04:08:20.255356 140337775261504 train.py:394] {'eval/walltime': 199.4667580127716, 'training/sps': 16813.17155846584, 'training/walltime': 1032.8714537620544, 'training/entropy_loss': Array(0.08108857, dtype=float32), 'training/policy_loss': Array(0.00463013, dtype=float32), 'training/total_loss': Array(1.0355062, dtype=float32), 'training/v_loss': Array(0.9497875, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2826852 , 0.09779566], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.762001,  8.523403], dtype=float32), 'eval/episode_reward': Array([-0.13217798,  2.0670202 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27664804, 0.1020318 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.465261697769165, 'eval/sps': 28665.733088824003}
I0728 04:08:20.258618 140337775261504 train.py:379] starting iteration 41, 16793600 steps, 1241.689942598343
I0728 04:08:49.140012 140337775261504 train.py:394] {'eval/walltime': 203.9397051334381, 'training/sps': 16786.277803229343, 'training/walltime': 1057.2723367214203, 'training/entropy_loss': Array(0.08273615, dtype=float32), 'training/policy_loss': Array(0.00424955, dtype=float32), 'training/total_loss': Array(0.11010928, dtype=float32), 'training/v_loss': Array(0.02312358, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29565632, 0.09169973], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.892662 ,  7.9496803], dtype=float32), 'eval/episode_reward': Array([-0.08157631,  2.3401484 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29069042, 0.09416641], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.472947120666504, 'eval/sps': 28616.479593196487}
I0728 04:08:49.143301 140337775261504 train.py:379] starting iteration 42, 17203200 steps, 1270.5746245384216
I0728 04:09:18.020652 140337775261504 train.py:394] {'eval/walltime': 208.4211323261261, 'training/sps': 16795.114113551183, 'training/walltime': 1081.6603817939758, 'training/entropy_loss': Array(0.08479981, dtype=float32), 'training/policy_loss': Array(0.0052404, dtype=float32), 'training/total_loss': Array(0.10282192, dtype=float32), 'training/v_loss': Array(0.01278169, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2704817 , 0.09667396], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.763962,  8.37933 ], dtype=float32), 'eval/episode_reward': Array([0.21475409, 2.4205453 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2647831 , 0.10006266], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.481427192687988, 'eval/sps': 28562.32947594197}
I0728 04:09:18.023914 140337775261504 train.py:379] starting iteration 43, 17612800 steps, 1299.4552376270294
I0728 04:09:46.892057 140337775261504 train.py:394] {'eval/walltime': 212.90689539909363, 'training/sps': 16804.326926072477, 'training/walltime': 1106.0350563526154, 'training/entropy_loss': Array(0.08551502, dtype=float32), 'training/policy_loss': Array(0.00425921, dtype=float32), 'training/total_loss': Array(0.10388456, dtype=float32), 'training/v_loss': Array(0.01411032, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2707594 , 0.09597978], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.825428,  8.279577], dtype=float32), 'eval/episode_reward': Array([0.03883053, 2.8325891 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26533645, 0.09961852], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.485763072967529, 'eval/sps': 28534.721499529038}
I0728 04:09:46.895350 140337775261504 train.py:379] starting iteration 44, 18022400 steps, 1328.3266744613647
I0728 04:10:15.770696 140337775261504 train.py:394] {'eval/walltime': 217.38176345825195, 'training/sps': 16792.02594374079, 'training/walltime': 1130.427586555481, 'training/entropy_loss': Array(0.08774051, dtype=float32), 'training/policy_loss': Array(0.00491035, dtype=float32), 'training/total_loss': Array(0.10146447, dtype=float32), 'training/v_loss': Array(0.00881361, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2652322 , 0.09564884], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.36118 ,  8.271358], dtype=float32), 'eval/episode_reward': Array([-0.18167037,  2.3720188 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2601262 , 0.09827179], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.474868059158325, 'eval/sps': 28604.195321029292}
I0728 04:10:15.774116 140337775261504 train.py:379] starting iteration 45, 18432000 steps, 1357.2054398059845
I0728 04:10:44.647776 140337775261504 train.py:394] {'eval/walltime': 221.86627006530762, 'training/sps': 16799.772976426513, 'training/walltime': 1154.8088684082031, 'training/entropy_loss': Array(0.08813028, dtype=float32), 'training/policy_loss': Array(0.00422165, dtype=float32), 'training/total_loss': Array(0.8751513, dtype=float32), 'training/v_loss': Array(0.78279924, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.265143  , 0.10098868], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.390453,  8.666723], dtype=float32), 'eval/episode_reward': Array([0.10900148, 1.9842949 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25962853, 0.1043548 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.484506607055664, 'eval/sps': 28542.716337759917}
I0728 04:10:44.651340 140337775261504 train.py:379] starting iteration 46, 18841600 steps, 1386.0826642513275
I0728 04:11:13.519455 140337775261504 train.py:394] {'eval/walltime': 226.34958696365356, 'training/sps': 16803.26910774382, 'training/walltime': 1179.1850774288177, 'training/entropy_loss': Array(0.08742563, dtype=float32), 'training/policy_loss': Array(0.0033732, dtype=float32), 'training/total_loss': Array(0.09987746, dtype=float32), 'training/v_loss': Array(0.00907863, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27833378, 0.09648556], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.506966,  8.291183], dtype=float32), 'eval/episode_reward': Array([-0.37582785,  2.3207805 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27408537, 0.09904185], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.483316898345947, 'eval/sps': 28550.29053315943}
I0728 04:11:13.526546 140337775261504 train.py:379] starting iteration 47, 19251200 steps, 1414.9578545093536
I0728 04:11:42.412080 140337775261504 train.py:394] {'eval/walltime': 230.83470749855042, 'training/sps': 16792.43053242126, 'training/walltime': 1203.5770199298859, 'training/entropy_loss': Array(0.08894359, dtype=float32), 'training/policy_loss': Array(0.00504142, dtype=float32), 'training/total_loss': Array(0.09708394, dtype=float32), 'training/v_loss': Array(0.00309892, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28294533, 0.10057655], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.935486,  8.650782], dtype=float32), 'eval/episode_reward': Array([-0.13178338,  1.6366694 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27867866, 0.10298549], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.485120534896851, 'eval/sps': 28538.80938184056}
I0728 04:11:42.415496 140337775261504 train.py:379] starting iteration 48, 19660800 steps, 1443.8468203544617
I0728 04:12:11.239235 140337775261504 train.py:394] {'eval/walltime': 235.29812145233154, 'training/sps': 16819.831441096045, 'training/walltime': 1227.9292259216309, 'training/entropy_loss': Array(0.09145716, dtype=float32), 'training/policy_loss': Array(0.00395621, dtype=float32), 'training/total_loss': Array(0.09700525, dtype=float32), 'training/v_loss': Array(0.00159188, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27034014, 0.0924533 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.832346,  7.982234], dtype=float32), 'eval/episode_reward': Array([0.08540289, 2.1413202 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2656177 , 0.09518988], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.463413953781128, 'eval/sps': 28677.599999786336}
I0728 04:12:11.242659 140337775261504 train.py:379] starting iteration 49, 20070400 steps, 1472.6739830970764
I0728 04:12:40.153776 140337775261504 train.py:394] {'eval/walltime': 239.7851791381836, 'training/sps': 16775.905218577598, 'training/walltime': 1252.3451960086823, 'training/entropy_loss': Array(0.09350543, dtype=float32), 'training/policy_loss': Array(0.00408254, dtype=float32), 'training/total_loss': Array(0.09855253, dtype=float32), 'training/v_loss': Array(0.00096457, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26897037, 0.0995247 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.734356,  8.555749], dtype=float32), 'eval/episode_reward': Array([-0.43984205,  2.224411  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26443207, 0.10185273], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.487057685852051, 'eval/sps': 28526.488617160263}
I0728 04:12:40.157260 140337775261504 train.py:379] starting iteration 50, 20480000 steps, 1501.5885841846466
I0728 04:13:09.032181 140337775261504 train.py:394] {'eval/walltime': 244.26132702827454, 'training/sps': 16793.289342660883, 'training/walltime': 1276.7358911037445, 'training/entropy_loss': Array(0.09369242, dtype=float32), 'training/policy_loss': Array(0.00489502, dtype=float32), 'training/total_loss': Array(0.7599797, dtype=float32), 'training/v_loss': Array(0.66139233, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27300864, 0.09335094], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.032179,  8.047789], dtype=float32), 'eval/episode_reward': Array([-0.01576321,  1.7731276 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26818216, 0.09608208], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.476147890090942, 'eval/sps': 28596.01674094808}
I0728 04:13:09.115468 140337775261504 train.py:379] starting iteration 51, 20889600 steps, 1530.5467739105225
I0728 04:13:37.943759 140337775261504 train.py:394] {'eval/walltime': 248.7228877544403, 'training/sps': 16815.59513266837, 'training/walltime': 1301.094232082367, 'training/entropy_loss': Array(0.09245237, dtype=float32), 'training/policy_loss': Array(0.00335567, dtype=float32), 'training/total_loss': Array(0.10248573, dtype=float32), 'training/v_loss': Array(0.00667769, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26645792, 0.09866284], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.484734,  8.500514], dtype=float32), 'eval/episode_reward': Array([-0.46936634,  3.8443682 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26121807, 0.10162845], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4615607261657715, 'eval/sps': 28689.512001779283}
I0728 04:13:37.947561 140337775261504 train.py:379] starting iteration 52, 21299200 steps, 1559.3788855075836
I0728 04:14:06.757419 140337775261504 train.py:394] {'eval/walltime': 253.18994617462158, 'training/sps': 16831.612352201053, 'training/walltime': 1325.4293932914734, 'training/entropy_loss': Array(0.09223761, dtype=float32), 'training/policy_loss': Array(0.00302503, dtype=float32), 'training/total_loss': Array(0.09743117, dtype=float32), 'training/v_loss': Array(0.00216853, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2650034 , 0.10321966], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.3791  ,  8.915321], dtype=float32), 'eval/episode_reward': Array([-0.04213637,  2.885792  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2600937 , 0.10592153], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.467058420181274, 'eval/sps': 28654.203272050723}
I0728 04:14:06.760818 140337775261504 train.py:379] starting iteration 53, 21708800 steps, 1588.1921424865723
I0728 04:14:35.583560 140337775261504 train.py:394] {'eval/walltime': 257.6433963775635, 'training/sps': 16813.497359937854, 'training/walltime': 1349.7907733917236, 'training/entropy_loss': Array(0.09051222, dtype=float32), 'training/policy_loss': Array(0.00277369, dtype=float32), 'training/total_loss': Array(0.09449033, dtype=float32), 'training/v_loss': Array(0.00120442, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26407957, 0.11224452], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.305914,  9.643467], dtype=float32), 'eval/episode_reward': Array([-0.2203358,  1.7682846], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2588802 , 0.11567825], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4534502029418945, 'eval/sps': 28741.760694987624}
I0728 04:14:35.587002 140337775261504 train.py:379] starting iteration 54, 22118400 steps, 1617.0183265209198
I0728 04:15:04.428885 140337775261504 train.py:394] {'eval/walltime': 262.0969259738922, 'training/sps': 16800.340750274117, 'training/walltime': 1374.1712312698364, 'training/entropy_loss': Array(0.08906955, dtype=float32), 'training/policy_loss': Array(0.00402618, dtype=float32), 'training/total_loss': Array(0.09392124, dtype=float32), 'training/v_loss': Array(0.00082552, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27949476, 0.10451917], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.576588,  9.066367], dtype=float32), 'eval/episode_reward': Array([-0.56272036,  2.946533  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27425617, 0.1086067 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.453529596328735, 'eval/sps': 28741.24831358856}
I0728 04:15:04.432284 140337775261504 train.py:379] starting iteration 55, 22528000 steps, 1645.8636088371277
I0728 04:15:33.325432 140337775261504 train.py:394] {'eval/walltime': 266.5639638900757, 'training/sps': 16774.330946126727, 'training/walltime': 1398.5894927978516, 'training/entropy_loss': Array(0.0843334, dtype=float32), 'training/policy_loss': Array(0.00274779, dtype=float32), 'training/total_loss': Array(0.8230971, dtype=float32), 'training/v_loss': Array(0.7360159, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28030586, 0.10200299], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.670458,  8.753621], dtype=float32), 'eval/episode_reward': Array([-0.04938328,  1.9850614 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27578205, 0.10449205], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.467037916183472, 'eval/sps': 28654.334796728137}
I0728 04:15:33.328863 140337775261504 train.py:379] starting iteration 56, 22937600 steps, 1674.7601869106293
I0728 04:16:02.130862 140337775261504 train.py:394] {'eval/walltime': 271.015741109848, 'training/sps': 16826.658276127077, 'training/walltime': 1422.9318187236786, 'training/entropy_loss': Array(0.07845389, dtype=float32), 'training/policy_loss': Array(0.00224251, dtype=float32), 'training/total_loss': Array(0.08862923, dtype=float32), 'training/v_loss': Array(0.00793282, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27830347, 0.10476326], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.44963 ,  9.063079], dtype=float32), 'eval/episode_reward': Array([-0.39033407,  2.807818  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27264935, 0.10915156], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.451777219772339, 'eval/sps': 28752.56188281269}
I0728 04:16:02.134314 140337775261504 train.py:379] starting iteration 57, 23347200 steps, 1703.5656380653381
I0728 04:16:30.996512 140337775261504 train.py:394] {'eval/walltime': 275.46148586273193, 'training/sps': 16780.990575741514, 'training/walltime': 1447.3403897285461, 'training/entropy_loss': Array(0.07252812, dtype=float32), 'training/policy_loss': Array(0.00254755, dtype=float32), 'training/total_loss': Array(0.07767619, dtype=float32), 'training/v_loss': Array(0.00260052, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28382236, 0.10016877], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.983265 ,  8.6051235], dtype=float32), 'eval/episode_reward': Array([-0.3671341,  2.0916293], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27901453, 0.10363928], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.445744752883911, 'eval/sps': 28791.5764657806}
I0728 04:16:30.999876 140337775261504 train.py:379] starting iteration 58, 23756800 steps, 1732.4311997890472
I0728 04:16:59.867192 140337775261504 train.py:394] {'eval/walltime': 279.9150047302246, 'training/sps': 16782.512488829118, 'training/walltime': 1471.7467472553253, 'training/entropy_loss': Array(0.06846008, dtype=float32), 'training/policy_loss': Array(0.00259096, dtype=float32), 'training/total_loss': Array(0.07269326, dtype=float32), 'training/v_loss': Array(0.00164222, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28248137, 0.10705825], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.873268,  9.182148], dtype=float32), 'eval/episode_reward': Array([-0.25755477,  1.7195323 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27774954, 0.11024788], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.453518867492676, 'eval/sps': 28741.317553250607}
I0728 04:16:59.870548 140337775261504 train.py:379] starting iteration 59, 24166400 steps, 1761.3018724918365
I0728 04:17:28.698447 140337775261504 train.py:394] {'eval/walltime': 284.39442443847656, 'training/sps': 16827.79024738284, 'training/walltime': 1496.087435722351, 'training/entropy_loss': Array(0.06458407, dtype=float32), 'training/policy_loss': Array(0.00136048, dtype=float32), 'training/total_loss': Array(0.06722936, dtype=float32), 'training/v_loss': Array(0.00128481, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29651845, 0.10290393], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.08265 ,  8.793427], dtype=float32), 'eval/episode_reward': Array([-0.51389015,  2.820265  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29218248, 0.10526843], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.479419708251953, 'eval/sps': 28575.129891088207}
I0728 04:17:28.701848 140337775261504 train.py:379] starting iteration 60, 24576000 steps, 1790.133172750473
I0728 04:17:57.600242 140337775261504 train.py:394] {'eval/walltime': 288.8773877620697, 'training/sps': 16781.435941166845, 'training/walltime': 1520.4953589439392, 'training/entropy_loss': Array(0.05844833, dtype=float32), 'training/policy_loss': Array(0.00168463, dtype=float32), 'training/total_loss': Array(0.81189376, dtype=float32), 'training/v_loss': Array(0.7517607, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28571942, 0.09913585], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.164448 ,  8.5048065], dtype=float32), 'eval/episode_reward': Array([-0.31907144,  2.453849  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28111488, 0.10190126], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.48296332359314, 'eval/sps': 28552.542316453022}
I0728 04:17:57.603664 140337775261504 train.py:379] starting iteration 61, 24985600 steps, 1819.0349879264832
I0728 04:18:26.471618 140337775261504 train.py:394] {'eval/walltime': 293.3230412006378, 'training/sps': 16776.93010407196, 'training/walltime': 1544.9098374843597, 'training/entropy_loss': Array(0.05336494, dtype=float32), 'training/policy_loss': Array(0.00134649, dtype=float32), 'training/total_loss': Array(0.06349075, dtype=float32), 'training/v_loss': Array(0.00877932, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26852694, 0.10085059], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.674667,  8.66487 ], dtype=float32), 'eval/episode_reward': Array([0.01286703, 2.2032084 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2633675 , 0.10361362], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.445653438568115, 'eval/sps': 28792.16784860924}
I0728 04:18:26.475020 140337775261504 train.py:379] starting iteration 62, 25395200 steps, 1847.9063436985016
I0728 04:18:55.342334 140337775261504 train.py:394] {'eval/walltime': 297.7951247692108, 'training/sps': 16795.363357304122, 'training/walltime': 1569.2975206375122, 'training/entropy_loss': Array(0.05150387, dtype=float32), 'training/policy_loss': Array(0.00182423, dtype=float32), 'training/total_loss': Array(0.05678801, dtype=float32), 'training/v_loss': Array(0.00345991, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2827313 , 0.09262898], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.901028,  7.976051], dtype=float32), 'eval/episode_reward': Array([-0.01121987,  1.7889845 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27863383, 0.09545807], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.472083568572998, 'eval/sps': 28622.00538905485}
I0728 04:18:55.345841 140337775261504 train.py:379] starting iteration 63, 25804800 steps, 1876.777164697647
I0728 04:19:24.186735 140337775261504 train.py:394] {'eval/walltime': 302.26250982284546, 'training/sps': 16810.32610421836, 'training/walltime': 1593.6634964942932, 'training/entropy_loss': Array(0.05073372, dtype=float32), 'training/policy_loss': Array(0.00132872, dtype=float32), 'training/total_loss': Array(0.05440041, dtype=float32), 'training/v_loss': Array(0.00233796, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26613462, 0.10871916], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.455149,  9.39404 ], dtype=float32), 'eval/episode_reward': Array([-0.12290438,  2.97483   ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26106593, 0.11184594], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4673850536346436, 'eval/sps': 28652.108216160996}
I0728 04:19:24.190108 140337775261504 train.py:379] starting iteration 64, 26214400 steps, 1905.6214320659637
I0728 04:19:53.079319 140337775261504 train.py:394] {'eval/walltime': 306.74356627464294, 'training/sps': 16786.52416043314, 'training/walltime': 1618.0640213489532, 'training/entropy_loss': Array(0.04819471, dtype=float32), 'training/policy_loss': Array(0.00151423, dtype=float32), 'training/total_loss': Array(0.05171007, dtype=float32), 'training/v_loss': Array(0.00200113, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28600794, 0.09814897], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.181465,  8.41973 ], dtype=float32), 'eval/episode_reward': Array([-0.4416107,  2.1027865], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28135243, 0.10065133], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.481056451797485, 'eval/sps': 28564.69258463713}
I0728 04:19:53.082750 140337775261504 train.py:379] starting iteration 65, 26624000 steps, 1934.5140731334686
I0728 04:20:21.954647 140337775261504 train.py:394] {'eval/walltime': 311.20795035362244, 'training/sps': 16786.988191074903, 'training/walltime': 1642.463871717453, 'training/entropy_loss': Array(0.0444805, dtype=float32), 'training/policy_loss': Array(0.00149436, dtype=float32), 'training/total_loss': Array(0.8766896, dtype=float32), 'training/v_loss': Array(0.8307147, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26306227, 0.0948992 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.189968,  8.126811], dtype=float32), 'eval/episode_reward': Array([-0.3345937,  2.3438058], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25819266, 0.09764964], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.464384078979492, 'eval/sps': 28671.368263919478}
I0728 04:20:21.957993 140337775261504 train.py:379] starting iteration 66, 27033600 steps, 1963.3893160820007
I0728 04:20:50.813006 140337775261504 train.py:394] {'eval/walltime': 315.6903085708618, 'training/sps': 16810.980953747443, 'training/walltime': 1666.8288984298706, 'training/entropy_loss': Array(0.03990995, dtype=float32), 'training/policy_loss': Array(0.00075966, dtype=float32), 'training/total_loss': Array(0.05066888, dtype=float32), 'training/v_loss': Array(0.00999927, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26615113, 0.09501785], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.413883,  8.192767], dtype=float32), 'eval/episode_reward': Array([0.22305869, 2.595528  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2607356 , 0.09770212], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.48235821723938, 'eval/sps': 28556.39683319049}
I0728 04:20:50.816326 140337775261504 train.py:379] starting iteration 67, 27443200 steps, 1992.24765086174
I0728 04:21:19.666577 140337775261504 train.py:394] {'eval/walltime': 320.1552517414093, 'training/sps': 16802.245931651876, 'training/walltime': 1691.2065918445587, 'training/entropy_loss': Array(0.03283864, dtype=float32), 'training/policy_loss': Array(0.00093522, dtype=float32), 'training/total_loss': Array(0.03871264, dtype=float32), 'training/v_loss': Array(0.00493878, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2873038 , 0.10713962], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.249393,  9.230714], dtype=float32), 'eval/episode_reward': Array([-0.0196307,  2.7238014], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2821998 , 0.11047795], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.464943170547485, 'eval/sps': 28667.778090511467}
I0728 04:21:19.669950 140337775261504 train.py:379] starting iteration 68, 27852800 steps, 2021.1012744903564
I0728 04:21:48.522614 140337775261504 train.py:394] {'eval/walltime': 324.61208510398865, 'training/sps': 16795.203925829624, 'training/walltime': 1715.5945065021515, 'training/entropy_loss': Array(0.02845973, dtype=float32), 'training/policy_loss': Array(0.00094175, dtype=float32), 'training/total_loss': Array(0.03410181, dtype=float32), 'training/v_loss': Array(0.00470032, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27396682, 0.10340574], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.155247,  8.901642], dtype=float32), 'eval/episode_reward': Array([-0.07984724,  1.7717828 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2688015 , 0.10726003], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.456833362579346, 'eval/sps': 28719.942969983815}
I0728 04:21:48.525991 140337775261504 train.py:379] starting iteration 69, 28262400 steps, 2049.9573154449463
I0728 04:22:17.354489 140337775261504 train.py:394] {'eval/walltime': 329.0607256889343, 'training/sps': 16805.929850223394, 'training/walltime': 1739.9668562412262, 'training/entropy_loss': Array(0.02418106, dtype=float32), 'training/policy_loss': Array(0.00088689, dtype=float32), 'training/total_loss': Array(0.030496, dtype=float32), 'training/v_loss': Array(0.00542805, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27939147, 0.10734924], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.573322,  9.212992], dtype=float32), 'eval/episode_reward': Array([-0.05858467,  1.9861184 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27415305, 0.11087249], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.448640584945679, 'eval/sps': 28772.8346572109}
I0728 04:22:17.357840 140337775261504 train.py:379] starting iteration 70, 28672000 steps, 2078.7891640663147
I0728 04:22:46.208111 140337775261504 train.py:394] {'eval/walltime': 333.51909852027893, 'training/sps': 16798.13722688568, 'training/walltime': 1764.350512266159, 'training/entropy_loss': Array(0.01999852, dtype=float32), 'training/policy_loss': Array(0.00146815, dtype=float32), 'training/total_loss': Array(0.8586494, dtype=float32), 'training/v_loss': Array(0.83718264, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28385025, 0.10593556], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.957832,  9.102846], dtype=float32), 'eval/episode_reward': Array([-0.41074395,  1.7585598 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27874166, 0.10927175], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4583728313446045, 'eval/sps': 28710.02602117427}
I0728 04:22:46.211483 140337775261504 train.py:379] starting iteration 71, 29081600 steps, 2107.642806529999
I0728 04:23:15.042402 140337775261504 train.py:394] {'eval/walltime': 337.96014523506165, 'training/sps': 16800.123066239037, 'training/walltime': 1788.7312860488892, 'training/entropy_loss': Array(0.01650267, dtype=float32), 'training/policy_loss': Array(0.00074241, dtype=float32), 'training/total_loss': Array(0.03318192, dtype=float32), 'training/v_loss': Array(0.01593683, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26787803, 0.09611346], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.592321,  8.284603], dtype=float32), 'eval/episode_reward': Array([-0.4516847,  2.7418215], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26292813, 0.09907726], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.441046714782715, 'eval/sps': 28822.034133064193}
I0728 04:23:15.045779 140337775261504 train.py:379] starting iteration 72, 29491200 steps, 2136.477103471756
I0728 04:23:43.881553 140337775261504 train.py:394] {'eval/walltime': 342.40008783340454, 'training/sps': 16795.113456792173, 'training/walltime': 1813.119332075119, 'training/entropy_loss': Array(0.01718442, dtype=float32), 'training/policy_loss': Array(0.00079423, dtype=float32), 'training/total_loss': Array(0.02751652, dtype=float32), 'training/v_loss': Array(0.00953787, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27581125, 0.10631756], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.295647,  9.114908], dtype=float32), 'eval/episode_reward': Array([-0.39508897,  2.3481302 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27103215, 0.10886121], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4399425983428955, 'eval/sps': 28829.20154142826}
I0728 04:23:43.884948 140337775261504 train.py:379] starting iteration 73, 29900800 steps, 2165.3162717819214
I0728 04:24:12.784568 140337775261504 train.py:394] {'eval/walltime': 346.8791344165802, 'training/sps': 16777.905630906254, 'training/walltime': 1837.5323910713196, 'training/entropy_loss': Array(0.01651838, dtype=float32), 'training/policy_loss': Array(0.00075109, dtype=float32), 'training/total_loss': Array(0.02655519, dtype=float32), 'training/v_loss': Array(0.00928571, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27733913, 0.09871633], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.398684,  8.51814 ], dtype=float32), 'eval/episode_reward': Array([-0.3713559,  2.5006547], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2728929 , 0.10121962], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.479046583175659, 'eval/sps': 28577.510330166642}
I0728 04:24:12.787875 140337775261504 train.py:379] starting iteration 74, 30310400 steps, 2194.21919298172
I0728 04:24:41.651750 140337775261504 train.py:394] {'eval/walltime': 351.32767963409424, 'training/sps': 16782.696927252357, 'training/walltime': 1861.9384803771973, 'training/entropy_loss': Array(0.01643253, dtype=float32), 'training/policy_loss': Array(0.00070536, dtype=float32), 'training/total_loss': Array(0.02560942, dtype=float32), 'training/v_loss': Array(0.00847152, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26638722, 0.10869662], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.460148,  9.326614], dtype=float32), 'eval/episode_reward': Array([-0.3936569,  2.788649 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2609076 , 0.11182462], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.448545217514038, 'eval/sps': 28773.451486130944}
I0728 04:24:41.658915 140337775261504 train.py:379] starting iteration 75, 30720000 steps, 2223.0902218818665
I0728 04:25:10.548154 140337775261504 train.py:394] {'eval/walltime': 355.77404260635376, 'training/sps': 16763.012947955172, 'training/walltime': 1886.3732285499573, 'training/entropy_loss': Array(0.01129735, dtype=float32), 'training/policy_loss': Array(0.00075593, dtype=float32), 'training/total_loss': Array(0.8219839, dtype=float32), 'training/v_loss': Array(0.80993056, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27368876, 0.09476828], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.129837,  8.159409], dtype=float32), 'eval/episode_reward': Array([-0.3228909,  2.156499 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26923084, 0.09720933], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4463629722595215, 'eval/sps': 28787.573303974747}
I0728 04:25:10.615462 140337775261504 train.py:379] starting iteration 76, 31129600 steps, 2252.0467762947083
I0728 04:25:39.473564 140337775261504 train.py:394] {'eval/walltime': 360.225163936615, 'training/sps': 16787.8830274334, 'training/walltime': 1910.771778345108, 'training/entropy_loss': Array(0.00978072, dtype=float32), 'training/policy_loss': Array(0.00072423, dtype=float32), 'training/total_loss': Array(0.03070829, dtype=float32), 'training/v_loss': Array(0.02020334, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28635642, 0.10893702], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.189915,  9.38732 ], dtype=float32), 'eval/episode_reward': Array([-0.73121166,  2.9775624 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28157485, 0.11213662], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4511213302612305, 'eval/sps': 28756.79868121857}
I0728 04:25:39.477320 140337775261504 train.py:379] starting iteration 77, 31539200 steps, 2280.90864443779
I0728 04:26:08.260887 140337775261504 train.py:394] {'eval/walltime': 364.7042233943939, 'training/sps': 16858.55209311165, 'training/walltime': 1935.0680522918701, 'training/entropy_loss': Array(0.00885962, dtype=float32), 'training/policy_loss': Array(0.00072361, dtype=float32), 'training/total_loss': Array(0.02450966, dtype=float32), 'training/v_loss': Array(0.01492643, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27592915, 0.09464736], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.312618,  8.150834], dtype=float32), 'eval/episode_reward': Array([-0.12303063,  1.8018292 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2707488 , 0.09805606], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.479059457778931, 'eval/sps': 28577.42818700434}
I0728 04:26:08.264211 140337775261504 train.py:379] starting iteration 78, 31948800 steps, 2309.6955358982086
I0728 04:26:37.067897 140337775261504 train.py:394] {'eval/walltime': 369.16339635849, 'training/sps': 16830.64920217168, 'training/walltime': 1959.404606103897, 'training/entropy_loss': Array(0.00755267, dtype=float32), 'training/policy_loss': Array(0.00061668, dtype=float32), 'training/total_loss': Array(0.02311368, dtype=float32), 'training/v_loss': Array(0.01494432, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29279882, 0.10321821], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.738144,  8.857597], dtype=float32), 'eval/episode_reward': Array([-0.55505955,  2.895337  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28832453, 0.1056989 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.459172964096069, 'eval/sps': 28704.87443089062}
I0728 04:26:37.071293 140337775261504 train.py:379] starting iteration 79, 32358400 steps, 2338.5026173591614
I0728 04:27:05.920202 140337775261504 train.py:394] {'eval/walltime': 373.60561060905457, 'training/sps': 16787.745555994385, 'training/walltime': 1983.8033556938171, 'training/entropy_loss': Array(0.00491372, dtype=float32), 'training/policy_loss': Array(0.00052193, dtype=float32), 'training/total_loss': Array(0.02238526, dtype=float32), 'training/v_loss': Array(0.01694961, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2921848 , 0.09367788], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.709492,  8.074036], dtype=float32), 'eval/episode_reward': Array([-0.3554364,  2.4028478], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2878908 , 0.09634867], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.442214250564575, 'eval/sps': 28814.458911731253}
I0728 04:27:05.923546 140337775261504 train.py:379] starting iteration 80, 32768000 steps, 2367.3548698425293
I0728 04:27:34.755588 140337775261504 train.py:394] {'eval/walltime': 378.0491659641266, 'training/sps': 16800.482864027308, 'training/walltime': 2008.183607339859, 'training/entropy_loss': Array(0.0001335, dtype=float32), 'training/policy_loss': Array(0.0008168, dtype=float32), 'training/total_loss': Array(0.9814786, dtype=float32), 'training/v_loss': Array(0.98052824, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.285843  , 0.09225811], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.141167,  7.937808], dtype=float32), 'eval/episode_reward': Array([-0.1556625,  2.1445625], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2813413 , 0.09452479], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4435553550720215, 'eval/sps': 28805.762451883616}
I0728 04:27:34.758980 140337775261504 train.py:379] starting iteration 81, 33177600 steps, 2396.190304517746
I0728 04:28:03.579306 140337775261504 train.py:394] {'eval/walltime': 382.5226321220398, 'training/sps': 16829.068925161893, 'training/walltime': 2032.5224463939667, 'training/entropy_loss': Array(-0.00207358, dtype=float32), 'training/policy_loss': Array(0.00066537, dtype=float32), 'training/total_loss': Array(0.03725808, dtype=float32), 'training/v_loss': Array(0.03866629, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28300217, 0.10598231], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.92306 ,  9.118031], dtype=float32), 'eval/episode_reward': Array([-0.6091959,  2.5231576], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2783589, 0.1085217], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.473466157913208, 'eval/sps': 28613.15934481322}
I0728 04:28:03.582674 140337775261504 train.py:379] starting iteration 82, 33587200 steps, 2425.013998270035
I0728 04:28:32.424390 140337775261504 train.py:394] {'eval/walltime': 387.00604605674744, 'training/sps': 16821.446053990145, 'training/walltime': 2056.872314929962, 'training/entropy_loss': Array(-0.00393582, dtype=float32), 'training/policy_loss': Array(0.00013269, dtype=float32), 'training/total_loss': Array(0.0318121, dtype=float32), 'training/v_loss': Array(0.03561522, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27888668, 0.09843609], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.534212,  8.461574], dtype=float32), 'eval/episode_reward': Array([-0.06683613,  2.1081567 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.274174  , 0.10114603], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.483413934707642, 'eval/sps': 28549.672607543147}
I0728 04:28:32.427783 140337775261504 train.py:379] starting iteration 83, 33996800 steps, 2453.859107732773
I0728 04:29:01.297767 140337775261504 train.py:394] {'eval/walltime': 391.4694483280182, 'training/sps': 16788.0511787344, 'training/walltime': 2081.2706203460693, 'training/entropy_loss': Array(-0.00344842, dtype=float32), 'training/policy_loss': Array(0.00048727, dtype=float32), 'training/total_loss': Array(0.02999573, dtype=float32), 'training/v_loss': Array(0.03295688, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28056082, 0.09672778], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.697258,  8.317648], dtype=float32), 'eval/episode_reward': Array([-0.6179298,  3.5863626], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27584973, 0.09919597], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.463402271270752, 'eval/sps': 28677.67506054474}
I0728 04:29:01.301072 140337775261504 train.py:379] starting iteration 84, 34406400 steps, 2482.732395887375
I0728 04:29:30.142553 140337775261504 train.py:394] {'eval/walltime': 395.93040919303894, 'training/sps': 16808.21533902409, 'training/walltime': 2105.6396560668945, 'training/entropy_loss': Array(-0.0023741, dtype=float32), 'training/policy_loss': Array(0.00055923, dtype=float32), 'training/total_loss': Array(0.02905271, dtype=float32), 'training/v_loss': Array(0.03086758, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2935875 , 0.09647063], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.829943,  8.288573], dtype=float32), 'eval/episode_reward': Array([-0.04412976,  2.2188787 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28921407, 0.09876347], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.460960865020752, 'eval/sps': 28693.369853045002}
I0728 04:29:30.145865 140337775261504 train.py:379] starting iteration 85, 34816000 steps, 2511.577189207077
I0728 04:29:59.028707 140337775261504 train.py:394] {'eval/walltime': 400.4041826725006, 'training/sps': 16785.8689187182, 'training/walltime': 2130.041133403778, 'training/entropy_loss': Array(0.0012392, dtype=float32), 'training/policy_loss': Array(0.00101896, dtype=float32), 'training/total_loss': Array(0.9513058, dtype=float32), 'training/v_loss': Array(0.9490477, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28235   , 0.10303045], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.845499,  8.891466], dtype=float32), 'eval/episode_reward': Array([-0.19169828,  1.9899815 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2778467 , 0.10581133], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.47377347946167, 'eval/sps': 28611.19379146623}
I0728 04:29:59.031926 140337775261504 train.py:379] starting iteration 86, 35225600 steps, 2540.463250398636
I0728 04:30:27.893179 140337775261504 train.py:394] {'eval/walltime': 404.8668963909149, 'training/sps': 16793.222368062183, 'training/walltime': 2154.4319257736206, 'training/entropy_loss': Array(0.00299547, dtype=float32), 'training/policy_loss': Array(0.00080923, dtype=float32), 'training/total_loss': Array(0.03136655, dtype=float32), 'training/v_loss': Array(0.02756185, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26146388, 0.10920168], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.041353,  9.374776], dtype=float32), 'eval/episode_reward': Array([-0.13836378,  1.8680437 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25526857, 0.11311781], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.462713718414307, 'eval/sps': 28682.09974389328}
I0728 04:30:27.896425 140337775261504 train.py:379] starting iteration 87, 35635200 steps, 2569.327748775482
I0728 04:30:56.752818 140337775261504 train.py:394] {'eval/walltime': 409.3131356239319, 'training/sps': 16786.109851464757, 'training/walltime': 2178.8330528736115, 'training/entropy_loss': Array(0.00062049, dtype=float32), 'training/policy_loss': Array(0.00089483, dtype=float32), 'training/total_loss': Array(0.02568834, dtype=float32), 'training/v_loss': Array(0.02417302, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28557312, 0.10590655], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.130384,  9.087064], dtype=float32), 'eval/episode_reward': Array([-0.65512127,  3.1734693 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28079072, 0.10901771], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.446239233016968, 'eval/sps': 28788.374464760054}
I0728 04:30:56.756015 140337775261504 train.py:379] starting iteration 88, 36044800 steps, 2598.1873395442963
I0728 04:31:25.605837 140337775261504 train.py:394] {'eval/walltime': 413.7686884403229, 'training/sps': 16796.092905743655, 'training/walltime': 2203.219676733017, 'training/entropy_loss': Array(-0.0040073, dtype=float32), 'training/policy_loss': Array(0.00092921, dtype=float32), 'training/total_loss': Array(0.02999421, dtype=float32), 'training/v_loss': Array(0.0330723, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27669019, 0.10833827], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.292694,  9.378304], dtype=float32), 'eval/episode_reward': Array([-0.57354605,  2.4236352 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27094364, 0.1124148 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.455552816390991, 'eval/sps': 28728.1972125022}
I0728 04:31:25.609055 140337775261504 train.py:379] starting iteration 89, 36454400 steps, 2627.040379524231
I0728 04:31:54.475692 140337775261504 train.py:394] {'eval/walltime': 418.22162914276123, 'training/sps': 16782.86972924004, 'training/walltime': 2227.6255147457123, 'training/entropy_loss': Array(-0.00408716, dtype=float32), 'training/policy_loss': Array(0.00063508, dtype=float32), 'training/total_loss': Array(0.03199781, dtype=float32), 'training/v_loss': Array(0.03544989, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27122018, 0.10040212], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.898281,  8.640728], dtype=float32), 'eval/episode_reward': Array([-0.0391231,  2.2391071], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26600218, 0.10355844], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4529407024383545, 'eval/sps': 28745.049295156656}
I0728 04:31:54.479207 140337775261504 train.py:379] starting iteration 90, 36864000 steps, 2655.9105308055878
I0728 04:32:23.350419 140337775261504 train.py:394] {'eval/walltime': 422.66234493255615, 'training/sps': 16771.144649100173, 'training/walltime': 2252.0484154224396, 'training/entropy_loss': Array(-0.00545911, dtype=float32), 'training/policy_loss': Array(0.00078459, dtype=float32), 'training/total_loss': Array(0.9931058, dtype=float32), 'training/v_loss': Array(0.9977803, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27145296, 0.10526577], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.88301 ,  9.073813], dtype=float32), 'eval/episode_reward': Array([-0.2914393,  3.7004786], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26633352, 0.10836441], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.440715789794922, 'eval/sps': 28824.181969526857}
I0728 04:32:23.353746 140337775261504 train.py:379] starting iteration 91, 37273600 steps, 2684.785069704056
I0728 04:32:52.208994 140337775261504 train.py:394] {'eval/walltime': 427.1249589920044, 'training/sps': 16797.160167177015, 'training/walltime': 2276.4334897994995, 'training/entropy_loss': Array(-0.00843225, dtype=float32), 'training/policy_loss': Array(0.00067953, dtype=float32), 'training/total_loss': Array(0.0485447, dtype=float32), 'training/v_loss': Array(0.05629742, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26803863, 0.10549908], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.604519,  9.049679], dtype=float32), 'eval/episode_reward': Array([-0.32540977,  2.7131033 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26231945, 0.10938145], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.462614059448242, 'eval/sps': 28682.74027170208}
I0728 04:32:52.212341 140337775261504 train.py:379] starting iteration 92, 37683200 steps, 2713.643665075302
I0728 04:33:21.018375 140337775261504 train.py:394] {'eval/walltime': 431.59309124946594, 'training/sps': 16834.927248552427, 'training/walltime': 2300.763859272003, 'training/entropy_loss': Array(-0.01045571, dtype=float32), 'training/policy_loss': Array(0.00055344, dtype=float32), 'training/total_loss': Array(0.05401931, dtype=float32), 'training/v_loss': Array(0.06392159, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28523415, 0.10185612], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.09399 ,  8.831027], dtype=float32), 'eval/episode_reward': Array([-0.20206508,  2.3872898 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28079784, 0.10428354], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.468132257461548, 'eval/sps': 28647.3167364835}
I0728 04:33:21.021843 140337775261504 train.py:379] starting iteration 93, 38092800 steps, 2742.4531672000885
I0728 04:33:49.860961 140337775261504 train.py:394] {'eval/walltime': 436.0721983909607, 'training/sps': 16819.574883867932, 'training/walltime': 2325.1164367198944, 'training/entropy_loss': Array(-0.01370417, dtype=float32), 'training/policy_loss': Array(0.00058923, dtype=float32), 'training/total_loss': Array(0.08439751, dtype=float32), 'training/v_loss': Array(0.09751245, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27007043, 0.10423779], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.791138,  8.977867], dtype=float32), 'eval/episode_reward': Array([-0.20554633,  3.6012533 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2649113, 0.1074823], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.479107141494751, 'eval/sps': 28577.123957183197}
I0728 04:33:49.864329 140337775261504 train.py:379] starting iteration 94, 38502400 steps, 2771.2956533432007
I0728 04:34:18.698517 140337775261504 train.py:394] {'eval/walltime': 440.52980613708496, 'training/sps': 16808.215996808176, 'training/walltime': 2349.4854714870453, 'training/entropy_loss': Array(-0.01521698, dtype=float32), 'training/policy_loss': Array(0.00051342, dtype=float32), 'training/total_loss': Array(0.12194766, dtype=float32), 'training/v_loss': Array(0.13665122, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27738422, 0.1057914 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.41351 ,  9.099218], dtype=float32), 'eval/episode_reward': Array([-0.2377358,  3.7244053], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2725474 , 0.10912924], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.457607746124268, 'eval/sps': 28714.953690416452}
I0728 04:34:18.701894 140337775261504 train.py:379] starting iteration 95, 38912000 steps, 2800.133217573166
I0728 04:34:47.542613 140337775261504 train.py:394] {'eval/walltime': 444.9712657928467, 'training/sps': 16792.880773089124, 'training/walltime': 2373.876760005951, 'training/entropy_loss': Array(-0.01631122, dtype=float32), 'training/policy_loss': Array(0.00066874, dtype=float32), 'training/total_loss': Array(1.2832453, dtype=float32), 'training/v_loss': Array(1.2988878, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2915504 , 0.09338418], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.600145,  8.050266], dtype=float32), 'eval/episode_reward': Array([-0.30072773,  3.5336337 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28702158, 0.09602106], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.441459655761719, 'eval/sps': 28819.35442866198}
I0728 04:34:47.545979 140337775261504 train.py:379] starting iteration 96, 39321600 steps, 2828.977302789688
I0728 04:35:16.435385 140337775261504 train.py:394] {'eval/walltime': 449.4369785785675, 'training/sps': 16775.849194239967, 'training/walltime': 2398.2928116321564, 'training/entropy_loss': Array(-0.01880262, dtype=float32), 'training/policy_loss': Array(0.0004223, dtype=float32), 'training/total_loss': Array(0.23134157, dtype=float32), 'training/v_loss': Array(0.24972188, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27377045, 0.10775622], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.13516 ,  9.220179], dtype=float32), 'eval/episode_reward': Array([-0.8094485,  5.474933 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26886037, 0.11050085], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.465712785720825, 'eval/sps': 28662.83752266417}
I0728 04:35:16.438775 140337775261504 train.py:379] starting iteration 97, 39731200 steps, 2857.870099067688
I0728 04:35:45.350563 140337775261504 train.py:394] {'eval/walltime': 453.91021728515625, 'training/sps': 16765.9240519132, 'training/walltime': 2422.7233171463013, 'training/entropy_loss': Array(-0.02041901, dtype=float32), 'training/policy_loss': Array(0.00036367, dtype=float32), 'training/total_loss': Array(0.32926035, dtype=float32), 'training/v_loss': Array(0.3493157, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2857967 , 0.10102061], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.107662,  8.628409], dtype=float32), 'eval/episode_reward': Array([-0.94959307,  5.0512795 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28110176, 0.10318913], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.473238706588745, 'eval/sps': 28614.614241682564}
I0728 04:35:45.353968 140337775261504 train.py:379] starting iteration 98, 40140800 steps, 2886.7852914333344
I0728 04:36:14.180275 140337775261504 train.py:394] {'eval/walltime': 458.3604483604431, 'training/sps': 16808.800951686724, 'training/walltime': 2447.0915038585663, 'training/entropy_loss': Array(-0.02190514, dtype=float32), 'training/policy_loss': Array(0.00045906, dtype=float32), 'training/total_loss': Array(0.41236848, dtype=float32), 'training/v_loss': Array(0.4338146, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29305625, 0.10785125], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.7602 ,  9.26034], dtype=float32), 'eval/episode_reward': Array([-1.2873671,  6.0133   ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28851184, 0.11000904], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.450231075286865, 'eval/sps': 28762.551389929573}
I0728 04:36:14.183685 140337775261504 train.py:379] starting iteration 99, 40550400 steps, 2915.6150093078613
I0728 04:36:43.042810 140337775261504 train.py:394] {'eval/walltime': 462.8130555152893, 'training/sps': 16787.385811439715, 'training/walltime': 2471.4907763004303, 'training/entropy_loss': Array(-0.02353174, dtype=float32), 'training/policy_loss': Array(0.00041379, dtype=float32), 'training/total_loss': Array(0.5567344, dtype=float32), 'training/v_loss': Array(0.57985234, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29298604, 0.11671845], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.798363,  9.983053], dtype=float32), 'eval/episode_reward': Array([-1.0097607,  5.934161 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2886163 , 0.11915445], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.452607154846191, 'eval/sps': 28747.202604812228}
I0728 04:36:43.046108 140337775261504 train.py:379] starting iteration 100, 40960000 steps, 2944.477432489395
I0728 04:37:11.932323 140337775261504 train.py:394] {'eval/walltime': 467.2754466533661, 'training/sps': 16775.76466700015, 'training/walltime': 2495.9069509506226, 'training/entropy_loss': Array(-0.02520712, dtype=float32), 'training/policy_loss': Array(0.00066121, dtype=float32), 'training/total_loss': Array(1.8469238, dtype=float32), 'training/v_loss': Array(1.8714697, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30581778, 0.11527945], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.85688 ,  9.902206], dtype=float32), 'eval/episode_reward': Array([-1.8644786,  7.3718624], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3012525 , 0.11849156], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.462391138076782, 'eval/sps': 28684.173134846693}
I0728 04:37:12.007658 140337775261504 train.py:379] starting iteration 101, 41369600 steps, 2973.4389724731445
I0728 04:37:40.924540 140337775261504 train.py:394] {'eval/walltime': 471.75324034690857, 'training/sps': 16765.917670754025, 'training/walltime': 2520.337465763092, 'training/entropy_loss': Array(-0.0309511, dtype=float32), 'training/policy_loss': Array(0.00040869, dtype=float32), 'training/total_loss': Array(1.1024911, dtype=float32), 'training/v_loss': Array(1.1330335, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29771537, 0.1270491 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.163078, 10.901838], dtype=float32), 'eval/episode_reward': Array([-2.7548397,  7.781835 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29313973, 0.12991679], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4777936935424805, 'eval/sps': 28585.50633643338}
I0728 04:37:40.932220 140337775261504 train.py:379] starting iteration 102, 41779200 steps, 3002.363528728485
I0728 04:38:09.776223 140337775261504 train.py:394] {'eval/walltime': 476.1949508190155, 'training/sps': 16790.969345252277, 'training/walltime': 2544.73153090477, 'training/entropy_loss': Array(-0.03385294, dtype=float32), 'training/policy_loss': Array(0.00027166, dtype=float32), 'training/total_loss': Array(1.3820074, dtype=float32), 'training/v_loss': Array(1.4155887, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30083382, 0.11884192], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.447346, 10.218867], dtype=float32), 'eval/episode_reward': Array([-2.2972066,  7.3514466], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29637498, 0.12224639], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.441710472106934, 'eval/sps': 28817.72704542873}
I0728 04:38:09.779584 140337775261504 train.py:379] starting iteration 103, 42188800 steps, 3031.2109081745148
I0728 04:38:38.665215 140337775261504 train.py:394] {'eval/walltime': 480.6563000679016, 'training/sps': 16776.374396139912, 'training/walltime': 2569.1468181610107, 'training/entropy_loss': Array(-0.03651042, dtype=float32), 'training/policy_loss': Array(-2.4740315e-05, dtype=float32), 'training/total_loss': Array(1.5815624, dtype=float32), 'training/v_loss': Array(1.6180975, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29103243, 0.1251412 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.611925, 10.747627], dtype=float32), 'eval/episode_reward': Array([-1.367874,  7.582486], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28617245, 0.12879992], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.461349248886108, 'eval/sps': 28690.871944615974}
I0728 04:38:38.670335 140337775261504 train.py:379] starting iteration 104, 42598400 steps, 3060.101651906967
I0728 04:39:07.538998 140337775261504 train.py:394] {'eval/walltime': 485.09683322906494, 'training/sps': 16773.20762925883, 'training/walltime': 2593.56671500206, 'training/entropy_loss': Array(-0.0377433, dtype=float32), 'training/policy_loss': Array(0.00049741, dtype=float32), 'training/total_loss': Array(1.6464286, dtype=float32), 'training/v_loss': Array(1.6836746, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2939791 , 0.10953627], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.858063 ,  9.4096575], dtype=float32), 'eval/episode_reward': Array([-0.7661094,  6.0665383], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28944093, 0.1125416 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.44053316116333, 'eval/sps': 28825.367439991503}
I0728 04:39:07.545195 140337775261504 train.py:379] starting iteration 105, 43008000 steps, 3088.976502895355
I0728 04:39:36.446568 140337775261504 train.py:394] {'eval/walltime': 489.5570056438446, 'training/sps': 16764.139152899257, 'training/walltime': 2617.999821662903, 'training/entropy_loss': Array(-0.03903791, dtype=float32), 'training/policy_loss': Array(0.00034974, dtype=float32), 'training/total_loss': Array(3.1706905, dtype=float32), 'training/v_loss': Array(3.2093787, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29075965, 0.11370806], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.559467,  9.81329 ], dtype=float32), 'eval/episode_reward': Array([-0.86933684,  6.0061226 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2861778 , 0.11747037], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.460172414779663, 'eval/sps': 28698.44214448901}
I0728 04:39:36.449981 140337775261504 train.py:379] starting iteration 106, 43417600 steps, 3117.8813054561615
I0728 04:40:05.348161 140337775261504 train.py:394] {'eval/walltime': 494.00329542160034, 'training/sps': 16756.63365015739, 'training/walltime': 2642.4438722133636, 'training/entropy_loss': Array(-0.0412169, dtype=float32), 'training/policy_loss': Array(0.00028717, dtype=float32), 'training/total_loss': Array(2.1894817, dtype=float32), 'training/v_loss': Array(2.2304115, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2815792 , 0.11121918], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.809643,  9.581909], dtype=float32), 'eval/episode_reward': Array([-1.0634801,  7.067743 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27668685, 0.11435723], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.446289777755737, 'eval/sps': 28788.0472029441}
I0728 04:40:05.351536 140337775261504 train.py:379] starting iteration 107, 43827200 steps, 3146.7828602790833
I0728 04:40:34.194544 140337775261504 train.py:394] {'eval/walltime': 498.45581674575806, 'training/sps': 16798.782092713947, 'training/walltime': 2666.826592206955, 'training/entropy_loss': Array(-0.04304616, dtype=float32), 'training/policy_loss': Array(0.00015735, dtype=float32), 'training/total_loss': Array(2.6323967, dtype=float32), 'training/v_loss': Array(2.6752858, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2935155 , 0.12610507], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.843971, 10.792269], dtype=float32), 'eval/episode_reward': Array([-2.224766 ,  7.6200347], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28934544, 0.1284625 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.452521324157715, 'eval/sps': 28747.756760988406}
I0728 04:40:34.197921 140337775261504 train.py:379] starting iteration 108, 44236800 steps, 3175.629245519638
I0728 04:41:03.081855 140337775261504 train.py:394] {'eval/walltime': 502.90763783454895, 'training/sps': 16770.107046809688, 'training/walltime': 2691.2510039806366, 'training/entropy_loss': Array(-0.04387324, dtype=float32), 'training/policy_loss': Array(0.00013479, dtype=float32), 'training/total_loss': Array(2.7580354, dtype=float32), 'training/v_loss': Array(2.8017738, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29777867, 0.12629756], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.176285 , 10.8353615], dtype=float32), 'eval/episode_reward': Array([-2.0267358,  7.8013062], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29340732, 0.12968978], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4518210887908936, 'eval/sps': 28752.278550071867}
I0728 04:41:03.085210 140337775261504 train.py:379] starting iteration 109, 44646400 steps, 3204.5165343284607
I0728 04:41:31.939283 140337775261504 train.py:394] {'eval/walltime': 507.3512420654297, 'training/sps': 16785.25817125389, 'training/walltime': 2715.6533691883087, 'training/entropy_loss': Array(-0.04556781, dtype=float32), 'training/policy_loss': Array(3.5912133e-05, dtype=float32), 'training/total_loss': Array(3.0382802, dtype=float32), 'training/v_loss': Array(3.0838122, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2983786 , 0.13013808], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.230436, 11.186417], dtype=float32), 'eval/episode_reward': Array([-1.4755588,  7.16381  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29396987, 0.13290548], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.443604230880737, 'eval/sps': 28805.44561337542}
I0728 04:41:31.942700 140337775261504 train.py:379] starting iteration 110, 45056000 steps, 3233.374023914337
I0728 04:42:00.865040 140337775261504 train.py:394] {'eval/walltime': 511.80716705322266, 'training/sps': 16746.674579319453, 'training/walltime': 2740.111956357956, 'training/entropy_loss': Array(-0.04595248, dtype=float32), 'training/policy_loss': Array(0.0002394, dtype=float32), 'training/total_loss': Array(4.4333825, dtype=float32), 'training/v_loss': Array(4.4790955, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2962289 , 0.11709715], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.094776, 10.047285], dtype=float32), 'eval/episode_reward': Array([-2.1195006,  7.2637672], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2922145 , 0.11965459], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.455924987792969, 'eval/sps': 28725.79775257813}
I0728 04:42:00.868424 140337775261504 train.py:379] starting iteration 111, 45465600 steps, 3262.2997481822968
I0728 04:42:29.975640 140337775261504 train.py:394] {'eval/walltime': 516.2679870128632, 'training/sps': 16785.21766406515, 'training/walltime': 2764.514380455017, 'training/entropy_loss': Array(-0.04685439, dtype=float32), 'training/policy_loss': Array(7.888368e-05, dtype=float32), 'training/total_loss': Array(3.079165, dtype=float32), 'training/v_loss': Array(3.1259406, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30317163, 0.12599684], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.655197, 10.788852], dtype=float32), 'eval/episode_reward': Array([-2.6793373,  8.204118 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29924628, 0.12832566], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.460819959640503, 'eval/sps': 28694.276199910903}
I0728 04:42:29.979182 140337775261504 train.py:379] starting iteration 112, 45875200 steps, 3291.410505771637
I0728 04:42:58.846713 140337775261504 train.py:394] {'eval/walltime': 520.7202594280243, 'training/sps': 16781.914936182133, 'training/walltime': 2788.921607017517, 'training/entropy_loss': Array(-0.04661895, dtype=float32), 'training/policy_loss': Array(0.00015052, dtype=float32), 'training/total_loss': Array(2.9341717, dtype=float32), 'training/v_loss': Array(2.98064, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2842126 , 0.12246309], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.002699, 10.526924], dtype=float32), 'eval/episode_reward': Array([-0.6384573,  7.3177104], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27995884, 0.12492523], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.452272415161133, 'eval/sps': 28749.363934724002}
I0728 04:42:58.850089 140337775261504 train.py:379] starting iteration 113, 46284800 steps, 3320.281412601471
I0728 04:43:27.746358 140337775261504 train.py:394] {'eval/walltime': 525.1723892688751, 'training/sps': 16761.873156905567, 'training/walltime': 2813.358016729355, 'training/entropy_loss': Array(-0.04701032, dtype=float32), 'training/policy_loss': Array(-1.10533365e-05, dtype=float32), 'training/total_loss': Array(2.927096, dtype=float32), 'training/v_loss': Array(2.974117, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30037838, 0.11955589], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.443338, 10.244429], dtype=float32), 'eval/episode_reward': Array([-2.1149206,  7.7511535], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29627776, 0.12189007], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.45212984085083, 'eval/sps': 28750.284599862072}
I0728 04:43:27.749686 140337775261504 train.py:379] starting iteration 114, 46694400 steps, 3349.1810104846954
I0728 04:43:56.602920 140337775261504 train.py:394] {'eval/walltime': 529.6334495544434, 'training/sps': 16797.746980630876, 'training/walltime': 2837.7422392368317, 'training/entropy_loss': Array(-0.04676455, dtype=float32), 'training/policy_loss': Array(2.7828606e-05, dtype=float32), 'training/total_loss': Array(2.6971807, dtype=float32), 'training/v_loss': Array(2.7439175, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30368233, 0.1292541 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.702232, 11.028893], dtype=float32), 'eval/episode_reward': Array([-2.0674338,  7.6364355], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29970735, 0.13185312], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.461060285568237, 'eval/sps': 28692.730383870105}
I0728 04:43:56.606375 140337775261504 train.py:379] starting iteration 115, 47104000 steps, 3378.0376992225647
I0728 04:44:25.513443 140337775261504 train.py:394] {'eval/walltime': 534.1011984348297, 'training/sps': 16765.093560327274, 'training/walltime': 2862.173954963684, 'training/entropy_loss': Array(-0.04805523, dtype=float32), 'training/policy_loss': Array(6.547407e-05, dtype=float32), 'training/total_loss': Array(4.237708, dtype=float32), 'training/v_loss': Array(4.285698, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29625788, 0.10961535], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.02782 ,  9.429365], dtype=float32), 'eval/episode_reward': Array([-1.2602102,  7.4289846], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29235855, 0.11191818], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4677488803863525, 'eval/sps': 28649.774959807295}
I0728 04:44:25.516832 140337775261504 train.py:379] starting iteration 116, 47513600 steps, 3406.9481563568115
I0728 04:44:54.358144 140337775261504 train.py:394] {'eval/walltime': 538.5574984550476, 'training/sps': 16802.990704735803, 'training/walltime': 2886.5505678653717, 'training/entropy_loss': Array(-0.04950462, dtype=float32), 'training/policy_loss': Array(-0.00010428, dtype=float32), 'training/total_loss': Array(3.2892408, dtype=float32), 'training/v_loss': Array(3.3388495, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29741603, 0.1266674 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.14264 , 10.892788], dtype=float32), 'eval/episode_reward': Array([-2.822266 ,  7.6735187], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29291928, 0.13015436], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4563000202178955, 'eval/sps': 28723.380252513005}
I0728 04:44:54.361657 140337775261504 train.py:379] starting iteration 117, 47923200 steps, 3435.7929816246033
I0728 04:45:23.268214 140337775261504 train.py:394] {'eval/walltime': 543.0283417701721, 'training/sps': 16767.720459553428, 'training/walltime': 2910.978456020355, 'training/entropy_loss': Array(-0.05024534, dtype=float32), 'training/policy_loss': Array(-7.215779e-05, dtype=float32), 'training/total_loss': Array(3.3726075, dtype=float32), 'training/v_loss': Array(3.422925, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3021885 , 0.12045814], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.584116 , 10.2805395], dtype=float32), 'eval/episode_reward': Array([-0.9866048,  7.937731 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2984331 , 0.12262249], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.470843315124512, 'eval/sps': 28629.945399112075}
I0728 04:45:23.271611 140337775261504 train.py:379] starting iteration 118, 48332800 steps, 3464.7029349803925
I0728 04:45:52.165040 140337775261504 train.py:394] {'eval/walltime': 547.4999101161957, 'training/sps': 16777.076409481437, 'training/walltime': 2935.3927216529846, 'training/entropy_loss': Array(-0.05030444, dtype=float32), 'training/policy_loss': Array(8.152274e-05, dtype=float32), 'training/total_loss': Array(3.3488903, dtype=float32), 'training/v_loss': Array(3.3991132, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29313105, 0.12070472], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.805157, 10.39183 ], dtype=float32), 'eval/episode_reward': Array([-2.1947615,  7.66986  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2888021 , 0.12323609], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.47156834602356, 'eval/sps': 28625.303270568773}
I0728 04:45:52.168543 140337775261504 train.py:379] starting iteration 119, 48742400 steps, 3493.5998668670654
I0728 04:46:21.078590 140337775261504 train.py:394] {'eval/walltime': 551.9804780483246, 'training/sps': 16772.40048657905, 'training/walltime': 2959.81379365921, 'training/entropy_loss': Array(-0.05098537, dtype=float32), 'training/policy_loss': Array(-3.1585718e-05, dtype=float32), 'training/total_loss': Array(3.4370046, dtype=float32), 'training/v_loss': Array(3.4880214, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30810353, 0.12529922], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.083706, 10.775773], dtype=float32), 'eval/episode_reward': Array([-2.6899216,  7.88778  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3038426, 0.1280702], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.480567932128906, 'eval/sps': 28567.80701440717}
I0728 04:46:21.081972 140337775261504 train.py:379] starting iteration 120, 49152000 steps, 3522.5132966041565
I0728 04:46:49.962941 140337775261504 train.py:394] {'eval/walltime': 556.4330554008484, 'training/sps': 16772.61057527788, 'training/walltime': 2984.234559774399, 'training/entropy_loss': Array(-0.05130866, dtype=float32), 'training/policy_loss': Array(-0.00016218, dtype=float32), 'training/total_loss': Array(5.0113716, dtype=float32), 'training/v_loss': Array(5.0628424, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33087134, 0.13444245], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.024466, 11.506623], dtype=float32), 'eval/episode_reward': Array([-3.5858657,  9.315275 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32719347, 0.13717847], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.452577352523804, 'eval/sps': 28747.39501772995}
I0728 04:46:49.966311 140337775261504 train.py:379] starting iteration 121, 49561600 steps, 3551.3976352214813
I0728 04:47:18.841436 140337775261504 train.py:394] {'eval/walltime': 560.8970911502838, 'training/sps': 16784.615491182798, 'training/walltime': 3008.6378593444824, 'training/entropy_loss': Array(-0.05115467, dtype=float32), 'training/policy_loss': Array(-0.00016068, dtype=float32), 'training/total_loss': Array(3.8472738, dtype=float32), 'training/v_loss': Array(3.8985891, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31597298, 0.13344283], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.73289 , 11.466959], dtype=float32), 'eval/episode_reward': Array([-3.8825374,  8.5425005], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31197765, 0.13589202], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.464035749435425, 'eval/sps': 28673.605496145145}
I0728 04:47:18.844847 140337775261504 train.py:379] starting iteration 122, 49971200 steps, 3580.276171684265
I0728 04:47:47.738632 140337775261504 train.py:394] {'eval/walltime': 565.3807156085968, 'training/sps': 16785.256695280015, 'training/walltime': 3033.0402266979218, 'training/entropy_loss': Array(-0.05157194, dtype=float32), 'training/policy_loss': Array(-0.0002511, dtype=float32), 'training/total_loss': Array(3.6771586, dtype=float32), 'training/v_loss': Array(3.7289815, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30696547, 0.12868752], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.018421, 11.020042], dtype=float32), 'eval/episode_reward': Array([-2.4952083,  8.520386 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30320898, 0.13088812], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.483624458312988, 'eval/sps': 28548.332089383188}
I0728 04:47:47.742076 140337775261504 train.py:379] starting iteration 123, 50380800 steps, 3609.1734001636505
I0728 04:48:16.624286 140337775261504 train.py:394] {'eval/walltime': 569.8312604427338, 'training/sps': 16770.576718656273, 'training/walltime': 3057.4639544487, 'training/entropy_loss': Array(-0.05250768, dtype=float32), 'training/policy_loss': Array(-0.00036323, dtype=float32), 'training/total_loss': Array(3.645957, dtype=float32), 'training/v_loss': Array(3.698828, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30349073, 0.11831275], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.654112, 10.158666], dtype=float32), 'eval/episode_reward': Array([-3.399787,  7.999829], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29916766, 0.12114978], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.450544834136963, 'eval/sps': 28760.523659531093}
I0728 04:48:16.627670 140337775261504 train.py:379] starting iteration 124, 50790400 steps, 3638.058993577957
I0728 04:48:45.534861 140337775261504 train.py:394] {'eval/walltime': 574.2923250198364, 'training/sps': 16760.49233491875, 'training/walltime': 3081.9023773670197, 'training/entropy_loss': Array(-0.05227315, dtype=float32), 'training/policy_loss': Array(-0.00013534, dtype=float32), 'training/total_loss': Array(3.4144452, dtype=float32), 'training/v_loss': Array(3.4668536, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30287495, 0.1263359 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.63673 , 10.888096], dtype=float32), 'eval/episode_reward': Array([-2.7871547,  8.856741 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29816872, 0.12992446], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.461064577102661, 'eval/sps': 28692.70278152586}
I0728 04:48:45.538244 140337775261504 train.py:379] starting iteration 125, 51200000 steps, 3666.9695687294006
I0728 04:49:14.174078 140337775261504 train.py:394] {'eval/walltime': 578.7274708747864, 'training/sps': 16930.742618280445, 'training/walltime': 3106.095055103302, 'training/entropy_loss': Array(-0.05240904, dtype=float32), 'training/policy_loss': Array(-0.00025932, dtype=float32), 'training/total_loss': Array(5.037326, dtype=float32), 'training/v_loss': Array(5.0899944, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30477086, 0.1253953 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.729664, 10.748769], dtype=float32), 'eval/episode_reward': Array([-2.7434506,  8.6564865], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3004471 , 0.12783585], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.435145854949951, 'eval/sps': 28860.38118839824}
I0728 04:49:14.255459 140337775261504 train.py:379] starting iteration 126, 51609600 steps, 3695.686779499054
I0728 04:49:43.036783 140337775261504 train.py:394] {'eval/walltime': 583.1785094738007, 'training/sps': 16840.459655514307, 'training/walltime': 3130.4174315929413, 'training/entropy_loss': Array(-0.0529584, dtype=float32), 'training/policy_loss': Array(-0.00018263, dtype=float32), 'training/total_loss': Array(3.7846408, dtype=float32), 'training/v_loss': Array(3.8377824, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2932683 , 0.12870772], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.77798 , 11.038428], dtype=float32), 'eval/episode_reward': Array([-2.6181633,  8.19689  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28888094, 0.13144724], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.451038599014282, 'eval/sps': 28757.333182495116}
I0728 04:49:43.040871 140337775261504 train.py:379] starting iteration 127, 52019200 steps, 3724.4721953868866
I0728 04:50:11.888365 140337775261504 train.py:394] {'eval/walltime': 587.6252491474152, 'training/sps': 16791.56721464347, 'training/walltime': 3154.8106281757355, 'training/entropy_loss': Array(-0.05344429, dtype=float32), 'training/policy_loss': Array(-0.00021704, dtype=float32), 'training/total_loss': Array(3.9129524, dtype=float32), 'training/v_loss': Array(3.9666138, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29845506, 0.12645055], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.2052  , 10.844965], dtype=float32), 'eval/episode_reward': Array([-2.0533924,  8.25992  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29356754, 0.12993953], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.446739673614502, 'eval/sps': 28785.13459186966}
I0728 04:50:11.891798 140337775261504 train.py:379] starting iteration 128, 52428800 steps, 3753.3231225013733
I0728 04:50:40.765758 140337775261504 train.py:394] {'eval/walltime': 592.1095430850983, 'training/sps': 16799.34684267953, 'training/walltime': 3179.192528486252, 'training/entropy_loss': Array(-0.05355221, dtype=float32), 'training/policy_loss': Array(-0.00041996, dtype=float32), 'training/total_loss': Array(3.9377553, dtype=float32), 'training/v_loss': Array(3.9917274, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29786202, 0.13757652], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.119053, 11.833582], dtype=float32), 'eval/episode_reward': Array([-2.484507,  8.984886], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29288095, 0.14091448], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.4842939376831055, 'eval/sps': 28544.069987110077}
I0728 04:50:40.769210 140337775261504 train.py:379] starting iteration 129, 52838400 steps, 3782.2005343437195
I0728 04:51:09.643826 140337775261504 train.py:394] {'eval/walltime': 596.5714190006256, 'training/sps': 16783.545232252818, 'training/walltime': 3203.5973842144012, 'training/entropy_loss': Array(-0.05333614, dtype=float32), 'training/policy_loss': Array(-0.00016468, dtype=float32), 'training/total_loss': Array(3.7909245, dtype=float32), 'training/v_loss': Array(3.8444257, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30787075, 0.11889466], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.055195, 10.195642], dtype=float32), 'eval/episode_reward': Array([-2.6332364,  8.306188 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3038762 , 0.12120583], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.461875915527344, 'eval/sps': 28687.485358918108}
I0728 04:51:09.647227 140337775261504 train.py:379] starting iteration 130, 53248000 steps, 3811.0785505771637
I0728 04:51:38.522531 140337775261504 train.py:394] {'eval/walltime': 601.047040939331, 'training/sps': 16792.53787889747, 'training/walltime': 3227.9891707897186, 'training/entropy_loss': Array(-0.05343717, dtype=float32), 'training/policy_loss': Array(-0.00031689, dtype=float32), 'training/total_loss': Array(5.6238947, dtype=float32), 'training/v_loss': Array(5.6776485, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28330302, 0.12210035], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.98214 , 10.437869], dtype=float32), 'eval/episode_reward': Array([-1.4865425,  7.2570577], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27838713, 0.12548548], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.475621938705444, 'eval/sps': 28599.37719337918}
I0728 04:51:38.525988 140337775261504 train.py:379] starting iteration 131, 53657600 steps, 3839.9573118686676
I0728 04:52:07.413188 140337775261504 train.py:394] {'eval/walltime': 605.5273225307465, 'training/sps': 16787.408284770238, 'training/walltime': 3252.3884105682373, 'training/entropy_loss': Array(-0.05353635, dtype=float32), 'training/policy_loss': Array(-0.00022809, dtype=float32), 'training/total_loss': Array(4.022317, dtype=float32), 'training/v_loss': Array(4.0760813, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29240775, 0.1199311 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.753183, 10.261885], dtype=float32), 'eval/episode_reward': Array([-1.9155554,  7.396033 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2880975 , 0.12218556], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.480281591415405, 'eval/sps': 28569.632820682236}
I0728 04:52:07.416568 140337775261504 train.py:379] starting iteration 132, 54067200 steps, 3868.8478920459747
I0728 04:52:36.301684 140337775261504 train.py:394] {'eval/walltime': 609.9745302200317, 'training/sps': 16766.13037867755, 'training/walltime': 3276.818615436554, 'training/entropy_loss': Array(-0.05361428, dtype=float32), 'training/policy_loss': Array(-0.00017995, dtype=float32), 'training/total_loss': Array(3.9367795, dtype=float32), 'training/v_loss': Array(3.9905744, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30712652, 0.12377233], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.924969, 10.652296], dtype=float32), 'eval/episode_reward': Array([-1.8381176,  8.376952 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30245417, 0.12708458], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.447207689285278, 'eval/sps': 28782.10529910538}
I0728 04:52:36.305137 140337775261504 train.py:379] starting iteration 133, 54476800 steps, 3897.7364616394043
I0728 04:53:05.178341 140337775261504 train.py:394] {'eval/walltime': 614.4577977657318, 'training/sps': 16799.157931396567, 'training/walltime': 3301.2007899284363, 'training/entropy_loss': Array(-0.05371464, dtype=float32), 'training/policy_loss': Array(-0.00029857, dtype=float32), 'training/total_loss': Array(3.6899903, dtype=float32), 'training/v_loss': Array(3.7440035, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31104815, 0.1253958 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.318314, 10.751817], dtype=float32), 'eval/episode_reward': Array([-3.4500883,  7.915693 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30699605, 0.12757039], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.483267545700073, 'eval/sps': 28550.60482008608}
I0728 04:53:05.181806 140337775261504 train.py:379] starting iteration 134, 54886400 steps, 3926.6131308078766
I0728 04:53:34.075847 140337775261504 train.py:394] {'eval/walltime': 618.9061861038208, 'training/sps': 16761.062525990576, 'training/walltime': 3325.6383814811707, 'training/entropy_loss': Array(-0.05374281, dtype=float32), 'training/policy_loss': Array(-0.000255, dtype=float32), 'training/total_loss': Array(3.4991562, dtype=float32), 'training/v_loss': Array(3.5531535, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2842722 , 0.12097226], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.987688, 10.377491], dtype=float32), 'eval/episode_reward': Array([-1.6835799,  8.006604 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27953547, 0.12353043], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.448388338088989, 'eval/sps': 28774.466227242272}
I0728 04:53:34.079356 140337775261504 train.py:379] starting iteration 135, 55296000 steps, 3955.5106797218323
I0728 04:54:02.978261 140337775261504 train.py:394] {'eval/walltime': 623.3737514019012, 'training/sps': 16770.537755682974, 'training/walltime': 3350.0621659755707, 'training/entropy_loss': Array(-0.0538496, dtype=float32), 'training/policy_loss': Array(-0.00025874, dtype=float32), 'training/total_loss': Array(5.6650043, dtype=float32), 'training/v_loss': Array(5.719113, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29681617, 0.12712193], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.081013, 10.872469], dtype=float32), 'eval/episode_reward': Array([-2.8102376,  7.8812876], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29209948, 0.12983608], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.467565298080444, 'eval/sps': 28650.952243495376}
I0728 04:54:02.981670 140337775261504 train.py:379] starting iteration 136, 55705600 steps, 3984.412994146347
I0728 04:54:31.856667 140337775261504 train.py:394] {'eval/walltime': 627.8211812973022, 'training/sps': 16773.045179205095, 'training/walltime': 3374.4822993278503, 'training/entropy_loss': Array(-0.05373023, dtype=float32), 'training/policy_loss': Array(-0.00035354, dtype=float32), 'training/total_loss': Array(4.1247835, dtype=float32), 'training/v_loss': Array(4.178867, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30118883, 0.11793619], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.428339, 10.146052], dtype=float32), 'eval/episode_reward': Array([-1.8341593,  7.871756 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29672682, 0.12056163], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.447429895401001, 'eval/sps': 28780.667264111857}
I0728 04:54:31.860061 140337775261504 train.py:379] starting iteration 137, 56115200 steps, 4013.2913851737976
I0728 04:55:00.702678 140337775261504 train.py:394] {'eval/walltime': 632.2777962684631, 'training/sps': 16801.99106088149, 'training/walltime': 3398.8603625297546, 'training/entropy_loss': Array(-0.05358238, dtype=float32), 'training/policy_loss': Array(-0.00031371, dtype=float32), 'training/total_loss': Array(3.8887982, dtype=float32), 'training/v_loss': Array(3.9426947, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3055091, 0.1299579], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.804218, 11.214092], dtype=float32), 'eval/episode_reward': Array([-2.9669468,  8.345248 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30066815, 0.13365102], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.456614971160889, 'eval/sps': 28721.350358578926}
I0728 04:55:00.705978 140337775261504 train.py:379] starting iteration 138, 56524800 steps, 4042.1373026371
I0728 04:55:29.532640 140337775261504 train.py:394] {'eval/walltime': 636.7246613502502, 'training/sps': 16806.14291700594, 'training/walltime': 3423.232403278351, 'training/entropy_loss': Array(-0.05350684, dtype=float32), 'training/policy_loss': Array(-0.0004176, dtype=float32), 'training/total_loss': Array(3.7463136, dtype=float32), 'training/v_loss': Array(3.8002381, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32564616, 0.11986966], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.570116, 10.278649], dtype=float32), 'eval/episode_reward': Array([-3.2895162,  8.404064 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32172006, 0.12265545], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.446865081787109, 'eval/sps': 28784.322808498448}
I0728 04:55:29.536015 140337775261504 train.py:379] starting iteration 139, 56934400 steps, 4070.9673385620117
I0728 04:55:58.406226 140337775261504 train.py:394] {'eval/walltime': 641.2108900547028, 'training/sps': 16804.40187909857, 'training/walltime': 3447.6069691181183, 'training/entropy_loss': Array(-0.05360191, dtype=float32), 'training/policy_loss': Array(-0.00041699, dtype=float32), 'training/total_loss': Array(3.512892, dtype=float32), 'training/v_loss': Array(3.5669107, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3227944 , 0.12608221], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.307133, 10.788886], dtype=float32), 'eval/episode_reward': Array([-2.8827205,  8.600765 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3188125, 0.1282083], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.486228704452515, 'eval/sps': 28531.759843844324}
I0728 04:55:58.412248 140337775261504 train.py:379] starting iteration 140, 57344000 steps, 4099.843571424484
I0728 04:56:27.298260 140337775261504 train.py:394] {'eval/walltime': 645.6752083301544, 'training/sps': 16777.476673490113, 'training/walltime': 3472.020652294159, 'training/entropy_loss': Array(-0.05375622, dtype=float32), 'training/policy_loss': Array(-0.00036395, dtype=float32), 'training/total_loss': Array(5.6279616, dtype=float32), 'training/v_loss': Array(5.6820817, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26807716, 0.13269268], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.526802, 11.493623], dtype=float32), 'eval/episode_reward': Array([-1.2401425,  8.254432 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26179487, 0.13745692], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.46431827545166, 'eval/sps': 28671.790876525283}
I0728 04:56:27.301649 140337775261504 train.py:379] starting iteration 141, 57753600 steps, 4128.732972860336
I0728 04:56:56.158737 140337775261504 train.py:394] {'eval/walltime': 650.116197347641, 'training/sps': 16781.3016897698, 'training/walltime': 3496.4287707805634, 'training/entropy_loss': Array(-0.05400354, dtype=float32), 'training/policy_loss': Array(-0.00024788, dtype=float32), 'training/total_loss': Array(4.14198, dtype=float32), 'training/v_loss': Array(4.196232, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29638618, 0.1319068 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.999573, 11.316853], dtype=float32), 'eval/episode_reward': Array([-1.8859158,  8.6248455], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29132995, 0.13542567], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.440989017486572, 'eval/sps': 28822.408588716356}
I0728 04:56:56.162113 140337775261504 train.py:379] starting iteration 142, 58163200 steps, 4157.593437194824
