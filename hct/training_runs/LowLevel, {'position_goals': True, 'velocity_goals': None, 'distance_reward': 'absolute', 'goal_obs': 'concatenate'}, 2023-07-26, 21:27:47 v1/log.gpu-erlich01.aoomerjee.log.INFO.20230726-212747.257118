I0726 21:27:47.992837 140267183036224 low_level_env.py:187] Initialising environment...
I0726 21:28:27.575555 140267183036224 low_level_env.py:289] Environment initialised.
I0726 21:28:27.578991 140267183036224 train.py:118] JAX is running on GPU.
I0726 21:28:27.579046 140267183036224 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0726 21:28:34.895162 140267183036224 train.py:367] Running initial eval
I0726 21:28:50.396604 140267183036224 train.py:373] {'eval/walltime': 15.373786449432373, 'eval/episode_goal_distance': (Array(0.64460295, dtype=float32), Array(0.36764514, dtype=float32)), 'eval/episode_reward': (Array(-12681.286, dtype=float32), Array(6588.938, dtype=float32)), 'eval/avg_episode_length': (Array(922.375, dtype=float32), Array(266.65134, dtype=float32)), 'eval/epoch_eval_time': 15.373786449432373, 'eval/sps': 8325.860413179213}
I0726 21:28:50.397902 140267183036224 train.py:379] starting iteration 0 22.818926572799683
I0726 21:29:18.630542 140267183036224 train.py:394] {'eval/walltime': 19.16065001487732, 'training/sps': 5027.554657807334, 'training/walltime': 24.44130563735962, 'training/entropy_loss': Array(-0.04533834, dtype=float32), 'training/policy_loss': Array(0.02193869, dtype=float32), 'training/total_loss': Array(12819.546, dtype=float32), 'training/v_loss': Array(12819.568, dtype=float32), 'eval/episode_goal_distance': (Array(0.79018843, dtype=float32), Array(0.4170466, dtype=float32)), 'eval/episode_reward': (Array(-14582.752, dtype=float32), Array(7082.1055, dtype=float32)), 'eval/avg_episode_length': (Array(945.6953, dtype=float32), Array(225.77795, dtype=float32)), 'eval/epoch_eval_time': 3.7868635654449463, 'eval/sps': 33801.05931673838}
I0726 21:29:18.656893 140267183036224 train.py:379] starting iteration 1 51.07791781425476
I0726 21:29:25.368066 140267183036224 train.py:394] {'eval/walltime': 22.95343804359436, 'training/sps': 42153.30223583504, 'training/walltime': 27.356379747390747, 'training/entropy_loss': Array(-0.04494804, dtype=float32), 'training/policy_loss': Array(0.00582565, dtype=float32), 'training/total_loss': Array(13704.625, dtype=float32), 'training/v_loss': Array(13704.664, dtype=float32), 'eval/episode_goal_distance': (Array(0.71307373, dtype=float32), Array(0.40292442, dtype=float32)), 'eval/episode_reward': (Array(-13448.461, dtype=float32), Array(6383.1294, dtype=float32)), 'eval/avg_episode_length': (Array(937.8047, dtype=float32), Array(240.88152, dtype=float32)), 'eval/epoch_eval_time': 3.792788028717041, 'eval/sps': 33748.260918050204}
I0726 21:29:25.370310 140267183036224 train.py:379] starting iteration 2 57.79133582115173
I0726 21:29:32.086899 140267183036224 train.py:394] {'eval/walltime': 26.744768619537354, 'training/sps': 42056.9030412089, 'training/walltime': 30.278135538101196, 'training/entropy_loss': Array(-0.04412136, dtype=float32), 'training/policy_loss': Array(0.00545859, dtype=float32), 'training/total_loss': Array(13080.832, dtype=float32), 'training/v_loss': Array(13080.87, dtype=float32), 'eval/episode_goal_distance': (Array(0.68714607, dtype=float32), Array(0.41221303, dtype=float32)), 'eval/episode_reward': (Array(-12878.971, dtype=float32), Array(6787.3354, dtype=float32)), 'eval/avg_episode_length': (Array(914.625, dtype=float32), Array(278.4376, dtype=float32)), 'eval/epoch_eval_time': 3.791330575942993, 'eval/sps': 33761.23433081627}
I0726 21:29:32.089128 140267183036224 train.py:379] starting iteration 3 64.51015400886536
I0726 21:29:38.832184 140267183036224 train.py:394] {'eval/walltime': 30.563893795013428, 'training/sps': 42078.1090864915, 'training/walltime': 33.198418855667114, 'training/entropy_loss': Array(-0.04252948, dtype=float32), 'training/policy_loss': Array(0.00525272, dtype=float32), 'training/total_loss': Array(12052.933, dtype=float32), 'training/v_loss': Array(12052.97, dtype=float32), 'eval/episode_goal_distance': (Array(0.76589155, dtype=float32), Array(0.43982813, dtype=float32)), 'eval/episode_reward': (Array(-14084.561, dtype=float32), Array(7100.149, dtype=float32)), 'eval/avg_episode_length': (Array(930.0703, dtype=float32), Array(254.28116, dtype=float32)), 'eval/epoch_eval_time': 3.819125175476074, 'eval/sps': 33515.528849887494}
I0726 21:29:38.834682 140267183036224 train.py:379] starting iteration 4 71.25570821762085
I0726 21:29:45.585021 140267183036224 train.py:394] {'eval/walltime': 34.380502462387085, 'training/sps': 41933.18802133646, 'training/walltime': 36.12879467010498, 'training/entropy_loss': Array(-0.04135018, dtype=float32), 'training/policy_loss': Array(0.00652094, dtype=float32), 'training/total_loss': Array(10690.555, dtype=float32), 'training/v_loss': Array(10690.59, dtype=float32), 'eval/episode_goal_distance': (Array(0.78755546, dtype=float32), Array(0.52487946, dtype=float32)), 'eval/episode_reward': (Array(-13724.469, dtype=float32), Array(7978.6035, dtype=float32)), 'eval/avg_episode_length': (Array(906.8281, dtype=float32), Array(289.68338, dtype=float32)), 'eval/epoch_eval_time': 3.8166086673736572, 'eval/sps': 33537.62755249448}
I0726 21:29:45.587328 140267183036224 train.py:379] starting iteration 5 78.00835394859314
I0726 21:29:52.344152 140267183036224 train.py:394] {'eval/walltime': 38.200188875198364, 'training/sps': 41884.42974857367, 'training/walltime': 39.06258177757263, 'training/entropy_loss': Array(-0.03925324, dtype=float32), 'training/policy_loss': Array(0.00762385, dtype=float32), 'training/total_loss': Array(9347.548, dtype=float32), 'training/v_loss': Array(9347.58, dtype=float32), 'eval/episode_goal_distance': (Array(0.7925571, dtype=float32), Array(0.48230064, dtype=float32)), 'eval/episode_reward': (Array(-14433.613, dtype=float32), Array(7804.389, dtype=float32)), 'eval/avg_episode_length': (Array(914.625, dtype=float32), Array(278.4376, dtype=float32)), 'eval/epoch_eval_time': 3.8196864128112793, 'eval/sps': 33510.6043183771}
I0726 21:29:52.346604 140267183036224 train.py:379] starting iteration 6 84.76762962341309
I0726 21:29:59.100757 140267183036224 train.py:394] {'eval/walltime': 42.01324510574341, 'training/sps': 41828.70548179342, 'training/walltime': 42.000277280807495, 'training/entropy_loss': Array(-0.03748193, dtype=float32), 'training/policy_loss': Array(0.01063188, dtype=float32), 'training/total_loss': Array(8502.887, dtype=float32), 'training/v_loss': Array(8502.914, dtype=float32), 'eval/episode_goal_distance': (Array(0.80379426, dtype=float32), Array(0.45110294, dtype=float32)), 'eval/episode_reward': (Array(-14789.529, dtype=float32), Array(6873.721, dtype=float32)), 'eval/avg_episode_length': (Array(945.6406, dtype=float32), Array(226.00534, dtype=float32)), 'eval/epoch_eval_time': 3.813056230545044, 'eval/sps': 33568.872909514765}
I0726 21:29:59.103114 140267183036224 train.py:379] starting iteration 7 91.52413940429688
I0726 21:30:05.849730 140267183036224 train.py:394] {'eval/walltime': 45.828113079071045, 'training/sps': 41961.12139141989, 'training/walltime': 44.92870235443115, 'training/entropy_loss': Array(-0.03563426, dtype=float32), 'training/policy_loss': Array(0.01586737, dtype=float32), 'training/total_loss': Array(8135.207, dtype=float32), 'training/v_loss': Array(8135.2266, dtype=float32), 'eval/episode_goal_distance': (Array(0.7758949, dtype=float32), Array(0.38782567, dtype=float32)), 'eval/episode_reward': (Array(-14122.138, dtype=float32), Array(6664.508, dtype=float32)), 'eval/avg_episode_length': (Array(906.9219, dtype=float32), Array(289.39264, dtype=float32)), 'eval/epoch_eval_time': 3.8148679733276367, 'eval/sps': 33552.930506359844}
I0726 21:30:05.852176 140267183036224 train.py:379] starting iteration 8 98.27320194244385
I0726 21:30:12.600957 140267183036224 train.py:394] {'eval/walltime': 49.64540243148804, 'training/sps': 41963.93659244866, 'training/walltime': 47.85693097114563, 'training/entropy_loss': Array(-0.03200779, dtype=float32), 'training/policy_loss': Array(0.01861529, dtype=float32), 'training/total_loss': Array(6250.2812, dtype=float32), 'training/v_loss': Array(6250.295, dtype=float32), 'eval/episode_goal_distance': (Array(0.81732, dtype=float32), Array(0.44043046, dtype=float32)), 'eval/episode_reward': (Array(-15092.951, dtype=float32), Array(7010.8037, dtype=float32)), 'eval/avg_episode_length': (Array(945.71875, dtype=float32), Array(225.68086, dtype=float32)), 'eval/epoch_eval_time': 3.817289352416992, 'eval/sps': 33531.64724569655}
I0726 21:30:12.603575 140267183036224 train.py:379] starting iteration 9 105.02460074424744
I0726 21:30:19.380222 140267183036224 train.py:394] {'eval/walltime': 53.48183631896973, 'training/sps': 41841.786146415216, 'training/walltime': 50.793708086013794, 'training/entropy_loss': Array(-0.02708281, dtype=float32), 'training/policy_loss': Array(0.02730076, dtype=float32), 'training/total_loss': Array(4906.7266, dtype=float32), 'training/v_loss': Array(4906.726, dtype=float32), 'eval/episode_goal_distance': (Array(0.8478966, dtype=float32), Array(0.43391305, dtype=float32)), 'eval/episode_reward': (Array(-15405.355, dtype=float32), Array(7066.8384, dtype=float32)), 'eval/avg_episode_length': (Array(945.5781, dtype=float32), Array(226.26508, dtype=float32)), 'eval/epoch_eval_time': 3.8364338874816895, 'eval/sps': 33364.31794580506}
I0726 21:30:19.382692 140267183036224 train.py:379] starting iteration 10 111.80371856689453
I0726 21:30:26.165871 140267183036224 train.py:394] {'eval/walltime': 57.315104722976685, 'training/sps': 41708.642722939614, 'training/walltime': 53.73986005783081, 'training/entropy_loss': Array(-0.0239144, dtype=float32), 'training/policy_loss': Array(0.02820623, dtype=float32), 'training/total_loss': Array(4039.1533, dtype=float32), 'training/v_loss': Array(4039.149, dtype=float32), 'eval/episode_goal_distance': (Array(0.7845483, dtype=float32), Array(0.37800294, dtype=float32)), 'eval/episode_reward': (Array(-14083.756, dtype=float32), Array(6172.4243, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.08247, dtype=float32)), 'eval/epoch_eval_time': 3.833268404006958, 'eval/sps': 33391.8699421622}
I0726 21:30:26.168226 140267183036224 train.py:379] starting iteration 11 118.58925175666809
I0726 21:30:32.958444 140267183036224 train.py:394] {'eval/walltime': 61.15287709236145, 'training/sps': 41705.41282118631, 'training/walltime': 56.68624019622803, 'training/entropy_loss': Array(-0.02146417, dtype=float32), 'training/policy_loss': Array(0.03038223, dtype=float32), 'training/total_loss': Array(3417.9075, dtype=float32), 'training/v_loss': Array(3417.8987, dtype=float32), 'eval/episode_goal_distance': (Array(0.8068073, dtype=float32), Array(0.39888772, dtype=float32)), 'eval/episode_reward': (Array(-14707.117, dtype=float32), Array(7085.549, dtype=float32)), 'eval/avg_episode_length': (Array(930.1406, dtype=float32), Array(254.02582, dtype=float32)), 'eval/epoch_eval_time': 3.8377723693847656, 'eval/sps': 33352.681628827224}
I0726 21:30:32.960823 140267183036224 train.py:379] starting iteration 12 125.38184905052185
I0726 21:30:39.748176 140267183036224 train.py:394] {'eval/walltime': 64.9924144744873, 'training/sps': 41737.2003340945, 'training/walltime': 59.63037633895874, 'training/entropy_loss': Array(-0.01748034, dtype=float32), 'training/policy_loss': Array(0.03783065, dtype=float32), 'training/total_loss': Array(2881.684, dtype=float32), 'training/v_loss': Array(2881.6638, dtype=float32), 'eval/episode_goal_distance': (Array(0.837546, dtype=float32), Array(0.41117284, dtype=float32)), 'eval/episode_reward': (Array(-14252.947, dtype=float32), Array(7760.819, dtype=float32)), 'eval/avg_episode_length': (Array(875.7422, dtype=float32), Array(328.7556, dtype=float32)), 'eval/epoch_eval_time': 3.8395373821258545, 'eval/sps': 33337.34959734385}
I0726 21:30:39.750618 140267183036224 train.py:379] starting iteration 13 132.171644449234
I0726 21:30:46.536158 140267183036224 train.py:394] {'eval/walltime': 68.82816076278687, 'training/sps': 41705.90891777683, 'training/walltime': 62.57672142982483, 'training/entropy_loss': Array(-0.01492838, dtype=float32), 'training/policy_loss': Array(0.03931054, dtype=float32), 'training/total_loss': Array(2483.3901, dtype=float32), 'training/v_loss': Array(2483.3657, dtype=float32), 'eval/episode_goal_distance': (Array(0.80797124, dtype=float32), Array(0.37302774, dtype=float32)), 'eval/episode_reward': (Array(-14191.443, dtype=float32), Array(6590.7207, dtype=float32)), 'eval/avg_episode_length': (Array(906.77344, dtype=float32), Array(289.85336, dtype=float32)), 'eval/epoch_eval_time': 3.8357462882995605, 'eval/sps': 33370.298862165924}
I0726 21:30:46.538383 140267183036224 train.py:379] starting iteration 14 138.95940852165222
I0726 21:30:53.326039 140267183036224 train.py:394] {'eval/walltime': 72.666672706604, 'training/sps': 41714.92170622613, 'training/walltime': 65.52242994308472, 'training/entropy_loss': Array(-0.01079639, dtype=float32), 'training/policy_loss': Array(0.0423592, dtype=float32), 'training/total_loss': Array(2247.8142, dtype=float32), 'training/v_loss': Array(2247.7825, dtype=float32), 'eval/episode_goal_distance': (Array(0.8279973, dtype=float32), Array(0.39020866, dtype=float32)), 'eval/episode_reward': (Array(-14850.641, dtype=float32), Array(6186.1855, dtype=float32)), 'eval/avg_episode_length': (Array(953.4297, dtype=float32), Array(209.99773, dtype=float32)), 'eval/epoch_eval_time': 3.8385119438171387, 'eval/sps': 33346.25549522525}
I0726 21:30:53.328374 140267183036224 train.py:379] starting iteration 15 145.74940037727356
I0726 21:31:00.112683 140267183036224 train.py:394] {'eval/walltime': 76.49914956092834, 'training/sps': 41674.83624632199, 'training/walltime': 68.47097182273865, 'training/entropy_loss': Array(-0.00737387, dtype=float32), 'training/policy_loss': Array(0.06485988, dtype=float32), 'training/total_loss': Array(2091.9836, dtype=float32), 'training/v_loss': Array(2091.9263, dtype=float32), 'eval/episode_goal_distance': (Array(0.8061013, dtype=float32), Array(0.37073436, dtype=float32)), 'eval/episode_reward': (Array(-14586.674, dtype=float32), Array(6312.871, dtype=float32)), 'eval/avg_episode_length': (Array(930.0781, dtype=float32), Array(254.2529, dtype=float32)), 'eval/epoch_eval_time': 3.832476854324341, 'eval/sps': 33398.76661109443}
I0726 21:31:00.114906 140267183036224 train.py:379] starting iteration 16 152.5359320640564
I0726 21:31:06.902185 140267183036224 train.py:394] {'eval/walltime': 80.33734321594238, 'training/sps': 41715.79618772672, 'training/walltime': 71.41661858558655, 'training/entropy_loss': Array(-0.00353094, dtype=float32), 'training/policy_loss': Array(0.07629218, dtype=float32), 'training/total_loss': Array(1891.66, dtype=float32), 'training/v_loss': Array(1891.5872, dtype=float32), 'eval/episode_goal_distance': (Array(0.7419448, dtype=float32), Array(0.34029552, dtype=float32)), 'eval/episode_reward': (Array(-13985.258, dtype=float32), Array(5891.0703, dtype=float32)), 'eval/avg_episode_length': (Array(968.9297, dtype=float32), Array(172.9922, dtype=float32)), 'eval/epoch_eval_time': 3.838193655014038, 'eval/sps': 33349.02079075316}
I0726 21:31:06.904396 140267183036224 train.py:379] starting iteration 17 159.32542300224304
I0726 21:31:13.679170 140267183036224 train.py:394] {'eval/walltime': 84.17303347587585, 'training/sps': 41856.01369028539, 'training/walltime': 74.35239744186401, 'training/entropy_loss': Array(-0.00078667, dtype=float32), 'training/policy_loss': Array(0.0716524, dtype=float32), 'training/total_loss': Array(892.5408, dtype=float32), 'training/v_loss': Array(892.46985, dtype=float32), 'eval/episode_goal_distance': (Array(0.7562846, dtype=float32), Array(0.403057, dtype=float32)), 'eval/episode_reward': (Array(-13351.67, dtype=float32), Array(6722.18, dtype=float32)), 'eval/avg_episode_length': (Array(922.33594, dtype=float32), Array(266.78525, dtype=float32)), 'eval/epoch_eval_time': 3.8356902599334717, 'eval/sps': 33370.786305936}
I0726 21:31:13.681437 140267183036224 train.py:379] starting iteration 18 166.10246300697327
I0726 21:31:20.456549 140267183036224 train.py:394] {'eval/walltime': 88.00865983963013, 'training/sps': 41853.03621728176, 'training/walltime': 77.28838515281677, 'training/entropy_loss': Array(0.00061392, dtype=float32), 'training/policy_loss': Array(0.06301752, dtype=float32), 'training/total_loss': Array(698.8949, dtype=float32), 'training/v_loss': Array(698.8313, dtype=float32), 'eval/episode_goal_distance': (Array(0.82937455, dtype=float32), Array(0.40944293, dtype=float32)), 'eval/episode_reward': (Array(-14316.653, dtype=float32), Array(7098.165, dtype=float32)), 'eval/avg_episode_length': (Array(930.09375, dtype=float32), Array(254.19638, dtype=float32)), 'eval/epoch_eval_time': 3.8356263637542725, 'eval/sps': 33371.342216637306}
I0726 21:31:20.458821 140267183036224 train.py:379] starting iteration 19 172.87984704971313
I0726 21:31:27.258061 140267183036224 train.py:394] {'eval/walltime': 91.86287498474121, 'training/sps': 41772.69930851635, 'training/walltime': 80.2300193309784, 'training/entropy_loss': Array(-0.00314704, dtype=float32), 'training/policy_loss': Array(0.06347705, dtype=float32), 'training/total_loss': Array(681.47534, dtype=float32), 'training/v_loss': Array(681.41504, dtype=float32), 'eval/episode_goal_distance': (Array(0.73148036, dtype=float32), Array(0.36602074, dtype=float32)), 'eval/episode_reward': (Array(-13136.865, dtype=float32), Array(6504.658, dtype=float32)), 'eval/avg_episode_length': (Array(906.8281, dtype=float32), Array(289.6834, dtype=float32)), 'eval/epoch_eval_time': 3.854215145111084, 'eval/sps': 33210.39308414394}
I0726 21:31:27.260215 140267183036224 train.py:379] starting iteration 20 179.68124103546143
I0726 21:31:34.066410 140267183036224 train.py:394] {'eval/walltime': 95.72438311576843, 'training/sps': 41777.04356834113, 'training/walltime': 83.17134761810303, 'training/entropy_loss': Array(-0.00855522, dtype=float32), 'training/policy_loss': Array(0.07306469, dtype=float32), 'training/total_loss': Array(705.3228, dtype=float32), 'training/v_loss': Array(705.2583, dtype=float32), 'eval/episode_goal_distance': (Array(0.6209454, dtype=float32), Array(0.3308438, dtype=float32)), 'eval/episode_reward': (Array(-12103.83, dtype=float32), Array(6205.7075, dtype=float32)), 'eval/avg_episode_length': (Array(914.5547, dtype=float32), Array(278.6671, dtype=float32)), 'eval/epoch_eval_time': 3.8615081310272217, 'eval/sps': 33147.67071743806}
I0726 21:31:34.068630 140267183036224 train.py:379] starting iteration 21 186.48965668678284
I0726 21:31:40.874533 140267183036224 train.py:394] {'eval/walltime': 99.58253026008606, 'training/sps': 41734.72300492294, 'training/walltime': 86.11565852165222, 'training/entropy_loss': Array(-0.01152242, dtype=float32), 'training/policy_loss': Array(0.07111248, dtype=float32), 'training/total_loss': Array(700.22473, dtype=float32), 'training/v_loss': Array(700.16504, dtype=float32), 'eval/episode_goal_distance': (Array(0.5568688, dtype=float32), Array(0.27075645, dtype=float32)), 'eval/episode_reward': (Array(-10869.309, dtype=float32), Array(5326.36, dtype=float32)), 'eval/avg_episode_length': (Array(914.5703, dtype=float32), Array(278.61603, dtype=float32)), 'eval/epoch_eval_time': 3.858147144317627, 'eval/sps': 33176.54698279756}
I0726 21:31:40.876874 140267183036224 train.py:379] starting iteration 22 193.29790091514587
I0726 21:31:47.677018 140267183036224 train.py:394] {'eval/walltime': 103.4351897239685, 'training/sps': 41736.26073943548, 'training/walltime': 89.05986094474792, 'training/entropy_loss': Array(-0.01625486, dtype=float32), 'training/policy_loss': Array(0.08482063, dtype=float32), 'training/total_loss': Array(728.65967, dtype=float32), 'training/v_loss': Array(728.5912, dtype=float32), 'eval/episode_goal_distance': (Array(0.49831057, dtype=float32), Array(0.26561835, dtype=float32)), 'eval/episode_reward': (Array(-10525.006, dtype=float32), Array(5092.3027, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.75842, dtype=float32)), 'eval/epoch_eval_time': 3.8526594638824463, 'eval/sps': 33223.80324551456}
I0726 21:31:47.679262 140267183036224 train.py:379] starting iteration 23 200.10028767585754
I0726 21:31:54.485916 140267183036224 train.py:394] {'eval/walltime': 107.28974628448486, 'training/sps': 41703.50953968976, 'training/walltime': 92.00637555122375, 'training/entropy_loss': Array(-0.01933558, dtype=float32), 'training/policy_loss': Array(0.04460713, dtype=float32), 'training/total_loss': Array(745.88635, dtype=float32), 'training/v_loss': Array(745.8611, dtype=float32), 'eval/episode_goal_distance': (Array(0.49686018, dtype=float32), Array(0.29269624, dtype=float32)), 'eval/episode_reward': (Array(-9789.018, dtype=float32), Array(5819.2554, dtype=float32)), 'eval/avg_episode_length': (Array(883.5, dtype=float32), Array(319.75714, dtype=float32)), 'eval/epoch_eval_time': 3.8545565605163574, 'eval/sps': 33207.4514903092}
I0726 21:31:54.488811 140267183036224 train.py:379] starting iteration 24 206.90983653068542
I0726 21:32:01.304896 140267183036224 train.py:394] {'eval/walltime': 111.14529538154602, 'training/sps': 41553.824587022464, 'training/walltime': 94.96350407600403, 'training/entropy_loss': Array(-0.01592051, dtype=float32), 'training/policy_loss': Array(0.03795778, dtype=float32), 'training/total_loss': Array(737.19617, dtype=float32), 'training/v_loss': Array(737.1742, dtype=float32), 'eval/episode_goal_distance': (Array(0.51930684, dtype=float32), Array(0.27392313, dtype=float32)), 'eval/episode_reward': (Array(-10849.381, dtype=float32), Array(5527.3804, dtype=float32)), 'eval/avg_episode_length': (Array(906.89844, dtype=float32), Array(289.46484, dtype=float32)), 'eval/epoch_eval_time': 3.8555490970611572, 'eval/sps': 33198.90287418888}
I0726 21:32:01.307115 140267183036224 train.py:379] starting iteration 25 213.7281403541565
I0726 21:32:08.134155 140267183036224 train.py:394] {'eval/walltime': 115.00302910804749, 'training/sps': 41431.92487725478, 'training/walltime': 97.92933297157288, 'training/entropy_loss': Array(-0.01159002, dtype=float32), 'training/policy_loss': Array(0.03316639, dtype=float32), 'training/total_loss': Array(698.61694, dtype=float32), 'training/v_loss': Array(698.5954, dtype=float32), 'eval/episode_goal_distance': (Array(0.53575784, dtype=float32), Array(0.2562041, dtype=float32)), 'eval/episode_reward': (Array(-10889.985, dtype=float32), Array(5070.9395, dtype=float32)), 'eval/avg_episode_length': (Array(914.60156, dtype=float32), Array(278.51434, dtype=float32)), 'eval/epoch_eval_time': 3.857733726501465, 'eval/sps': 33180.10237997472}
I0726 21:32:08.136527 140267183036224 train.py:379] starting iteration 26 220.55755305290222
I0726 21:32:14.973551 140267183036224 train.py:394] {'eval/walltime': 118.86110210418701, 'training/sps': 41297.583770070225, 'training/walltime': 100.90480971336365, 'training/entropy_loss': Array(-0.00755443, dtype=float32), 'training/policy_loss': Array(0.03135636, dtype=float32), 'training/total_loss': Array(661.92255, dtype=float32), 'training/v_loss': Array(661.89874, dtype=float32), 'eval/episode_goal_distance': (Array(0.6079662, dtype=float32), Array(0.32466203, dtype=float32)), 'eval/episode_reward': (Array(-11847.406, dtype=float32), Array(5922.189, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.08308, dtype=float32)), 'eval/epoch_eval_time': 3.8580729961395264, 'eval/sps': 33177.18460176353}
I0726 21:32:14.975925 140267183036224 train.py:379] starting iteration 27 227.39695072174072
I0726 21:32:21.818646 140267183036224 train.py:394] {'eval/walltime': 122.71912097930908, 'training/sps': 41217.04574695068, 'training/walltime': 103.88610053062439, 'training/entropy_loss': Array(-0.00367485, dtype=float32), 'training/policy_loss': Array(0.03159789, dtype=float32), 'training/total_loss': Array(604.1614, dtype=float32), 'training/v_loss': Array(604.1334, dtype=float32), 'eval/episode_goal_distance': (Array(0.56104696, dtype=float32), Array(0.2697763, dtype=float32)), 'eval/episode_reward': (Array(-10752.498, dtype=float32), Array(5390.1313, dtype=float32)), 'eval/avg_episode_length': (Array(891.2656, dtype=float32), Array(310.28146, dtype=float32)), 'eval/epoch_eval_time': 3.8580188751220703, 'eval/sps': 33177.650017575404}
I0726 21:32:21.820783 140267183036224 train.py:379] starting iteration 28 234.24180912971497
I0726 21:32:28.662138 140267183036224 train.py:394] {'eval/walltime': 126.57456541061401, 'training/sps': 41199.54709889909, 'training/walltime': 106.86865758895874, 'training/entropy_loss': Array(0.0003888, dtype=float32), 'training/policy_loss': Array(0.06211872, dtype=float32), 'training/total_loss': Array(574.20447, dtype=float32), 'training/v_loss': Array(574.14185, dtype=float32), 'eval/episode_goal_distance': (Array(0.51122737, dtype=float32), Array(0.22656082, dtype=float32)), 'eval/episode_reward': (Array(-10083.88, dtype=float32), Array(5021.26, dtype=float32)), 'eval/avg_episode_length': (Array(875.8828, dtype=float32), Array(328.38425, dtype=float32)), 'eval/epoch_eval_time': 3.8554444313049316, 'eval/sps': 33199.804142080844}
I0726 21:32:28.664261 140267183036224 train.py:379] starting iteration 29 241.08528661727905
I0726 21:32:35.509808 140267183036224 train.py:394] {'eval/walltime': 130.4327290058136, 'training/sps': 41177.3681517237, 'training/walltime': 109.85282111167908, 'training/entropy_loss': Array(0.00596417, dtype=float32), 'training/policy_loss': Array(0.03250118, dtype=float32), 'training/total_loss': Array(552.98865, dtype=float32), 'training/v_loss': Array(552.9502, dtype=float32), 'eval/episode_goal_distance': (Array(0.46120033, dtype=float32), Array(0.17952842, dtype=float32)), 'eval/episode_reward': (Array(-8944.299, dtype=float32), Array(4751.108, dtype=float32)), 'eval/avg_episode_length': (Array(844.59375, dtype=float32), Array(361.13202, dtype=float32)), 'eval/epoch_eval_time': 3.858163595199585, 'eval/sps': 33176.40552081838}
I0726 21:32:35.512336 140267183036224 train.py:379] starting iteration 30 247.9333610534668
I0726 21:32:42.362451 140267183036224 train.py:394] {'eval/walltime': 134.29164719581604, 'training/sps': 41125.31951520587, 'training/walltime': 112.84076142311096, 'training/entropy_loss': Array(0.01159386, dtype=float32), 'training/policy_loss': Array(0.03413442, dtype=float32), 'training/total_loss': Array(504.17047, dtype=float32), 'training/v_loss': Array(504.12476, dtype=float32), 'eval/episode_goal_distance': (Array(0.50190115, dtype=float32), Array(0.22756079, dtype=float32)), 'eval/episode_reward': (Array(-10744.433, dtype=float32), Array(5336.7437, dtype=float32)), 'eval/avg_episode_length': (Array(914.6172, dtype=float32), Array(278.46335, dtype=float32)), 'eval/epoch_eval_time': 3.8589181900024414, 'eval/sps': 33169.91801785749}
I0726 21:32:42.364701 140267183036224 train.py:379] starting iteration 31 254.78572726249695
I0726 21:32:49.216558 140267183036224 train.py:394] {'eval/walltime': 138.14478969573975, 'training/sps': 41021.78576818137, 'training/walltime': 115.83624291419983, 'training/entropy_loss': Array(0.01861765, dtype=float32), 'training/policy_loss': Array(0.03306139, dtype=float32), 'training/total_loss': Array(470.6989, dtype=float32), 'training/v_loss': Array(470.64722, dtype=float32), 'eval/episode_goal_distance': (Array(0.4610908, dtype=float32), Array(0.17320879, dtype=float32)), 'eval/episode_reward': (Array(-9345.996, dtype=float32), Array(5011.445, dtype=float32)), 'eval/avg_episode_length': (Array(860.1953, dtype=float32), Array(345.60706, dtype=float32)), 'eval/epoch_eval_time': 3.853142499923706, 'eval/sps': 33219.63825696414}
I0726 21:32:49.218965 140267183036224 train.py:379] starting iteration 32 261.6399908065796
I0726 21:32:56.062186 140267183036224 train.py:394] {'eval/walltime': 142.00083923339844, 'training/sps': 41180.4081973127, 'training/walltime': 118.82018613815308, 'training/entropy_loss': Array(0.02828342, dtype=float32), 'training/policy_loss': Array(0.03896944, dtype=float32), 'training/total_loss': Array(453.4403, dtype=float32), 'training/v_loss': Array(453.37305, dtype=float32), 'eval/episode_goal_distance': (Array(0.5013024, dtype=float32), Array(0.19508283, dtype=float32)), 'eval/episode_reward': (Array(-10961.381, dtype=float32), Array(4737.392, dtype=float32)), 'eval/avg_episode_length': (Array(930.09375, dtype=float32), Array(254.19601, dtype=float32)), 'eval/epoch_eval_time': 3.8560495376586914, 'eval/sps': 33194.59429914865}
I0726 21:32:56.064394 140267183036224 train.py:379] starting iteration 33 268.4854202270508
I0726 21:33:02.913522 140267183036224 train.py:394] {'eval/walltime': 145.86070609092712, 'training/sps': 41151.871164439144, 'training/walltime': 121.80619859695435, 'training/entropy_loss': Array(0.0378718, dtype=float32), 'training/policy_loss': Array(0.04131348, dtype=float32), 'training/total_loss': Array(826.1658, dtype=float32), 'training/v_loss': Array(826.0866, dtype=float32), 'eval/episode_goal_distance': (Array(0.47181442, dtype=float32), Array(0.19380343, dtype=float32)), 'eval/episode_reward': (Array(-9744.63, dtype=float32), Array(5365.133, dtype=float32)), 'eval/avg_episode_length': (Array(860.28906, dtype=float32), Array(345.3749, dtype=float32)), 'eval/epoch_eval_time': 3.8598668575286865, 'eval/sps': 33161.76560606889}
I0726 21:33:02.915769 140267183036224 train.py:379] starting iteration 34 275.3367953300476
I0726 21:33:09.763782 140267183036224 train.py:394] {'eval/walltime': 149.71858835220337, 'training/sps': 41140.10816083398, 'training/walltime': 124.79306483268738, 'training/entropy_loss': Array(0.04597854, dtype=float32), 'training/policy_loss': Array(0.05250246, dtype=float32), 'training/total_loss': Array(519.0957, dtype=float32), 'training/v_loss': Array(518.9972, dtype=float32), 'eval/episode_goal_distance': (Array(0.505959, dtype=float32), Array(0.21846463, dtype=float32)), 'eval/episode_reward': (Array(-11032.006, dtype=float32), Array(5351.172, dtype=float32)), 'eval/avg_episode_length': (Array(891.3594, dtype=float32), Array(310.01428, dtype=float32)), 'eval/epoch_eval_time': 3.857882261276245, 'eval/sps': 33178.824891782904}
I0726 21:33:09.766056 140267183036224 train.py:379] starting iteration 35 282.1870822906494
I0726 21:33:16.625581 140267183036224 train.py:394] {'eval/walltime': 153.57584381103516, 'training/sps': 41002.6159072586, 'training/walltime': 127.78994679450989, 'training/entropy_loss': Array(0.05201378, dtype=float32), 'training/policy_loss': Array(0.04235214, dtype=float32), 'training/total_loss': Array(473.67242, dtype=float32), 'training/v_loss': Array(473.57806, dtype=float32), 'eval/episode_goal_distance': (Array(0.5679486, dtype=float32), Array(0.26103452, dtype=float32)), 'eval/episode_reward': (Array(-12564.754, dtype=float32), Array(5978.5166, dtype=float32)), 'eval/avg_episode_length': (Array(930.0547, dtype=float32), Array(254.338, dtype=float32)), 'eval/epoch_eval_time': 3.857255458831787, 'eval/sps': 33184.21643734383}
I0726 21:33:16.627671 140267183036224 train.py:379] starting iteration 36 289.04869651794434
I0726 21:33:23.488067 140267183036224 train.py:394] {'eval/walltime': 157.447927236557, 'training/sps': 41165.7483924549, 'training/walltime': 130.7749526500702, 'training/entropy_loss': Array(0.05379069, dtype=float32), 'training/policy_loss': Array(0.0303733, dtype=float32), 'training/total_loss': Array(500.37643, dtype=float32), 'training/v_loss': Array(500.29224, dtype=float32), 'eval/episode_goal_distance': (Array(0.5959388, dtype=float32), Array(0.2942766, dtype=float32)), 'eval/episode_reward': (Array(-12677.621, dtype=float32), Array(7216.018, dtype=float32)), 'eval/avg_episode_length': (Array(883.4531, dtype=float32), Array(319.88574, dtype=float32)), 'eval/epoch_eval_time': 3.8720834255218506, 'eval/sps': 33057.13899559101}
I0726 21:33:23.490252 140267183036224 train.py:379] starting iteration 37 295.91127848625183
I0726 21:33:30.395025 140267183036224 train.py:394] {'eval/walltime': 161.34870767593384, 'training/sps': 40951.915658434584, 'training/walltime': 133.77554488182068, 'training/entropy_loss': Array(0.0581822, dtype=float32), 'training/policy_loss': Array(0.01815064, dtype=float32), 'training/total_loss': Array(537.99805, dtype=float32), 'training/v_loss': Array(537.9217, dtype=float32), 'eval/episode_goal_distance': (Array(0.5433461, dtype=float32), Array(0.2845776, dtype=float32)), 'eval/episode_reward': (Array(-11733.246, dtype=float32), Array(6673.677, dtype=float32)), 'eval/avg_episode_length': (Array(875.7656, dtype=float32), Array(328.69394, dtype=float32)), 'eval/epoch_eval_time': 3.900780439376831, 'eval/sps': 32813.94633440293}
I0726 21:33:30.397210 140267183036224 train.py:379] starting iteration 38 302.81823563575745
I0726 21:33:37.312647 140267183036224 train.py:394] {'eval/walltime': 165.24133729934692, 'training/sps': 40696.31784113628, 'training/walltime': 136.79498267173767, 'training/entropy_loss': Array(0.06278007, dtype=float32), 'training/policy_loss': Array(0.01288414, dtype=float32), 'training/total_loss': Array(565.4175, dtype=float32), 'training/v_loss': Array(565.3418, dtype=float32), 'eval/episode_goal_distance': (Array(0.59204054, dtype=float32), Array(0.27716562, dtype=float32)), 'eval/episode_reward': (Array(-12784.982, dtype=float32), Array(7036.895, dtype=float32)), 'eval/avg_episode_length': (Array(891.3047, dtype=float32), Array(310.17053, dtype=float32)), 'eval/epoch_eval_time': 3.892629623413086, 'eval/sps': 32882.655783667564}
I0726 21:33:37.314742 140267183036224 train.py:379] starting iteration 39 309.73576736450195
I0726 21:33:44.233062 140267183036224 train.py:394] {'eval/walltime': 169.14334082603455, 'training/sps': 40782.53654749224, 'training/walltime': 139.8080370426178, 'training/entropy_loss': Array(0.06286365, dtype=float32), 'training/policy_loss': Array(0.01363571, dtype=float32), 'training/total_loss': Array(571.9693, dtype=float32), 'training/v_loss': Array(571.8928, dtype=float32), 'eval/episode_goal_distance': (Array(0.5718006, dtype=float32), Array(0.3252799, dtype=float32)), 'eval/episode_reward': (Array(-11452.358, dtype=float32), Array(7563.8706, dtype=float32)), 'eval/avg_episode_length': (Array(813.6406, dtype=float32), Array(387.93854, dtype=float32)), 'eval/epoch_eval_time': 3.902003526687622, 'eval/sps': 32803.66076671851}
I0726 21:33:44.235226 140267183036224 train.py:379] starting iteration 40 316.65625190734863
I0726 21:33:51.167239 140267183036224 train.py:394] {'eval/walltime': 173.05477142333984, 'training/sps': 40727.14835328391, 'training/walltime': 142.82518911361694, 'training/entropy_loss': Array(0.06129886, dtype=float32), 'training/policy_loss': Array(0.00816961, dtype=float32), 'training/total_loss': Array(568.6222, dtype=float32), 'training/v_loss': Array(568.5528, dtype=float32), 'eval/episode_goal_distance': (Array(0.60683554, dtype=float32), Array(0.36377117, dtype=float32)), 'eval/episode_reward': (Array(-12827.339, dtype=float32), Array(7615.2544, dtype=float32)), 'eval/avg_episode_length': (Array(891.25, dtype=float32), Array(310.32593, dtype=float32)), 'eval/epoch_eval_time': 3.911430597305298, 'eval/sps': 32724.599559093047}
I0726 21:33:51.169349 140267183036224 train.py:379] starting iteration 41 323.5903754234314
I0726 21:33:58.110326 140267183036224 train.py:394] {'eval/walltime': 176.9693865776062, 'training/sps': 40647.81112709572, 'training/walltime': 145.8482301235199, 'training/entropy_loss': Array(0.05873342, dtype=float32), 'training/policy_loss': Array(0.01723029, dtype=float32), 'training/total_loss': Array(561.73846, dtype=float32), 'training/v_loss': Array(561.66254, dtype=float32), 'eval/episode_goal_distance': (Array(0.5798123, dtype=float32), Array(0.2613656, dtype=float32)), 'eval/episode_reward': (Array(-12439.063, dtype=float32), Array(6391.591, dtype=float32)), 'eval/avg_episode_length': (Array(891.28125, dtype=float32), Array(310.23688, dtype=float32)), 'eval/epoch_eval_time': 3.9146151542663574, 'eval/sps': 32697.9779507824}
I0726 21:33:58.112564 140267183036224 train.py:379] starting iteration 42 330.5335900783539
I0726 21:34:05.072443 140267183036224 train.py:394] {'eval/walltime': 180.89798378944397, 'training/sps': 40582.28826478688, 'training/walltime': 148.87615203857422, 'training/entropy_loss': Array(0.05907545, dtype=float32), 'training/policy_loss': Array(0.0072259, dtype=float32), 'training/total_loss': Array(569.16455, dtype=float32), 'training/v_loss': Array(569.0982, dtype=float32), 'eval/episode_goal_distance': (Array(0.5300747, dtype=float32), Array(0.2748279, dtype=float32)), 'eval/episode_reward': (Array(-10550.787, dtype=float32), Array(6753.954, dtype=float32)), 'eval/avg_episode_length': (Array(805.8047, dtype=float32), Array(394.17407, dtype=float32)), 'eval/epoch_eval_time': 3.9285972118377686, 'eval/sps': 32581.60434831713}
I0726 21:34:05.075346 140267183036224 train.py:379] starting iteration 43 337.49637246131897
I0726 21:34:12.042548 140267183036224 train.py:394] {'eval/walltime': 184.83623456954956, 'training/sps': 40612.54295080975, 'training/walltime': 151.90181827545166, 'training/entropy_loss': Array(0.05315831, dtype=float32), 'training/policy_loss': Array(0.00601206, dtype=float32), 'training/total_loss': Array(586.943, dtype=float32), 'training/v_loss': Array(586.8838, dtype=float32), 'eval/episode_goal_distance': (Array(0.5777846, dtype=float32), Array(0.28667754, dtype=float32)), 'eval/episode_reward': (Array(-11947.656, dtype=float32), Array(7184.296, dtype=float32)), 'eval/avg_episode_length': (Array(852.40625, dtype=float32), Array(353.51285, dtype=float32)), 'eval/epoch_eval_time': 3.938250780105591, 'eval/sps': 32501.739261146828}
I0726 21:34:12.044921 140267183036224 train.py:379] starting iteration 44 344.46594762802124
I0726 21:34:19.016169 140267183036224 train.py:394] {'eval/walltime': 188.7819836139679, 'training/sps': 40660.17956883788, 'training/walltime': 154.92393970489502, 'training/entropy_loss': Array(0.04911314, dtype=float32), 'training/policy_loss': Array(0.00586872, dtype=float32), 'training/total_loss': Array(583.9967, dtype=float32), 'training/v_loss': Array(583.9418, dtype=float32), 'eval/episode_goal_distance': (Array(0.5453855, dtype=float32), Array(0.28195634, dtype=float32)), 'eval/episode_reward': (Array(-11439.963, dtype=float32), Array(6584.445, dtype=float32)), 'eval/avg_episode_length': (Array(867.9375, dtype=float32), Array(337.4562, dtype=float32)), 'eval/epoch_eval_time': 3.945749044418335, 'eval/sps': 32439.97490947101}
I0726 21:34:19.018617 140267183036224 train.py:379] starting iteration 45 351.4396438598633
I0726 21:34:26.001318 140267183036224 train.py:394] {'eval/walltime': 192.73927426338196, 'training/sps': 40694.59230127303, 'training/walltime': 157.943505525589, 'training/entropy_loss': Array(0.04401737, dtype=float32), 'training/policy_loss': Array(0.00590572, dtype=float32), 'training/total_loss': Array(532.8965, dtype=float32), 'training/v_loss': Array(532.84656, dtype=float32), 'eval/episode_goal_distance': (Array(0.57174945, dtype=float32), Array(0.32115462, dtype=float32)), 'eval/episode_reward': (Array(-11773.036, dtype=float32), Array(7588.3374, dtype=float32)), 'eval/avg_episode_length': (Array(852.5469, dtype=float32), Array(353.17596, dtype=float32)), 'eval/epoch_eval_time': 3.9572906494140625, 'eval/sps': 32345.362355163972}
I0726 21:34:26.003617 140267183036224 train.py:379] starting iteration 46 358.42464327812195
I0726 21:34:32.993442 140267183036224 train.py:394] {'eval/walltime': 196.70381665229797, 'training/sps': 40668.6304793585, 'training/walltime': 160.964998960495, 'training/entropy_loss': Array(0.03975905, dtype=float32), 'training/policy_loss': Array(0.00434339, dtype=float32), 'training/total_loss': Array(511.3136, dtype=float32), 'training/v_loss': Array(511.2695, dtype=float32), 'eval/episode_goal_distance': (Array(0.5889433, dtype=float32), Array(0.30714047, dtype=float32)), 'eval/episode_reward': (Array(-12904.238, dtype=float32), Array(6998.1416, dtype=float32)), 'eval/avg_episode_length': (Array(906.8125, dtype=float32), Array(289.73267, dtype=float32)), 'eval/epoch_eval_time': 3.9645423889160156, 'eval/sps': 32286.197861790988}
I0726 21:34:32.995832 140267183036224 train.py:379] starting iteration 47 365.4168589115143
I0726 21:34:39.989738 140267183036224 train.py:394] {'eval/walltime': 200.6769826412201, 'training/sps': 40732.41095822422, 'training/walltime': 163.9817612171173, 'training/entropy_loss': Array(0.03762588, dtype=float32), 'training/policy_loss': Array(0.00365714, dtype=float32), 'training/total_loss': Array(487.4081, dtype=float32), 'training/v_loss': Array(487.36682, dtype=float32), 'eval/episode_goal_distance': (Array(0.584038, dtype=float32), Array(0.2859798, dtype=float32)), 'eval/episode_reward': (Array(-12693.127, dtype=float32), Array(6833.624, dtype=float32)), 'eval/avg_episode_length': (Array(914.53906, dtype=float32), Array(278.71793, dtype=float32)), 'eval/epoch_eval_time': 3.973165988922119, 'eval/sps': 32216.12194327807}
I0726 21:34:39.992192 140267183036224 train.py:379] starting iteration 48 372.4132170677185
I0726 21:34:46.976305 140267183036224 train.py:394] {'eval/walltime': 204.63683605194092, 'training/sps': 40681.9877683155, 'training/walltime': 167.00226259231567, 'training/entropy_loss': Array(0.03469884, dtype=float32), 'training/policy_loss': Array(0.00358022, dtype=float32), 'training/total_loss': Array(467.77036, dtype=float32), 'training/v_loss': Array(467.73206, dtype=float32), 'eval/episode_goal_distance': (Array(0.5451869, dtype=float32), Array(0.2544862, dtype=float32)), 'eval/episode_reward': (Array(-11994.668, dtype=float32), Array(6102.674, dtype=float32)), 'eval/avg_episode_length': (Array(883.53125, dtype=float32), Array(319.67175, dtype=float32)), 'eval/epoch_eval_time': 3.959853410720825, 'eval/sps': 32324.428892608867}
I0726 21:34:46.978677 140267183036224 train.py:379] starting iteration 49 379.39970231056213
I0726 21:34:53.971105 140267183036224 train.py:394] {'eval/walltime': 208.60165572166443, 'training/sps': 40637.84681165515, 'training/walltime': 170.02604484558105, 'training/entropy_loss': Array(0.03311843, dtype=float32), 'training/policy_loss': Array(0.0058035, dtype=float32), 'training/total_loss': Array(469.7929, dtype=float32), 'training/v_loss': Array(469.75397, dtype=float32), 'eval/episode_goal_distance': (Array(0.5023594, dtype=float32), Array(0.2635242, dtype=float32)), 'eval/episode_reward': (Array(-11483.524, dtype=float32), Array(5634.071, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.08243, dtype=float32)), 'eval/epoch_eval_time': 3.9648196697235107, 'eval/sps': 32283.93991722861}
I0726 21:34:53.973620 140267183036224 train.py:379] starting iteration 50 386.3946464061737
I0726 21:35:00.970452 140267183036224 train.py:394] {'eval/walltime': 212.56643652915955, 'training/sps': 40576.99409574259, 'training/walltime': 173.05436182022095, 'training/entropy_loss': Array(0.02293453, dtype=float32), 'training/policy_loss': Array(0.00508849, dtype=float32), 'training/total_loss': Array(797.7292, dtype=float32), 'training/v_loss': Array(797.7011, dtype=float32), 'eval/episode_goal_distance': (Array(0.55504256, dtype=float32), Array(0.2968537, dtype=float32)), 'eval/episode_reward': (Array(-11472.436, dtype=float32), Array(6921.509, dtype=float32)), 'eval/avg_episode_length': (Array(875.83594, dtype=float32), Array(328.50854, dtype=float32)), 'eval/epoch_eval_time': 3.964780807495117, 'eval/sps': 32284.256359904113}
I0726 21:35:00.972967 140267183036224 train.py:379] starting iteration 51 393.3939936161041
I0726 21:35:07.982075 140267183036224 train.py:394] {'eval/walltime': 216.5435814857483, 'training/sps': 40578.629804970726, 'training/walltime': 176.08255672454834, 'training/entropy_loss': Array(0.01664197, dtype=float32), 'training/policy_loss': Array(0.00230566, dtype=float32), 'training/total_loss': Array(501.99536, dtype=float32), 'training/v_loss': Array(501.9764, dtype=float32), 'eval/episode_goal_distance': (Array(0.5180781, dtype=float32), Array(0.2624163, dtype=float32)), 'eval/episode_reward': (Array(-11034.047, dtype=float32), Array(6603.4873, dtype=float32)), 'eval/avg_episode_length': (Array(844.66406, dtype=float32), Array(360.9686, dtype=float32)), 'eval/epoch_eval_time': 3.977144956588745, 'eval/sps': 32183.891056811633}
I0726 21:35:08.026640 140267183036224 train.py:379] starting iteration 52 400.44766569137573
I0726 21:35:15.029086 140267183036224 train.py:394] {'eval/walltime': 220.5237832069397, 'training/sps': 40710.82844560071, 'training/walltime': 179.10091829299927, 'training/entropy_loss': Array(0.01155864, dtype=float32), 'training/policy_loss': Array(0.00177005, dtype=float32), 'training/total_loss': Array(439.5278, dtype=float32), 'training/v_loss': Array(439.51447, dtype=float32), 'eval/episode_goal_distance': (Array(0.56053865, dtype=float32), Array(0.2882302, dtype=float32)), 'eval/episode_reward': (Array(-12649.358, dtype=float32), Array(6301.9336, dtype=float32)), 'eval/avg_episode_length': (Array(930.1406, dtype=float32), Array(254.02603, dtype=float32)), 'eval/epoch_eval_time': 3.9802017211914062, 'eval/sps': 32159.17407364101}
I0726 21:35:15.031805 140267183036224 train.py:379] starting iteration 53 407.4528315067291
I0726 21:35:22.031947 140267183036224 train.py:394] {'eval/walltime': 224.4899024963379, 'training/sps': 40550.674765533644, 'training/walltime': 182.13120079040527, 'training/entropy_loss': Array(0.00761939, dtype=float32), 'training/policy_loss': Array(0.00181748, dtype=float32), 'training/total_loss': Array(418.97614, dtype=float32), 'training/v_loss': Array(418.96667, dtype=float32), 'eval/episode_goal_distance': (Array(0.511407, dtype=float32), Array(0.28901625, dtype=float32)), 'eval/episode_reward': (Array(-11274.052, dtype=float32), Array(6951.268, dtype=float32)), 'eval/avg_episode_length': (Array(883.52344, dtype=float32), Array(319.69275, dtype=float32)), 'eval/epoch_eval_time': 3.9661192893981934, 'eval/sps': 32273.361101910356}
I0726 21:35:22.034284 140267183036224 train.py:379] starting iteration 54 414.45530915260315
I0726 21:35:29.052061 140267183036224 train.py:394] {'eval/walltime': 228.46935868263245, 'training/sps': 40515.049366006184, 'training/walltime': 185.16414785385132, 'training/entropy_loss': Array(0.00263509, dtype=float32), 'training/policy_loss': Array(0.00145745, dtype=float32), 'training/total_loss': Array(408.79205, dtype=float32), 'training/v_loss': Array(408.78793, dtype=float32), 'eval/episode_goal_distance': (Array(0.52400196, dtype=float32), Array(0.28496206, dtype=float32)), 'eval/episode_reward': (Array(-12243.686, dtype=float32), Array(6158.3247, dtype=float32)), 'eval/avg_episode_length': (Array(922.3203, dtype=float32), Array(266.83902, dtype=float32)), 'eval/epoch_eval_time': 3.9794561862945557, 'eval/sps': 32165.198963827857}
I0726 21:35:29.054521 140267183036224 train.py:379] starting iteration 55 421.475546836853
I0726 21:35:36.080864 140267183036224 train.py:394] {'eval/walltime': 232.46409559249878, 'training/sps': 40582.01665339906, 'training/walltime': 188.19209003448486, 'training/entropy_loss': Array(-0.00162781, dtype=float32), 'training/policy_loss': Array(0.00177278, dtype=float32), 'training/total_loss': Array(374.18457, dtype=float32), 'training/v_loss': Array(374.18445, dtype=float32), 'eval/episode_goal_distance': (Array(0.54267395, dtype=float32), Array(0.28964075, dtype=float32)), 'eval/episode_reward': (Array(-12257.908, dtype=float32), Array(6331.053, dtype=float32)), 'eval/avg_episode_length': (Array(937.85156, dtype=float32), Array(240.70012, dtype=float32)), 'eval/epoch_eval_time': 3.994736909866333, 'eval/sps': 32042.160194294993}
I0726 21:35:36.083245 140267183036224 train.py:379] starting iteration 56 428.50427079200745
I0726 21:35:43.108219 140267183036224 train.py:394] {'eval/walltime': 236.47075271606445, 'training/sps': 40761.91959258649, 'training/walltime': 191.2066683769226, 'training/entropy_loss': Array(-0.00394541, dtype=float32), 'training/policy_loss': Array(0.00167335, dtype=float32), 'training/total_loss': Array(359.29755, dtype=float32), 'training/v_loss': Array(359.29984, dtype=float32), 'eval/episode_goal_distance': (Array(0.54810417, dtype=float32), Array(0.2749892, dtype=float32)), 'eval/episode_reward': (Array(-11908.6875, dtype=float32), Array(6089.872, dtype=float32)), 'eval/avg_episode_length': (Array(914.53906, dtype=float32), Array(278.71802, dtype=float32)), 'eval/epoch_eval_time': 4.006657123565674, 'eval/sps': 31946.83149879519}
I0726 21:35:43.110608 140267183036224 train.py:379] starting iteration 57 435.5316345691681
I0726 21:35:50.142054 140267183036224 train.py:394] {'eval/walltime': 240.4705958366394, 'training/sps': 40585.57984470425, 'training/walltime': 194.23434472084045, 'training/entropy_loss': Array(-0.00644236, dtype=float32), 'training/policy_loss': Array(0.0012407, dtype=float32), 'training/total_loss': Array(344.23108, dtype=float32), 'training/v_loss': Array(344.23627, dtype=float32), 'eval/episode_goal_distance': (Array(0.5265097, dtype=float32), Array(0.30982682, dtype=float32)), 'eval/episode_reward': (Array(-11439.389, dtype=float32), Array(6869.294, dtype=float32)), 'eval/avg_episode_length': (Array(875.6797, dtype=float32), Array(328.9211, dtype=float32)), 'eval/epoch_eval_time': 3.999843120574951, 'eval/sps': 32001.25508462463}
I0726 21:35:50.144538 140267183036224 train.py:379] starting iteration 58 442.5655641555786
I0726 21:35:57.195168 140267183036224 train.py:394] {'eval/walltime': 244.48808312416077, 'training/sps': 40564.81029589004, 'training/walltime': 197.26357126235962, 'training/entropy_loss': Array(-0.00775434, dtype=float32), 'training/policy_loss': Array(0.00163121, dtype=float32), 'training/total_loss': Array(327.10117, dtype=float32), 'training/v_loss': Array(327.1073, dtype=float32), 'eval/episode_goal_distance': (Array(0.585449, dtype=float32), Array(0.28984883, dtype=float32)), 'eval/episode_reward': (Array(-13393.619, dtype=float32), Array(6419.5015, dtype=float32)), 'eval/avg_episode_length': (Array(945.6328, dtype=float32), Array(226.03813, dtype=float32)), 'eval/epoch_eval_time': 4.017487287521362, 'eval/sps': 31860.710648065586}
I0726 21:35:57.197699 140267183036224 train.py:379] starting iteration 59 449.6187252998352
I0726 21:36:04.240477 140267183036224 train.py:394] {'eval/walltime': 248.5106167793274, 'training/sps': 40737.38512843321, 'training/walltime': 200.27996516227722, 'training/entropy_loss': Array(-0.00991746, dtype=float32), 'training/policy_loss': Array(0.00180631, dtype=float32), 'training/total_loss': Array(322.1785, dtype=float32), 'training/v_loss': Array(322.18658, dtype=float32), 'eval/episode_goal_distance': (Array(0.54678375, dtype=float32), Array(0.26530713, dtype=float32)), 'eval/episode_reward': (Array(-11696.422, dtype=float32), Array(6436.499, dtype=float32)), 'eval/avg_episode_length': (Array(883.6094, dtype=float32), Array(319.45712, dtype=float32)), 'eval/epoch_eval_time': 4.022533655166626, 'eval/sps': 31820.740601037392}
I0726 21:36:04.243148 140267183036224 train.py:379] starting iteration 60 456.664174079895
I0726 21:36:11.301311 140267183036224 train.py:394] {'eval/walltime': 252.53798580169678, 'training/sps': 40594.47617289314, 'training/walltime': 203.30697798728943, 'training/entropy_loss': Array(-0.01243494, dtype=float32), 'training/policy_loss': Array(0.0017673, dtype=float32), 'training/total_loss': Array(312.20587, dtype=float32), 'training/v_loss': Array(312.2165, dtype=float32), 'eval/episode_goal_distance': (Array(0.52406234, dtype=float32), Array(0.2593083, dtype=float32)), 'eval/episode_reward': (Array(-11346.926, dtype=float32), Array(6542.6226, dtype=float32)), 'eval/avg_episode_length': (Array(875.71875, dtype=float32), Array(328.81778, dtype=float32)), 'eval/epoch_eval_time': 4.027369022369385, 'eval/sps': 31782.535766909918}
I0726 21:36:11.303689 140267183036224 train.py:379] starting iteration 61 463.72471499443054
I0726 21:36:18.361271 140267183036224 train.py:394] {'eval/walltime': 256.580607175827, 'training/sps': 40805.42603239741, 'training/walltime': 206.3183422088623, 'training/entropy_loss': Array(-0.01616659, dtype=float32), 'training/policy_loss': Array(0.00160468, dtype=float32), 'training/total_loss': Array(301.11084, dtype=float32), 'training/v_loss': Array(301.12543, dtype=float32), 'eval/episode_goal_distance': (Array(0.56103796, dtype=float32), Array(0.31720427, dtype=float32)), 'eval/episode_reward': (Array(-12285.626, dtype=float32), Array(6841.264, dtype=float32)), 'eval/avg_episode_length': (Array(914.60156, dtype=float32), Array(278.51413, dtype=float32)), 'eval/epoch_eval_time': 4.042621374130249, 'eval/sps': 31662.623865570047}
I0726 21:36:18.363775 140267183036224 train.py:379] starting iteration 62 470.78480052948
I0726 21:36:25.423566 140267183036224 train.py:394] {'eval/walltime': 260.6117401123047, 'training/sps': 40617.40786145907, 'training/walltime': 209.3436460494995, 'training/entropy_loss': Array(-0.01645609, dtype=float32), 'training/policy_loss': Array(0.00191033, dtype=float32), 'training/total_loss': Array(302.26758, dtype=float32), 'training/v_loss': Array(302.2821, dtype=float32), 'eval/episode_goal_distance': (Array(0.585439, dtype=float32), Array(0.31478292, dtype=float32)), 'eval/episode_reward': (Array(-12639.107, dtype=float32), Array(6610.115, dtype=float32)), 'eval/avg_episode_length': (Array(906.71875, dtype=float32), Array(290.02344, dtype=float32)), 'eval/epoch_eval_time': 4.031132936477661, 'eval/sps': 31752.860056221398}
I0726 21:36:25.426071 140267183036224 train.py:379] starting iteration 63 477.847097158432
I0726 21:36:32.476184 140267183036224 train.py:394] {'eval/walltime': 264.64779710769653, 'training/sps': 40814.616172198126, 'training/walltime': 212.35433220863342, 'training/entropy_loss': Array(-0.01834593, dtype=float32), 'training/policy_loss': Array(0.00150392, dtype=float32), 'training/total_loss': Array(282.3716, dtype=float32), 'training/v_loss': Array(282.3885, dtype=float32), 'eval/episode_goal_distance': (Array(0.5126631, dtype=float32), Array(0.24126151, dtype=float32)), 'eval/episode_reward': (Array(-10677.436, dtype=float32), Array(6141.798, dtype=float32)), 'eval/avg_episode_length': (Array(860.1406, dtype=float32), Array(345.74213, dtype=float32)), 'eval/epoch_eval_time': 4.036056995391846, 'eval/sps': 31714.12102112125}
I0726 21:36:32.478680 140267183036224 train.py:379] starting iteration 64 484.8997061252594
I0726 21:36:39.539944 140267183036224 train.py:394] {'eval/walltime': 268.6819770336151, 'training/sps': 40638.474845507175, 'training/walltime': 215.3780677318573, 'training/entropy_loss': Array(-0.02245526, dtype=float32), 'training/policy_loss': Array(0.00237474, dtype=float32), 'training/total_loss': Array(270.07623, dtype=float32), 'training/v_loss': Array(270.0963, dtype=float32), 'eval/episode_goal_distance': (Array(0.52399254, dtype=float32), Array(0.25647035, dtype=float32)), 'eval/episode_reward': (Array(-11328.433, dtype=float32), Array(6595.2896, dtype=float32)), 'eval/avg_episode_length': (Array(852.46094, dtype=float32), Array(353.38208, dtype=float32)), 'eval/epoch_eval_time': 4.034179925918579, 'eval/sps': 31728.877330838066}
I0726 21:36:39.542243 140267183036224 train.py:379] starting iteration 65 491.96326899528503
I0726 21:36:46.599968 140267183036224 train.py:394] {'eval/walltime': 272.7253177165985, 'training/sps': 40811.55233234572, 'training/walltime': 218.3889799118042, 'training/entropy_loss': Array(-0.02617478, dtype=float32), 'training/policy_loss': Array(0.00138129, dtype=float32), 'training/total_loss': Array(275.44458, dtype=float32), 'training/v_loss': Array(275.4694, dtype=float32), 'eval/episode_goal_distance': (Array(0.45917988, dtype=float32), Array(0.21281365, dtype=float32)), 'eval/episode_reward': (Array(-10055.102, dtype=float32), Array(5410.4536, dtype=float32)), 'eval/avg_episode_length': (Array(875.6172, dtype=float32), Array(329.08652, dtype=float32)), 'eval/epoch_eval_time': 4.043340682983398, 'eval/sps': 31656.991096173122}
I0726 21:36:46.602397 140267183036224 train.py:379] starting iteration 66 499.0234224796295
I0726 21:36:53.665754 140267183036224 train.py:394] {'eval/walltime': 276.7622151374817, 'training/sps': 40646.45512893715, 'training/walltime': 221.4121217727661, 'training/entropy_loss': Array(-0.02817918, dtype=float32), 'training/policy_loss': Array(0.00163223, dtype=float32), 'training/total_loss': Array(557.8866, dtype=float32), 'training/v_loss': Array(557.91315, dtype=float32), 'eval/episode_goal_distance': (Array(0.4984388, dtype=float32), Array(0.2545942, dtype=float32)), 'eval/episode_reward': (Array(-10287.197, dtype=float32), Array(6656.72, dtype=float32)), 'eval/avg_episode_length': (Array(821.4297, dtype=float32), Array(381.54074, dtype=float32)), 'eval/epoch_eval_time': 4.036897420883179, 'eval/sps': 31707.518585398335}
I0726 21:36:53.668096 140267183036224 train.py:379] starting iteration 67 506.0891206264496
I0726 21:37:00.734950 140267183036224 train.py:394] {'eval/walltime': 280.8019368648529, 'training/sps': 40639.519475419766, 'training/walltime': 224.4357795715332, 'training/entropy_loss': Array(-0.02880656, dtype=float32), 'training/policy_loss': Array(0.0013774, dtype=float32), 'training/total_loss': Array(529.5304, dtype=float32), 'training/v_loss': Array(529.5578, dtype=float32), 'eval/episode_goal_distance': (Array(0.53104174, dtype=float32), Array(0.24941815, dtype=float32)), 'eval/episode_reward': (Array(-10087.145, dtype=float32), Array(7030.8115, dtype=float32)), 'eval/avg_episode_length': (Array(751.58594, dtype=float32), Array(430.26672, dtype=float32)), 'eval/epoch_eval_time': 4.039721727371216, 'eval/sps': 31685.350783627848}
I0726 21:37:00.737283 140267183036224 train.py:379] starting iteration 68 513.1583087444305
I0726 21:37:07.803103 140267183036224 train.py:394] {'eval/walltime': 284.84118938446045, 'training/sps': 40646.91673517441, 'training/walltime': 227.45888710021973, 'training/entropy_loss': Array(-0.03087957, dtype=float32), 'training/policy_loss': Array(0.00164759, dtype=float32), 'training/total_loss': Array(370.82187, dtype=float32), 'training/v_loss': Array(370.8511, dtype=float32), 'eval/episode_goal_distance': (Array(0.538219, dtype=float32), Array(0.2849987, dtype=float32)), 'eval/episode_reward': (Array(-11455.809, dtype=float32), Array(6214.618, dtype=float32)), 'eval/avg_episode_length': (Array(899.02344, dtype=float32), Array(300.32974, dtype=float32)), 'eval/epoch_eval_time': 4.039252519607544, 'eval/sps': 31689.031418227984}
I0726 21:37:07.805500 140267183036224 train.py:379] starting iteration 69 520.2265264987946
I0726 21:37:14.867869 140267183036224 train.py:394] {'eval/walltime': 288.8851454257965, 'training/sps': 40755.547135525085, 'training/walltime': 230.47393679618835, 'training/entropy_loss': Array(-0.03371735, dtype=float32), 'training/policy_loss': Array(0.00160115, dtype=float32), 'training/total_loss': Array(339.6554, dtype=float32), 'training/v_loss': Array(339.6875, dtype=float32), 'eval/episode_goal_distance': (Array(0.52572614, dtype=float32), Array(0.28850737, dtype=float32)), 'eval/episode_reward': (Array(-11010.313, dtype=float32), Array(6863.458, dtype=float32)), 'eval/avg_episode_length': (Array(829.1172, dtype=float32), Array(375.09457, dtype=float32)), 'eval/epoch_eval_time': 4.04395604133606, 'eval/sps': 31652.17393355018}
I0726 21:37:14.870137 140267183036224 train.py:379] starting iteration 70 527.2911636829376
I0726 21:37:21.930859 140267183036224 train.py:394] {'eval/walltime': 292.91871762275696, 'training/sps': 40639.99374230799, 'training/walltime': 233.49755930900574, 'training/entropy_loss': Array(-0.03594835, dtype=float32), 'training/policy_loss': Array(0.00088485, dtype=float32), 'training/total_loss': Array(312.06464, dtype=float32), 'training/v_loss': Array(312.09967, dtype=float32), 'eval/episode_goal_distance': (Array(0.4827816, dtype=float32), Array(0.21014324, dtype=float32)), 'eval/episode_reward': (Array(-10733.771, dtype=float32), Array(5283.9297, dtype=float32)), 'eval/avg_episode_length': (Array(906.8203, dtype=float32), Array(289.70795, dtype=float32)), 'eval/epoch_eval_time': 4.033572196960449, 'eval/sps': 31733.657847120245}
I0726 21:37:21.933325 140267183036224 train.py:379] starting iteration 71 534.3543512821198
I0726 21:37:28.991850 140267183036224 train.py:394] {'eval/walltime': 296.96082520484924, 'training/sps': 40783.31751154349, 'training/walltime': 236.51055598258972, 'training/entropy_loss': Array(-0.03498234, dtype=float32), 'training/policy_loss': Array(0.00141084, dtype=float32), 'training/total_loss': Array(297.86804, dtype=float32), 'training/v_loss': Array(297.9016, dtype=float32), 'eval/episode_goal_distance': (Array(0.49260715, dtype=float32), Array(0.2579612, dtype=float32)), 'eval/episode_reward': (Array(-10547.231, dtype=float32), Array(5970.319, dtype=float32)), 'eval/avg_episode_length': (Array(868.0078, dtype=float32), Array(337.27664, dtype=float32)), 'eval/epoch_eval_time': 4.042107582092285, 'eval/sps': 31666.648499678067}
I0726 21:37:28.994323 140267183036224 train.py:379] starting iteration 72 541.4153485298157
I0726 21:37:36.061784 140267183036224 train.py:394] {'eval/walltime': 301.00108456611633, 'training/sps': 40636.651678403476, 'training/walltime': 239.5344271659851, 'training/entropy_loss': Array(-0.03646397, dtype=float32), 'training/policy_loss': Array(0.00208198, dtype=float32), 'training/total_loss': Array(304.07187, dtype=float32), 'training/v_loss': Array(304.10626, dtype=float32), 'eval/episode_goal_distance': (Array(0.53005433, dtype=float32), Array(0.28775203, dtype=float32)), 'eval/episode_reward': (Array(-10898.4375, dtype=float32), Array(6245.491, dtype=float32)), 'eval/avg_episode_length': (Array(875.7422, dtype=float32), Array(328.75616, dtype=float32)), 'eval/epoch_eval_time': 4.04025936126709, 'eval/sps': 31681.134440799156}
I0726 21:37:36.064128 140267183036224 train.py:379] starting iteration 73 548.4851531982422
I0726 21:37:43.141081 140267183036224 train.py:394] {'eval/walltime': 305.05383133888245, 'training/sps': 40678.59063474475, 'training/walltime': 242.55518078804016, 'training/entropy_loss': Array(-0.03837651, dtype=float32), 'training/policy_loss': Array(0.00141198, dtype=float32), 'training/total_loss': Array(290.66278, dtype=float32), 'training/v_loss': Array(290.69977, dtype=float32), 'eval/episode_goal_distance': (Array(0.481374, dtype=float32), Array(0.24140923, dtype=float32)), 'eval/episode_reward': (Array(-10514.872, dtype=float32), Array(5634.4756, dtype=float32)), 'eval/avg_episode_length': (Array(883.53906, dtype=float32), Array(319.65024, dtype=float32)), 'eval/epoch_eval_time': 4.052746772766113, 'eval/sps': 31583.5178403304}
I0726 21:37:43.143446 140267183036224 train.py:379] starting iteration 74 555.5644717216492
I0726 21:37:50.214046 140267183036224 train.py:394] {'eval/walltime': 309.0923182964325, 'training/sps': 40572.205931726865, 'training/walltime': 245.58385515213013, 'training/entropy_loss': Array(-0.03873219, dtype=float32), 'training/policy_loss': Array(0.00141887, dtype=float32), 'training/total_loss': Array(283.86572, dtype=float32), 'training/v_loss': Array(283.903, dtype=float32), 'eval/episode_goal_distance': (Array(0.5037298, dtype=float32), Array(0.24039762, dtype=float32)), 'eval/episode_reward': (Array(-10966.324, dtype=float32), Array(5147.763, dtype=float32)), 'eval/avg_episode_length': (Array(914.66406, dtype=float32), Array(278.31036, dtype=float32)), 'eval/epoch_eval_time': 4.038486957550049, 'eval/sps': 31695.038598725918}
I0726 21:37:50.216500 140267183036224 train.py:379] starting iteration 75 562.6375262737274
I0726 21:37:57.273984 140267183036224 train.py:394] {'eval/walltime': 313.12976241111755, 'training/sps': 40734.94135514816, 'training/walltime': 248.60043001174927, 'training/entropy_loss': Array(-0.03993298, dtype=float32), 'training/policy_loss': Array(0.00072975, dtype=float32), 'training/total_loss': Array(277.18652, dtype=float32), 'training/v_loss': Array(277.2257, dtype=float32), 'eval/episode_goal_distance': (Array(0.48972964, dtype=float32), Array(0.24747008, dtype=float32)), 'eval/episode_reward': (Array(-10679.965, dtype=float32), Array(5283.0513, dtype=float32)), 'eval/avg_episode_length': (Array(906.8203, dtype=float32), Array(289.7082, dtype=float32)), 'eval/epoch_eval_time': 4.037444114685059, 'eval/sps': 31703.225199931876}
I0726 21:37:57.276306 140267183036224 train.py:379] starting iteration 76 569.6973321437836
I0726 21:38:04.349653 140267183036224 train.py:394] {'eval/walltime': 317.1715006828308, 'training/sps': 40579.227255087266, 'training/walltime': 251.62858033180237, 'training/entropy_loss': Array(-0.04018841, dtype=float32), 'training/policy_loss': Array(0.00082086, dtype=float32), 'training/total_loss': Array(268.52737, dtype=float32), 'training/v_loss': Array(268.56674, dtype=float32), 'eval/episode_goal_distance': (Array(0.6036047, dtype=float32), Array(0.32424828, dtype=float32)), 'eval/episode_reward': (Array(-12297.915, dtype=float32), Array(6691.948, dtype=float32)), 'eval/avg_episode_length': (Array(898.9453, dtype=float32), Array(300.56207, dtype=float32)), 'eval/epoch_eval_time': 4.041738271713257, 'eval/sps': 31669.54201261081}
I0726 21:38:04.352189 140267183036224 train.py:379] starting iteration 77 576.7732152938843
I0726 21:38:11.409165 140267183036224 train.py:394] {'eval/walltime': 321.20900774002075, 'training/sps': 40743.36217828289, 'training/walltime': 254.64453172683716, 'training/entropy_loss': Array(-0.04135423, dtype=float32), 'training/policy_loss': Array(0.00080048, dtype=float32), 'training/total_loss': Array(263.23892, dtype=float32), 'training/v_loss': Array(263.27948, dtype=float32), 'eval/episode_goal_distance': (Array(0.5408555, dtype=float32), Array(0.24732171, dtype=float32)), 'eval/episode_reward': (Array(-11833.584, dtype=float32), Array(4712.9263, dtype=float32)), 'eval/avg_episode_length': (Array(968.875, dtype=float32), Array(173.2967, dtype=float32)), 'eval/epoch_eval_time': 4.037507057189941, 'eval/sps': 31702.73096416246}
I0726 21:38:11.411616 140267183036224 train.py:379] starting iteration 78 583.8326420783997
I0726 21:38:18.482205 140267183036224 train.py:394] {'eval/walltime': 325.2468819618225, 'training/sps': 40563.87485705336, 'training/walltime': 257.673828125, 'training/entropy_loss': Array(-0.04055303, dtype=float32), 'training/policy_loss': Array(0.00074948, dtype=float32), 'training/total_loss': Array(262.29065, dtype=float32), 'training/v_loss': Array(262.33044, dtype=float32), 'eval/episode_goal_distance': (Array(0.5034877, dtype=float32), Array(0.24035992, dtype=float32)), 'eval/episode_reward': (Array(-10659.659, dtype=float32), Array(5807.163, dtype=float32)), 'eval/avg_episode_length': (Array(868.02344, dtype=float32), Array(337.23666, dtype=float32)), 'eval/epoch_eval_time': 4.037874221801758, 'eval/sps': 31699.848229270636}
I0726 21:38:18.484694 140267183036224 train.py:379] starting iteration 79 590.9057207107544
I0726 21:38:25.547100 140267183036224 train.py:394] {'eval/walltime': 329.28312134742737, 'training/sps': 40650.654849650586, 'training/walltime': 260.6966576576233, 'training/entropy_loss': Array(-0.04224596, dtype=float32), 'training/policy_loss': Array(0.00055413, dtype=float32), 'training/total_loss': Array(271.92477, dtype=float32), 'training/v_loss': Array(271.96646, dtype=float32), 'eval/episode_goal_distance': (Array(0.5318644, dtype=float32), Array(0.26618403, dtype=float32)), 'eval/episode_reward': (Array(-10823.109, dtype=float32), Array(5880.202, dtype=float32)), 'eval/avg_episode_length': (Array(875.78906, dtype=float32), Array(328.63184, dtype=float32)), 'eval/epoch_eval_time': 4.036239385604858, 'eval/sps': 31712.68791848884}
I0726 21:38:25.549492 140267183036224 train.py:379] starting iteration 80 597.9705173969269
I0726 21:38:32.617000 140267183036224 train.py:394] {'eval/walltime': 333.3209228515625, 'training/sps': 40603.27084289534, 'training/walltime': 263.72301483154297, 'training/entropy_loss': Array(-0.04395447, dtype=float32), 'training/policy_loss': Array(0.00107935, dtype=float32), 'training/total_loss': Array(277.16583, dtype=float32), 'training/v_loss': Array(277.20868, dtype=float32), 'eval/episode_goal_distance': (Array(0.4952551, dtype=float32), Array(0.25780174, dtype=float32)), 'eval/episode_reward': (Array(-10553.813, dtype=float32), Array(5773.7275, dtype=float32)), 'eval/avg_episode_length': (Array(883.53125, dtype=float32), Array(319.6714, dtype=float32)), 'eval/epoch_eval_time': 4.037801504135132, 'eval/sps': 31700.41911889789}
I0726 21:38:32.619492 140267183036224 train.py:379] starting iteration 81 605.0405180454254
I0726 21:38:39.683199 140267183036224 train.py:394] {'eval/walltime': 337.36351346969604, 'training/sps': 40719.77655622785, 'training/walltime': 266.74071311950684, 'training/entropy_loss': Array(-0.04610411, dtype=float32), 'training/policy_loss': Array(0.00065219, dtype=float32), 'training/total_loss': Array(268.6494, dtype=float32), 'training/v_loss': Array(268.6949, dtype=float32), 'eval/episode_goal_distance': (Array(0.5303589, dtype=float32), Array(0.24747393, dtype=float32)), 'eval/episode_reward': (Array(-10791.288, dtype=float32), Array(5736.006, dtype=float32)), 'eval/avg_episode_length': (Array(860.27344, dtype=float32), Array(345.41418, dtype=float32)), 'eval/epoch_eval_time': 4.042590618133545, 'eval/sps': 31662.864754556154}
I0726 21:38:39.685498 140267183036224 train.py:379] starting iteration 82 612.1065244674683
I0726 21:38:46.758253 140267183036224 train.py:394] {'eval/walltime': 341.4061415195465, 'training/sps': 40598.97217176852, 'training/walltime': 269.7673907279968, 'training/entropy_loss': Array(-0.04563034, dtype=float32), 'training/policy_loss': Array(0.00072454, dtype=float32), 'training/total_loss': Array(259.01248, dtype=float32), 'training/v_loss': Array(259.05737, dtype=float32), 'eval/episode_goal_distance': (Array(0.53792286, dtype=float32), Array(0.26657194, dtype=float32)), 'eval/episode_reward': (Array(-10903.909, dtype=float32), Array(5560.477, dtype=float32)), 'eval/avg_episode_length': (Array(906.8281, dtype=float32), Array(289.6836, dtype=float32)), 'eval/epoch_eval_time': 4.042628049850464, 'eval/sps': 31662.571580072694}
I0726 21:38:46.760647 140267183036224 train.py:379] starting iteration 83 619.1816732883453
I0726 21:38:53.817349 140267183036224 train.py:394] {'eval/walltime': 345.4421443939209, 'training/sps': 40727.019621580366, 'training/walltime': 272.78455233573914, 'training/entropy_loss': Array(-0.04572197, dtype=float32), 'training/policy_loss': Array(3.1324722e-05, dtype=float32), 'training/total_loss': Array(514.09607, dtype=float32), 'training/v_loss': Array(514.1417, dtype=float32), 'eval/episode_goal_distance': (Array(0.5313437, dtype=float32), Array(0.26540178, dtype=float32)), 'eval/episode_reward': (Array(-10971.884, dtype=float32), Array(6291.565, dtype=float32)), 'eval/avg_episode_length': (Array(875.77344, dtype=float32), Array(328.67343, dtype=float32)), 'eval/epoch_eval_time': 4.03600287437439, 'eval/sps': 31714.54629348869}
I0726 21:38:53.819797 140267183036224 train.py:379] starting iteration 84 626.240823507309
I0726 21:39:00.885594 140267183036224 train.py:394] {'eval/walltime': 349.4789445400238, 'training/sps': 40614.57198897078, 'training/walltime': 275.8100674152374, 'training/entropy_loss': Array(-0.04554885, dtype=float32), 'training/policy_loss': Array(0.00025334, dtype=float32), 'training/total_loss': Array(371.65527, dtype=float32), 'training/v_loss': Array(371.7006, dtype=float32), 'eval/episode_goal_distance': (Array(0.50855136, dtype=float32), Array(0.24707942, dtype=float32)), 'eval/episode_reward': (Array(-11345.745, dtype=float32), Array(5411.972, dtype=float32)), 'eval/avg_episode_length': (Array(937.90625, dtype=float32), Array(240.48848, dtype=float32)), 'eval/epoch_eval_time': 4.036800146102905, 'eval/sps': 31708.28264152987}
I0726 21:39:00.888087 140267183036224 train.py:379] starting iteration 85 633.3091139793396
I0726 21:39:07.949854 140267183036224 train.py:394] {'eval/walltime': 353.5197732448578, 'training/sps': 40723.71149598674, 'training/walltime': 278.82747411727905, 'training/entropy_loss': Array(-0.04374527, dtype=float32), 'training/policy_loss': Array(0.00054543, dtype=float32), 'training/total_loss': Array(301.11584, dtype=float32), 'training/v_loss': Array(301.15906, dtype=float32), 'eval/episode_goal_distance': (Array(0.51851344, dtype=float32), Array(0.25625333, dtype=float32)), 'eval/episode_reward': (Array(-11073.517, dtype=float32), Array(5497.482, dtype=float32)), 'eval/avg_episode_length': (Array(914.6328, dtype=float32), Array(278.41232, dtype=float32)), 'eval/epoch_eval_time': 4.040828704833984, 'eval/sps': 31676.67064106812}
I0726 21:39:07.952395 140267183036224 train.py:379] starting iteration 86 640.3734211921692
I0726 21:39:15.021289 140267183036224 train.py:394] {'eval/walltime': 357.56075644493103, 'training/sps': 40629.67132600207, 'training/walltime': 281.8518648147583, 'training/entropy_loss': Array(-0.04483266, dtype=float32), 'training/policy_loss': Array(0.00040392, dtype=float32), 'training/total_loss': Array(285.64328, dtype=float32), 'training/v_loss': Array(285.68774, dtype=float32), 'eval/episode_goal_distance': (Array(0.5212233, dtype=float32), Array(0.26949388, dtype=float32)), 'eval/episode_reward': (Array(-11209.065, dtype=float32), Array(5465.795, dtype=float32)), 'eval/avg_episode_length': (Array(914.6328, dtype=float32), Array(278.41226, dtype=float32)), 'eval/epoch_eval_time': 4.040983200073242, 'eval/sps': 31675.459575699304}
I0726 21:39:15.023805 140267183036224 train.py:379] starting iteration 87 647.4448311328888
I0726 21:39:22.084796 140267183036224 train.py:394] {'eval/walltime': 361.6011805534363, 'training/sps': 40728.213639361486, 'training/walltime': 284.86893796920776, 'training/entropy_loss': Array(-0.04566661, dtype=float32), 'training/policy_loss': Array(0.00015491, dtype=float32), 'training/total_loss': Array(259.4873, dtype=float32), 'training/v_loss': Array(259.5328, dtype=float32), 'eval/episode_goal_distance': (Array(0.5585959, dtype=float32), Array(0.2696142, dtype=float32)), 'eval/episode_reward': (Array(-11749.894, dtype=float32), Array(5653.7036, dtype=float32)), 'eval/avg_episode_length': (Array(906.8203, dtype=float32), Array(289.70847, dtype=float32)), 'eval/epoch_eval_time': 4.040424108505249, 'eval/sps': 31679.842650813574}
I0726 21:39:22.087225 140267183036224 train.py:379] starting iteration 88 654.5082511901855
I0726 21:39:29.159113 140267183036224 train.py:394] {'eval/walltime': 365.64330649375916, 'training/sps': 40604.57597675075, 'training/walltime': 287.89519786834717, 'training/entropy_loss': Array(-0.04690757, dtype=float32), 'training/policy_loss': Array(0.00064759, dtype=float32), 'training/total_loss': Array(252.3212, dtype=float32), 'training/v_loss': Array(252.36746, dtype=float32), 'eval/episode_goal_distance': (Array(0.5190795, dtype=float32), Array(0.26102936, dtype=float32)), 'eval/episode_reward': (Array(-11068.858, dtype=float32), Array(5379.7466, dtype=float32)), 'eval/avg_episode_length': (Array(914.60156, dtype=float32), Array(278.51407, dtype=float32)), 'eval/epoch_eval_time': 4.042125940322876, 'eval/sps': 31666.504678420693}
I0726 21:39:29.161346 140267183036224 train.py:379] starting iteration 89 661.5823721885681
I0726 21:39:36.219824 140267183036224 train.py:394] {'eval/walltime': 369.67842841148376, 'training/sps': 40689.850230368356, 'training/walltime': 290.9151155948639, 'training/entropy_loss': Array(-0.04629772, dtype=float32), 'training/policy_loss': Array(0.00034287, dtype=float32), 'training/total_loss': Array(254.73691, dtype=float32), 'training/v_loss': Array(254.78287, dtype=float32), 'eval/episode_goal_distance': (Array(0.5198376, dtype=float32), Array(0.26375276, dtype=float32)), 'eval/episode_reward': (Array(-11334.184, dtype=float32), Array(5521.5215, dtype=float32)), 'eval/avg_episode_length': (Array(922.35156, dtype=float32), Array(266.73172, dtype=float32)), 'eval/epoch_eval_time': 4.035121917724609, 'eval/sps': 31721.470282657217}
I0726 21:39:36.221920 140267183036224 train.py:379] starting iteration 90 668.6429462432861
I0726 21:39:43.292315 140267183036224 train.py:394] {'eval/walltime': 373.72188091278076, 'training/sps': 40641.480704848815, 'training/walltime': 293.93862748146057, 'training/entropy_loss': Array(-0.04748425, dtype=float32), 'training/policy_loss': Array(0.00026105, dtype=float32), 'training/total_loss': Array(251.5523, dtype=float32), 'training/v_loss': Array(251.59953, dtype=float32), 'eval/episode_goal_distance': (Array(0.50256276, dtype=float32), Array(0.23116831, dtype=float32)), 'eval/episode_reward': (Array(-10716.85, dtype=float32), Array(5234.8647, dtype=float32)), 'eval/avg_episode_length': (Array(906.7422, dtype=float32), Array(289.95074, dtype=float32)), 'eval/epoch_eval_time': 4.043452501296997, 'eval/sps': 31656.115648432155}
I0726 21:39:43.294469 140267183036224 train.py:379] starting iteration 91 675.715494632721
I0726 21:39:50.358013 140267183036224 train.py:394] {'eval/walltime': 377.7621338367462, 'training/sps': 40690.21644849392, 'training/walltime': 296.9585180282593, 'training/entropy_loss': Array(-0.04637543, dtype=float32), 'training/policy_loss': Array(0.00043556, dtype=float32), 'training/total_loss': Array(243.62643, dtype=float32), 'training/v_loss': Array(243.67236, dtype=float32), 'eval/episode_goal_distance': (Array(0.5088004, dtype=float32), Array(0.268187, dtype=float32)), 'eval/episode_reward': (Array(-10613.402, dtype=float32), Array(5725.5454, dtype=float32)), 'eval/avg_episode_length': (Array(898.97656, dtype=float32), Array(300.46915, dtype=float32)), 'eval/epoch_eval_time': 4.040252923965454, 'eval/sps': 31681.184918089166}
I0726 21:39:50.360361 140267183036224 train.py:379] starting iteration 92 682.7813866138458
I0726 21:39:57.434513 140267183036224 train.py:394] {'eval/walltime': 381.80406641960144, 'training/sps': 40575.67156790501, 'training/walltime': 299.9869337081909, 'training/entropy_loss': Array(-0.0466641, dtype=float32), 'training/policy_loss': Array(0.00060501, dtype=float32), 'training/total_loss': Array(242.56752, dtype=float32), 'training/v_loss': Array(242.61357, dtype=float32), 'eval/episode_goal_distance': (Array(0.54481673, dtype=float32), Array(0.24784626, dtype=float32)), 'eval/episode_reward': (Array(-11707.268, dtype=float32), Array(5363.604, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.08266, dtype=float32)), 'eval/epoch_eval_time': 4.041932582855225, 'eval/sps': 31668.01953672881}
I0726 21:39:57.436981 140267183036224 train.py:379] starting iteration 93 689.8580079078674
I0726 21:40:04.500868 140267183036224 train.py:394] {'eval/walltime': 385.8388464450836, 'training/sps': 40621.51513730437, 'training/walltime': 303.01193165779114, 'training/entropy_loss': Array(-0.04623372, dtype=float32), 'training/policy_loss': Array(0.00046616, dtype=float32), 'training/total_loss': Array(249.93002, dtype=float32), 'training/v_loss': Array(249.9758, dtype=float32), 'eval/episode_goal_distance': (Array(0.5007827, dtype=float32), Array(0.23318915, dtype=float32)), 'eval/episode_reward': (Array(-10902.209, dtype=float32), Array(5194.465, dtype=float32)), 'eval/avg_episode_length': (Array(914.6797, dtype=float32), Array(278.25977, dtype=float32)), 'eval/epoch_eval_time': 4.034780025482178, 'eval/sps': 31724.158241985773}
I0726 21:40:04.503377 140267183036224 train.py:379] starting iteration 94 696.9244034290314
I0726 21:40:11.580200 140267183036224 train.py:394] {'eval/walltime': 389.88214135169983, 'training/sps': 40555.91419895873, 'training/walltime': 306.04182267189026, 'training/entropy_loss': Array(-0.04622059, dtype=float32), 'training/policy_loss': Array(2.5635447e-05, dtype=float32), 'training/total_loss': Array(239.67596, dtype=float32), 'training/v_loss': Array(239.72217, dtype=float32), 'eval/episode_goal_distance': (Array(0.5171506, dtype=float32), Array(0.26371935, dtype=float32)), 'eval/episode_reward': (Array(-11218.338, dtype=float32), Array(5357.7236, dtype=float32)), 'eval/avg_episode_length': (Array(922.3125, dtype=float32), Array(266.866, dtype=float32)), 'eval/epoch_eval_time': 4.043294906616211, 'eval/sps': 31657.34950239427}
I0726 21:40:11.582546 140267183036224 train.py:379] starting iteration 95 704.0035707950592
I0726 21:40:18.655099 140267183036224 train.py:394] {'eval/walltime': 393.9216160774231, 'training/sps': 40559.76007704052, 'training/walltime': 309.07142639160156, 'training/entropy_loss': Array(-0.04710923, dtype=float32), 'training/policy_loss': Array(0.00033746, dtype=float32), 'training/total_loss': Array(241.6534, dtype=float32), 'training/v_loss': Array(241.70016, dtype=float32), 'eval/episode_goal_distance': (Array(0.5063305, dtype=float32), Array(0.23547171, dtype=float32)), 'eval/episode_reward': (Array(-10862.199, dtype=float32), Array(5475.645, dtype=float32)), 'eval/avg_episode_length': (Array(883.52344, dtype=float32), Array(319.69287, dtype=float32)), 'eval/epoch_eval_time': 4.039474725723267, 'eval/sps': 31687.28824688503}
I0726 21:40:18.657515 140267183036224 train.py:379] starting iteration 96 711.0785410404205
I0726 21:40:25.724947 140267183036224 train.py:394] {'eval/walltime': 397.96295642852783, 'training/sps': 40653.31619055492, 'training/walltime': 312.0940580368042, 'training/entropy_loss': Array(-0.04709135, dtype=float32), 'training/policy_loss': Array(0.00027662, dtype=float32), 'training/total_loss': Array(240.03087, dtype=float32), 'training/v_loss': Array(240.0777, dtype=float32), 'eval/episode_goal_distance': (Array(0.54435325, dtype=float32), Array(0.29455197, dtype=float32)), 'eval/episode_reward': (Array(-11093.088, dtype=float32), Array(5918.9893, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.30652, dtype=float32)), 'eval/epoch_eval_time': 4.041340351104736, 'eval/sps': 31672.66027594287}
I0726 21:40:25.727251 140267183036224 train.py:379] starting iteration 97 718.1482770442963
I0726 21:40:32.804883 140267183036224 train.py:394] {'eval/walltime': 402.00762128829956, 'training/sps': 40559.36428496499, 'training/walltime': 315.1236913204193, 'training/entropy_loss': Array(-0.04768313, dtype=float32), 'training/policy_loss': Array(0.00028868, dtype=float32), 'training/total_loss': Array(253.59798, dtype=float32), 'training/v_loss': Array(253.64536, dtype=float32), 'eval/episode_goal_distance': (Array(0.5103754, dtype=float32), Array(0.22399694, dtype=float32)), 'eval/episode_reward': (Array(-11092.773, dtype=float32), Array(4823.4507, dtype=float32)), 'eval/avg_episode_length': (Array(937.9297, dtype=float32), Array(240.39796, dtype=float32)), 'eval/epoch_eval_time': 4.0446648597717285, 'eval/sps': 31646.626961133195}
I0726 21:40:32.807032 140267183036224 train.py:379] starting iteration 98 725.2280578613281
I0726 21:40:39.874642 140267183036224 train.py:394] {'eval/walltime': 406.05451107025146, 'training/sps': 40725.931871176435, 'training/walltime': 318.14093351364136, 'training/entropy_loss': Array(-0.04769104, dtype=float32), 'training/policy_loss': Array(0.00044551, dtype=float32), 'training/total_loss': Array(256.01907, dtype=float32), 'training/v_loss': Array(256.0663, dtype=float32), 'eval/episode_goal_distance': (Array(0.512755, dtype=float32), Array(0.27166313, dtype=float32)), 'eval/episode_reward': (Array(-11442.828, dtype=float32), Array(5689.8623, dtype=float32)), 'eval/avg_episode_length': (Array(914.5625, dtype=float32), Array(278.6416, dtype=float32)), 'eval/epoch_eval_time': 4.046889781951904, 'eval/sps': 31629.228097796815}
I0726 21:40:39.876971 140267183036224 train.py:379] starting iteration 99 732.2979969978333
I0726 21:40:46.952152 140267183036224 train.py:394] {'eval/walltime': 410.0978453159332, 'training/sps': 40576.5979672497, 'training/walltime': 321.16928005218506, 'training/entropy_loss': Array(-0.04616173, dtype=float32), 'training/policy_loss': Array(0.00042791, dtype=float32), 'training/total_loss': Array(245.51164, dtype=float32), 'training/v_loss': Array(245.55737, dtype=float32), 'eval/episode_goal_distance': (Array(0.4963762, dtype=float32), Array(0.23265009, dtype=float32)), 'eval/episode_reward': (Array(-10910.494, dtype=float32), Array(4843.7754, dtype=float32)), 'eval/avg_episode_length': (Array(937.89844, dtype=float32), Array(240.51874, dtype=float32)), 'eval/epoch_eval_time': 4.043334245681763, 'eval/sps': 31657.041496557606}
I0726 21:40:46.954389 140267183036224 train.py:379] starting iteration 100 739.3754141330719
I0726 21:40:54.024898 140267183036224 train.py:394] {'eval/walltime': 414.1496272087097, 'training/sps': 40751.8702609974, 'training/walltime': 324.18460178375244, 'training/entropy_loss': Array(-0.04756251, dtype=float32), 'training/policy_loss': Array(-0.00028227, dtype=float32), 'training/total_loss': Array(494.57318, dtype=float32), 'training/v_loss': Array(494.62103, dtype=float32), 'eval/episode_goal_distance': (Array(0.5065715, dtype=float32), Array(0.23199911, dtype=float32)), 'eval/episode_reward': (Array(-10944.588, dtype=float32), Array(5547.0625, dtype=float32)), 'eval/avg_episode_length': (Array(899.1172, dtype=float32), Array(300.0515, dtype=float32)), 'eval/epoch_eval_time': 4.051781892776489, 'eval/sps': 31591.039050793483}
I0726 21:40:54.027066 140267183036224 train.py:379] starting iteration 101 746.4480910301208
I0726 21:41:01.100791 140267183036224 train.py:394] {'eval/walltime': 418.1941285133362, 'training/sps': 40612.146127932545, 'training/walltime': 327.2102975845337, 'training/entropy_loss': Array(-0.04694016, dtype=float32), 'training/policy_loss': Array(0.00013453, dtype=float32), 'training/total_loss': Array(309.58942, dtype=float32), 'training/v_loss': Array(309.63623, dtype=float32), 'eval/episode_goal_distance': (Array(0.5066918, dtype=float32), Array(0.22357994, dtype=float32)), 'eval/episode_reward': (Array(-10384.951, dtype=float32), Array(5073.456, dtype=float32)), 'eval/avg_episode_length': (Array(883.53906, dtype=float32), Array(319.65002, dtype=float32)), 'eval/epoch_eval_time': 4.044501304626465, 'eval/sps': 31647.906715614623}
I0726 21:41:01.103120 140267183036224 train.py:379] starting iteration 102 753.5241456031799
I0726 21:41:08.152147 140267183036224 train.py:394] {'eval/walltime': 422.2255582809448, 'training/sps': 40767.93286734351, 'training/walltime': 330.2244312763214, 'training/entropy_loss': Array(-0.04555255, dtype=float32), 'training/policy_loss': Array(3.6694073e-05, dtype=float32), 'training/total_loss': Array(260.511, dtype=float32), 'training/v_loss': Array(260.5565, dtype=float32), 'eval/episode_goal_distance': (Array(0.5398233, dtype=float32), Array(0.24264847, dtype=float32)), 'eval/episode_reward': (Array(-11719.814, dtype=float32), Array(4912.0024, dtype=float32)), 'eval/avg_episode_length': (Array(937.9219, dtype=float32), Array(240.4278, dtype=float32)), 'eval/epoch_eval_time': 4.031429767608643, 'eval/sps': 31750.52211710161}
I0726 21:41:08.214684 140267183036224 train.py:379] starting iteration 103 760.6357080936432
I0726 21:41:15.290912 140267183036224 train.py:394] {'eval/walltime': 426.2724175453186, 'training/sps': 40610.48531632964, 'training/walltime': 333.2502508163452, 'training/entropy_loss': Array(-0.04435556, dtype=float32), 'training/policy_loss': Array(2.9125134e-05, dtype=float32), 'training/total_loss': Array(244.91443, dtype=float32), 'training/v_loss': Array(244.95874, dtype=float32), 'eval/episode_goal_distance': (Array(0.49386707, dtype=float32), Array(0.23442253, dtype=float32)), 'eval/episode_reward': (Array(-10671.895, dtype=float32), Array(4874.346, dtype=float32)), 'eval/avg_episode_length': (Array(937.7969, dtype=float32), Array(240.91205, dtype=float32)), 'eval/epoch_eval_time': 4.046859264373779, 'eval/sps': 31629.466615466063}
I0726 21:41:15.293228 140267183036224 train.py:379] starting iteration 104 767.7142536640167
I0726 21:41:22.362390 140267183036224 train.py:394] {'eval/walltime': 430.31097626686096, 'training/sps': 40590.46387125873, 'training/walltime': 336.2775628566742, 'training/entropy_loss': Array(-0.04514879, dtype=float32), 'training/policy_loss': Array(5.8531867e-05, dtype=float32), 'training/total_loss': Array(234.28954, dtype=float32), 'training/v_loss': Array(234.33463, dtype=float32), 'eval/episode_goal_distance': (Array(0.5004326, dtype=float32), Array(0.2486534, dtype=float32)), 'eval/episode_reward': (Array(-10541.031, dtype=float32), Array(5320.5356, dtype=float32)), 'eval/avg_episode_length': (Array(906.84375, dtype=float32), Array(289.63486, dtype=float32)), 'eval/epoch_eval_time': 4.038558721542358, 'eval/sps': 31694.475387277707}
I0726 21:41:22.364481 140267183036224 train.py:379] starting iteration 105 774.7855069637299
I0726 21:41:29.445833 140267183036224 train.py:394] {'eval/walltime': 434.36531019210815, 'training/sps': 40665.412045780104, 'training/walltime': 339.29929542541504, 'training/entropy_loss': Array(-0.04539025, dtype=float32), 'training/policy_loss': Array(0.00049527, dtype=float32), 'training/total_loss': Array(240.53754, dtype=float32), 'training/v_loss': Array(240.58243, dtype=float32), 'eval/episode_goal_distance': (Array(0.52837986, dtype=float32), Array(0.25268292, dtype=float32)), 'eval/episode_reward': (Array(-11684.277, dtype=float32), Array(5424.881, dtype=float32)), 'eval/avg_episode_length': (Array(930.0547, dtype=float32), Array(254.338, dtype=float32)), 'eval/epoch_eval_time': 4.054333925247192, 'eval/sps': 31571.153822066062}
I0726 21:41:29.448004 140267183036224 train.py:379] starting iteration 106 781.8690297603607
I0726 21:41:36.513814 140267183036224 train.py:394] {'eval/walltime': 438.4022755622864, 'training/sps': 40615.420148601246, 'training/walltime': 342.32474732398987, 'training/entropy_loss': Array(-0.04479648, dtype=float32), 'training/policy_loss': Array(0.00012948, dtype=float32), 'training/total_loss': Array(233.99762, dtype=float32), 'training/v_loss': Array(234.0423, dtype=float32), 'eval/episode_goal_distance': (Array(0.47124714, dtype=float32), Array(0.23049194, dtype=float32)), 'eval/episode_reward': (Array(-10353.736, dtype=float32), Array(4975.366, dtype=float32)), 'eval/avg_episode_length': (Array(906.8594, dtype=float32), Array(289.58624, dtype=float32)), 'eval/epoch_eval_time': 4.036965370178223, 'eval/sps': 31706.984891562024}
I0726 21:41:36.516140 140267183036224 train.py:379] starting iteration 107 788.9371671676636
I0726 21:41:43.591311 140267183036224 train.py:394] {'eval/walltime': 442.45693373680115, 'training/sps': 40728.36490806612, 'training/walltime': 345.3418092727661, 'training/entropy_loss': Array(-0.04464216, dtype=float32), 'training/policy_loss': Array(0.00024239, dtype=float32), 'training/total_loss': Array(234.14423, dtype=float32), 'training/v_loss': Array(234.18861, dtype=float32), 'eval/episode_goal_distance': (Array(0.49337375, dtype=float32), Array(0.19946066, dtype=float32)), 'eval/episode_reward': (Array(-10762.338, dtype=float32), Array(4542.503, dtype=float32)), 'eval/avg_episode_length': (Array(945.6328, dtype=float32), Array(226.0382, dtype=float32)), 'eval/epoch_eval_time': 4.0546581745147705, 'eval/sps': 31568.629090494916}
I0726 21:41:43.593602 140267183036224 train.py:379] starting iteration 108 796.0146276950836
I0726 21:41:50.661897 140267183036224 train.py:394] {'eval/walltime': 446.4954106807709, 'training/sps': 40600.63843847842, 'training/walltime': 348.3683626651764, 'training/entropy_loss': Array(-0.04472781, dtype=float32), 'training/policy_loss': Array(0.00021942, dtype=float32), 'training/total_loss': Array(227.52075, dtype=float32), 'training/v_loss': Array(227.56525, dtype=float32), 'eval/episode_goal_distance': (Array(0.48836476, dtype=float32), Array(0.23569931, dtype=float32)), 'eval/episode_reward': (Array(-10519.079, dtype=float32), Array(4978.1875, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.0825, dtype=float32)), 'eval/epoch_eval_time': 4.038476943969727, 'eval/sps': 31695.117187961223}
I0726 21:41:50.664108 140267183036224 train.py:379] starting iteration 109 803.085134267807
I0726 21:41:57.720556 140267183036224 train.py:394] {'eval/walltime': 450.53135347366333, 'training/sps': 40728.41640405173, 'training/walltime': 351.38542079925537, 'training/entropy_loss': Array(-0.04416678, dtype=float32), 'training/policy_loss': Array(0.00036722, dtype=float32), 'training/total_loss': Array(234.61258, dtype=float32), 'training/v_loss': Array(234.65637, dtype=float32), 'eval/episode_goal_distance': (Array(0.49470666, dtype=float32), Array(0.23028801, dtype=float32)), 'eval/episode_reward': (Array(-10945.811, dtype=float32), Array(4227.4043, dtype=float32)), 'eval/avg_episode_length': (Array(968.9375, dtype=float32), Array(172.94873, dtype=float32)), 'eval/epoch_eval_time': 4.035942792892456, 'eval/sps': 31715.0184153789}
I0726 21:41:57.722951 140267183036224 train.py:379] starting iteration 110 810.1439771652222
I0726 21:42:04.794163 140267183036224 train.py:394] {'eval/walltime': 454.56990909576416, 'training/sps': 40565.51908715081, 'training/walltime': 354.41459441185, 'training/entropy_loss': Array(-0.04501547, dtype=float32), 'training/policy_loss': Array(5.9577127e-05, dtype=float32), 'training/total_loss': Array(225.88629, dtype=float32), 'training/v_loss': Array(225.93124, dtype=float32), 'eval/episode_goal_distance': (Array(0.48025173, dtype=float32), Array(0.23300533, dtype=float32)), 'eval/episode_reward': (Array(-10032.3, dtype=float32), Array(4701.2876, dtype=float32)), 'eval/avg_episode_length': (Array(914.6406, dtype=float32), Array(278.38663, dtype=float32)), 'eval/epoch_eval_time': 4.03855562210083, 'eval/sps': 31694.499711611064}
I0726 21:42:04.796445 140267183036224 train.py:379] starting iteration 111 817.2174711227417
I0726 21:42:11.864528 140267183036224 train.py:394] {'eval/walltime': 458.6079032421112, 'training/sps': 40600.01477177482, 'training/walltime': 357.4411942958832, 'training/entropy_loss': Array(-0.04545264, dtype=float32), 'training/policy_loss': Array(-5.7461948e-05, dtype=float32), 'training/total_loss': Array(228.85927, dtype=float32), 'training/v_loss': Array(228.90475, dtype=float32), 'eval/episode_goal_distance': (Array(0.48074937, dtype=float32), Array(0.22219299, dtype=float32)), 'eval/episode_reward': (Array(-10751.185, dtype=float32), Array(4766.0723, dtype=float32)), 'eval/avg_episode_length': (Array(930.0625, dtype=float32), Array(254.30974, dtype=float32)), 'eval/epoch_eval_time': 4.037994146347046, 'eval/sps': 31698.906774244497}
I0726 21:42:11.866768 140267183036224 train.py:379] starting iteration 112 824.2877938747406
I0726 21:42:18.929879 140267183036224 train.py:394] {'eval/walltime': 462.64581871032715, 'training/sps': 40664.46233620772, 'training/walltime': 360.46299743652344, 'training/entropy_loss': Array(-0.04624216, dtype=float32), 'training/policy_loss': Array(0.00063593, dtype=float32), 'training/total_loss': Array(223.40231, dtype=float32), 'training/v_loss': Array(223.4479, dtype=float32), 'eval/episode_goal_distance': (Array(0.48518398, dtype=float32), Array(0.23190975, dtype=float32)), 'eval/episode_reward': (Array(-10576.47, dtype=float32), Array(5144.3384, dtype=float32)), 'eval/avg_episode_length': (Array(906.7422, dtype=float32), Array(289.95065, dtype=float32)), 'eval/epoch_eval_time': 4.037915468215942, 'eval/sps': 31699.5244223262}
I0726 21:42:18.932180 140267183036224 train.py:379] starting iteration 113 831.3532061576843
I0726 21:42:25.994091 140267183036224 train.py:394] {'eval/walltime': 466.67989206314087, 'training/sps': 40629.31900832805, 'training/walltime': 363.4874143600464, 'training/entropy_loss': Array(-0.04590358, dtype=float32), 'training/policy_loss': Array(0.00050401, dtype=float32), 'training/total_loss': Array(217.00943, dtype=float32), 'training/v_loss': Array(217.05484, dtype=float32), 'eval/episode_goal_distance': (Array(0.49271, dtype=float32), Array(0.22560462, dtype=float32)), 'eval/episode_reward': (Array(-10198.23, dtype=float32), Array(5487.973, dtype=float32)), 'eval/avg_episode_length': (Array(867.9844, dtype=float32), Array(337.3368, dtype=float32)), 'eval/epoch_eval_time': 4.034073352813721, 'eval/sps': 31729.71555183087}
I0726 21:42:25.996371 140267183036224 train.py:379] starting iteration 114 838.4173972606659
I0726 21:42:33.069837 140267183036224 train.py:394] {'eval/walltime': 470.7221233844757, 'training/sps': 40582.94653805696, 'training/walltime': 366.5152871608734, 'training/entropy_loss': Array(-0.04641006, dtype=float32), 'training/policy_loss': Array(0.00057188, dtype=float32), 'training/total_loss': Array(214.32639, dtype=float32), 'training/v_loss': Array(214.37224, dtype=float32), 'eval/episode_goal_distance': (Array(0.5057236, dtype=float32), Array(0.23117909, dtype=float32)), 'eval/episode_reward': (Array(-10058.518, dtype=float32), Array(5062.564, dtype=float32)), 'eval/avg_episode_length': (Array(875.78125, dtype=float32), Array(328.65268, dtype=float32)), 'eval/epoch_eval_time': 4.042231321334839, 'eval/sps': 31665.67913231928}
I0726 21:42:33.072035 140267183036224 train.py:379] starting iteration 115 845.4930613040924
I0726 21:42:40.135282 140267183036224 train.py:394] {'eval/walltime': 474.7621810436249, 'training/sps': 40692.458875156815, 'training/walltime': 369.5350112915039, 'training/entropy_loss': Array(-0.04654459, dtype=float32), 'training/policy_loss': Array(0.0021768, dtype=float32), 'training/total_loss': Array(218.45613, dtype=float32), 'training/v_loss': Array(218.5005, dtype=float32), 'eval/episode_goal_distance': (Array(0.45673466, dtype=float32), Array(0.20542446, dtype=float32)), 'eval/episode_reward': (Array(-9739.752, dtype=float32), Array(5035.7603, dtype=float32)), 'eval/avg_episode_length': (Array(883.53906, dtype=float32), Array(319.65, dtype=float32)), 'eval/epoch_eval_time': 4.04005765914917, 'eval/sps': 31682.71613899605}
I0726 21:42:40.137419 140267183036224 train.py:379] starting iteration 116 852.5584452152252
I0726 21:42:47.204069 140267183036224 train.py:394] {'eval/walltime': 478.7976288795471, 'training/sps': 40584.250365627195, 'training/walltime': 372.56278681755066, 'training/entropy_loss': Array(-0.04577184, dtype=float32), 'training/policy_loss': Array(0.00397076, dtype=float32), 'training/total_loss': Array(443.72937, dtype=float32), 'training/v_loss': Array(443.77118, dtype=float32), 'eval/episode_goal_distance': (Array(0.45758986, dtype=float32), Array(0.22974497, dtype=float32)), 'eval/episode_reward': (Array(-9101.879, dtype=float32), Array(5255.1196, dtype=float32)), 'eval/avg_episode_length': (Array(844.64844, dtype=float32), Array(361.00494, dtype=float32)), 'eval/epoch_eval_time': 4.035447835922241, 'eval/sps': 31718.908335423326}
I0726 21:42:47.206385 140267183036224 train.py:379] starting iteration 117 859.6274104118347
I0726 21:42:54.269669 140267183036224 train.py:394] {'eval/walltime': 482.83560729026794, 'training/sps': 40663.618543634904, 'training/walltime': 375.5846526622772, 'training/entropy_loss': Array(-0.0449625, dtype=float32), 'training/policy_loss': Array(0.00098304, dtype=float32), 'training/total_loss': Array(378.6242, dtype=float32), 'training/v_loss': Array(378.6682, dtype=float32), 'eval/episode_goal_distance': (Array(0.47926313, dtype=float32), Array(0.2456311, dtype=float32)), 'eval/episode_reward': (Array(-9863.914, dtype=float32), Array(5498.579, dtype=float32)), 'eval/avg_episode_length': (Array(883.53125, dtype=float32), Array(319.67105, dtype=float32)), 'eval/epoch_eval_time': 4.037978410720825, 'eval/sps': 31699.030301935305}
I0726 21:42:54.271946 140267183036224 train.py:379] starting iteration 118 866.6929724216461
I0726 21:43:01.333471 140267183036224 train.py:394] {'eval/walltime': 486.87007451057434, 'training/sps': 40639.202235504315, 'training/walltime': 378.60833406448364, 'training/entropy_loss': Array(-0.04144213, dtype=float32), 'training/policy_loss': Array(0.00120595, dtype=float32), 'training/total_loss': Array(258.64093, dtype=float32), 'training/v_loss': Array(258.68118, dtype=float32), 'eval/episode_goal_distance': (Array(0.42904532, dtype=float32), Array(0.1780729, dtype=float32)), 'eval/episode_reward': (Array(-8870.057, dtype=float32), Array(4786.714, dtype=float32)), 'eval/avg_episode_length': (Array(860.21094, dtype=float32), Array(345.56854, dtype=float32)), 'eval/epoch_eval_time': 4.0344672203063965, 'eval/sps': 31726.617917663756}
I0726 21:43:01.335614 140267183036224 train.py:379] starting iteration 119 873.7566406726837
I0726 21:43:08.400786 140267183036224 train.py:394] {'eval/walltime': 490.9146296977997, 'training/sps': 40724.931063402895, 'training/walltime': 381.6256504058838, 'training/entropy_loss': Array(-0.04108931, dtype=float32), 'training/policy_loss': Array(0.00142974, dtype=float32), 'training/total_loss': Array(214.31894, dtype=float32), 'training/v_loss': Array(214.35861, dtype=float32), 'eval/episode_goal_distance': (Array(0.41949052, dtype=float32), Array(0.16304666, dtype=float32)), 'eval/episode_reward': (Array(-8489.914, dtype=float32), Array(5298.868, dtype=float32)), 'eval/avg_episode_length': (Array(798.0078, dtype=float32), Array(400.08182, dtype=float32)), 'eval/epoch_eval_time': 4.044555187225342, 'eval/sps': 31647.485094105232}
I0726 21:43:08.402890 140267183036224 train.py:379] starting iteration 120 880.8239159584045
I0726 21:43:15.475810 140267183036224 train.py:394] {'eval/walltime': 494.95889353752136, 'training/sps': 40618.605064202944, 'training/walltime': 384.6508650779724, 'training/entropy_loss': Array(-0.04056332, dtype=float32), 'training/policy_loss': Array(0.00105381, dtype=float32), 'training/total_loss': Array(208.80414, dtype=float32), 'training/v_loss': Array(208.84364, dtype=float32), 'eval/episode_goal_distance': (Array(0.42177266, dtype=float32), Array(0.1743927, dtype=float32)), 'eval/episode_reward': (Array(-8415.251, dtype=float32), Array(4795.317, dtype=float32)), 'eval/avg_episode_length': (Array(829.125, dtype=float32), Array(375.07733, dtype=float32)), 'eval/epoch_eval_time': 4.04426383972168, 'eval/sps': 31649.764969045336}
I0726 21:43:15.478101 140267183036224 train.py:379] starting iteration 121 887.8991267681122
I0726 21:43:22.550229 140267183036224 train.py:394] {'eval/walltime': 499.0004496574402, 'training/sps': 40592.85836859573, 'training/walltime': 387.67799854278564, 'training/entropy_loss': Array(-0.04056501, dtype=float32), 'training/policy_loss': Array(0.00086926, dtype=float32), 'training/total_loss': Array(192.4975, dtype=float32), 'training/v_loss': Array(192.53719, dtype=float32), 'eval/episode_goal_distance': (Array(0.40543965, dtype=float32), Array(0.17572768, dtype=float32)), 'eval/episode_reward': (Array(-7787.618, dtype=float32), Array(5246.498, dtype=float32)), 'eval/avg_episode_length': (Array(766.89844, dtype=float32), Array(421.30676, dtype=float32)), 'eval/epoch_eval_time': 4.041556119918823, 'eval/sps': 31670.96934993716}
I0726 21:43:22.552393 140267183036224 train.py:379] starting iteration 122 894.9734199047089
I0726 21:43:29.630811 140267183036224 train.py:394] {'eval/walltime': 503.04746437072754, 'training/sps': 40583.3044431655, 'training/walltime': 390.7058446407318, 'training/entropy_loss': Array(-0.0406722, dtype=float32), 'training/policy_loss': Array(0.00099269, dtype=float32), 'training/total_loss': Array(192.85121, dtype=float32), 'training/v_loss': Array(192.89088, dtype=float32), 'eval/episode_goal_distance': (Array(0.43817568, dtype=float32), Array(0.18669899, dtype=float32)), 'eval/episode_reward': (Array(-8284.3545, dtype=float32), Array(5377.2314, dtype=float32)), 'eval/avg_episode_length': (Array(790.1875, dtype=float32), Array(405.79883, dtype=float32)), 'eval/epoch_eval_time': 4.0470147132873535, 'eval/sps': 31628.25170359382}
I0726 21:43:29.633356 140267183036224 train.py:379] starting iteration 123 902.0543820858002
I0726 21:43:36.711115 140267183036224 train.py:394] {'eval/walltime': 507.0905408859253, 'training/sps': 40538.31545105984, 'training/walltime': 393.73705101013184, 'training/entropy_loss': Array(-0.03899026, dtype=float32), 'training/policy_loss': Array(0.00106194, dtype=float32), 'training/total_loss': Array(194.21304, dtype=float32), 'training/v_loss': Array(194.25098, dtype=float32), 'eval/episode_goal_distance': (Array(0.40002942, dtype=float32), Array(0.1655788, dtype=float32)), 'eval/episode_reward': (Array(-7291.3867, dtype=float32), Array(5353.181, dtype=float32)), 'eval/avg_episode_length': (Array(712.46875, dtype=float32), Array(450.92664, dtype=float32)), 'eval/epoch_eval_time': 4.043076515197754, 'eval/sps': 31659.05951046274}
I0726 21:43:36.713256 140267183036224 train.py:379] starting iteration 124 909.134281873703
I0726 21:43:43.784495 140267183036224 train.py:394] {'eval/walltime': 511.1352219581604, 'training/sps': 40647.60595844496, 'training/walltime': 396.76010727882385, 'training/entropy_loss': Array(-0.03724669, dtype=float32), 'training/policy_loss': Array(0.00148869, dtype=float32), 'training/total_loss': Array(179.8578, dtype=float32), 'training/v_loss': Array(179.89355, dtype=float32), 'eval/episode_goal_distance': (Array(0.3979348, dtype=float32), Array(0.16229162, dtype=float32)), 'eval/episode_reward': (Array(-7941.0415, dtype=float32), Array(5153.3516, dtype=float32)), 'eval/avg_episode_length': (Array(782.40625, dtype=float32), Array(411.21402, dtype=float32)), 'eval/epoch_eval_time': 4.044681072235107, 'eval/sps': 31646.500110642017}
I0726 21:43:43.786655 140267183036224 train.py:379] starting iteration 125 916.2076814174652
I0726 21:43:50.858462 140267183036224 train.py:394] {'eval/walltime': 515.176500082016, 'training/sps': 40593.72480505767, 'training/walltime': 399.78717613220215, 'training/entropy_loss': Array(-0.03435022, dtype=float32), 'training/policy_loss': Array(0.00125454, dtype=float32), 'training/total_loss': Array(189.59914, dtype=float32), 'training/v_loss': Array(189.63223, dtype=float32), 'eval/episode_goal_distance': (Array(0.40171188, dtype=float32), Array(0.14532024, dtype=float32)), 'eval/episode_reward': (Array(-7889.369, dtype=float32), Array(4860.739, dtype=float32)), 'eval/avg_episode_length': (Array(782.4297, dtype=float32), Array(411.16992, dtype=float32)), 'eval/epoch_eval_time': 4.041278123855591, 'eval/sps': 31673.147968811732}
I0726 21:43:50.860722 140267183036224 train.py:379] starting iteration 126 923.2817482948303
I0726 21:43:57.922349 140267183036224 train.py:394] {'eval/walltime': 519.2227754592896, 'training/sps': 40797.528534344354, 'training/walltime': 402.7991232872009, 'training/entropy_loss': Array(-0.03034246, dtype=float32), 'training/policy_loss': Array(0.00159856, dtype=float32), 'training/total_loss': Array(172.92892, dtype=float32), 'training/v_loss': Array(172.95766, dtype=float32), 'eval/episode_goal_distance': (Array(0.3983947, dtype=float32), Array(0.15926038, dtype=float32)), 'eval/episode_reward': (Array(-8097.5444, dtype=float32), Array(5180.271, dtype=float32)), 'eval/avg_episode_length': (Array(790.27344, dtype=float32), Array(405.63263, dtype=float32)), 'eval/epoch_eval_time': 4.04627537727356, 'eval/sps': 31634.030822254193}
I0726 21:43:57.924585 140267183036224 train.py:379] starting iteration 127 930.3456103801727
I0726 21:44:05.000600 140267183036224 train.py:394] {'eval/walltime': 523.2666430473328, 'training/sps': 40573.20563513688, 'training/walltime': 405.82772302627563, 'training/entropy_loss': Array(-0.02827159, dtype=float32), 'training/policy_loss': Array(0.00179351, dtype=float32), 'training/total_loss': Array(170.52856, dtype=float32), 'training/v_loss': Array(170.55505, dtype=float32), 'eval/episode_goal_distance': (Array(0.3779907, dtype=float32), Array(0.14616387, dtype=float32)), 'eval/episode_reward': (Array(-8056.2285, dtype=float32), Array(4375.059, dtype=float32)), 'eval/avg_episode_length': (Array(844.625, dtype=float32), Array(361.0595, dtype=float32)), 'eval/epoch_eval_time': 4.043867588043213, 'eval/sps': 31652.8662754603}
I0726 21:44:05.002869 140267183036224 train.py:379] starting iteration 128 937.4238946437836
I0726 21:44:12.057525 140267183036224 train.py:394] {'eval/walltime': 527.29945063591, 'training/sps': 40711.02138995784, 'training/walltime': 408.8460702896118, 'training/entropy_loss': Array(-0.02571023, dtype=float32), 'training/policy_loss': Array(0.00162692, dtype=float32), 'training/total_loss': Array(157.93073, dtype=float32), 'training/v_loss': Array(157.9548, dtype=float32), 'eval/episode_goal_distance': (Array(0.39078426, dtype=float32), Array(0.15961792, dtype=float32)), 'eval/episode_reward': (Array(-8634.652, dtype=float32), Array(4373.679, dtype=float32)), 'eval/avg_episode_length': (Array(898.9922, dtype=float32), Array(300.4228, dtype=float32)), 'eval/epoch_eval_time': 4.0328075885772705, 'eval/sps': 31739.674454728192}
I0726 21:44:12.059922 140267183036224 train.py:379] starting iteration 129 944.4809484481812
I0726 21:44:19.134788 140267183036224 train.py:394] {'eval/walltime': 531.342123746872, 'training/sps': 40572.56045349668, 'training/walltime': 411.8747181892395, 'training/entropy_loss': Array(-0.02906805, dtype=float32), 'training/policy_loss': Array(0.00409412, dtype=float32), 'training/total_loss': Array(185.05467, dtype=float32), 'training/v_loss': Array(185.07964, dtype=float32), 'eval/episode_goal_distance': (Array(0.3587004, dtype=float32), Array(0.14133124, dtype=float32)), 'eval/episode_reward': (Array(-8301.44, dtype=float32), Array(3981.544, dtype=float32)), 'eval/avg_episode_length': (Array(914.60156, dtype=float32), Array(278.51453, dtype=float32)), 'eval/epoch_eval_time': 4.042673110961914, 'eval/sps': 31662.21865748222}
I0726 21:44:19.137210 140267183036224 train.py:379] starting iteration 130 951.5582365989685
I0726 21:44:26.192582 140267183036224 train.py:394] {'eval/walltime': 535.3747510910034, 'training/sps': 40698.77626518368, 'training/walltime': 414.8939735889435, 'training/entropy_loss': Array(-0.02254364, dtype=float32), 'training/policy_loss': Array(0.00084323, dtype=float32), 'training/total_loss': Array(132.40616, dtype=float32), 'training/v_loss': Array(132.42786, dtype=float32), 'eval/episode_goal_distance': (Array(0.35656965, dtype=float32), Array(0.1285416, dtype=float32)), 'eval/episode_reward': (Array(-7910.5312, dtype=float32), Array(3792.2158, dtype=float32)), 'eval/avg_episode_length': (Array(906.7422, dtype=float32), Array(289.9506, dtype=float32)), 'eval/epoch_eval_time': 4.03262734413147, 'eval/sps': 31741.093108013953}
I0726 21:44:26.194934 140267183036224 train.py:379] starting iteration 131 958.615960597992
I0726 21:44:33.263459 140267183036224 train.py:394] {'eval/walltime': 539.4106452465057, 'training/sps': 40566.43225114166, 'training/walltime': 417.92307901382446, 'training/entropy_loss': Array(-0.0191458, dtype=float32), 'training/policy_loss': Array(0.00088987, dtype=float32), 'training/total_loss': Array(115.32419, dtype=float32), 'training/v_loss': Array(115.34245, dtype=float32), 'eval/episode_goal_distance': (Array(0.35452196, dtype=float32), Array(0.11620408, dtype=float32)), 'eval/episode_reward': (Array(-8664.158, dtype=float32), Array(2979.4443, dtype=float32)), 'eval/avg_episode_length': (Array(976.6953, dtype=float32), Array(150.43117, dtype=float32)), 'eval/epoch_eval_time': 4.035894155502319, 'eval/sps': 31715.40061958556}
I0726 21:44:33.266307 140267183036224 train.py:379] starting iteration 132 965.6873335838318
I0726 21:44:40.326220 140267183036224 train.py:394] {'eval/walltime': 543.4486899375916, 'training/sps': 40711.53913302087, 'training/walltime': 420.9413878917694, 'training/entropy_loss': Array(-0.01885558, dtype=float32), 'training/policy_loss': Array(0.0012377, dtype=float32), 'training/total_loss': Array(109.010376, dtype=float32), 'training/v_loss': Array(109.028, dtype=float32), 'eval/episode_goal_distance': (Array(0.35399348, dtype=float32), Array(0.11406287, dtype=float32)), 'eval/episode_reward': (Array(-8482.96, dtype=float32), Array(3150.1755, dtype=float32)), 'eval/avg_episode_length': (Array(945.60156, dtype=float32), Array(226.16757, dtype=float32)), 'eval/epoch_eval_time': 4.038044691085815, 'eval/sps': 31698.509994841406}
I0726 21:44:40.328599 140267183036224 train.py:379] starting iteration 133 972.7496249675751
I0726 21:44:47.401884 140267183036224 train.py:394] {'eval/walltime': 547.4892835617065, 'training/sps': 40565.12637541933, 'training/walltime': 423.97059082984924, 'training/entropy_loss': Array(-0.01884806, dtype=float32), 'training/policy_loss': Array(0.00064988, dtype=float32), 'training/total_loss': Array(261.23477, dtype=float32), 'training/v_loss': Array(261.253, dtype=float32), 'eval/episode_goal_distance': (Array(0.36234546, dtype=float32), Array(0.12748279, dtype=float32)), 'eval/episode_reward': (Array(-8268.348, dtype=float32), Array(3805.9001, dtype=float32)), 'eval/avg_episode_length': (Array(914.5625, dtype=float32), Array(278.6417, dtype=float32)), 'eval/epoch_eval_time': 4.04059362411499, 'eval/sps': 31678.513581784853}
I0726 21:44:47.404142 140267183036224 train.py:379] starting iteration 134 979.8251669406891
I0726 21:44:54.465584 140267183036224 train.py:394] {'eval/walltime': 551.523410320282, 'training/sps': 40637.946142247616, 'training/walltime': 426.9943656921387, 'training/entropy_loss': Array(-0.01630804, dtype=float32), 'training/policy_loss': Array(0.00098631, dtype=float32), 'training/total_loss': Array(156.26157, dtype=float32), 'training/v_loss': Array(156.27689, dtype=float32), 'eval/episode_goal_distance': (Array(0.35223544, dtype=float32), Array(0.12977418, dtype=float32)), 'eval/episode_reward': (Array(-8444.452, dtype=float32), Array(3748.0764, dtype=float32)), 'eval/avg_episode_length': (Array(930.15625, dtype=float32), Array(253.96892, dtype=float32)), 'eval/epoch_eval_time': 4.0341267585754395, 'eval/sps': 31729.295498191113}
I0726 21:44:54.467898 140267183036224 train.py:379] starting iteration 135 986.8889241218567
I0726 21:45:01.543570 140267183036224 train.py:394] {'eval/walltime': 555.5658197402954, 'training/sps': 40557.51310201, 'training/walltime': 430.02413725852966, 'training/entropy_loss': Array(-0.01174283, dtype=float32), 'training/policy_loss': Array(0.00115345, dtype=float32), 'training/total_loss': Array(111.44462, dtype=float32), 'training/v_loss': Array(111.45521, dtype=float32), 'eval/episode_goal_distance': (Array(0.35499287, dtype=float32), Array(0.13294363, dtype=float32)), 'eval/episode_reward': (Array(-8108.469, dtype=float32), Array(3774.484, dtype=float32)), 'eval/avg_episode_length': (Array(914.58594, dtype=float32), Array(278.565, dtype=float32)), 'eval/epoch_eval_time': 4.042409420013428, 'eval/sps': 31664.284019893963}
I0726 21:45:01.545687 140267183036224 train.py:379] starting iteration 136 993.9667131900787
I0726 21:45:08.615978 140267183036224 train.py:394] {'eval/walltime': 559.6060333251953, 'training/sps': 40599.592608265615, 'training/walltime': 433.0507686138153, 'training/entropy_loss': Array(-0.01076802, dtype=float32), 'training/policy_loss': Array(0.00090485, dtype=float32), 'training/total_loss': Array(104.97768, dtype=float32), 'training/v_loss': Array(104.98753, dtype=float32), 'eval/episode_goal_distance': (Array(0.3582148, dtype=float32), Array(0.12048142, dtype=float32)), 'eval/episode_reward': (Array(-8603.634, dtype=float32), Array(3518.806, dtype=float32)), 'eval/avg_episode_length': (Array(945.60156, dtype=float32), Array(226.16757, dtype=float32)), 'eval/epoch_eval_time': 4.040213584899902, 'eval/sps': 31681.493393912056}
I0726 21:45:08.618131 140267183036224 train.py:379] starting iteration 137 1001.0391569137573
I0726 21:45:15.689206 140267183036224 train.py:394] {'eval/walltime': 563.6479017734528, 'training/sps': 40610.22932662385, 'training/walltime': 436.07660722732544, 'training/entropy_loss': Array(-0.01025468, dtype=float32), 'training/policy_loss': Array(0.00095725, dtype=float32), 'training/total_loss': Array(86.48473, dtype=float32), 'training/v_loss': Array(86.49404, dtype=float32), 'eval/episode_goal_distance': (Array(0.36140573, dtype=float32), Array(0.12814683, dtype=float32)), 'eval/episode_reward': (Array(-8407.626, dtype=float32), Array(3591.721, dtype=float32)), 'eval/avg_episode_length': (Array(937.875, dtype=float32), Array(240.60915, dtype=float32)), 'eval/epoch_eval_time': 4.041868448257446, 'eval/sps': 31668.522030988934}
I0726 21:45:15.691453 140267183036224 train.py:379] starting iteration 138 1008.1124787330627
I0726 21:45:22.762653 140267183036224 train.py:394] {'eval/walltime': 567.6901822090149, 'training/sps': 40614.75762088065, 'training/walltime': 439.10210847854614, 'training/entropy_loss': Array(-0.0097457, dtype=float32), 'training/policy_loss': Array(0.00107444, dtype=float32), 'training/total_loss': Array(80.2467, dtype=float32), 'training/v_loss': Array(80.25537, dtype=float32), 'eval/episode_goal_distance': (Array(0.37385237, dtype=float32), Array(0.12549464, dtype=float32)), 'eval/episode_reward': (Array(-8933.982, dtype=float32), Array(3638.8918, dtype=float32)), 'eval/avg_episode_length': (Array(945.66406, dtype=float32), Array(225.90796, dtype=float32)), 'eval/epoch_eval_time': 4.042280435562134, 'eval/sps': 31665.29439024432}
I0726 21:45:22.764919 140267183036224 train.py:379] starting iteration 139 1015.1859447956085
I0726 21:45:29.843806 140267183036224 train.py:394] {'eval/walltime': 571.7379169464111, 'training/sps': 40586.04646137614, 'training/walltime': 442.12975001335144, 'training/entropy_loss': Array(-0.00872496, dtype=float32), 'training/policy_loss': Array(0.00143467, dtype=float32), 'training/total_loss': Array(77.846405, dtype=float32), 'training/v_loss': Array(77.8537, dtype=float32), 'eval/episode_goal_distance': (Array(0.34267655, dtype=float32), Array(0.10773651, dtype=float32)), 'eval/episode_reward': (Array(-8485.494, dtype=float32), Array(2814.176, dtype=float32)), 'eval/avg_episode_length': (Array(961.1406, dtype=float32), Array(192.73662, dtype=float32)), 'eval/epoch_eval_time': 4.04773473739624, 'eval/sps': 31622.625568180814}
I0726 21:45:29.846243 140267183036224 train.py:379] starting iteration 140 1022.2672688961029
I0726 21:45:36.925981 140267183036224 train.py:394] {'eval/walltime': 575.7802519798279, 'training/sps': 40502.26296683457, 'training/walltime': 445.16365456581116, 'training/entropy_loss': Array(-0.01121466, dtype=float32), 'training/policy_loss': Array(0.00077727, dtype=float32), 'training/total_loss': Array(75.61864, dtype=float32), 'training/v_loss': Array(75.629074, dtype=float32), 'eval/episode_goal_distance': (Array(0.3278125, dtype=float32), Array(0.12978332, dtype=float32)), 'eval/episode_reward': (Array(-7972.5195, dtype=float32), Array(3266.0364, dtype=float32)), 'eval/avg_episode_length': (Array(961.1797, dtype=float32), Array(192.54257, dtype=float32)), 'eval/epoch_eval_time': 4.042335033416748, 'eval/sps': 31664.866702503165}
I0726 21:45:36.928285 140267183036224 train.py:379] starting iteration 141 1029.3493111133575
I0726 21:45:44.004708 140267183036224 train.py:394] {'eval/walltime': 579.8328521251678, 'training/sps': 40683.994848233495, 'training/walltime': 448.1840069293976, 'training/entropy_loss': Array(-0.0101483, dtype=float32), 'training/policy_loss': Array(0.0010695, dtype=float32), 'training/total_loss': Array(80.80242, dtype=float32), 'training/v_loss': Array(80.81149, dtype=float32), 'eval/episode_goal_distance': (Array(0.35322326, dtype=float32), Array(0.12779519, dtype=float32)), 'eval/episode_reward': (Array(-8314.246, dtype=float32), Array(3681.6077, dtype=float32)), 'eval/avg_episode_length': (Array(930.14844, dtype=float32), Array(253.99713, dtype=float32)), 'eval/epoch_eval_time': 4.052600145339966, 'eval/sps': 31584.660565929655}
I0726 21:45:44.006870 140267183036224 train.py:379] starting iteration 142 1036.427895784378
I0726 21:45:51.077923 140267183036224 train.py:394] {'eval/walltime': 583.8756768703461, 'training/sps': 40624.74904094831, 'training/walltime': 451.2087640762329, 'training/entropy_loss': Array(-0.00720736, dtype=float32), 'training/policy_loss': Array(0.0013087, dtype=float32), 'training/total_loss': Array(84.273506, dtype=float32), 'training/v_loss': Array(84.2794, dtype=float32), 'eval/episode_goal_distance': (Array(0.35764608, dtype=float32), Array(0.11074376, dtype=float32)), 'eval/episode_reward': (Array(-9029.885, dtype=float32), Array(3222.9941, dtype=float32)), 'eval/avg_episode_length': (Array(961.15625, dtype=float32), Array(192.6588, dtype=float32)), 'eval/epoch_eval_time': 4.042824745178223, 'eval/sps': 31661.031102736382}
I0726 21:45:51.080142 140267183036224 train.py:379] starting iteration 143 1043.5011682510376
I0726 21:45:58.153328 140267183036224 train.py:394] {'eval/walltime': 587.9287905693054, 'training/sps': 40734.51638102759, 'training/walltime': 454.2253704071045, 'training/entropy_loss': Array(-0.00472754, dtype=float32), 'training/policy_loss': Array(0.00105976, dtype=float32), 'training/total_loss': Array(74.71315, dtype=float32), 'training/v_loss': Array(74.71682, dtype=float32), 'eval/episode_goal_distance': (Array(0.34542423, dtype=float32), Array(0.11112668, dtype=float32)), 'eval/episode_reward': (Array(-8233.27, dtype=float32), Array(3343.0408, dtype=float32)), 'eval/avg_episode_length': (Array(945.6328, dtype=float32), Array(226.0377, dtype=float32)), 'eval/epoch_eval_time': 4.053113698959351, 'eval/sps': 31580.65860152514}
I0726 21:45:58.155571 140267183036224 train.py:379] starting iteration 144 1050.576596736908
I0726 21:46:05.233814 140267183036224 train.py:394] {'eval/walltime': 591.9742767810822, 'training/sps': 40565.05932783484, 'training/walltime': 457.2545783519745, 'training/entropy_loss': Array(-0.00597163, dtype=float32), 'training/policy_loss': Array(0.00113552, dtype=float32), 'training/total_loss': Array(76.709236, dtype=float32), 'training/v_loss': Array(76.71407, dtype=float32), 'eval/episode_goal_distance': (Array(0.3575589, dtype=float32), Array(0.12513718, dtype=float32)), 'eval/episode_reward': (Array(-8666.383, dtype=float32), Array(3503.344, dtype=float32)), 'eval/avg_episode_length': (Array(937.9297, dtype=float32), Array(240.39783, dtype=float32)), 'eval/epoch_eval_time': 4.045486211776733, 'eval/sps': 31640.201770403215}
I0726 21:46:05.235996 140267183036224 train.py:379] starting iteration 145 1057.6570217609406
I0726 21:46:12.307980 140267183036224 train.py:394] {'eval/walltime': 596.0148866176605, 'training/sps': 40582.83469400514, 'training/walltime': 460.2824594974518, 'training/entropy_loss': Array(-0.00451646, dtype=float32), 'training/policy_loss': Array(0.00124799, dtype=float32), 'training/total_loss': Array(79.70587, dtype=float32), 'training/v_loss': Array(79.70914, dtype=float32), 'eval/episode_goal_distance': (Array(0.33900514, dtype=float32), Array(0.11647464, dtype=float32)), 'eval/episode_reward': (Array(-7757.434, dtype=float32), Array(3551.5933, dtype=float32)), 'eval/avg_episode_length': (Array(914.6094, dtype=float32), Array(278.4888, dtype=float32)), 'eval/epoch_eval_time': 4.040609836578369, 'eval/sps': 31678.386475540472}
I0726 21:46:12.310195 140267183036224 train.py:379] starting iteration 146 1064.731220960617
I0726 21:46:19.378636 140267183036224 train.py:394] {'eval/walltime': 600.0603740215302, 'training/sps': 40693.50949530347, 'training/walltime': 463.3021056652069, 'training/entropy_loss': Array(-0.00312591, dtype=float32), 'training/policy_loss': Array(0.00120082, dtype=float32), 'training/total_loss': Array(103.60226, dtype=float32), 'training/v_loss': Array(103.60417, dtype=float32), 'eval/episode_goal_distance': (Array(0.3349514, dtype=float32), Array(0.12474845, dtype=float32)), 'eval/episode_reward': (Array(-8099.76, dtype=float32), Array(3346.89, dtype=float32)), 'eval/avg_episode_length': (Array(937.91406, dtype=float32), Array(240.45859, dtype=float32)), 'eval/epoch_eval_time': 4.045487403869629, 'eval/sps': 31640.192446913614}
I0726 21:46:19.380843 140267183036224 train.py:379] starting iteration 147 1071.8018689155579
I0726 21:46:26.440395 140267183036224 train.py:394] {'eval/walltime': 604.0974864959717, 'training/sps': 40702.14462765112, 'training/walltime': 466.32111120224, 'training/entropy_loss': Array(-0.00227659, dtype=float32), 'training/policy_loss': Array(0.00130919, dtype=float32), 'training/total_loss': Array(76.55962, dtype=float32), 'training/v_loss': Array(76.56059, dtype=float32), 'eval/episode_goal_distance': (Array(0.33939064, dtype=float32), Array(0.11083788, dtype=float32)), 'eval/episode_reward': (Array(-7936.1147, dtype=float32), Array(3265.4097, dtype=float32)), 'eval/avg_episode_length': (Array(937.85156, dtype=float32), Array(240.70055, dtype=float32)), 'eval/epoch_eval_time': 4.037112474441528, 'eval/sps': 31705.829552768853}
I0726 21:46:26.442730 140267183036224 train.py:379] starting iteration 148 1078.8637554645538
I0726 21:46:33.515729 140267183036224 train.py:394] {'eval/walltime': 608.145414352417, 'training/sps': 40667.5330100737, 'training/walltime': 469.34268617630005, 'training/entropy_loss': Array(-0.00350511, dtype=float32), 'training/policy_loss': Array(0.00100179, dtype=float32), 'training/total_loss': Array(76.75328, dtype=float32), 'training/v_loss': Array(76.75578, dtype=float32), 'eval/episode_goal_distance': (Array(0.35834563, dtype=float32), Array(0.12989363, dtype=float32)), 'eval/episode_reward': (Array(-8726.016, dtype=float32), Array(3242.3928, dtype=float32)), 'eval/avg_episode_length': (Array(968.9531, dtype=float32), Array(172.86198, dtype=float32)), 'eval/epoch_eval_time': 4.0479278564453125, 'eval/sps': 31621.116912000303}
I0726 21:46:33.520335 140267183036224 train.py:379] starting iteration 149 1085.9413459300995
I0726 21:46:40.586709 140267183036224 train.py:394] {'eval/walltime': 612.1839823722839, 'training/sps': 40635.927583608034, 'training/walltime': 472.3666112422943, 'training/entropy_loss': Array(-0.00196229, dtype=float32), 'training/policy_loss': Array(0.00117309, dtype=float32), 'training/total_loss': Array(72.71724, dtype=float32), 'training/v_loss': Array(72.71803, dtype=float32), 'eval/episode_goal_distance': (Array(0.3697248, dtype=float32), Array(0.12163576, dtype=float32)), 'eval/episode_reward': (Array(-8640.545, dtype=float32), Array(3803.598, dtype=float32)), 'eval/avg_episode_length': (Array(914.5781, dtype=float32), Array(278.59076, dtype=float32)), 'eval/epoch_eval_time': 4.038568019866943, 'eval/sps': 31694.402414501652}
I0726 21:46:40.588858 140267183036224 train.py:379] starting iteration 150 1093.0098838806152
I0726 21:46:47.668820 140267183036224 train.py:394] {'eval/walltime': 616.2359013557434, 'training/sps': 40628.26849735434, 'training/walltime': 475.3911063671112, 'training/entropy_loss': Array(-0.00150487, dtype=float32), 'training/policy_loss': Array(0.00100007, dtype=float32), 'training/total_loss': Array(259.22015, dtype=float32), 'training/v_loss': Array(259.22064, dtype=float32), 'eval/episode_goal_distance': (Array(0.3493549, dtype=float32), Array(0.12474737, dtype=float32)), 'eval/episode_reward': (Array(-8562.928, dtype=float32), Array(3132.273, dtype=float32)), 'eval/avg_episode_length': (Array(968.9219, dtype=float32), Array(173.03578, dtype=float32)), 'eval/epoch_eval_time': 4.051918983459473, 'eval/sps': 31589.970214734}
I0726 21:46:47.671028 140267183036224 train.py:379] starting iteration 151 1100.0920548439026
I0726 21:46:54.752454 140267183036224 train.py:394] {'eval/walltime': 620.2844750881195, 'training/sps': 40562.85326665675, 'training/walltime': 478.42047905921936, 'training/entropy_loss': Array(-0.00128602, dtype=float32), 'training/policy_loss': Array(0.00122468, dtype=float32), 'training/total_loss': Array(112.89235, dtype=float32), 'training/v_loss': Array(112.89241, dtype=float32), 'eval/episode_goal_distance': (Array(0.3655818, dtype=float32), Array(0.13430114, dtype=float32)), 'eval/episode_reward': (Array(-8225.143, dtype=float32), Array(3590.3455, dtype=float32)), 'eval/avg_episode_length': (Array(930.09375, dtype=float32), Array(254.19601, dtype=float32)), 'eval/epoch_eval_time': 4.048573732376099, 'eval/sps': 31616.07234083325}
I0726 21:46:54.754615 140267183036224 train.py:379] starting iteration 152 1107.1756403446198
I0726 21:47:01.831443 140267183036224 train.py:394] {'eval/walltime': 624.3395462036133, 'training/sps': 40712.767619563856, 'training/walltime': 481.4386968612671, 'training/entropy_loss': Array(-0.00650733, dtype=float32), 'training/policy_loss': Array(0.00087382, dtype=float32), 'training/total_loss': Array(92.78822, dtype=float32), 'training/v_loss': Array(92.793846, dtype=float32), 'eval/episode_goal_distance': (Array(0.36320853, dtype=float32), Array(0.12518543, dtype=float32)), 'eval/episode_reward': (Array(-8604.215, dtype=float32), Array(3369.1016, dtype=float32)), 'eval/avg_episode_length': (Array(953.35156, dtype=float32), Array(210.34958, dtype=float32)), 'eval/epoch_eval_time': 4.055071115493774, 'eval/sps': 31565.41435511022}
I0726 21:47:01.833717 140267183036224 train.py:379] starting iteration 153 1114.254742383957
I0726 21:47:08.907171 140267183036224 train.py:394] {'eval/walltime': 628.3883216381073, 'training/sps': 40672.029159084006, 'training/walltime': 484.4599378108978, 'training/entropy_loss': Array(-0.00739427, dtype=float32), 'training/policy_loss': Array(0.00116797, dtype=float32), 'training/total_loss': Array(89.629715, dtype=float32), 'training/v_loss': Array(89.63594, dtype=float32), 'eval/episode_goal_distance': (Array(0.41522926, dtype=float32), Array(0.14371349, dtype=float32)), 'eval/episode_reward': (Array(-9617.308, dtype=float32), Array(3389.6477, dtype=float32)), 'eval/avg_episode_length': (Array(976.6953, dtype=float32), Array(150.43118, dtype=float32)), 'eval/epoch_eval_time': 4.0487754344940186, 'eval/sps': 31614.49728959748}
I0726 21:47:08.986921 140267183036224 train.py:379] starting iteration 154 1121.4079446792603
I0726 21:47:16.067631 140267183036224 train.py:394] {'eval/walltime': 632.4422359466553, 'training/sps': 40643.68891679801, 'training/walltime': 487.4832854270935, 'training/entropy_loss': Array(-0.00740138, dtype=float32), 'training/policy_loss': Array(0.00115629, dtype=float32), 'training/total_loss': Array(77.322136, dtype=float32), 'training/v_loss': Array(77.32838, dtype=float32), 'eval/episode_goal_distance': (Array(0.3924108, dtype=float32), Array(0.13495626, dtype=float32)), 'eval/episode_reward': (Array(-8608.613, dtype=float32), Array(3642.513, dtype=float32)), 'eval/avg_episode_length': (Array(922.3594, dtype=float32), Array(266.7048, dtype=float32)), 'eval/epoch_eval_time': 4.053914308547974, 'eval/sps': 31574.42172127385}
I0726 21:47:16.069871 140267183036224 train.py:379] starting iteration 155 1128.49089717865
I0726 21:47:23.132909 140267183036224 train.py:394] {'eval/walltime': 636.4874536991119, 'training/sps': 40763.615382157455, 'training/walltime': 490.49773836135864, 'training/entropy_loss': Array(-0.00862023, dtype=float32), 'training/policy_loss': Array(0.00103078, dtype=float32), 'training/total_loss': Array(80.17544, dtype=float32), 'training/v_loss': Array(80.18303, dtype=float32), 'eval/episode_goal_distance': (Array(0.37057966, dtype=float32), Array(0.13491622, dtype=float32)), 'eval/episode_reward': (Array(-8330.631, dtype=float32), Array(3465.575, dtype=float32)), 'eval/avg_episode_length': (Array(922.3281, dtype=float32), Array(266.81204, dtype=float32)), 'eval/epoch_eval_time': 4.045217752456665, 'eval/sps': 31642.301560222677}
I0726 21:47:23.135048 140267183036224 train.py:379] starting iteration 156 1135.5560746192932
I0726 21:47:30.218298 140267183036224 train.py:394] {'eval/walltime': 640.5440549850464, 'training/sps': 40642.88765105268, 'training/walltime': 493.5211455821991, 'training/entropy_loss': Array(-0.01125384, dtype=float32), 'training/policy_loss': Array(0.00078958, dtype=float32), 'training/total_loss': Array(84.38362, dtype=float32), 'training/v_loss': Array(84.39408, dtype=float32), 'eval/episode_goal_distance': (Array(0.4134354, dtype=float32), Array(0.15925078, dtype=float32)), 'eval/episode_reward': (Array(-9361.879, dtype=float32), Array(3773.4019, dtype=float32)), 'eval/avg_episode_length': (Array(953.41406, dtype=float32), Array(210.06808, dtype=float32)), 'eval/epoch_eval_time': 4.056601285934448, 'eval/sps': 31553.50772180088}
I0726 21:47:30.220426 140267183036224 train.py:379] starting iteration 157 1142.6414518356323
I0726 21:47:37.294634 140267183036224 train.py:394] {'eval/walltime': 644.5984356403351, 'training/sps': 40734.53569784071, 'training/walltime': 496.5377504825592, 'training/entropy_loss': Array(-0.01509881, dtype=float32), 'training/policy_loss': Array(0.00072941, dtype=float32), 'training/total_loss': Array(91.964165, dtype=float32), 'training/v_loss': Array(91.97853, dtype=float32), 'eval/episode_goal_distance': (Array(0.44532406, dtype=float32), Array(0.17440484, dtype=float32)), 'eval/episode_reward': (Array(-9799.499, dtype=float32), Array(4068.099, dtype=float32)), 'eval/avg_episode_length': (Array(937.8203, dtype=float32), Array(240.8214, dtype=float32)), 'eval/epoch_eval_time': 4.054380655288696, 'eval/sps': 31570.78993878675}
I0726 21:47:37.296809 140267183036224 train.py:379] starting iteration 158 1149.717835187912
I0726 21:47:44.380006 140267183036224 train.py:394] {'eval/walltime': 648.6589224338531, 'training/sps': 40697.50363212588, 'training/walltime': 499.5571002960205, 'training/entropy_loss': Array(-0.01704483, dtype=float32), 'training/policy_loss': Array(0.00071572, dtype=float32), 'training/total_loss': Array(115.63839, dtype=float32), 'training/v_loss': Array(115.65471, dtype=float32), 'eval/episode_goal_distance': (Array(0.40553147, dtype=float32), Array(0.1545241, dtype=float32)), 'eval/episode_reward': (Array(-9006.855, dtype=float32), Array(4229.7817, dtype=float32)), 'eval/avg_episode_length': (Array(906.9219, dtype=float32), Array(289.39206, dtype=float32)), 'eval/epoch_eval_time': 4.060486793518066, 'eval/sps': 31523.31395445788}
I0726 21:47:44.382130 140267183036224 train.py:379] starting iteration 159 1156.8031568527222
I0726 21:47:51.455515 140267183036224 train.py:394] {'eval/walltime': 652.7091901302338, 'training/sps': 40691.66211072906, 'training/walltime': 502.5768835544586, 'training/entropy_loss': Array(-0.0181372, dtype=float32), 'training/policy_loss': Array(0.00084195, dtype=float32), 'training/total_loss': Array(104.19694, dtype=float32), 'training/v_loss': Array(104.21423, dtype=float32), 'eval/episode_goal_distance': (Array(0.40094358, dtype=float32), Array(0.14961885, dtype=float32)), 'eval/episode_reward': (Array(-8843.998, dtype=float32), Array(4027.595, dtype=float32)), 'eval/avg_episode_length': (Array(922.3906, dtype=float32), Array(266.59732, dtype=float32)), 'eval/epoch_eval_time': 4.050267696380615, 'eval/sps': 31602.84939051877}
I0726 21:47:51.457624 140267183036224 train.py:379] starting iteration 160 1163.8786499500275
I0726 21:47:58.539536 140267183036224 train.py:394] {'eval/walltime': 656.7689390182495, 'training/sps': 40706.51983142918, 'training/walltime': 505.59556460380554, 'training/entropy_loss': Array(-0.01888405, dtype=float32), 'training/policy_loss': Array(0.00133405, dtype=float32), 'training/total_loss': Array(106.830925, dtype=float32), 'training/v_loss': Array(106.84848, dtype=float32), 'eval/episode_goal_distance': (Array(0.40945476, dtype=float32), Array(0.16638644, dtype=float32)), 'eval/episode_reward': (Array(-8716.512, dtype=float32), Array(4342.572, dtype=float32)), 'eval/avg_episode_length': (Array(883.5547, dtype=float32), Array(319.6073, dtype=float32)), 'eval/epoch_eval_time': 4.059748888015747, 'eval/sps': 31529.0436750539}
I0726 21:47:58.541709 140267183036224 train.py:379] starting iteration 161 1170.9627358913422
I0726 21:48:05.624731 140267183036224 train.py:394] {'eval/walltime': 660.8229913711548, 'training/sps': 40615.42654994069, 'training/walltime': 508.6210160255432, 'training/entropy_loss': Array(-0.02249697, dtype=float32), 'training/policy_loss': Array(0.00086415, dtype=float32), 'training/total_loss': Array(110.40799, dtype=float32), 'training/v_loss': Array(110.42962, dtype=float32), 'eval/episode_goal_distance': (Array(0.43072748, dtype=float32), Array(0.1549191, dtype=float32)), 'eval/episode_reward': (Array(-8998.137, dtype=float32), Array(4424.6504, dtype=float32)), 'eval/avg_episode_length': (Array(883.53125, dtype=float32), Array(319.6712, dtype=float32)), 'eval/epoch_eval_time': 4.054052352905273, 'eval/sps': 31573.34658203681}
I0726 21:48:05.626874 140267183036224 train.py:379] starting iteration 162 1178.0478999614716
I0726 21:48:12.717169 140267183036224 train.py:394] {'eval/walltime': 664.8952081203461, 'training/sps': 40762.161379052784, 'training/walltime': 511.6355764865875, 'training/entropy_loss': Array(-0.02379564, dtype=float32), 'training/policy_loss': Array(0.00054568, dtype=float32), 'training/total_loss': Array(136.2701, dtype=float32), 'training/v_loss': Array(136.29337, dtype=float32), 'eval/episode_goal_distance': (Array(0.39102122, dtype=float32), Array(0.17294112, dtype=float32)), 'eval/episode_reward': (Array(-8793.404, dtype=float32), Array(3908.8235, dtype=float32)), 'eval/avg_episode_length': (Array(922.35156, dtype=float32), Array(266.7316, dtype=float32)), 'eval/epoch_eval_time': 4.072216749191284, 'eval/sps': 31432.511549248935}
I0726 21:48:12.719407 140267183036224 train.py:379] starting iteration 163 1185.1404330730438
I0726 21:48:19.814657 140267183036224 train.py:394] {'eval/walltime': 668.957380771637, 'training/sps': 40559.89094547447, 'training/walltime': 514.6651704311371, 'training/entropy_loss': Array(-0.02524729, dtype=float32), 'training/policy_loss': Array(0.00060594, dtype=float32), 'training/total_loss': Array(140.44037, dtype=float32), 'training/v_loss': Array(140.465, dtype=float32), 'eval/episode_goal_distance': (Array(0.3666997, dtype=float32), Array(0.1367116, dtype=float32)), 'eval/episode_reward': (Array(-8603.333, dtype=float32), Array(3187.9705, dtype=float32)), 'eval/avg_episode_length': (Array(953.3828, dtype=float32), Array(210.20876, dtype=float32)), 'eval/epoch_eval_time': 4.0621726512908936, 'eval/sps': 31510.23134364406}
I0726 21:48:19.816853 140267183036224 train.py:379] starting iteration 164 1192.2378787994385
I0726 21:48:26.895117 140267183036224 train.py:394] {'eval/walltime': 673.0097484588623, 'training/sps': 40656.975301527724, 'training/walltime': 517.687530040741, 'training/entropy_loss': Array(-0.02815693, dtype=float32), 'training/policy_loss': Array(0.00065227, dtype=float32), 'training/total_loss': Array(114.12103, dtype=float32), 'training/v_loss': Array(114.148544, dtype=float32), 'eval/episode_goal_distance': (Array(0.39985916, dtype=float32), Array(0.15599275, dtype=float32)), 'eval/episode_reward': (Array(-9483.865, dtype=float32), Array(3348.9463, dtype=float32)), 'eval/avg_episode_length': (Array(968.9531, dtype=float32), Array(172.86198, dtype=float32)), 'eval/epoch_eval_time': 4.052367687225342, 'eval/sps': 31586.47237354754}
I0726 21:48:26.897259 140267183036224 train.py:379] starting iteration 165 1199.3182847499847
I0726 21:48:33.992343 140267183036224 train.py:394] {'eval/walltime': 677.0788321495056, 'training/sps': 40653.8260534639, 'training/walltime': 520.7101237773895, 'training/entropy_loss': Array(-0.02953674, dtype=float32), 'training/policy_loss': Array(0.00065596, dtype=float32), 'training/total_loss': Array(117.93074, dtype=float32), 'training/v_loss': Array(117.95961, dtype=float32), 'eval/episode_goal_distance': (Array(0.4136066, dtype=float32), Array(0.16695336, dtype=float32)), 'eval/episode_reward': (Array(-9446.327, dtype=float32), Array(3911.1326, dtype=float32)), 'eval/avg_episode_length': (Array(945.66406, dtype=float32), Array(225.90799, dtype=float32)), 'eval/epoch_eval_time': 4.0690836906433105, 'eval/sps': 31456.713533400824}
I0726 21:48:33.994490 140267183036224 train.py:379] starting iteration 166 1206.4155158996582
I0726 21:48:41.073920 140267183036224 train.py:394] {'eval/walltime': 681.1341540813446, 'training/sps': 40680.19280379748, 'training/walltime': 523.7307584285736, 'training/entropy_loss': Array(-0.03154723, dtype=float32), 'training/policy_loss': Array(0.00045212, dtype=float32), 'training/total_loss': Array(229.06024, dtype=float32), 'training/v_loss': Array(229.09134, dtype=float32), 'eval/episode_goal_distance': (Array(0.4483623, dtype=float32), Array(0.17845626, dtype=float32)), 'eval/episode_reward': (Array(-9678.087, dtype=float32), Array(4288.3267, dtype=float32)), 'eval/avg_episode_length': (Array(922.33594, dtype=float32), Array(266.785, dtype=float32)), 'eval/epoch_eval_time': 4.055321931838989, 'eval/sps': 31563.46207561261}
I0726 21:48:41.078756 140267183036224 train.py:379] starting iteration 167 1213.499766588211
I0726 21:48:48.165267 140267183036224 train.py:394] {'eval/walltime': 685.1876487731934, 'training/sps': 40566.76751397337, 'training/walltime': 526.7598388195038, 'training/entropy_loss': Array(-0.03499838, dtype=float32), 'training/policy_loss': Array(0.00012678, dtype=float32), 'training/total_loss': Array(220.57924, dtype=float32), 'training/v_loss': Array(220.6141, dtype=float32), 'eval/episode_goal_distance': (Array(0.45386767, dtype=float32), Array(0.21364757, dtype=float32)), 'eval/episode_reward': (Array(-9411.076, dtype=float32), Array(4851.804, dtype=float32)), 'eval/avg_episode_length': (Array(899.0469, dtype=float32), Array(300.2605, dtype=float32)), 'eval/epoch_eval_time': 4.053494691848755, 'eval/sps': 31577.69029706576}
I0726 21:48:48.167368 140267183036224 train.py:379] starting iteration 168 1220.5883944034576
I0726 21:48:55.260210 140267183036224 train.py:394] {'eval/walltime': 689.2478911876678, 'training/sps': 40566.1097654562, 'training/walltime': 529.7889683246613, 'training/entropy_loss': Array(-0.03364192, dtype=float32), 'training/policy_loss': Array(0.00035961, dtype=float32), 'training/total_loss': Array(157.76712, dtype=float32), 'training/v_loss': Array(157.8004, dtype=float32), 'eval/episode_goal_distance': (Array(0.41714758, dtype=float32), Array(0.17642114, dtype=float32)), 'eval/episode_reward': (Array(-9504.819, dtype=float32), Array(3786.534, dtype=float32)), 'eval/avg_episode_length': (Array(953.46094, dtype=float32), Array(209.85657, dtype=float32)), 'eval/epoch_eval_time': 4.060242414474487, 'eval/sps': 31525.211288786286}
I0726 21:48:55.262492 140267183036224 train.py:379] starting iteration 169 1227.6835176944733
I0726 21:49:02.361029 140267183036224 train.py:394] {'eval/walltime': 693.3121283054352, 'training/sps': 40541.07371578053, 'training/walltime': 532.8199684619904, 'training/entropy_loss': Array(-0.03229643, dtype=float32), 'training/policy_loss': Array(0.0004318, dtype=float32), 'training/total_loss': Array(136.89705, dtype=float32), 'training/v_loss': Array(136.9289, dtype=float32), 'eval/episode_goal_distance': (Array(0.39489156, dtype=float32), Array(0.1681437, dtype=float32)), 'eval/episode_reward': (Array(-8591.206, dtype=float32), Array(4057.0222, dtype=float32)), 'eval/avg_episode_length': (Array(914.6094, dtype=float32), Array(278.4887, dtype=float32)), 'eval/epoch_eval_time': 4.064237117767334, 'eval/sps': 31494.22543296787}
I0726 21:49:02.363334 140267183036224 train.py:379] starting iteration 170 1234.7843599319458
I0726 21:49:09.460026 140267183036224 train.py:394] {'eval/walltime': 697.3883469104767, 'training/sps': 40730.35403513727, 'training/walltime': 535.8368830680847, 'training/entropy_loss': Array(-0.03164234, dtype=float32), 'training/policy_loss': Array(0.00065382, dtype=float32), 'training/total_loss': Array(131.71262, dtype=float32), 'training/v_loss': Array(131.74362, dtype=float32), 'eval/episode_goal_distance': (Array(0.40385652, dtype=float32), Array(0.16645999, dtype=float32)), 'eval/episode_reward': (Array(-8985.518, dtype=float32), Array(4093.7869, dtype=float32)), 'eval/avg_episode_length': (Array(922.3594, dtype=float32), Array(266.70468, dtype=float32)), 'eval/epoch_eval_time': 4.076218605041504, 'eval/sps': 31401.65246331206}
I0726 21:49:09.462200 140267183036224 train.py:379] starting iteration 171 1241.8832259178162
I0726 21:49:16.563291 140267183036224 train.py:394] {'eval/walltime': 701.456508398056, 'training/sps': 40561.359283357015, 'training/walltime': 538.8663673400879, 'training/entropy_loss': Array(-0.0315542, dtype=float32), 'training/policy_loss': Array(0.00019291, dtype=float32), 'training/total_loss': Array(115.44438, dtype=float32), 'training/v_loss': Array(115.47574, dtype=float32), 'eval/episode_goal_distance': (Array(0.38168162, dtype=float32), Array(0.14586714, dtype=float32)), 'eval/episode_reward': (Array(-8828.545, dtype=float32), Array(3535.4836, dtype=float32)), 'eval/avg_episode_length': (Array(953.3906, dtype=float32), Array(210.1736, dtype=float32)), 'eval/epoch_eval_time': 4.068161487579346, 'eval/sps': 31463.84439034722}
I0726 21:49:16.565532 140267183036224 train.py:379] starting iteration 172 1248.9865577220917
I0726 21:49:23.659094 140267183036224 train.py:394] {'eval/walltime': 705.5187311172485, 'training/sps': 40581.24018442637, 'training/walltime': 541.8943674564362, 'training/entropy_loss': Array(-0.0309952, dtype=float32), 'training/policy_loss': Array(0.00072963, dtype=float32), 'training/total_loss': Array(103.97357, dtype=float32), 'training/v_loss': Array(104.003845, dtype=float32), 'eval/episode_goal_distance': (Array(0.36792415, dtype=float32), Array(0.13305302, dtype=float32)), 'eval/episode_reward': (Array(-8647.663, dtype=float32), Array(3310.78, dtype=float32)), 'eval/avg_episode_length': (Array(953.3594, dtype=float32), Array(210.3145, dtype=float32)), 'eval/epoch_eval_time': 4.062222719192505, 'eval/sps': 31509.84297223468}
I0726 21:49:23.661203 140267183036224 train.py:379] starting iteration 173 1256.0822296142578
I0726 21:49:30.765617 140267183036224 train.py:394] {'eval/walltime': 709.6036307811737, 'training/sps': 40741.70027935233, 'training/walltime': 544.9104418754578, 'training/entropy_loss': Array(-0.02972394, dtype=float32), 'training/policy_loss': Array(0.0008285, dtype=float32), 'training/total_loss': Array(97.9962, dtype=float32), 'training/v_loss': Array(98.02509, dtype=float32), 'eval/episode_goal_distance': (Array(0.35956365, dtype=float32), Array(0.14796738, dtype=float32)), 'eval/episode_reward': (Array(-8359.451, dtype=float32), Array(3902.7036, dtype=float32)), 'eval/avg_episode_length': (Array(930.08594, dtype=float32), Array(254.2248, dtype=float32)), 'eval/epoch_eval_time': 4.084899663925171, 'eval/sps': 31334.91897742357}
I0726 21:49:30.767927 140267183036224 train.py:379] starting iteration 174 1263.1889526844025
I0726 21:49:37.871994 140267183036224 train.py:394] {'eval/walltime': 713.6751127243042, 'training/sps': 40565.573364980075, 'training/walltime': 547.9396114349365, 'training/entropy_loss': Array(-0.02731323, dtype=float32), 'training/policy_loss': Array(0.00048161, dtype=float32), 'training/total_loss': Array(95.532616, dtype=float32), 'training/v_loss': Array(95.55945, dtype=float32), 'eval/episode_goal_distance': (Array(0.3745826, dtype=float32), Array(0.15716676, dtype=float32)), 'eval/episode_reward': (Array(-8910.467, dtype=float32), Array(4072.2527, dtype=float32)), 'eval/avg_episode_length': (Array(922.3594, dtype=float32), Array(266.70474, dtype=float32)), 'eval/epoch_eval_time': 4.071481943130493, 'eval/sps': 31438.18437312852}
I0726 21:49:37.874380 140267183036224 train.py:379] starting iteration 175 1270.295405626297
I0726 21:49:44.976428 140267183036224 train.py:394] {'eval/walltime': 717.7432675361633, 'training/sps': 40546.80509118813, 'training/walltime': 550.970183134079, 'training/entropy_loss': Array(-0.0294381, dtype=float32), 'training/policy_loss': Array(0.0009299, dtype=float32), 'training/total_loss': Array(108.363754, dtype=float32), 'training/v_loss': Array(108.39226, dtype=float32), 'eval/episode_goal_distance': (Array(0.38554358, dtype=float32), Array(0.14748657, dtype=float32)), 'eval/episode_reward': (Array(-8818.083, dtype=float32), Array(4008.6072, dtype=float32)), 'eval/avg_episode_length': (Array(922.3125, dtype=float32), Array(266.86575, dtype=float32)), 'eval/epoch_eval_time': 4.068154811859131, 'eval/sps': 31463.8960215736}
I0726 21:49:44.978788 140267183036224 train.py:379] starting iteration 176 1277.3998148441315
I0726 21:49:52.063144 140267183036224 train.py:394] {'eval/walltime': 721.8028681278229, 'training/sps': 40669.71838017765, 'training/walltime': 553.9915957450867, 'training/entropy_loss': Array(-0.03218267, dtype=float32), 'training/policy_loss': Array(0.00102588, dtype=float32), 'training/total_loss': Array(111.69713, dtype=float32), 'training/v_loss': Array(111.72829, dtype=float32), 'eval/episode_goal_distance': (Array(0.3931949, dtype=float32), Array(0.17051753, dtype=float32)), 'eval/episode_reward': (Array(-8817.772, dtype=float32), Array(3975.856, dtype=float32)), 'eval/avg_episode_length': (Array(922.3125, dtype=float32), Array(266.86578, dtype=float32)), 'eval/epoch_eval_time': 4.059600591659546, 'eval/sps': 31530.19542439129}
I0726 21:49:52.065404 140267183036224 train.py:379] starting iteration 177 1284.4864311218262
I0726 21:49:59.169673 140267183036224 train.py:394] {'eval/walltime': 725.8723151683807, 'training/sps': 40536.50444784165, 'training/walltime': 557.0229375362396, 'training/entropy_loss': Array(-0.03403883, dtype=float32), 'training/policy_loss': Array(0.00069382, dtype=float32), 'training/total_loss': Array(110.08655, dtype=float32), 'training/v_loss': Array(110.11989, dtype=float32), 'eval/episode_goal_distance': (Array(0.43108684, dtype=float32), Array(0.17139149, dtype=float32)), 'eval/episode_reward': (Array(-9593.744, dtype=float32), Array(4036.71, dtype=float32)), 'eval/avg_episode_length': (Array(930.1172, dtype=float32), Array(254.11075, dtype=float32)), 'eval/epoch_eval_time': 4.069447040557861, 'eval/sps': 31453.90484856957}
I0726 21:49:59.171845 140267183036224 train.py:379] starting iteration 178 1291.5928704738617
I0726 21:50:06.273746 140267183036224 train.py:394] {'eval/walltime': 729.9421248435974, 'training/sps': 40574.642994743204, 'training/walltime': 560.0514299869537, 'training/entropy_loss': Array(-0.0326185, dtype=float32), 'training/policy_loss': Array(0.00051043, dtype=float32), 'training/total_loss': Array(115.5314, dtype=float32), 'training/v_loss': Array(115.56351, dtype=float32), 'eval/episode_goal_distance': (Array(0.38254339, dtype=float32), Array(0.16021083, dtype=float32)), 'eval/episode_reward': (Array(-8752.802, dtype=float32), Array(4269.0586, dtype=float32)), 'eval/avg_episode_length': (Array(899.0625, dtype=float32), Array(300.2139, dtype=float32)), 'eval/epoch_eval_time': 4.069809675216675, 'eval/sps': 31451.102192680628}
I0726 21:50:06.275989 140267183036224 train.py:379] starting iteration 179 1298.6970150470734
I0726 21:50:13.371415 140267183036224 train.py:394] {'eval/walltime': 734.0057082176208, 'training/sps': 40577.71928649119, 'training/walltime': 563.0796928405762, 'training/entropy_loss': Array(-0.02991624, dtype=float32), 'training/policy_loss': Array(0.00220919, dtype=float32), 'training/total_loss': Array(145.70134, dtype=float32), 'training/v_loss': Array(145.72905, dtype=float32), 'eval/episode_goal_distance': (Array(0.3698398, dtype=float32), Array(0.13045868, dtype=float32)), 'eval/episode_reward': (Array(-8130.3994, dtype=float32), Array(4142.1494, dtype=float32)), 'eval/avg_episode_length': (Array(875.6719, dtype=float32), Array(328.94174, dtype=float32)), 'eval/epoch_eval_time': 4.0635833740234375, 'eval/sps': 31499.292180946337}
I0726 21:50:13.373731 140267183036224 train.py:379] starting iteration 180 1305.7947573661804
I0726 21:50:20.486792 140267183036224 train.py:394] {'eval/walltime': 738.0847046375275, 'training/sps': 40546.94225580548, 'training/walltime': 566.1102542877197, 'training/entropy_loss': Array(-0.03075613, dtype=float32), 'training/policy_loss': Array(0.00577917, dtype=float32), 'training/total_loss': Array(117.650795, dtype=float32), 'training/v_loss': Array(117.67577, dtype=float32), 'eval/episode_goal_distance': (Array(0.40295425, dtype=float32), Array(0.15523903, dtype=float32)), 'eval/episode_reward': (Array(-8542.684, dtype=float32), Array(3932.9326, dtype=float32)), 'eval/avg_episode_length': (Array(906.84375, dtype=float32), Array(289.63495, dtype=float32)), 'eval/epoch_eval_time': 4.078996419906616, 'eval/sps': 31380.267797079952}
I0726 21:50:20.489080 140267183036224 train.py:379] starting iteration 181 1312.910106420517
I0726 21:50:27.588231 140267183036224 train.py:394] {'eval/walltime': 742.1520009040833, 'training/sps': 40576.85033854058, 'training/walltime': 569.1385819911957, 'training/entropy_loss': Array(-0.03401332, dtype=float32), 'training/policy_loss': Array(0.00400465, dtype=float32), 'training/total_loss': Array(120.34593, dtype=float32), 'training/v_loss': Array(120.375946, dtype=float32), 'eval/episode_goal_distance': (Array(0.396518, dtype=float32), Array(0.15550102, dtype=float32)), 'eval/episode_reward': (Array(-7851.787, dtype=float32), Array(4569.273, dtype=float32)), 'eval/avg_episode_length': (Array(813.5469, dtype=float32), Array(388.13376, dtype=float32)), 'eval/epoch_eval_time': 4.067296266555786, 'eval/sps': 31470.537578613927}
I0726 21:50:27.590362 140267183036224 train.py:379] starting iteration 182 1320.0113770961761
I0726 21:50:34.690244 140267183036224 train.py:394] {'eval/walltime': 746.2222094535828, 'training/sps': 40606.89855478049, 'training/walltime': 572.1646687984467, 'training/entropy_loss': Array(-0.03524368, dtype=float32), 'training/policy_loss': Array(0.00459127, dtype=float32), 'training/total_loss': Array(122.39998, dtype=float32), 'training/v_loss': Array(122.43063, dtype=float32), 'eval/episode_goal_distance': (Array(0.35809138, dtype=float32), Array(0.13133849, dtype=float32)), 'eval/episode_reward': (Array(-7582.0273, dtype=float32), Array(3969.0798, dtype=float32)), 'eval/avg_episode_length': (Array(852.3828, dtype=float32), Array(353.56906, dtype=float32)), 'eval/epoch_eval_time': 4.070208549499512, 'eval/sps': 31448.02003222645}
I0726 21:50:34.693678 140267183036224 train.py:379] starting iteration 183 1327.1147017478943
I0726 21:50:41.806154 140267183036224 train.py:394] {'eval/walltime': 750.3041279315948, 'training/sps': 40598.761099757445, 'training/walltime': 575.1913621425629, 'training/entropy_loss': Array(-0.03685702, dtype=float32), 'training/policy_loss': Array(0.00242241, dtype=float32), 'training/total_loss': Array(401.73657, dtype=float32), 'training/v_loss': Array(401.771, dtype=float32), 'eval/episode_goal_distance': (Array(0.40382186, dtype=float32), Array(0.15555348, dtype=float32)), 'eval/episode_reward': (Array(-8296.918, dtype=float32), Array(4721.676, dtype=float32)), 'eval/avg_episode_length': (Array(836.96875, dtype=float32), Array(368.0053, dtype=float32)), 'eval/epoch_eval_time': 4.081918478012085, 'eval/sps': 31357.80410350984}
I0726 21:50:41.810954 140267183036224 train.py:379] starting iteration 184 1334.2319648265839
I0726 21:50:48.923663 140267183036224 train.py:394] {'eval/walltime': 754.383496761322, 'training/sps': 40560.59637279737, 'training/walltime': 578.2209033966064, 'training/entropy_loss': Array(-0.03960642, dtype=float32), 'training/policy_loss': Array(0.00157797, dtype=float32), 'training/total_loss': Array(213.13853, dtype=float32), 'training/v_loss': Array(213.17656, dtype=float32), 'eval/episode_goal_distance': (Array(0.41145447, dtype=float32), Array(0.15036136, dtype=float32)), 'eval/episode_reward': (Array(-8150.489, dtype=float32), Array(4778.5034, dtype=float32)), 'eval/avg_episode_length': (Array(813.6172, dtype=float32), Array(387.98755, dtype=float32)), 'eval/epoch_eval_time': 4.079368829727173, 'eval/sps': 31377.403059816144}
I0726 21:50:48.926054 140267183036224 train.py:379] starting iteration 185 1341.347080230713
I0726 21:50:56.035192 140267183036224 train.py:394] {'eval/walltime': 758.4573729038239, 'training/sps': 40530.65167791806, 'training/walltime': 581.2526829242706, 'training/entropy_loss': Array(-0.03766489, dtype=float32), 'training/policy_loss': Array(0.00211501, dtype=float32), 'training/total_loss': Array(170.20836, dtype=float32), 'training/v_loss': Array(170.24391, dtype=float32), 'eval/episode_goal_distance': (Array(0.41143435, dtype=float32), Array(0.16968955, dtype=float32)), 'eval/episode_reward': (Array(-8451.702, dtype=float32), Array(4904.8765, dtype=float32)), 'eval/avg_episode_length': (Array(836.8594, dtype=float32), Array(368.25244, dtype=float32)), 'eval/epoch_eval_time': 4.073876142501831, 'eval/sps': 31419.708288282225}
I0726 21:50:56.037471 140267183036224 train.py:379] starting iteration 186 1348.4584975242615
I0726 21:51:03.138182 140267183036224 train.py:394] {'eval/walltime': 762.5314333438873, 'training/sps': 40645.724273836604, 'training/walltime': 584.2758791446686, 'training/entropy_loss': Array(-0.03659946, dtype=float32), 'training/policy_loss': Array(0.00459658, dtype=float32), 'training/total_loss': Array(151.77145, dtype=float32), 'training/v_loss': Array(151.80347, dtype=float32), 'eval/episode_goal_distance': (Array(0.42603666, dtype=float32), Array(0.17731178, dtype=float32)), 'eval/episode_reward': (Array(-8558.799, dtype=float32), Array(4676.7207, dtype=float32)), 'eval/avg_episode_length': (Array(844.6172, dtype=float32), Array(361.07764, dtype=float32)), 'eval/epoch_eval_time': 4.074060440063477, 'eval/sps': 31418.286960417718}
I0726 21:51:03.140339 140267183036224 train.py:379] starting iteration 187 1355.5613656044006
I0726 21:51:10.256809 140267183036224 train.py:394] {'eval/walltime': 766.6150407791138, 'training/sps': 40562.952230975425, 'training/walltime': 587.3052444458008, 'training/entropy_loss': Array(-0.0381656, dtype=float32), 'training/policy_loss': Array(0.00246724, dtype=float32), 'training/total_loss': Array(155.0354, dtype=float32), 'training/v_loss': Array(155.07109, dtype=float32), 'eval/episode_goal_distance': (Array(0.42442033, dtype=float32), Array(0.191251, dtype=float32)), 'eval/episode_reward': (Array(-8652.053, dtype=float32), Array(4781.6274, dtype=float32)), 'eval/avg_episode_length': (Array(860.21875, dtype=float32), Array(345.5492, dtype=float32)), 'eval/epoch_eval_time': 4.08360743522644, 'eval/sps': 31344.83469097275}
I0726 21:51:10.259169 140267183036224 train.py:379] starting iteration 188 1362.680195569992
I0726 21:51:17.373388 140267183036224 train.py:394] {'eval/walltime': 770.6964426040649, 'training/sps': 40564.49422128643, 'training/walltime': 590.3344945907593, 'training/entropy_loss': Array(-0.03774025, dtype=float32), 'training/policy_loss': Array(0.00263949, dtype=float32), 'training/total_loss': Array(131.445, dtype=float32), 'training/v_loss': Array(131.48012, dtype=float32), 'eval/episode_goal_distance': (Array(0.39951414, dtype=float32), Array(0.16468309, dtype=float32)), 'eval/episode_reward': (Array(-8449.186, dtype=float32), Array(4658.5605, dtype=float32)), 'eval/avg_episode_length': (Array(867.9375, dtype=float32), Array(337.45612, dtype=float32)), 'eval/epoch_eval_time': 4.081401824951172, 'eval/sps': 31361.77359883729}
I0726 21:51:17.375705 140267183036224 train.py:379] starting iteration 189 1369.796730041504
I0726 21:51:24.488140 140267183036224 train.py:394] {'eval/walltime': 774.7770266532898, 'training/sps': 40575.30740871077, 'training/walltime': 593.3629374504089, 'training/entropy_loss': Array(-0.03558158, dtype=float32), 'training/policy_loss': Array(0.00183439, dtype=float32), 'training/total_loss': Array(119.33383, dtype=float32), 'training/v_loss': Array(119.367584, dtype=float32), 'eval/episode_goal_distance': (Array(0.37829453, dtype=float32), Array(0.13140966, dtype=float32)), 'eval/episode_reward': (Array(-7580.157, dtype=float32), Array(4387.5396, dtype=float32)), 'eval/avg_episode_length': (Array(821.4375, dtype=float32), Array(381.52402, dtype=float32)), 'eval/epoch_eval_time': 4.0805840492248535, 'eval/sps': 31368.058703340477}
I0726 21:51:24.490291 140267183036224 train.py:379] starting iteration 190 1376.9113171100616
I0726 21:51:31.588135 140267183036224 train.py:394] {'eval/walltime': 778.8564035892487, 'training/sps': 40756.02089153407, 'training/walltime': 596.3779520988464, 'training/entropy_loss': Array(-0.03633046, dtype=float32), 'training/policy_loss': Array(0.00184952, dtype=float32), 'training/total_loss': Array(120.96611, dtype=float32), 'training/v_loss': Array(121.00059, dtype=float32), 'eval/episode_goal_distance': (Array(0.41837856, dtype=float32), Array(0.18025686, dtype=float32)), 'eval/episode_reward': (Array(-8845.318, dtype=float32), Array(4860.469, dtype=float32)), 'eval/avg_episode_length': (Array(875.71875, dtype=float32), Array(328.818, dtype=float32)), 'eval/epoch_eval_time': 4.079376935958862, 'eval/sps': 31377.340708995664}
I0726 21:51:31.590477 140267183036224 train.py:379] starting iteration 191 1384.011502981186
I0726 21:51:38.705658 140267183036224 train.py:394] {'eval/walltime': 782.9412069320679, 'training/sps': 40596.87114365187, 'training/walltime': 599.4047863483429, 'training/entropy_loss': Array(-0.037159, dtype=float32), 'training/policy_loss': Array(0.00152359, dtype=float32), 'training/total_loss': Array(129.95601, dtype=float32), 'training/v_loss': Array(129.99164, dtype=float32), 'eval/episode_goal_distance': (Array(0.41375685, dtype=float32), Array(0.1680514, dtype=float32)), 'eval/episode_reward': (Array(-8083.427, dtype=float32), Array(5192.25, dtype=float32)), 'eval/avg_episode_length': (Array(782.5, dtype=float32), Array(411.03708, dtype=float32)), 'eval/epoch_eval_time': 4.084803342819214, 'eval/sps': 31335.657865883473}
I0726 21:51:38.707891 140267183036224 train.py:379] starting iteration 192 1391.1289176940918
I0726 21:51:45.815075 140267183036224 train.py:394] {'eval/walltime': 787.0165960788727, 'training/sps': 40576.05490043439, 'training/walltime': 602.433173418045, 'training/entropy_loss': Array(-0.03819518, dtype=float32), 'training/policy_loss': Array(0.00277407, dtype=float32), 'training/total_loss': Array(126.81593, dtype=float32), 'training/v_loss': Array(126.851364, dtype=float32), 'eval/episode_goal_distance': (Array(0.4008342, dtype=float32), Array(0.16492818, dtype=float32)), 'eval/episode_reward': (Array(-8164.744, dtype=float32), Array(5300.6934, dtype=float32)), 'eval/avg_episode_length': (Array(790.1953, dtype=float32), Array(405.78387, dtype=float32)), 'eval/epoch_eval_time': 4.07538914680481, 'eval/sps': 31408.043597592314}
I0726 21:51:45.817522 140267183036224 train.py:379] starting iteration 193 1398.2385475635529
I0726 21:51:52.920910 140267183036224 train.py:394] {'eval/walltime': 791.0938045978546, 'training/sps': 40651.74178896001, 'training/walltime': 605.45592212677, 'training/entropy_loss': Array(-0.03846473, dtype=float32), 'training/policy_loss': Array(0.00137527, dtype=float32), 'training/total_loss': Array(132.21707, dtype=float32), 'training/v_loss': Array(132.25415, dtype=float32), 'eval/episode_goal_distance': (Array(0.40694687, dtype=float32), Array(0.17521285, dtype=float32)), 'eval/episode_reward': (Array(-8242.003, dtype=float32), Array(5101.0493, dtype=float32)), 'eval/avg_episode_length': (Array(797.84375, dtype=float32), Array(400.40643, dtype=float32)), 'eval/epoch_eval_time': 4.077208518981934, 'eval/sps': 31394.028390768006}
I0726 21:51:52.923338 140267183036224 train.py:379] starting iteration 194 1405.344363451004
I0726 21:52:00.035412 140267183036224 train.py:394] {'eval/walltime': 795.1777191162109, 'training/sps': 40625.267794203784, 'training/walltime': 608.4806406497955, 'training/entropy_loss': Array(-0.03935168, dtype=float32), 'training/policy_loss': Array(0.0013134, dtype=float32), 'training/total_loss': Array(135.45415, dtype=float32), 'training/v_loss': Array(135.49219, dtype=float32), 'eval/episode_goal_distance': (Array(0.39758146, dtype=float32), Array(0.14931343, dtype=float32)), 'eval/episode_reward': (Array(-7228.4717, dtype=float32), Array(5183.3276, dtype=float32)), 'eval/avg_episode_length': (Array(735.9531, dtype=float32), Array(439.04248, dtype=float32)), 'eval/epoch_eval_time': 4.083914518356323, 'eval/sps': 31342.47776849083}
I0726 21:52:00.037747 140267183036224 train.py:379] starting iteration 195 1412.458773612976
I0726 21:52:07.151805 140267183036224 train.py:394] {'eval/walltime': 799.2579965591431, 'training/sps': 40551.513877275895, 'training/walltime': 611.5108604431152, 'training/entropy_loss': Array(-0.03944348, dtype=float32), 'training/policy_loss': Array(0.0016541, dtype=float32), 'training/total_loss': Array(137.61249, dtype=float32), 'training/v_loss': Array(137.65027, dtype=float32), 'eval/episode_goal_distance': (Array(0.41038734, dtype=float32), Array(0.16344239, dtype=float32)), 'eval/episode_reward': (Array(-7853.3, dtype=float32), Array(5052.8867, dtype=float32)), 'eval/avg_episode_length': (Array(782.4922, dtype=float32), Array(411.05197, dtype=float32)), 'eval/epoch_eval_time': 4.080277442932129, 'eval/sps': 31370.415808788213}
I0726 21:52:07.154080 140267183036224 train.py:379] starting iteration 196 1419.5751054286957
I0726 21:52:14.265720 140267183036224 train.py:394] {'eval/walltime': 803.3314163684845, 'training/sps': 40490.14308275945, 'training/walltime': 614.5456731319427, 'training/entropy_loss': Array(-0.03930406, dtype=float32), 'training/policy_loss': Array(0.00181318, dtype=float32), 'training/total_loss': Array(179.09764, dtype=float32), 'training/v_loss': Array(179.13513, dtype=float32), 'eval/episode_goal_distance': (Array(0.44073004, dtype=float32), Array(0.20358096, dtype=float32)), 'eval/episode_reward': (Array(-9679.891, dtype=float32), Array(4595.5205, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.08287, dtype=float32)), 'eval/epoch_eval_time': 4.073419809341431, 'eval/sps': 31423.228145172292}
I0726 21:52:14.267887 140267183036224 train.py:379] starting iteration 197 1426.6889126300812
I0726 21:52:21.379974 140267183036224 train.py:394] {'eval/walltime': 807.4163677692413, 'training/sps': 40639.279141695675, 'training/walltime': 617.5693488121033, 'training/entropy_loss': Array(-0.04339236, dtype=float32), 'training/policy_loss': Array(0.0003325, dtype=float32), 'training/total_loss': Array(168.54718, dtype=float32), 'training/v_loss': Array(168.59024, dtype=float32), 'eval/episode_goal_distance': (Array(0.43196356, dtype=float32), Array(0.19246823, dtype=float32)), 'eval/episode_reward': (Array(-9218.008, dtype=float32), Array(4193.835, dtype=float32)), 'eval/avg_episode_length': (Array(922.3125, dtype=float32), Array(266.86575, dtype=float32)), 'eval/epoch_eval_time': 4.084951400756836, 'eval/sps': 31334.522113600888}
I0726 21:52:21.382175 140267183036224 train.py:379] starting iteration 198 1433.8032014369965
I0726 21:52:28.497904 140267183036224 train.py:394] {'eval/walltime': 811.5025932788849, 'training/sps': 40607.27287940612, 'training/walltime': 620.5954077243805, 'training/entropy_loss': Array(-0.04369197, dtype=float32), 'training/policy_loss': Array(8.0529164e-05, dtype=float32), 'training/total_loss': Array(149.43408, dtype=float32), 'training/v_loss': Array(149.47769, dtype=float32), 'eval/episode_goal_distance': (Array(0.4440335, dtype=float32), Array(0.19596937, dtype=float32)), 'eval/episode_reward': (Array(-9557.574, dtype=float32), Array(3963.2603, dtype=float32)), 'eval/avg_episode_length': (Array(937.91406, dtype=float32), Array(240.45874, dtype=float32)), 'eval/epoch_eval_time': 4.086225509643555, 'eval/sps': 31324.751827308122}
I0726 21:52:28.500293 140267183036224 train.py:379] starting iteration 199 1440.9213190078735
I0726 21:52:35.614853 140267183036224 train.py:394] {'eval/walltime': 815.5852999687195, 'training/sps': 40573.41644150637, 'training/walltime': 623.623991727829, 'training/entropy_loss': Array(-0.04146611, dtype=float32), 'training/policy_loss': Array(0.00048793, dtype=float32), 'training/total_loss': Array(136.95554, dtype=float32), 'training/v_loss': Array(136.9965, dtype=float32), 'eval/episode_goal_distance': (Array(0.4229728, dtype=float32), Array(0.17511386, dtype=float32)), 'eval/episode_reward': (Array(-9592.617, dtype=float32), Array(3720.3882, dtype=float32)), 'eval/avg_episode_length': (Array(945.5781, dtype=float32), Array(226.26508, dtype=float32)), 'eval/epoch_eval_time': 4.082706689834595, 'eval/sps': 31351.75013152506}
I0726 21:52:35.616949 140267183036224 train.py:379] starting iteration 200 1448.0379753112793
I0726 21:52:42.729658 140267183036224 train.py:394] {'eval/walltime': 819.6639597415924, 'training/sps': 40545.328237935704, 'training/walltime': 626.6546738147736, 'training/entropy_loss': Array(-0.03983238, dtype=float32), 'training/policy_loss': Array(0.00011098, dtype=float32), 'training/total_loss': Array(293.12488, dtype=float32), 'training/v_loss': Array(293.16458, dtype=float32), 'eval/episode_goal_distance': (Array(0.4033003, dtype=float32), Array(0.16081107, dtype=float32)), 'eval/episode_reward': (Array(-8735.437, dtype=float32), Array(4420.2056, dtype=float32)), 'eval/avg_episode_length': (Array(883.5469, dtype=float32), Array(319.62833, dtype=float32)), 'eval/epoch_eval_time': 4.078659772872925, 'eval/sps': 31382.85788172996}
I0726 21:52:42.732028 140267183036224 train.py:379] starting iteration 201 1455.1530542373657
I0726 21:52:49.838285 140267183036224 train.py:394] {'eval/walltime': 823.7461287975311, 'training/sps': 40679.23919298531, 'training/walltime': 629.6753792762756, 'training/entropy_loss': Array(-0.0356815, dtype=float32), 'training/policy_loss': Array(0.0003566, dtype=float32), 'training/total_loss': Array(145.60875, dtype=float32), 'training/v_loss': Array(145.64407, dtype=float32), 'eval/episode_goal_distance': (Array(0.39892888, dtype=float32), Array(0.15765922, dtype=float32)), 'eval/episode_reward': (Array(-9279.547, dtype=float32), Array(3820.8093, dtype=float32)), 'eval/avg_episode_length': (Array(961.1719, dtype=float32), Array(192.5814, dtype=float32)), 'eval/epoch_eval_time': 4.082169055938721, 'eval/sps': 31355.879250955124}
I0726 21:52:49.840659 140267183036224 train.py:379] starting iteration 202 1462.2616846561432
I0726 21:52:56.969572 140267183036224 train.py:394] {'eval/walltime': 827.8349990844727, 'training/sps': 40494.05921700204, 'training/walltime': 632.7098984718323, 'training/entropy_loss': Array(-0.03392145, dtype=float32), 'training/policy_loss': Array(5.8347752e-05, dtype=float32), 'training/total_loss': Array(116.19493, dtype=float32), 'training/v_loss': Array(116.22878, dtype=float32), 'eval/episode_goal_distance': (Array(0.40622228, dtype=float32), Array(0.16785957, dtype=float32)), 'eval/episode_reward': (Array(-8645.932, dtype=float32), Array(4370.616, dtype=float32)), 'eval/avg_episode_length': (Array(899.03906, dtype=float32), Array(300.2833, dtype=float32)), 'eval/epoch_eval_time': 4.088870286941528, 'eval/sps': 31304.490242399912}
I0726 21:52:56.971901 140267183036224 train.py:379] starting iteration 203 1469.3929271697998
I0726 21:53:04.097250 140267183036224 train.py:394] {'eval/walltime': 831.9276027679443, 'training/sps': 40564.45590954797, 'training/walltime': 635.7391514778137, 'training/entropy_loss': Array(-0.03604116, dtype=float32), 'training/policy_loss': Array(0.0005499, dtype=float32), 'training/total_loss': Array(122.07338, dtype=float32), 'training/v_loss': Array(122.10887, dtype=float32), 'eval/episode_goal_distance': (Array(0.38891435, dtype=float32), Array(0.15031703, dtype=float32)), 'eval/episode_reward': (Array(-8669.354, dtype=float32), Array(3878.051, dtype=float32)), 'eval/avg_episode_length': (Array(914.5625, dtype=float32), Array(278.64166, dtype=float32)), 'eval/epoch_eval_time': 4.09260368347168, 'eval/sps': 31275.9333421261}
I0726 21:53:04.099501 140267183036224 train.py:379] starting iteration 204 1476.5205266475677
I0726 21:53:11.219321 140267183036224 train.py:394] {'eval/walltime': 836.0112974643707, 'training/sps': 40518.88749728083, 'training/walltime': 638.771811246872, 'training/entropy_loss': Array(-0.03725749, dtype=float32), 'training/policy_loss': Array(0.00097834, dtype=float32), 'training/total_loss': Array(121.73894, dtype=float32), 'training/v_loss': Array(121.77522, dtype=float32), 'eval/episode_goal_distance': (Array(0.4026712, dtype=float32), Array(0.15248954, dtype=float32)), 'eval/episode_reward': (Array(-8786.686, dtype=float32), Array(4108.586, dtype=float32)), 'eval/avg_episode_length': (Array(906.8672, dtype=float32), Array(289.5624, dtype=float32)), 'eval/epoch_eval_time': 4.083694696426392, 'eval/sps': 31344.164908314957}
I0726 21:53:11.523677 140267183036224 train.py:379] starting iteration 205 1483.9446892738342
I0726 21:53:18.651123 140267183036224 train.py:394] {'eval/walltime': 840.1022942066193, 'training/sps': 40519.260201041216, 'training/walltime': 641.8044431209564, 'training/entropy_loss': Array(-0.0356527, dtype=float32), 'training/policy_loss': Array(0.00064226, dtype=float32), 'training/total_loss': Array(109.56876, dtype=float32), 'training/v_loss': Array(109.603775, dtype=float32), 'eval/episode_goal_distance': (Array(0.3846383, dtype=float32), Array(0.14070305, dtype=float32)), 'eval/episode_reward': (Array(-8806.88, dtype=float32), Array(3691.9033, dtype=float32)), 'eval/avg_episode_length': (Array(937.8828, dtype=float32), Array(240.57939, dtype=float32)), 'eval/epoch_eval_time': 4.090996742248535, 'eval/sps': 31288.218511180563}
I0726 21:53:18.653508 140267183036224 train.py:379] starting iteration 206 1491.074533700943
I0726 21:53:25.774433 140267183036224 train.py:394] {'eval/walltime': 844.1888935565948, 'training/sps': 40539.629167329025, 'training/walltime': 644.8355512619019, 'training/entropy_loss': Array(-0.03439035, dtype=float32), 'training/policy_loss': Array(0.0007577, dtype=float32), 'training/total_loss': Array(102.30287, dtype=float32), 'training/v_loss': Array(102.3365, dtype=float32), 'eval/episode_goal_distance': (Array(0.40145072, dtype=float32), Array(0.16177459, dtype=float32)), 'eval/episode_reward': (Array(-8771.621, dtype=float32), Array(4547.3804, dtype=float32)), 'eval/avg_episode_length': (Array(875.8125, dtype=float32), Array(328.5699, dtype=float32)), 'eval/epoch_eval_time': 4.086599349975586, 'eval/sps': 31321.886252628286}
I0726 21:53:25.776682 140267183036224 train.py:379] starting iteration 207 1498.1977081298828
I0726 21:53:32.895144 140267183036224 train.py:394] {'eval/walltime': 848.2755515575409, 'training/sps': 40577.98125604216, 'training/walltime': 647.8637945652008, 'training/entropy_loss': Array(-0.03539006, dtype=float32), 'training/policy_loss': Array(0.00081706, dtype=float32), 'training/total_loss': Array(99.5381, dtype=float32), 'training/v_loss': Array(99.57267, dtype=float32), 'eval/episode_goal_distance': (Array(0.39072472, dtype=float32), Array(0.14668652, dtype=float32)), 'eval/episode_reward': (Array(-8127.984, dtype=float32), Array(4385.9526, dtype=float32)), 'eval/avg_episode_length': (Array(844.6406, dtype=float32), Array(361.0231, dtype=float32)), 'eval/epoch_eval_time': 4.086658000946045, 'eval/sps': 31321.436726628093}
I0726 21:53:32.897609 140267183036224 train.py:379] starting iteration 208 1505.318635225296
I0726 21:53:40.028255 140267183036224 train.py:394] {'eval/walltime': 852.3847963809967, 'training/sps': 40716.25731164164, 'training/walltime': 650.8817536830902, 'training/entropy_loss': Array(-0.03771256, dtype=float32), 'training/policy_loss': Array(0.00137959, dtype=float32), 'training/total_loss': Array(107.885544, dtype=float32), 'training/v_loss': Array(107.92187, dtype=float32), 'eval/episode_goal_distance': (Array(0.3689529, dtype=float32), Array(0.13832276, dtype=float32)), 'eval/episode_reward': (Array(-8465.564, dtype=float32), Array(3507.0864, dtype=float32)), 'eval/avg_episode_length': (Array(937.8281, dtype=float32), Array(240.79097, dtype=float32)), 'eval/epoch_eval_time': 4.1092448234558105, 'eval/sps': 31149.275718343306}
I0726 21:53:40.030492 140267183036224 train.py:379] starting iteration 209 1512.4515187740326
I0726 21:53:47.154007 140267183036224 train.py:394] {'eval/walltime': 856.4818735122681, 'training/sps': 40649.04859443842, 'training/walltime': 653.9047026634216, 'training/entropy_loss': Array(-0.037673, dtype=float32), 'training/policy_loss': Array(0.0013619, dtype=float32), 'training/total_loss': Array(108.52254, dtype=float32), 'training/v_loss': Array(108.55885, dtype=float32), 'eval/episode_goal_distance': (Array(0.38947737, dtype=float32), Array(0.16703254, dtype=float32)), 'eval/episode_reward': (Array(-8808.232, dtype=float32), Array(3759.2942, dtype=float32)), 'eval/avg_episode_length': (Array(937.8281, dtype=float32), Array(240.79068, dtype=float32)), 'eval/epoch_eval_time': 4.097077131271362, 'eval/sps': 31241.78430106352}
I0726 21:53:47.156360 140267183036224 train.py:379] starting iteration 210 1519.577386379242
I0726 21:53:54.272746 140267183036224 train.py:394] {'eval/walltime': 860.5660197734833, 'training/sps': 40570.36315710674, 'training/walltime': 656.9335145950317, 'training/entropy_loss': Array(-0.03871817, dtype=float32), 'training/policy_loss': Array(0.00066499, dtype=float32), 'training/total_loss': Array(111.707054, dtype=float32), 'training/v_loss': Array(111.74509, dtype=float32), 'eval/episode_goal_distance': (Array(0.4140706, dtype=float32), Array(0.16643715, dtype=float32)), 'eval/episode_reward': (Array(-9670.453, dtype=float32), Array(3751.8477, dtype=float32)), 'eval/avg_episode_length': (Array(953.5078, dtype=float32), Array(209.64508, dtype=float32)), 'eval/epoch_eval_time': 4.08414626121521, 'eval/sps': 31340.699331838932}
I0726 21:53:54.275081 140267183036224 train.py:379] starting iteration 211 1526.6961073875427
I0726 21:54:01.403050 140267183036224 train.py:394] {'eval/walltime': 864.6572904586792, 'training/sps': 40511.78832217194, 'training/walltime': 659.9667057991028, 'training/entropy_loss': Array(-0.03817554, dtype=float32), 'training/policy_loss': Array(0.00067222, dtype=float32), 'training/total_loss': Array(109.275085, dtype=float32), 'training/v_loss': Array(109.31259, dtype=float32), 'eval/episode_goal_distance': (Array(0.39819738, dtype=float32), Array(0.15035808, dtype=float32)), 'eval/episode_reward': (Array(-8829.01, dtype=float32), Array(3839.9824, dtype=float32)), 'eval/avg_episode_length': (Array(922.4375, dtype=float32), Array(266.43643, dtype=float32)), 'eval/epoch_eval_time': 4.091270685195923, 'eval/sps': 31286.123517361535}
I0726 21:54:01.405274 140267183036224 train.py:379] starting iteration 212 1533.8263006210327
I0726 21:54:08.517445 140267183036224 train.py:394] {'eval/walltime': 868.7390687465668, 'training/sps': 40595.96300398866, 'training/walltime': 662.9936077594757, 'training/entropy_loss': Array(-0.03615214, dtype=float32), 'training/policy_loss': Array(0.00056335, dtype=float32), 'training/total_loss': Array(101.147156, dtype=float32), 'training/v_loss': Array(101.18275, dtype=float32), 'eval/episode_goal_distance': (Array(0.39533734, dtype=float32), Array(0.16572814, dtype=float32)), 'eval/episode_reward': (Array(-9161.742, dtype=float32), Array(3689.1885, dtype=float32)), 'eval/avg_episode_length': (Array(945.72656, dtype=float32), Array(225.64798, dtype=float32)), 'eval/epoch_eval_time': 4.081778287887573, 'eval/sps': 31358.8810984252}
I0726 21:54:08.519652 140267183036224 train.py:379] starting iteration 213 1540.9406781196594
I0726 21:54:15.657695 140267183036224 train.py:394] {'eval/walltime': 872.8525824546814, 'training/sps': 40674.37551020923, 'training/walltime': 666.0146744251251, 'training/entropy_loss': Array(-0.03750513, dtype=float32), 'training/policy_loss': Array(0.00049684, dtype=float32), 'training/total_loss': Array(141.89636, dtype=float32), 'training/v_loss': Array(141.93335, dtype=float32), 'eval/episode_goal_distance': (Array(0.40389502, dtype=float32), Array(0.18312907, dtype=float32)), 'eval/episode_reward': (Array(-9258.275, dtype=float32), Array(3758.3762, dtype=float32)), 'eval/avg_episode_length': (Array(976.7031, dtype=float32), Array(150.38074, dtype=float32)), 'eval/epoch_eval_time': 4.113513708114624, 'eval/sps': 31116.94990769027}
I0726 21:54:15.659946 140267183036224 train.py:379] starting iteration 214 1548.080971956253
I0726 21:54:22.783313 140267183036224 train.py:394] {'eval/walltime': 876.9483382701874, 'training/sps': 40632.544544574215, 'training/walltime': 669.0388512611389, 'training/entropy_loss': Array(-0.03915808, dtype=float32), 'training/policy_loss': Array(7.195737e-05, dtype=float32), 'training/total_loss': Array(133.70811, dtype=float32), 'training/v_loss': Array(133.74721, dtype=float32), 'eval/episode_goal_distance': (Array(0.40702513, dtype=float32), Array(0.14740734, dtype=float32)), 'eval/episode_reward': (Array(-9372.303, dtype=float32), Array(3911.0046, dtype=float32)), 'eval/avg_episode_length': (Array(930.1875, dtype=float32), Array(253.85524, dtype=float32)), 'eval/epoch_eval_time': 4.0957558155059814, 'eval/sps': 31251.863090912106}
I0726 21:54:22.785454 140267183036224 train.py:379] starting iteration 215 1555.2064797878265
I0726 21:54:29.915596 140267183036224 train.py:394] {'eval/walltime': 881.04745221138, 'training/sps': 40586.049657417745, 'training/walltime': 672.0664925575256, 'training/entropy_loss': Array(-0.04025119, dtype=float32), 'training/policy_loss': Array(0.00077275, dtype=float32), 'training/total_loss': Array(125.27781, dtype=float32), 'training/v_loss': Array(125.3173, dtype=float32), 'eval/episode_goal_distance': (Array(0.40450114, dtype=float32), Array(0.17430592, dtype=float32)), 'eval/episode_reward': (Array(-8499.1045, dtype=float32), Array(4324.1704, dtype=float32)), 'eval/avg_episode_length': (Array(883.4297, dtype=float32), Array(319.95004, dtype=float32)), 'eval/epoch_eval_time': 4.099113941192627, 'eval/sps': 31226.260561754163}
I0726 21:54:29.917710 140267183036224 train.py:379] starting iteration 216 1562.3387362957
I0726 21:54:37.035357 140267183036224 train.py:394] {'eval/walltime': 885.1348333358765, 'training/sps': 40597.48831809012, 'training/walltime': 675.0932807922363, 'training/entropy_loss': Array(-0.04063449, dtype=float32), 'training/policy_loss': Array(0.00092492, dtype=float32), 'training/total_loss': Array(231.5188, dtype=float32), 'training/v_loss': Array(231.5585, dtype=float32), 'eval/episode_goal_distance': (Array(0.38808253, dtype=float32), Array(0.15428947, dtype=float32)), 'eval/episode_reward': (Array(-8616.896, dtype=float32), Array(4109.8604, dtype=float32)), 'eval/avg_episode_length': (Array(906.78125, dtype=float32), Array(289.82928, dtype=float32)), 'eval/epoch_eval_time': 4.08738112449646, 'eval/sps': 31315.895460022413}
I0726 21:54:37.039965 140267183036224 train.py:379] starting iteration 217 1569.460975408554
I0726 21:54:44.167182 140267183036224 train.py:394] {'eval/walltime': 889.2255208492279, 'training/sps': 40543.72710607151, 'training/walltime': 678.1240825653076, 'training/entropy_loss': Array(-0.04134848, dtype=float32), 'training/policy_loss': Array(0.000764, dtype=float32), 'training/total_loss': Array(205.49973, dtype=float32), 'training/v_loss': Array(205.54031, dtype=float32), 'eval/episode_goal_distance': (Array(0.4073541, dtype=float32), Array(0.16730115, dtype=float32)), 'eval/episode_reward': (Array(-8548.285, dtype=float32), Array(4433.508, dtype=float32)), 'eval/avg_episode_length': (Array(875.64844, dtype=float32), Array(329.00375, dtype=float32)), 'eval/epoch_eval_time': 4.09068751335144, 'eval/sps': 31290.583693382992}
I0726 21:54:44.169382 140267183036224 train.py:379] starting iteration 218 1576.590408563614
I0726 21:54:51.307484 140267183036224 train.py:394] {'eval/walltime': 893.3417451381683, 'training/sps': 40708.36535001404, 'training/walltime': 681.1426267623901, 'training/entropy_loss': Array(-0.04069497, dtype=float32), 'training/policy_loss': Array(0.00057072, dtype=float32), 'training/total_loss': Array(147.84845, dtype=float32), 'training/v_loss': Array(147.88857, dtype=float32), 'eval/episode_goal_distance': (Array(0.41395545, dtype=float32), Array(0.16113643, dtype=float32)), 'eval/episode_reward': (Array(-8770.476, dtype=float32), Array(4191.333, dtype=float32)), 'eval/avg_episode_length': (Array(891.3594, dtype=float32), Array(310.0143, dtype=float32)), 'eval/epoch_eval_time': 4.11622428894043, 'eval/sps': 31096.459039881152}
I0726 21:54:51.309840 140267183036224 train.py:379] starting iteration 219 1583.7308661937714
I0726 21:54:58.440754 140267183036224 train.py:394] {'eval/walltime': 897.4460399150848, 'training/sps': 40645.32359688775, 'training/walltime': 684.1658527851105, 'training/entropy_loss': Array(-0.04202019, dtype=float32), 'training/policy_loss': Array(0.00035007, dtype=float32), 'training/total_loss': Array(140.69856, dtype=float32), 'training/v_loss': Array(140.74023, dtype=float32), 'eval/episode_goal_distance': (Array(0.4373971, dtype=float32), Array(0.18915614, dtype=float32)), 'eval/episode_reward': (Array(-8964.469, dtype=float32), Array(4680.0767, dtype=float32)), 'eval/avg_episode_length': (Array(867.96094, dtype=float32), Array(337.39645, dtype=float32)), 'eval/epoch_eval_time': 4.104294776916504, 'eval/sps': 31186.843771529613}
I0726 21:54:58.443242 140267183036224 train.py:379] starting iteration 220 1590.864268541336
I0726 21:55:05.573627 140267183036224 train.py:394] {'eval/walltime': 901.5479419231415, 'training/sps': 40618.4257994065, 'training/walltime': 687.1910808086395, 'training/entropy_loss': Array(-0.04235076, dtype=float32), 'training/policy_loss': Array(0.00085982, dtype=float32), 'training/total_loss': Array(145.3468, dtype=float32), 'training/v_loss': Array(145.38829, dtype=float32), 'eval/episode_goal_distance': (Array(0.384247, dtype=float32), Array(0.18712799, dtype=float32)), 'eval/episode_reward': (Array(-8620.415, dtype=float32), Array(4612.757, dtype=float32)), 'eval/avg_episode_length': (Array(883.5, dtype=float32), Array(319.7573, dtype=float32)), 'eval/epoch_eval_time': 4.101902008056641, 'eval/sps': 31205.0360414735}
I0726 21:55:05.575870 140267183036224 train.py:379] starting iteration 221 1597.9968960285187
I0726 21:55:12.706895 140267183036224 train.py:394] {'eval/walltime': 905.6465573310852, 'training/sps': 40567.770142555004, 'training/walltime': 690.2200863361359, 'training/entropy_loss': Array(-0.04244677, dtype=float32), 'training/policy_loss': Array(0.00156636, dtype=float32), 'training/total_loss': Array(131.21432, dtype=float32), 'training/v_loss': Array(131.2552, dtype=float32), 'eval/episode_goal_distance': (Array(0.4222592, dtype=float32), Array(0.18954252, dtype=float32)), 'eval/episode_reward': (Array(-8969.945, dtype=float32), Array(4626.2456, dtype=float32)), 'eval/avg_episode_length': (Array(875.71094, dtype=float32), Array(328.83835, dtype=float32)), 'eval/epoch_eval_time': 4.098615407943726, 'eval/sps': 31230.05875396774}
I0726 21:55:12.709173 140267183036224 train.py:379] starting iteration 222 1605.1301996707916
I0726 21:55:19.842376 140267183036224 train.py:394] {'eval/walltime': 909.7462096214294, 'training/sps': 40549.90587613837, 'training/walltime': 693.2504262924194, 'training/entropy_loss': Array(-0.04234016, dtype=float32), 'training/policy_loss': Array(0.00031309, dtype=float32), 'training/total_loss': Array(124.38099, dtype=float32), 'training/v_loss': Array(124.423004, dtype=float32), 'eval/episode_goal_distance': (Array(0.40371245, dtype=float32), Array(0.16628428, dtype=float32)), 'eval/episode_reward': (Array(-8435.709, dtype=float32), Array(4742.586, dtype=float32)), 'eval/avg_episode_length': (Array(852.47656, dtype=float32), Array(353.34448, dtype=float32)), 'eval/epoch_eval_time': 4.099652290344238, 'eval/sps': 31222.160060128448}
I0726 21:55:19.844598 140267183036224 train.py:379] starting iteration 223 1612.2656235694885
I0726 21:55:26.964257 140267183036224 train.py:394] {'eval/walltime': 913.8380122184753, 'training/sps': 40628.479876294245, 'training/walltime': 696.2749056816101, 'training/entropy_loss': Array(-0.04141316, dtype=float32), 'training/policy_loss': Array(0.00065394, dtype=float32), 'training/total_loss': Array(120.42609, dtype=float32), 'training/v_loss': Array(120.46684, dtype=float32), 'eval/episode_goal_distance': (Array(0.40220064, dtype=float32), Array(0.16773076, dtype=float32)), 'eval/episode_reward': (Array(-8034.5615, dtype=float32), Array(5103.7627, dtype=float32)), 'eval/avg_episode_length': (Array(805.8594, dtype=float32), Array(394.06345, dtype=float32)), 'eval/epoch_eval_time': 4.091802597045898, 'eval/sps': 31282.05649324588}
I0726 21:55:26.966687 140267183036224 train.py:379] starting iteration 224 1619.3877136707306
I0726 21:55:34.089992 140267183036224 train.py:394] {'eval/walltime': 917.9435677528381, 'training/sps': 40764.12801552244, 'training/walltime': 699.2893207073212, 'training/entropy_loss': Array(-0.04225611, dtype=float32), 'training/policy_loss': Array(0.00047688, dtype=float32), 'training/total_loss': Array(123.80531, dtype=float32), 'training/v_loss': Array(123.84709, dtype=float32), 'eval/episode_goal_distance': (Array(0.41487974, dtype=float32), Array(0.16624361, dtype=float32)), 'eval/episode_reward': (Array(-7530.575, dtype=float32), Array(5348.4155, dtype=float32)), 'eval/avg_episode_length': (Array(743.58594, dtype=float32), Array(435.05795, dtype=float32)), 'eval/epoch_eval_time': 4.105555534362793, 'eval/sps': 31177.266737390844}
I0726 21:55:34.092215 140267183036224 train.py:379] starting iteration 225 1626.5132412910461
I0726 21:55:41.229077 140267183036224 train.py:394] {'eval/walltime': 922.04558801651, 'training/sps': 40535.71059075467, 'training/walltime': 702.3207218647003, 'training/entropy_loss': Array(-0.04044175, dtype=float32), 'training/policy_loss': Array(0.00098325, dtype=float32), 'training/total_loss': Array(118.15154, dtype=float32), 'training/v_loss': Array(118.19101, dtype=float32), 'eval/episode_goal_distance': (Array(0.3823297, dtype=float32), Array(0.14846787, dtype=float32)), 'eval/episode_reward': (Array(-7809.954, dtype=float32), Array(4585.3975, dtype=float32)), 'eval/avg_episode_length': (Array(813.6406, dtype=float32), Array(387.9387, dtype=float32)), 'eval/epoch_eval_time': 4.102020263671875, 'eval/sps': 31204.136443105308}
I0726 21:55:41.231502 140267183036224 train.py:379] starting iteration 226 1633.6525275707245
I0726 21:55:48.355679 140267183036224 train.py:394] {'eval/walltime': 926.1442422866821, 'training/sps': 40660.705642966306, 'training/walltime': 705.3428041934967, 'training/entropy_loss': Array(-0.03961883, dtype=float32), 'training/policy_loss': Array(0.00164986, dtype=float32), 'training/total_loss': Array(116.469315, dtype=float32), 'training/v_loss': Array(116.507286, dtype=float32), 'eval/episode_goal_distance': (Array(0.38968557, dtype=float32), Array(0.14946336, dtype=float32)), 'eval/episode_reward': (Array(-8008.9165, dtype=float32), Array(4378.962, dtype=float32)), 'eval/avg_episode_length': (Array(836.8906, dtype=float32), Array(368.18134, dtype=float32)), 'eval/epoch_eval_time': 4.098654270172119, 'eval/sps': 31229.762639780973}
I0726 21:55:48.357981 140267183036224 train.py:379] starting iteration 227 1640.7790076732635
I0726 21:55:55.487635 140267183036224 train.py:394] {'eval/walltime': 930.2376134395599, 'training/sps': 40545.59297909243, 'training/walltime': 708.3734664916992, 'training/entropy_loss': Array(-0.04096937, dtype=float32), 'training/policy_loss': Array(0.00128258, dtype=float32), 'training/total_loss': Array(121.46154, dtype=float32), 'training/v_loss': Array(121.50122, dtype=float32), 'eval/episode_goal_distance': (Array(0.40546626, dtype=float32), Array(0.16474801, dtype=float32)), 'eval/episode_reward': (Array(-7943.0234, dtype=float32), Array(5356.5815, dtype=float32)), 'eval/avg_episode_length': (Array(759.1953, dtype=float32), Array(425.9621, dtype=float32)), 'eval/epoch_eval_time': 4.093371152877808, 'eval/sps': 31270.06939256186}
I0726 21:55:55.490059 140267183036224 train.py:379] starting iteration 228 1647.911084651947
I0726 21:56:02.617907 140267183036224 train.py:394] {'eval/walltime': 934.3319933414459, 'training/sps': 40552.91141041252, 'training/walltime': 711.4035818576813, 'training/entropy_loss': Array(-0.04045581, dtype=float32), 'training/policy_loss': Array(0.00169155, dtype=float32), 'training/total_loss': Array(119.16215, dtype=float32), 'training/v_loss': Array(119.200905, dtype=float32), 'eval/episode_goal_distance': (Array(0.36718905, dtype=float32), Array(0.13388662, dtype=float32)), 'eval/episode_reward': (Array(-7163.0654, dtype=float32), Array(4613.4897, dtype=float32)), 'eval/avg_episode_length': (Array(774.58594, dtype=float32), Array(416.48584, dtype=float32)), 'eval/epoch_eval_time': 4.094379901885986, 'eval/sps': 31262.365258543694}
I0726 21:56:02.620351 140267183036224 train.py:379] starting iteration 229 1655.0413768291473
I0726 21:56:09.731914 140267183036224 train.py:394] {'eval/walltime': 938.4237639904022, 'training/sps': 40736.805550933925, 'training/walltime': 714.4200186729431, 'training/entropy_loss': Array(-0.03945659, dtype=float32), 'training/policy_loss': Array(0.00259509, dtype=float32), 'training/total_loss': Array(124.207596, dtype=float32), 'training/v_loss': Array(124.24445, dtype=float32), 'eval/episode_goal_distance': (Array(0.36438918, dtype=float32), Array(0.15189023, dtype=float32)), 'eval/episode_reward': (Array(-6913.4355, dtype=float32), Array(4879.2437, dtype=float32)), 'eval/avg_episode_length': (Array(751.47656, dtype=float32), Array(430.45612, dtype=float32)), 'eval/epoch_eval_time': 4.091770648956299, 'eval/sps': 31282.300740059654}
I0726 21:56:09.734187 140267183036224 train.py:379] starting iteration 230 1662.1552135944366
I0726 21:56:16.870476 140267183036224 train.py:394] {'eval/walltime': 942.5309431552887, 'training/sps': 40612.84377331966, 'training/walltime': 717.4456624984741, 'training/entropy_loss': Array(-0.03687092, dtype=float32), 'training/policy_loss': Array(0.00359563, dtype=float32), 'training/total_loss': Array(137.3471, dtype=float32), 'training/v_loss': Array(137.38037, dtype=float32), 'eval/episode_goal_distance': (Array(0.4136357, dtype=float32), Array(0.17028551, dtype=float32)), 'eval/episode_reward': (Array(-7880.295, dtype=float32), Array(5174.054, dtype=float32)), 'eval/avg_episode_length': (Array(782.4844, dtype=float32), Array(411.06644, dtype=float32)), 'eval/epoch_eval_time': 4.107179164886475, 'eval/sps': 31164.94188865949}
I0726 21:56:16.873040 140267183036224 train.py:379] starting iteration 231 1669.2940661907196
I0726 21:56:24.015947 140267183036224 train.py:394] {'eval/walltime': 946.6389501094818, 'training/sps': 40535.806234539414, 'training/walltime': 720.4770565032959, 'training/entropy_loss': Array(-0.03718829, dtype=float32), 'training/policy_loss': Array(0.00368221, dtype=float32), 'training/total_loss': Array(141.11746, dtype=float32), 'training/v_loss': Array(141.15096, dtype=float32), 'eval/episode_goal_distance': (Array(0.4205365, dtype=float32), Array(0.19041826, dtype=float32)), 'eval/episode_reward': (Array(-7715.9204, dtype=float32), Array(5254.9067, dtype=float32)), 'eval/avg_episode_length': (Array(759.21875, dtype=float32), Array(425.92072, dtype=float32)), 'eval/epoch_eval_time': 4.108006954193115, 'eval/sps': 31158.661956340686}
I0726 21:56:24.018456 140267183036224 train.py:379] starting iteration 232 1676.439481973648
I0726 21:56:31.162638 140267183036224 train.py:394] {'eval/walltime': 950.7525970935822, 'training/sps': 40595.85428576627, 'training/walltime': 723.5039665699005, 'training/entropy_loss': Array(-0.03817901, dtype=float32), 'training/policy_loss': Array(0.00303136, dtype=float32), 'training/total_loss': Array(163.0932, dtype=float32), 'training/v_loss': Array(163.12836, dtype=float32), 'eval/episode_goal_distance': (Array(0.43890962, dtype=float32), Array(0.18259071, dtype=float32)), 'eval/episode_reward': (Array(-8523.546, dtype=float32), Array(5188.2554, dtype=float32)), 'eval/avg_episode_length': (Array(813.5156, dtype=float32), Array(388.1989, dtype=float32)), 'eval/epoch_eval_time': 4.113646984100342, 'eval/sps': 31115.9417652348}
I0726 21:56:31.167372 140267183036224 train.py:379] starting iteration 233 1683.5883827209473
I0726 21:56:38.305023 140267183036224 train.py:394] {'eval/walltime': 954.8544185161591, 'training/sps': 40527.292517986774, 'training/walltime': 726.5359973907471, 'training/entropy_loss': Array(-0.03142106, dtype=float32), 'training/policy_loss': Array(0.00543537, dtype=float32), 'training/total_loss': Array(513.3308, dtype=float32), 'training/v_loss': Array(513.3568, dtype=float32), 'eval/episode_goal_distance': (Array(0.4206028, dtype=float32), Array(0.17979293, dtype=float32)), 'eval/episode_reward': (Array(-8264.933, dtype=float32), Array(5251.5747, dtype=float32)), 'eval/avg_episode_length': (Array(798.03906, dtype=float32), Array(400.02, dtype=float32)), 'eval/epoch_eval_time': 4.101821422576904, 'eval/sps': 31205.64910395003}
I0726 21:56:38.307479 140267183036224 train.py:379] starting iteration 234 1690.7285041809082
I0726 21:56:45.442825 140267183036224 train.py:394] {'eval/walltime': 958.956191778183, 'training/sps': 40553.75380568089, 'training/walltime': 729.5660498142242, 'training/entropy_loss': Array(-0.03225181, dtype=float32), 'training/policy_loss': Array(0.00484942, dtype=float32), 'training/total_loss': Array(228.5487, dtype=float32), 'training/v_loss': Array(228.57611, dtype=float32), 'eval/episode_goal_distance': (Array(0.3919757, dtype=float32), Array(0.14473027, dtype=float32)), 'eval/episode_reward': (Array(-7568.5444, dtype=float32), Array(4749.284, dtype=float32)), 'eval/avg_episode_length': (Array(782.46875, dtype=float32), Array(411.09625, dtype=float32)), 'eval/epoch_eval_time': 4.101773262023926, 'eval/sps': 31206.015501900594}
I0726 21:56:45.445039 140267183036224 train.py:379] starting iteration 235 1697.8660645484924
I0726 21:56:52.580423 140267183036224 train.py:394] {'eval/walltime': 963.0612580776215, 'training/sps': 40596.20602329723, 'training/walltime': 732.5929336547852, 'training/entropy_loss': Array(-0.02925387, dtype=float32), 'training/policy_loss': Array(0.00493638, dtype=float32), 'training/total_loss': Array(193.61671, dtype=float32), 'training/v_loss': Array(193.64102, dtype=float32), 'eval/episode_goal_distance': (Array(0.41064337, dtype=float32), Array(0.17309791, dtype=float32)), 'eval/episode_reward': (Array(-8381.091, dtype=float32), Array(4915.8735, dtype=float32)), 'eval/avg_episode_length': (Array(813.4922, dtype=float32), Array(388.24786, dtype=float32)), 'eval/epoch_eval_time': 4.105066299438477, 'eval/sps': 31180.98239180908}
I0726 21:56:52.582895 140267183036224 train.py:379] starting iteration 236 1705.0039212703705
I0726 21:56:59.706982 140267183036224 train.py:394] {'eval/walltime': 967.1645171642303, 'training/sps': 40723.63105215525, 'training/walltime': 735.6103463172913, 'training/entropy_loss': Array(-0.02893859, dtype=float32), 'training/policy_loss': Array(0.00570886, dtype=float32), 'training/total_loss': Array(202.53703, dtype=float32), 'training/v_loss': Array(202.56026, dtype=float32), 'eval/episode_goal_distance': (Array(0.40123594, dtype=float32), Array(0.16755442, dtype=float32)), 'eval/episode_reward': (Array(-8896.646, dtype=float32), Array(4437.249, dtype=float32)), 'eval/avg_episode_length': (Array(875.7578, dtype=float32), Array(328.71463, dtype=float32)), 'eval/epoch_eval_time': 4.103259086608887, 'eval/sps': 31194.715541539157}
I0726 21:56:59.709422 140267183036224 train.py:379] starting iteration 237 1712.130448102951
I0726 21:57:06.850844 140267183036224 train.py:394] {'eval/walltime': 971.2790651321411, 'training/sps': 40643.140847613875, 'training/walltime': 738.633734703064, 'training/entropy_loss': Array(-0.02870801, dtype=float32), 'training/policy_loss': Array(0.00574055, dtype=float32), 'training/total_loss': Array(237.15414, dtype=float32), 'training/v_loss': Array(237.1771, dtype=float32), 'eval/episode_goal_distance': (Array(0.40091842, dtype=float32), Array(0.16906211, dtype=float32)), 'eval/episode_reward': (Array(-8110.324, dtype=float32), Array(4957.051, dtype=float32)), 'eval/avg_episode_length': (Array(805.8203, dtype=float32), Array(394.14282, dtype=float32)), 'eval/epoch_eval_time': 4.114547967910767, 'eval/sps': 31109.12814682635}
I0726 21:57:06.853117 140267183036224 train.py:379] starting iteration 238 1719.2741436958313
I0726 21:57:13.990474 140267183036224 train.py:394] {'eval/walltime': 975.3862001895905, 'training/sps': 40597.46593319705, 'training/walltime': 741.6605246067047, 'training/entropy_loss': Array(-0.03136637, dtype=float32), 'training/policy_loss': Array(0.00606192, dtype=float32), 'training/total_loss': Array(202.76625, dtype=float32), 'training/v_loss': Array(202.79155, dtype=float32), 'eval/episode_goal_distance': (Array(0.4175089, dtype=float32), Array(0.18070039, dtype=float32)), 'eval/episode_reward': (Array(-8261.92, dtype=float32), Array(4719.391, dtype=float32)), 'eval/avg_episode_length': (Array(829.1875, dtype=float32), Array(374.94046, dtype=float32)), 'eval/epoch_eval_time': 4.107135057449341, 'eval/sps': 31165.276575903983}
I0726 21:57:13.993012 140267183036224 train.py:379] starting iteration 239 1726.4140384197235
I0726 21:57:21.132993 140267183036224 train.py:394] {'eval/walltime': 979.4952883720398, 'training/sps': 40587.957784107435, 'training/walltime': 744.6880235671997, 'training/entropy_loss': Array(-0.03775735, dtype=float32), 'training/policy_loss': Array(0.00740072, dtype=float32), 'training/total_loss': Array(221.49236, dtype=float32), 'training/v_loss': Array(221.52272, dtype=float32), 'eval/episode_goal_distance': (Array(0.42935428, dtype=float32), Array(0.18235727, dtype=float32)), 'eval/episode_reward': (Array(-8926.094, dtype=float32), Array(4350.6777, dtype=float32)), 'eval/avg_episode_length': (Array(883.5469, dtype=float32), Array(319.6286, dtype=float32)), 'eval/epoch_eval_time': 4.109088182449341, 'eval/sps': 31150.463148177536}
I0726 21:57:21.135532 140267183036224 train.py:379] starting iteration 240 1733.5565588474274
I0726 21:57:28.273511 140267183036224 train.py:394] {'eval/walltime': 983.600387096405, 'training/sps': 40562.92669175018, 'training/walltime': 747.7173907756805, 'training/entropy_loss': Array(-0.04255978, dtype=float32), 'training/policy_loss': Array(0.00644881, dtype=float32), 'training/total_loss': Array(196.90704, dtype=float32), 'training/v_loss': Array(196.94315, dtype=float32), 'eval/episode_goal_distance': (Array(0.4117098, dtype=float32), Array(0.16128768, dtype=float32)), 'eval/episode_reward': (Array(-9113.701, dtype=float32), Array(3919.7812, dtype=float32)), 'eval/avg_episode_length': (Array(937.89844, dtype=float32), Array(240.51854, dtype=float32)), 'eval/epoch_eval_time': 4.105098724365234, 'eval/sps': 31180.736102709066}
I0726 21:57:28.275979 140267183036224 train.py:379] starting iteration 241 1740.6970047950745
I0726 21:57:35.397314 140267183036224 train.py:394] {'eval/walltime': 987.6933453083038, 'training/sps': 40623.36576370026, 'training/walltime': 750.742250919342, 'training/entropy_loss': Array(-0.04536456, dtype=float32), 'training/policy_loss': Array(0.0010383, dtype=float32), 'training/total_loss': Array(158.65405, dtype=float32), 'training/v_loss': Array(158.6984, dtype=float32), 'eval/episode_goal_distance': (Array(0.43162638, dtype=float32), Array(0.19359629, dtype=float32)), 'eval/episode_reward': (Array(-8842.914, dtype=float32), Array(4687.0366, dtype=float32)), 'eval/avg_episode_length': (Array(883.40625, dtype=float32), Array(320.01447, dtype=float32)), 'eval/epoch_eval_time': 4.092958211898804, 'eval/sps': 31273.224248389844}
I0726 21:57:35.399538 140267183036224 train.py:379] starting iteration 242 1747.820564508438
I0726 21:57:42.519978 140267183036224 train.py:394] {'eval/walltime': 991.7929399013519, 'training/sps': 40724.03005668063, 'training/walltime': 753.7596340179443, 'training/entropy_loss': Array(-0.04459633, dtype=float32), 'training/policy_loss': Array(0.00132083, dtype=float32), 'training/total_loss': Array(149.46408, dtype=float32), 'training/v_loss': Array(149.50735, dtype=float32), 'eval/episode_goal_distance': (Array(0.40464875, dtype=float32), Array(0.14390741, dtype=float32)), 'eval/episode_reward': (Array(-9077.539, dtype=float32), Array(3789.791, dtype=float32)), 'eval/avg_episode_length': (Array(930.0547, dtype=float32), Array(254.338, dtype=float32)), 'eval/epoch_eval_time': 4.099594593048096, 'eval/sps': 31222.599477776785}
I0726 21:57:42.522485 140267183036224 train.py:379] starting iteration 243 1754.9435105323792
I0726 21:57:49.658684 140267183036224 train.py:394] {'eval/walltime': 995.906409740448, 'training/sps': 40699.01730313655, 'training/walltime': 756.7788715362549, 'training/entropy_loss': Array(-0.04370882, dtype=float32), 'training/policy_loss': Array(0.00148906, dtype=float32), 'training/total_loss': Array(130.08862, dtype=float32), 'training/v_loss': Array(130.13084, dtype=float32), 'eval/episode_goal_distance': (Array(0.41970688, dtype=float32), Array(0.16659623, dtype=float32)), 'eval/episode_reward': (Array(-9228.134, dtype=float32), Array(4412.867, dtype=float32)), 'eval/avg_episode_length': (Array(899.10156, dtype=float32), Array(300.09753, dtype=float32)), 'eval/epoch_eval_time': 4.113469839096069, 'eval/sps': 31117.281761357917}
I0726 21:57:49.661106 140267183036224 train.py:379] starting iteration 244 1762.082132101059
I0726 21:57:56.807237 140267183036224 train.py:394] {'eval/walltime': 1000.0177192687988, 'training/sps': 40537.661813278995, 'training/walltime': 759.8101267814636, 'training/entropy_loss': Array(-0.04113459, dtype=float32), 'training/policy_loss': Array(0.00049275, dtype=float32), 'training/total_loss': Array(116.58066, dtype=float32), 'training/v_loss': Array(116.62129, dtype=float32), 'eval/episode_goal_distance': (Array(0.38730115, dtype=float32), Array(0.15861402, dtype=float32)), 'eval/episode_reward': (Array(-8462.517, dtype=float32), Array(4266.432, dtype=float32)), 'eval/avg_episode_length': (Array(868., dtype=float32), Array(337.2962, dtype=float32)), 'eval/epoch_eval_time': 4.11130952835083, 'eval/sps': 31133.6325123019}
I0726 21:57:56.809550 140267183036224 train.py:379] starting iteration 245 1769.2305762767792
I0726 21:58:03.950623 140267183036224 train.py:394] {'eval/walltime': 1004.1279501914978, 'training/sps': 40591.00412701978, 'training/walltime': 762.8373985290527, 'training/entropy_loss': Array(-0.03903074, dtype=float32), 'training/policy_loss': Array(0.00053199, dtype=float32), 'training/total_loss': Array(113.63348, dtype=float32), 'training/v_loss': Array(113.67198, dtype=float32), 'eval/episode_goal_distance': (Array(0.39116377, dtype=float32), Array(0.15387289, dtype=float32)), 'eval/episode_reward': (Array(-9070.418, dtype=float32), Array(3615.0547, dtype=float32)), 'eval/avg_episode_length': (Array(953.46875, dtype=float32), Array(209.82158, dtype=float32)), 'eval/epoch_eval_time': 4.110230922698975, 'eval/sps': 31141.802591458552}
I0726 21:58:03.953859 140267183036224 train.py:379] starting iteration 246 1776.3748850822449
I0726 21:58:11.089376 140267183036224 train.py:394] {'eval/walltime': 1008.233293056488, 'training/sps': 40597.50750516098, 'training/walltime': 765.864185333252, 'training/entropy_loss': Array(-0.03731167, dtype=float32), 'training/policy_loss': Array(0.00121082, dtype=float32), 'training/total_loss': Array(106.16259, dtype=float32), 'training/v_loss': Array(106.1987, dtype=float32), 'eval/episode_goal_distance': (Array(0.40225703, dtype=float32), Array(0.1601212, dtype=float32)), 'eval/episode_reward': (Array(-9177.973, dtype=float32), Array(4187.781, dtype=float32)), 'eval/avg_episode_length': (Array(914.5703, dtype=float32), Array(278.6159, dtype=float32)), 'eval/epoch_eval_time': 4.105342864990234, 'eval/sps': 31178.8818155885}
I0726 21:58:11.091643 140267183036224 train.py:379] starting iteration 247 1783.5126686096191
I0726 21:58:18.231290 140267183036224 train.py:394] {'eval/walltime': 1012.335896730423, 'training/sps': 40507.22565282245, 'training/walltime': 768.8977181911469, 'training/entropy_loss': Array(-0.03809267, dtype=float32), 'training/policy_loss': Array(0.00162603, dtype=float32), 'training/total_loss': Array(108.30554, dtype=float32), 'training/v_loss': Array(108.34201, dtype=float32), 'eval/episode_goal_distance': (Array(0.4164802, dtype=float32), Array(0.15889427, dtype=float32)), 'eval/episode_reward': (Array(-9705.494, dtype=float32), Array(4028.5227, dtype=float32)), 'eval/avg_episode_length': (Array(953.375, dtype=float32), Array(210.24385, dtype=float32)), 'eval/epoch_eval_time': 4.1026036739349365, 'eval/sps': 31199.699062627507}
I0726 21:58:18.233576 140267183036224 train.py:379] starting iteration 248 1790.6546022891998
I0726 21:58:25.374290 140267183036224 train.py:394] {'eval/walltime': 1016.4409863948822, 'training/sps': 40526.55956848085, 'training/walltime': 771.9298038482666, 'training/entropy_loss': Array(-0.03892036, dtype=float32), 'training/policy_loss': Array(0.0009045, dtype=float32), 'training/total_loss': Array(107.7233, dtype=float32), 'training/v_loss': Array(107.76132, dtype=float32), 'eval/episode_goal_distance': (Array(0.41617328, dtype=float32), Array(0.15969561, dtype=float32)), 'eval/episode_reward': (Array(-9431.523, dtype=float32), Array(3980.3865, dtype=float32)), 'eval/avg_episode_length': (Array(930.1094, dtype=float32), Array(254.13913, dtype=float32)), 'eval/epoch_eval_time': 4.1050896644592285, 'eval/sps': 31180.804918389447}
I0726 21:58:25.376782 140267183036224 train.py:379] starting iteration 249 1797.7978084087372
I0726 21:58:32.504886 140267183036224 train.py:394] {'eval/walltime': 1020.5455591678619, 'training/sps': 40688.009602253915, 'training/walltime': 774.9498581886292, 'training/entropy_loss': Array(-0.03885794, dtype=float32), 'training/policy_loss': Array(0.00464827, dtype=float32), 'training/total_loss': Array(112.18227, dtype=float32), 'training/v_loss': Array(112.21648, dtype=float32), 'eval/episode_goal_distance': (Array(0.4162659, dtype=float32), Array(0.17078647, dtype=float32)), 'eval/episode_reward': (Array(-9031.599, dtype=float32), Array(4352.8286, dtype=float32)), 'eval/avg_episode_length': (Array(899.0156, dtype=float32), Array(300.35294, dtype=float32)), 'eval/epoch_eval_time': 4.104572772979736, 'eval/sps': 31184.73153713333}
I0726 21:58:32.507320 140267183036224 train.py:379] starting iteration 250 1804.9283447265625
I0726 21:58:39.649791 140267183036224 train.py:394] {'eval/walltime': 1024.662765264511, 'training/sps': 40664.895475273996, 'training/walltime': 777.9716291427612, 'training/entropy_loss': Array(-0.03808823, dtype=float32), 'training/policy_loss': Array(0.00262619, dtype=float32), 'training/total_loss': Array(260.2668, dtype=float32), 'training/v_loss': Array(260.30225, dtype=float32), 'eval/episode_goal_distance': (Array(0.3901522, dtype=float32), Array(0.1705392, dtype=float32)), 'eval/episode_reward': (Array(-8957.305, dtype=float32), Array(3993.897, dtype=float32)), 'eval/avg_episode_length': (Array(937.8906, dtype=float32), Array(240.54886, dtype=float32)), 'eval/epoch_eval_time': 4.11720609664917, 'eval/sps': 31089.043636696766}
I0726 21:58:39.652192 140267183036224 train.py:379] starting iteration 251 1812.0732188224792
I0726 21:58:46.798493 140267183036224 train.py:394] {'eval/walltime': 1028.7801861763, 'training/sps': 40617.73436429925, 'training/walltime': 780.9969086647034, 'training/entropy_loss': Array(-0.03853309, dtype=float32), 'training/policy_loss': Array(0.00058411, dtype=float32), 'training/total_loss': Array(123.3228, dtype=float32), 'training/v_loss': Array(123.36075, dtype=float32), 'eval/episode_goal_distance': (Array(0.37878525, dtype=float32), Array(0.15698683, dtype=float32)), 'eval/episode_reward': (Array(-8672.59, dtype=float32), Array(3521.944, dtype=float32)), 'eval/avg_episode_length': (Array(937.8594, dtype=float32), Array(240.67001, dtype=float32)), 'eval/epoch_eval_time': 4.11742091178894, 'eval/sps': 31087.421651139004}
I0726 21:58:46.803138 140267183036224 train.py:379] starting iteration 252 1819.2241473197937
I0726 21:58:53.954655 140267183036224 train.py:394] {'eval/walltime': 1032.8983535766602, 'training/sps': 40561.7742676789, 'training/walltime': 784.0263619422913, 'training/entropy_loss': Array(-0.03804078, dtype=float32), 'training/policy_loss': Array(0.00031487, dtype=float32), 'training/total_loss': Array(123.26835, dtype=float32), 'training/v_loss': Array(123.306076, dtype=float32), 'eval/episode_goal_distance': (Array(0.4313135, dtype=float32), Array(0.1673103, dtype=float32)), 'eval/episode_reward': (Array(-9475.148, dtype=float32), Array(3382.853, dtype=float32)), 'eval/avg_episode_length': (Array(961.1953, dtype=float32), Array(192.46529, dtype=float32)), 'eval/epoch_eval_time': 4.118167400360107, 'eval/sps': 31081.7865220358}
I0726 21:58:53.957058 140267183036224 train.py:379] starting iteration 253 1826.378083705902
I0726 21:59:01.101239 140267183036224 train.py:394] {'eval/walltime': 1037.0074281692505, 'training/sps': 40555.84718182323, 'training/walltime': 787.0562579631805, 'training/entropy_loss': Array(-0.04037539, dtype=float32), 'training/policy_loss': Array(0.00041019, dtype=float32), 'training/total_loss': Array(121.689026, dtype=float32), 'training/v_loss': Array(121.72899, dtype=float32), 'eval/episode_goal_distance': (Array(0.43613508, dtype=float32), Array(0.1766936, dtype=float32)), 'eval/episode_reward': (Array(-9082.984, dtype=float32), Array(4497.7554, dtype=float32)), 'eval/avg_episode_length': (Array(891.2656, dtype=float32), Array(310.28156, dtype=float32)), 'eval/epoch_eval_time': 4.109074592590332, 'eval/sps': 31150.566171472125}
I0726 21:59:01.103565 140267183036224 train.py:379] starting iteration 254 1833.5245907306671
I0726 21:59:08.245759 140267183036224 train.py:394] {'eval/walltime': 1041.1166608333588, 'training/sps': 40562.22756794933, 'training/walltime': 790.0856773853302, 'training/entropy_loss': Array(-0.03966823, dtype=float32), 'training/policy_loss': Array(4.0855033e-05, dtype=float32), 'training/total_loss': Array(141.54869, dtype=float32), 'training/v_loss': Array(141.58832, dtype=float32), 'eval/episode_goal_distance': (Array(0.419365, dtype=float32), Array(0.18814602, dtype=float32)), 'eval/episode_reward': (Array(-9248.906, dtype=float32), Array(4018.8052, dtype=float32)), 'eval/avg_episode_length': (Array(937.89844, dtype=float32), Array(240.51881, dtype=float32)), 'eval/epoch_eval_time': 4.109232664108276, 'eval/sps': 31149.36789002105}
I0726 21:59:08.248019 140267183036224 train.py:379] starting iteration 255 1840.6690447330475
I0726 21:59:15.396142 140267183036224 train.py:394] {'eval/walltime': 1045.2294931411743, 'training/sps': 40529.34810477066, 'training/walltime': 793.1175544261932, 'training/entropy_loss': Array(-0.04091067, dtype=float32), 'training/policy_loss': Array(0.00065763, dtype=float32), 'training/total_loss': Array(152.71637, dtype=float32), 'training/v_loss': Array(152.75662, dtype=float32), 'eval/episode_goal_distance': (Array(0.39830297, dtype=float32), Array(0.14714961, dtype=float32)), 'eval/episode_reward': (Array(-8235.223, dtype=float32), Array(4335.279, dtype=float32)), 'eval/avg_episode_length': (Array(852.4219, dtype=float32), Array(353.47537, dtype=float32)), 'eval/epoch_eval_time': 4.112832307815552, 'eval/sps': 31122.10525986279}
I0726 21:59:15.511280 140267183036224 train.py:379] starting iteration 256 1847.9322729110718
I0726 21:59:22.672201 140267183036224 train.py:394] {'eval/walltime': 1049.3598930835724, 'training/sps': 40611.426106222614, 'training/walltime': 796.1433038711548, 'training/entropy_loss': Array(-0.03941359, dtype=float32), 'training/policy_loss': Array(0.00638299, dtype=float32), 'training/total_loss': Array(173.54547, dtype=float32), 'training/v_loss': Array(173.57852, dtype=float32), 'eval/episode_goal_distance': (Array(0.4269672, dtype=float32), Array(0.1650739, dtype=float32)), 'eval/episode_reward': (Array(-9622.344, dtype=float32), Array(4093.393, dtype=float32)), 'eval/avg_episode_length': (Array(937.875, dtype=float32), Array(240.6094, dtype=float32)), 'eval/epoch_eval_time': 4.130399942398071, 'eval/sps': 30989.73508257518}
I0726 21:59:22.674915 140267183036224 train.py:379] starting iteration 257 1855.0959401130676
I0726 21:59:29.812575 140267183036224 train.py:394] {'eval/walltime': 1053.4740672111511, 'training/sps': 40695.083920629295, 'training/walltime': 799.1628332138062, 'training/entropy_loss': Array(-0.04033287, dtype=float32), 'training/policy_loss': Array(0.00323956, dtype=float32), 'training/total_loss': Array(169.78952, dtype=float32), 'training/v_loss': Array(169.82661, dtype=float32), 'eval/episode_goal_distance': (Array(0.4466579, dtype=float32), Array(0.17684904, dtype=float32)), 'eval/episode_reward': (Array(-9660.934, dtype=float32), Array(4256.4775, dtype=float32)), 'eval/avg_episode_length': (Array(922.375, dtype=float32), Array(266.65103, dtype=float32)), 'eval/epoch_eval_time': 4.114174127578735, 'eval/sps': 31111.95492236744}
I0726 21:59:29.815093 140267183036224 train.py:379] starting iteration 258 1862.236118555069
I0726 21:59:36.956018 140267183036224 train.py:394] {'eval/walltime': 1057.5812692642212, 'training/sps': 40554.49731239822, 'training/walltime': 802.1928300857544, 'training/entropy_loss': Array(-0.04155233, dtype=float32), 'training/policy_loss': Array(0.00085584, dtype=float32), 'training/total_loss': Array(137.60666, dtype=float32), 'training/v_loss': Array(137.64734, dtype=float32), 'eval/episode_goal_distance': (Array(0.39301512, dtype=float32), Array(0.16886012, dtype=float32)), 'eval/episode_reward': (Array(-8636.612, dtype=float32), Array(4000.3396, dtype=float32)), 'eval/avg_episode_length': (Array(930.03906, dtype=float32), Array(254.39494, dtype=float32)), 'eval/epoch_eval_time': 4.107202053070068, 'eval/sps': 31164.768215949356}
I0726 21:59:36.958307 140267183036224 train.py:379] starting iteration 259 1869.3793334960938
I0726 21:59:44.100498 140267183036224 train.py:394] {'eval/walltime': 1061.693930864334, 'training/sps': 40606.62661386767, 'training/walltime': 805.2189371585846, 'training/entropy_loss': Array(-0.04252218, dtype=float32), 'training/policy_loss': Array(0.00072657, dtype=float32), 'training/total_loss': Array(126.34355, dtype=float32), 'training/v_loss': Array(126.385345, dtype=float32), 'eval/episode_goal_distance': (Array(0.45108077, dtype=float32), Array(0.18425462, dtype=float32)), 'eval/episode_reward': (Array(-9314.341, dtype=float32), Array(4407.222, dtype=float32)), 'eval/avg_episode_length': (Array(899.02344, dtype=float32), Array(300.32996, dtype=float32)), 'eval/epoch_eval_time': 4.112661600112915, 'eval/sps': 31123.397071250816}
I0726 21:59:44.102683 140267183036224 train.py:379] starting iteration 260 1876.523708820343
I0726 21:59:51.241414 140267183036224 train.py:394] {'eval/walltime': 1065.7993731498718, 'training/sps': 40554.707924406204, 'training/walltime': 808.2489182949066, 'training/entropy_loss': Array(-0.04377282, dtype=float32), 'training/policy_loss': Array(0.00076661, dtype=float32), 'training/total_loss': Array(136.39561, dtype=float32), 'training/v_loss': Array(136.43863, dtype=float32), 'eval/episode_goal_distance': (Array(0.4504504, dtype=float32), Array(0.21564622, dtype=float32)), 'eval/episode_reward': (Array(-10083.164, dtype=float32), Array(4490.507, dtype=float32)), 'eval/avg_episode_length': (Array(937.7969, dtype=float32), Array(240.91182, dtype=float32)), 'eval/epoch_eval_time': 4.10544228553772, 'eval/sps': 31178.126763809785}
I0726 21:59:51.243538 140267183036224 train.py:379] starting iteration 261 1883.6645641326904
I0726 21:59:58.381405 140267183036224 train.py:394] {'eval/walltime': 1069.904245853424, 'training/sps': 40561.9530327728, 'training/walltime': 811.2783582210541, 'training/entropy_loss': Array(-0.04288872, dtype=float32), 'training/policy_loss': Array(7.6048054e-05, dtype=float32), 'training/total_loss': Array(126.922195, dtype=float32), 'training/v_loss': Array(126.965, dtype=float32), 'eval/episode_goal_distance': (Array(0.43840128, dtype=float32), Array(0.16977139, dtype=float32)), 'eval/episode_reward': (Array(-9783.396, dtype=float32), Array(4150.0513, dtype=float32)), 'eval/avg_episode_length': (Array(930.0703, dtype=float32), Array(254.28123, dtype=float32)), 'eval/epoch_eval_time': 4.104872703552246, 'eval/sps': 31182.452963579664}
I0726 21:59:58.383640 140267183036224 train.py:379] starting iteration 262 1890.8046653270721
I0726 22:00:05.523056 140267183036224 train.py:394] {'eval/walltime': 1074.0106174945831, 'training/sps': 40560.574028569215, 'training/walltime': 814.3079011440277, 'training/entropy_loss': Array(-0.04290565, dtype=float32), 'training/policy_loss': Array(-0.00026019, dtype=float32), 'training/total_loss': Array(124.57222, dtype=float32), 'training/v_loss': Array(124.61538, dtype=float32), 'eval/episode_goal_distance': (Array(0.41304815, dtype=float32), Array(0.18206021, dtype=float32)), 'eval/episode_reward': (Array(-9052.364, dtype=float32), Array(3856.196, dtype=float32)), 'eval/avg_episode_length': (Array(945.64844, dtype=float32), Array(225.97284, dtype=float32)), 'eval/epoch_eval_time': 4.106371641159058, 'eval/sps': 31171.070518076864}
I0726 22:00:05.525252 140267183036224 train.py:379] starting iteration 263 1897.9462780952454
I0726 22:00:12.651346 140267183036224 train.py:394] {'eval/walltime': 1078.105463027954, 'training/sps': 40582.74841472924, 'training/walltime': 817.3357887268066, 'training/entropy_loss': Array(-0.04174277, dtype=float32), 'training/policy_loss': Array(-5.8801634e-05, dtype=float32), 'training/total_loss': Array(120.21518, dtype=float32), 'training/v_loss': Array(120.25698, dtype=float32), 'eval/episode_goal_distance': (Array(0.40039855, dtype=float32), Array(0.17119461, dtype=float32)), 'eval/episode_reward': (Array(-8887.476, dtype=float32), Array(4081.735, dtype=float32)), 'eval/avg_episode_length': (Array(922.3594, dtype=float32), Array(266.7044, dtype=float32)), 'eval/epoch_eval_time': 4.094845533370972, 'eval/sps': 31258.810364606707}
I0726 22:00:12.653580 140267183036224 train.py:379] starting iteration 264 1905.0746066570282
I0726 22:00:19.792448 140267183036224 train.py:394] {'eval/walltime': 1082.22199177742, 'training/sps': 40704.93487674185, 'training/walltime': 820.3545873165131, 'training/entropy_loss': Array(-0.04166363, dtype=float32), 'training/policy_loss': Array(3.7796126e-05, dtype=float32), 'training/total_loss': Array(116.99896, dtype=float32), 'training/v_loss': Array(117.04059, dtype=float32), 'eval/episode_goal_distance': (Array(0.42389208, dtype=float32), Array(0.17567278, dtype=float32)), 'eval/episode_reward': (Array(-9494.207, dtype=float32), Array(4005.8567, dtype=float32)), 'eval/avg_episode_length': (Array(930.15625, dtype=float32), Array(253.96889, dtype=float32)), 'eval/epoch_eval_time': 4.116528749465942, 'eval/sps': 31094.159130215252}
I0726 22:00:19.794847 140267183036224 train.py:379] starting iteration 265 1912.2158722877502
I0726 22:00:26.931542 140267183036224 train.py:394] {'eval/walltime': 1086.3295006752014, 'training/sps': 40613.75586880727, 'training/walltime': 823.380163192749, 'training/entropy_loss': Array(-0.04159972, dtype=float32), 'training/policy_loss': Array(-1.9260617e-05, dtype=float32), 'training/total_loss': Array(115.61688, dtype=float32), 'training/v_loss': Array(115.6585, dtype=float32), 'eval/episode_goal_distance': (Array(0.39697507, dtype=float32), Array(0.16570327, dtype=float32)), 'eval/episode_reward': (Array(-8719.977, dtype=float32), Array(4284.382, dtype=float32)), 'eval/avg_episode_length': (Array(906.84375, dtype=float32), Array(289.63495, dtype=float32)), 'eval/epoch_eval_time': 4.107508897781372, 'eval/sps': 31162.44010308483}
I0726 22:00:26.933801 140267183036224 train.py:379] starting iteration 266 1919.3548271656036
I0726 22:00:34.067264 140267183036224 train.py:394] {'eval/walltime': 1090.4324970245361, 'training/sps': 40596.592941621275, 'training/walltime': 826.4070181846619, 'training/entropy_loss': Array(-0.04116602, dtype=float32), 'training/policy_loss': Array(0.00031643, dtype=float32), 'training/total_loss': Array(220.14322, dtype=float32), 'training/v_loss': Array(220.18407, dtype=float32), 'eval/episode_goal_distance': (Array(0.4241162, dtype=float32), Array(0.16536283, dtype=float32)), 'eval/episode_reward': (Array(-9538.627, dtype=float32), Array(3883.4448, dtype=float32)), 'eval/avg_episode_length': (Array(953.4219, dtype=float32), Array(210.03256, dtype=float32)), 'eval/epoch_eval_time': 4.102996349334717, 'eval/sps': 31196.713109616743}
I0726 22:00:34.069481 140267183036224 train.py:379] starting iteration 267 1926.490506887436
I0726 22:00:41.215446 140267183036224 train.py:394] {'eval/walltime': 1094.5402653217316, 'training/sps': 40491.76225356521, 'training/walltime': 829.4417095184326, 'training/entropy_loss': Array(-0.04184638, dtype=float32), 'training/policy_loss': Array(3.6921207e-05, dtype=float32), 'training/total_loss': Array(153.50357, dtype=float32), 'training/v_loss': Array(153.54536, dtype=float32), 'eval/episode_goal_distance': (Array(0.44361556, dtype=float32), Array(0.19354022, dtype=float32)), 'eval/episode_reward': (Array(-9833.465, dtype=float32), Array(4261.3545, dtype=float32)), 'eval/avg_episode_length': (Array(937.8906, dtype=float32), Array(240.54897, dtype=float32)), 'eval/epoch_eval_time': 4.107768297195435, 'eval/sps': 31160.47224167721}
I0726 22:00:41.217711 140267183036224 train.py:379] starting iteration 268 1933.6387362480164
I0726 22:00:48.350194 140267183036224 train.py:394] {'eval/walltime': 1098.6390058994293, 'training/sps': 40548.55959232102, 'training/walltime': 832.4721500873566, 'training/entropy_loss': Array(-0.04241217, dtype=float32), 'training/policy_loss': Array(-0.00020735, dtype=float32), 'training/total_loss': Array(121.77142, dtype=float32), 'training/v_loss': Array(121.81404, dtype=float32), 'eval/episode_goal_distance': (Array(0.41848415, dtype=float32), Array(0.16438767, dtype=float32)), 'eval/episode_reward': (Array(-8926.1875, dtype=float32), Array(3960.305, dtype=float32)), 'eval/avg_episode_length': (Array(922.375, dtype=float32), Array(266.651, dtype=float32)), 'eval/epoch_eval_time': 4.098740577697754, 'eval/sps': 31229.10503203818}
I0726 22:00:48.352354 140267183036224 train.py:379] starting iteration 269 1940.773380279541
I0726 22:00:55.492068 140267183036224 train.py:394] {'eval/walltime': 1102.740938425064, 'training/sps': 40523.293485386734, 'training/walltime': 835.5044801235199, 'training/entropy_loss': Array(-0.04110757, dtype=float32), 'training/policy_loss': Array(0.00019237, dtype=float32), 'training/total_loss': Array(122.65738, dtype=float32), 'training/v_loss': Array(122.698296, dtype=float32), 'eval/episode_goal_distance': (Array(0.41224688, dtype=float32), Array(0.16914037, dtype=float32)), 'eval/episode_reward': (Array(-9238.982, dtype=float32), Array(3922.2485, dtype=float32)), 'eval/avg_episode_length': (Array(945.6094, dtype=float32), Array(226.1352, dtype=float32)), 'eval/epoch_eval_time': 4.101932525634766, 'eval/sps': 31204.80388209025}
I0726 22:00:55.494420 140267183036224 train.py:379] starting iteration 270 1947.9154455661774
I0726 22:01:02.617131 140267183036224 train.py:394] {'eval/walltime': 1106.8372015953064, 'training/sps': 40648.269558283864, 'training/walltime': 838.527487039566, 'training/entropy_loss': Array(-0.03993754, dtype=float32), 'training/policy_loss': Array(0.00023794, dtype=float32), 'training/total_loss': Array(126.055115, dtype=float32), 'training/v_loss': Array(126.09481, dtype=float32), 'eval/episode_goal_distance': (Array(0.4179045, dtype=float32), Array(0.16403064, dtype=float32)), 'eval/episode_reward': (Array(-9421.465, dtype=float32), Array(4092.969, dtype=float32)), 'eval/avg_episode_length': (Array(930.0469, dtype=float32), Array(254.36642, dtype=float32)), 'eval/epoch_eval_time': 4.09626317024231, 'eval/sps': 31247.9922993884}
I0726 22:01:02.619589 140267183036224 train.py:379] starting iteration 271 1955.0406153202057
I0726 22:01:09.757042 140267183036224 train.py:394] {'eval/walltime': 1110.9504296779633, 'training/sps': 40678.975914374285, 'training/walltime': 841.5482120513916, 'training/entropy_loss': Array(-0.04016116, dtype=float32), 'training/policy_loss': Array(-1.3763223e-05, dtype=float32), 'training/total_loss': Array(148.67302, dtype=float32), 'training/v_loss': Array(148.7132, dtype=float32), 'eval/episode_goal_distance': (Array(0.38706452, dtype=float32), Array(0.14537646, dtype=float32)), 'eval/episode_reward': (Array(-8186.123, dtype=float32), Array(4177.903, dtype=float32)), 'eval/avg_episode_length': (Array(883.52344, dtype=float32), Array(319.69296, dtype=float32)), 'eval/epoch_eval_time': 4.11322808265686, 'eval/sps': 31119.110690628386}
I0726 22:01:09.759305 140267183036224 train.py:379] starting iteration 272 1962.1803314685822
I0726 22:01:16.900361 140267183036224 train.py:394] {'eval/walltime': 1115.0630149841309, 'training/sps': 40620.535462178756, 'training/walltime': 844.573282957077, 'training/entropy_loss': Array(-0.03956404, dtype=float32), 'training/policy_loss': Array(0.00037785, dtype=float32), 'training/total_loss': Array(138.66763, dtype=float32), 'training/v_loss': Array(138.70682, dtype=float32), 'eval/episode_goal_distance': (Array(0.38974762, dtype=float32), Array(0.15287222, dtype=float32)), 'eval/episode_reward': (Array(-9328.861, dtype=float32), Array(3676.2903, dtype=float32)), 'eval/avg_episode_length': (Array(968.9297, dtype=float32), Array(172.99251, dtype=float32)), 'eval/epoch_eval_time': 4.1125853061676025, 'eval/sps': 31123.974451797923}
I0726 22:01:16.902630 140267183036224 train.py:379] starting iteration 273 1969.3236560821533
I0726 22:01:24.032511 140267183036224 train.py:394] {'eval/walltime': 1119.1623027324677, 'training/sps': 40593.130125128446, 'training/walltime': 847.600396156311, 'training/entropy_loss': Array(-0.0385322, dtype=float32), 'training/policy_loss': Array(0.00025297, dtype=float32), 'training/total_loss': Array(136.92249, dtype=float32), 'training/v_loss': Array(136.96075, dtype=float32), 'eval/episode_goal_distance': (Array(0.39369488, dtype=float32), Array(0.15880677, dtype=float32)), 'eval/episode_reward': (Array(-9050.482, dtype=float32), Array(3566.4407, dtype=float32)), 'eval/avg_episode_length': (Array(953.3672, dtype=float32), Array(210.27913, dtype=float32)), 'eval/epoch_eval_time': 4.099287748336792, 'eval/sps': 31224.93658854116}
I0726 22:01:24.034812 140267183036224 train.py:379] starting iteration 274 1976.455837726593
I0726 22:01:31.174161 140267183036224 train.py:394] {'eval/walltime': 1123.2683639526367, 'training/sps': 40556.82373912683, 'training/walltime': 850.6302192211151, 'training/entropy_loss': Array(-0.03942911, dtype=float32), 'training/policy_loss': Array(0.00035028, dtype=float32), 'training/total_loss': Array(146.31036, dtype=float32), 'training/v_loss': Array(146.34944, dtype=float32), 'eval/episode_goal_distance': (Array(0.38981953, dtype=float32), Array(0.1638814, dtype=float32)), 'eval/episode_reward': (Array(-8566.318, dtype=float32), Array(4112.765, dtype=float32)), 'eval/avg_episode_length': (Array(914.53906, dtype=float32), Array(278.7178, dtype=float32)), 'eval/epoch_eval_time': 4.106061220169067, 'eval/sps': 31173.427071973758}
I0726 22:01:31.176409 140267183036224 train.py:379] starting iteration 275 1983.5974342823029
I0726 22:01:38.314711 140267183036224 train.py:394] {'eval/walltime': 1127.3741438388824, 'training/sps': 40567.0357282287, 'training/walltime': 853.6592795848846, 'training/entropy_loss': Array(-0.04123537, dtype=float32), 'training/policy_loss': Array(3.459284e-05, dtype=float32), 'training/total_loss': Array(124.79559, dtype=float32), 'training/v_loss': Array(124.83679, dtype=float32), 'eval/episode_goal_distance': (Array(0.4196105, dtype=float32), Array(0.1705874, dtype=float32)), 'eval/episode_reward': (Array(-9414.549, dtype=float32), Array(3970.6033, dtype=float32)), 'eval/avg_episode_length': (Array(930.0156, dtype=float32), Array(254.48001, dtype=float32)), 'eval/epoch_eval_time': 4.1057798862457275, 'eval/sps': 31175.563119883067}
I0726 22:01:38.317073 140267183036224 train.py:379] starting iteration 276 1990.7380993366241
I0726 22:01:45.434706 140267183036224 train.py:394] {'eval/walltime': 1131.4691545963287, 'training/sps': 40700.21610763197, 'training/walltime': 856.6784281730652, 'training/entropy_loss': Array(-0.04113388, dtype=float32), 'training/policy_loss': Array(0.00015096, dtype=float32), 'training/total_loss': Array(116.8706, dtype=float32), 'training/v_loss': Array(116.911575, dtype=float32), 'eval/episode_goal_distance': (Array(0.41320613, dtype=float32), Array(0.17166583, dtype=float32)), 'eval/episode_reward': (Array(-9281.363, dtype=float32), Array(4113.7456, dtype=float32)), 'eval/avg_episode_length': (Array(930.10156, dtype=float32), Array(254.16779, dtype=float32)), 'eval/epoch_eval_time': 4.095010757446289, 'eval/sps': 31257.549144955785}
I0726 22:01:45.437484 140267183036224 train.py:379] starting iteration 277 1997.8585102558136
I0726 22:01:52.586594 140267183036224 train.py:394] {'eval/walltime': 1135.5950815677643, 'training/sps': 40698.35525907456, 'training/walltime': 859.697714805603, 'training/entropy_loss': Array(-0.04109879, dtype=float32), 'training/policy_loss': Array(0.00024429, dtype=float32), 'training/total_loss': Array(117.04193, dtype=float32), 'training/v_loss': Array(117.08278, dtype=float32), 'eval/episode_goal_distance': (Array(0.43628478, dtype=float32), Array(0.168723, dtype=float32)), 'eval/episode_reward': (Array(-9880.559, dtype=float32), Array(3970.395, dtype=float32)), 'eval/avg_episode_length': (Array(945.58594, dtype=float32), Array(226.23262, dtype=float32)), 'eval/epoch_eval_time': 4.125926971435547, 'eval/sps': 31023.331456461663}
I0726 22:01:52.589299 140267183036224 train.py:379] starting iteration 278 2005.0103237628937
I0726 22:01:59.719637 140267183036224 train.py:394] {'eval/walltime': 1139.7025446891785, 'training/sps': 40700.27074651688, 'training/walltime': 862.7168593406677, 'training/entropy_loss': Array(-0.04083587, dtype=float32), 'training/policy_loss': Array(-0.00021592, dtype=float32), 'training/total_loss': Array(107.61023, dtype=float32), 'training/v_loss': Array(107.65128, dtype=float32), 'eval/episode_goal_distance': (Array(0.4000777, dtype=float32), Array(0.16782038, dtype=float32)), 'eval/episode_reward': (Array(-9176.887, dtype=float32), Array(3850.7427, dtype=float32)), 'eval/avg_episode_length': (Array(945.6953, dtype=float32), Array(225.77802, dtype=float32)), 'eval/epoch_eval_time': 4.107463121414185, 'eval/sps': 31162.787398546396}
I0726 22:01:59.722080 140267183036224 train.py:379] starting iteration 279 2012.1431052684784
I0726 22:02:06.858263 140267183036224 train.py:394] {'eval/walltime': 1143.8104927539825, 'training/sps': 40625.36065883896, 'training/walltime': 865.7415709495544, 'training/entropy_loss': Array(-0.04189118, dtype=float32), 'training/policy_loss': Array(-4.9158636e-05, dtype=float32), 'training/total_loss': Array(118.61715, dtype=float32), 'training/v_loss': Array(118.65908, dtype=float32), 'eval/episode_goal_distance': (Array(0.402835, dtype=float32), Array(0.15273245, dtype=float32)), 'eval/episode_reward': (Array(-9040.984, dtype=float32), Array(4100.6963, dtype=float32)), 'eval/avg_episode_length': (Array(899.0469, dtype=float32), Array(300.2603, dtype=float32)), 'eval/epoch_eval_time': 4.107948064804077, 'eval/sps': 31159.10863057729}
I0726 22:02:06.860592 140267183036224 train.py:379] starting iteration 280 2019.2816178798676
I0726 22:02:13.984495 140267183036224 train.py:394] {'eval/walltime': 1147.9060719013214, 'training/sps': 40625.23577201423, 'training/walltime': 868.7662918567657, 'training/entropy_loss': Array(-0.04041871, dtype=float32), 'training/policy_loss': Array(-4.439929e-05, dtype=float32), 'training/total_loss': Array(109.50369, dtype=float32), 'training/v_loss': Array(109.54416, dtype=float32), 'eval/episode_goal_distance': (Array(0.3869971, dtype=float32), Array(0.14873421, dtype=float32)), 'eval/episode_reward': (Array(-9144.396, dtype=float32), Array(3631.4668, dtype=float32)), 'eval/avg_episode_length': (Array(953.4297, dtype=float32), Array(209.99765, dtype=float32)), 'eval/epoch_eval_time': 4.095579147338867, 'eval/sps': 31253.21118093126}
I0726 22:02:13.986970 140267183036224 train.py:379] starting iteration 281 2026.4079954624176
I0726 22:02:21.125132 140267183036224 train.py:394] {'eval/walltime': 1152.0063436031342, 'training/sps': 40496.595087302885, 'training/walltime': 871.8006210327148, 'training/entropy_loss': Array(-0.03839039, dtype=float32), 'training/policy_loss': Array(0.00028505, dtype=float32), 'training/total_loss': Array(98.99904, dtype=float32), 'training/v_loss': Array(99.03714, dtype=float32), 'eval/episode_goal_distance': (Array(0.39970195, dtype=float32), Array(0.14361823, dtype=float32)), 'eval/episode_reward': (Array(-8609.676, dtype=float32), Array(3861.3706, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.7583, dtype=float32)), 'eval/epoch_eval_time': 4.100271701812744, 'eval/sps': 31217.443454640033}
I0726 22:02:21.127455 140267183036224 train.py:379] starting iteration 282 2033.5484817028046
I0726 22:02:28.250115 140267183036224 train.py:394] {'eval/walltime': 1156.099803686142, 'training/sps': 40613.320618130936, 'training/walltime': 874.8262293338776, 'training/entropy_loss': Array(-0.03810846, dtype=float32), 'training/policy_loss': Array(0.00042828, dtype=float32), 'training/total_loss': Array(99.83411, dtype=float32), 'training/v_loss': Array(99.87179, dtype=float32), 'eval/episode_goal_distance': (Array(0.3712244, dtype=float32), Array(0.14460663, dtype=float32)), 'eval/episode_reward': (Array(-8149.494, dtype=float32), Array(4048.6045, dtype=float32)), 'eval/avg_episode_length': (Array(883.4844, dtype=float32), Array(319.80002, dtype=float32)), 'eval/epoch_eval_time': 4.0934600830078125, 'eval/sps': 31269.39005252191}
I0726 22:02:28.252526 140267183036224 train.py:379] starting iteration 283 2040.6735520362854
I0726 22:02:35.372950 140267183036224 train.py:394] {'eval/walltime': 1160.1998476982117, 'training/sps': 40730.888364509614, 'training/walltime': 877.8431043624878, 'training/entropy_loss': Array(-0.0398211, dtype=float32), 'training/policy_loss': Array(0.0005594, dtype=float32), 'training/total_loss': Array(245.24606, dtype=float32), 'training/v_loss': Array(245.28531, dtype=float32), 'eval/episode_goal_distance': (Array(0.37473089, dtype=float32), Array(0.13390349, dtype=float32)), 'eval/episode_reward': (Array(-8685.27, dtype=float32), Array(3455.5098, dtype=float32)), 'eval/avg_episode_length': (Array(937.8594, dtype=float32), Array(240.67004, dtype=float32)), 'eval/epoch_eval_time': 4.100044012069702, 'eval/sps': 31219.177068147033}
I0726 22:02:35.375278 140267183036224 train.py:379] starting iteration 284 2047.7963042259216
I0726 22:02:42.499076 140267183036224 train.py:394] {'eval/walltime': 1164.2981779575348, 'training/sps': 40664.13187334786, 'training/walltime': 880.8649320602417, 'training/entropy_loss': Array(-0.04057813, dtype=float32), 'training/policy_loss': Array(0.00052326, dtype=float32), 'training/total_loss': Array(126.69379, dtype=float32), 'training/v_loss': Array(126.73384, dtype=float32), 'eval/episode_goal_distance': (Array(0.3856998, dtype=float32), Array(0.14503612, dtype=float32)), 'eval/episode_reward': (Array(-8840.924, dtype=float32), Array(3946.1907, dtype=float32)), 'eval/avg_episode_length': (Array(906.84375, dtype=float32), Array(289.63498, dtype=float32)), 'eval/epoch_eval_time': 4.09833025932312, 'eval/sps': 31232.231640878173}
I0726 22:02:42.501503 140267183036224 train.py:379] starting iteration 285 2054.9225294589996
I0726 22:02:49.638287 140267183036224 train.py:394] {'eval/walltime': 1168.4037454128265, 'training/sps': 40586.49391209768, 'training/walltime': 883.8925402164459, 'training/entropy_loss': Array(-0.04016078, dtype=float32), 'training/policy_loss': Array(0.00029282, dtype=float32), 'training/total_loss': Array(104.66754, dtype=float32), 'training/v_loss': Array(104.70741, dtype=float32), 'eval/episode_goal_distance': (Array(0.39630535, dtype=float32), Array(0.14757363, dtype=float32)), 'eval/episode_reward': (Array(-9067.308, dtype=float32), Array(3875.86, dtype=float32)), 'eval/avg_episode_length': (Array(930.1328, dtype=float32), Array(254.05408, dtype=float32)), 'eval/epoch_eval_time': 4.105567455291748, 'eval/sps': 31177.176211054146}
I0726 22:02:49.640725 140267183036224 train.py:379] starting iteration 286 2062.061750650406
I0726 22:02:56.761679 140267183036224 train.py:394] {'eval/walltime': 1172.4964864253998, 'training/sps': 40627.035497374, 'training/walltime': 886.9171271324158, 'training/entropy_loss': Array(-0.03945255, dtype=float32), 'training/policy_loss': Array(0.00050177, dtype=float32), 'training/total_loss': Array(117.79682, dtype=float32), 'training/v_loss': Array(117.83577, dtype=float32), 'eval/episode_goal_distance': (Array(0.39315405, dtype=float32), Array(0.15059833, dtype=float32)), 'eval/episode_reward': (Array(-9149.191, dtype=float32), Array(3738.055, dtype=float32)), 'eval/avg_episode_length': (Array(945.5469, dtype=float32), Array(226.39487, dtype=float32)), 'eval/epoch_eval_time': 4.092741012573242, 'eval/sps': 31274.8838997565}
I0726 22:02:56.764102 140267183036224 train.py:379] starting iteration 287 2069.1851284503937
I0726 22:03:03.900665 140267183036224 train.py:394] {'eval/walltime': 1176.6007149219513, 'training/sps': 40572.442278885, 'training/walltime': 889.9457838535309, 'training/entropy_loss': Array(-0.03920745, dtype=float32), 'training/policy_loss': Array(0.00093182, dtype=float32), 'training/total_loss': Array(133.35028, dtype=float32), 'training/v_loss': Array(133.38857, dtype=float32), 'eval/episode_goal_distance': (Array(0.4001364, dtype=float32), Array(0.14428681, dtype=float32)), 'eval/episode_reward': (Array(-8793.288, dtype=float32), Array(3715.3213, dtype=float32)), 'eval/avg_episode_length': (Array(922.3906, dtype=float32), Array(266.59738, dtype=float32)), 'eval/epoch_eval_time': 4.104228496551514, 'eval/sps': 31187.347416828557}
I0726 22:03:03.903228 140267183036224 train.py:379] starting iteration 288 2076.324253797531
I0726 22:03:11.016129 140267183036224 train.py:394] {'eval/walltime': 1180.6878929138184, 'training/sps': 40660.532421933, 'training/walltime': 892.9678790569305, 'training/entropy_loss': Array(-0.03818624, dtype=float32), 'training/policy_loss': Array(0.00115616, dtype=float32), 'training/total_loss': Array(117.16823, dtype=float32), 'training/v_loss': Array(117.20526, dtype=float32), 'eval/episode_goal_distance': (Array(0.38187236, dtype=float32), Array(0.1530712, dtype=float32)), 'eval/episode_reward': (Array(-8773.791, dtype=float32), Array(3711.0686, dtype=float32)), 'eval/avg_episode_length': (Array(937.91406, dtype=float32), Array(240.45842, dtype=float32)), 'eval/epoch_eval_time': 4.087177991867065, 'eval/sps': 31317.45185913185}
I0726 22:03:11.018652 140267183036224 train.py:379] starting iteration 289 2083.4396784305573
I0726 22:03:18.148009 140267183036224 train.py:394] {'eval/walltime': 1184.791597366333, 'training/sps': 40660.27580089283, 'training/walltime': 895.9899933338165, 'training/entropy_loss': Array(-0.03984238, dtype=float32), 'training/policy_loss': Array(0.00161824, dtype=float32), 'training/total_loss': Array(154.5217, dtype=float32), 'training/v_loss': Array(154.55994, dtype=float32), 'eval/episode_goal_distance': (Array(0.39285147, dtype=float32), Array(0.13759087, dtype=float32)), 'eval/episode_reward': (Array(-8608.6045, dtype=float32), Array(3800.123, dtype=float32)), 'eval/avg_episode_length': (Array(899.02344, dtype=float32), Array(300.32974, dtype=float32)), 'eval/epoch_eval_time': 4.103704452514648, 'eval/sps': 31191.330048528416}
I0726 22:03:18.150445 140267183036224 train.py:379] starting iteration 290 2090.5714707374573
I0726 22:03:25.275010 140267183036224 train.py:394] {'eval/walltime': 1188.8860228061676, 'training/sps': 40600.93268788335, 'training/walltime': 899.0165247917175, 'training/entropy_loss': Array(-0.03952582, dtype=float32), 'training/policy_loss': Array(0.00227495, dtype=float32), 'training/total_loss': Array(149.01288, dtype=float32), 'training/v_loss': Array(149.05014, dtype=float32), 'eval/episode_goal_distance': (Array(0.3699959, dtype=float32), Array(0.12463648, dtype=float32)), 'eval/episode_reward': (Array(-7969.4453, dtype=float32), Array(3759.2773, dtype=float32)), 'eval/avg_episode_length': (Array(883.4375, dtype=float32), Array(319.92886, dtype=float32)), 'eval/epoch_eval_time': 4.094425439834595, 'eval/sps': 31262.01756043478}
I0726 22:03:25.277464 140267183036224 train.py:379] starting iteration 291 2097.6984899044037
I0726 22:03:32.400928 140267183036224 train.py:394] {'eval/walltime': 1192.9788060188293, 'training/sps': 40592.8551714817, 'training/walltime': 902.0436584949493, 'training/entropy_loss': Array(-0.04081985, dtype=float32), 'training/policy_loss': Array(0.00285282, dtype=float32), 'training/total_loss': Array(128.37213, dtype=float32), 'training/v_loss': Array(128.4101, dtype=float32), 'eval/episode_goal_distance': (Array(0.41063017, dtype=float32), Array(0.18232569, dtype=float32)), 'eval/episode_reward': (Array(-8729.442, dtype=float32), Array(4715.363, dtype=float32)), 'eval/avg_episode_length': (Array(883.41406, dtype=float32), Array(319.99307, dtype=float32)), 'eval/epoch_eval_time': 4.092783212661743, 'eval/sps': 31274.561429007415}
I0726 22:03:32.403263 140267183036224 train.py:379] starting iteration 292 2104.8242886066437
I0726 22:03:39.521810 140267183036224 train.py:394] {'eval/walltime': 1197.0683546066284, 'training/sps': 40616.0122810391, 'training/walltime': 905.069066286087, 'training/entropy_loss': Array(-0.0413892, dtype=float32), 'training/policy_loss': Array(0.00270227, dtype=float32), 'training/total_loss': Array(118.80049, dtype=float32), 'training/v_loss': Array(118.83917, dtype=float32), 'eval/episode_goal_distance': (Array(0.39583516, dtype=float32), Array(0.16142839, dtype=float32)), 'eval/episode_reward': (Array(-8438.887, dtype=float32), Array(4376.9175, dtype=float32)), 'eval/avg_episode_length': (Array(883.4531, dtype=float32), Array(319.8856, dtype=float32)), 'eval/epoch_eval_time': 4.089548587799072, 'eval/sps': 31299.298015893604}
I0726 22:03:39.524267 140267183036224 train.py:379] starting iteration 293 2111.945292711258
I0726 22:03:46.635215 140267183036224 train.py:394] {'eval/walltime': 1201.1552815437317, 'training/sps': 40683.609473527664, 'training/walltime': 908.089447259903, 'training/entropy_loss': Array(-0.0405615, dtype=float32), 'training/policy_loss': Array(0.00238137, dtype=float32), 'training/total_loss': Array(112.13539, dtype=float32), 'training/v_loss': Array(112.17357, dtype=float32), 'eval/episode_goal_distance': (Array(0.39523706, dtype=float32), Array(0.16211508, dtype=float32)), 'eval/episode_reward': (Array(-8045.027, dtype=float32), Array(4657.5547, dtype=float32)), 'eval/avg_episode_length': (Array(844.59375, dtype=float32), Array(361.13214, dtype=float32)), 'eval/epoch_eval_time': 4.0869269371032715, 'eval/sps': 31319.37565067501}
I0726 22:03:46.637450 140267183036224 train.py:379] starting iteration 294 2119.0584762096405
I0726 22:03:53.760435 140267183036224 train.py:394] {'eval/walltime': 1205.247183561325, 'training/sps': 40588.74090309927, 'training/walltime': 911.1168878078461, 'training/entropy_loss': Array(-0.04063959, dtype=float32), 'training/policy_loss': Array(0.00235803, dtype=float32), 'training/total_loss': Array(120.10033, dtype=float32), 'training/v_loss': Array(120.1386, dtype=float32), 'eval/episode_goal_distance': (Array(0.3995641, dtype=float32), Array(0.1614045, dtype=float32)), 'eval/episode_reward': (Array(-9022.904, dtype=float32), Array(4173.8247, dtype=float32)), 'eval/avg_episode_length': (Array(914.5703, dtype=float32), Array(278.6161, dtype=float32)), 'eval/epoch_eval_time': 4.091902017593384, 'eval/sps': 31281.296436145378}
I0726 22:03:53.762773 140267183036224 train.py:379] starting iteration 295 2126.1837990283966
I0726 22:04:00.889261 140267183036224 train.py:394] {'eval/walltime': 1209.3406250476837, 'training/sps': 40561.435895208095, 'training/walltime': 914.1463663578033, 'training/entropy_loss': Array(-0.04311277, dtype=float32), 'training/policy_loss': Array(0.00202117, dtype=float32), 'training/total_loss': Array(116.64612, dtype=float32), 'training/v_loss': Array(116.68721, dtype=float32), 'eval/episode_goal_distance': (Array(0.42925233, dtype=float32), Array(0.15727213, dtype=float32)), 'eval/episode_reward': (Array(-8893.474, dtype=float32), Array(4636.3867, dtype=float32)), 'eval/avg_episode_length': (Array(860.2578, dtype=float32), Array(345.45242, dtype=float32)), 'eval/epoch_eval_time': 4.093441486358643, 'eval/sps': 31269.53211046472}
I0726 22:04:00.891398 140267183036224 train.py:379] starting iteration 296 2133.312424182892
I0726 22:04:08.013207 140267183036224 train.py:394] {'eval/walltime': 1213.4259905815125, 'training/sps': 40516.434831601204, 'training/walltime': 917.1792097091675, 'training/entropy_loss': Array(-0.04329456, dtype=float32), 'training/policy_loss': Array(0.00073146, dtype=float32), 'training/total_loss': Array(120.86554, dtype=float32), 'training/v_loss': Array(120.90811, dtype=float32), 'eval/episode_goal_distance': (Array(0.42587733, dtype=float32), Array(0.17362136, dtype=float32)), 'eval/episode_reward': (Array(-9216.344, dtype=float32), Array(4615.342, dtype=float32)), 'eval/avg_episode_length': (Array(891.1875, dtype=float32), Array(310.50433, dtype=float32)), 'eval/epoch_eval_time': 4.085365533828735, 'eval/sps': 31331.34573640968}
I0726 22:04:08.015563 140267183036224 train.py:379] starting iteration 297 2140.436589241028
I0726 22:04:15.134896 140267183036224 train.py:394] {'eval/walltime': 1217.515790939331, 'training/sps': 40609.42937959366, 'training/walltime': 920.2051079273224, 'training/entropy_loss': Array(-0.04317208, dtype=float32), 'training/policy_loss': Array(0.00097877, dtype=float32), 'training/total_loss': Array(123.76015, dtype=float32), 'training/v_loss': Array(123.80235, dtype=float32), 'eval/episode_goal_distance': (Array(0.40193713, dtype=float32), Array(0.16274898, dtype=float32)), 'eval/episode_reward': (Array(-8583.908, dtype=float32), Array(4407.556, dtype=float32)), 'eval/avg_episode_length': (Array(867.9297, dtype=float32), Array(337.47623, dtype=float32)), 'eval/epoch_eval_time': 4.0898003578186035, 'eval/sps': 31297.371216494288}
I0726 22:04:15.139542 140267183036224 train.py:379] starting iteration 298 2147.5605523586273
I0726 22:04:22.258761 140267183036224 train.py:394] {'eval/walltime': 1221.6138017177582, 'training/sps': 40725.507085020006, 'training/walltime': 923.2223815917969, 'training/entropy_loss': Array(-0.04228809, dtype=float32), 'training/policy_loss': Array(0.00178769, dtype=float32), 'training/total_loss': Array(116.356155, dtype=float32), 'training/v_loss': Array(116.39665, dtype=float32), 'eval/episode_goal_distance': (Array(0.39406055, dtype=float32), Array(0.16521408, dtype=float32)), 'eval/episode_reward': (Array(-8232.097, dtype=float32), Array(4555.9883, dtype=float32)), 'eval/avg_episode_length': (Array(844.65625, dtype=float32), Array(360.98685, dtype=float32)), 'eval/epoch_eval_time': 4.098010778427124, 'eval/sps': 31234.666505471774}
I0726 22:04:22.261109 140267183036224 train.py:379] starting iteration 299 2154.6821353435516
I0726 22:04:29.383471 140267183036224 train.py:394] {'eval/walltime': 1225.705617427826, 'training/sps': 40594.699989965455, 'training/walltime': 926.2493777275085, 'training/entropy_loss': Array(-0.0426123, dtype=float32), 'training/policy_loss': Array(0.00123505, dtype=float32), 'training/total_loss': Array(125.59986, dtype=float32), 'training/v_loss': Array(125.641235, dtype=float32), 'eval/episode_goal_distance': (Array(0.42409712, dtype=float32), Array(0.17956737, dtype=float32)), 'eval/episode_reward': (Array(-8498.445, dtype=float32), Array(5144.468, dtype=float32)), 'eval/avg_episode_length': (Array(821.40625, dtype=float32), Array(381.59116, dtype=float32)), 'eval/epoch_eval_time': 4.091815710067749, 'eval/sps': 31281.956243791996}
I0726 22:04:29.385870 140267183036224 train.py:379] starting iteration 300 2161.8068969249725
I0726 22:04:36.501853 140267183036224 train.py:394] {'eval/walltime': 1229.7915875911713, 'training/sps': 40601.80586611543, 'training/walltime': 929.2758440971375, 'training/entropy_loss': Array(-0.04197768, dtype=float32), 'training/policy_loss': Array(0.00352971, dtype=float32), 'training/total_loss': Array(308.1038, dtype=float32), 'training/v_loss': Array(308.1422, dtype=float32), 'eval/episode_goal_distance': (Array(0.39284247, dtype=float32), Array(0.1647807, dtype=float32)), 'eval/episode_reward': (Array(-8684.049, dtype=float32), Array(4227.85, dtype=float32)), 'eval/avg_episode_length': (Array(891.25, dtype=float32), Array(310.32608, dtype=float32)), 'eval/epoch_eval_time': 4.085970163345337, 'eval/sps': 31326.709418553757}
I0726 22:04:36.504214 140267183036224 train.py:379] starting iteration 301 2168.9252395629883
I0726 22:04:43.624067 140267183036224 train.py:394] {'eval/walltime': 1233.875163078308, 'training/sps': 40519.0690700512, 'training/walltime': 932.3084902763367, 'training/entropy_loss': Array(-0.04103316, dtype=float32), 'training/policy_loss': Array(0.00113765, dtype=float32), 'training/total_loss': Array(130.7442, dtype=float32), 'training/v_loss': Array(130.78412, dtype=float32), 'eval/episode_goal_distance': (Array(0.4348687, dtype=float32), Array(0.16742127, dtype=float32)), 'eval/episode_reward': (Array(-8885.502, dtype=float32), Array(4797.6733, dtype=float32)), 'eval/avg_episode_length': (Array(852.41406, dtype=float32), Array(353.4938, dtype=float32)), 'eval/epoch_eval_time': 4.083575487136841, 'eval/sps': 31345.07991910441}
I0726 22:04:43.626432 140267183036224 train.py:379] starting iteration 302 2176.0474574565887
I0726 22:04:50.728645 140267183036224 train.py:394] {'eval/walltime': 1237.9508571624756, 'training/sps': 40649.487817285706, 'training/walltime': 935.3314065933228, 'training/entropy_loss': Array(-0.04184672, dtype=float32), 'training/policy_loss': Array(0.00151726, dtype=float32), 'training/total_loss': Array(124.677635, dtype=float32), 'training/v_loss': Array(124.717964, dtype=float32), 'eval/episode_goal_distance': (Array(0.40169033, dtype=float32), Array(0.17480181, dtype=float32)), 'eval/episode_reward': (Array(-8380.345, dtype=float32), Array(4762.2983, dtype=float32)), 'eval/avg_episode_length': (Array(852.4531, dtype=float32), Array(353.40048, dtype=float32)), 'eval/epoch_eval_time': 4.0756940841674805, 'eval/sps': 31405.6936945369}
I0726 22:04:50.730991 140267183036224 train.py:379] starting iteration 303 2183.152017354965
I0726 22:04:57.842773 140267183036224 train.py:394] {'eval/walltime': 1242.0291390419006, 'training/sps': 40553.99951089332, 'training/walltime': 938.3614406585693, 'training/entropy_loss': Array(-0.04322032, dtype=float32), 'training/policy_loss': Array(0.00104222, dtype=float32), 'training/total_loss': Array(129.94212, dtype=float32), 'training/v_loss': Array(129.9843, dtype=float32), 'eval/episode_goal_distance': (Array(0.39788067, dtype=float32), Array(0.17249553, dtype=float32)), 'eval/episode_reward': (Array(-8510.631, dtype=float32), Array(4665.2188, dtype=float32)), 'eval/avg_episode_length': (Array(860.0703, dtype=float32), Array(345.91592, dtype=float32)), 'eval/epoch_eval_time': 4.078281879425049, 'eval/sps': 31385.76581617877}
I0726 22:04:57.845112 140267183036224 train.py:379] starting iteration 304 2190.2661385536194
I0726 22:05:04.956283 140267183036224 train.py:394] {'eval/walltime': 1246.109834432602, 'training/sps': 40595.16042141784, 'training/walltime': 941.3884024620056, 'training/entropy_loss': Array(-0.04201584, dtype=float32), 'training/policy_loss': Array(0.00095412, dtype=float32), 'training/total_loss': Array(154.87866, dtype=float32), 'training/v_loss': Array(154.91971, dtype=float32), 'eval/episode_goal_distance': (Array(0.39699465, dtype=float32), Array(0.14919798, dtype=float32)), 'eval/episode_reward': (Array(-8384.326, dtype=float32), Array(4303.4756, dtype=float32)), 'eval/avg_episode_length': (Array(883.4922, dtype=float32), Array(319.77853, dtype=float32)), 'eval/epoch_eval_time': 4.080695390701294, 'eval/sps': 31367.202828144047}
I0726 22:05:04.958531 140267183036224 train.py:379] starting iteration 305 2197.3795573711395
I0726 22:05:12.064954 140267183036224 train.py:394] {'eval/walltime': 1250.182727098465, 'training/sps': 40552.806113464685, 'training/walltime': 944.4185256958008, 'training/entropy_loss': Array(-0.04116254, dtype=float32), 'training/policy_loss': Array(0.00126822, dtype=float32), 'training/total_loss': Array(142.2454, dtype=float32), 'training/v_loss': Array(142.28531, dtype=float32), 'eval/episode_goal_distance': (Array(0.39142528, dtype=float32), Array(0.14937037, dtype=float32)), 'eval/episode_reward': (Array(-7501.5312, dtype=float32), Array(5197.241, dtype=float32)), 'eval/avg_episode_length': (Array(751.41406, dtype=float32), Array(430.56424, dtype=float32)), 'eval/epoch_eval_time': 4.072892665863037, 'eval/sps': 31427.295168574514}
I0726 22:05:12.067142 140267183036224 train.py:379] starting iteration 306 2204.4881689548492
I0726 22:05:19.175956 140267183036224 train.py:394] {'eval/walltime': 1254.2702944278717, 'training/sps': 40720.32347700169, 'training/walltime': 947.4361834526062, 'training/entropy_loss': Array(-0.04223607, dtype=float32), 'training/policy_loss': Array(0.0016986, dtype=float32), 'training/total_loss': Array(157.692, dtype=float32), 'training/v_loss': Array(157.73254, dtype=float32), 'eval/episode_goal_distance': (Array(0.42397857, dtype=float32), Array(0.1815192, dtype=float32)), 'eval/episode_reward': (Array(-8711.604, dtype=float32), Array(5069.353, dtype=float32)), 'eval/avg_episode_length': (Array(829.2344, dtype=float32), Array(374.83768, dtype=float32)), 'eval/epoch_eval_time': 4.087567329406738, 'eval/sps': 31314.46889673073}
I0726 22:05:19.311382 140267183036224 train.py:379] starting iteration 307 2211.732397079468
I0726 22:05:26.423777 140267183036224 train.py:394] {'eval/walltime': 1258.3452770709991, 'training/sps': 40505.00041102593, 'training/walltime': 950.4698829650879, 'training/entropy_loss': Array(-0.04167153, dtype=float32), 'training/policy_loss': Array(0.00188683, dtype=float32), 'training/total_loss': Array(163.59604, dtype=float32), 'training/v_loss': Array(163.63583, dtype=float32), 'eval/episode_goal_distance': (Array(0.42708588, dtype=float32), Array(0.18285602, dtype=float32)), 'eval/episode_reward': (Array(-8835.7, dtype=float32), Array(4917.8984, dtype=float32)), 'eval/avg_episode_length': (Array(844.625, dtype=float32), Array(361.05945, dtype=float32)), 'eval/epoch_eval_time': 4.074982643127441, 'eval/sps': 31411.17673614516}
I0726 22:05:26.426056 140267183036224 train.py:379] starting iteration 308 2218.84708237648
I0726 22:05:33.534022 140267183036224 train.py:394] {'eval/walltime': 1262.431958436966, 'training/sps': 40717.23839271388, 'training/walltime': 953.4877693653107, 'training/entropy_loss': Array(-0.04080148, dtype=float32), 'training/policy_loss': Array(0.00173194, dtype=float32), 'training/total_loss': Array(152.38583, dtype=float32), 'training/v_loss': Array(152.42493, dtype=float32), 'eval/episode_goal_distance': (Array(0.43243262, dtype=float32), Array(0.17696114, dtype=float32)), 'eval/episode_reward': (Array(-8618.238, dtype=float32), Array(5291.3516, dtype=float32)), 'eval/avg_episode_length': (Array(813.5547, dtype=float32), Array(388.11777, dtype=float32)), 'eval/epoch_eval_time': 4.086681365966797, 'eval/sps': 31321.257650758565}
I0726 22:05:33.536291 140267183036224 train.py:379] starting iteration 309 2225.9573168754578
I0726 22:05:40.650276 140267183036224 train.py:394] {'eval/walltime': 1266.5136179924011, 'training/sps': 40570.650580603964, 'training/walltime': 956.5165598392487, 'training/entropy_loss': Array(-0.04467473, dtype=float32), 'training/policy_loss': Array(0.00442171, dtype=float32), 'training/total_loss': Array(198.92526, dtype=float32), 'training/v_loss': Array(198.96552, dtype=float32), 'eval/episode_goal_distance': (Array(0.444404, dtype=float32), Array(0.17526183, dtype=float32)), 'eval/episode_reward': (Array(-9709.932, dtype=float32), Array(4215.0547, dtype=float32)), 'eval/avg_episode_length': (Array(922.3672, dtype=float32), Array(266.67795, dtype=float32)), 'eval/epoch_eval_time': 4.081659555435181, 'eval/sps': 31359.793305042764}
I0726 22:05:40.652605 140267183036224 train.py:379] starting iteration 310 2233.0736310482025
I0726 22:05:47.763224 140267183036224 train.py:394] {'eval/walltime': 1270.5916302204132, 'training/sps': 40566.668531037, 'training/walltime': 959.5456476211548, 'training/entropy_loss': Array(-0.04656729, dtype=float32), 'training/policy_loss': Array(4.2392916e-05, dtype=float32), 'training/total_loss': Array(157.12875, dtype=float32), 'training/v_loss': Array(157.1753, dtype=float32), 'eval/episode_goal_distance': (Array(0.4576827, dtype=float32), Array(0.19206856, dtype=float32)), 'eval/episode_reward': (Array(-9921.885, dtype=float32), Array(4358.5806, dtype=float32)), 'eval/avg_episode_length': (Array(914.60156, dtype=float32), Array(278.51425, dtype=float32)), 'eval/epoch_eval_time': 4.078012228012085, 'eval/sps': 31387.8411449483}
I0726 22:05:47.765734 140267183036224 train.py:379] starting iteration 311 2240.1867587566376
I0726 22:05:54.870175 140267183036224 train.py:394] {'eval/walltime': 1274.6614315509796, 'training/sps': 40540.48376633873, 'training/walltime': 962.576691865921, 'training/entropy_loss': Array(-0.04762653, dtype=float32), 'training/policy_loss': Array(5.664189e-05, dtype=float32), 'training/total_loss': Array(164.75766, dtype=float32), 'training/v_loss': Array(164.80524, dtype=float32), 'eval/episode_goal_distance': (Array(0.45170838, dtype=float32), Array(0.18702899, dtype=float32)), 'eval/episode_reward': (Array(-9806.061, dtype=float32), Array(4461.8535, dtype=float32)), 'eval/avg_episode_length': (Array(914.5547, dtype=float32), Array(278.667, dtype=float32)), 'eval/epoch_eval_time': 4.069801330566406, 'eval/sps': 31451.166679476675}
I0726 22:05:54.872448 140267183036224 train.py:379] starting iteration 312 2247.2934749126434
I0726 22:06:01.984458 140267183036224 train.py:394] {'eval/walltime': 1278.7529590129852, 'training/sps': 40728.93780824016, 'training/walltime': 965.5937113761902, 'training/entropy_loss': Array(-0.04752861, dtype=float32), 'training/policy_loss': Array(0.00021988, dtype=float32), 'training/total_loss': Array(158.12753, dtype=float32), 'training/v_loss': Array(158.17485, dtype=float32), 'eval/episode_goal_distance': (Array(0.43192947, dtype=float32), Array(0.19194034, dtype=float32)), 'eval/episode_reward': (Array(-9352.195, dtype=float32), Array(5032.9487, dtype=float32)), 'eval/avg_episode_length': (Array(860.3203, dtype=float32), Array(345.29828, dtype=float32)), 'eval/epoch_eval_time': 4.091527462005615, 'eval/sps': 31284.160057245714}
I0726 22:06:01.986792 140267183036224 train.py:379] starting iteration 313 2254.407817840576
I0726 22:06:09.099338 140267183036224 train.py:394] {'eval/walltime': 1282.8318932056427, 'training/sps': 40553.91016319887, 'training/walltime': 968.623752117157, 'training/entropy_loss': Array(-0.04676699, dtype=float32), 'training/policy_loss': Array(-0.00036874, dtype=float32), 'training/total_loss': Array(148.04666, dtype=float32), 'training/v_loss': Array(148.0938, dtype=float32), 'eval/episode_goal_distance': (Array(0.45249677, dtype=float32), Array(0.20981857, dtype=float32)), 'eval/episode_reward': (Array(-10011.631, dtype=float32), Array(4062.993, dtype=float32)), 'eval/avg_episode_length': (Array(945.6953, dtype=float32), Array(225.77791, dtype=float32)), 'eval/epoch_eval_time': 4.078934192657471, 'eval/sps': 31380.746526976105}
I0726 22:06:09.101748 140267183036224 train.py:379] starting iteration 314 2261.522773742676
I0726 22:06:16.213893 140267183036224 train.py:394] {'eval/walltime': 1286.910005569458, 'training/sps': 40547.56110305941, 'training/walltime': 971.6542673110962, 'training/entropy_loss': Array(-0.04716343, dtype=float32), 'training/policy_loss': Array(7.385522e-05, dtype=float32), 'training/total_loss': Array(147.41048, dtype=float32), 'training/v_loss': Array(147.45757, dtype=float32), 'eval/episode_goal_distance': (Array(0.45251992, dtype=float32), Array(0.18057878, dtype=float32)), 'eval/episode_reward': (Array(-9390.577, dtype=float32), Array(4524.461, dtype=float32)), 'eval/avg_episode_length': (Array(891.34375, dtype=float32), Array(310.0588, dtype=float32)), 'eval/epoch_eval_time': 4.078112363815308, 'eval/sps': 31387.070433794685}
I0726 22:06:16.216363 140267183036224 train.py:379] starting iteration 315 2268.637388944626
I0726 22:06:23.312550 140267183036224 train.py:394] {'eval/walltime': 1290.9813635349274, 'training/sps': 40672.87009101591, 'training/walltime': 974.6754457950592, 'training/entropy_loss': Array(-0.04699823, dtype=float32), 'training/policy_loss': Array(-0.00020036, dtype=float32), 'training/total_loss': Array(145.60225, dtype=float32), 'training/v_loss': Array(145.64946, dtype=float32), 'eval/episode_goal_distance': (Array(0.4734536, dtype=float32), Array(0.21264492, dtype=float32)), 'eval/episode_reward': (Array(-9960.454, dtype=float32), Array(4730.316, dtype=float32)), 'eval/avg_episode_length': (Array(922.2969, dtype=float32), Array(266.91934, dtype=float32)), 'eval/epoch_eval_time': 4.07135796546936, 'eval/sps': 31439.141702992878}
I0726 22:06:23.314843 140267183036224 train.py:379] starting iteration 316 2275.735868692398
I0726 22:06:30.420083 140267183036224 train.py:394] {'eval/walltime': 1295.0609822273254, 'training/sps': 40658.056164081194, 'training/walltime': 977.6977250576019, 'training/entropy_loss': Array(-0.04755599, dtype=float32), 'training/policy_loss': Array(6.0832965e-05, dtype=float32), 'training/total_loss': Array(253.4808, dtype=float32), 'training/v_loss': Array(253.5283, dtype=float32), 'eval/episode_goal_distance': (Array(0.45082328, dtype=float32), Array(0.20359536, dtype=float32)), 'eval/episode_reward': (Array(-9862.672, dtype=float32), Array(4315.6436, dtype=float32)), 'eval/avg_episode_length': (Array(930.0625, dtype=float32), Array(254.30971, dtype=float32)), 'eval/epoch_eval_time': 4.079618692398071, 'eval/sps': 31375.481301356467}
I0726 22:06:30.422306 140267183036224 train.py:379] starting iteration 317 2282.8433318138123
I0726 22:06:37.527651 140267183036224 train.py:394] {'eval/walltime': 1299.136920928955, 'training/sps': 40608.773446545005, 'training/walltime': 980.7236721515656, 'training/entropy_loss': Array(-0.04701594, dtype=float32), 'training/policy_loss': Array(-0.0005846, dtype=float32), 'training/total_loss': Array(172.06772, dtype=float32), 'training/v_loss': Array(172.1153, dtype=float32), 'eval/episode_goal_distance': (Array(0.44621766, dtype=float32), Array(0.20841993, dtype=float32)), 'eval/episode_reward': (Array(-10060.081, dtype=float32), Array(4170.034, dtype=float32)), 'eval/avg_episode_length': (Array(953.3828, dtype=float32), Array(210.20886, dtype=float32)), 'eval/epoch_eval_time': 4.075938701629639, 'eval/sps': 31403.808881822275}
I0726 22:06:37.530164 140267183036224 train.py:379] starting iteration 318 2289.951189994812
I0726 22:06:44.630234 140267183036224 train.py:394] {'eval/walltime': 1303.2064878940582, 'training/sps': 40594.134057280695, 'training/walltime': 983.7507104873657, 'training/entropy_loss': Array(-0.0463036, dtype=float32), 'training/policy_loss': Array(-0.00034157, dtype=float32), 'training/total_loss': Array(141.23265, dtype=float32), 'training/v_loss': Array(141.2793, dtype=float32), 'eval/episode_goal_distance': (Array(0.43280727, dtype=float32), Array(0.2044626, dtype=float32)), 'eval/episode_reward': (Array(-9428.113, dtype=float32), Array(4244.901, dtype=float32)), 'eval/avg_episode_length': (Array(937.875, dtype=float32), Array(240.6096, dtype=float32)), 'eval/epoch_eval_time': 4.069566965103149, 'eval/sps': 31452.977945223625}
I0726 22:06:44.633137 140267183036224 train.py:379] starting iteration 319 2297.054162979126
I0726 22:06:51.740823 140267183036224 train.py:394] {'eval/walltime': 1307.2920129299164, 'training/sps': 40708.19493797561, 'training/walltime': 986.7692673206329, 'training/entropy_loss': Array(-0.04619364, dtype=float32), 'training/policy_loss': Array(-0.00034125, dtype=float32), 'training/total_loss': Array(147.19638, dtype=float32), 'training/v_loss': Array(147.24292, dtype=float32), 'eval/episode_goal_distance': (Array(0.42600596, dtype=float32), Array(0.18225214, dtype=float32)), 'eval/episode_reward': (Array(-9585.504, dtype=float32), Array(3824.361, dtype=float32)), 'eval/avg_episode_length': (Array(961.1953, dtype=float32), Array(192.46541, dtype=float32)), 'eval/epoch_eval_time': 4.085525035858154, 'eval/sps': 31330.122536653093}
I0726 22:06:51.743298 140267183036224 train.py:379] starting iteration 320 2304.164323568344
I0726 22:06:58.858022 140267183036224 train.py:394] {'eval/walltime': 1311.3713240623474, 'training/sps': 40540.19358129165, 'training/walltime': 989.8003332614899, 'training/entropy_loss': Array(-0.0448258, dtype=float32), 'training/policy_loss': Array(-0.00052294, dtype=float32), 'training/total_loss': Array(132.52383, dtype=float32), 'training/v_loss': Array(132.56917, dtype=float32), 'eval/episode_goal_distance': (Array(0.42497814, dtype=float32), Array(0.1975434, dtype=float32)), 'eval/episode_reward': (Array(-9676.346, dtype=float32), Array(3869.3474, dtype=float32)), 'eval/avg_episode_length': (Array(945.6406, dtype=float32), Array(226.00542, dtype=float32)), 'eval/epoch_eval_time': 4.07931113243103, 'eval/sps': 31377.846858108984}
I0726 22:06:58.860596 140267183036224 train.py:379] starting iteration 321 2311.2816224098206
I0726 22:07:05.969868 140267183036224 train.py:394] {'eval/walltime': 1315.444242477417, 'training/sps': 40517.396750800785, 'training/walltime': 992.8331046104431, 'training/entropy_loss': Array(-0.04443267, dtype=float32), 'training/policy_loss': Array(-0.00039448, dtype=float32), 'training/total_loss': Array(169.4111, dtype=float32), 'training/v_loss': Array(169.45593, dtype=float32), 'eval/episode_goal_distance': (Array(0.42660043, dtype=float32), Array(0.1717334, dtype=float32)), 'eval/episode_reward': (Array(-9225.209, dtype=float32), Array(3991.066, dtype=float32)), 'eval/avg_episode_length': (Array(930.1172, dtype=float32), Array(254.11086, dtype=float32)), 'eval/epoch_eval_time': 4.07291841506958, 'eval/sps': 31427.096483545276}
I0726 22:07:05.974689 140267183036224 train.py:379] starting iteration 322 2318.3956999778748
I0726 22:07:13.071259 140267183036224 train.py:394] {'eval/walltime': 1319.5155975818634, 'training/sps': 40670.79029218563, 'training/walltime': 995.8544375896454, 'training/entropy_loss': Array(-0.04294448, dtype=float32), 'training/policy_loss': Array(-9.9977e-05, dtype=float32), 'training/total_loss': Array(152.1989, dtype=float32), 'training/v_loss': Array(152.24194, dtype=float32), 'eval/episode_goal_distance': (Array(0.39725846, dtype=float32), Array(0.15902707, dtype=float32)), 'eval/episode_reward': (Array(-8595.689, dtype=float32), Array(3880.9644, dtype=float32)), 'eval/avg_episode_length': (Array(899.1172, dtype=float32), Array(300.05133, dtype=float32)), 'eval/epoch_eval_time': 4.071355104446411, 'eval/sps': 31439.163795908775}
I0726 22:07:13.073613 140267183036224 train.py:379] starting iteration 323 2325.494639635086
I0726 22:07:20.192577 140267183036224 train.py:394] {'eval/walltime': 1323.6044309139252, 'training/sps': 40598.27180489995, 'training/walltime': 998.8811674118042, 'training/entropy_loss': Array(-0.04155301, dtype=float32), 'training/policy_loss': Array(-0.00018405, dtype=float32), 'training/total_loss': Array(140.80997, dtype=float32), 'training/v_loss': Array(140.85168, dtype=float32), 'eval/episode_goal_distance': (Array(0.39762053, dtype=float32), Array(0.15017462, dtype=float32)), 'eval/episode_reward': (Array(-8786.191, dtype=float32), Array(3999.0535, dtype=float32)), 'eval/avg_episode_length': (Array(899.0625, dtype=float32), Array(300.2139, dtype=float32)), 'eval/epoch_eval_time': 4.088833332061768, 'eval/sps': 31304.773172414156}
I0726 22:07:20.194910 140267183036224 train.py:379] starting iteration 324 2332.6159348487854
I0726 22:07:27.304023 140267183036224 train.py:394] {'eval/walltime': 1327.6807231903076, 'training/sps': 40563.55879702711, 'training/walltime': 1001.9104874134064, 'training/entropy_loss': Array(-0.04088204, dtype=float32), 'training/policy_loss': Array(5.9235776e-05, dtype=float32), 'training/total_loss': Array(134.61867, dtype=float32), 'training/v_loss': Array(134.65948, dtype=float32), 'eval/episode_goal_distance': (Array(0.42884493, dtype=float32), Array(0.16053584, dtype=float32)), 'eval/episode_reward': (Array(-9652.211, dtype=float32), Array(3917.5967, dtype=float32)), 'eval/avg_episode_length': (Array(930.10156, dtype=float32), Array(254.16748, dtype=float32)), 'eval/epoch_eval_time': 4.076292276382446, 'eval/sps': 31401.08493731345}
I0726 22:07:27.306412 140267183036224 train.py:379] starting iteration 325 2339.7274379730225
I0726 22:07:34.415393 140267183036224 train.py:394] {'eval/walltime': 1331.7560131549835, 'training/sps': 40551.10867449423, 'training/walltime': 1004.9407374858856, 'training/entropy_loss': Array(-0.04038742, dtype=float32), 'training/policy_loss': Array(-0.00022799, dtype=float32), 'training/total_loss': Array(118.152214, dtype=float32), 'training/v_loss': Array(118.19283, dtype=float32), 'eval/episode_goal_distance': (Array(0.39076054, dtype=float32), Array(0.15199074, dtype=float32)), 'eval/episode_reward': (Array(-8592.227, dtype=float32), Array(4054.5464, dtype=float32)), 'eval/avg_episode_length': (Array(899.03906, dtype=float32), Array(300.2833, dtype=float32)), 'eval/epoch_eval_time': 4.075289964675903, 'eval/sps': 31408.807988999008}
I0726 22:07:34.417748 140267183036224 train.py:379] starting iteration 326 2346.838773727417
I0726 22:07:41.543412 140267183036224 train.py:394] {'eval/walltime': 1335.8552758693695, 'training/sps': 40650.690118155275, 'training/walltime': 1007.9635643959045, 'training/entropy_loss': Array(-0.03930254, dtype=float32), 'training/policy_loss': Array(-0.00033333, dtype=float32), 'training/total_loss': Array(156.1694, dtype=float32), 'training/v_loss': Array(156.20903, dtype=float32), 'eval/episode_goal_distance': (Array(0.37840822, dtype=float32), Array(0.1409105, dtype=float32)), 'eval/episode_reward': (Array(-8525.49, dtype=float32), Array(3761.5918, dtype=float32)), 'eval/avg_episode_length': (Array(914.5703, dtype=float32), Array(278.6165, dtype=float32)), 'eval/epoch_eval_time': 4.099262714385986, 'eval/sps': 31225.12727735057}
I0726 22:07:41.545912 140267183036224 train.py:379] starting iteration 327 2353.966938018799
I0726 22:07:48.654113 140267183036224 train.py:394] {'eval/walltime': 1339.9364168643951, 'training/sps': 40642.56074370302, 'training/walltime': 1010.9869959354401, 'training/entropy_loss': Array(-0.03744148, dtype=float32), 'training/policy_loss': Array(-0.00012145, dtype=float32), 'training/total_loss': Array(123.046875, dtype=float32), 'training/v_loss': Array(123.08445, dtype=float32), 'eval/episode_goal_distance': (Array(0.37753093, dtype=float32), Array(0.13888586, dtype=float32)), 'eval/episode_reward': (Array(-8666.396, dtype=float32), Array(4080.2234, dtype=float32)), 'eval/avg_episode_length': (Array(906.8281, dtype=float32), Array(289.68378, dtype=float32)), 'eval/epoch_eval_time': 4.081140995025635, 'eval/sps': 31363.777962098073}
I0726 22:07:48.656435 140267183036224 train.py:379] starting iteration 328 2361.0774602890015
I0726 22:07:55.766430 140267183036224 train.py:394] {'eval/walltime': 1344.0141897201538, 'training/sps': 40571.97597497271, 'training/walltime': 1014.0156874656677, 'training/entropy_loss': Array(-0.03759806, dtype=float32), 'training/policy_loss': Array(0.00023386, dtype=float32), 'training/total_loss': Array(112.56574, dtype=float32), 'training/v_loss': Array(112.6031, dtype=float32), 'eval/episode_goal_distance': (Array(0.37375286, dtype=float32), Array(0.1238388, dtype=float32)), 'eval/episode_reward': (Array(-8119.8584, dtype=float32), Array(4024.5276, dtype=float32)), 'eval/avg_episode_length': (Array(875.75, dtype=float32), Array(328.73544, dtype=float32)), 'eval/epoch_eval_time': 4.077772855758667, 'eval/sps': 31389.683665002885}
I0726 22:07:55.768791 140267183036224 train.py:379] starting iteration 329 2368.1898176670074
I0726 22:08:02.869392 140267183036224 train.py:394] {'eval/walltime': 1348.087346315384, 'training/sps': 40636.89518510625, 'training/walltime': 1017.0395405292511, 'training/entropy_loss': Array(-0.03688395, dtype=float32), 'training/policy_loss': Array(-0.00011834, dtype=float32), 'training/total_loss': Array(105.46055, dtype=float32), 'training/v_loss': Array(105.49755, dtype=float32), 'eval/episode_goal_distance': (Array(0.39896572, dtype=float32), Array(0.13445742, dtype=float32)), 'eval/episode_reward': (Array(-8829.96, dtype=float32), Array(4112.174, dtype=float32)), 'eval/avg_episode_length': (Array(906.83594, dtype=float32), Array(289.6591, dtype=float32)), 'eval/epoch_eval_time': 4.0731565952301025, 'eval/sps': 31425.258766111587}
I0726 22:08:02.871930 140267183036224 train.py:379] starting iteration 330 2375.292956352234
I0726 22:08:09.982870 140267183036224 train.py:394] {'eval/walltime': 1352.1750755310059, 'training/sps': 40692.65485828521, 'training/walltime': 1020.0592501163483, 'training/entropy_loss': Array(-0.03607288, dtype=float32), 'training/policy_loss': Array(0.00032331, dtype=float32), 'training/total_loss': Array(99.00702, dtype=float32), 'training/v_loss': Array(99.04277, dtype=float32), 'eval/episode_goal_distance': (Array(0.37695378, dtype=float32), Array(0.13588579, dtype=float32)), 'eval/episode_reward': (Array(-8775.649, dtype=float32), Array(3261.598, dtype=float32)), 'eval/avg_episode_length': (Array(961.21094, dtype=float32), Array(192.3885, dtype=float32)), 'eval/epoch_eval_time': 4.087729215621948, 'eval/sps': 31313.228750775947}
I0726 22:08:09.985367 140267183036224 train.py:379] starting iteration 331 2382.406393289566
I0726 22:08:17.099093 140267183036224 train.py:394] {'eval/walltime': 1356.2580087184906, 'training/sps': 40592.26691107272, 'training/walltime': 1023.0864276885986, 'training/entropy_loss': Array(-0.03635387, dtype=float32), 'training/policy_loss': Array(0.00019071, dtype=float32), 'training/total_loss': Array(98.53999, dtype=float32), 'training/v_loss': Array(98.576164, dtype=float32), 'eval/episode_goal_distance': (Array(0.3778733, dtype=float32), Array(0.14058149, dtype=float32)), 'eval/episode_reward': (Array(-8785.192, dtype=float32), Array(3724.8235, dtype=float32)), 'eval/avg_episode_length': (Array(922.4219, dtype=float32), Array(266.48978, dtype=float32)), 'eval/epoch_eval_time': 4.082933187484741, 'eval/sps': 31350.01091674816}
I0726 22:08:17.101605 140267183036224 train.py:379] starting iteration 332 2389.522630929947
I0726 22:08:24.223744 140267183036224 train.py:394] {'eval/walltime': 1360.343802690506, 'training/sps': 40561.60188854494, 'training/walltime': 1026.1158938407898, 'training/entropy_loss': Array(-0.03553046, dtype=float32), 'training/policy_loss': Array(0.00026267, dtype=float32), 'training/total_loss': Array(100.03641, dtype=float32), 'training/v_loss': Array(100.071655, dtype=float32), 'eval/episode_goal_distance': (Array(0.37392998, dtype=float32), Array(0.13893642, dtype=float32)), 'eval/episode_reward': (Array(-8475.132, dtype=float32), Array(4142.14, dtype=float32)), 'eval/avg_episode_length': (Array(899.0156, dtype=float32), Array(300.35303, dtype=float32)), 'eval/epoch_eval_time': 4.085793972015381, 'eval/sps': 31328.060317456984}
I0726 22:08:24.226249 140267183036224 train.py:379] starting iteration 333 2396.647274494171
I0726 22:08:31.331379 140267183036224 train.py:394] {'eval/walltime': 1364.422913312912, 'training/sps': 40658.02729756878, 'training/walltime': 1029.1381752490997, 'training/entropy_loss': Array(-0.03479651, dtype=float32), 'training/policy_loss': Array(-9.113086e-05, dtype=float32), 'training/total_loss': Array(232.49426, dtype=float32), 'training/v_loss': Array(232.52914, dtype=float32), 'eval/episode_goal_distance': (Array(0.3619952, dtype=float32), Array(0.12675369, dtype=float32)), 'eval/episode_reward': (Array(-7705.921, dtype=float32), Array(4177.017, dtype=float32)), 'eval/avg_episode_length': (Array(844.7344, dtype=float32), Array(360.80527, dtype=float32)), 'eval/epoch_eval_time': 4.079110622406006, 'eval/sps': 31379.389246497318}
I0726 22:08:31.333599 140267183036224 train.py:379] starting iteration 334 2403.7546253204346
I0726 22:08:38.453927 140267183036224 train.py:394] {'eval/walltime': 1368.5208129882812, 'training/sps': 40702.414635038156, 'training/walltime': 1032.1571607589722, 'training/entropy_loss': Array(-0.03437711, dtype=float32), 'training/policy_loss': Array(0.00015268, dtype=float32), 'training/total_loss': Array(102.87232, dtype=float32), 'training/v_loss': Array(102.90655, dtype=float32), 'eval/episode_goal_distance': (Array(0.37139213, dtype=float32), Array(0.12232222, dtype=float32)), 'eval/episode_reward': (Array(-8616.098, dtype=float32), Array(3692.8152, dtype=float32)), 'eval/avg_episode_length': (Array(922.39844, dtype=float32), Array(266.57053, dtype=float32)), 'eval/epoch_eval_time': 4.097899675369263, 'eval/sps': 31235.51334586196}
I0726 22:08:38.456130 140267183036224 train.py:379] starting iteration 335 2410.8771562576294
I0726 22:08:45.580025 140267183036224 train.py:394] {'eval/walltime': 1372.6125557422638, 'training/sps': 40572.53809608491, 'training/walltime': 1035.18581032753, 'training/entropy_loss': Array(-0.03470856, dtype=float32), 'training/policy_loss': Array(0.00029314, dtype=float32), 'training/total_loss': Array(90.96991, dtype=float32), 'training/v_loss': Array(91.004326, dtype=float32), 'eval/episode_goal_distance': (Array(0.3587957, dtype=float32), Array(0.13544585, dtype=float32)), 'eval/episode_reward': (Array(-8309.911, dtype=float32), Array(3741.9521, dtype=float32)), 'eval/avg_episode_length': (Array(922.40625, dtype=float32), Array(266.5443, dtype=float32)), 'eval/epoch_eval_time': 4.091742753982544, 'eval/sps': 31282.514003456356}
I0726 22:08:45.582200 140267183036224 train.py:379] starting iteration 336 2418.0032255649567
I0726 22:08:52.698305 140267183036224 train.py:394] {'eval/walltime': 1376.69864320755, 'training/sps': 40602.128919594645, 'training/walltime': 1038.2122526168823, 'training/entropy_loss': Array(-0.03301514, dtype=float32), 'training/policy_loss': Array(0.00020171, dtype=float32), 'training/total_loss': Array(88.58797, dtype=float32), 'training/v_loss': Array(88.62078, dtype=float32), 'eval/episode_goal_distance': (Array(0.3648852, dtype=float32), Array(0.11722989, dtype=float32)), 'eval/episode_reward': (Array(-8091.976, dtype=float32), Array(3964.5237, dtype=float32)), 'eval/avg_episode_length': (Array(883.4922, dtype=float32), Array(319.77838, dtype=float32)), 'eval/epoch_eval_time': 4.086087465286255, 'eval/sps': 31325.810102558054}
I0726 22:08:52.700496 140267183036224 train.py:379] starting iteration 337 2425.1215229034424
I0726 22:08:59.814517 140267183036224 train.py:394] {'eval/walltime': 1380.7839946746826, 'training/sps': 40620.477835701764, 'training/walltime': 1041.2373278141022, 'training/entropy_loss': Array(-0.03236198, dtype=float32), 'training/policy_loss': Array(0.00044273, dtype=float32), 'training/total_loss': Array(108.3493, dtype=float32), 'training/v_loss': Array(108.38122, dtype=float32), 'eval/episode_goal_distance': (Array(0.3549133, dtype=float32), Array(0.12730397, dtype=float32)), 'eval/episode_reward': (Array(-8349.568, dtype=float32), Array(3243.267, dtype=float32)), 'eval/avg_episode_length': (Array(953.3828, dtype=float32), Array(210.20876, dtype=float32)), 'eval/epoch_eval_time': 4.085351467132568, 'eval/sps': 31331.45361660665}
I0726 22:08:59.817670 140267183036224 train.py:379] starting iteration 338 2432.238679409027
I0726 22:09:06.928107 140267183036224 train.py:394] {'eval/walltime': 1384.8716278076172, 'training/sps': 40703.719721244524, 'training/walltime': 1044.2562165260315, 'training/entropy_loss': Array(-0.03152477, dtype=float32), 'training/policy_loss': Array(4.333361e-05, dtype=float32), 'training/total_loss': Array(98.48326, dtype=float32), 'training/v_loss': Array(98.51475, dtype=float32), 'eval/episode_goal_distance': (Array(0.35583955, dtype=float32), Array(0.12618403, dtype=float32)), 'eval/episode_reward': (Array(-8350.911, dtype=float32), Array(3760.116, dtype=float32)), 'eval/avg_episode_length': (Array(922.35156, dtype=float32), Array(266.73184, dtype=float32)), 'eval/epoch_eval_time': 4.08763313293457, 'eval/sps': 31313.964790207818}
I0726 22:09:06.930290 140267183036224 train.py:379] starting iteration 339 2439.3513164520264
I0726 22:09:14.054061 140267183036224 train.py:394] {'eval/walltime': 1388.965609073639, 'training/sps': 40604.684741692676, 'training/walltime': 1047.2824683189392, 'training/entropy_loss': Array(-0.03177667, dtype=float32), 'training/policy_loss': Array(0.00059037, dtype=float32), 'training/total_loss': Array(110.8858, dtype=float32), 'training/v_loss': Array(110.916985, dtype=float32), 'eval/episode_goal_distance': (Array(0.3708083, dtype=float32), Array(0.13417791, dtype=float32)), 'eval/episode_reward': (Array(-8799.59, dtype=float32), Array(3894.197, dtype=float32)), 'eval/avg_episode_length': (Array(930.10156, dtype=float32), Array(254.16766, dtype=float32)), 'eval/epoch_eval_time': 4.0939812660217285, 'eval/sps': 31265.409312530217}
I0726 22:09:14.056420 140267183036224 train.py:379] starting iteration 340 2446.4774463176727
I0726 22:09:21.174890 140267183036224 train.py:394] {'eval/walltime': 1393.050583600998, 'training/sps': 40555.76739980784, 'training/walltime': 1050.312370300293, 'training/entropy_loss': Array(-0.03192212, dtype=float32), 'training/policy_loss': Array(0.00023433, dtype=float32), 'training/total_loss': Array(98.97168, dtype=float32), 'training/v_loss': Array(99.003365, dtype=float32), 'eval/episode_goal_distance': (Array(0.35101026, dtype=float32), Array(0.13306712, dtype=float32)), 'eval/episode_reward': (Array(-8754.668, dtype=float32), Array(3481.9128, dtype=float32)), 'eval/avg_episode_length': (Array(953.4219, dtype=float32), Array(210.03271, dtype=float32)), 'eval/epoch_eval_time': 4.084974527359009, 'eval/sps': 31334.34471689441}
I0726 22:09:21.177848 140267183036224 train.py:379] starting iteration 341 2453.5988578796387
I0726 22:09:28.290929 140267183036224 train.py:394] {'eval/walltime': 1397.135148525238, 'training/sps': 40628.345362168904, 'training/walltime': 1053.336859703064, 'training/entropy_loss': Array(-0.03188871, dtype=float32), 'training/policy_loss': Array(0.00023461, dtype=float32), 'training/total_loss': Array(87.09659, dtype=float32), 'training/v_loss': Array(87.12824, dtype=float32), 'eval/episode_goal_distance': (Array(0.36218154, dtype=float32), Array(0.12606846, dtype=float32)), 'eval/episode_reward': (Array(-8507.93, dtype=float32), Array(3564.6619, dtype=float32)), 'eval/avg_episode_length': (Array(945.66406, dtype=float32), Array(225.90788, dtype=float32)), 'eval/epoch_eval_time': 4.084564924240112, 'eval/sps': 31337.48694760017}
I0726 22:09:28.294749 140267183036224 train.py:379] starting iteration 342 2460.7157588005066
I0726 22:09:35.395594 140267183036224 train.py:394] {'eval/walltime': 1401.213496208191, 'training/sps': 40707.60332989783, 'training/walltime': 1056.3554604053497, 'training/entropy_loss': Array(-0.0308093, dtype=float32), 'training/policy_loss': Array(0.00035067, dtype=float32), 'training/total_loss': Array(89.9367, dtype=float32), 'training/v_loss': Array(89.96716, dtype=float32), 'eval/episode_goal_distance': (Array(0.34453213, dtype=float32), Array(0.10971816, dtype=float32)), 'eval/episode_reward': (Array(-8136.8037, dtype=float32), Array(3380.9983, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.75833, dtype=float32)), 'eval/epoch_eval_time': 4.078347682952881, 'eval/sps': 31385.259411557345}
I0726 22:09:35.397755 140267183036224 train.py:379] starting iteration 343 2467.8187816143036
I0726 22:09:42.525977 140267183036224 train.py:394] {'eval/walltime': 1405.3107755184174, 'training/sps': 40587.772397096334, 'training/walltime': 1059.3829731941223, 'training/entropy_loss': Array(-0.03059485, dtype=float32), 'training/policy_loss': Array(0.00029967, dtype=float32), 'training/total_loss': Array(110.70385, dtype=float32), 'training/v_loss': Array(110.734146, dtype=float32), 'eval/episode_goal_distance': (Array(0.3648827, dtype=float32), Array(0.11726458, dtype=float32)), 'eval/episode_reward': (Array(-8309.533, dtype=float32), Array(3772.2944, dtype=float32)), 'eval/avg_episode_length': (Array(906.7969, dtype=float32), Array(289.78064, dtype=float32)), 'eval/epoch_eval_time': 4.09727931022644, 'eval/sps': 31240.24268507239}
I0726 22:09:42.528144 140267183036224 train.py:379] starting iteration 344 2474.9491698741913
I0726 22:09:49.644283 140267183036224 train.py:394] {'eval/walltime': 1409.3946733474731, 'training/sps': 40571.5352318133, 'training/walltime': 1062.411697626114, 'training/entropy_loss': Array(-0.02974484, dtype=float32), 'training/policy_loss': Array(0.00017893, dtype=float32), 'training/total_loss': Array(82.40189, dtype=float32), 'training/v_loss': Array(82.43146, dtype=float32), 'eval/episode_goal_distance': (Array(0.34914306, dtype=float32), Array(0.12285157, dtype=float32)), 'eval/episode_reward': (Array(-8307.816, dtype=float32), Array(3399.7166, dtype=float32)), 'eval/avg_episode_length': (Array(945.5625, dtype=float32), Array(226.32986, dtype=float32)), 'eval/epoch_eval_time': 4.083897829055786, 'eval/sps': 31342.605852995624}
I0726 22:09:49.646461 140267183036224 train.py:379] starting iteration 345 2482.0674872398376
I0726 22:09:56.770292 140267183036224 train.py:394] {'eval/walltime': 1413.4848983287811, 'training/sps': 40554.222881851914, 'training/walltime': 1065.44171500206, 'training/entropy_loss': Array(-0.02984679, dtype=float32), 'training/policy_loss': Array(0.00013157, dtype=float32), 'training/total_loss': Array(75.0157, dtype=float32), 'training/v_loss': Array(75.04542, dtype=float32), 'eval/episode_goal_distance': (Array(0.34103286, dtype=float32), Array(0.1223366, dtype=float32)), 'eval/episode_reward': (Array(-7946.8613, dtype=float32), Array(3723.1782, dtype=float32)), 'eval/avg_episode_length': (Array(906.875, dtype=float32), Array(289.53812, dtype=float32)), 'eval/epoch_eval_time': 4.090224981307983, 'eval/sps': 31294.122104517537}
I0726 22:09:56.772610 140267183036224 train.py:379] starting iteration 346 2489.1936359405518
I0726 22:10:03.877233 140267183036224 train.py:394] {'eval/walltime': 1417.5658314228058, 'training/sps': 40685.831901345344, 'training/walltime': 1068.4619309902191, 'training/entropy_loss': Array(-0.03146954, dtype=float32), 'training/policy_loss': Array(0.00028796, dtype=float32), 'training/total_loss': Array(75.94499, dtype=float32), 'training/v_loss': Array(75.97618, dtype=float32), 'eval/episode_goal_distance': (Array(0.36391324, dtype=float32), Array(0.12431435, dtype=float32)), 'eval/episode_reward': (Array(-8317.029, dtype=float32), Array(3607.8083, dtype=float32)), 'eval/avg_episode_length': (Array(906.8203, dtype=float32), Array(289.70798, dtype=float32)), 'eval/epoch_eval_time': 4.080933094024658, 'eval/sps': 31365.3757733541}
I0726 22:10:03.879450 140267183036224 train.py:379] starting iteration 347 2496.3004751205444
I0726 22:10:10.998518 140267183036224 train.py:394] {'eval/walltime': 1421.6591260433197, 'training/sps': 40657.82202577366, 'training/walltime': 1071.4842276573181, 'training/entropy_loss': Array(-0.03257579, dtype=float32), 'training/policy_loss': Array(0.00011395, dtype=float32), 'training/total_loss': Array(78.60961, dtype=float32), 'training/v_loss': Array(78.642075, dtype=float32), 'eval/episode_goal_distance': (Array(0.34637463, dtype=float32), Array(0.13430297, dtype=float32)), 'eval/episode_reward': (Array(-8239.248, dtype=float32), Array(3516.645, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.08253, dtype=float32)), 'eval/epoch_eval_time': 4.093294620513916, 'eval/sps': 31270.654049311874}
I0726 22:10:11.000728 140267183036224 train.py:379] starting iteration 348 2503.4217541217804
I0726 22:10:18.112303 140267183036224 train.py:394] {'eval/walltime': 1425.7393827438354, 'training/sps': 40585.21550763916, 'training/walltime': 1074.511931180954, 'training/entropy_loss': Array(-0.03280292, dtype=float32), 'training/policy_loss': Array(1.1810255e-05, dtype=float32), 'training/total_loss': Array(77.52014, dtype=float32), 'training/v_loss': Array(77.552925, dtype=float32), 'eval/episode_goal_distance': (Array(0.3629818, dtype=float32), Array(0.12742059, dtype=float32)), 'eval/episode_reward': (Array(-7854.099, dtype=float32), Array(4030.4326, dtype=float32)), 'eval/avg_episode_length': (Array(867.8594, dtype=float32), Array(337.65536, dtype=float32)), 'eval/epoch_eval_time': 4.080256700515747, 'eval/sps': 31370.57528361407}
I0726 22:10:18.114665 140267183036224 train.py:379] starting iteration 349 2510.535691022873
I0726 22:10:25.238060 140267183036224 train.py:394] {'eval/walltime': 1429.8286159038544, 'training/sps': 40546.78276215221, 'training/walltime': 1077.5425045490265, 'training/entropy_loss': Array(-0.03335901, dtype=float32), 'training/policy_loss': Array(-0.00022027, dtype=float32), 'training/total_loss': Array(81.62545, dtype=float32), 'training/v_loss': Array(81.65903, dtype=float32), 'eval/episode_goal_distance': (Array(0.35906968, dtype=float32), Array(0.13159731, dtype=float32)), 'eval/episode_reward': (Array(-8292.923, dtype=float32), Array(3939.9502, dtype=float32)), 'eval/avg_episode_length': (Array(906.77344, dtype=float32), Array(289.8535, dtype=float32)), 'eval/epoch_eval_time': 4.089233160018921, 'eval/sps': 31301.712323835247}
I0726 22:10:25.240218 140267183036224 train.py:379] starting iteration 350 2517.6612441539764
I0726 22:10:32.355140 140267183036224 train.py:394] {'eval/walltime': 1433.9119670391083, 'training/sps': 40582.422474108826, 'training/walltime': 1080.5704164505005, 'training/entropy_loss': Array(-0.03372471, dtype=float32), 'training/policy_loss': Array(0.00025896, dtype=float32), 'training/total_loss': Array(231.70032, dtype=float32), 'training/v_loss': Array(231.73376, dtype=float32), 'eval/episode_goal_distance': (Array(0.37241375, dtype=float32), Array(0.14635855, dtype=float32)), 'eval/episode_reward': (Array(-8905.4375, dtype=float32), Array(3512.2466, dtype=float32)), 'eval/avg_episode_length': (Array(945.5703, dtype=float32), Array(226.29768, dtype=float32)), 'eval/epoch_eval_time': 4.083351135253906, 'eval/sps': 31346.802114298418}
I0726 22:10:32.357428 140267183036224 train.py:379] starting iteration 351 2524.77845454216
I0726 22:10:39.481047 140267183036224 train.py:394] {'eval/walltime': 1438.00874710083, 'training/sps': 40643.0767469204, 'training/walltime': 1083.5938096046448, 'training/entropy_loss': Array(-0.03334048, dtype=float32), 'training/policy_loss': Array(-0.00030362, dtype=float32), 'training/total_loss': Array(92.051315, dtype=float32), 'training/v_loss': Array(92.08495, dtype=float32), 'eval/episode_goal_distance': (Array(0.3726927, dtype=float32), Array(0.1387526, dtype=float32)), 'eval/episode_reward': (Array(-8597.009, dtype=float32), Array(3892.5015, dtype=float32)), 'eval/avg_episode_length': (Array(930.0781, dtype=float32), Array(254.25266, dtype=float32)), 'eval/epoch_eval_time': 4.096780061721802, 'eval/sps': 31244.0497345625}
I0726 22:10:39.483370 140267183036224 train.py:379] starting iteration 352 2531.9043962955475
I0726 22:10:46.607399 140267183036224 train.py:394] {'eval/walltime': 1442.1001257896423, 'training/sps': 40566.99421864769, 'training/walltime': 1086.6228730678558, 'training/entropy_loss': Array(-0.03110497, dtype=float32), 'training/policy_loss': Array(4.0920604e-05, dtype=float32), 'training/total_loss': Array(83.12366, dtype=float32), 'training/v_loss': Array(83.154724, dtype=float32), 'eval/episode_goal_distance': (Array(0.35579693, dtype=float32), Array(0.12504767, dtype=float32)), 'eval/episode_reward': (Array(-8155.5703, dtype=float32), Array(3663.5422, dtype=float32)), 'eval/avg_episode_length': (Array(922.33594, dtype=float32), Array(266.78528, dtype=float32)), 'eval/epoch_eval_time': 4.091378688812256, 'eval/sps': 31285.297630847985}
I0726 22:10:46.609571 140267183036224 train.py:379] starting iteration 353 2539.030597448349
I0726 22:10:53.739530 140267183036224 train.py:394] {'eval/walltime': 1446.1954941749573, 'training/sps': 40540.46782199752, 'training/walltime': 1089.653918504715, 'training/entropy_loss': Array(-0.03139335, dtype=float32), 'training/policy_loss': Array(-2.2162581e-05, dtype=float32), 'training/total_loss': Array(87.18246, dtype=float32), 'training/v_loss': Array(87.21387, dtype=float32), 'eval/episode_goal_distance': (Array(0.36187953, dtype=float32), Array(0.1191473, dtype=float32)), 'eval/episode_reward': (Array(-8717.252, dtype=float32), Array(3333.3625, dtype=float32)), 'eval/avg_episode_length': (Array(937.8828, dtype=float32), Array(240.57921, dtype=float32)), 'eval/epoch_eval_time': 4.095368385314941, 'eval/sps': 31254.81958081692}
I0726 22:10:53.741757 140267183036224 train.py:379] starting iteration 354 2546.1627838611603
I0726 22:11:00.862656 140267183036224 train.py:394] {'eval/walltime': 1450.2809398174286, 'training/sps': 40529.25886566595, 'training/walltime': 1092.6858022212982, 'training/entropy_loss': Array(-0.03192965, dtype=float32), 'training/policy_loss': Array(0.00031592, dtype=float32), 'training/total_loss': Array(98.65318, dtype=float32), 'training/v_loss': Array(98.68479, dtype=float32), 'eval/episode_goal_distance': (Array(0.36433285, dtype=float32), Array(0.13541761, dtype=float32)), 'eval/episode_reward': (Array(-8672.943, dtype=float32), Array(3540.5002, dtype=float32)), 'eval/avg_episode_length': (Array(937.84375, dtype=float32), Array(240.73064, dtype=float32)), 'eval/epoch_eval_time': 4.0854456424713135, 'eval/sps': 31330.73138199238}
I0726 22:11:00.865049 140267183036224 train.py:379] starting iteration 355 2553.2860746383667
I0726 22:11:07.972926 140267183036224 train.py:394] {'eval/walltime': 1454.3702480793, 'training/sps': 40756.1723668675, 'training/walltime': 1095.7008056640625, 'training/entropy_loss': Array(-0.03131359, dtype=float32), 'training/policy_loss': Array(0.00058683, dtype=float32), 'training/total_loss': Array(97.83466, dtype=float32), 'training/v_loss': Array(97.86539, dtype=float32), 'eval/episode_goal_distance': (Array(0.366597, dtype=float32), Array(0.13036257, dtype=float32)), 'eval/episode_reward': (Array(-8538.318, dtype=float32), Array(3643.2178, dtype=float32)), 'eval/avg_episode_length': (Array(922.3203, dtype=float32), Array(266.83875, dtype=float32)), 'eval/epoch_eval_time': 4.089308261871338, 'eval/sps': 31301.13745482836}
I0726 22:11:07.975281 140267183036224 train.py:379] starting iteration 356 2560.396306991577
I0726 22:11:15.105249 140267183036224 train.py:394] {'eval/walltime': 1458.4705595970154, 'training/sps': 40607.06811978152, 'training/walltime': 1098.7268798351288, 'training/entropy_loss': Array(-0.03164661, dtype=float32), 'training/policy_loss': Array(0.00022145, dtype=float32), 'training/total_loss': Array(108.71934, dtype=float32), 'training/v_loss': Array(108.75076, dtype=float32), 'eval/episode_goal_distance': (Array(0.36104774, dtype=float32), Array(0.14245953, dtype=float32)), 'eval/episode_reward': (Array(-8279.574, dtype=float32), Array(3974.7268, dtype=float32)), 'eval/avg_episode_length': (Array(906.8047, dtype=float32), Array(289.75635, dtype=float32)), 'eval/epoch_eval_time': 4.100311517715454, 'eval/sps': 31217.14031896703}
I0726 22:11:15.107611 140267183036224 train.py:379] starting iteration 357 2567.5286371707916
I0726 22:11:22.234010 140267183036224 train.py:394] {'eval/walltime': 1462.5682401657104, 'training/sps': 40618.59546069159, 'training/walltime': 1101.7520952224731, 'training/entropy_loss': Array(-0.03276492, dtype=float32), 'training/policy_loss': Array(0.000583, dtype=float32), 'training/total_loss': Array(101.75742, dtype=float32), 'training/v_loss': Array(101.7896, dtype=float32), 'eval/episode_goal_distance': (Array(0.36243144, dtype=float32), Array(0.1275924, dtype=float32)), 'eval/episode_reward': (Array(-8055.545, dtype=float32), Array(3603.4346, dtype=float32)), 'eval/avg_episode_length': (Array(898.9922, dtype=float32), Array(300.42273, dtype=float32)), 'eval/epoch_eval_time': 4.097680568695068, 'eval/sps': 31237.183536920347}
I0726 22:11:22.391297 140267183036224 train.py:379] starting iteration 358 2574.8123123645782
I0726 22:11:29.513462 140267183036224 train.py:394] {'eval/walltime': 1466.6537623405457, 'training/sps': 40515.78189922829, 'training/walltime': 1104.784987449646, 'training/entropy_loss': Array(-0.03311366, dtype=float32), 'training/policy_loss': Array(-1.2114382e-05, dtype=float32), 'training/total_loss': Array(88.31446, dtype=float32), 'training/v_loss': Array(88.347595, dtype=float32), 'eval/episode_goal_distance': (Array(0.36090663, dtype=float32), Array(0.12439869, dtype=float32)), 'eval/episode_reward': (Array(-8337.605, dtype=float32), Array(3780.737, dtype=float32)), 'eval/avg_episode_length': (Array(906.8047, dtype=float32), Array(289.7566, dtype=float32)), 'eval/epoch_eval_time': 4.085522174835205, 'eval/sps': 31330.14447661468}
I0726 22:11:29.515802 140267183036224 train.py:379] starting iteration 359 2581.9368286132812
I0726 22:11:36.631558 140267183036224 train.py:394] {'eval/walltime': 1470.749080657959, 'training/sps': 40730.58900958006, 'training/walltime': 1107.801884651184, 'training/entropy_loss': Array(-0.03217565, dtype=float32), 'training/policy_loss': Array(7.486621e-05, dtype=float32), 'training/total_loss': Array(106.82312, dtype=float32), 'training/v_loss': Array(106.85521, dtype=float32), 'eval/episode_goal_distance': (Array(0.3648238, dtype=float32), Array(0.12565635, dtype=float32)), 'eval/episode_reward': (Array(-8741.467, dtype=float32), Array(3462.9463, dtype=float32)), 'eval/avg_episode_length': (Array(945.6953, dtype=float32), Array(225.77797, dtype=float32)), 'eval/epoch_eval_time': 4.09531831741333, 'eval/sps': 31255.201691097576}
I0726 22:11:36.633835 140267183036224 train.py:379] starting iteration 360 2589.054860830307
I0726 22:11:43.755043 140267183036224 train.py:394] {'eval/walltime': 1474.8360180854797, 'training/sps': 40547.261246876325, 'training/walltime': 1110.8324222564697, 'training/entropy_loss': Array(-0.03374224, dtype=float32), 'training/policy_loss': Array(0.00019328, dtype=float32), 'training/total_loss': Array(107.5643, dtype=float32), 'training/v_loss': Array(107.59784, dtype=float32), 'eval/episode_goal_distance': (Array(0.35135928, dtype=float32), Array(0.14274818, dtype=float32)), 'eval/episode_reward': (Array(-8402.029, dtype=float32), Array(3563.5608, dtype=float32)), 'eval/avg_episode_length': (Array(945.60156, dtype=float32), Array(226.16785, dtype=float32)), 'eval/epoch_eval_time': 4.086937427520752, 'eval/sps': 31319.295259592047}
I0726 22:11:43.759757 140267183036224 train.py:379] starting iteration 361 2596.180768966675
I0726 22:11:50.891597 140267183036224 train.py:394] {'eval/walltime': 1478.9310376644135, 'training/sps': 40515.39970467712, 'training/walltime': 1113.865343093872, 'training/entropy_loss': Array(-0.03388717, dtype=float32), 'training/policy_loss': Array(0.00012867, dtype=float32), 'training/total_loss': Array(86.478455, dtype=float32), 'training/v_loss': Array(86.51221, dtype=float32), 'eval/episode_goal_distance': (Array(0.35599062, dtype=float32), Array(0.14143687, dtype=float32)), 'eval/episode_reward': (Array(-8321.738, dtype=float32), Array(3782.4065, dtype=float32)), 'eval/avg_episode_length': (Array(922.27344, dtype=float32), Array(266.9997, dtype=float32)), 'eval/epoch_eval_time': 4.095019578933716, 'eval/sps': 31257.481809972043}
I0726 22:11:50.893722 140267183036224 train.py:379] starting iteration 362 2603.3147480487823
I0726 22:11:58.002483 140267183036224 train.py:394] {'eval/walltime': 1483.008401632309, 'training/sps': 40584.9214860053, 'training/walltime': 1116.8930685520172, 'training/entropy_loss': Array(-0.03323118, dtype=float32), 'training/policy_loss': Array(-9.0664274e-05, dtype=float32), 'training/total_loss': Array(85.99305, dtype=float32), 'training/v_loss': Array(86.02637, dtype=float32), 'eval/episode_goal_distance': (Array(0.35108188, dtype=float32), Array(0.13137552, dtype=float32)), 'eval/episode_reward': (Array(-7948.515, dtype=float32), Array(4048.1821, dtype=float32)), 'eval/avg_episode_length': (Array(875.6719, dtype=float32), Array(328.94168, dtype=float32)), 'eval/epoch_eval_time': 4.077363967895508, 'eval/sps': 31392.83149796067}
I0726 22:11:58.004907 140267183036224 train.py:379] starting iteration 363 2610.4259338378906
I0726 22:12:05.119153 140267183036224 train.py:394] {'eval/walltime': 1487.0920314788818, 'training/sps': 40593.81432827634, 'training/walltime': 1119.9201307296753, 'training/entropy_loss': Array(-0.0322272, dtype=float32), 'training/policy_loss': Array(5.0876613e-05, dtype=float32), 'training/total_loss': Array(82.19913, dtype=float32), 'training/v_loss': Array(82.23131, dtype=float32), 'eval/episode_goal_distance': (Array(0.34986398, dtype=float32), Array(0.12790145, dtype=float32)), 'eval/episode_reward': (Array(-8085.8447, dtype=float32), Array(3791.701, dtype=float32)), 'eval/avg_episode_length': (Array(899.0078, dtype=float32), Array(300.37616, dtype=float32)), 'eval/epoch_eval_time': 4.083629846572876, 'eval/sps': 31344.662667558383}
I0726 22:12:05.121397 140267183036224 train.py:379] starting iteration 364 2617.542423248291
I0726 22:12:12.246435 140267183036224 train.py:394] {'eval/walltime': 1491.1905648708344, 'training/sps': 40650.39835327563, 'training/walltime': 1122.942979335785, 'training/entropy_loss': Array(-0.03285614, dtype=float32), 'training/policy_loss': Array(0.00031685, dtype=float32), 'training/total_loss': Array(76.31494, dtype=float32), 'training/v_loss': Array(76.34748, dtype=float32), 'eval/episode_goal_distance': (Array(0.3561894, dtype=float32), Array(0.14494842, dtype=float32)), 'eval/episode_reward': (Array(-8466.897, dtype=float32), Array(3823.096, dtype=float32)), 'eval/avg_episode_length': (Array(937.8125, dtype=float32), Array(240.85126, dtype=float32)), 'eval/epoch_eval_time': 4.098533391952515, 'eval/sps': 31230.683700498445}
I0726 22:12:12.248834 140267183036224 train.py:379] starting iteration 365 2624.669859647751
I0726 22:12:19.380855 140267183036224 train.py:394] {'eval/walltime': 1495.2876346111298, 'training/sps': 40535.42366210848, 'training/walltime': 1125.9744019508362, 'training/entropy_loss': Array(-0.03288022, dtype=float32), 'training/policy_loss': Array(0.00014686, dtype=float32), 'training/total_loss': Array(76.362656, dtype=float32), 'training/v_loss': Array(76.395386, dtype=float32), 'eval/episode_goal_distance': (Array(0.35971922, dtype=float32), Array(0.13667077, dtype=float32)), 'eval/episode_reward': (Array(-8683.598, dtype=float32), Array(3785.086, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.08257, dtype=float32)), 'eval/epoch_eval_time': 4.09706974029541, 'eval/sps': 31241.840660191163}
I0726 22:12:19.383146 140267183036224 train.py:379] starting iteration 366 2631.804172039032
I0726 22:12:26.514854 140267183036224 train.py:394] {'eval/walltime': 1499.3828506469727, 'training/sps': 40515.1226181364, 'training/walltime': 1129.007343530655, 'training/entropy_loss': Array(-0.03292019, dtype=float32), 'training/policy_loss': Array(0.00038037, dtype=float32), 'training/total_loss': Array(191.47354, dtype=float32), 'training/v_loss': Array(191.50607, dtype=float32), 'eval/episode_goal_distance': (Array(0.35125995, dtype=float32), Array(0.12653407, dtype=float32)), 'eval/episode_reward': (Array(-8209.681, dtype=float32), Array(3794.518, dtype=float32)), 'eval/avg_episode_length': (Array(914.58594, dtype=float32), Array(278.56522, dtype=float32)), 'eval/epoch_eval_time': 4.0952160358428955, 'eval/sps': 31255.982316853395}
I0726 22:12:26.517219 140267183036224 train.py:379] starting iteration 367 2638.9382457733154
I0726 22:12:33.643365 140267183036224 train.py:394] {'eval/walltime': 1503.477979183197, 'training/sps': 40589.11489303458, 'training/walltime': 1132.0347561836243, 'training/entropy_loss': Array(-0.03402103, dtype=float32), 'training/policy_loss': Array(0.0001149, dtype=float32), 'training/total_loss': Array(116.436325, dtype=float32), 'training/v_loss': Array(116.47023, dtype=float32), 'eval/episode_goal_distance': (Array(0.37779444, dtype=float32), Array(0.12847069, dtype=float32)), 'eval/episode_reward': (Array(-8678.109, dtype=float32), Array(3863.7332, dtype=float32)), 'eval/avg_episode_length': (Array(914.59375, dtype=float32), Array(278.5394, dtype=float32)), 'eval/epoch_eval_time': 4.095128536224365, 'eval/sps': 31256.650155849245}
I0726 22:12:33.645656 140267183036224 train.py:379] starting iteration 368 2646.0666823387146
I0726 22:12:40.762707 140267183036224 train.py:394] {'eval/walltime': 1507.568342924118, 'training/sps': 40647.42964329046, 'training/walltime': 1135.0578255653381, 'training/entropy_loss': Array(-0.03543219, dtype=float32), 'training/policy_loss': Array(0.00017875, dtype=float32), 'training/total_loss': Array(89.354294, dtype=float32), 'training/v_loss': Array(89.38955, dtype=float32), 'eval/episode_goal_distance': (Array(0.3545878, dtype=float32), Array(0.12977797, dtype=float32)), 'eval/episode_reward': (Array(-8526.108, dtype=float32), Array(3625.3628, dtype=float32)), 'eval/avg_episode_length': (Array(922.33594, dtype=float32), Array(266.7851, dtype=float32)), 'eval/epoch_eval_time': 4.0903637409210205, 'eval/sps': 31293.060497152375}
I0726 22:12:40.765054 140267183036224 train.py:379] starting iteration 369 2653.1860797405243
I0726 22:12:47.883511 140267183036224 train.py:394] {'eval/walltime': 1511.6632826328278, 'training/sps': 40688.55245780386, 'training/walltime': 1138.0778396129608, 'training/entropy_loss': Array(-0.03614921, dtype=float32), 'training/policy_loss': Array(0.00020465, dtype=float32), 'training/total_loss': Array(85.58025, dtype=float32), 'training/v_loss': Array(85.6162, dtype=float32), 'eval/episode_goal_distance': (Array(0.4030271, dtype=float32), Array(0.16468129, dtype=float32)), 'eval/episode_reward': (Array(-9084.851, dtype=float32), Array(4840.226, dtype=float32)), 'eval/avg_episode_length': (Array(891.28906, dtype=float32), Array(310.21448, dtype=float32)), 'eval/epoch_eval_time': 4.094939708709717, 'eval/sps': 31258.091475132314}
I0726 22:12:47.885646 140267183036224 train.py:379] starting iteration 370 2660.306671857834
I0726 22:12:55.017278 140267183036224 train.py:394] {'eval/walltime': 1515.7636270523071, 'training/sps': 40585.148393630116, 'training/walltime': 1141.1055481433868, 'training/entropy_loss': Array(-0.03660513, dtype=float32), 'training/policy_loss': Array(0.0001515, dtype=float32), 'training/total_loss': Array(92.63394, dtype=float32), 'training/v_loss': Array(92.6704, dtype=float32), 'eval/episode_goal_distance': (Array(0.37088767, dtype=float32), Array(0.13481677, dtype=float32)), 'eval/episode_reward': (Array(-8003.968, dtype=float32), Array(4211.799, dtype=float32)), 'eval/avg_episode_length': (Array(868.03125, dtype=float32), Array(337.21667, dtype=float32)), 'eval/epoch_eval_time': 4.10034441947937, 'eval/sps': 31216.889828062893}
I0726 22:12:55.019659 140267183036224 train.py:379] starting iteration 371 2667.440684556961
I0726 22:13:02.152775 140267183036224 train.py:394] {'eval/walltime': 1519.863532781601, 'training/sps': 40560.04735032036, 'training/walltime': 1144.135130405426, 'training/entropy_loss': Array(-0.0368657, dtype=float32), 'training/policy_loss': Array(-0.00017032, dtype=float32), 'training/total_loss': Array(112.884476, dtype=float32), 'training/v_loss': Array(112.92151, dtype=float32), 'eval/episode_goal_distance': (Array(0.36872798, dtype=float32), Array(0.16518223, dtype=float32)), 'eval/episode_reward': (Array(-8654.855, dtype=float32), Array(3894.4087, dtype=float32)), 'eval/avg_episode_length': (Array(930.1328, dtype=float32), Array(254.05392, dtype=float32)), 'eval/epoch_eval_time': 4.099905729293823, 'eval/sps': 31220.230037349418}
I0726 22:13:02.155098 140267183036224 train.py:379] starting iteration 372 2674.5761239528656
I0726 22:13:09.285658 140267183036224 train.py:394] {'eval/walltime': 1523.9605553150177, 'training/sps': 40554.13672419066, 'training/walltime': 1147.1651542186737, 'training/entropy_loss': Array(-0.03765189, dtype=float32), 'training/policy_loss': Array(-0.00011597, dtype=float32), 'training/total_loss': Array(112.08891, dtype=float32), 'training/v_loss': Array(112.126686, dtype=float32), 'eval/episode_goal_distance': (Array(0.39371157, dtype=float32), Array(0.16813688, dtype=float32)), 'eval/episode_reward': (Array(-8809.484, dtype=float32), Array(4224.865, dtype=float32)), 'eval/avg_episode_length': (Array(914.6719, dtype=float32), Array(278.2851, dtype=float32)), 'eval/epoch_eval_time': 4.097022533416748, 'eval/sps': 31242.200636190613}
I0726 22:13:09.287839 140267183036224 train.py:379] starting iteration 373 2681.708865404129
I0726 22:13:16.414942 140267183036224 train.py:394] {'eval/walltime': 1528.0588991641998, 'training/sps': 40620.231324284796, 'training/walltime': 1150.1902477741241, 'training/entropy_loss': Array(-0.03849522, dtype=float32), 'training/policy_loss': Array(-0.00011975, dtype=float32), 'training/total_loss': Array(124.089096, dtype=float32), 'training/v_loss': Array(124.12771, dtype=float32), 'eval/episode_goal_distance': (Array(0.38623196, dtype=float32), Array(0.12927863, dtype=float32)), 'eval/episode_reward': (Array(-8462.124, dtype=float32), Array(3862.334, dtype=float32)), 'eval/avg_episode_length': (Array(899.03906, dtype=float32), Array(300.2832, dtype=float32)), 'eval/epoch_eval_time': 4.098343849182129, 'eval/sps': 31232.128076697092}
I0726 22:13:16.419652 140267183036224 train.py:379] starting iteration 374 2688.8406631946564
I0726 22:13:23.562321 140267183036224 train.py:394] {'eval/walltime': 1532.1768083572388, 'training/sps': 40677.33852634999, 'training/walltime': 1153.211094379425, 'training/entropy_loss': Array(-0.03882847, dtype=float32), 'training/policy_loss': Array(9.957432e-05, dtype=float32), 'training/total_loss': Array(110.68149, dtype=float32), 'training/v_loss': Array(110.72021, dtype=float32), 'eval/episode_goal_distance': (Array(0.40487218, dtype=float32), Array(0.14646907, dtype=float32)), 'eval/episode_reward': (Array(-9426.887, dtype=float32), Array(3558.4155, dtype=float32)), 'eval/avg_episode_length': (Array(945.71094, dtype=float32), Array(225.7129, dtype=float32)), 'eval/epoch_eval_time': 4.11790919303894, 'eval/sps': 31083.735458852694}
I0726 22:13:23.564517 140267183036224 train.py:379] starting iteration 375 2695.9855427742004
I0726 22:13:30.695317 140267183036224 train.py:394] {'eval/walltime': 1536.2824082374573, 'training/sps': 40667.18324429856, 'training/walltime': 1156.2326953411102, 'training/entropy_loss': Array(-0.03803961, dtype=float32), 'training/policy_loss': Array(-4.8449547e-05, dtype=float32), 'training/total_loss': Array(97.07948, dtype=float32), 'training/v_loss': Array(97.117584, dtype=float32), 'eval/episode_goal_distance': (Array(0.39937097, dtype=float32), Array(0.1616606, dtype=float32)), 'eval/episode_reward': (Array(-9064.438, dtype=float32), Array(3657.1519, dtype=float32)), 'eval/avg_episode_length': (Array(945.6719, dtype=float32), Array(225.87543, dtype=float32)), 'eval/epoch_eval_time': 4.105599880218506, 'eval/sps': 31176.929982077956}
I0726 22:13:30.697637 140267183036224 train.py:379] starting iteration 376 2703.118663072586
I0726 22:13:37.833711 140267183036224 train.py:394] {'eval/walltime': 1540.3837943077087, 'training/sps': 40539.58133635964, 'training/walltime': 1159.2638070583344, 'training/entropy_loss': Array(-0.03932945, dtype=float32), 'training/policy_loss': Array(-1.5486952e-05, dtype=float32), 'training/total_loss': Array(141.83313, dtype=float32), 'training/v_loss': Array(141.87248, dtype=float32), 'eval/episode_goal_distance': (Array(0.412481, dtype=float32), Array(0.15692405, dtype=float32)), 'eval/episode_reward': (Array(-9114.947, dtype=float32), Array(3636.1685, dtype=float32)), 'eval/avg_episode_length': (Array(937.89844, dtype=float32), Array(240.51888, dtype=float32)), 'eval/epoch_eval_time': 4.101386070251465, 'eval/sps': 31208.96150899348}
I0726 22:13:37.836044 140267183036224 train.py:379] starting iteration 377 2710.2570700645447
I0726 22:13:44.963600 140267183036224 train.py:394] {'eval/walltime': 1544.4792969226837, 'training/sps': 40572.67862879676, 'training/walltime': 1162.2924461364746, 'training/entropy_loss': Array(-0.04046027, dtype=float32), 'training/policy_loss': Array(0.00013308, dtype=float32), 'training/total_loss': Array(118.216354, dtype=float32), 'training/v_loss': Array(118.256676, dtype=float32), 'eval/episode_goal_distance': (Array(0.4194, dtype=float32), Array(0.15893193, dtype=float32)), 'eval/episode_reward': (Array(-9088.193, dtype=float32), Array(4300.645, dtype=float32)), 'eval/avg_episode_length': (Array(883.4844, dtype=float32), Array(319.7998, dtype=float32)), 'eval/epoch_eval_time': 4.095502614974976, 'eval/sps': 31253.795207449064}
I0726 22:13:44.965771 140267183036224 train.py:379] starting iteration 378 2717.386797428131
I0726 22:13:52.099411 140267183036224 train.py:394] {'eval/walltime': 1548.573968887329, 'training/sps': 40482.294007844386, 'training/walltime': 1165.3278472423553, 'training/entropy_loss': Array(-0.04047879, dtype=float32), 'training/policy_loss': Array(-0.00012798, dtype=float32), 'training/total_loss': Array(105.91814, dtype=float32), 'training/v_loss': Array(105.95874, dtype=float32), 'eval/episode_goal_distance': (Array(0.39000714, dtype=float32), Array(0.17746606, dtype=float32)), 'eval/episode_reward': (Array(-9081.604, dtype=float32), Array(4256.1484, dtype=float32)), 'eval/avg_episode_length': (Array(922.3203, dtype=float32), Array(266.83887, dtype=float32)), 'eval/epoch_eval_time': 4.094671964645386, 'eval/sps': 31260.13539184336}
I0726 22:13:52.101720 140267183036224 train.py:379] starting iteration 379 2724.522746324539
I0726 22:13:59.233301 140267183036224 train.py:394] {'eval/walltime': 1552.6711156368256, 'training/sps': 40543.210433135995, 'training/walltime': 1168.3586876392365, 'training/entropy_loss': Array(-0.04167924, dtype=float32), 'training/policy_loss': Array(0.00016756, dtype=float32), 'training/total_loss': Array(114.027054, dtype=float32), 'training/v_loss': Array(114.068565, dtype=float32), 'eval/episode_goal_distance': (Array(0.39501587, dtype=float32), Array(0.15240943, dtype=float32)), 'eval/episode_reward': (Array(-8731.145, dtype=float32), Array(4068.222, dtype=float32)), 'eval/avg_episode_length': (Array(914.60156, dtype=float32), Array(278.51416, dtype=float32)), 'eval/epoch_eval_time': 4.09714674949646, 'eval/sps': 31241.253444419883}
I0726 22:13:59.238010 140267183036224 train.py:379] starting iteration 380 2731.6590213775635
I0726 22:14:06.363160 140267183036224 train.py:394] {'eval/walltime': 1556.769728899002, 'training/sps': 40654.97730040376, 'training/walltime': 1171.381195783615, 'training/entropy_loss': Array(-0.04244387, dtype=float32), 'training/policy_loss': Array(-4.0169794e-05, dtype=float32), 'training/total_loss': Array(111.725136, dtype=float32), 'training/v_loss': Array(111.767624, dtype=float32), 'eval/episode_goal_distance': (Array(0.39980575, dtype=float32), Array(0.16228136, dtype=float32)), 'eval/episode_reward': (Array(-9181.922, dtype=float32), Array(3325.9045, dtype=float32)), 'eval/avg_episode_length': (Array(968.9297, dtype=float32), Array(172.99237, dtype=float32)), 'eval/epoch_eval_time': 4.098613262176514, 'eval/sps': 31230.075103994397}
I0726 22:14:06.366854 140267183036224 train.py:379] starting iteration 381 2738.787873029709
I0726 22:14:13.495067 140267183036224 train.py:394] {'eval/walltime': 1560.8729214668274, 'training/sps': 40670.0489338432, 'training/walltime': 1174.4025838375092, 'training/entropy_loss': Array(-0.04238827, dtype=float32), 'training/policy_loss': Array(-0.00039622, dtype=float32), 'training/total_loss': Array(109.27102, dtype=float32), 'training/v_loss': Array(109.3138, dtype=float32), 'eval/episode_goal_distance': (Array(0.4186592, dtype=float32), Array(0.18383802, dtype=float32)), 'eval/episode_reward': (Array(-9352.604, dtype=float32), Array(4240.2856, dtype=float32)), 'eval/avg_episode_length': (Array(922.46094, dtype=float32), Array(266.35596, dtype=float32)), 'eval/epoch_eval_time': 4.103192567825317, 'eval/sps': 31195.221253736992}
I0726 22:14:13.497212 140267183036224 train.py:379] starting iteration 382 2745.9182374477386
I0726 22:14:20.630452 140267183036224 train.py:394] {'eval/walltime': 1564.971533536911, 'training/sps': 40540.53478831489, 'training/walltime': 1177.4336242675781, 'training/entropy_loss': Array(-0.04391038, dtype=float32), 'training/policy_loss': Array(-0.00014579, dtype=float32), 'training/total_loss': Array(115.54661, dtype=float32), 'training/v_loss': Array(115.59067, dtype=float32), 'eval/episode_goal_distance': (Array(0.4311843, dtype=float32), Array(0.15639594, dtype=float32)), 'eval/episode_reward': (Array(-9546.564, dtype=float32), Array(3865.5737, dtype=float32)), 'eval/avg_episode_length': (Array(945.58594, dtype=float32), Array(226.23268, dtype=float32)), 'eval/epoch_eval_time': 4.098612070083618, 'eval/sps': 31230.084187349938}
I0726 22:14:20.632589 140267183036224 train.py:379] starting iteration 383 2753.053615808487
I0726 22:14:27.758617 140267183036224 train.py:394] {'eval/walltime': 1569.0696923732758, 'training/sps': 40631.60597921418, 'training/walltime': 1180.4578709602356, 'training/entropy_loss': Array(-0.04463353, dtype=float32), 'training/policy_loss': Array(6.172779e-05, dtype=float32), 'training/total_loss': Array(255.08261, dtype=float32), 'training/v_loss': Array(255.12717, dtype=float32), 'eval/episode_goal_distance': (Array(0.40092188, dtype=float32), Array(0.163885, dtype=float32)), 'eval/episode_reward': (Array(-8867.631, dtype=float32), Array(4351.3438, dtype=float32)), 'eval/avg_episode_length': (Array(891.3047, dtype=float32), Array(310.1701, dtype=float32)), 'eval/epoch_eval_time': 4.098158836364746, 'eval/sps': 31233.538062068343}
I0726 22:14:27.763447 140267183036224 train.py:379] starting iteration 384 2760.1844577789307
I0726 22:14:34.890088 140267183036224 train.py:394] {'eval/walltime': 1573.1630823612213, 'training/sps': 40565.43288148523, 'training/walltime': 1183.4870510101318, 'training/entropy_loss': Array(-0.04405514, dtype=float32), 'training/policy_loss': Array(-0.00080125, dtype=float32), 'training/total_loss': Array(146.00446, dtype=float32), 'training/v_loss': Array(146.04929, dtype=float32), 'eval/episode_goal_distance': (Array(0.4389335, dtype=float32), Array(0.18165407, dtype=float32)), 'eval/episode_reward': (Array(-9773.0205, dtype=float32), Array(4081.2432, dtype=float32)), 'eval/avg_episode_length': (Array(945.6406, dtype=float32), Array(226.00528, dtype=float32)), 'eval/epoch_eval_time': 4.093389987945557, 'eval/sps': 31269.92550842738}
I0726 22:14:34.892451 140267183036224 train.py:379] starting iteration 385 2767.3134768009186
I0726 22:14:42.007302 140267183036224 train.py:394] {'eval/walltime': 1577.2497067451477, 'training/sps': 40626.3886026426, 'training/walltime': 1186.5116860866547, 'training/entropy_loss': Array(-0.0441898, dtype=float32), 'training/policy_loss': Array(-0.00138302, dtype=float32), 'training/total_loss': Array(131.41068, dtype=float32), 'training/v_loss': Array(131.45624, dtype=float32), 'eval/episode_goal_distance': (Array(0.41481665, dtype=float32), Array(0.18948564, dtype=float32)), 'eval/episode_reward': (Array(-8817.012, dtype=float32), Array(4351.3867, dtype=float32)), 'eval/avg_episode_length': (Array(906.7344, dtype=float32), Array(289.97495, dtype=float32)), 'eval/epoch_eval_time': 4.086624383926392, 'eval/sps': 31321.694380196197}
I0726 22:14:42.009613 140267183036224 train.py:379] starting iteration 386 2774.4306387901306
I0726 22:14:49.135666 140267183036224 train.py:394] {'eval/walltime': 1581.3525834083557, 'training/sps': 40694.6790567669, 'training/walltime': 1189.531245470047, 'training/entropy_loss': Array(-0.04333701, dtype=float32), 'training/policy_loss': Array(-0.00063799, dtype=float32), 'training/total_loss': Array(122.64431, dtype=float32), 'training/v_loss': Array(122.688286, dtype=float32), 'eval/episode_goal_distance': (Array(0.42923582, dtype=float32), Array(0.1692733, dtype=float32)), 'eval/episode_reward': (Array(-9663.209, dtype=float32), Array(3861.1711, dtype=float32)), 'eval/avg_episode_length': (Array(937.8281, dtype=float32), Array(240.79068, dtype=float32)), 'eval/epoch_eval_time': 4.102876663208008, 'eval/sps': 31197.62315738679}
I0726 22:14:49.137990 140267183036224 train.py:379] starting iteration 387 2781.5590159893036
I0726 22:14:56.264113 140267183036224 train.py:394] {'eval/walltime': 1585.4517147541046, 'training/sps': 40642.41972646986, 'training/walltime': 1192.5546875, 'training/entropy_loss': Array(-0.04372332, dtype=float32), 'training/policy_loss': Array(-0.00058927, dtype=float32), 'training/total_loss': Array(149.39668, dtype=float32), 'training/v_loss': Array(149.441, dtype=float32), 'eval/episode_goal_distance': (Array(0.39170393, dtype=float32), Array(0.17218985, dtype=float32)), 'eval/episode_reward': (Array(-8893.599, dtype=float32), Array(4168.6694, dtype=float32)), 'eval/avg_episode_length': (Array(906.7656, dtype=float32), Array(289.87784, dtype=float32)), 'eval/epoch_eval_time': 4.099131345748901, 'eval/sps': 31226.127977759326}
I0726 22:14:56.266423 140267183036224 train.py:379] starting iteration 388 2788.687448978424
I0726 22:15:03.405319 140267183036224 train.py:394] {'eval/walltime': 1589.556211233139, 'training/sps': 40543.43368525784, 'training/walltime': 1195.5855112075806, 'training/entropy_loss': Array(-0.0444207, dtype=float32), 'training/policy_loss': Array(-0.00047877, dtype=float32), 'training/total_loss': Array(133.9729, dtype=float32), 'training/v_loss': Array(134.01779, dtype=float32), 'eval/episode_goal_distance': (Array(0.4572184, dtype=float32), Array(0.19429475, dtype=float32)), 'eval/episode_reward': (Array(-9933.168, dtype=float32), Array(4141.2466, dtype=float32)), 'eval/avg_episode_length': (Array(945.5781, dtype=float32), Array(226.26508, dtype=float32)), 'eval/epoch_eval_time': 4.104496479034424, 'eval/sps': 31185.311195616323}
I0726 22:15:03.407683 140267183036224 train.py:379] starting iteration 389 2795.828709602356
I0726 22:15:10.533049 140267183036224 train.py:394] {'eval/walltime': 1593.6489033699036, 'training/sps': 40565.397760763546, 'training/walltime': 1198.6146938800812, 'training/entropy_loss': Array(-0.0456456, dtype=float32), 'training/policy_loss': Array(-0.00010999, dtype=float32), 'training/total_loss': Array(159.40561, dtype=float32), 'training/v_loss': Array(159.45134, dtype=float32), 'eval/episode_goal_distance': (Array(0.45197698, dtype=float32), Array(0.18654293, dtype=float32)), 'eval/episode_reward': (Array(-9833.361, dtype=float32), Array(4421.3223, dtype=float32)), 'eval/avg_episode_length': (Array(930.03125, dtype=float32), Array(254.42346, dtype=float32)), 'eval/epoch_eval_time': 4.092692136764526, 'eval/sps': 31275.2573911387}
I0726 22:15:10.535393 140267183036224 train.py:379] starting iteration 390 2802.956419467926
I0726 22:15:17.661931 140267183036224 train.py:394] {'eval/walltime': 1597.7482669353485, 'training/sps': 40641.47750007117, 'training/walltime': 1201.6382060050964, 'training/entropy_loss': Array(-0.04634133, dtype=float32), 'training/policy_loss': Array(-0.00024007, dtype=float32), 'training/total_loss': Array(164.99763, dtype=float32), 'training/v_loss': Array(165.04422, dtype=float32), 'eval/episode_goal_distance': (Array(0.45491898, dtype=float32), Array(0.19885339, dtype=float32)), 'eval/episode_reward': (Array(-9689.896, dtype=float32), Array(4195.2217, dtype=float32)), 'eval/avg_episode_length': (Array(914.4922, dtype=float32), Array(278.87057, dtype=float32)), 'eval/epoch_eval_time': 4.099363565444946, 'eval/sps': 31224.35908806904}
I0726 22:15:17.664522 140267183036224 train.py:379] starting iteration 391 2810.0855481624603
I0726 22:15:24.791548 140267183036224 train.py:394] {'eval/walltime': 1601.859990119934, 'training/sps': 40799.92168635261, 'training/walltime': 1204.649976491928, 'training/entropy_loss': Array(-0.0466332, dtype=float32), 'training/policy_loss': Array(-0.00028113, dtype=float32), 'training/total_loss': Array(151.01291, dtype=float32), 'training/v_loss': Array(151.05983, dtype=float32), 'eval/episode_goal_distance': (Array(0.46099925, dtype=float32), Array(0.21761833, dtype=float32)), 'eval/episode_reward': (Array(-10024.807, dtype=float32), Array(4565.0083, dtype=float32)), 'eval/avg_episode_length': (Array(930.10156, dtype=float32), Array(254.16772, dtype=float32)), 'eval/epoch_eval_time': 4.111723184585571, 'eval/sps': 31130.50034103922}
I0726 22:15:24.793952 140267183036224 train.py:379] starting iteration 392 2817.2149786949158
I0726 22:15:31.930021 140267183036224 train.py:394] {'eval/walltime': 1605.9745440483093, 'training/sps': 40716.09005007785, 'training/walltime': 1207.6679480075836, 'training/entropy_loss': Array(-0.04796425, dtype=float32), 'training/policy_loss': Array(-0.00017052, dtype=float32), 'training/total_loss': Array(157.64127, dtype=float32), 'training/v_loss': Array(157.68939, dtype=float32), 'eval/episode_goal_distance': (Array(0.44919235, dtype=float32), Array(0.20160511, dtype=float32)), 'eval/episode_reward': (Array(-9785.533, dtype=float32), Array(4326.4688, dtype=float32)), 'eval/avg_episode_length': (Array(930.15625, dtype=float32), Array(253.96884, dtype=float32)), 'eval/epoch_eval_time': 4.114553928375244, 'eval/sps': 31109.0830812235}
I0726 22:15:31.932349 140267183036224 train.py:379] starting iteration 393 2824.3533754348755
I0726 22:15:39.060410 140267183036224 train.py:394] {'eval/walltime': 1610.0750894546509, 'training/sps': 40636.51711016183, 'training/walltime': 1210.6918292045593, 'training/entropy_loss': Array(-0.04791894, dtype=float32), 'training/policy_loss': Array(-0.00075714, dtype=float32), 'training/total_loss': Array(188.73135, dtype=float32), 'training/v_loss': Array(188.78003, dtype=float32), 'eval/episode_goal_distance': (Array(0.43788925, dtype=float32), Array(0.19688113, dtype=float32)), 'eval/episode_reward': (Array(-8810.266, dtype=float32), Array(5491.047, dtype=float32)), 'eval/avg_episode_length': (Array(813.6875, dtype=float32), Array(387.8413, dtype=float32)), 'eval/epoch_eval_time': 4.100545406341553, 'eval/sps': 31215.359742644516}
I0726 22:15:39.062814 140267183036224 train.py:379] starting iteration 394 2831.4838399887085
I0726 22:15:46.191838 140267183036224 train.py:394] {'eval/walltime': 1614.1774580478668, 'training/sps': 40647.23088985936, 'training/walltime': 1213.714913368225, 'training/entropy_loss': Array(-0.04808707, dtype=float32), 'training/policy_loss': Array(-4.60031e-05, dtype=float32), 'training/total_loss': Array(158.60175, dtype=float32), 'training/v_loss': Array(158.64986, dtype=float32), 'eval/episode_goal_distance': (Array(0.4593702, dtype=float32), Array(0.20444527, dtype=float32)), 'eval/episode_reward': (Array(-9613.178, dtype=float32), Array(4816.883, dtype=float32)), 'eval/avg_episode_length': (Array(891.28906, dtype=float32), Array(310.21472, dtype=float32)), 'eval/epoch_eval_time': 4.102368593215942, 'eval/sps': 31201.486919452505}
I0726 22:15:46.194158 140267183036224 train.py:379] starting iteration 395 2838.6151835918427
I0726 22:15:53.323137 140267183036224 train.py:394] {'eval/walltime': 1618.2814674377441, 'training/sps': 40669.52261791884, 'training/walltime': 1216.736340522766, 'training/entropy_loss': Array(-0.0483037, dtype=float32), 'training/policy_loss': Array(-0.00052655, dtype=float32), 'training/total_loss': Array(155.74088, dtype=float32), 'training/v_loss': Array(155.7897, dtype=float32), 'eval/episode_goal_distance': (Array(0.47787046, dtype=float32), Array(0.21983257, dtype=float32)), 'eval/episode_reward': (Array(-10506.98, dtype=float32), Array(4629.3345, dtype=float32)), 'eval/avg_episode_length': (Array(937.96094, dtype=float32), Array(240.27658, dtype=float32)), 'eval/epoch_eval_time': 4.104009389877319, 'eval/sps': 31189.012460769805}
I0726 22:15:53.325467 140267183036224 train.py:379] starting iteration 396 2845.746492624283
I0726 22:16:00.438640 140267183036224 train.py:394] {'eval/walltime': 1622.3753950595856, 'training/sps': 40746.406123802975, 'training/walltime': 1219.7520666122437, 'training/entropy_loss': Array(-0.04882179, dtype=float32), 'training/policy_loss': Array(-0.00018185, dtype=float32), 'training/total_loss': Array(166.08044, dtype=float32), 'training/v_loss': Array(166.12946, dtype=float32), 'eval/episode_goal_distance': (Array(0.47708148, dtype=float32), Array(0.20841078, dtype=float32)), 'eval/episode_reward': (Array(-10078.915, dtype=float32), Array(4696.625, dtype=float32)), 'eval/avg_episode_length': (Array(906.78906, dtype=float32), Array(289.80563, dtype=float32)), 'eval/epoch_eval_time': 4.093927621841431, 'eval/sps': 31265.81899423668}
I0726 22:16:00.441005 140267183036224 train.py:379] starting iteration 397 2852.8620312213898
I0726 22:16:07.569553 140267183036224 train.py:394] {'eval/walltime': 1626.48419713974, 'training/sps': 40739.86461835966, 'training/walltime': 1222.7682769298553, 'training/entropy_loss': Array(-0.04972997, dtype=float32), 'training/policy_loss': Array(-0.00014147, dtype=float32), 'training/total_loss': Array(162.91638, dtype=float32), 'training/v_loss': Array(162.96625, dtype=float32), 'eval/episode_goal_distance': (Array(0.4963756, dtype=float32), Array(0.21774992, dtype=float32)), 'eval/episode_reward': (Array(-10595.828, dtype=float32), Array(4769.305, dtype=float32)), 'eval/avg_episode_length': (Array(922.4219, dtype=float32), Array(266.4901, dtype=float32)), 'eval/epoch_eval_time': 4.108802080154419, 'eval/sps': 31152.63220349359}
I0726 22:16:07.572035 140267183036224 train.py:379] starting iteration 398 2859.9930613040924
I0726 22:16:14.707363 140267183036224 train.py:394] {'eval/walltime': 1630.5939993858337, 'training/sps': 40660.311086093774, 'training/walltime': 1225.790388584137, 'training/entropy_loss': Array(-0.05016055, dtype=float32), 'training/policy_loss': Array(-0.0002603, dtype=float32), 'training/total_loss': Array(167.48079, dtype=float32), 'training/v_loss': Array(167.53122, dtype=float32), 'eval/episode_goal_distance': (Array(0.5148531, dtype=float32), Array(0.21004067, dtype=float32)), 'eval/episode_reward': (Array(-11169.99, dtype=float32), Array(4720.7227, dtype=float32)), 'eval/avg_episode_length': (Array(937.8047, dtype=float32), Array(240.8816, dtype=float32)), 'eval/epoch_eval_time': 4.10980224609375, 'eval/sps': 31145.050865077596}
I0726 22:16:14.709767 140267183036224 train.py:379] starting iteration 399 2867.1307928562164
I0726 22:16:21.830647 140267183036224 train.py:394] {'eval/walltime': 1634.6833682060242, 'training/sps': 40581.08042083389, 'training/walltime': 1228.8184006214142, 'training/entropy_loss': Array(-0.05025636, dtype=float32), 'training/policy_loss': Array(-0.00050125, dtype=float32), 'training/total_loss': Array(163.94678, dtype=float32), 'training/v_loss': Array(163.99754, dtype=float32), 'eval/episode_goal_distance': (Array(0.4826715, dtype=float32), Array(0.2348819, dtype=float32)), 'eval/episode_reward': (Array(-10307.826, dtype=float32), Array(4312.5327, dtype=float32)), 'eval/avg_episode_length': (Array(945.65625, dtype=float32), Array(225.94055, dtype=float32)), 'eval/epoch_eval_time': 4.08936882019043, 'eval/sps': 31300.673925038493}
I0726 22:16:21.833031 140267183036224 train.py:379] starting iteration 400 2874.254056453705
I0726 22:16:28.979124 140267183036224 train.py:394] {'eval/walltime': 1638.793731212616, 'training/sps': 40524.82927547281, 'training/walltime': 1231.8506157398224, 'training/entropy_loss': Array(-0.05014282, dtype=float32), 'training/policy_loss': Array(-0.00035092, dtype=float32), 'training/total_loss': Array(295.62427, dtype=float32), 'training/v_loss': Array(295.67474, dtype=float32), 'eval/episode_goal_distance': (Array(0.4839327, dtype=float32), Array(0.24532686, dtype=float32)), 'eval/episode_reward': (Array(-10288.911, dtype=float32), Array(5151.11, dtype=float32)), 'eval/avg_episode_length': (Array(914.5625, dtype=float32), Array(278.64142, dtype=float32)), 'eval/epoch_eval_time': 4.110363006591797, 'eval/sps': 31140.801869500614}
I0726 22:16:28.981568 140267183036224 train.py:379] starting iteration 401 2881.4025933742523
I0726 22:16:36.105357 140267183036224 train.py:394] {'eval/walltime': 1642.8865518569946, 'training/sps': 40589.456924051774, 'training/walltime': 1234.8780028820038, 'training/entropy_loss': Array(-0.04992384, dtype=float32), 'training/policy_loss': Array(-0.00070275, dtype=float32), 'training/total_loss': Array(182.52309, dtype=float32), 'training/v_loss': Array(182.5737, dtype=float32), 'eval/episode_goal_distance': (Array(0.47524568, dtype=float32), Array(0.1962368, dtype=float32)), 'eval/episode_reward': (Array(-10340.934, dtype=float32), Array(4285.611, dtype=float32)), 'eval/avg_episode_length': (Array(937.875, dtype=float32), Array(240.60938, dtype=float32)), 'eval/epoch_eval_time': 4.092820644378662, 'eval/sps': 31274.275401196304}
I0726 22:16:36.107735 140267183036224 train.py:379] starting iteration 402 2888.5287618637085
I0726 22:16:43.233880 140267183036224 train.py:394] {'eval/walltime': 1646.9970984458923, 'training/sps': 40795.17118271753, 'training/walltime': 1237.8901240825653, 'training/entropy_loss': Array(-0.0492015, dtype=float32), 'training/policy_loss': Array(-0.00088037, dtype=float32), 'training/total_loss': Array(162.83537, dtype=float32), 'training/v_loss': Array(162.88544, dtype=float32), 'eval/episode_goal_distance': (Array(0.490865, dtype=float32), Array(0.24367505, dtype=float32)), 'eval/episode_reward': (Array(-10344.565, dtype=float32), Array(5044.819, dtype=float32)), 'eval/avg_episode_length': (Array(914.5469, dtype=float32), Array(278.69257, dtype=float32)), 'eval/epoch_eval_time': 4.110546588897705, 'eval/sps': 31139.411081173224}
I0726 22:16:43.236267 140267183036224 train.py:379] starting iteration 403 2895.6572921276093
I0726 22:16:50.366292 140267183036224 train.py:394] {'eval/walltime': 1651.1068723201752, 'training/sps': 40733.688994730896, 'training/walltime': 1240.9067916870117, 'training/entropy_loss': Array(-0.04876939, dtype=float32), 'training/policy_loss': Array(-3.49421e-05, dtype=float32), 'training/total_loss': Array(163.97842, dtype=float32), 'training/v_loss': Array(164.02722, dtype=float32), 'eval/episode_goal_distance': (Array(0.45986104, dtype=float32), Array(0.21186508, dtype=float32)), 'eval/episode_reward': (Array(-10036.686, dtype=float32), Array(4709.692, dtype=float32)), 'eval/avg_episode_length': (Array(914.5547, dtype=float32), Array(278.66702, dtype=float32)), 'eval/epoch_eval_time': 4.109773874282837, 'eval/sps': 31145.265874837514}
I0726 22:16:50.368677 140267183036224 train.py:379] starting iteration 404 2902.7897033691406
I0726 22:16:57.518541 140267183036224 train.py:394] {'eval/walltime': 1655.225327014923, 'training/sps': 40583.04879601524, 'training/walltime': 1243.9346568584442, 'training/entropy_loss': Array(-0.04913851, dtype=float32), 'training/policy_loss': Array(-0.00022201, dtype=float32), 'training/total_loss': Array(186.2587, dtype=float32), 'training/v_loss': Array(186.30804, dtype=float32), 'eval/episode_goal_distance': (Array(0.44131488, dtype=float32), Array(0.18557595, dtype=float32)), 'eval/episode_reward': (Array(-9318.01, dtype=float32), Array(4555.433, dtype=float32)), 'eval/avg_episode_length': (Array(899.125, dtype=float32), Array(300.02795, dtype=float32)), 'eval/epoch_eval_time': 4.118454694747925, 'eval/sps': 31079.618324618816}
I0726 22:16:57.520893 140267183036224 train.py:379] starting iteration 405 2909.9419190883636
I0726 22:17:04.649065 140267183036224 train.py:394] {'eval/walltime': 1659.3267922401428, 'training/sps': 40645.46463427281, 'training/walltime': 1246.957872390747, 'training/entropy_loss': Array(-0.04854213, dtype=float32), 'training/policy_loss': Array(-0.00024792, dtype=float32), 'training/total_loss': Array(170.48036, dtype=float32), 'training/v_loss': Array(170.52914, dtype=float32), 'eval/episode_goal_distance': (Array(0.4614315, dtype=float32), Array(0.22658584, dtype=float32)), 'eval/episode_reward': (Array(-10049.297, dtype=float32), Array(4796.425, dtype=float32)), 'eval/avg_episode_length': (Array(914.6172, dtype=float32), Array(278.46304, dtype=float32)), 'eval/epoch_eval_time': 4.101465225219727, 'eval/sps': 31208.359201227337}
I0726 22:17:04.651365 140267183036224 train.py:379] starting iteration 406 2917.0723910331726
I0726 22:17:11.803354 140267183036224 train.py:394] {'eval/walltime': 1663.4411783218384, 'training/sps': 40500.15602344218, 'training/walltime': 1249.9919347763062, 'training/entropy_loss': Array(-0.04841813, dtype=float32), 'training/policy_loss': Array(-0.00059118, dtype=float32), 'training/total_loss': Array(178.52621, dtype=float32), 'training/v_loss': Array(178.57523, dtype=float32), 'eval/episode_goal_distance': (Array(0.48310328, dtype=float32), Array(0.20600353, dtype=float32)), 'eval/episode_reward': (Array(-9962.156, dtype=float32), Array(4762.763, dtype=float32)), 'eval/avg_episode_length': (Array(906.78906, dtype=float32), Array(289.80527, dtype=float32)), 'eval/epoch_eval_time': 4.114386081695557, 'eval/sps': 31110.35217853222}
I0726 22:17:11.805683 140267183036224 train.py:379] starting iteration 407 2924.2267088890076
I0726 22:17:18.935515 140267183036224 train.py:394] {'eval/walltime': 1667.5395879745483, 'training/sps': 40583.68472426035, 'training/walltime': 1253.0197525024414, 'training/entropy_loss': Array(-0.04762325, dtype=float32), 'training/policy_loss': Array(-0.00025281, dtype=float32), 'training/total_loss': Array(174.28902, dtype=float32), 'training/v_loss': Array(174.33688, dtype=float32), 'eval/episode_goal_distance': (Array(0.42237684, dtype=float32), Array(0.17675863, dtype=float32)), 'eval/episode_reward': (Array(-9353.882, dtype=float32), Array(3988.739, dtype=float32)), 'eval/avg_episode_length': (Array(937.89844, dtype=float32), Array(240.51884, dtype=float32)), 'eval/epoch_eval_time': 4.098409652709961, 'eval/sps': 31231.626617745133}
I0726 22:17:18.937837 140267183036224 train.py:379] starting iteration 408 2931.3588633537292
I0726 22:17:26.066526 140267183036224 train.py:394] {'eval/walltime': 1671.6445434093475, 'training/sps': 40686.49674991042, 'training/walltime': 1256.0399191379547, 'training/entropy_loss': Array(-0.04686214, dtype=float32), 'training/policy_loss': Array(-0.0003501, dtype=float32), 'training/total_loss': Array(157.28769, dtype=float32), 'training/v_loss': Array(157.3349, dtype=float32), 'eval/episode_goal_distance': (Array(0.43889442, dtype=float32), Array(0.19073826, dtype=float32)), 'eval/episode_reward': (Array(-9410.024, dtype=float32), Array(4741.952, dtype=float32)), 'eval/avg_episode_length': (Array(883.46094, dtype=float32), Array(319.86465, dtype=float32)), 'eval/epoch_eval_time': 4.104955434799194, 'eval/sps': 31181.824512611667}
I0726 22:17:26.235252 140267183036224 train.py:379] starting iteration 409 2938.656272649765
I0726 22:17:33.368336 140267183036224 train.py:394] {'eval/walltime': 1675.7572329044342, 'training/sps': 40734.55823414583, 'training/walltime': 1259.0565223693848, 'training/entropy_loss': Array(-0.04684005, dtype=float32), 'training/policy_loss': Array(8.332689e-06, dtype=float32), 'training/total_loss': Array(185.77364, dtype=float32), 'training/v_loss': Array(185.82047, dtype=float32), 'eval/episode_goal_distance': (Array(0.42153412, dtype=float32), Array(0.17934224, dtype=float32)), 'eval/episode_reward': (Array(-9605.533, dtype=float32), Array(3837.9714, dtype=float32)), 'eval/avg_episode_length': (Array(968.9375, dtype=float32), Array(172.94876, dtype=float32)), 'eval/epoch_eval_time': 4.11268949508667, 'eval/sps': 31123.18597183631}
I0726 22:17:33.370871 140267183036224 train.py:379] starting iteration 410 2945.7918968200684
I0726 22:17:40.513453 140267183036224 train.py:394] {'eval/walltime': 1679.866540670395, 'training/sps': 40559.11851474116, 'training/walltime': 1262.0861740112305, 'training/entropy_loss': Array(-0.04703628, dtype=float32), 'training/policy_loss': Array(-0.00029892, dtype=float32), 'training/total_loss': Array(164.86334, dtype=float32), 'training/v_loss': Array(164.91068, dtype=float32), 'eval/episode_goal_distance': (Array(0.43920028, dtype=float32), Array(0.18122745, dtype=float32)), 'eval/episode_reward': (Array(-9587.743, dtype=float32), Array(4250.5938, dtype=float32)), 'eval/avg_episode_length': (Array(922.4375, dtype=float32), Array(266.43677, dtype=float32)), 'eval/epoch_eval_time': 4.109307765960693, 'eval/sps': 31148.79860308432}
I0726 22:17:40.515939 140267183036224 train.py:379] starting iteration 411 2952.93696475029
I0726 22:17:47.649077 140267183036224 train.py:394] {'eval/walltime': 1683.972454071045, 'training/sps': 40639.80147061375, 'training/walltime': 1265.1098108291626, 'training/entropy_loss': Array(-0.04699749, dtype=float32), 'training/policy_loss': Array(-0.00049278, dtype=float32), 'training/total_loss': Array(164.39081, dtype=float32), 'training/v_loss': Array(164.4383, dtype=float32), 'eval/episode_goal_distance': (Array(0.44738597, dtype=float32), Array(0.19160351, dtype=float32)), 'eval/episode_reward': (Array(-9931.837, dtype=float32), Array(4155.6514, dtype=float32)), 'eval/avg_episode_length': (Array(937.875, dtype=float32), Array(240.60934, dtype=float32)), 'eval/epoch_eval_time': 4.105913400650024, 'eval/sps': 31174.54936573572}
I0726 22:17:47.651626 140267183036224 train.py:379] starting iteration 412 2960.0726523399353
I0726 22:17:54.785682 140267183036224 train.py:394] {'eval/walltime': 1688.0779340267181, 'training/sps': 40621.62079136627, 'training/walltime': 1268.1348009109497, 'training/entropy_loss': Array(-0.04720356, dtype=float32), 'training/policy_loss': Array(-0.00014617, dtype=float32), 'training/total_loss': Array(156.1034, dtype=float32), 'training/v_loss': Array(156.15076, dtype=float32), 'eval/episode_goal_distance': (Array(0.4219655, dtype=float32), Array(0.19786662, dtype=float32)), 'eval/episode_reward': (Array(-8998.209, dtype=float32), Array(4504.868, dtype=float32)), 'eval/avg_episode_length': (Array(899., dtype=float32), Array(300.39972, dtype=float32)), 'eval/epoch_eval_time': 4.105479955673218, 'eval/sps': 31177.840686597756}
I0726 22:17:54.788017 140267183036224 train.py:379] starting iteration 413 2967.209043741226
I0726 22:18:01.916004 140267183036224 train.py:394] {'eval/walltime': 1692.1806480884552, 'training/sps': 40664.37570950171, 'training/walltime': 1271.1566104888916, 'training/entropy_loss': Array(-0.0469666, dtype=float32), 'training/policy_loss': Array(-0.00022118, dtype=float32), 'training/total_loss': Array(161.55249, dtype=float32), 'training/v_loss': Array(161.59967, dtype=float32), 'eval/episode_goal_distance': (Array(0.41772795, dtype=float32), Array(0.16908658, dtype=float32)), 'eval/episode_reward': (Array(-9392.977, dtype=float32), Array(4038.293, dtype=float32)), 'eval/avg_episode_length': (Array(930.2031, dtype=float32), Array(253.79839, dtype=float32)), 'eval/epoch_eval_time': 4.1027140617370605, 'eval/sps': 31198.8596021741}
I0726 22:18:01.918444 140267183036224 train.py:379] starting iteration 414 2974.339469432831
I0726 22:18:09.044022 140267183036224 train.py:394] {'eval/walltime': 1696.2907845973969, 'training/sps': 40798.08077527393, 'training/walltime': 1274.1685168743134, 'training/entropy_loss': Array(-0.0470856, dtype=float32), 'training/policy_loss': Array(3.0378105e-05, dtype=float32), 'training/total_loss': Array(152.68892, dtype=float32), 'training/v_loss': Array(152.73596, dtype=float32), 'eval/episode_goal_distance': (Array(0.4691369, dtype=float32), Array(0.22941877, dtype=float32)), 'eval/episode_reward': (Array(-9940.518, dtype=float32), Array(4636.633, dtype=float32)), 'eval/avg_episode_length': (Array(937.85156, dtype=float32), Array(240.70003, dtype=float32)), 'eval/epoch_eval_time': 4.11013650894165, 'eval/sps': 31142.517948378234}
I0726 22:18:09.046388 140267183036224 train.py:379] starting iteration 415 2981.4674139022827
I0726 22:18:16.176970 140267183036224 train.py:394] {'eval/walltime': 1700.3994014263153, 'training/sps': 40710.08241129376, 'training/walltime': 1277.1869337558746, 'training/entropy_loss': Array(-0.04731666, dtype=float32), 'training/policy_loss': Array(-0.00020165, dtype=float32), 'training/total_loss': Array(155.63919, dtype=float32), 'training/v_loss': Array(155.6867, dtype=float32), 'eval/episode_goal_distance': (Array(0.43604046, dtype=float32), Array(0.2086457, dtype=float32)), 'eval/episode_reward': (Array(-9477.73, dtype=float32), Array(5077.5093, dtype=float32)), 'eval/avg_episode_length': (Array(899.03906, dtype=float32), Array(300.28366, dtype=float32)), 'eval/epoch_eval_time': 4.108616828918457, 'eval/sps': 31154.03682793521}
I0726 22:18:16.179536 140267183036224 train.py:379] starting iteration 416 2988.6005618572235
I0726 22:18:23.313610 140267183036224 train.py:394] {'eval/walltime': 1704.505099773407, 'training/sps': 40625.181334407855, 'training/walltime': 1280.2116587162018, 'training/entropy_loss': Array(-0.04805633, dtype=float32), 'training/policy_loss': Array(-0.00011268, dtype=float32), 'training/total_loss': Array(238.89389, dtype=float32), 'training/v_loss': Array(238.94206, dtype=float32), 'eval/episode_goal_distance': (Array(0.46558678, dtype=float32), Array(0.2127198, dtype=float32)), 'eval/episode_reward': (Array(-10123.776, dtype=float32), Array(4654.712, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.75845, dtype=float32)), 'eval/epoch_eval_time': 4.105698347091675, 'eval/sps': 31176.182266451815}
I0726 22:18:23.316086 140267183036224 train.py:379] starting iteration 417 2995.7371122837067
I0726 22:18:30.450108 140267183036224 train.py:394] {'eval/walltime': 1708.6117610931396, 'training/sps': 40637.731460608775, 'training/walltime': 1283.235449552536, 'training/entropy_loss': Array(-0.04763104, dtype=float32), 'training/policy_loss': Array(-0.00070788, dtype=float32), 'training/total_loss': Array(179.14099, dtype=float32), 'training/v_loss': Array(179.18932, dtype=float32), 'eval/episode_goal_distance': (Array(0.4503155, dtype=float32), Array(0.19039255, dtype=float32)), 'eval/episode_reward': (Array(-9584.986, dtype=float32), Array(4955.9873, dtype=float32)), 'eval/avg_episode_length': (Array(891.3125, dtype=float32), Array(310.14816, dtype=float32)), 'eval/epoch_eval_time': 4.106661319732666, 'eval/sps': 31168.871751112045}
I0726 22:18:30.452599 140267183036224 train.py:379] starting iteration 418 3002.873624563217
I0726 22:18:37.590623 140267183036224 train.py:394] {'eval/walltime': 1712.7199039459229, 'training/sps': 40604.63675708765, 'training/walltime': 1286.2617049217224, 'training/entropy_loss': Array(-0.04551083, dtype=float32), 'training/policy_loss': Array(-0.00076788, dtype=float32), 'training/total_loss': Array(139.5217, dtype=float32), 'training/v_loss': Array(139.568, dtype=float32), 'eval/episode_goal_distance': (Array(0.4027812, dtype=float32), Array(0.15247963, dtype=float32)), 'eval/episode_reward': (Array(-8942.281, dtype=float32), Array(3758.6104, dtype=float32)), 'eval/avg_episode_length': (Array(922.46094, dtype=float32), Array(266.35587, dtype=float32)), 'eval/epoch_eval_time': 4.108142852783203, 'eval/sps': 31157.631218515682}
I0726 22:18:37.592970 140267183036224 train.py:379] starting iteration 419 3010.0139956474304
I0726 22:18:44.724879 140267183036224 train.py:394] {'eval/walltime': 1716.824066400528, 'training/sps': 40632.829646392354, 'training/walltime': 1289.2858605384827, 'training/entropy_loss': Array(-0.0445356, dtype=float32), 'training/policy_loss': Array(-0.00033407, dtype=float32), 'training/total_loss': Array(131.0723, dtype=float32), 'training/v_loss': Array(131.11716, dtype=float32), 'eval/episode_goal_distance': (Array(0.39633176, dtype=float32), Array(0.19773136, dtype=float32)), 'eval/episode_reward': (Array(-8843.211, dtype=float32), Array(4170.914, dtype=float32)), 'eval/avg_episode_length': (Array(906.875, dtype=float32), Array(289.53778, dtype=float32)), 'eval/epoch_eval_time': 4.1041624546051025, 'eval/sps': 31187.84926663338}
I0726 22:18:44.727324 140267183036224 train.py:379] starting iteration 420 3017.148350954056
I0726 22:18:51.854495 140267183036224 train.py:394] {'eval/walltime': 1720.935311794281, 'training/sps': 40791.69700766801, 'training/walltime': 1292.2982382774353, 'training/entropy_loss': Array(-0.04520752, dtype=float32), 'training/policy_loss': Array(-0.00010624, dtype=float32), 'training/total_loss': Array(137.0922, dtype=float32), 'training/v_loss': Array(137.13751, dtype=float32), 'eval/episode_goal_distance': (Array(0.45150715, dtype=float32), Array(0.20326687, dtype=float32)), 'eval/episode_reward': (Array(-9537.492, dtype=float32), Array(4634.5024, dtype=float32)), 'eval/avg_episode_length': (Array(898.9219, dtype=float32), Array(300.63168, dtype=float32)), 'eval/epoch_eval_time': 4.111245393753052, 'eval/sps': 31134.118190680914}
I0726 22:18:51.856872 140267183036224 train.py:379] starting iteration 421 3024.277898311615
I0726 22:18:58.993247 140267183036224 train.py:394] {'eval/walltime': 1725.0510687828064, 'training/sps': 40727.97225545784, 'training/walltime': 1295.3153293132782, 'training/entropy_loss': Array(-0.04551379, dtype=float32), 'training/policy_loss': Array(0.00043785, dtype=float32), 'training/total_loss': Array(151.4277, dtype=float32), 'training/v_loss': Array(151.4728, dtype=float32), 'eval/episode_goal_distance': (Array(0.41326714, dtype=float32), Array(0.15891849, dtype=float32)), 'eval/episode_reward': (Array(-9038.674, dtype=float32), Array(4284.193, dtype=float32)), 'eval/avg_episode_length': (Array(891.28125, dtype=float32), Array(310.23706, dtype=float32)), 'eval/epoch_eval_time': 4.115756988525391, 'eval/sps': 31099.989711943694}
I0726 22:18:58.995825 140267183036224 train.py:379] starting iteration 422 3031.4168508052826
I0726 22:19:06.136873 140267183036224 train.py:394] {'eval/walltime': 1729.1636452674866, 'training/sps': 40622.799031477, 'training/walltime': 1298.3402316570282, 'training/entropy_loss': Array(-0.04530388, dtype=float32), 'training/policy_loss': Array(-0.00055307, dtype=float32), 'training/total_loss': Array(148.84473, dtype=float32), 'training/v_loss': Array(148.8906, dtype=float32), 'eval/episode_goal_distance': (Array(0.4533208, dtype=float32), Array(0.20153573, dtype=float32)), 'eval/episode_reward': (Array(-9823.619, dtype=float32), Array(4457.893, dtype=float32)), 'eval/avg_episode_length': (Array(922.3203, dtype=float32), Array(266.83902, dtype=float32)), 'eval/epoch_eval_time': 4.112576484680176, 'eval/sps': 31124.041212805365}
I0726 22:19:06.139237 140267183036224 train.py:379] starting iteration 423 3038.5602629184723
I0726 22:19:13.277392 140267183036224 train.py:394] {'eval/walltime': 1733.2722640037537, 'training/sps': 40606.19151598312, 'training/walltime': 1301.3663711547852, 'training/entropy_loss': Array(-0.04469429, dtype=float32), 'training/policy_loss': Array(0.00066732, dtype=float32), 'training/total_loss': Array(150.74562, dtype=float32), 'training/v_loss': Array(150.78964, dtype=float32), 'eval/episode_goal_distance': (Array(0.46216357, dtype=float32), Array(0.19496807, dtype=float32)), 'eval/episode_reward': (Array(-9902.098, dtype=float32), Array(4639.219, dtype=float32)), 'eval/avg_episode_length': (Array(914.5547, dtype=float32), Array(278.66687, dtype=float32)), 'eval/epoch_eval_time': 4.10861873626709, 'eval/sps': 31154.022365262143}
I0726 22:19:13.279928 140267183036224 train.py:379] starting iteration 424 3045.7009534835815
I0726 22:19:20.421358 140267183036224 train.py:394] {'eval/walltime': 1737.3816359043121, 'training/sps': 40574.77715350262, 'training/walltime': 1304.394853591919, 'training/entropy_loss': Array(-0.04574606, dtype=float32), 'training/policy_loss': Array(0.00049062, dtype=float32), 'training/total_loss': Array(155.0083, dtype=float32), 'training/v_loss': Array(155.05356, dtype=float32), 'eval/episode_goal_distance': (Array(0.40169877, dtype=float32), Array(0.17875643, dtype=float32)), 'eval/episode_reward': (Array(-8181.734, dtype=float32), Array(4843.6396, dtype=float32)), 'eval/avg_episode_length': (Array(836.8281, dtype=float32), Array(368.32245, dtype=float32)), 'eval/epoch_eval_time': 4.109371900558472, 'eval/sps': 31148.312466585114}
I0726 22:19:20.423923 140267183036224 train.py:379] starting iteration 425 3052.8449490070343
I0726 22:19:27.569272 140267183036224 train.py:394] {'eval/walltime': 1741.491592168808, 'training/sps': 40529.12182066317, 'training/walltime': 1307.426747560501, 'training/entropy_loss': Array(-0.04545163, dtype=float32), 'training/policy_loss': Array(0.00041729, dtype=float32), 'training/total_loss': Array(142.8955, dtype=float32), 'training/v_loss': Array(142.94052, dtype=float32), 'eval/episode_goal_distance': (Array(0.44392085, dtype=float32), Array(0.20994005, dtype=float32)), 'eval/episode_reward': (Array(-9158.418, dtype=float32), Array(5048.675, dtype=float32)), 'eval/avg_episode_length': (Array(867.91406, dtype=float32), Array(337.51587, dtype=float32)), 'eval/epoch_eval_time': 4.10995626449585, 'eval/sps': 31143.88372103546}
I0726 22:19:27.571632 140267183036224 train.py:379] starting iteration 426 3059.9926583766937
I0726 22:19:34.707795 140267183036224 train.py:394] {'eval/walltime': 1745.5938374996185, 'training/sps': 40557.9694986638, 'training/walltime': 1310.4564850330353, 'training/entropy_loss': Array(-0.04683208, dtype=float32), 'training/policy_loss': Array(0.00075189, dtype=float32), 'training/total_loss': Array(188.50381, dtype=float32), 'training/v_loss': Array(188.54987, dtype=float32), 'eval/episode_goal_distance': (Array(0.48327696, dtype=float32), Array(0.22212185, dtype=float32)), 'eval/episode_reward': (Array(-9591.729, dtype=float32), Array(5029.673, dtype=float32)), 'eval/avg_episode_length': (Array(883.53906, dtype=float32), Array(319.64987, dtype=float32)), 'eval/epoch_eval_time': 4.102245330810547, 'eval/sps': 31202.424447566857}
I0726 22:19:34.710317 140267183036224 train.py:379] starting iteration 427 3067.131343126297
I0726 22:19:41.837269 140267183036224 train.py:394] {'eval/walltime': 1749.6932790279388, 'training/sps': 40641.512752653114, 'training/walltime': 1313.4799945354462, 'training/entropy_loss': Array(-0.04770033, dtype=float32), 'training/policy_loss': Array(3.161475e-05, dtype=float32), 'training/total_loss': Array(162.1492, dtype=float32), 'training/v_loss': Array(162.19687, dtype=float32), 'eval/episode_goal_distance': (Array(0.43581763, dtype=float32), Array(0.18932228, dtype=float32)), 'eval/episode_reward': (Array(-8774.822, dtype=float32), Array(4909.727, dtype=float32)), 'eval/avg_episode_length': (Array(852.3906, dtype=float32), Array(353.5505, dtype=float32)), 'eval/epoch_eval_time': 4.0994415283203125, 'eval/sps': 31223.765265520487}
I0726 22:19:41.839617 140267183036224 train.py:379] starting iteration 428 3074.2606434822083
I0726 22:19:48.979100 140267183036224 train.py:394] {'eval/walltime': 1753.8097405433655, 'training/sps': 40702.68464600754, 'training/walltime': 1316.498960018158, 'training/entropy_loss': Array(-0.04840939, dtype=float32), 'training/policy_loss': Array(0.00031487, dtype=float32), 'training/total_loss': Array(158.93224, dtype=float32), 'training/v_loss': Array(158.98032, dtype=float32), 'eval/episode_goal_distance': (Array(0.4349203, dtype=float32), Array(0.19428141, dtype=float32)), 'eval/episode_reward': (Array(-8090.744, dtype=float32), Array(5302.5664, dtype=float32)), 'eval/avg_episode_length': (Array(767.02344, dtype=float32), Array(421.08136, dtype=float32)), 'eval/epoch_eval_time': 4.116461515426636, 'eval/sps': 31094.66699015985}
I0726 22:19:48.981503 140267183036224 train.py:379] starting iteration 429 3081.4025297164917
I0726 22:19:56.128593 140267183036224 train.py:394] {'eval/walltime': 1757.9249196052551, 'training/sps': 40580.37428082274, 'training/walltime': 1319.5270247459412, 'training/entropy_loss': Array(-0.04709651, dtype=float32), 'training/policy_loss': Array(-0.00011377, dtype=float32), 'training/total_loss': Array(153.83452, dtype=float32), 'training/v_loss': Array(153.88173, dtype=float32), 'eval/episode_goal_distance': (Array(0.4237018, dtype=float32), Array(0.20126197, dtype=float32)), 'eval/episode_reward': (Array(-9118.416, dtype=float32), Array(4591.917, dtype=float32)), 'eval/avg_episode_length': (Array(883.3906, dtype=float32), Array(320.05713, dtype=float32)), 'eval/epoch_eval_time': 4.115179061889648, 'eval/sps': 31104.357325638146}
I0726 22:19:56.130979 140267183036224 train.py:379] starting iteration 430 3088.552004337311
I0726 22:20:03.278035 140267183036224 train.py:394] {'eval/walltime': 1762.0414326190948, 'training/sps': 40596.311545745084, 'training/walltime': 1322.553900718689, 'training/entropy_loss': Array(-0.04553989, dtype=float32), 'training/policy_loss': Array(-0.00013227, dtype=float32), 'training/total_loss': Array(154.84885, dtype=float32), 'training/v_loss': Array(154.89453, dtype=float32), 'eval/episode_goal_distance': (Array(0.42570323, dtype=float32), Array(0.17990051, dtype=float32)), 'eval/episode_reward': (Array(-8586.328, dtype=float32), Array(4550.154, dtype=float32)), 'eval/avg_episode_length': (Array(852.4922, dtype=float32), Array(353.30716, dtype=float32)), 'eval/epoch_eval_time': 4.116513013839722, 'eval/sps': 31094.277989566373}
I0726 22:20:03.280365 140267183036224 train.py:379] starting iteration 431 3095.701390981674
I0726 22:20:10.411268 140267183036224 train.py:394] {'eval/walltime': 1766.1440358161926, 'training/sps': 40626.49428205722, 'training/walltime': 1325.5785279273987, 'training/entropy_loss': Array(-0.04547925, dtype=float32), 'training/policy_loss': Array(0.00066647, dtype=float32), 'training/total_loss': Array(148.52151, dtype=float32), 'training/v_loss': Array(148.56631, dtype=float32), 'eval/episode_goal_distance': (Array(0.44974324, dtype=float32), Array(0.1935153, dtype=float32)), 'eval/episode_reward': (Array(-8988.825, dtype=float32), Array(5430.26, dtype=float32)), 'eval/avg_episode_length': (Array(813.6172, dtype=float32), Array(387.98764, dtype=float32)), 'eval/epoch_eval_time': 4.102603197097778, 'eval/sps': 31199.70268890456}
I0726 22:20:10.413515 140267183036224 train.py:379] starting iteration 432 3102.834541320801
I0726 22:20:17.563531 140267183036224 train.py:394] {'eval/walltime': 1770.258465051651, 'training/sps': 40528.45891838517, 'training/walltime': 1328.6104714870453, 'training/entropy_loss': Array(-0.04498225, dtype=float32), 'training/policy_loss': Array(0.00046905, dtype=float32), 'training/total_loss': Array(147.29413, dtype=float32), 'training/v_loss': Array(147.33864, dtype=float32), 'eval/episode_goal_distance': (Array(0.4150542, dtype=float32), Array(0.17293116, dtype=float32)), 'eval/episode_reward': (Array(-8858.881, dtype=float32), Array(4970.4907, dtype=float32)), 'eval/avg_episode_length': (Array(844.58594, dtype=float32), Array(361.14996, dtype=float32)), 'eval/epoch_eval_time': 4.114429235458374, 'eval/sps': 31110.025880841276}
I0726 22:20:17.565874 140267183036224 train.py:379] starting iteration 433 3109.9869000911713
I0726 22:20:24.700446 140267183036224 train.py:394] {'eval/walltime': 1774.359031200409, 'training/sps': 40548.68081800702, 'training/walltime': 1331.6409029960632, 'training/entropy_loss': Array(-0.04250681, dtype=float32), 'training/policy_loss': Array(0.00156691, dtype=float32), 'training/total_loss': Array(322.2182, dtype=float32), 'training/v_loss': Array(322.25912, dtype=float32), 'eval/episode_goal_distance': (Array(0.42882678, dtype=float32), Array(0.17617096, dtype=float32)), 'eval/episode_reward': (Array(-8712.772, dtype=float32), Array(5143.4214, dtype=float32)), 'eval/avg_episode_length': (Array(813.5625, dtype=float32), Array(388.1011, dtype=float32)), 'eval/epoch_eval_time': 4.100566148757935, 'eval/sps': 31215.201842011822}
I0726 22:20:24.702774 140267183036224 train.py:379] starting iteration 434 3117.1238000392914
I0726 22:20:31.834268 140267183036224 train.py:394] {'eval/walltime': 1778.4605152606964, 'training/sps': 40603.24525287411, 'training/walltime': 1334.6672620773315, 'training/entropy_loss': Array(-0.04290351, dtype=float32), 'training/policy_loss': Array(0.00076746, dtype=float32), 'training/total_loss': Array(174.0722, dtype=float32), 'training/v_loss': Array(174.11435, dtype=float32), 'eval/episode_goal_distance': (Array(0.39544827, dtype=float32), Array(0.14329532, dtype=float32)), 'eval/episode_reward': (Array(-8100.1436, dtype=float32), Array(4413.698, dtype=float32)), 'eval/avg_episode_length': (Array(828.9844, dtype=float32), Array(375.38593, dtype=float32)), 'eval/epoch_eval_time': 4.101484060287476, 'eval/sps': 31208.21588442999}
I0726 22:20:31.836659 140267183036224 train.py:379] starting iteration 435 3124.2576858997345
I0726 22:20:38.951275 140267183036224 train.py:394] {'eval/walltime': 1782.5596356391907, 'training/sps': 40797.24111661807, 'training/walltime': 1337.6792304515839, 'training/entropy_loss': Array(-0.04303937, dtype=float32), 'training/policy_loss': Array(0.00032905, dtype=float32), 'training/total_loss': Array(148.38177, dtype=float32), 'training/v_loss': Array(148.4245, dtype=float32), 'eval/episode_goal_distance': (Array(0.41650409, dtype=float32), Array(0.18913172, dtype=float32)), 'eval/episode_reward': (Array(-8396.725, dtype=float32), Array(5024.5415, dtype=float32)), 'eval/avg_episode_length': (Array(821.2969, dtype=float32), Array(381.8243, dtype=float32)), 'eval/epoch_eval_time': 4.099120378494263, 'eval/sps': 31226.21152370706}
I0726 22:20:38.953490 140267183036224 train.py:379] starting iteration 436 3131.3745169639587
I0726 22:20:46.085986 140267183036224 train.py:394] {'eval/walltime': 1786.673844575882, 'training/sps': 40761.20714514446, 'training/walltime': 1340.6938614845276, 'training/entropy_loss': Array(-0.0430341, dtype=float32), 'training/policy_loss': Array(0.00074631, dtype=float32), 'training/total_loss': Array(142.71725, dtype=float32), 'training/v_loss': Array(142.75954, dtype=float32), 'eval/episode_goal_distance': (Array(0.42960763, dtype=float32), Array(0.1970093, dtype=float32)), 'eval/episode_reward': (Array(-8459.633, dtype=float32), Array(4808.5195, dtype=float32)), 'eval/avg_episode_length': (Array(852.5156, dtype=float32), Array(353.25125, dtype=float32)), 'eval/epoch_eval_time': 4.114208936691284, 'eval/sps': 31111.69169326139}
I0726 22:20:46.088251 140267183036224 train.py:379] starting iteration 437 3138.50927734375
I0726 22:20:53.219900 140267183036224 train.py:394] {'eval/walltime': 1790.7784616947174, 'training/sps': 40641.522367004254, 'training/walltime': 1343.7173702716827, 'training/entropy_loss': Array(-0.04201774, dtype=float32), 'training/policy_loss': Array(0.00116005, dtype=float32), 'training/total_loss': Array(158.12671, dtype=float32), 'training/v_loss': Array(158.16757, dtype=float32), 'eval/episode_goal_distance': (Array(0.44350874, dtype=float32), Array(0.17704505, dtype=float32)), 'eval/episode_reward': (Array(-8967.262, dtype=float32), Array(4750.0474, dtype=float32)), 'eval/avg_episode_length': (Array(860.1797, dtype=float32), Array(345.64572, dtype=float32)), 'eval/epoch_eval_time': 4.104617118835449, 'eval/sps': 31184.394620542782}
I0726 22:20:53.222037 140267183036224 train.py:379] starting iteration 438 3145.6430633068085
I0726 22:21:00.371356 140267183036224 train.py:394] {'eval/walltime': 1794.901515007019, 'training/sps': 40653.82284673897, 'training/walltime': 1346.7399642467499, 'training/entropy_loss': Array(-0.0402226, dtype=float32), 'training/policy_loss': Array(0.00110782, dtype=float32), 'training/total_loss': Array(145.69542, dtype=float32), 'training/v_loss': Array(145.73453, dtype=float32), 'eval/episode_goal_distance': (Array(0.4284378, dtype=float32), Array(0.16778623, dtype=float32)), 'eval/episode_reward': (Array(-8396.041, dtype=float32), Array(5148.3696, dtype=float32)), 'eval/avg_episode_length': (Array(798.09375, dtype=float32), Array(399.91135, dtype=float32)), 'eval/epoch_eval_time': 4.123053312301636, 'eval/sps': 31044.953898145406}
I0726 22:21:00.373506 140267183036224 train.py:379] starting iteration 439 3152.7945322990417
I0726 22:21:07.509325 140267183036224 train.py:394] {'eval/walltime': 1799.0068619251251, 'training/sps': 40597.171734039395, 'training/walltime': 1349.7667760849, 'training/entropy_loss': Array(-0.03835057, dtype=float32), 'training/policy_loss': Array(0.00115742, dtype=float32), 'training/total_loss': Array(137.5129, dtype=float32), 'training/v_loss': Array(137.55008, dtype=float32), 'eval/episode_goal_distance': (Array(0.4206887, dtype=float32), Array(0.17983988, dtype=float32)), 'eval/episode_reward': (Array(-8560.556, dtype=float32), Array(4534.8657, dtype=float32)), 'eval/avg_episode_length': (Array(852.3906, dtype=float32), Array(353.55066, dtype=float32)), 'eval/epoch_eval_time': 4.105346918106079, 'eval/sps': 31178.85103338606}
I0726 22:21:07.511667 140267183036224 train.py:379] starting iteration 440 3159.9326922893524
I0726 22:21:14.645623 140267183036224 train.py:394] {'eval/walltime': 1803.11736369133, 'training/sps': 40684.69496429524, 'training/walltime': 1352.787076473236, 'training/entropy_loss': Array(-0.03659583, dtype=float32), 'training/policy_loss': Array(0.00123785, dtype=float32), 'training/total_loss': Array(149.03116, dtype=float32), 'training/v_loss': Array(149.06653, dtype=float32), 'eval/episode_goal_distance': (Array(0.4137044, dtype=float32), Array(0.16459274, dtype=float32)), 'eval/episode_reward': (Array(-8541.619, dtype=float32), Array(4478.1436, dtype=float32)), 'eval/avg_episode_length': (Array(867.89844, dtype=float32), Array(337.5558, dtype=float32)), 'eval/epoch_eval_time': 4.110501766204834, 'eval/sps': 31139.750638808393}
I0726 22:21:14.648052 140267183036224 train.py:379] starting iteration 441 3167.0690789222717
I0726 22:21:21.775757 140267183036224 train.py:394] {'eval/walltime': 1807.2245135307312, 'training/sps': 40743.83243010826, 'training/walltime': 1355.8029930591583, 'training/entropy_loss': Array(-0.03472193, dtype=float32), 'training/policy_loss': Array(0.00207297, dtype=float32), 'training/total_loss': Array(153.84808, dtype=float32), 'training/v_loss': Array(153.88074, dtype=float32), 'eval/episode_goal_distance': (Array(0.42619365, dtype=float32), Array(0.18084252, dtype=float32)), 'eval/episode_reward': (Array(-9085.229, dtype=float32), Array(4631.066, dtype=float32)), 'eval/avg_episode_length': (Array(883.5781, dtype=float32), Array(319.54297, dtype=float32)), 'eval/epoch_eval_time': 4.107149839401245, 'eval/sps': 31165.164409648198}
I0726 22:21:21.778059 140267183036224 train.py:379] starting iteration 442 3174.1990854740143
I0726 22:21:28.916046 140267183036224 train.py:394] {'eval/walltime': 1811.3467874526978, 'training/sps': 40795.12274684186, 'training/walltime': 1358.8151178359985, 'training/entropy_loss': Array(-0.04440556, dtype=float32), 'training/policy_loss': Array(0.00422848, dtype=float32), 'training/total_loss': Array(219.70102, dtype=float32), 'training/v_loss': Array(219.74121, dtype=float32), 'eval/episode_goal_distance': (Array(0.4341732, dtype=float32), Array(0.18548074, dtype=float32)), 'eval/episode_reward': (Array(-9610.21, dtype=float32), Array(4003.7786, dtype=float32)), 'eval/avg_episode_length': (Array(930.1172, dtype=float32), Array(254.11118, dtype=float32)), 'eval/epoch_eval_time': 4.122273921966553, 'eval/sps': 31050.82350736579}
I0726 22:21:28.918355 140267183036224 train.py:379] starting iteration 443 3181.339376449585
I0726 22:21:36.052385 140267183036224 train.py:394] {'eval/walltime': 1815.4578649997711, 'training/sps': 40698.5802230874, 'training/walltime': 1361.8343877792358, 'training/entropy_loss': Array(-0.0447771, dtype=float32), 'training/policy_loss': Array(3.1100673e-05, dtype=float32), 'training/total_loss': Array(159.20314, dtype=float32), 'training/v_loss': Array(159.24788, dtype=float32), 'eval/episode_goal_distance': (Array(0.44223568, dtype=float32), Array(0.20837413, dtype=float32)), 'eval/episode_reward': (Array(-9715.121, dtype=float32), Array(4481.233, dtype=float32)), 'eval/avg_episode_length': (Array(937.7969, dtype=float32), Array(240.9119, dtype=float32)), 'eval/epoch_eval_time': 4.111077547073364, 'eval/sps': 31135.389331471488}
I0726 22:21:36.054828 140267183036224 train.py:379] starting iteration 444 3188.475853919983
I0726 22:21:43.196809 140267183036224 train.py:394] {'eval/walltime': 1819.5765306949615, 'training/sps': 40692.73839265144, 'training/walltime': 1364.85409116745, 'training/entropy_loss': Array(-0.04512349, dtype=float32), 'training/policy_loss': Array(8.599374e-05, dtype=float32), 'training/total_loss': Array(139.26263, dtype=float32), 'training/v_loss': Array(139.30768, dtype=float32), 'eval/episode_goal_distance': (Array(0.4633937, dtype=float32), Array(0.20895651, dtype=float32)), 'eval/episode_reward': (Array(-10397.004, dtype=float32), Array(4074.2905, dtype=float32)), 'eval/avg_episode_length': (Array(961.1797, dtype=float32), Array(192.54279, dtype=float32)), 'eval/epoch_eval_time': 4.11866569519043, 'eval/sps': 31078.026106724796}
I0726 22:21:43.199070 140267183036224 train.py:379] starting iteration 445 3195.620095729828
I0726 22:21:50.324446 140267183036224 train.py:394] {'eval/walltime': 1823.679009437561, 'training/sps': 40699.0140892784, 'training/walltime': 1367.873328924179, 'training/entropy_loss': Array(-0.04635613, dtype=float32), 'training/policy_loss': Array(8.784498e-05, dtype=float32), 'training/total_loss': Array(137.46568, dtype=float32), 'training/v_loss': Array(137.51196, dtype=float32), 'eval/episode_goal_distance': (Array(0.46459857, dtype=float32), Array(0.19114685, dtype=float32)), 'eval/episode_reward': (Array(-9894.922, dtype=float32), Array(4742.865, dtype=float32)), 'eval/avg_episode_length': (Array(883.4844, dtype=float32), Array(319.80002, dtype=float32)), 'eval/epoch_eval_time': 4.102478742599487, 'eval/sps': 31200.649176037976}
I0726 22:21:50.326729 140267183036224 train.py:379] starting iteration 446 3202.747755050659
I0726 22:21:57.465186 140267183036224 train.py:394] {'eval/walltime': 1827.7906348705292, 'training/sps': 40643.253024313984, 'training/walltime': 1370.8967089653015, 'training/entropy_loss': Array(-0.04616271, dtype=float32), 'training/policy_loss': Array(-5.277832e-05, dtype=float32), 'training/total_loss': Array(138.33707, dtype=float32), 'training/v_loss': Array(138.38327, dtype=float32), 'eval/episode_goal_distance': (Array(0.46053058, dtype=float32), Array(0.19397345, dtype=float32)), 'eval/episode_reward': (Array(-10024.561, dtype=float32), Array(4291.2856, dtype=float32)), 'eval/avg_episode_length': (Array(922.3828, dtype=float32), Array(266.62427, dtype=float32)), 'eval/epoch_eval_time': 4.11162543296814, 'eval/sps': 31131.2404514431}
I0726 22:21:57.467387 140267183036224 train.py:379] starting iteration 447 3209.8884134292603
I0726 22:22:04.590426 140267183036224 train.py:394] {'eval/walltime': 1831.8919851779938, 'training/sps': 40715.462831452576, 'training/walltime': 1373.91472697258, 'training/entropy_loss': Array(-0.04580589, dtype=float32), 'training/policy_loss': Array(-0.00045089, dtype=float32), 'training/total_loss': Array(137.66158, dtype=float32), 'training/v_loss': Array(137.70782, dtype=float32), 'eval/episode_goal_distance': (Array(0.44425765, dtype=float32), Array(0.18806833, dtype=float32)), 'eval/episode_reward': (Array(-9408.443, dtype=float32), Array(4509.0537, dtype=float32)), 'eval/avg_episode_length': (Array(898.97656, dtype=float32), Array(300.469, dtype=float32)), 'eval/epoch_eval_time': 4.1013503074646, 'eval/sps': 31209.23364362112}
I0726 22:22:04.593029 140267183036224 train.py:379] starting iteration 448 3217.014054298401
I0726 22:22:11.741912 140267183036224 train.py:394] {'eval/walltime': 1836.0258889198303, 'training/sps': 40806.802350977436, 'training/walltime': 1376.9259896278381, 'training/entropy_loss': Array(-0.04529663, dtype=float32), 'training/policy_loss': Array(-0.00041427, dtype=float32), 'training/total_loss': Array(138.11115, dtype=float32), 'training/v_loss': Array(138.15685, dtype=float32), 'eval/episode_goal_distance': (Array(0.41726527, dtype=float32), Array(0.17705776, dtype=float32)), 'eval/episode_reward': (Array(-9249.408, dtype=float32), Array(4392.457, dtype=float32)), 'eval/avg_episode_length': (Array(914.53906, dtype=float32), Array(278.71777, dtype=float32)), 'eval/epoch_eval_time': 4.133903741836548, 'eval/sps': 30963.46891307491}
I0726 22:22:11.744236 140267183036224 train.py:379] starting iteration 449 3224.165261745453
I0726 22:22:18.883098 140267183036224 train.py:394] {'eval/walltime': 1840.1413564682007, 'training/sps': 40692.96972037858, 'training/walltime': 1379.9456758499146, 'training/entropy_loss': Array(-0.04631729, dtype=float32), 'training/policy_loss': Array(-0.00012686, dtype=float32), 'training/total_loss': Array(135.61984, dtype=float32), 'training/v_loss': Array(135.66629, dtype=float32), 'eval/episode_goal_distance': (Array(0.42564178, dtype=float32), Array(0.17747469, dtype=float32)), 'eval/episode_reward': (Array(-9282.939, dtype=float32), Array(4396.519, dtype=float32)), 'eval/avg_episode_length': (Array(899.0703, dtype=float32), Array(300.19077, dtype=float32)), 'eval/epoch_eval_time': 4.115467548370361, 'eval/sps': 31102.176969099248}
I0726 22:22:18.885389 140267183036224 train.py:379] starting iteration 450 3231.3064155578613
I0726 22:22:26.026587 140267183036224 train.py:394] {'eval/walltime': 1844.256618976593, 'training/sps': 40658.31275710426, 'training/walltime': 1382.967936038971, 'training/entropy_loss': Array(-0.04648983, dtype=float32), 'training/policy_loss': Array(4.4618304e-05, dtype=float32), 'training/total_loss': Array(262.75272, dtype=float32), 'training/v_loss': Array(262.79913, dtype=float32), 'eval/episode_goal_distance': (Array(0.4475904, dtype=float32), Array(0.21211034, dtype=float32)), 'eval/episode_reward': (Array(-9639.274, dtype=float32), Array(4379.548, dtype=float32)), 'eval/avg_episode_length': (Array(922.35156, dtype=float32), Array(266.73166, dtype=float32)), 'eval/epoch_eval_time': 4.115262508392334, 'eval/sps': 31103.726612571405}
I0726 22:22:26.028834 140267183036224 train.py:379] starting iteration 451 3238.449860095978
I0726 22:22:33.162291 140267183036224 train.py:394] {'eval/walltime': 1848.3635230064392, 'training/sps': 40649.23133563791, 'training/walltime': 1385.9908714294434, 'training/entropy_loss': Array(-0.04635526, dtype=float32), 'training/policy_loss': Array(-0.00037317, dtype=float32), 'training/total_loss': Array(146.6221, dtype=float32), 'training/v_loss': Array(146.66882, dtype=float32), 'eval/episode_goal_distance': (Array(0.47271702, dtype=float32), Array(0.1909155, dtype=float32)), 'eval/episode_reward': (Array(-10461.949, dtype=float32), Array(4233.87, dtype=float32)), 'eval/avg_episode_length': (Array(937.77344, dtype=float32), Array(241.00272, dtype=float32)), 'eval/epoch_eval_time': 4.106904029846191, 'eval/sps': 31167.029730858783}
I0726 22:22:33.164824 140267183036224 train.py:379] starting iteration 452 3245.585849761963
I0726 22:22:40.304851 140267183036224 train.py:394] {'eval/walltime': 1852.4765303134918, 'training/sps': 40642.246661202975, 'training/walltime': 1389.0143263339996, 'training/entropy_loss': Array(-0.04567168, dtype=float32), 'training/policy_loss': Array(0.00017089, dtype=float32), 'training/total_loss': Array(132.42226, dtype=float32), 'training/v_loss': Array(132.46774, dtype=float32), 'eval/episode_goal_distance': (Array(0.43994993, dtype=float32), Array(0.19116051, dtype=float32)), 'eval/episode_reward': (Array(-9688.518, dtype=float32), Array(4313.657, dtype=float32)), 'eval/avg_episode_length': (Array(922.28125, dtype=float32), Array(266.9729, dtype=float32)), 'eval/epoch_eval_time': 4.113007307052612, 'eval/sps': 31120.7810840786}
I0726 22:22:40.307052 140267183036224 train.py:379] starting iteration 453 3252.7280774116516
I0726 22:22:47.434912 140267183036224 train.py:394] {'eval/walltime': 1856.5755326747894, 'training/sps': 40617.77277674319, 'training/walltime': 1392.0396029949188, 'training/entropy_loss': Array(-0.04572976, dtype=float32), 'training/policy_loss': Array(-0.00031544, dtype=float32), 'training/total_loss': Array(136.81331, dtype=float32), 'training/v_loss': Array(136.85934, dtype=float32), 'eval/episode_goal_distance': (Array(0.43762106, dtype=float32), Array(0.16617048, dtype=float32)), 'eval/episode_reward': (Array(-9197.248, dtype=float32), Array(4490.784, dtype=float32)), 'eval/avg_episode_length': (Array(875.7344, dtype=float32), Array(328.77655, dtype=float32)), 'eval/epoch_eval_time': 4.099002361297607, 'eval/sps': 31227.110579043303}
I0726 22:22:47.437348 140267183036224 train.py:379] starting iteration 454 3259.858374118805
I0726 22:22:54.564986 140267183036224 train.py:394] {'eval/walltime': 1860.6814324855804, 'training/sps': 40713.912558583965, 'training/walltime': 1395.0577359199524, 'training/entropy_loss': Array(-0.04640985, dtype=float32), 'training/policy_loss': Array(-0.00022799, dtype=float32), 'training/total_loss': Array(153.15033, dtype=float32), 'training/v_loss': Array(153.19698, dtype=float32), 'eval/episode_goal_distance': (Array(0.4490341, dtype=float32), Array(0.19081011, dtype=float32)), 'eval/episode_reward': (Array(-9614.328, dtype=float32), Array(4300.0083, dtype=float32)), 'eval/avg_episode_length': (Array(922.375, dtype=float32), Array(266.65125, dtype=float32)), 'eval/epoch_eval_time': 4.105899810791016, 'eval/sps': 31174.652548411883}
I0726 22:22:54.567165 140267183036224 train.py:379] starting iteration 455 3266.9881908893585
I0726 22:23:01.692760 140267183036224 train.py:394] {'eval/walltime': 1864.7899460792542, 'training/sps': 40777.05773170232, 'training/walltime': 1398.0711951255798, 'training/entropy_loss': Array(-0.04649481, dtype=float32), 'training/policy_loss': Array(-0.00021724, dtype=float32), 'training/total_loss': Array(152.23323, dtype=float32), 'training/v_loss': Array(152.27994, dtype=float32), 'eval/episode_goal_distance': (Array(0.4257412, dtype=float32), Array(0.16781583, dtype=float32)), 'eval/episode_reward': (Array(-9442.981, dtype=float32), Array(4449.5923, dtype=float32)), 'eval/avg_episode_length': (Array(906.84375, dtype=float32), Array(289.63504, dtype=float32)), 'eval/epoch_eval_time': 4.108513593673706, 'eval/sps': 31154.81964014785}
I0726 22:23:01.695157 140267183036224 train.py:379] starting iteration 456 3274.116183280945
I0726 22:23:08.836852 140267183036224 train.py:394] {'eval/walltime': 1868.9101316928864, 'training/sps': 40715.906707250164, 'training/walltime': 1401.0891802310944, 'training/entropy_loss': Array(-0.04689646, dtype=float32), 'training/policy_loss': Array(-0.00045861, dtype=float32), 'training/total_loss': Array(146.0603, dtype=float32), 'training/v_loss': Array(146.10767, dtype=float32), 'eval/episode_goal_distance': (Array(0.47556937, dtype=float32), Array(0.18541494, dtype=float32)), 'eval/episode_reward': (Array(-10368.967, dtype=float32), Array(3804.8743, dtype=float32)), 'eval/avg_episode_length': (Array(953.3906, dtype=float32), Array(210.17363, dtype=float32)), 'eval/epoch_eval_time': 4.120185613632202, 'eval/sps': 31066.56155890025}
I0726 22:23:08.838983 140267183036224 train.py:379] starting iteration 457 3281.260009288788
I0726 22:23:15.966690 140267183036224 train.py:394] {'eval/walltime': 1873.009773015976, 'training/sps': 40626.04915127249, 'training/walltime': 1404.1138405799866, 'training/entropy_loss': Array(-0.0464563, dtype=float32), 'training/policy_loss': Array(-0.00070809, dtype=float32), 'training/total_loss': Array(146.94237, dtype=float32), 'training/v_loss': Array(146.98953, dtype=float32), 'eval/episode_goal_distance': (Array(0.4504557, dtype=float32), Array(0.1825246, dtype=float32)), 'eval/episode_reward': (Array(-9962.251, dtype=float32), Array(4319.817, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.08244, dtype=float32)), 'eval/epoch_eval_time': 4.0996413230896, 'eval/sps': 31222.24358484507}
I0726 22:23:15.968968 140267183036224 train.py:379] starting iteration 458 3288.3899941444397
I0726 22:23:23.109943 140267183036224 train.py:394] {'eval/walltime': 1877.1190071105957, 'training/sps': 40577.82151810864, 'training/walltime': 1407.1420958042145, 'training/entropy_loss': Array(-0.04690708, dtype=float32), 'training/policy_loss': Array(8.625511e-05, dtype=float32), 'training/total_loss': Array(187.44803, dtype=float32), 'training/v_loss': Array(187.49484, dtype=float32), 'eval/episode_goal_distance': (Array(0.45917574, dtype=float32), Array(0.20633869, dtype=float32)), 'eval/episode_reward': (Array(-10102.884, dtype=float32), Array(4585.0654, dtype=float32)), 'eval/avg_episode_length': (Array(914.6406, dtype=float32), Array(278.38678, dtype=float32)), 'eval/epoch_eval_time': 4.109234094619751, 'eval/sps': 31149.357046265945}
I0726 22:23:23.112284 140267183036224 train.py:379] starting iteration 459 3295.5333099365234
I0726 22:23:30.245444 140267183036224 train.py:394] {'eval/walltime': 1881.2225940227509, 'training/sps': 40609.099810583975, 'training/walltime': 1410.168018579483, 'training/entropy_loss': Array(-0.04747786, dtype=float32), 'training/policy_loss': Array(-8.31113e-05, dtype=float32), 'training/total_loss': Array(219.93504, dtype=float32), 'training/v_loss': Array(219.9826, dtype=float32), 'eval/episode_goal_distance': (Array(0.4569931, dtype=float32), Array(0.21155581, dtype=float32)), 'eval/episode_reward': (Array(-9814.051, dtype=float32), Array(4773.1226, dtype=float32)), 'eval/avg_episode_length': (Array(899.0078, dtype=float32), Array(300.37622, dtype=float32)), 'eval/epoch_eval_time': 4.103586912155151, 'eval/sps': 31192.22347182505}
I0726 22:23:30.648116 140267183036224 train.py:379] starting iteration 460 3303.069132566452
I0726 22:23:37.769664 140267183036224 train.py:394] {'eval/walltime': 1885.3170919418335, 'training/sps': 40646.84941694514, 'training/walltime': 1413.1911311149597, 'training/entropy_loss': Array(-0.04677502, dtype=float32), 'training/policy_loss': Array(7.4991285e-06, dtype=float32), 'training/total_loss': Array(169.8621, dtype=float32), 'training/v_loss': Array(169.90887, dtype=float32), 'eval/episode_goal_distance': (Array(0.4224748, dtype=float32), Array(0.19477901, dtype=float32)), 'eval/episode_reward': (Array(-9161.661, dtype=float32), Array(4595.8193, dtype=float32)), 'eval/avg_episode_length': (Array(891.2969, dtype=float32), Array(310.1923, dtype=float32)), 'eval/epoch_eval_time': 4.094497919082642, 'eval/sps': 31261.464172065807}
I0726 22:23:37.772262 140267183036224 train.py:379] starting iteration 461 3310.1932876110077
I0726 22:23:44.889737 140267183036224 train.py:394] {'eval/walltime': 1889.416978597641, 'training/sps': 40769.40985711218, 'training/walltime': 1416.2051556110382, 'training/entropy_loss': Array(-0.04699671, dtype=float32), 'training/policy_loss': Array(-4.6409106e-05, dtype=float32), 'training/total_loss': Array(155.37778, dtype=float32), 'training/v_loss': Array(155.42484, dtype=float32), 'eval/episode_goal_distance': (Array(0.4745353, dtype=float32), Array(0.2051418, dtype=float32)), 'eval/episode_reward': (Array(-9888.848, dtype=float32), Array(4064.2695, dtype=float32)), 'eval/avg_episode_length': (Array(937.8672, dtype=float32), Array(240.63962, dtype=float32)), 'eval/epoch_eval_time': 4.099886655807495, 'eval/sps': 31220.375280055076}
I0726 22:23:44.891950 140267183036224 train.py:379] starting iteration 462 3317.3129754066467
I0726 22:23:52.017485 140267183036224 train.py:394] {'eval/walltime': 1893.5191271305084, 'training/sps': 40691.678174219815, 'training/walltime': 1419.2249376773834, 'training/entropy_loss': Array(-0.04656086, dtype=float32), 'training/policy_loss': Array(-0.00062456, dtype=float32), 'training/total_loss': Array(140.37396, dtype=float32), 'training/v_loss': Array(140.42114, dtype=float32), 'eval/episode_goal_distance': (Array(0.47025627, dtype=float32), Array(0.20938317, dtype=float32)), 'eval/episode_reward': (Array(-10299.744, dtype=float32), Array(4678.396, dtype=float32)), 'eval/avg_episode_length': (Array(922.2969, dtype=float32), Array(266.9193, dtype=float32)), 'eval/epoch_eval_time': 4.102148532867432, 'eval/sps': 31203.160727709455}
I0726 22:23:52.019788 140267183036224 train.py:379] starting iteration 463 3324.440813779831
I0726 22:23:59.142472 140267183036224 train.py:394] {'eval/walltime': 1897.6153166294098, 'training/sps': 40650.58751903884, 'training/walltime': 1422.2477722167969, 'training/entropy_loss': Array(-0.04599392, dtype=float32), 'training/policy_loss': Array(-0.00070391, dtype=float32), 'training/total_loss': Array(137.99454, dtype=float32), 'training/v_loss': Array(138.04123, dtype=float32), 'eval/episode_goal_distance': (Array(0.4399463, dtype=float32), Array(0.20342343, dtype=float32)), 'eval/episode_reward': (Array(-9566.011, dtype=float32), Array(4260.936, dtype=float32)), 'eval/avg_episode_length': (Array(906.8594, dtype=float32), Array(289.58646, dtype=float32)), 'eval/epoch_eval_time': 4.096189498901367, 'eval/sps': 31248.554305002413}
I0726 22:23:59.144743 140267183036224 train.py:379] starting iteration 464 3331.5657691955566
I0726 22:24:06.280477 140267183036224 train.py:394] {'eval/walltime': 1901.721251487732, 'training/sps': 40606.96574074359, 'training/walltime': 1425.2738540172577, 'training/entropy_loss': Array(-0.0449548, dtype=float32), 'training/policy_loss': Array(-5.6356017e-05, dtype=float32), 'training/total_loss': Array(136.01129, dtype=float32), 'training/v_loss': Array(136.0563, dtype=float32), 'eval/episode_goal_distance': (Array(0.42096925, dtype=float32), Array(0.1954798, dtype=float32)), 'eval/episode_reward': (Array(-9259.742, dtype=float32), Array(4067.9058, dtype=float32)), 'eval/avg_episode_length': (Array(937.875, dtype=float32), Array(240.60957, dtype=float32)), 'eval/epoch_eval_time': 4.1059348583221436, 'eval/sps': 31174.386447111377}
I0726 22:24:06.282741 140267183036224 train.py:379] starting iteration 465 3338.703767299652
I0726 22:24:13.389594 140267183036224 train.py:394] {'eval/walltime': 1905.81148147583, 'training/sps': 40784.059777827264, 'training/walltime': 1428.2867958545685, 'training/entropy_loss': Array(-0.0440155, dtype=float32), 'training/policy_loss': Array(-0.00018134, dtype=float32), 'training/total_loss': Array(126.515564, dtype=float32), 'training/v_loss': Array(126.55976, dtype=float32), 'eval/episode_goal_distance': (Array(0.42374337, dtype=float32), Array(0.16277608, dtype=float32)), 'eval/episode_reward': (Array(-9343.4795, dtype=float32), Array(4275.1895, dtype=float32)), 'eval/avg_episode_length': (Array(906.8203, dtype=float32), Array(289.708, dtype=float32)), 'eval/epoch_eval_time': 4.0902299880981445, 'eval/sps': 31294.083797844538}
I0726 22:24:13.391993 140267183036224 train.py:379] starting iteration 466 3345.8130190372467
I0726 22:24:20.515150 140267183036224 train.py:394] {'eval/walltime': 1909.9143266677856, 'training/sps': 40734.12682920661, 'training/walltime': 1431.3034310340881, 'training/entropy_loss': Array(-0.04429261, dtype=float32), 'training/policy_loss': Array(0.00013842, dtype=float32), 'training/total_loss': Array(227.3866, dtype=float32), 'training/v_loss': Array(227.43076, dtype=float32), 'eval/episode_goal_distance': (Array(0.43751687, dtype=float32), Array(0.18322353, dtype=float32)), 'eval/episode_reward': (Array(-9320.861, dtype=float32), Array(4478.0977, dtype=float32)), 'eval/avg_episode_length': (Array(899.0156, dtype=float32), Array(300.3531, dtype=float32)), 'eval/epoch_eval_time': 4.102845191955566, 'eval/sps': 31197.86246163251}
I0726 22:24:20.517475 140267183036224 train.py:379] starting iteration 467 3352.9385018348694
I0726 22:24:27.647862 140267183036224 train.py:394] {'eval/walltime': 1914.0104038715363, 'training/sps': 40545.50685809578, 'training/walltime': 1434.3340997695923, 'training/entropy_loss': Array(-0.04412004, dtype=float32), 'training/policy_loss': Array(-0.00060137, dtype=float32), 'training/total_loss': Array(157.13275, dtype=float32), 'training/v_loss': Array(157.17746, dtype=float32), 'eval/episode_goal_distance': (Array(0.46200368, dtype=float32), Array(0.18708403, dtype=float32)), 'eval/episode_reward': (Array(-9835.579, dtype=float32), Array(4411.996, dtype=float32)), 'eval/avg_episode_length': (Array(922.4531, dtype=float32), Array(266.3827, dtype=float32)), 'eval/epoch_eval_time': 4.09607720375061, 'eval/sps': 31249.410993229238}
I0726 22:24:27.650061 140267183036224 train.py:379] starting iteration 468 3360.071087360382
I0726 22:24:34.783845 140267183036224 train.py:394] {'eval/walltime': 1918.1195921897888, 'training/sps': 40675.258270521255, 'training/walltime': 1437.3551008701324, 'training/entropy_loss': Array(-0.04380823, dtype=float32), 'training/policy_loss': Array(1.0334988e-06, dtype=float32), 'training/total_loss': Array(125.99924, dtype=float32), 'training/v_loss': Array(126.04304, dtype=float32), 'eval/episode_goal_distance': (Array(0.4084497, dtype=float32), Array(0.14844863, dtype=float32)), 'eval/episode_reward': (Array(-9080.05, dtype=float32), Array(3939.755, dtype=float32)), 'eval/avg_episode_length': (Array(922.3125, dtype=float32), Array(266.86575, dtype=float32)), 'eval/epoch_eval_time': 4.1091883182525635, 'eval/sps': 31149.704050174107}
I0726 22:24:34.786150 140267183036224 train.py:379] starting iteration 469 3367.207176208496
I0726 22:24:41.909992 140267183036224 train.py:394] {'eval/walltime': 1922.218671798706, 'training/sps': 40674.54884921086, 'training/walltime': 1440.3761546611786, 'training/entropy_loss': Array(-0.04377217, dtype=float32), 'training/policy_loss': Array(0.00011806, dtype=float32), 'training/total_loss': Array(116.670135, dtype=float32), 'training/v_loss': Array(116.7138, dtype=float32), 'eval/episode_goal_distance': (Array(0.4150108, dtype=float32), Array(0.15967791, dtype=float32)), 'eval/episode_reward': (Array(-8936.897, dtype=float32), Array(4235.5923, dtype=float32)), 'eval/avg_episode_length': (Array(891.21875, dtype=float32), Array(310.41537, dtype=float32)), 'eval/epoch_eval_time': 4.099079608917236, 'eval/sps': 31226.522100606613}
I0726 22:24:41.912435 140267183036224 train.py:379] starting iteration 470 3374.3334612846375
I0726 22:24:49.041236 140267183036224 train.py:394] {'eval/walltime': 1926.3221096992493, 'training/sps': 40666.20136519229, 'training/walltime': 1443.397828578949, 'training/entropy_loss': Array(-0.04467206, dtype=float32), 'training/policy_loss': Array(9.484579e-05, dtype=float32), 'training/total_loss': Array(125.80285, dtype=float32), 'training/v_loss': Array(125.84743, dtype=float32), 'eval/episode_goal_distance': (Array(0.4338792, dtype=float32), Array(0.1801335, dtype=float32)), 'eval/episode_reward': (Array(-9594.341, dtype=float32), Array(4390.4214, dtype=float32)), 'eval/avg_episode_length': (Array(922.3672, dtype=float32), Array(266.678, dtype=float32)), 'eval/epoch_eval_time': 4.103437900543213, 'eval/sps': 31193.356181424206}
I0726 22:24:49.043929 140267183036224 train.py:379] starting iteration 471 3381.4649546146393
I0726 22:24:56.170272 140267183036224 train.py:394] {'eval/walltime': 1930.4301793575287, 'training/sps': 40759.92738129471, 'training/walltime': 1446.4125542640686, 'training/entropy_loss': Array(-0.04506195, dtype=float32), 'training/policy_loss': Array(-0.00012248, dtype=float32), 'training/total_loss': Array(150.28284, dtype=float32), 'training/v_loss': Array(150.32803, dtype=float32), 'eval/episode_goal_distance': (Array(0.43561423, dtype=float32), Array(0.1839294, dtype=float32)), 'eval/episode_reward': (Array(-9876.145, dtype=float32), Array(4273.682, dtype=float32)), 'eval/avg_episode_length': (Array(937.83594, dtype=float32), Array(240.76064, dtype=float32)), 'eval/epoch_eval_time': 4.108069658279419, 'eval/sps': 31158.18636181797}
I0726 22:24:56.172657 140267183036224 train.py:379] starting iteration 472 3388.593683719635
I0726 22:25:03.308355 140267183036224 train.py:394] {'eval/walltime': 1934.5392537117004, 'training/sps': 40648.35932224457, 'training/walltime': 1449.4355545043945, 'training/entropy_loss': Array(-0.04348954, dtype=float32), 'training/policy_loss': Array(-0.00029952, dtype=float32), 'training/total_loss': Array(132.85294, dtype=float32), 'training/v_loss': Array(132.89673, dtype=float32), 'eval/episode_goal_distance': (Array(0.42574888, dtype=float32), Array(0.16304603, dtype=float32)), 'eval/episode_reward': (Array(-9333.898, dtype=float32), Array(3949.086, dtype=float32)), 'eval/avg_episode_length': (Array(922.3047, dtype=float32), Array(266.8923, dtype=float32)), 'eval/epoch_eval_time': 4.109074354171753, 'eval/sps': 31150.56797890443}
I0726 22:25:03.310840 140267183036224 train.py:379] starting iteration 473 3395.731866121292
I0726 22:25:10.440731 140267183036224 train.py:394] {'eval/walltime': 1938.643330335617, 'training/sps': 40659.31670843206, 'training/walltime': 1452.4577400684357, 'training/entropy_loss': Array(-0.04160652, dtype=float32), 'training/policy_loss': Array(6.943251e-05, dtype=float32), 'training/total_loss': Array(119.17074, dtype=float32), 'training/v_loss': Array(119.21228, dtype=float32), 'eval/episode_goal_distance': (Array(0.4175378, dtype=float32), Array(0.16451673, dtype=float32)), 'eval/episode_reward': (Array(-9344.759, dtype=float32), Array(4165.3306, dtype=float32)), 'eval/avg_episode_length': (Array(906.7656, dtype=float32), Array(289.8778, dtype=float32)), 'eval/epoch_eval_time': 4.104076623916626, 'eval/sps': 31188.501514342173}
I0726 22:25:10.443050 140267183036224 train.py:379] starting iteration 474 3402.86407661438
I0726 22:25:17.575189 140267183036224 train.py:394] {'eval/walltime': 1942.749097108841, 'training/sps': 40652.70052408781, 'training/walltime': 1455.4804174900055, 'training/entropy_loss': Array(-0.04306081, dtype=float32), 'training/policy_loss': Array(0.00042526, dtype=float32), 'training/total_loss': Array(124.77156, dtype=float32), 'training/v_loss': Array(124.8142, dtype=float32), 'eval/episode_goal_distance': (Array(0.44952622, dtype=float32), Array(0.1774943, dtype=float32)), 'eval/episode_reward': (Array(-10232.525, dtype=float32), Array(4042.2612, dtype=float32)), 'eval/avg_episode_length': (Array(945.6328, dtype=float32), Array(226.03795, dtype=float32)), 'eval/epoch_eval_time': 4.105766773223877, 'eval/sps': 31175.662688578264}
I0726 22:25:17.577463 140267183036224 train.py:379] starting iteration 475 3409.998489379883
I0726 22:25:24.696761 140267183036224 train.py:394] {'eval/walltime': 1946.8446044921875, 'training/sps': 40686.38433437547, 'training/walltime': 1458.500592470169, 'training/entropy_loss': Array(-0.04389898, dtype=float32), 'training/policy_loss': Array(-0.00027618, dtype=float32), 'training/total_loss': Array(185.06107, dtype=float32), 'training/v_loss': Array(185.10524, dtype=float32), 'eval/episode_goal_distance': (Array(0.40328544, dtype=float32), Array(0.16595861, dtype=float32)), 'eval/episode_reward': (Array(-8572.227, dtype=float32), Array(4098.7207, dtype=float32)), 'eval/avg_episode_length': (Array(899.0781, dtype=float32), Array(300.1672, dtype=float32)), 'eval/epoch_eval_time': 4.095507383346558, 'eval/sps': 31253.758818866416}
I0726 22:25:24.698920 140267183036224 train.py:379] starting iteration 476 3417.11994600296
I0726 22:25:31.811750 140267183036224 train.py:394] {'eval/walltime': 1950.9452171325684, 'training/sps': 40843.04324672493, 'training/walltime': 1461.5091831684113, 'training/entropy_loss': Array(-0.04488448, dtype=float32), 'training/policy_loss': Array(-5.25221e-05, dtype=float32), 'training/total_loss': Array(159.89645, dtype=float32), 'training/v_loss': Array(159.94138, dtype=float32), 'eval/episode_goal_distance': (Array(0.4353006, dtype=float32), Array(0.18633, dtype=float32)), 'eval/episode_reward': (Array(-9026.681, dtype=float32), Array(4714.274, dtype=float32)), 'eval/avg_episode_length': (Array(875.7578, dtype=float32), Array(328.7148, dtype=float32)), 'eval/epoch_eval_time': 4.100612640380859, 'eval/sps': 31214.84793260344}
I0726 22:25:31.814541 140267183036224 train.py:379] starting iteration 477 3424.235566139221
I0726 22:25:38.934340 140267183036224 train.py:394] {'eval/walltime': 1955.0424120426178, 'training/sps': 40702.78107936479, 'training/walltime': 1464.5281414985657, 'training/entropy_loss': Array(-0.04435514, dtype=float32), 'training/policy_loss': Array(-0.00022291, dtype=float32), 'training/total_loss': Array(152.28647, dtype=float32), 'training/v_loss': Array(152.33104, dtype=float32), 'eval/episode_goal_distance': (Array(0.43363965, dtype=float32), Array(0.1770176, dtype=float32)), 'eval/episode_reward': (Array(-9632.338, dtype=float32), Array(4001.0764, dtype=float32)), 'eval/avg_episode_length': (Array(945.65625, dtype=float32), Array(225.94044, dtype=float32)), 'eval/epoch_eval_time': 4.0971949100494385, 'eval/sps': 31240.88621853128}
I0726 22:25:38.936598 140267183036224 train.py:379] starting iteration 478 3431.3576242923737
I0726 22:25:46.071049 140267183036224 train.py:394] {'eval/walltime': 1959.1490077972412, 'training/sps': 40633.640126222264, 'training/walltime': 1467.5522367954254, 'training/entropy_loss': Array(-0.04300729, dtype=float32), 'training/policy_loss': Array(-0.00024486, dtype=float32), 'training/total_loss': Array(142.40613, dtype=float32), 'training/v_loss': Array(142.44939, dtype=float32), 'eval/episode_goal_distance': (Array(0.42401832, dtype=float32), Array(0.17548244, dtype=float32)), 'eval/episode_reward': (Array(-9220.973, dtype=float32), Array(4111.392, dtype=float32)), 'eval/avg_episode_length': (Array(922.2656, dtype=float32), Array(267.02692, dtype=float32)), 'eval/epoch_eval_time': 4.106595754623413, 'eval/sps': 31169.36938725735}
I0726 22:25:46.073265 140267183036224 train.py:379] starting iteration 479 3438.4942910671234
I0726 22:25:53.193544 140267183036224 train.py:394] {'eval/walltime': 1963.2397480010986, 'training/sps': 40609.61816425163, 'training/walltime': 1470.5781209468842, 'training/entropy_loss': Array(-0.04233912, dtype=float32), 'training/policy_loss': Array(-0.00061466, dtype=float32), 'training/total_loss': Array(116.82312, dtype=float32), 'training/v_loss': Array(116.866066, dtype=float32), 'eval/episode_goal_distance': (Array(0.4245348, dtype=float32), Array(0.17926435, dtype=float32)), 'eval/episode_reward': (Array(-9412.588, dtype=float32), Array(3875.3635, dtype=float32)), 'eval/avg_episode_length': (Array(937.9219, dtype=float32), Array(240.42792, dtype=float32)), 'eval/epoch_eval_time': 4.090740203857422, 'eval/sps': 31290.18065710958}
I0726 22:25:53.196034 140267183036224 train.py:379] starting iteration 480 3445.617059469223
I0726 22:26:00.310400 140267183036224 train.py:394] {'eval/walltime': 1967.3315041065216, 'training/sps': 40697.82178292877, 'training/walltime': 1473.5974471569061, 'training/entropy_loss': Array(-0.04099207, dtype=float32), 'training/policy_loss': Array(-0.00042655, dtype=float32), 'training/total_loss': Array(111.3781, dtype=float32), 'training/v_loss': Array(111.41952, dtype=float32), 'eval/episode_goal_distance': (Array(0.38761413, dtype=float32), Array(0.15595052, dtype=float32)), 'eval/episode_reward': (Array(-8624.51, dtype=float32), Array(3790.0376, dtype=float32)), 'eval/avg_episode_length': (Array(914.625, dtype=float32), Array(278.43762, dtype=float32)), 'eval/epoch_eval_time': 4.091756105422974, 'eval/sps': 31282.41192830538}
I0726 22:26:00.312796 140267183036224 train.py:379] starting iteration 481 3452.733822107315
I0726 22:26:07.426319 140267183036224 train.py:394] {'eval/walltime': 1971.430180311203, 'training/sps': 40807.97843384057, 'training/walltime': 1476.6086230278015, 'training/entropy_loss': Array(-0.04007066, dtype=float32), 'training/policy_loss': Array(0.00017462, dtype=float32), 'training/total_loss': Array(102.78499, dtype=float32), 'training/v_loss': Array(102.82489, dtype=float32), 'eval/episode_goal_distance': (Array(0.3992695, dtype=float32), Array(0.16019426, dtype=float32)), 'eval/episode_reward': (Array(-8917.691, dtype=float32), Array(4237.1064, dtype=float32)), 'eval/avg_episode_length': (Array(906.77344, dtype=float32), Array(289.8536, dtype=float32)), 'eval/epoch_eval_time': 4.0986762046813965, 'eval/sps': 31229.595510326453}
I0726 22:26:07.430987 140267183036224 train.py:379] starting iteration 482 3459.851996421814
I0726 22:26:14.569152 140267183036224 train.py:394] {'eval/walltime': 1975.5296108722687, 'training/sps': 40497.90291869996, 'training/walltime': 1479.6428542137146, 'training/entropy_loss': Array(-0.04122505, dtype=float32), 'training/policy_loss': Array(0.00013849, dtype=float32), 'training/total_loss': Array(102.32352, dtype=float32), 'training/v_loss': Array(102.36461, dtype=float32), 'eval/episode_goal_distance': (Array(0.4189927, dtype=float32), Array(0.1797367, dtype=float32)), 'eval/episode_reward': (Array(-8853.895, dtype=float32), Array(4504.381, dtype=float32)), 'eval/avg_episode_length': (Array(899.0078, dtype=float32), Array(300.37622, dtype=float32)), 'eval/epoch_eval_time': 4.099430561065674, 'eval/sps': 31223.848798825748}
I0726 22:26:14.571502 140267183036224 train.py:379] starting iteration 483 3466.9925277233124
I0726 22:26:21.708760 140267183036224 train.py:394] {'eval/walltime': 1979.629099369049, 'training/sps': 40532.31234120005, 'training/walltime': 1482.674509525299, 'training/entropy_loss': Array(-0.04273178, dtype=float32), 'training/policy_loss': Array(-0.00049982, dtype=float32), 'training/total_loss': Array(238.49275, dtype=float32), 'training/v_loss': Array(238.53596, dtype=float32), 'eval/episode_goal_distance': (Array(0.4057353, dtype=float32), Array(0.18875445, dtype=float32)), 'eval/episode_reward': (Array(-8772.986, dtype=float32), Array(4374.3633, dtype=float32)), 'eval/avg_episode_length': (Array(899.02344, dtype=float32), Array(300.3301, dtype=float32)), 'eval/epoch_eval_time': 4.0994884967803955, 'eval/sps': 31223.40753011675}
I0726 22:26:21.710960 140267183036224 train.py:379] starting iteration 484 3474.131985425949
I0726 22:26:28.843211 140267183036224 train.py:394] {'eval/walltime': 1983.727456331253, 'training/sps': 40557.70778743822, 'training/walltime': 1485.7042665481567, 'training/entropy_loss': Array(-0.04360463, dtype=float32), 'training/policy_loss': Array(-0.00016126, dtype=float32), 'training/total_loss': Array(126.74456, dtype=float32), 'training/v_loss': Array(126.78833, dtype=float32), 'eval/episode_goal_distance': (Array(0.4151306, dtype=float32), Array(0.18275045, dtype=float32)), 'eval/episode_reward': (Array(-8670.066, dtype=float32), Array(4648.969, dtype=float32)), 'eval/avg_episode_length': (Array(868.08594, dtype=float32), Array(337.07657, dtype=float32)), 'eval/epoch_eval_time': 4.0983569622039795, 'eval/sps': 31232.028146998022}
I0726 22:26:28.848170 140267183036224 train.py:379] starting iteration 485 3481.269181251526
I0726 22:26:35.984451 140267183036224 train.py:394] {'eval/walltime': 1987.821669101715, 'training/sps': 40465.648651857104, 'training/walltime': 1488.7409162521362, 'training/entropy_loss': Array(-0.04473524, dtype=float32), 'training/policy_loss': Array(-0.00028107, dtype=float32), 'training/total_loss': Array(119.75897, dtype=float32), 'training/v_loss': Array(119.80399, dtype=float32), 'eval/episode_goal_distance': (Array(0.4539045, dtype=float32), Array(0.19355801, dtype=float32)), 'eval/episode_reward': (Array(-9752.7705, dtype=float32), Array(4097.513, dtype=float32)), 'eval/avg_episode_length': (Array(953.4219, dtype=float32), Array(210.03279, dtype=float32)), 'eval/epoch_eval_time': 4.094212770462036, 'eval/sps': 31263.641431501634}
I0726 22:26:35.986865 140267183036224 train.py:379] starting iteration 486 3488.4078907966614
I0726 22:26:43.108830 140267183036224 train.py:394] {'eval/walltime': 1991.9177145957947, 'training/sps': 40658.819537839656, 'training/walltime': 1491.7631387710571, 'training/entropy_loss': Array(-0.04432512, dtype=float32), 'training/policy_loss': Array(-0.00057092, dtype=float32), 'training/total_loss': Array(116.05173, dtype=float32), 'training/v_loss': Array(116.09663, dtype=float32), 'eval/episode_goal_distance': (Array(0.42767528, dtype=float32), Array(0.17816709, dtype=float32)), 'eval/episode_reward': (Array(-9672.443, dtype=float32), Array(4107.136, dtype=float32)), 'eval/avg_episode_length': (Array(945.60156, dtype=float32), Array(226.16798, dtype=float32)), 'eval/epoch_eval_time': 4.09604549407959, 'eval/sps': 31249.652911573066}
I0726 22:26:43.111345 140267183036224 train.py:379] starting iteration 487 3495.5323708057404
I0726 22:26:50.237055 140267183036224 train.py:394] {'eval/walltime': 1996.0213975906372, 'training/sps': 40709.709404393616, 'training/walltime': 1494.7815833091736, 'training/entropy_loss': Array(-0.04413112, dtype=float32), 'training/policy_loss': Array(0.00012655, dtype=float32), 'training/total_loss': Array(128.67393, dtype=float32), 'training/v_loss': Array(128.71794, dtype=float32), 'eval/episode_goal_distance': (Array(0.45858726, dtype=float32), Array(0.17499924, dtype=float32)), 'eval/episode_reward': (Array(-10260.754, dtype=float32), Array(4431.144, dtype=float32)), 'eval/avg_episode_length': (Array(937.875, dtype=float32), Array(240.60995, dtype=float32)), 'eval/epoch_eval_time': 4.103682994842529, 'eval/sps': 31191.49314429726}
I0726 22:26:50.239395 140267183036224 train.py:379] starting iteration 488 3502.66041970253
I0726 22:26:57.379128 140267183036224 train.py:394] {'eval/walltime': 2000.128203868866, 'training/sps': 40565.11360443377, 'training/walltime': 1497.8107872009277, 'training/entropy_loss': Array(-0.04488734, dtype=float32), 'training/policy_loss': Array(-0.00028274, dtype=float32), 'training/total_loss': Array(122.79616, dtype=float32), 'training/v_loss': Array(122.841324, dtype=float32), 'eval/episode_goal_distance': (Array(0.42564347, dtype=float32), Array(0.1749847, dtype=float32)), 'eval/episode_reward': (Array(-9496.141, dtype=float32), Array(3783.7156, dtype=float32)), 'eval/avg_episode_length': (Array(945.6406, dtype=float32), Array(226.00542, dtype=float32)), 'eval/epoch_eval_time': 4.10680627822876, 'eval/sps': 31167.771579234464}
I0726 22:26:57.381481 140267183036224 train.py:379] starting iteration 489 3509.802507162094
I0726 22:27:04.508148 140267183036224 train.py:394] {'eval/walltime': 2004.2238554954529, 'training/sps': 40588.7856536687, 'training/walltime': 1500.8382244110107, 'training/entropy_loss': Array(-0.0449516, dtype=float32), 'training/policy_loss': Array(-0.00035541, dtype=float32), 'training/total_loss': Array(129.2851, dtype=float32), 'training/v_loss': Array(129.3304, dtype=float32), 'eval/episode_goal_distance': (Array(0.41683495, dtype=float32), Array(0.17010058, dtype=float32)), 'eval/episode_reward': (Array(-9129.895, dtype=float32), Array(3763.7368, dtype=float32)), 'eval/avg_episode_length': (Array(930.08594, dtype=float32), Array(254.22441, dtype=float32)), 'eval/epoch_eval_time': 4.095651626586914, 'eval/sps': 31252.65810428999}
I0726 22:27:04.510559 140267183036224 train.py:379] starting iteration 490 3516.9315843582153
I0726 22:27:11.642569 140267183036224 train.py:394] {'eval/walltime': 2008.321501493454, 'training/sps': 40544.55955127828, 'training/walltime': 1503.8689639568329, 'training/entropy_loss': Array(-0.0444873, dtype=float32), 'training/policy_loss': Array(-0.00019472, dtype=float32), 'training/total_loss': Array(123.02903, dtype=float32), 'training/v_loss': Array(123.0737, dtype=float32), 'eval/episode_goal_distance': (Array(0.41581705, dtype=float32), Array(0.17895047, dtype=float32)), 'eval/episode_reward': (Array(-9490.883, dtype=float32), Array(4025.3354, dtype=float32)), 'eval/avg_episode_length': (Array(953.4297, dtype=float32), Array(209.99734, dtype=float32)), 'eval/epoch_eval_time': 4.097645998001099, 'eval/sps': 31237.44707630688}
I0726 22:27:11.645033 140267183036224 train.py:379] starting iteration 491 3524.066059589386
I0726 22:27:18.769810 140267183036224 train.py:394] {'eval/walltime': 2012.4151182174683, 'training/sps': 40585.75881969352, 'training/walltime': 1506.8966269493103, 'training/entropy_loss': Array(-0.04484929, dtype=float32), 'training/policy_loss': Array(-0.0005635, dtype=float32), 'training/total_loss': Array(129.02249, dtype=float32), 'training/v_loss': Array(129.0679, dtype=float32), 'eval/episode_goal_distance': (Array(0.4186433, dtype=float32), Array(0.15416417, dtype=float32)), 'eval/episode_reward': (Array(-9113.377, dtype=float32), Array(4178.512, dtype=float32)), 'eval/avg_episode_length': (Array(906.8125, dtype=float32), Array(289.732, dtype=float32)), 'eval/epoch_eval_time': 4.093616724014282, 'eval/sps': 31268.19353876409}
I0726 22:27:18.772074 140267183036224 train.py:379] starting iteration 492 3531.1930997371674
I0726 22:27:25.898621 140267183036224 train.py:394] {'eval/walltime': 2016.5133981704712, 'training/sps': 40627.557511607, 'training/walltime': 1509.9211750030518, 'training/entropy_loss': Array(-0.04436234, dtype=float32), 'training/policy_loss': Array(3.0369029e-05, dtype=float32), 'training/total_loss': Array(179.23601, dtype=float32), 'training/v_loss': Array(179.28035, dtype=float32), 'eval/episode_goal_distance': (Array(0.42439675, dtype=float32), Array(0.18047595, dtype=float32)), 'eval/episode_reward': (Array(-9059.005, dtype=float32), Array(4273.376, dtype=float32)), 'eval/avg_episode_length': (Array(906.7969, dtype=float32), Array(289.78055, dtype=float32)), 'eval/epoch_eval_time': 4.09827995300293, 'eval/sps': 31232.615016016818}
I0726 22:27:25.901216 140267183036224 train.py:379] starting iteration 493 3538.3222427368164
I0726 22:27:33.026353 140267183036224 train.py:394] {'eval/walltime': 2020.6143715381622, 'training/sps': 40681.24279064716, 'training/walltime': 1512.9417316913605, 'training/entropy_loss': Array(-0.04415196, dtype=float32), 'training/policy_loss': Array(-0.00042351, dtype=float32), 'training/total_loss': Array(154.81519, dtype=float32), 'training/v_loss': Array(154.85976, dtype=float32), 'eval/episode_goal_distance': (Array(0.42129272, dtype=float32), Array(0.20094754, dtype=float32)), 'eval/episode_reward': (Array(-9372.4375, dtype=float32), Array(4124.089, dtype=float32)), 'eval/avg_episode_length': (Array(945.6328, dtype=float32), Array(226.03798, dtype=float32)), 'eval/epoch_eval_time': 4.10097336769104, 'eval/sps': 31212.102231248453}
I0726 22:27:33.028745 140267183036224 train.py:379] starting iteration 494 3545.449771165848
I0726 22:27:40.162747 140267183036224 train.py:394] {'eval/walltime': 2024.7169148921967, 'training/sps': 40583.96914082989, 'training/walltime': 1515.9695281982422, 'training/entropy_loss': Array(-0.04351621, dtype=float32), 'training/policy_loss': Array(-0.00020029, dtype=float32), 'training/total_loss': Array(138.2938, dtype=float32), 'training/v_loss': Array(138.3375, dtype=float32), 'eval/episode_goal_distance': (Array(0.41779447, dtype=float32), Array(0.19059649, dtype=float32)), 'eval/episode_reward': (Array(-9893.6875, dtype=float32), Array(4140.5225, dtype=float32)), 'eval/avg_episode_length': (Array(969., dtype=float32), Array(172.6011, dtype=float32)), 'eval/epoch_eval_time': 4.102543354034424, 'eval/sps': 31200.15779336624}
I0726 22:27:40.165051 140267183036224 train.py:379] starting iteration 495 3552.586076974869
I0726 22:27:47.288920 140267183036224 train.py:394] {'eval/walltime': 2028.8096075057983, 'training/sps': 40587.88746469697, 'training/walltime': 1518.997032403946, 'training/entropy_loss': Array(-0.0440575, dtype=float32), 'training/policy_loss': Array(-0.00021915, dtype=float32), 'training/total_loss': Array(123.86427, dtype=float32), 'training/v_loss': Array(123.908554, dtype=float32), 'eval/episode_goal_distance': (Array(0.4214709, dtype=float32), Array(0.20728786, dtype=float32)), 'eval/episode_reward': (Array(-9057.629, dtype=float32), Array(4708.4136, dtype=float32)), 'eval/avg_episode_length': (Array(899.0078, dtype=float32), Array(300.37613, dtype=float32)), 'eval/epoch_eval_time': 4.092692613601685, 'eval/sps': 31275.25374727725}
I0726 22:27:47.291259 140267183036224 train.py:379] starting iteration 496 3559.7122843265533
I0726 22:27:54.425071 140267183036224 train.py:394] {'eval/walltime': 2032.9059662818909, 'training/sps': 40503.953135767784, 'training/walltime': 1522.0308103561401, 'training/entropy_loss': Array(-0.04589199, dtype=float32), 'training/policy_loss': Array(-0.00032519, dtype=float32), 'training/total_loss': Array(133.31897, dtype=float32), 'training/v_loss': Array(133.3652, dtype=float32), 'eval/episode_goal_distance': (Array(0.4658912, dtype=float32), Array(0.20153627, dtype=float32)), 'eval/episode_reward': (Array(-10111.994, dtype=float32), Array(3651.5842, dtype=float32)), 'eval/avg_episode_length': (Array(961.2031, dtype=float32), Array(192.4264, dtype=float32)), 'eval/epoch_eval_time': 4.096358776092529, 'eval/sps': 31247.262995380926}
I0726 22:27:54.427491 140267183036224 train.py:379] starting iteration 497 3566.848517179489
I0726 22:28:01.541699 140267183036224 train.py:394] {'eval/walltime': 2036.9928839206696, 'training/sps': 40639.352843735534, 'training/walltime': 1525.0544805526733, 'training/entropy_loss': Array(-0.04627963, dtype=float32), 'training/policy_loss': Array(-0.00023547, dtype=float32), 'training/total_loss': Array(129.72455, dtype=float32), 'training/v_loss': Array(129.77106, dtype=float32), 'eval/episode_goal_distance': (Array(0.4200778, dtype=float32), Array(0.18599421, dtype=float32)), 'eval/episode_reward': (Array(-9414.243, dtype=float32), Array(4520.2466, dtype=float32)), 'eval/avg_episode_length': (Array(914.5469, dtype=float32), Array(278.69235, dtype=float32)), 'eval/epoch_eval_time': 4.0869176387786865, 'eval/sps': 31319.446906752655}
I0726 22:28:01.544165 140267183036224 train.py:379] starting iteration 498 3573.9651906490326
I0726 22:28:08.679280 140267183036224 train.py:394] {'eval/walltime': 2041.1036388874054, 'training/sps': 40678.0929926913, 'training/walltime': 1528.0752711296082, 'training/entropy_loss': Array(-0.04589874, dtype=float32), 'training/policy_loss': Array(-0.00013613, dtype=float32), 'training/total_loss': Array(127.27644, dtype=float32), 'training/v_loss': Array(127.32248, dtype=float32), 'eval/episode_goal_distance': (Array(0.42507654, dtype=float32), Array(0.17338789, dtype=float32)), 'eval/episode_reward': (Array(-9545.631, dtype=float32), Array(4037.4316, dtype=float32)), 'eval/avg_episode_length': (Array(937.7969, dtype=float32), Array(240.91214, dtype=float32)), 'eval/epoch_eval_time': 4.11075496673584, 'eval/sps': 31137.83259663343}
I0726 22:28:08.681602 140267183036224 train.py:379] starting iteration 499 3581.1026282310486
I0726 22:28:15.811557 140267183036224 train.py:394] {'eval/walltime': 2045.2020468711853, 'training/sps': 40582.45442884012, 'training/walltime': 1531.1031806468964, 'training/entropy_loss': Array(-0.04458705, dtype=float32), 'training/policy_loss': Array(-0.00038563, dtype=float32), 'training/total_loss': Array(123.32172, dtype=float32), 'training/v_loss': Array(123.366684, dtype=float32), 'eval/episode_goal_distance': (Array(0.43931854, dtype=float32), Array(0.15763594, dtype=float32)), 'eval/episode_reward': (Array(-9437.618, dtype=float32), Array(4039.0645, dtype=float32)), 'eval/avg_episode_length': (Array(899.1172, dtype=float32), Array(300.05093, dtype=float32)), 'eval/epoch_eval_time': 4.098407983779907, 'eval/sps': 31231.639335707936}
I0726 22:28:15.813887 140267183036224 train.py:379] starting iteration 500 3588.2349138259888
I0726 22:28:22.939876 140267183036224 train.py:394] {'eval/walltime': 2049.2965784072876, 'training/sps': 40583.14785890374, 'training/walltime': 1534.131038427353, 'training/entropy_loss': Array(-0.04472937, dtype=float32), 'training/policy_loss': Array(-8.248867e-05, dtype=float32), 'training/total_loss': Array(254.46393, dtype=float32), 'training/v_loss': Array(254.50873, dtype=float32), 'eval/episode_goal_distance': (Array(0.43010408, dtype=float32), Array(0.16681258, dtype=float32)), 'eval/episode_reward': (Array(-9315.992, dtype=float32), Array(4184.3076, dtype=float32)), 'eval/avg_episode_length': (Array(922.2969, dtype=float32), Array(266.91916, dtype=float32)), 'eval/epoch_eval_time': 4.094531536102295, 'eval/sps': 31261.207508453328}
I0726 22:28:22.942276 140267183036224 train.py:379] starting iteration 501 3595.3633024692535
I0726 22:28:30.066860 140267183036224 train.py:394] {'eval/walltime': 2053.3866350650787, 'training/sps': 40544.639289203165, 'training/walltime': 1537.1617720127106, 'training/entropy_loss': Array(-0.04498146, dtype=float32), 'training/policy_loss': Array(0.00013577, dtype=float32), 'training/total_loss': Array(130.9271, dtype=float32), 'training/v_loss': Array(130.97194, dtype=float32), 'eval/episode_goal_distance': (Array(0.44949192, dtype=float32), Array(0.19308527, dtype=float32)), 'eval/episode_reward': (Array(-9760.139, dtype=float32), Array(4844.718, dtype=float32)), 'eval/avg_episode_length': (Array(899.03906, dtype=float32), Array(300.2835, dtype=float32)), 'eval/epoch_eval_time': 4.090056657791138, 'eval/sps': 31295.409992957713}
I0726 22:28:30.069468 140267183036224 train.py:379] starting iteration 502 3602.4904935359955
I0726 22:28:37.191278 140267183036224 train.py:394] {'eval/walltime': 2057.477581501007, 'training/sps': 40592.11025764294, 'training/walltime': 1540.1889612674713, 'training/entropy_loss': Array(-0.04607895, dtype=float32), 'training/policy_loss': Array(-2.8405047e-05, dtype=float32), 'training/total_loss': Array(131.70836, dtype=float32), 'training/v_loss': Array(131.75447, dtype=float32), 'eval/episode_goal_distance': (Array(0.430912, dtype=float32), Array(0.17112991, dtype=float32)), 'eval/episode_reward': (Array(-9722.621, dtype=float32), Array(3838.2395, dtype=float32)), 'eval/avg_episode_length': (Array(953.3594, dtype=float32), Array(210.31438, dtype=float32)), 'eval/epoch_eval_time': 4.090946435928345, 'eval/sps': 31288.603262035474}
I0726 22:28:37.193738 140267183036224 train.py:379] starting iteration 503 3609.614764690399
I0726 22:28:44.320779 140267183036224 train.py:394] {'eval/walltime': 2061.580778360367, 'training/sps': 40694.030006114415, 'training/walltime': 1543.2085688114166, 'training/entropy_loss': Array(-0.04485859, dtype=float32), 'training/policy_loss': Array(-0.0011852, dtype=float32), 'training/total_loss': Array(130.68338, dtype=float32), 'training/v_loss': Array(130.72943, dtype=float32), 'eval/episode_goal_distance': (Array(0.4289437, dtype=float32), Array(0.16191252, dtype=float32)), 'eval/episode_reward': (Array(-9241.805, dtype=float32), Array(4016.7317, dtype=float32)), 'eval/avg_episode_length': (Array(914.5156, dtype=float32), Array(278.79422, dtype=float32)), 'eval/epoch_eval_time': 4.103196859359741, 'eval/sps': 31195.18862664878}
I0726 22:28:44.323251 140267183036224 train.py:379] starting iteration 504 3616.7442767620087
I0726 22:28:51.450281 140267183036224 train.py:394] {'eval/walltime': 2065.6770668029785, 'training/sps': 40598.76429780184, 'training/walltime': 1546.2352619171143, 'training/entropy_loss': Array(-0.04421348, dtype=float32), 'training/policy_loss': Array(-0.00014274, dtype=float32), 'training/total_loss': Array(139.6836, dtype=float32), 'training/v_loss': Array(139.72794, dtype=float32), 'eval/episode_goal_distance': (Array(0.42553413, dtype=float32), Array(0.17174697, dtype=float32)), 'eval/episode_reward': (Array(-9163.826, dtype=float32), Array(4665.53, dtype=float32)), 'eval/avg_episode_length': (Array(891.34375, dtype=float32), Array(310.05905, dtype=float32)), 'eval/epoch_eval_time': 4.096288442611694, 'eval/sps': 31247.79951247532}
I0726 22:28:51.452655 140267183036224 train.py:379] starting iteration 505 3623.873680830002
I0726 22:28:58.576469 140267183036224 train.py:394] {'eval/walltime': 2069.7670447826385, 'training/sps': 40557.95992380333, 'training/walltime': 1549.2650001049042, 'training/entropy_loss': Array(-0.04453441, dtype=float32), 'training/policy_loss': Array(-0.00029976, dtype=float32), 'training/total_loss': Array(125.76397, dtype=float32), 'training/v_loss': Array(125.8088, dtype=float32), 'eval/episode_goal_distance': (Array(0.4075363, dtype=float32), Array(0.1644351, dtype=float32)), 'eval/episode_reward': (Array(-9236.15, dtype=float32), Array(3775.7886, dtype=float32)), 'eval/avg_episode_length': (Array(945.60156, dtype=float32), Array(226.1676, dtype=float32)), 'eval/epoch_eval_time': 4.089977979660034, 'eval/sps': 31296.01201682743}
I0726 22:28:58.579129 140267183036224 train.py:379] starting iteration 506 3631.0001554489136
I0726 22:29:05.702801 140267183036224 train.py:394] {'eval/walltime': 2073.856426715851, 'training/sps': 40551.91270681909, 'training/walltime': 1552.2951900959015, 'training/entropy_loss': Array(-0.04329311, dtype=float32), 'training/policy_loss': Array(-5.510284e-05, dtype=float32), 'training/total_loss': Array(129.79048, dtype=float32), 'training/v_loss': Array(129.83383, dtype=float32), 'eval/episode_goal_distance': (Array(0.4036013, dtype=float32), Array(0.14838476, dtype=float32)), 'eval/episode_reward': (Array(-9018.757, dtype=float32), Array(3557.1968, dtype=float32)), 'eval/avg_episode_length': (Array(945.6094, dtype=float32), Array(226.13551, dtype=float32)), 'eval/epoch_eval_time': 4.08938193321228, 'eval/sps': 31300.573556222906}
I0726 22:29:05.705257 140267183036224 train.py:379] starting iteration 507 3638.1262826919556
I0726 22:29:12.819452 140267183036224 train.py:394] {'eval/walltime': 2077.9385261535645, 'training/sps': 40580.26884121034, 'training/walltime': 1555.3232626914978, 'training/entropy_loss': Array(-0.04258779, dtype=float32), 'training/policy_loss': Array(-0.00014422, dtype=float32), 'training/total_loss': Array(128.42848, dtype=float32), 'training/v_loss': Array(128.47122, dtype=float32), 'eval/episode_goal_distance': (Array(0.39993882, dtype=float32), Array(0.14651021, dtype=float32)), 'eval/episode_reward': (Array(-8201.13, dtype=float32), Array(4259.563, dtype=float32)), 'eval/avg_episode_length': (Array(852.5156, dtype=float32), Array(353.25125, dtype=float32)), 'eval/epoch_eval_time': 4.082099437713623, 'eval/sps': 31356.41401026051}
I0726 22:29:12.821761 140267183036224 train.py:379] starting iteration 508 3645.242787361145
I0726 22:29:19.945245 140267183036224 train.py:394] {'eval/walltime': 2082.041480064392, 'training/sps': 40735.00896548492, 'training/walltime': 1558.3398325443268, 'training/entropy_loss': Array(-0.041072, dtype=float32), 'training/policy_loss': Array(-0.00035743, dtype=float32), 'training/total_loss': Array(142.40793, dtype=float32), 'training/v_loss': Array(142.44936, dtype=float32), 'eval/episode_goal_distance': (Array(0.3822505, dtype=float32), Array(0.15543106, dtype=float32)), 'eval/episode_reward': (Array(-8810.517, dtype=float32), Array(3627.5398, dtype=float32)), 'eval/avg_episode_length': (Array(937.90625, dtype=float32), Array(240.48833, dtype=float32)), 'eval/epoch_eval_time': 4.102953910827637, 'eval/sps': 31197.03578980252}
I0726 22:29:19.947743 140267183036224 train.py:379] starting iteration 509 3652.3687691688538
I0726 22:29:27.074329 140267183036224 train.py:394] {'eval/walltime': 2086.1350758075714, 'training/sps': 40597.31563526855, 'training/walltime': 1561.3666336536407, 'training/entropy_loss': Array(-0.04148367, dtype=float32), 'training/policy_loss': Array(0.00017718, dtype=float32), 'training/total_loss': Array(164.84262, dtype=float32), 'training/v_loss': Array(164.88394, dtype=float32), 'eval/episode_goal_distance': (Array(0.41102087, dtype=float32), Array(0.16323392, dtype=float32)), 'eval/episode_reward': (Array(-9268.2295, dtype=float32), Array(3947.925, dtype=float32)), 'eval/avg_episode_length': (Array(930.1094, dtype=float32), Array(254.13945, dtype=float32)), 'eval/epoch_eval_time': 4.093595743179321, 'eval/sps': 31268.35379709181}
I0726 22:29:27.076597 140267183036224 train.py:379] starting iteration 510 3659.4976229667664
I0726 22:29:34.202636 140267183036224 train.py:394] {'eval/walltime': 2090.228275537491, 'training/sps': 40570.1012859104, 'training/walltime': 1564.3954651355743, 'training/entropy_loss': Array(-0.04153817, dtype=float32), 'training/policy_loss': Array(-1.6324464e-05, dtype=float32), 'training/total_loss': Array(144.20341, dtype=float32), 'training/v_loss': Array(144.24496, dtype=float32), 'eval/episode_goal_distance': (Array(0.42498106, dtype=float32), Array(0.1558544, dtype=float32)), 'eval/episode_reward': (Array(-9946.646, dtype=float32), Array(3512.0686, dtype=float32)), 'eval/avg_episode_length': (Array(961.14844, dtype=float32), Array(192.69783, dtype=float32)), 'eval/epoch_eval_time': 4.093199729919434, 'eval/sps': 31271.378981186295}
I0726 22:29:35.295765 140267183036224 train.py:410] total steps: 62791680
