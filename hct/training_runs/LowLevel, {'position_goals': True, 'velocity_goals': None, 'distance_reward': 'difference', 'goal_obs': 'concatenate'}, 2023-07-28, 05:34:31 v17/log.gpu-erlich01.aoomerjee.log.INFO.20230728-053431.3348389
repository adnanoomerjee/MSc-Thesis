I0728 05:34:31.787760 139730147526464 low_level_env.py:188] Initialising environment...
I0728 05:35:11.475574 139730147526464 low_level_env.py:293] Environment initialised.
I0728 05:35:11.481296 139730147526464 train.py:118] JAX is running on GPU.
I0728 05:35:11.481448 139730147526464 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 05:35:19.890919 139730147526464 train.py:367] Running initial eval
I0728 05:35:35.940333 139730147526464 train.py:373] {'eval/walltime': 15.893990516662598, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30957234, 0.14577967], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.094692, 12.579665], dtype=float32), 'eval/episode_reward': Array([-2.8437552,  9.624915 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30489862, 0.14868756], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.893990516662598, 'eval/sps': 8053.358271845584}
I0728 05:35:35.942308 139730147526464 train.py:379] starting iteration 0, 0 steps, 24.461079835891724
I0728 05:36:18.971904 139730147526464 train.py:394] {'eval/walltime': 19.71424651145935, 'training/sps': 50150.04325778242, 'training/walltime': 39.203954219818115, 'training/entropy_loss': Array(-0.04443291, dtype=float32), 'training/policy_loss': Array(0.00468163, dtype=float32), 'training/total_loss': Array(1.1371077, dtype=float32), 'training/v_loss': Array(1.1768591, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2961857 , 0.11336814], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.997583,  9.700535], dtype=float32), 'eval/episode_reward': Array([-0.8260851,  6.8124785], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29180163, 0.11625519], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.820255994796753, 'eval/sps': 33505.60804677434}
I0728 05:36:18.992547 139730147526464 train.py:379] starting iteration 1, 1966080 steps, 67.51131963729858
I0728 05:36:42.941406 139730147526464 train.py:394] {'eval/walltime': 23.695759296417236, 'training/sps': 98488.19173937617, 'training/walltime': 59.166550397872925, 'training/entropy_loss': Array(-0.04102638, dtype=float32), 'training/policy_loss': Array(0.0025628, dtype=float32), 'training/total_loss': Array(1.0016837, dtype=float32), 'training/v_loss': Array(1.0401474, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2838316 , 0.10554002], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.89631 ,  9.049551], dtype=float32), 'eval/episode_reward': Array([-0.1475252,  6.021959 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27975512, 0.10743057], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9815127849578857, 'eval/sps': 32148.584448499747}
I0728 05:36:42.943539 139730147526464 train.py:379] starting iteration 2, 3932160 steps, 91.46231174468994
I0728 05:37:07.079111 139730147526464 train.py:394] {'eval/walltime': 27.716240882873535, 'training/sps': 97765.59744554607, 'training/walltime': 79.27669191360474, 'training/entropy_loss': Array(-0.03640106, dtype=float32), 'training/policy_loss': Array(0.00607578, dtype=float32), 'training/total_loss': Array(1.2135656, dtype=float32), 'training/v_loss': Array(1.243891, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.24446827, 0.1022234 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.469341,  8.769581], dtype=float32), 'eval/episode_reward': Array([2.5508196, 5.81696  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.23860706, 0.10554577], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020481586456299, 'eval/sps': 31836.982025036647}
I0728 05:37:07.081160 139730147526464 train.py:379] starting iteration 3, 5898240 steps, 115.59993243217468
I0728 05:37:31.648447 139730147526464 train.py:394] {'eval/walltime': 31.733968019485474, 'training/sps': 95703.64724405018, 'training/walltime': 99.82010960578918, 'training/entropy_loss': Array(-0.03180932, dtype=float32), 'training/policy_loss': Array(0.01024457, dtype=float32), 'training/total_loss': Array(1.2341845, dtype=float32), 'training/v_loss': Array(1.2557492, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2464799 , 0.09724552], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.600443 ,  8.3229885], dtype=float32), 'eval/episode_reward': Array([2.9111307, 5.704409 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24076217, 0.10000196], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0177271366119385, 'eval/sps': 31858.80863675068}
I0728 05:37:31.650464 139730147526464 train.py:379] starting iteration 4, 7864320 steps, 140.16923713684082
I0728 05:37:56.624534 139730147526464 train.py:394] {'eval/walltime': 35.756800174713135, 'training/sps': 93863.25814297624, 'training/walltime': 120.76632475852966, 'training/entropy_loss': Array(-0.02634051, dtype=float32), 'training/policy_loss': Array(0.0166191, dtype=float32), 'training/total_loss': Array(1.2665894, dtype=float32), 'training/v_loss': Array(1.2763109, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20613621, 0.09545258], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.174076,  8.116439], dtype=float32), 'eval/episode_reward': Array([5.3712425, 5.682407 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1998168 , 0.09820197], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022832155227661, 'eval/sps': 31818.379455296013}
I0728 05:37:56.626561 139730147526464 train.py:379] starting iteration 5, 9830400 steps, 165.14533352851868
I0728 05:38:21.818541 139730147526464 train.py:394] {'eval/walltime': 39.78904843330383, 'training/sps': 92944.59040894781, 'training/walltime': 141.91957306861877, 'training/entropy_loss': Array(-0.01967537, dtype=float32), 'training/policy_loss': Array(0.02294706, dtype=float32), 'training/total_loss': Array(1.2665365, dtype=float32), 'training/v_loss': Array(1.2632647, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1889411 , 0.09251604], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.639955,  7.8293  ], dtype=float32), 'eval/episode_reward': Array([7.3689137, 6.9207387], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18029034, 0.09630927], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.032248258590698, 'eval/sps': 31744.07719745336}
I0728 05:38:21.822737 139730147526464 train.py:379] starting iteration 6, 11796480 steps, 190.34149289131165
I0728 05:38:47.156174 139730147526464 train.py:394] {'eval/walltime': 43.836138010025024, 'training/sps': 92386.33417445974, 'training/walltime': 163.2006425857544, 'training/entropy_loss': Array(-0.01305976, dtype=float32), 'training/policy_loss': Array(0.03122218, dtype=float32), 'training/total_loss': Array(1.3952508, dtype=float32), 'training/v_loss': Array(1.3770884, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.17163745, 0.11129781], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.999999,  9.510086], dtype=float32), 'eval/episode_reward': Array([10.833601,  8.708444], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.16117735, 0.11615503], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.047089576721191, 'eval/sps': 31627.66664129561}
I0728 05:38:47.158200 139730147526464 train.py:379] starting iteration 7, 13762560 steps, 215.6769733428955
I0728 05:39:12.594726 139730147526464 train.py:394] {'eval/walltime': 47.85388398170471, 'training/sps': 91813.97952767814, 'training/walltime': 184.61437511444092, 'training/entropy_loss': Array(-0.00392935, dtype=float32), 'training/policy_loss': Array(0.03820554, dtype=float32), 'training/total_loss': Array(1.4361933, dtype=float32), 'training/v_loss': Array(1.4019172, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.14848524, 0.10593752], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([11.936718,  9.055291], dtype=float32), 'eval/episode_reward': Array([10.560316,  8.671144], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.13534215, 0.1119495 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0177459716796875, 'eval/sps': 31858.659283649886}
I0728 05:39:12.596795 139730147526464 train.py:379] starting iteration 8, 15728640 steps, 241.11556839942932
I0728 05:39:38.104126 139730147526464 train.py:394] {'eval/walltime': 51.901989221572876, 'training/sps': 91639.07947897079, 'training/walltime': 206.06897735595703, 'training/entropy_loss': Array(0.01011499, dtype=float32), 'training/policy_loss': Array(0.04452198, dtype=float32), 'training/total_loss': Array(1.3158259, dtype=float32), 'training/v_loss': Array(1.261189, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.15464337, 0.1132035 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([12.5163  ,  9.621163], dtype=float32), 'eval/episode_reward': Array([11.701427,  9.191621], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.14155613, 0.11964063], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.048105239868164, 'eval/sps': 31619.731310189112}
I0728 05:39:38.106153 139730147526464 train.py:379] starting iteration 9, 17694720 steps, 266.6249258518219
I0728 05:40:03.340411 139730147526464 train.py:394] {'eval/walltime': 55.933616161346436, 'training/sps': 92747.93977829875, 'training/walltime': 227.267076253891, 'training/entropy_loss': Array(0.02510507, dtype=float32), 'training/policy_loss': Array(0.04616141, dtype=float32), 'training/total_loss': Array(1.28192, dtype=float32), 'training/v_loss': Array(1.2106535, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.14789483, 0.10487752], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([11.926947 ,  8.9296665], dtype=float32), 'eval/episode_reward': Array([10.797619,  9.179883], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.13280463, 0.11276653], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.03162693977356, 'eval/sps': 31748.969314901256}
I0728 05:40:03.342430 139730147526464 train.py:379] starting iteration 10, 19660800 steps, 291.861202955246
I0728 05:40:28.819602 139730147526464 train.py:394] {'eval/walltime': 59.96388626098633, 'training/sps': 91692.24719401341, 'training/walltime': 248.70923805236816, 'training/entropy_loss': Array(0.03923232, dtype=float32), 'training/policy_loss': Array(0.04474581, dtype=float32), 'training/total_loss': Array(1.2502279, dtype=float32), 'training/v_loss': Array(1.1662498, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16273588, 0.11081117], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.139292,  9.503268], dtype=float32), 'eval/episode_reward': Array([8.780218, 8.611994], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.14938417, 0.11780266], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.030270099639893, 'eval/sps': 31759.65799697566}
I0728 05:40:28.821561 139730147526464 train.py:379] starting iteration 11, 21626880 steps, 317.34033393859863
I0728 05:40:54.382499 139730147526464 train.py:394] {'eval/walltime': 63.98293852806091, 'training/sps': 91288.7776519598, 'training/walltime': 270.2461678981781, 'training/entropy_loss': Array(0.05349433, dtype=float32), 'training/policy_loss': Array(0.04326358, dtype=float32), 'training/total_loss': Array(1.1484146, dtype=float32), 'training/v_loss': Array(1.0516567, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18543363, 0.1120188 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.147525,  9.660899], dtype=float32), 'eval/episode_reward': Array([8.297595, 9.136574], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.17344932, 0.11995228], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019052267074585, 'eval/sps': 31848.304399676173}
I0728 05:40:54.384510 139730147526464 train.py:379] starting iteration 12, 23592960 steps, 342.90328311920166
I0728 05:41:20.005045 139730147526464 train.py:394] {'eval/walltime': 68.00740694999695, 'training/sps': 91059.16413181541, 'training/walltime': 291.83740496635437, 'training/entropy_loss': Array(0.06858403, dtype=float32), 'training/policy_loss': Array(0.04389392, dtype=float32), 'training/total_loss': Array(1.0065554, dtype=float32), 'training/v_loss': Array(0.89407754, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2169578 , 0.11693842], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.886139, 10.177211], dtype=float32), 'eval/episode_reward': Array([6.657216, 9.116322], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2073459 , 0.12345915], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024468421936035, 'eval/sps': 31805.442751722112}
I0728 05:41:20.006990 139730147526464 train.py:379] starting iteration 13, 25559040 steps, 368.525762796402
I0728 05:41:45.682653 139730147526464 train.py:394] {'eval/walltime': 72.05499720573425, 'training/sps': 90923.5612347735, 'training/walltime': 313.4608430862427, 'training/entropy_loss': Array(0.07851383, dtype=float32), 'training/policy_loss': Array(0.03938916, dtype=float32), 'training/total_loss': Array(0.8944085, dtype=float32), 'training/v_loss': Array(0.77650553, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23755231, 0.110071  ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.740913,  9.55653 ], dtype=float32), 'eval/episode_reward': Array([4.516779 , 6.5998826], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2294001 , 0.11561166], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.047590255737305, 'eval/sps': 31623.75436065073}
I0728 05:41:45.684635 139730147526464 train.py:379] starting iteration 14, 27525120 steps, 394.20340752601624
I0728 05:42:11.358838 139730147526464 train.py:394] {'eval/walltime': 76.07611155509949, 'training/sps': 90817.6215139315, 'training/walltime': 335.1095051765442, 'training/entropy_loss': Array(0.08552234, dtype=float32), 'training/policy_loss': Array(0.04330659, dtype=float32), 'training/total_loss': Array(0.9035816, dtype=float32), 'training/v_loss': Array(0.7747528, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.22924645, 0.11536405], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.967133, 10.034936], dtype=float32), 'eval/episode_reward': Array([4.729371, 7.503283], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22010863, 0.12156085], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021114349365234, 'eval/sps': 31831.97215473513}
I0728 05:42:11.360827 139730147526464 train.py:379] starting iteration 15, 29491200 steps, 419.87960052490234
I0728 05:42:36.950806 139730147526464 train.py:394] {'eval/walltime': 80.11539459228516, 'training/sps': 91251.29218950646, 'training/walltime': 356.6552822589874, 'training/entropy_loss': Array(0.09048985, dtype=float32), 'training/policy_loss': Array(0.04644866, dtype=float32), 'training/total_loss': Array(0.9331862, dtype=float32), 'training/v_loss': Array(0.7962477, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.21509306, 0.1172057 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.753738 , 10.2495365], dtype=float32), 'eval/episode_reward': Array([5.7237806, 7.9055076], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20433688, 0.12461074], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.039283037185669, 'eval/sps': 31688.79200136041}
I0728 05:42:36.952833 139730147526464 train.py:379] starting iteration 16, 31457280 steps, 445.4716055393219
I0728 05:43:02.577482 139730147526464 train.py:394] {'eval/walltime': 84.13748693466187, 'training/sps': 91030.37242627694, 'training/walltime': 378.2533483505249, 'training/entropy_loss': Array(0.09764472, dtype=float32), 'training/policy_loss': Array(0.05297568, dtype=float32), 'training/total_loss': Array(1.0065391, dtype=float32), 'training/v_loss': Array(0.85591865, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20261014, 0.10591692], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.712284,  9.214031], dtype=float32), 'eval/episode_reward': Array([6.641894, 8.346622], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.19295362, 0.1117602 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022092342376709, 'eval/sps': 31824.23204246053}
I0728 05:43:02.579489 139730147526464 train.py:379] starting iteration 17, 33423360 steps, 471.0982623100281
I0728 05:43:28.135751 139730147526464 train.py:394] {'eval/walltime': 88.19891405105591, 'training/sps': 91487.92386953907, 'training/walltime': 399.7433977127075, 'training/entropy_loss': Array(0.1069005, dtype=float32), 'training/policy_loss': Array(0.05675594, dtype=float32), 'training/total_loss': Array(1.0930266, dtype=float32), 'training/v_loss': Array(0.92937016, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19768408, 0.11698866], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.316442, 10.164581], dtype=float32), 'eval/episode_reward': Array([6.4022903, 8.500346 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18728596, 0.12272631], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.061427116394043, 'eval/sps': 31516.01551172126}
I0728 05:43:28.137740 139730147526464 train.py:379] starting iteration 18, 35389440 steps, 496.6565134525299
I0728 05:43:53.746196 139730147526464 train.py:394] {'eval/walltime': 92.25607299804688, 'training/sps': 91246.99284594627, 'training/walltime': 421.29018998146057, 'training/entropy_loss': Array(0.11153398, dtype=float32), 'training/policy_loss': Array(0.05802798, dtype=float32), 'training/total_loss': Array(1.1449466, dtype=float32), 'training/v_loss': Array(0.9753847, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20955305, 0.12133113], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.364193, 10.571492], dtype=float32), 'eval/episode_reward': Array([6.2967367, 9.329973 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.19924684, 0.12789327], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.057158946990967, 'eval/sps': 31549.170656706094}
I0728 05:43:53.748202 139730147526464 train.py:379] starting iteration 19, 37355520 steps, 522.2669756412506
I0728 05:44:19.396090 139730147526464 train.py:394] {'eval/walltime': 96.29185891151428, 'training/sps': 90991.9251148762, 'training/walltime': 442.8973820209503, 'training/entropy_loss': Array(0.11563092, dtype=float32), 'training/policy_loss': Array(0.06210502, dtype=float32), 'training/total_loss': Array(1.2496489, dtype=float32), 'training/v_loss': Array(1.071913, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.19618294, 0.09805974], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.20467 ,  8.526275], dtype=float32), 'eval/episode_reward': Array([6.593804 , 7.7385273], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.18660405, 0.10299148], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.035785913467407, 'eval/sps': 31716.251244364656}
I0728 05:44:19.398098 139730147526464 train.py:379] starting iteration 20, 39321600 steps, 547.9168720245361
I0728 05:44:45.145251 139730147526464 train.py:394] {'eval/walltime': 100.34821724891663, 'training/sps': 90661.57959743595, 'training/walltime': 464.583304643631, 'training/entropy_loss': Array(0.11416899, dtype=float32), 'training/policy_loss': Array(0.06255129, dtype=float32), 'training/total_loss': Array(1.4445746, dtype=float32), 'training/v_loss': Array(1.2678542, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.18699217, 0.10027025], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([15.440961,  8.697215], dtype=float32), 'eval/episode_reward': Array([7.9528604, 8.174399 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.17765316, 0.10444362], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.056358337402344, 'eval/sps': 31555.3975643015}
I0728 05:44:45.147259 139730147526464 train.py:379] starting iteration 21, 41287680 steps, 573.6660325527191
I0728 05:45:10.854904 139730147526464 train.py:394] {'eval/walltime': 104.38402676582336, 'training/sps': 90740.13155394583, 'training/walltime': 486.2504541873932, 'training/entropy_loss': Array(0.12543647, dtype=float32), 'training/policy_loss': Array(0.06251942, dtype=float32), 'training/total_loss': Array(1.4904442, dtype=float32), 'training/v_loss': Array(1.3024883, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2042127 , 0.11064475], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.91883 ,  9.618935], dtype=float32), 'eval/episode_reward': Array([6.846194, 8.342196], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.19449948, 0.11656811], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.035809516906738, 'eval/sps': 31716.065751811322}
I0728 05:45:10.857041 139730147526464 train.py:379] starting iteration 22, 43253760 steps, 599.3758144378662
I0728 05:45:36.565342 139730147526464 train.py:394] {'eval/walltime': 108.44141983985901, 'training/sps': 90828.08362551939, 'training/walltime': 507.8966226577759, 'training/entropy_loss': Array(0.12936245, dtype=float32), 'training/policy_loss': Array(0.06107446, dtype=float32), 'training/total_loss': Array(1.525038, dtype=float32), 'training/v_loss': Array(1.3346012, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20138744, 0.10642767], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([16.65244 ,  9.238134], dtype=float32), 'eval/episode_reward': Array([4.9249125, 8.663186 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.19219457, 0.11107852], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0573930740356445, 'eval/sps': 31547.35014931302}
I0728 05:45:36.567358 139730147526464 train.py:379] starting iteration 23, 45219840 steps, 625.0861308574677
I0728 05:46:02.273672 139730147526464 train.py:394] {'eval/walltime': 112.48476052284241, 'training/sps': 90777.79072823258, 'training/walltime': 529.5547835826874, 'training/entropy_loss': Array(0.11862893, dtype=float32), 'training/policy_loss': Array(0.05020978, dtype=float32), 'training/total_loss': Array(1.4104923, dtype=float32), 'training/v_loss': Array(1.2416537, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.24557564, 0.11887567], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.499859 , 10.2881365], dtype=float32), 'eval/episode_reward': Array([1.9959028, 7.6211853], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.23847175, 0.12263856], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.043340682983398, 'eval/sps': 31656.991096173122}
I0728 05:46:02.275679 139730147526464 train.py:379] starting iteration 24, 47185920 steps, 650.7944519519806
I0728 05:46:27.953745 139730147526464 train.py:394] {'eval/walltime': 116.49688291549683, 'training/sps': 90765.03239406482, 'training/walltime': 551.2159888744354, 'training/entropy_loss': Array(0.11759727, dtype=float32), 'training/policy_loss': Array(0.0347926, dtype=float32), 'training/total_loss': Array(1.2674422, dtype=float32), 'training/v_loss': Array(1.1150523, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.24923825, 0.09144964], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.90176 ,  7.892665], dtype=float32), 'eval/episode_reward': Array([1.8555324, 5.777054 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24327847, 0.0939202 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.012122392654419, 'eval/sps': 31903.313875555836}
I0728 05:46:27.955723 139730147526464 train.py:379] starting iteration 25, 49152000 steps, 676.4744961261749
I0728 05:46:53.697189 139730147526464 train.py:394] {'eval/walltime': 120.54603123664856, 'training/sps': 90654.45241117846, 'training/walltime': 572.9036164283752, 'training/entropy_loss': Array(0.13157728, dtype=float32), 'training/policy_loss': Array(0.03053355, dtype=float32), 'training/total_loss': Array(1.6489489, dtype=float32), 'training/v_loss': Array(1.486838, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2358227 , 0.10541662], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.663433,  9.14028 ], dtype=float32), 'eval/episode_reward': Array([3.1524897, 7.6715107], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22881486, 0.10901917], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.049148321151733, 'eval/sps': 31611.585905944754}
I0728 05:46:53.727675 139730147526464 train.py:379] starting iteration 26, 51118080 steps, 702.2464468479156
I0728 05:47:19.490627 139730147526464 train.py:394] {'eval/walltime': 124.589106798172, 'training/sps': 90544.648570248, 'training/walltime': 594.6175446510315, 'training/entropy_loss': Array(0.14310984, dtype=float32), 'training/policy_loss': Array(0.03054214, dtype=float32), 'training/total_loss': Array(1.6829531, dtype=float32), 'training/v_loss': Array(1.5093012, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.24363661, 0.10782784], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.380327,  9.340268], dtype=float32), 'eval/episode_reward': Array([2.9845667, 7.6067185], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.23662895, 0.11151303], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0430755615234375, 'eval/sps': 31659.066978151997}
I0728 05:47:19.492656 139730147526464 train.py:379] starting iteration 27, 53084160 steps, 728.0114290714264
I0728 05:47:45.244409 139730147526464 train.py:394] {'eval/walltime': 128.6206750869751, 'training/sps': 90537.83895784251, 'training/walltime': 616.3331060409546, 'training/entropy_loss': Array(0.1546162, dtype=float32), 'training/policy_loss': Array(0.03187535, dtype=float32), 'training/total_loss': Array(1.8316869, dtype=float32), 'training/v_loss': Array(1.6451954, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.24816488, 0.09919311], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.786345,  8.644776], dtype=float32), 'eval/episode_reward': Array([2.6863992, 6.6857524], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24146768, 0.10330877], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.031568288803101, 'eval/sps': 31749.431196662397}
I0728 05:47:45.246255 139730147526464 train.py:379] starting iteration 28, 55050240 steps, 753.7650287151337
I0728 05:48:10.964695 139730147526464 train.py:394] {'eval/walltime': 132.64818477630615, 'training/sps': 90660.4433186953, 'training/walltime': 638.0193004608154, 'training/entropy_loss': Array(0.16595513, dtype=float32), 'training/policy_loss': Array(0.35092092, dtype=float32), 'training/total_loss': Array(2.1307163, dtype=float32), 'training/v_loss': Array(1.6138401, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.252204  , 0.11400858], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.08412  ,  9.8614645], dtype=float32), 'eval/episode_reward': Array([3.174953, 8.331529], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2449322 , 0.11791875], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.027509689331055, 'eval/sps': 31781.42571303411}
I0728 05:48:10.966572 139730147526464 train.py:379] starting iteration 29, 57016320 steps, 779.4853446483612
I0728 05:48:36.673105 139730147526464 train.py:394] {'eval/walltime': 136.69077825546265, 'training/sps': 90772.20495613804, 'training/walltime': 659.6787941455841, 'training/entropy_loss': Array(0.17535147, dtype=float32), 'training/policy_loss': Array(0.04027957, dtype=float32), 'training/total_loss': Array(2.0017085, dtype=float32), 'training/v_loss': Array(1.7860775, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23045899, 0.10013738], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.224724,  8.671818], dtype=float32), 'eval/episode_reward': Array([5.3046083, 7.070963 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22296745, 0.10381229], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.042593479156494, 'eval/sps': 31662.842346123754}
I0728 05:48:36.674957 139730147526464 train.py:379] starting iteration 30, 58982400 steps, 805.1937305927277
I0728 05:49:02.425547 139730147526464 train.py:394] {'eval/walltime': 140.70856380462646, 'training/sps': 90485.62887047259, 'training/walltime': 681.4068853855133, 'training/entropy_loss': Array(0.18172108, dtype=float32), 'training/policy_loss': Array(0.05192105, dtype=float32), 'training/total_loss': Array(2.4289894, dtype=float32), 'training/v_loss': Array(2.1953473, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.21371338, 0.10112151], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.80523 ,  8.706922], dtype=float32), 'eval/episode_reward': Array([5.4951696, 8.659371 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20630322, 0.10407181], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.017785549163818, 'eval/sps': 31858.34545764628}
I0728 05:49:02.427452 139730147526464 train.py:379] starting iteration 31, 60948480 steps, 830.9462237358093
I0728 05:49:28.187850 139730147526464 train.py:394] {'eval/walltime': 144.76621794700623, 'training/sps': 90609.71060270464, 'training/walltime': 703.1052219867706, 'training/entropy_loss': Array(0.17271051, dtype=float32), 'training/policy_loss': Array(0.06448986, dtype=float32), 'training/total_loss': Array(2.5322952, dtype=float32), 'training/v_loss': Array(2.295095, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23249382, 0.11926605], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.389675, 10.300103], dtype=float32), 'eval/episode_reward': Array([5.010893 , 7.8004684], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22466269, 0.12360042], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.057654142379761, 'eval/sps': 31545.32040153863}
I0728 05:49:28.189697 139730147526464 train.py:379] starting iteration 32, 62914560 steps, 856.7084696292877
I0728 05:49:53.916390 139730147526464 train.py:394] {'eval/walltime': 148.81177639961243, 'training/sps': 90699.62050063285, 'training/walltime': 724.7820491790771, 'training/entropy_loss': Array(0.16882196, dtype=float32), 'training/policy_loss': Array(0.08296606, dtype=float32), 'training/total_loss': Array(2.9473612, dtype=float32), 'training/v_loss': Array(2.6955733, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.24744253, 0.1088648 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.66964,  9.40688], dtype=float32), 'eval/episode_reward': Array([3.275301, 7.452301], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24025269, 0.11279061], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.045558452606201, 'eval/sps': 31639.636776856045}
I0728 05:49:53.918324 139730147526464 train.py:379] starting iteration 33, 64880640 steps, 882.4370975494385
I0728 05:50:19.690039 139730147526464 train.py:394] {'eval/walltime': 152.85826897621155, 'training/sps': 90518.71290639453, 'training/walltime': 746.502198934555, 'training/entropy_loss': Array(0.16822745, dtype=float32), 'training/policy_loss': Array(0.09855768, dtype=float32), 'training/total_loss': Array(3.4207268, dtype=float32), 'training/v_loss': Array(3.1539416, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27477312, 0.12545687], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.021553, 10.852282], dtype=float32), 'eval/episode_reward': Array([0.0397695, 7.8754396], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26793367, 0.1296774 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.046492576599121, 'eval/sps': 31632.332835657326}
I0728 05:50:19.692043 139730147526464 train.py:379] starting iteration 34, 66846720 steps, 908.2108144760132
I0728 05:50:45.440103 139730147526464 train.py:394] {'eval/walltime': 156.87917947769165, 'training/sps': 90507.7666429387, 'training/walltime': 768.2249755859375, 'training/entropy_loss': Array(0.154924, dtype=float32), 'training/policy_loss': Array(0.11553013, dtype=float32), 'training/total_loss': Array(3.872194, dtype=float32), 'training/v_loss': Array(3.6017401, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2711664 , 0.12163996], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.675335, 10.555617], dtype=float32), 'eval/episode_reward': Array([-1.413693,  7.785485], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26368314, 0.12686385], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0209105014801025, 'eval/sps': 31833.585938528857}
I0728 05:50:45.441983 139730147526464 train.py:379] starting iteration 35, 68812800 steps, 933.9607560634613
I0728 05:51:11.186259 139730147526464 train.py:394] {'eval/walltime': 160.90634274482727, 'training/sps': 90549.40795417996, 'training/walltime': 789.9377624988556, 'training/entropy_loss': Array(0.14177261, dtype=float32), 'training/policy_loss': Array(0.09848209, dtype=float32), 'training/total_loss': Array(4.349616, dtype=float32), 'training/v_loss': Array(4.1093616, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29261303, 0.12147663], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.55692  , 10.5021105], dtype=float32), 'eval/episode_reward': Array([-2.4805503,  7.7724776], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28645694, 0.12478109], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.02716326713562, 'eval/sps': 31784.159595556182}
I0728 05:51:11.188201 139730147526464 train.py:379] starting iteration 36, 70778880 steps, 959.706974029541
I0728 05:51:36.930800 139730147526464 train.py:394] {'eval/walltime': 164.9571089744568, 'training/sps': 90656.09482376075, 'training/walltime': 811.624997138977, 'training/entropy_loss': Array(0.13409376, dtype=float32), 'training/policy_loss': Array(0.09050624, dtype=float32), 'training/total_loss': Array(4.624943, dtype=float32), 'training/v_loss': Array(4.4003425, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.316612  , 0.12323666], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.639927, 10.617505], dtype=float32), 'eval/episode_reward': Array([-2.6335008,  7.605734 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31107402, 0.12624757], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.050766229629517, 'eval/sps': 31598.959985332673}
I0728 05:51:36.932732 139730147526464 train.py:379] starting iteration 37, 72744960 steps, 985.451504945755
I0728 05:52:02.665182 139730147526464 train.py:394] {'eval/walltime': 168.97627210617065, 'training/sps': 90573.27903303238, 'training/walltime': 833.3320615291595, 'training/entropy_loss': Array(0.13671565, dtype=float32), 'training/policy_loss': Array(0.07815675, dtype=float32), 'training/total_loss': Array(4.8708634, dtype=float32), 'training/v_loss': Array(4.655991, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29980695, 0.12386362], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.151638, 10.718703], dtype=float32), 'eval/episode_reward': Array([-1.191045,  8.786741], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2932781 , 0.12835927], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019163131713867, 'eval/sps': 31847.425895703254}
I0728 05:52:02.668951 139730147526464 train.py:379] starting iteration 38, 74711040 steps, 1011.1877107620239
I0728 05:52:28.426236 139730147526464 train.py:394] {'eval/walltime': 172.98660564422607, 'training/sps': 90425.9592325433, 'training/walltime': 855.0744905471802, 'training/entropy_loss': Array(0.14440207, dtype=float32), 'training/policy_loss': Array(0.0732775, dtype=float32), 'training/total_loss': Array(5.08041, dtype=float32), 'training/v_loss': Array(4.862731, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30086607, 0.11910147], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.311806, 10.284425], dtype=float32), 'eval/episode_reward': Array([-1.9017453,  8.364408 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29475385, 0.12254799], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.01033353805542, 'eval/sps': 31917.54470927778}
I0728 05:52:28.428131 139730147526464 train.py:379] starting iteration 39, 76677120 steps, 1036.946904182434
I0728 05:52:54.228358 139730147526464 train.py:394] {'eval/walltime': 177.04945731163025, 'training/sps': 90465.94527380785, 'training/walltime': 876.8073093891144, 'training/entropy_loss': Array(0.15186106, dtype=float32), 'training/policy_loss': Array(0.0683871, dtype=float32), 'training/total_loss': Array(5.0509033, dtype=float32), 'training/v_loss': Array(4.830655, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32775363, 0.13952552], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.562702, 12.108395], dtype=float32), 'eval/episode_reward': Array([-3.6379902,  9.838779 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32189697, 0.1435581 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.062851667404175, 'eval/sps': 31504.96510294243}
I0728 05:52:54.230194 139730147526464 train.py:379] starting iteration 40, 78643200 steps, 1062.748967885971
I0728 05:53:20.002651 139730147526464 train.py:394] {'eval/walltime': 181.06229615211487, 'training/sps': 90373.11246518871, 'training/walltime': 898.5624525547028, 'training/entropy_loss': Array(0.16213608, dtype=float32), 'training/policy_loss': Array(0.06955987, dtype=float32), 'training/total_loss': Array(5.0393534, dtype=float32), 'training/v_loss': Array(4.807658, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30011982, 0.12099556], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.191309, 10.467909], dtype=float32), 'eval/episode_reward': Array([-1.5614852,  8.564711 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29392064, 0.12452876], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.012838840484619, 'eval/sps': 31897.617893008082}
I0728 05:53:20.004479 139730147526464 train.py:379] starting iteration 41, 80609280 steps, 1088.5232527256012
I0728 05:53:45.735209 139730147526464 train.py:394] {'eval/walltime': 185.10866594314575, 'training/sps': 90686.24087130364, 'training/walltime': 920.2424778938293, 'training/entropy_loss': Array(0.17057443, dtype=float32), 'training/policy_loss': Array(0.07509749, dtype=float32), 'training/total_loss': Array(5.072927, dtype=float32), 'training/v_loss': Array(4.8272552, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32607284, 0.12676133], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.47433 , 10.997187], dtype=float32), 'eval/episode_reward': Array([-2.6632442,  8.98174  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3203106 , 0.13036475], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.046369791030884, 'eval/sps': 31633.292706890676}
I0728 05:53:45.737070 139730147526464 train.py:379] starting iteration 42, 82575360 steps, 1114.2558426856995
I0728 05:54:11.483075 139730147526464 train.py:394] {'eval/walltime': 189.15209126472473, 'training/sps': 90612.7622441312, 'training/walltime': 941.9400837421417, 'training/entropy_loss': Array(0.17601502, dtype=float32), 'training/policy_loss': Array(0.07025672, dtype=float32), 'training/total_loss': Array(4.9585257, dtype=float32), 'training/v_loss': Array(4.712254, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.299631  , 0.13290326], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.172201, 11.470904], dtype=float32), 'eval/episode_reward': Array([-2.1934855,  9.721232 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2934931 , 0.13608776], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0434253215789795, 'eval/sps': 31656.328439377558}
I0728 05:54:11.485895 139730147526464 train.py:379] starting iteration 43, 84541440 steps, 1140.00466132164
I0728 05:54:37.229698 139730147526464 train.py:394] {'eval/walltime': 193.189923286438, 'training/sps': 90598.00376483802, 'training/walltime': 963.6412241458893, 'training/entropy_loss': Array(0.18354282, dtype=float32), 'training/policy_loss': Array(0.07295606, dtype=float32), 'training/total_loss': Array(4.862277, dtype=float32), 'training/v_loss': Array(4.605778, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2849327 , 0.13315119], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.836565, 11.515552], dtype=float32), 'eval/episode_reward': Array([-0.9723066,  8.89004  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27804625, 0.13730727], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.037832021713257, 'eval/sps': 31700.179529927413}
I0728 05:54:37.231562 139730147526464 train.py:379] starting iteration 44, 86507520 steps, 1165.7503349781036
I0728 05:55:03.041128 139730147526464 train.py:394] {'eval/walltime': 197.2714672088623, 'training/sps': 90512.30458008766, 'training/walltime': 985.3629117012024, 'training/entropy_loss': Array(0.19018215, dtype=float32), 'training/policy_loss': Array(0.06909209, dtype=float32), 'training/total_loss': Array(4.8554, dtype=float32), 'training/v_loss': Array(4.5961256, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3036855 , 0.12080262], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.454027, 10.43711 ], dtype=float32), 'eval/episode_reward': Array([-3.2745159,  8.857228 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29743367, 0.12465817], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.081543922424316, 'eval/sps': 31360.681750050062}
I0728 05:55:03.044039 139730147526464 train.py:379] starting iteration 45, 88473600 steps, 1191.5628123283386
I0728 05:55:28.838263 139730147526464 train.py:394] {'eval/walltime': 201.33948397636414, 'training/sps': 90514.03424201492, 'training/walltime': 1007.0841841697693, 'training/entropy_loss': Array(0.19647418, dtype=float32), 'training/policy_loss': Array(0.07139459, dtype=float32), 'training/total_loss': Array(4.5358524, dtype=float32), 'training/v_loss': Array(4.267984, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3006562 , 0.12254939], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.103954, 10.614787], dtype=float32), 'eval/episode_reward': Array([-2.6044989,  7.935501 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29448694, 0.12603727], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.068016767501831, 'eval/sps': 31464.963719558313}
I0728 05:55:28.840206 139730147526464 train.py:379] starting iteration 46, 90439680 steps, 1217.3589794635773
I0728 05:55:54.535209 139730147526464 train.py:394] {'eval/walltime': 205.38496232032776, 'training/sps': 90833.31708607708, 'training/walltime': 1028.7291054725647, 'training/entropy_loss': Array(0.20776398, dtype=float32), 'training/policy_loss': Array(0.07400055, dtype=float32), 'training/total_loss': Array(4.384324, dtype=float32), 'training/v_loss': Array(4.10256, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2968896 , 0.11965332], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.7855  , 10.361359], dtype=float32), 'eval/episode_reward': Array([-1.6543614,  8.358096 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29039866, 0.12362718], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.045478343963623, 'eval/sps': 31640.263305572393}
I0728 05:55:54.537169 139730147526464 train.py:379] starting iteration 47, 92405760 steps, 1243.0559430122375
I0728 05:56:20.295907 139730147526464 train.py:394] {'eval/walltime': 209.42530846595764, 'training/sps': 90546.57334126251, 'training/walltime': 1050.4425721168518, 'training/entropy_loss': Array(0.21776746, dtype=float32), 'training/policy_loss': Array(0.07789102, dtype=float32), 'training/total_loss': Array(4.3462296, dtype=float32), 'training/v_loss': Array(4.050571, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3032809 , 0.12336862], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.353006, 10.59716 ], dtype=float32), 'eval/episode_reward': Array([-1.5493042,  9.114338 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2971298 , 0.12696625], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.040346145629883, 'eval/sps': 31680.453947849815}
I0728 05:56:20.297857 139730147526464 train.py:379] starting iteration 48, 94371840 steps, 1268.816630601883
I0728 05:56:46.081261 139730147526464 train.py:394] {'eval/walltime': 213.46356439590454, 'training/sps': 90434.51534307523, 'training/walltime': 1072.182944059372, 'training/entropy_loss': Array(0.2275899, dtype=float32), 'training/policy_loss': Array(0.0775369, dtype=float32), 'training/total_loss': Array(4.2809305, dtype=float32), 'training/v_loss': Array(3.9758034, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2843755 , 0.12445402], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.740185, 10.709666], dtype=float32), 'eval/episode_reward': Array([-0.8224381,  8.757729 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2778124 , 0.12792331], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.038255929946899, 'eval/sps': 31696.85186388945}
I0728 05:56:46.083226 139730147526464 train.py:379] starting iteration 49, 96337920 steps, 1294.6019983291626
I0728 05:57:11.860036 139730147526464 train.py:394] {'eval/walltime': 217.52739262580872, 'training/sps': 90567.90938000147, 'training/walltime': 1093.8912954330444, 'training/entropy_loss': Array(0.23571214, dtype=float32), 'training/policy_loss': Array(0.07070016, dtype=float32), 'training/total_loss': Array(4.224822, dtype=float32), 'training/v_loss': Array(3.9184098, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28329223, 0.12292356], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.58443 , 10.669883], dtype=float32), 'eval/episode_reward': Array([-1.2899177,  8.313337 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27651298, 0.12699294], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.063828229904175, 'eval/sps': 31497.394269299184}
I0728 05:57:11.861962 139730147526464 train.py:379] starting iteration 50, 98304000 steps, 1320.3807349205017
I0728 05:57:37.649124 139730147526464 train.py:394] {'eval/walltime': 221.58848690986633, 'training/sps': 90513.33084481244, 'training/walltime': 1115.6127367019653, 'training/entropy_loss': Array(0.24558534, dtype=float32), 'training/policy_loss': Array(0.07167733, dtype=float32), 'training/total_loss': Array(4.560048, dtype=float32), 'training/v_loss': Array(4.2427855, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2872502 , 0.11137065], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.96    ,  9.616075], dtype=float32), 'eval/episode_reward': Array([-0.9435839,  8.289957 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28098935, 0.11458866], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.061094284057617, 'eval/sps': 31518.598448325014}
I0728 05:57:37.689149 139730147526464 train.py:379] starting iteration 51, 100270080 steps, 1346.207921743393
I0728 05:58:03.402209 139730147526464 train.py:394] {'eval/walltime': 225.63804507255554, 'training/sps': 90775.1746227296, 'training/walltime': 1137.271521806717, 'training/entropy_loss': Array(0.2528882, dtype=float32), 'training/policy_loss': Array(0.07437187, dtype=float32), 'training/total_loss': Array(4.6474905, dtype=float32), 'training/v_loss': Array(4.3202305, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29334357, 0.13130665], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.48058 , 11.345777], dtype=float32), 'eval/episode_reward': Array([-2.4652112,  9.343682 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28666717, 0.13555747], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.049558162689209, 'eval/sps': 31608.38660852779}
I0728 05:58:03.404179 139730147526464 train.py:379] starting iteration 52, 102236160 steps, 1371.9229526519775
I0728 05:58:29.187806 139730147526464 train.py:394] {'eval/walltime': 229.68437504768372, 'training/sps': 90471.58870335389, 'training/walltime': 1159.0029850006104, 'training/entropy_loss': Array(0.26255253, dtype=float32), 'training/policy_loss': Array(0.07443963, dtype=float32), 'training/total_loss': Array(4.5983233, dtype=float32), 'training/v_loss': Array(4.261331, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2777776 , 0.12487571], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.146841, 10.798025], dtype=float32), 'eval/episode_reward': Array([-0.8098784,  8.615979 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27100122, 0.12840359], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.046329975128174, 'eval/sps': 31633.60397861408}
I0728 05:58:29.189628 139730147526464 train.py:379] starting iteration 53, 104202240 steps, 1397.7084016799927
I0728 05:58:54.945399 139730147526464 train.py:394] {'eval/walltime': 233.72317719459534, 'training/sps': 90549.89117843712, 'training/walltime': 1180.715656042099, 'training/entropy_loss': Array(0.27289575, dtype=float32), 'training/policy_loss': Array(0.07362334, dtype=float32), 'training/total_loss': Array(4.552408, dtype=float32), 'training/v_loss': Array(4.2058887, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28411466, 0.13659903], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.755276 , 11.7432785], dtype=float32), 'eval/episode_reward': Array([-1.6696973,  8.11806  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27754498, 0.14022347], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.038802146911621, 'eval/sps': 31692.5651081667}
I0728 05:58:54.947247 139730147526464 train.py:379] starting iteration 54, 106168320 steps, 1423.4660203456879
I0728 05:59:20.679116 139730147526464 train.py:394] {'eval/walltime': 237.7736747264862, 'training/sps': 90699.84695296173, 'training/walltime': 1202.392429113388, 'training/entropy_loss': Array(0.28234655, dtype=float32), 'training/policy_loss': Array(0.07288967, dtype=float32), 'training/total_loss': Array(4.4594097, dtype=float32), 'training/v_loss': Array(4.1041737, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30535665, 0.13368651], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.510496, 11.5309  ], dtype=float32), 'eval/episode_reward': Array([-2.4966295,  8.158069 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29890907, 0.13774012], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.050497531890869, 'eval/sps': 31601.056164635294}
I0728 05:59:20.680949 139730147526464 train.py:379] starting iteration 55, 108134400 steps, 1449.1997232437134
I0728 05:59:46.401388 139730147526464 train.py:394] {'eval/walltime': 241.82885241508484, 'training/sps': 90774.29029725482, 'training/walltime': 1224.0514252185822, 'training/entropy_loss': Array(0.2947356, dtype=float32), 'training/policy_loss': Array(0.06980089, dtype=float32), 'training/total_loss': Array(4.576545, dtype=float32), 'training/v_loss': Array(4.2120085, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2968493 , 0.13156465], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.826351, 11.343342], dtype=float32), 'eval/episode_reward': Array([-1.205277,  8.823793], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2904914 , 0.13519946], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.055177688598633, 'eval/sps': 31564.58479239502}
I0728 05:59:46.404232 139730147526464 train.py:379] starting iteration 56, 110100480 steps, 1474.9230053424835
I0728 06:00:12.218047 139730147526464 train.py:394] {'eval/walltime': 245.8807578086853, 'training/sps': 90371.89625143146, 'training/walltime': 1245.8068611621857, 'training/entropy_loss': Array(0.30578893, dtype=float32), 'training/policy_loss': Array(0.06982742, dtype=float32), 'training/total_loss': Array(4.4908257, dtype=float32), 'training/v_loss': Array(4.1152096, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29290384, 0.12600498], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.411457, 10.922339], dtype=float32), 'eval/episode_reward': Array([-1.35216  ,  7.7146235], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28613058, 0.13050328], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.051905393600464, 'eval/sps': 31590.076165687835}
I0728 06:00:12.221369 139730147526464 train.py:379] starting iteration 57, 112066560 steps, 1500.7401416301727
I0728 06:00:37.948223 139730147526464 train.py:394] {'eval/walltime': 249.91268110275269, 'training/sps': 90641.88713396523, 'training/walltime': 1267.497495174408, 'training/entropy_loss': Array(0.31542534, dtype=float32), 'training/policy_loss': Array(0.07032104, dtype=float32), 'training/total_loss': Array(4.6428385, dtype=float32), 'training/v_loss': Array(4.257092, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28864545, 0.12994452], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.116774, 11.235045], dtype=float32), 'eval/episode_reward': Array([-0.50064325,  8.571211  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2817499, 0.1345165], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.031923294067383, 'eval/sps': 31746.635703199176}
I0728 06:00:37.950040 139730147526464 train.py:379] starting iteration 58, 114032640 steps, 1526.4688127040863
I0728 06:01:03.684803 139730147526464 train.py:394] {'eval/walltime': 253.93736004829407, 'training/sps': 90578.9786443801, 'training/walltime': 1289.2031936645508, 'training/entropy_loss': Array(0.32098156, dtype=float32), 'training/policy_loss': Array(0.06817849, dtype=float32), 'training/total_loss': Array(4.6252966, dtype=float32), 'training/v_loss': Array(4.2361364, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31089947, 0.15095504], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.05693 , 13.038735], dtype=float32), 'eval/episode_reward': Array([-2.4412146,  9.344139 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30450085, 0.15493986], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024678945541382, 'eval/sps': 31803.779067098236}
I0728 06:01:03.686732 139730147526464 train.py:379] starting iteration 59, 115998720 steps, 1552.2055056095123
I0728 06:01:29.482908 139730147526464 train.py:394] {'eval/walltime': 257.99288272857666, 'training/sps': 90453.63658406235, 'training/walltime': 1310.9389698505402, 'training/entropy_loss': Array(0.3279817, dtype=float32), 'training/policy_loss': Array(0.07209004, dtype=float32), 'training/total_loss': Array(4.5896845, dtype=float32), 'training/v_loss': Array(4.1896124, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30083525, 0.1197662 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.185875, 10.315961], dtype=float32), 'eval/episode_reward': Array([-0.617118,  8.009772], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2947911 , 0.12304835], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.055522680282593, 'eval/sps': 31561.899683687832}
I0728 06:01:29.484930 139730147526464 train.py:379] starting iteration 60, 117964800 steps, 1578.0037033557892
I0728 06:01:55.192846 139730147526464 train.py:394] {'eval/walltime': 262.0410282611847, 'training/sps': 90791.62421863878, 'training/walltime': 1332.5938308238983, 'training/entropy_loss': Array(0.3372151, dtype=float32), 'training/policy_loss': Array(0.06451075, dtype=float32), 'training/total_loss': Array(4.5759974, dtype=float32), 'training/v_loss': Array(4.1742716, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29729828, 0.12231182], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.864529, 10.639628], dtype=float32), 'eval/episode_reward': Array([-1.0191522,  9.092036 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29075783, 0.12673338], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.048145532608032, 'eval/sps': 31619.41658691691}
I0728 06:01:55.194854 139730147526464 train.py:379] starting iteration 61, 119930880 steps, 1603.7136256694794
I0728 06:02:20.910036 139730147526464 train.py:394] {'eval/walltime': 266.06252360343933, 'training/sps': 90647.92022447403, 'training/walltime': 1354.2830212116241, 'training/entropy_loss': Array(0.34396738, dtype=float32), 'training/policy_loss': Array(0.06650972, dtype=float32), 'training/total_loss': Array(4.55226, dtype=float32), 'training/v_loss': Array(4.1417828, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29848805, 0.13121392], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.964897 , 11.3224325], dtype=float32), 'eval/episode_reward': Array([-2.277362,  9.489786], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.291689  , 0.13578571], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021495342254639, 'eval/sps': 31828.95642202515}
I0728 06:02:20.912027 139730147526464 train.py:379] starting iteration 62, 121896960 steps, 1629.4308001995087
I0728 06:02:46.676882 139730147526464 train.py:394] {'eval/walltime': 270.0949342250824, 'training/sps': 90489.44468121754, 'training/walltime': 1376.0101962089539, 'training/entropy_loss': Array(0.34916294, dtype=float32), 'training/policy_loss': Array(0.06617711, dtype=float32), 'training/total_loss': Array(4.6248803, dtype=float32), 'training/v_loss': Array(4.2095404, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2911518, 0.1372671], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.41898 , 11.882055], dtype=float32), 'eval/episode_reward': Array([-0.529173,  9.144237], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28448412, 0.1413374 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.032410621643066, 'eval/sps': 31742.79903762491}
I0728 06:02:46.678821 139730147526464 train.py:379] starting iteration 63, 123863040 steps, 1655.1975934505463
I0728 06:03:12.442284 139730147526464 train.py:394] {'eval/walltime': 274.15482687950134, 'training/sps': 90607.55715368669, 'training/walltime': 1397.7090485095978, 'training/entropy_loss': Array(0.35187408, dtype=float32), 'training/policy_loss': Array(0.06463976, dtype=float32), 'training/total_loss': Array(4.6304855, dtype=float32), 'training/v_loss': Array(4.2139716, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28521347, 0.12969331], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.883522, 11.246446], dtype=float32), 'eval/episode_reward': Array([-1.5512776,  8.787264 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27838165, 0.13372104], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.059892654418945, 'eval/sps': 31527.927188094447}
I0728 06:03:12.444327 139730147526464 train.py:379] starting iteration 64, 125829120 steps, 1680.9630999565125
I0728 06:03:38.169426 139730147526464 train.py:394] {'eval/walltime': 278.1773533821106, 'training/sps': 90612.5392134562, 'training/walltime': 1419.4067077636719, 'training/entropy_loss': Array(0.3569405, dtype=float32), 'training/policy_loss': Array(0.06404953, dtype=float32), 'training/total_loss': Array(4.615119, dtype=float32), 'training/v_loss': Array(4.194129, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30216601, 0.1403647 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.308475, 12.12597 ], dtype=float32), 'eval/episode_reward': Array([-1.5686191,  9.048036 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29567444, 0.14436658], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022526502609253, 'eval/sps': 31820.797182311042}
I0728 06:03:38.171415 139730147526464 train.py:379] starting iteration 65, 127795200 steps, 1706.690188407898
I0728 06:04:03.899015 139730147526464 train.py:394] {'eval/walltime': 282.21711564064026, 'training/sps': 90673.33377953433, 'training/walltime': 1441.0898191928864, 'training/entropy_loss': Array(0.3660948, dtype=float32), 'training/policy_loss': Array(0.06110555, dtype=float32), 'training/total_loss': Array(4.512314, dtype=float32), 'training/v_loss': Array(4.0851135, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28535426, 0.12374678], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.832932, 10.688106], dtype=float32), 'eval/episode_reward': Array([-1.9775579,  8.693731 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2784413 , 0.12805703], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.039762258529663, 'eval/sps': 31685.032882748816}
I0728 06:04:03.901001 139730147526464 train.py:379] starting iteration 66, 129761280 steps, 1732.4197735786438
I0728 06:04:29.578242 139730147526464 train.py:394] {'eval/walltime': 286.25467681884766, 'training/sps': 90874.64868725499, 'training/walltime': 1462.724895954132, 'training/entropy_loss': Array(0.3752551, dtype=float32), 'training/policy_loss': Array(0.06080856, dtype=float32), 'training/total_loss': Array(4.5863733, dtype=float32), 'training/v_loss': Array(4.1503096, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28055048, 0.10530829], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.457624 ,  9.0950985], dtype=float32), 'eval/episode_reward': Array([-0.82068336,  7.945542  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27456754, 0.10788783], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0375611782073975, 'eval/sps': 31702.306008606323}
I0728 06:04:29.580228 139730147526464 train.py:379] starting iteration 67, 131727360 steps, 1758.0990018844604
I0728 06:04:55.280800 139730147526464 train.py:394] {'eval/walltime': 290.28481698036194, 'training/sps': 90746.79387630548, 'training/walltime': 1484.3904547691345, 'training/entropy_loss': Array(0.38396737, dtype=float32), 'training/policy_loss': Array(0.06172433, dtype=float32), 'training/total_loss': Array(4.5102715, dtype=float32), 'training/v_loss': Array(4.06458, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30831277, 0.13935293], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.858397, 12.021535], dtype=float32), 'eval/episode_reward': Array([-1.6286334,  9.059597 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30211902, 0.14312765], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.030140161514282, 'eval/sps': 31760.681978838515}
I0728 06:04:55.282859 139730147526464 train.py:379] starting iteration 68, 133693440 steps, 1783.801632642746
I0728 06:05:21.049617 139730147526464 train.py:394] {'eval/walltime': 294.3375849723816, 'training/sps': 90564.91347390352, 'training/walltime': 1506.0995242595673, 'training/entropy_loss': Array(0.38970637, dtype=float32), 'training/policy_loss': Array(0.05561836, dtype=float32), 'training/total_loss': Array(4.5344267, dtype=float32), 'training/v_loss': Array(4.0891027, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26707512, 0.1223892 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.342388, 10.525128], dtype=float32), 'eval/episode_reward': Array([0.73301065, 7.835982  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26025635, 0.12560464], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.052767992019653, 'eval/sps': 31583.352477133183}
I0728 06:05:21.051627 139730147526464 train.py:379] starting iteration 69, 135659520 steps, 1809.5703999996185
I0728 06:05:46.731399 139730147526464 train.py:394] {'eval/walltime': 298.3580298423767, 'training/sps': 90791.71818205199, 'training/walltime': 1527.754362821579, 'training/entropy_loss': Array(0.392767, dtype=float32), 'training/policy_loss': Array(0.05378305, dtype=float32), 'training/total_loss': Array(4.487625, dtype=float32), 'training/v_loss': Array(4.0410748, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29705763, 0.1412104 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.841442, 12.158891], dtype=float32), 'eval/episode_reward': Array([-1.3267046,  8.685204 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29047573, 0.14574477], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020444869995117, 'eval/sps': 31837.272774282676}
I0728 06:05:46.733442 139730147526464 train.py:379] starting iteration 70, 137625600 steps, 1835.252215385437
I0728 06:06:12.453644 139730147526464 train.py:394] {'eval/walltime': 302.4114043712616, 'training/sps': 90761.21328493916, 'training/walltime': 1549.416479587555, 'training/entropy_loss': Array(0.39822212, dtype=float32), 'training/policy_loss': Array(0.04747237, dtype=float32), 'training/total_loss': Array(4.404379, dtype=float32), 'training/v_loss': Array(3.9586844, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29429275, 0.14024775], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.540695, 12.135424], dtype=float32), 'eval/episode_reward': Array([-0.9936057,  8.644814 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2874387 , 0.14498802], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.053374528884888, 'eval/sps': 31578.626422960653}
I0728 06:06:12.455655 139730147526464 train.py:379] starting iteration 71, 139591680 steps, 1860.9744284152985
I0728 06:06:38.153832 139730147526464 train.py:394] {'eval/walltime': 306.4327657222748, 'training/sps': 90719.8221022492, 'training/walltime': 1571.088479757309, 'training/entropy_loss': Array(0.40877324, dtype=float32), 'training/policy_loss': Array(0.04701777, dtype=float32), 'training/total_loss': Array(4.39363, dtype=float32), 'training/v_loss': Array(3.937839, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28943208, 0.118264  ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.241522 , 10.2094345], dtype=float32), 'eval/episode_reward': Array([-2.1008844,  8.989243 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28329173, 0.12135   ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021361351013184, 'eval/sps': 31830.016958747154}
I0728 06:06:38.155834 139730147526464 train.py:379] starting iteration 72, 141557760 steps, 1886.674607038498
I0728 06:07:03.889942 139730147526464 train.py:394] {'eval/walltime': 310.4856140613556, 'training/sps': 90701.4311533015, 'training/walltime': 1592.7648742198944, 'training/entropy_loss': Array(0.42033845, dtype=float32), 'training/policy_loss': Array(0.04782457, dtype=float32), 'training/total_loss': Array(4.337207, dtype=float32), 'training/v_loss': Array(3.869044, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27039295, 0.12077378], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.592838, 10.475739], dtype=float32), 'eval/episode_reward': Array([0.30771986, 8.599552  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26310754, 0.12532946], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0528483390808105, 'eval/sps': 31582.72634229153}
I0728 06:07:03.891968 139730147526464 train.py:379] starting iteration 73, 143523840 steps, 1912.4107403755188
I0728 06:07:29.475158 139730147526464 train.py:394] {'eval/walltime': 314.5365378856659, 'training/sps': 91335.82550896652, 'training/walltime': 1614.2907102108002, 'training/entropy_loss': Array(0.4257254, dtype=float32), 'training/policy_loss': Array(0.04248437, dtype=float32), 'training/total_loss': Array(4.200725, dtype=float32), 'training/v_loss': Array(3.732515, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27549863, 0.12583353], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.986961, 10.823268], dtype=float32), 'eval/episode_reward': Array([0.04036772, 8.740884  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26874182, 0.12929648], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.050923824310303, 'eval/sps': 31597.730678579934}
I0728 06:07:29.477081 139730147526464 train.py:379] starting iteration 74, 145489920 steps, 1937.995854139328
I0728 06:07:54.891599 139730147526464 train.py:394] {'eval/walltime': 318.5958032608032, 'training/sps': 92087.32751537791, 'training/walltime': 1635.6408791542053, 'training/entropy_loss': Array(0.42959425, dtype=float32), 'training/policy_loss': Array(0.04217488, dtype=float32), 'training/total_loss': Array(4.157545, dtype=float32), 'training/v_loss': Array(3.685776, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27837318, 0.1285807 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.280546, 11.081601], dtype=float32), 'eval/episode_reward': Array([-1.2140558,  8.310629 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27166978, 0.13250656], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.059265375137329, 'eval/sps': 31532.799206474552}
I0728 06:07:54.893839 139730147526464 train.py:379] starting iteration 75, 147456000 steps, 1963.4126107692719
I0728 06:08:20.451921 139730147526464 train.py:394] {'eval/walltime': 322.64637994766235, 'training/sps': 91434.4554506384, 'training/walltime': 1657.1434953212738, 'training/entropy_loss': Array(0.43563592, dtype=float32), 'training/policy_loss': Array(0.04028279, dtype=float32), 'training/total_loss': Array(4.536873, dtype=float32), 'training/v_loss': Array(4.0609536, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28645357, 0.11826967], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.01568 , 10.213311], dtype=float32), 'eval/episode_reward': Array([-1.2087338,  8.57915  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28013438, 0.12203375], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.050576686859131, 'eval/sps': 31600.43862772855}
I0728 06:08:20.501253 139730147526464 train.py:379] starting iteration 76, 149422080 steps, 1989.0200262069702
I0728 06:08:46.107298 139730147526464 train.py:394] {'eval/walltime': 326.71205043792725, 'training/sps': 91296.51232931898, 'training/walltime': 1678.6786005496979, 'training/entropy_loss': Array(0.4192525, dtype=float32), 'training/policy_loss': Array(0.05500005, dtype=float32), 'training/total_loss': Array(4.4823685, dtype=float32), 'training/v_loss': Array(4.008116, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29158923, 0.12626429], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.41571 , 10.876143], dtype=float32), 'eval/episode_reward': Array([-0.9570084,  8.517112 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28554225, 0.12970415], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.065670490264893, 'eval/sps': 31483.121986027047}
I0728 06:08:46.109451 139730147526464 train.py:379] starting iteration 77, 151388160 steps, 2014.6282241344452
I0728 06:09:11.671790 139730147526464 train.py:394] {'eval/walltime': 330.7714579105377, 'training/sps': 91454.3832835765, 'training/walltime': 1700.1765313148499, 'training/entropy_loss': Array(0.3446449, dtype=float32), 'training/policy_loss': Array(0.05094947, dtype=float32), 'training/total_loss': Array(4.8196125, dtype=float32), 'training/v_loss': Array(4.4240184, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2724767 , 0.12138742], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.776024, 10.493133], dtype=float32), 'eval/episode_reward': Array([0.41356218, 8.93742   ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26571634, 0.12577346], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.059407472610474, 'eval/sps': 31531.695417037634}
I0728 06:09:11.673805 139730147526464 train.py:379] starting iteration 78, 153354240 steps, 2040.1925783157349
I0728 06:09:37.125993 139730147526464 train.py:394] {'eval/walltime': 334.8033649921417, 'training/sps': 91807.3466355915, 'training/walltime': 1721.5918109416962, 'training/entropy_loss': Array(0.23664278, dtype=float32), 'training/policy_loss': Array(0.03127195, dtype=float32), 'training/total_loss': Array(5.538281, dtype=float32), 'training/v_loss': Array(5.270366, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30473417, 0.1345113 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.697947, 11.564567], dtype=float32), 'eval/episode_reward': Array([-1.5148617,  9.612942 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2999587 , 0.13733003], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.031907081604004, 'eval/sps': 31746.763357720552}
I0728 06:09:37.128100 139730147526464 train.py:379] starting iteration 79, 155320320 steps, 2065.646872282028
I0728 06:10:02.691375 139730147526464 train.py:394] {'eval/walltime': 338.85207200050354, 'training/sps': 91405.09466375136, 'training/walltime': 1743.1013340950012, 'training/entropy_loss': Array(0.2095289, dtype=float32), 'training/policy_loss': Array(0.01535586, dtype=float32), 'training/total_loss': Array(5.6909027, dtype=float32), 'training/v_loss': Array(5.4660172, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32791844, 0.1455323 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.725891, 12.447739], dtype=float32), 'eval/episode_reward': Array([-3.371838, 11.221824], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3237292 , 0.14842993], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.048707008361816, 'eval/sps': 31615.03159790049}
I0728 06:10:02.693400 139730147526464 train.py:379] starting iteration 80, 157286400 steps, 2091.212173461914
I0728 06:10:28.388620 139730147526464 train.py:394] {'eval/walltime': 342.8837299346924, 'training/sps': 90775.6972322118, 'training/walltime': 1764.759994506836, 'training/entropy_loss': Array(0.2146453, dtype=float32), 'training/policy_loss': Array(0.01338086, dtype=float32), 'training/total_loss': Array(5.313848, dtype=float32), 'training/v_loss': Array(5.085822, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30614722, 0.13054277], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.870338, 11.190387], dtype=float32), 'eval/episode_reward': Array([-1.8848349,  9.527997 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30157763, 0.13317804], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.031657934188843, 'eval/sps': 31748.725236471037}
I0728 06:10:28.390654 139730147526464 train.py:379] starting iteration 81, 159252480 steps, 2116.9094281196594
I0728 06:10:54.081185 139730147526464 train.py:394] {'eval/walltime': 346.94361329078674, 'training/sps': 90913.16932794917, 'training/walltime': 1786.3859043121338, 'training/entropy_loss': Array(0.21106774, dtype=float32), 'training/policy_loss': Array(0.01158441, dtype=float32), 'training/total_loss': Array(5.1775007, dtype=float32), 'training/v_loss': Array(4.9548483, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30609965, 0.12557428], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.91153 , 10.734122], dtype=float32), 'eval/episode_reward': Array([-2.5976639,  8.649513 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3016654 , 0.12819876], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.05988335609436, 'eval/sps': 31527.999396302115}
I0728 06:10:54.083229 139730147526464 train.py:379] starting iteration 82, 161218560 steps, 2142.6020028591156
I0728 06:11:19.784547 139730147526464 train.py:394] {'eval/walltime': 350.9789574146271, 'training/sps': 90764.9904350644, 'training/walltime': 1808.0471196174622, 'training/entropy_loss': Array(0.19720486, dtype=float32), 'training/policy_loss': Array(0.00993848, dtype=float32), 'training/total_loss': Array(5.1755724, dtype=float32), 'training/v_loss': Array(4.9684296, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30772495, 0.12730594], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.012373, 10.913669], dtype=float32), 'eval/episode_reward': Array([-2.5449963,  9.18345  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30339736, 0.13004601], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.035344123840332, 'eval/sps': 31719.72354074867}
I0728 06:11:19.786544 139730147526464 train.py:379] starting iteration 83, 163184640 steps, 2168.3053176403046
