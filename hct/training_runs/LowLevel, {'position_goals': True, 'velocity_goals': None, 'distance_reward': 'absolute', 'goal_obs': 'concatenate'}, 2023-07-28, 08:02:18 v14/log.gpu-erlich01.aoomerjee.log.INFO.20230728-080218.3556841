I0728 08:02:18.211961 140143444526912 low_level_env.py:188] Initialising environment...
I0728 08:02:57.767843 140143444526912 low_level_env.py:293] Environment initialised.
I0728 08:02:57.784051 140143444526912 train.py:118] JAX is running on GPU.
I0728 08:02:57.784155 140143444526912 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 08:03:06.277631 140143444526912 train.py:367] Running initial eval
I0728 08:03:22.065761 140143444526912 train.py:373] {'eval/walltime': 15.655210494995117, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30524325, 0.15092595], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.735325, 13.001383], dtype=float32), 'eval/episode_reward': Array([-24546.15 ,  10720.807], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30029616, 0.15418051], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.655210494995117, 'eval/sps': 8176.191565161062}
I0728 08:03:22.066892 140143444526912 train.py:379] starting iteration 0, 0 steps, 24.282875061035156
I0728 08:04:04.881602 140143444526912 train.py:394] {'eval/walltime': 19.582303524017334, 'training/sps': 50565.64713676925, 'training/walltime': 38.88173317909241, 'training/entropy_loss': Array(-0.04457694, dtype=float32), 'training/policy_loss': Array(0.01283191, dtype=float32), 'training/total_loss': Array(158474.81, dtype=float32), 'training/v_loss': Array(158474.84, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30973017, 0.13008994], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.176058, 11.139432], dtype=float32), 'eval/episode_reward': Array([-25285.422,   8845.279], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3055196 , 0.13235609], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.927093029022217, 'eval/sps': 32594.083983762906}
I0728 08:04:04.901878 140143444526912 train.py:379] starting iteration 1, 1966080 steps, 67.11786460876465
I0728 08:04:28.915203 140143444526912 train.py:394] {'eval/walltime': 23.56560707092285, 'training/sps': 98180.02802385841, 'training/walltime': 58.90698719024658, 'training/entropy_loss': Array(-0.04425986, dtype=float32), 'training/policy_loss': Array(0.00173762, dtype=float32), 'training/total_loss': Array(153747.1, dtype=float32), 'training/v_loss': Array(153747.16, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30513847, 0.1259413 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.771055, 10.777033], dtype=float32), 'eval/episode_reward': Array([-24709.984,   8812.558], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30096978, 0.1279159 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9833035469055176, 'eval/sps': 32134.131504850666}
I0728 08:04:28.917301 140143444526912 train.py:379] starting iteration 2, 3932160 steps, 91.13328886032104
I0728 08:04:53.397108 140143444526912 train.py:394] {'eval/walltime': 27.56628942489624, 'training/sps': 96026.52087131316, 'training/walltime': 79.38133096694946, 'training/entropy_loss': Array(-0.04492144, dtype=float32), 'training/policy_loss': Array(0.00103217, dtype=float32), 'training/total_loss': Array(155359.17, dtype=float32), 'training/v_loss': Array(155359.22, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2953887 , 0.10708226], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.977657,  9.17708 ], dtype=float32), 'eval/episode_reward': Array([-23475.627,   8466.205], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29124022, 0.10922251], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.000682353973389, 'eval/sps': 31994.542099267954}
I0728 08:04:53.399147 140143444526912 train.py:379] starting iteration 3, 5898240 steps, 115.61513471603394
I0728 08:05:18.274745 140143444526912 train.py:394] {'eval/walltime': 31.55232858657837, 'training/sps': 94138.0190070199, 'training/walltime': 100.26641035079956, 'training/entropy_loss': Array(-0.04588174, dtype=float32), 'training/policy_loss': Array(0.00084819, dtype=float32), 'training/total_loss': Array(156239.47, dtype=float32), 'training/v_loss': Array(156239.53, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31317055, 0.13015729], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.436583, 11.196053], dtype=float32), 'eval/episode_reward': Array([-24836.719,   9645.107], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30827448, 0.13358371], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.986039161682129, 'eval/sps': 32112.077881839814}
I0728 08:05:18.277715 140143444526912 train.py:379] starting iteration 4, 7864320 steps, 140.4936957359314
I0728 08:05:43.290517 140143444526912 train.py:394] {'eval/walltime': 35.53357648849487, 'training/sps': 93504.71771676395, 'training/walltime': 121.29294300079346, 'training/entropy_loss': Array(-0.04681453, dtype=float32), 'training/policy_loss': Array(0.00083806, dtype=float32), 'training/total_loss': Array(154819.03, dtype=float32), 'training/v_loss': Array(154819.08, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28333762, 0.1257836 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.863792 , 10.8203745], dtype=float32), 'eval/episode_reward': Array([-23198.39 ,   9091.885], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2780195 , 0.12930006], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.981247901916504, 'eval/sps': 32150.723379567247}
I0728 08:05:43.292452 140143444526912 train.py:379] starting iteration 5, 9830400 steps, 165.50843954086304
I0728 08:06:08.213318 140143444526912 train.py:394] {'eval/walltime': 39.51100158691406, 'training/sps': 93896.05317054797, 'training/walltime': 142.2318422794342, 'training/entropy_loss': Array(-0.04811831, dtype=float32), 'training/policy_loss': Array(0.00092385, dtype=float32), 'training/total_loss': Array(154298.42, dtype=float32), 'training/v_loss': Array(154298.47, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.299214  , 0.12276861], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.29576, 10.53877], dtype=float32), 'eval/episode_reward': Array([-24053.838,   8887.543], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29481226, 0.12527847], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9774250984191895, 'eval/sps': 32181.624250038814}
I0728 08:06:08.215281 140143444526912 train.py:379] starting iteration 6, 11796480 steps, 190.43126893043518
I0728 08:06:33.358126 140143444526912 train.py:394] {'eval/walltime': 43.52481150627136, 'training/sps': 93072.22028054428, 'training/walltime': 163.35608315467834, 'training/entropy_loss': Array(-0.04917766, dtype=float32), 'training/policy_loss': Array(0.00094, dtype=float32), 'training/total_loss': Array(153496.03, dtype=float32), 'training/v_loss': Array(153496.06, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3169387 , 0.12624258], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.77075 , 10.788253], dtype=float32), 'eval/episode_reward': Array([-25881.902,   9049.034], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31292105, 0.12841958], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0138099193573, 'eval/sps': 31889.90076054614}
I0728 08:06:33.360101 140143444526912 train.py:379] starting iteration 7, 13762560 steps, 215.57608819007874
I0728 08:06:58.672852 140143444526912 train.py:394] {'eval/walltime': 47.52460050582886, 'training/sps': 92271.53240445159, 'training/walltime': 184.6636300086975, 'training/entropy_loss': Array(-0.05001799, dtype=float32), 'training/policy_loss': Array(0.00102528, dtype=float32), 'training/total_loss': Array(151414.67, dtype=float32), 'training/v_loss': Array(151414.72, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28338534, 0.11949708], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.894505, 10.263874], dtype=float32), 'eval/episode_reward': Array([-23369.512,   9043.628], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27853653, 0.12227822], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.999788999557495, 'eval/sps': 32001.68809258711}
I0728 08:06:58.674737 140143444526912 train.py:379] starting iteration 8, 15728640 steps, 240.89072465896606
I0728 08:07:24.177391 140143444526912 train.py:394] {'eval/walltime': 51.52631402015686, 'training/sps': 91468.89960325573, 'training/walltime': 206.15814900398254, 'training/entropy_loss': Array(-0.05095325, dtype=float32), 'training/policy_loss': Array(0.00121198, dtype=float32), 'training/total_loss': Array(153305.42, dtype=float32), 'training/v_loss': Array(153305.47, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30703694, 0.12873313], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.93753 , 11.070117], dtype=float32), 'eval/episode_reward': Array([-25066.748,   9442.082], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30264753, 0.13126633], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.001713514328003, 'eval/sps': 31986.297755124207}
I0728 08:07:24.180901 140143444526912 train.py:379] starting iteration 9, 17694720 steps, 266.39688873291016
I0728 08:07:49.556284 140143444526912 train.py:394] {'eval/walltime': 55.551806688308716, 'training/sps': 92108.17266343682, 'training/walltime': 227.50348615646362, 'training/entropy_loss': Array(-0.05162727, dtype=float32), 'training/policy_loss': Array(0.00125723, dtype=float32), 'training/total_loss': Array(153815.28, dtype=float32), 'training/v_loss': Array(153815.34, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29045832, 0.11217605], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.529373 ,  9.6101675], dtype=float32), 'eval/episode_reward': Array([-23777.977,   8368.364], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28639698, 0.11391043], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0254926681518555, 'eval/sps': 31797.35017596395}
I0728 08:07:49.558143 140143444526912 train.py:379] starting iteration 10, 19660800 steps, 291.77413034439087
I0728 08:08:14.973708 140143444526912 train.py:394] {'eval/walltime': 59.57609748840332, 'training/sps': 91935.28422360048, 'training/walltime': 248.88896417617798, 'training/entropy_loss': Array(-0.05208036, dtype=float32), 'training/policy_loss': Array(0.0011932, dtype=float32), 'training/total_loss': Array(153878.47, dtype=float32), 'training/v_loss': Array(153878.5, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29496112, 0.13222274], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.911922, 11.342732], dtype=float32), 'eval/episode_reward': Array([-23590.443,   9886.123], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29025573, 0.13560924], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0242908000946045, 'eval/sps': 31806.846562129886}
I0728 08:08:14.977190 140143444526912 train.py:379] starting iteration 11, 21626880 steps, 317.1931653022766
I0728 08:08:40.352931 140143444526912 train.py:394] {'eval/walltime': 63.59938287734985, 'training/sps': 92097.41153777692, 'training/walltime': 270.23679542541504, 'training/entropy_loss': Array(-0.0524083, dtype=float32), 'training/policy_loss': Array(0.00108805, dtype=float32), 'training/total_loss': Array(155927.11, dtype=float32), 'training/v_loss': Array(155927.16, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3125338 , 0.12246559], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.410898, 10.470744], dtype=float32), 'eval/episode_reward': Array([-25366.99 ,   9071.422], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3084936 , 0.12478315], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023285388946533, 'eval/sps': 31814.79503086303}
I0728 08:08:40.354806 140143444526912 train.py:379] starting iteration 12, 23592960 steps, 342.57079458236694
I0728 08:09:05.753096 140143444526912 train.py:394] {'eval/walltime': 67.61974596977234, 'training/sps': 91986.44546381694, 'training/walltime': 291.6103792190552, 'training/entropy_loss': Array(-0.05271433, dtype=float32), 'training/policy_loss': Array(0.00111056, dtype=float32), 'training/total_loss': Array(157712.92, dtype=float32), 'training/v_loss': Array(157712.97, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31443274, 0.1269098 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.532082, 10.886008], dtype=float32), 'eval/episode_reward': Array([-25552.059,   9369.501], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.309762  , 0.12976395], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020363092422485, 'eval/sps': 31837.920371235203}
I0728 08:09:05.754966 140143444526912 train.py:379] starting iteration 13, 25559040 steps, 367.97095346450806
I0728 08:09:31.146083 140143444526912 train.py:394] {'eval/walltime': 71.63696241378784, 'training/sps': 92005.59325968211, 'training/walltime': 312.979514837265, 'training/entropy_loss': Array(-0.05284861, dtype=float32), 'training/policy_loss': Array(0.00101477, dtype=float32), 'training/total_loss': Array(156430.78, dtype=float32), 'training/v_loss': Array(156430.81, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29981214, 0.14324237], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.35519 , 12.288827], dtype=float32), 'eval/episode_reward': Array([-25175.477,  10106.2  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.295174  , 0.14634429], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.017216444015503, 'eval/sps': 31862.858719171876}
I0728 08:09:31.148011 140143444526912 train.py:379] starting iteration 14, 27525120 steps, 393.3639991283417
I0728 08:09:56.561439 140143444526912 train.py:394] {'eval/walltime': 75.67054605484009, 'training/sps': 91979.7783969149, 'training/walltime': 334.35464787483215, 'training/entropy_loss': Array(-0.05298588, dtype=float32), 'training/policy_loss': Array(0.00100244, dtype=float32), 'training/total_loss': Array(155335.66, dtype=float32), 'training/v_loss': Array(155335.69, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2963131 , 0.11150622], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.03331 ,  9.638386], dtype=float32), 'eval/episode_reward': Array([-24729.504,   8656.844], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29158744, 0.11445945], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.033583641052246, 'eval/sps': 31733.567812320976}
I0728 08:09:56.563287 140143444526912 train.py:379] starting iteration 15, 29491200 steps, 418.77927470207214
I0728 08:10:21.951988 140143444526912 train.py:394] {'eval/walltime': 79.69354057312012, 'training/sps': 92039.78291330431, 'training/walltime': 355.7158455848694, 'training/entropy_loss': Array(-0.05316115, dtype=float32), 'training/policy_loss': Array(0.00090548, dtype=float32), 'training/total_loss': Array(153787.94, dtype=float32), 'training/v_loss': Array(153787.97, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30915076, 0.1309783 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.127197, 11.231457], dtype=float32), 'eval/episode_reward': Array([-24813.477,   9242.488], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30514842, 0.13350117], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022994518280029, 'eval/sps': 31817.095305097377}
I0728 08:10:21.953845 140143444526912 train.py:379] starting iteration 16, 31457280 steps, 444.1698327064514
I0728 08:10:47.347453 140143444526912 train.py:394] {'eval/walltime': 83.71164345741272, 'training/sps': 91998.43486694046, 'training/walltime': 377.0866439342499, 'training/entropy_loss': Array(-0.05329426, dtype=float32), 'training/policy_loss': Array(0.00084084, dtype=float32), 'training/total_loss': Array(157739.58, dtype=float32), 'training/v_loss': Array(157739.62, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.298837  , 0.13614255], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.225122, 11.668101], dtype=float32), 'eval/episode_reward': Array([-24095.957,   9066.851], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2939071 , 0.13971782], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0181028842926025, 'eval/sps': 31855.829401574603}
I0728 08:10:47.349303 140143444526912 train.py:379] starting iteration 17, 33423360 steps, 469.56529092788696
I0728 08:11:12.727677 140143444526912 train.py:394] {'eval/walltime': 87.74323773384094, 'training/sps': 92121.1323505351, 'training/walltime': 398.4289782047272, 'training/entropy_loss': Array(-0.05348342, dtype=float32), 'training/policy_loss': Array(0.00049945, dtype=float32), 'training/total_loss': Array(155099.33, dtype=float32), 'training/v_loss': Array(155099.38, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3111062 , 0.13209693], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.287617, 11.290871], dtype=float32), 'eval/episode_reward': Array([-24860.73,   9568.48], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3071798 , 0.13427997], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.031594276428223, 'eval/sps': 31749.22654007763}
I0728 08:11:12.729538 140143444526912 train.py:379] starting iteration 18, 35389440 steps, 494.94552636146545
I0728 08:11:38.159926 140143444526912 train.py:394] {'eval/walltime': 91.7686710357666, 'training/sps': 91870.36047269925, 'training/walltime': 419.8295691013336, 'training/entropy_loss': Array(-0.05367939, dtype=float32), 'training/policy_loss': Array(0.00039656, dtype=float32), 'training/total_loss': Array(157510.25, dtype=float32), 'training/v_loss': Array(157510.3, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29851335, 0.1397687 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.164682, 12.009038], dtype=float32), 'eval/episode_reward': Array([-24528.094,  10330.312], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29344133, 0.14328001], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.025433301925659, 'eval/sps': 31797.819116458402}
I0728 08:11:38.161782 140143444526912 train.py:379] starting iteration 19, 37355520 steps, 520.3777701854706
I0728 08:12:03.548851 140143444526912 train.py:394] {'eval/walltime': 95.79153752326965, 'training/sps': 92055.00154436435, 'training/walltime': 441.1872353553772, 'training/entropy_loss': Array(-0.05388452, dtype=float32), 'training/policy_loss': Array(0.00022343, dtype=float32), 'training/total_loss': Array(157236.64, dtype=float32), 'training/v_loss': Array(157236.69, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28613758, 0.1231525 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.147934, 10.538675], dtype=float32), 'eval/episode_reward': Array([-23672.193,   9139.478], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28111672, 0.12625898], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022866487503052, 'eval/sps': 31818.107908286107}
I0728 08:12:03.550750 140143444526912 train.py:379] starting iteration 20, 39321600 steps, 545.7667374610901
I0728 08:12:28.972567 140143444526912 train.py:394] {'eval/walltime': 99.7931911945343, 'training/sps': 91806.05369612914, 'training/walltime': 462.6028165817261, 'training/entropy_loss': Array(-0.0540241, dtype=float32), 'training/policy_loss': Array(0.00017487, dtype=float32), 'training/total_loss': Array(153121.66, dtype=float32), 'training/v_loss': Array(153121.7, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30334705, 0.13034752], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.652618, 11.202617], dtype=float32), 'eval/episode_reward': Array([-25075.416,   9367.472], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29854792, 0.13342687], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.001653671264648, 'eval/sps': 31986.776096879963}
I0728 08:12:28.974440 140143444526912 train.py:379] starting iteration 21, 41287680 steps, 571.1904270648956
I0728 08:12:54.444708 140143444526912 train.py:394] {'eval/walltime': 103.80043387413025, 'training/sps': 91629.33279595064, 'training/walltime': 484.05970096588135, 'training/entropy_loss': Array(-0.05409012, dtype=float32), 'training/policy_loss': Array(0.00014589, dtype=float32), 'training/total_loss': Array(155931.47, dtype=float32), 'training/v_loss': Array(155931.53, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30909848, 0.11677789], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.194138,  9.962554], dtype=float32), 'eval/episode_reward': Array([-25055.691,   8263.255], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.305566  , 0.11830354], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.007242679595947, 'eval/sps': 31942.163286428742}
I0728 08:12:54.448423 140143444526912 train.py:379] starting iteration 22, 43253760 steps, 596.6644110679626
I0728 08:13:19.884891 140143444526912 train.py:394] {'eval/walltime': 107.82514977455139, 'training/sps': 91842.2973900157, 'training/walltime': 505.4668309688568, 'training/entropy_loss': Array(-0.05418615, dtype=float32), 'training/policy_loss': Array(0.00010982, dtype=float32), 'training/total_loss': Array(155544.25, dtype=float32), 'training/v_loss': Array(155544.28, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29415601, 0.131612  ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.838785, 11.333711], dtype=float32), 'eval/episode_reward': Array([-23494.354,   9692.71 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28871998, 0.13561359], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024715900421143, 'eval/sps': 31803.487045285903}
I0728 08:13:19.886787 140143444526912 train.py:379] starting iteration 23, 45219840 steps, 622.1027753353119
I0728 08:13:45.333510 140143444526912 train.py:394] {'eval/walltime': 111.85291242599487, 'training/sps': 91811.2072731276, 'training/walltime': 526.8812100887299, 'training/entropy_loss': Array(-0.0542645, dtype=float32), 'training/policy_loss': Array(0.00010211, dtype=float32), 'training/total_loss': Array(153508.28, dtype=float32), 'training/v_loss': Array(153508.31, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30297762, 0.12132655], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.620903, 10.364345], dtype=float32), 'eval/episode_reward': Array([-24375.793,   9343.932], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29859075, 0.12381952], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0277626514434814, 'eval/sps': 31779.429692592978}
I0728 08:13:45.335379 140143444526912 train.py:379] starting iteration 24, 47185920 steps, 647.5513663291931
I0728 08:14:10.752531 140143444526912 train.py:394] {'eval/walltime': 115.84829235076904, 'training/sps': 91798.42456166235, 'training/walltime': 548.2985711097717, 'training/entropy_loss': Array(-0.05435121, dtype=float32), 'training/policy_loss': Array(4.369701e-05, dtype=float32), 'training/total_loss': Array(153374.33, dtype=float32), 'training/v_loss': Array(153374.38, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30147675, 0.13197592], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.506826, 11.350289], dtype=float32), 'eval/episode_reward': Array([-24586.096,   9285.868], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29673117, 0.13472745], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.99537992477417, 'eval/sps': 32037.003341361815}
I0728 08:14:10.754435 140143444526912 train.py:379] starting iteration 25, 49152000 steps, 672.9704217910767
I0728 08:14:36.155535 140143444526912 train.py:394] {'eval/walltime': 119.84381818771362, 'training/sps': 91867.45176504292, 'training/walltime': 569.69983959198, 'training/entropy_loss': Array(-0.05443729, dtype=float32), 'training/policy_loss': Array(4.05673e-05, dtype=float32), 'training/total_loss': Array(151467.19, dtype=float32), 'training/v_loss': Array(151467.25, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29937965, 0.13156624], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.275528, 11.319499], dtype=float32), 'eval/episode_reward': Array([-24482.568,  10006.65 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2944908, 0.1343623], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.99552583694458, 'eval/sps': 32035.8333855458}
I0728 08:14:36.184593 140143444526912 train.py:379] starting iteration 26, 51118080 steps, 698.4005801677704
I0728 08:15:01.606046 140143444526912 train.py:394] {'eval/walltime': 123.85245275497437, 'training/sps': 91838.4290234632, 'training/walltime': 591.1078712940216, 'training/entropy_loss': Array(-0.05442132, dtype=float32), 'training/policy_loss': Array(1.1308828e-05, dtype=float32), 'training/total_loss': Array(154169.89, dtype=float32), 'training/v_loss': Array(154169.94, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30180058, 0.1318586 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.45514 , 11.364187], dtype=float32), 'eval/episode_reward': Array([-24904.746,   9561.015], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29677826, 0.13544196], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.008634567260742, 'eval/sps': 31931.07225223262}
I0728 08:15:01.608026 140143444526912 train.py:379] starting iteration 27, 53084160 steps, 723.8240134716034
I0728 08:15:27.056224 140143444526912 train.py:394] {'eval/walltime': 127.87800097465515, 'training/sps': 91794.53741001517, 'training/walltime': 612.5261392593384, 'training/entropy_loss': Array(-0.05435628, dtype=float32), 'training/policy_loss': Array(1.610445e-05, dtype=float32), 'training/total_loss': Array(154087.06, dtype=float32), 'training/v_loss': Array(154087.12, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3125888 , 0.12596905], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.471195, 10.785941], dtype=float32), 'eval/episode_reward': Array([-25397.07 ,   9086.731], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30852893, 0.12844801], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.025548219680786, 'eval/sps': 31796.911380718728}
I0728 08:15:27.058074 140143444526912 train.py:379] starting iteration 28, 55050240 steps, 749.2740623950958
I0728 08:15:52.484616 140143444526912 train.py:394] {'eval/walltime': 131.90428233146667, 'training/sps': 91891.64406060481, 'training/walltime': 633.9217734336853, 'training/entropy_loss': Array(-0.05441564, dtype=float32), 'training/policy_loss': Array(-4.1184376e-05, dtype=float32), 'training/total_loss': Array(151222.75, dtype=float32), 'training/v_loss': Array(151222.8, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30823833, 0.14292791], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.999756, 12.29679 ], dtype=float32), 'eval/episode_reward': Array([-25384.258,  10333.331], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3030337 , 0.14678927], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.026281356811523, 'eval/sps': 31791.121547790997}
I0728 08:15:52.486496 140143444526912 train.py:379] starting iteration 29, 57016320 steps, 774.7024841308594
I0728 08:16:17.896932 140143444526912 train.py:394] {'eval/walltime': 135.89675450325012, 'training/sps': 91815.67240260677, 'training/walltime': 655.3351111412048, 'training/entropy_loss': Array(-0.05437982, dtype=float32), 'training/policy_loss': Array(-2.451278e-05, dtype=float32), 'training/total_loss': Array(153843.08, dtype=float32), 'training/v_loss': Array(153843.12, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30542088, 0.12392288], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.80167 , 10.610067], dtype=float32), 'eval/episode_reward': Array([-24943.992,   8923.85 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30106673, 0.12613186], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9924721717834473, 'eval/sps': 32060.336175824134}
I0728 08:16:17.898810 140143444526912 train.py:379] starting iteration 30, 58982400 steps, 800.1147975921631
I0728 08:16:43.344177 140143444526912 train.py:394] {'eval/walltime': 139.89858198165894, 'training/sps': 91707.26951369655, 'training/walltime': 676.7737605571747, 'training/entropy_loss': Array(-0.05435018, dtype=float32), 'training/policy_loss': Array(1.0852331e-05, dtype=float32), 'training/total_loss': Array(149734.95, dtype=float32), 'training/v_loss': Array(149735., dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30551142, 0.13929811], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.803774 , 11.9917345], dtype=float32), 'eval/episode_reward': Array([-24573.312,   9647.811], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3005196 , 0.14278375], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0018274784088135, 'eval/sps': 31985.386849033963}
I0728 08:16:43.348397 140143444526912 train.py:379] starting iteration 31, 60948480 steps, 825.5643694400787
I0728 08:17:08.734012 140143444526912 train.py:394] {'eval/walltime': 143.8864221572876, 'training/sps': 91904.17312495736, 'training/walltime': 698.1664779186249, 'training/entropy_loss': Array(-0.05436224, dtype=float32), 'training/policy_loss': Array(-7.6173324e-05, dtype=float32), 'training/total_loss': Array(149625.1, dtype=float32), 'training/v_loss': Array(149625.16, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31980878, 0.14604692], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.028765, 12.555268], dtype=float32), 'eval/episode_reward': Array([-25852.676,  10225.087], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3151645 , 0.14929159], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.987840175628662, 'eval/sps': 32097.575219353286}
I0728 08:17:08.735984 140143444526912 train.py:379] starting iteration 32, 62914560 steps, 850.9519712924957
I0728 08:17:34.148020 140143444526912 train.py:394] {'eval/walltime': 147.88216495513916, 'training/sps': 91824.43934480475, 'training/walltime': 719.5777711868286, 'training/entropy_loss': Array(-0.0543595, dtype=float32), 'training/policy_loss': Array(1.3848492e-05, dtype=float32), 'training/total_loss': Array(150218.81, dtype=float32), 'training/v_loss': Array(150218.86, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3021299 , 0.12504308], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.521324, 10.720002], dtype=float32), 'eval/episode_reward': Array([-24796.262,   8747.765], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29761353, 0.12756063], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9957427978515625, 'eval/sps': 32034.093903347144}
I0728 08:17:34.150016 140143444526912 train.py:379] starting iteration 33, 64880640 steps, 876.3660037517548
I0728 08:17:59.555743 140143444526912 train.py:394] {'eval/walltime': 151.88279151916504, 'training/sps': 91870.64910187134, 'training/walltime': 740.9782948493958, 'training/entropy_loss': Array(-0.05438165, dtype=float32), 'training/policy_loss': Array(-7.4832005e-06, dtype=float32), 'training/total_loss': Array(147969.9, dtype=float32), 'training/v_loss': Array(147969.94, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29415226, 0.13293868], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.827797, 11.413639], dtype=float32), 'eval/episode_reward': Array([-23985.336,   9054.06 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28871158, 0.13672234], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.000626564025879, 'eval/sps': 31994.988272834955}
I0728 08:17:59.557752 140143444526912 train.py:379] starting iteration 34, 66846720 steps, 901.7737395763397
I0728 08:18:25.045862 140143444526912 train.py:394] {'eval/walltime': 155.9089765548706, 'training/sps': 91629.13934955483, 'training/walltime': 762.435224533081, 'training/entropy_loss': Array(-0.05437378, dtype=float32), 'training/policy_loss': Array(6.8625577e-06, dtype=float32), 'training/total_loss': Array(148685.06, dtype=float32), 'training/v_loss': Array(148685.11, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29286656, 0.14168978], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.659473, 12.21795 ], dtype=float32), 'eval/episode_reward': Array([-23360.836,  10198.792], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28692567, 0.14625601], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.026185035705566, 'eval/sps': 31791.882107963953}
I0728 08:18:25.047859 140143444526912 train.py:379] starting iteration 35, 68812800 steps, 927.2638471126556
I0728 08:18:50.442283 140143444526912 train.py:394] {'eval/walltime': 159.93279266357422, 'training/sps': 92019.92570972543, 'training/walltime': 783.8010318279266, 'training/entropy_loss': Array(-0.0543943, dtype=float32), 'training/policy_loss': Array(4.324793e-05, dtype=float32), 'training/total_loss': Array(148152.92, dtype=float32), 'training/v_loss': Array(148152.97, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29234216, 0.11527409], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.661991,  9.976991], dtype=float32), 'eval/episode_reward': Array([-23510.549,   8412.477], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2877684 , 0.11793981], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023816108703613, 'eval/sps': 31810.598830083924}
I0728 08:18:50.444299 140143444526912 train.py:379] starting iteration 36, 70778880 steps, 952.6602873802185
I0728 08:19:15.862509 140143444526912 train.py:394] {'eval/walltime': 163.94118332862854, 'training/sps': 91851.902208078, 'training/walltime': 805.2059233188629, 'training/entropy_loss': Array(-0.0544317, dtype=float32), 'training/policy_loss': Array(-1.48526005e-05, dtype=float32), 'training/total_loss': Array(146110.53, dtype=float32), 'training/v_loss': Array(146110.56, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3150445 , 0.14124095], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.62195 , 12.091825], dtype=float32), 'eval/episode_reward': Array([-25367.92 ,  10002.203], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31086177, 0.14361665], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.008390665054321, 'eval/sps': 31933.01519133873}
I0728 08:19:15.864511 140143444526912 train.py:379] starting iteration 37, 72744960 steps, 978.0804991722107
I0728 08:19:41.196901 140143444526912 train.py:394] {'eval/walltime': 167.9497992992401, 'training/sps': 92233.18100441026, 'training/walltime': 826.5223300457001, 'training/entropy_loss': Array(-0.05441422, dtype=float32), 'training/policy_loss': Array(1.755113e-05, dtype=float32), 'training/total_loss': Array(146005.52, dtype=float32), 'training/v_loss': Array(146005.56, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30623418, 0.13330044], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.851316, 11.440597], dtype=float32), 'eval/episode_reward': Array([-24848.348,   9122.727], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30128866, 0.136768  ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.008615970611572, 'eval/sps': 31931.220385890883}
I0728 08:19:41.201203 140143444526912 train.py:379] starting iteration 38, 74711040 steps, 1003.4171757698059
I0728 08:20:06.601672 140143444526912 train.py:394] {'eval/walltime': 171.96999526023865, 'training/sps': 91986.41775932325, 'training/walltime': 847.8959202766418, 'training/entropy_loss': Array(-0.05445404, dtype=float32), 'training/policy_loss': Array(-6.769281e-05, dtype=float32), 'training/total_loss': Array(147106.25, dtype=float32), 'training/v_loss': Array(147106.3, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31669146, 0.14673941], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.750397, 12.62142 ], dtype=float32), 'eval/episode_reward': Array([-25157.184,  10076.068], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31184813, 0.14983547], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020195960998535, 'eval/sps': 31839.243967651615}
I0728 08:20:06.603528 140143444526912 train.py:379] starting iteration 39, 76677120 steps, 1028.8195157051086
I0728 08:20:32.007273 140143444526912 train.py:394] {'eval/walltime': 175.99719619750977, 'training/sps': 91993.11760226698, 'training/walltime': 869.2679538726807, 'training/entropy_loss': Array(-0.05443459, dtype=float32), 'training/policy_loss': Array(-3.061173e-05, dtype=float32), 'training/total_loss': Array(145396.53, dtype=float32), 'training/v_loss': Array(145396.6, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33297986, 0.15198982], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.177208, 13.031684], dtype=float32), 'eval/episode_reward': Array([-26231.94 ,  10767.069], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32879913, 0.154631  ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.027200937271118, 'eval/sps': 31783.862288911365}
I0728 08:20:32.009144 140143444526912 train.py:379] starting iteration 40, 78643200 steps, 1054.2251315116882
I0728 08:20:57.420576 140143444526912 train.py:394] {'eval/walltime': 180.01623964309692, 'training/sps': 91928.94123582962, 'training/walltime': 890.6549074649811, 'training/entropy_loss': Array(-0.0544332, dtype=float32), 'training/policy_loss': Array(-0.00010355, dtype=float32), 'training/total_loss': Array(143807.33, dtype=float32), 'training/v_loss': Array(143807.38, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31071037, 0.1334654 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.202272, 11.47367 ], dtype=float32), 'eval/episode_reward': Array([-25184.303,   9281.373], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3060248 , 0.13646084], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019043445587158, 'eval/sps': 31848.37430422451}
I0728 08:20:57.422713 140143444526912 train.py:379] starting iteration 41, 80609280 steps, 1079.6386976242065
I0728 08:21:22.841794 140143444526912 train.py:394] {'eval/walltime': 184.04493927955627, 'training/sps': 91933.20566875808, 'training/walltime': 912.0408689975739, 'training/entropy_loss': Array(-0.05447017, dtype=float32), 'training/policy_loss': Array(-3.4777982e-05, dtype=float32), 'training/total_loss': Array(142309.28, dtype=float32), 'training/v_loss': Array(142309.33, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31984136, 0.12469048], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.054207, 10.69599 ], dtype=float32), 'eval/episode_reward': Array([-25967.152,   9027.438], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3160333 , 0.12647983], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.028699636459351, 'eval/sps': 31772.038511288385}
I0728 08:21:22.843659 140143444526912 train.py:379] starting iteration 42, 82575360 steps, 1105.0596466064453
I0728 08:21:48.266853 140143444526912 train.py:394] {'eval/walltime': 188.04289960861206, 'training/sps': 91784.93743347019, 'training/walltime': 933.4613771438599, 'training/entropy_loss': Array(-0.0544766, dtype=float32), 'training/policy_loss': Array(-5.314041e-05, dtype=float32), 'training/total_loss': Array(140896.62, dtype=float32), 'training/v_loss': Array(140896.67, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2986142, 0.1268808], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.219925, 10.916897], dtype=float32), 'eval/episode_reward': Array([-23656.408,   9169.555], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2935956 , 0.13024004], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.997960329055786, 'eval/sps': 32016.325692313774}
I0728 08:21:48.268785 140143444526912 train.py:379] starting iteration 43, 84541440 steps, 1130.4847736358643
I0728 08:22:13.723832 140143444526912 train.py:394] {'eval/walltime': 192.05090594291687, 'training/sps': 91697.6520925625, 'training/walltime': 954.9022750854492, 'training/entropy_loss': Array(-0.05446894, dtype=float32), 'training/policy_loss': Array(-0.00010193, dtype=float32), 'training/total_loss': Array(141462.33, dtype=float32), 'training/v_loss': Array(141462.38, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29089487, 0.11686729], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.587427,  9.99662 ], dtype=float32), 'eval/episode_reward': Array([-23928.305,   8936.686], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28599727, 0.11967532], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.00800633430481, 'eval/sps': 31936.077272243547}
I0728 08:22:13.727803 140143444526912 train.py:379] starting iteration 44, 86507520 steps, 1155.943775653839
I0728 08:22:39.124607 140143444526912 train.py:394] {'eval/walltime': 196.05069756507874, 'training/sps': 91907.08006116754, 'training/walltime': 976.2943158149719, 'training/entropy_loss': Array(-0.05443957, dtype=float32), 'training/policy_loss': Array(-4.485729e-05, dtype=float32), 'training/total_loss': Array(139339.66, dtype=float32), 'training/v_loss': Array(139339.7, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30285758, 0.12506124], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.580925, 10.769131], dtype=float32), 'eval/episode_reward': Array([-24212.773,   9326.068], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29821414, 0.12779689], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9997916221618652, 'eval/sps': 32001.66710955225}
I0728 08:22:39.126462 140143444526912 train.py:379] starting iteration 45, 88473600 steps, 1181.3424501419067
I0728 08:23:04.586127 140143444526912 train.py:394] {'eval/walltime': 200.07552909851074, 'training/sps': 91742.49928456348, 'training/walltime': 997.7247326374054, 'training/entropy_loss': Array(-0.05444062, dtype=float32), 'training/policy_loss': Array(-6.4606385e-05, dtype=float32), 'training/total_loss': Array(136726.56, dtype=float32), 'training/v_loss': Array(136726.6, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3144606 , 0.13038312], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.563198, 11.200525], dtype=float32), 'eval/episode_reward': Array([-25107.436,   9433.621], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3102994 , 0.13277023], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024831533432007, 'eval/sps': 31802.57333425664}
I0728 08:23:04.588089 140143444526912 train.py:379] starting iteration 46, 90439680 steps, 1206.804077386856
I0728 08:23:30.030049 140143444526912 train.py:394] {'eval/walltime': 204.11389255523682, 'training/sps': 91877.83368681901, 'training/walltime': 1019.1235828399658, 'training/entropy_loss': Array(-0.05445162, dtype=float32), 'training/policy_loss': Array(-6.9387665e-05, dtype=float32), 'training/total_loss': Array(138884.23, dtype=float32), 'training/v_loss': Array(138884.28, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3242423 , 0.12251239], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.401209, 10.501669], dtype=float32), 'eval/episode_reward': Array([-25596.559,   8768.726], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31986976, 0.12512355], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.038363456726074, 'eval/sps': 31696.007893199978}
I0728 08:23:30.032011 140143444526912 train.py:379] starting iteration 47, 92405760 steps, 1232.247998714447
I0728 08:23:55.459636 140143444526912 train.py:394] {'eval/walltime': 208.12190294265747, 'training/sps': 91808.81235171927, 'training/walltime': 1040.5385205745697, 'training/entropy_loss': Array(-0.05448563, dtype=float32), 'training/policy_loss': Array(-5.1722454e-05, dtype=float32), 'training/total_loss': Array(138095.92, dtype=float32), 'training/v_loss': Array(138095.97, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3260915, 0.1267803], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.60424 , 10.876404], dtype=float32), 'eval/episode_reward': Array([-25637.248,   9630.814], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32183546, 0.12934917], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.008010387420654, 'eval/sps': 31936.044976763173}
I0728 08:23:55.461671 140143444526912 train.py:379] starting iteration 48, 94371840 steps, 1257.6776583194733
I0728 08:24:20.907122 140143444526912 train.py:394] {'eval/walltime': 212.1327874660492, 'training/sps': 91744.24158005649, 'training/walltime': 1061.9685304164886, 'training/entropy_loss': Array(-0.05448315, dtype=float32), 'training/policy_loss': Array(-3.132499e-05, dtype=float32), 'training/total_loss': Array(135623.67, dtype=float32), 'training/v_loss': Array(135623.73, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3092057 , 0.12056191], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.143057, 10.390739], dtype=float32), 'eval/episode_reward': Array([-24628.615,   8911.452], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30475742, 0.12338188], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.010884523391724, 'eval/sps': 31913.160115554605}
I0728 08:24:20.909146 140143444526912 train.py:379] starting iteration 49, 96337920 steps, 1283.1251337528229
I0728 08:24:46.347374 140143444526912 train.py:394] {'eval/walltime': 216.14567947387695, 'training/sps': 91784.97523276045, 'training/walltime': 1083.3890297412872, 'training/entropy_loss': Array(-0.05446759, dtype=float32), 'training/policy_loss': Array(-1.4502274e-05, dtype=float32), 'training/total_loss': Array(136926.38, dtype=float32), 'training/v_loss': Array(136926.44, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29304838, 0.12690514], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.735172, 10.899173], dtype=float32), 'eval/episode_reward': Array([-23480.547,   9406.641], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28828588, 0.12975648], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.012892007827759, 'eval/sps': 31897.195277200695}
I0728 08:24:46.349443 140143444526912 train.py:379] starting iteration 50, 98304000 steps, 1308.565430879593
I0728 08:25:11.810699 140143444526912 train.py:394] {'eval/walltime': 220.18364882469177, 'training/sps': 91793.61880686399, 'training/walltime': 1104.8075120449066, 'training/entropy_loss': Array(-0.0544273, dtype=float32), 'training/policy_loss': Array(8.091432e-06, dtype=float32), 'training/total_loss': Array(133947.22, dtype=float32), 'training/v_loss': Array(133947.28, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2969725 , 0.13229012], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.089634, 11.322577], dtype=float32), 'eval/episode_reward': Array([-24037.91 ,   9550.003], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29242277, 0.13472714], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.037969350814819, 'eval/sps': 31699.101424375833}
I0728 08:25:11.851289 140143444526912 train.py:379] starting iteration 51, 100270080 steps, 1334.067275762558
I0728 08:25:37.226148 140143444526912 train.py:394] {'eval/walltime': 224.1738154888153, 'training/sps': 91959.89272200891, 'training/walltime': 1126.1872673034668, 'training/entropy_loss': Array(-0.05442562, dtype=float32), 'training/policy_loss': Array(2.912806e-05, dtype=float32), 'training/total_loss': Array(134064.4, dtype=float32), 'training/v_loss': Array(134064.47, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30125135, 0.12798306], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.429012 , 11.0630865], dtype=float32), 'eval/episode_reward': Array([-24003.71 ,   9298.882], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2965391 , 0.13079236], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.990166664123535, 'eval/sps': 32078.860552586968}
I0728 08:25:37.228357 140143444526912 train.py:379] starting iteration 52, 102236160 steps, 1359.444343805313
I0728 08:26:02.650610 140143444526912 train.py:394] {'eval/walltime': 228.1706771850586, 'training/sps': 91784.45319666802, 'training/walltime': 1147.6078884601593, 'training/entropy_loss': Array(-0.05440485, dtype=float32), 'training/policy_loss': Array(5.739628e-06, dtype=float32), 'training/total_loss': Array(131896.33, dtype=float32), 'training/v_loss': Array(131896.39, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2870051 , 0.12950923], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.244715, 11.154043], dtype=float32), 'eval/episode_reward': Array([-23482.895,   9546.963], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28201562, 0.13258462], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.996861696243286, 'eval/sps': 32025.126143421283}
I0728 08:26:02.652570 140143444526912 train.py:379] starting iteration 53, 104202240 steps, 1384.8685581684113
I0728 08:26:28.116565 140143444526912 train.py:394] {'eval/walltime': 232.18905234336853, 'training/sps': 91698.73396448379, 'training/walltime': 1169.0485334396362, 'training/entropy_loss': Array(-0.05435654, dtype=float32), 'training/policy_loss': Array(-7.118227e-05, dtype=float32), 'training/total_loss': Array(132281.88, dtype=float32), 'training/v_loss': Array(132281.92, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28198618, 0.14111263], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.782703, 12.135183], dtype=float32), 'eval/episode_reward': Array([-23591.523,  10538.672], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27640688, 0.14482586], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0183751583099365, 'eval/sps': 31853.670938438394}
I0728 08:26:28.118561 140143444526912 train.py:379] starting iteration 54, 106168320 steps, 1410.3345487117767
I0728 08:26:53.589696 140143444526912 train.py:394] {'eval/walltime': 236.21868515014648, 'training/sps': 91713.65232816196, 'training/walltime': 1190.485690832138, 'training/entropy_loss': Array(-0.05437194, dtype=float32), 'training/policy_loss': Array(-1.5141839e-05, dtype=float32), 'training/total_loss': Array(129786.55, dtype=float32), 'training/v_loss': Array(129786.586, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31448567, 0.13898431], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.58799 , 11.979361], dtype=float32), 'eval/episode_reward': Array([-25179.83 ,  10553.108], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30983436, 0.14233407], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.029632806777954, 'eval/sps': 31764.68083759405}
I0728 08:26:53.591542 140143444526912 train.py:379] starting iteration 55, 108134400 steps, 1435.8075299263
I0728 08:27:19.085115 140143444526912 train.py:394] {'eval/walltime': 240.2310130596161, 'training/sps': 91545.15596914897, 'training/walltime': 1211.9623050689697, 'training/entropy_loss': Array(-0.05446059, dtype=float32), 'training/policy_loss': Array(-5.2068564e-05, dtype=float32), 'training/total_loss': Array(129091.3, dtype=float32), 'training/v_loss': Array(129091.35, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31991196, 0.1275274 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.044552, 10.962694], dtype=float32), 'eval/episode_reward': Array([-25986.168,   9215.673], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3155047 , 0.13027617], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0123279094696045, 'eval/sps': 31901.679745043697}
I0728 08:27:19.086943 140143444526912 train.py:379] starting iteration 56, 110100480 steps, 1461.3029305934906
I0728 08:27:44.511069 140143444526912 train.py:394] {'eval/walltime': 244.247793674469, 'training/sps': 91861.05571239891, 'training/walltime': 1233.3650636672974, 'training/entropy_loss': Array(-0.05440983, dtype=float32), 'training/policy_loss': Array(1.2176812e-06, dtype=float32), 'training/total_loss': Array(126855.016, dtype=float32), 'training/v_loss': Array(126855.06, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29556274, 0.13315853], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.909752, 11.521239], dtype=float32), 'eval/episode_reward': Array([-24167.451,   9750.431], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29015908, 0.13668238], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016780614852905, 'eval/sps': 31866.315906498014}
I0728 08:27:44.512947 140143444526912 train.py:379] starting iteration 57, 112066560 steps, 1486.7289354801178
I0728 08:28:09.966325 140143444526912 train.py:394] {'eval/walltime': 248.26427793502808, 'training/sps': 91735.13174955682, 'training/walltime': 1254.7972016334534, 'training/entropy_loss': Array(-0.05441622, dtype=float32), 'training/policy_loss': Array(1.0407366e-05, dtype=float32), 'training/total_loss': Array(126914.05, dtype=float32), 'training/v_loss': Array(126914.09, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31298953, 0.1318443 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.440882, 11.287881], dtype=float32), 'eval/episode_reward': Array([-25381.414,   9575.192], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30843052, 0.134561  ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016484260559082, 'eval/sps': 31868.667146770495}
I0728 08:28:09.968190 140143444526912 train.py:379] starting iteration 58, 114032640 steps, 1512.184178352356
I0728 08:28:35.436995 140143444526912 train.py:394] {'eval/walltime': 252.2659831047058, 'training/sps': 91603.45692288644, 'training/walltime': 1276.2601470947266, 'training/entropy_loss': Array(-0.05441044, dtype=float32), 'training/policy_loss': Array(-2.6556172e-06, dtype=float32), 'training/total_loss': Array(126987.25, dtype=float32), 'training/v_loss': Array(126987.305, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30888438, 0.13910432], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.047356, 12.007778], dtype=float32), 'eval/episode_reward': Array([-25005.395,  10432.653], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30373427, 0.14287084], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.001705169677734, 'eval/sps': 31986.364455307463}
I0728 08:28:35.438841 140143444526912 train.py:379] starting iteration 59, 115998720 steps, 1537.6548292636871
I0728 08:29:00.866021 140143444526912 train.py:394] {'eval/walltime': 256.2678065299988, 'training/sps': 91783.80449081594, 'training/walltime': 1297.6809196472168, 'training/entropy_loss': Array(-0.05445855, dtype=float32), 'training/policy_loss': Array(-2.6211406e-05, dtype=float32), 'training/total_loss': Array(126234.55, dtype=float32), 'training/v_loss': Array(126234.59, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3304323 , 0.14201713], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.926636, 12.151172], dtype=float32), 'eval/episode_reward': Array([-26538.312 ,   9344.6875], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32631654, 0.14490168], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.001823425292969, 'eval/sps': 31985.419244385896}
I0728 08:29:00.867877 140143444526912 train.py:379] starting iteration 60, 117964800 steps, 1563.0838651657104
I0728 08:29:26.375419 140143444526912 train.py:394] {'eval/walltime': 260.31015372276306, 'training/sps': 91612.0327553283, 'training/walltime': 1319.141855955124, 'training/entropy_loss': Array(-0.0544863, dtype=float32), 'training/policy_loss': Array(-3.6031497e-06, dtype=float32), 'training/total_loss': Array(124967.83, dtype=float32), 'training/v_loss': Array(124967.875, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32361495, 0.13016063], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.390553, 11.215495], dtype=float32), 'eval/episode_reward': Array([-25607.924,   9098.839], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31969014, 0.13271673], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.042347192764282, 'eval/sps': 31664.771454841226}
I0728 08:29:26.377286 140143444526912 train.py:379] starting iteration 61, 119930880 steps, 1588.593274116516
I0728 08:29:51.831569 140143444526912 train.py:394] {'eval/walltime': 264.3070125579834, 'training/sps': 91653.2123895316, 'training/walltime': 1340.5931499004364, 'training/entropy_loss': Array(-0.05448718, dtype=float32), 'training/policy_loss': Array(-4.153396e-06, dtype=float32), 'training/total_loss': Array(124276.74, dtype=float32), 'training/v_loss': Array(124276.8, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3057844 , 0.12967177], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.822704, 11.162583], dtype=float32), 'eval/episode_reward': Array([-24610.885,   9346.963], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30084947, 0.1334141 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.996858835220337, 'eval/sps': 32025.149067578634}
I0728 08:29:51.835219 140143444526912 train.py:379] starting iteration 62, 121896960 steps, 1614.0511960983276
I0728 08:30:17.278588 140143444526912 train.py:394] {'eval/walltime': 268.319988489151, 'training/sps': 91762.41867832557, 'training/walltime': 1362.0189146995544, 'training/entropy_loss': Array(-0.05447304, dtype=float32), 'training/policy_loss': Array(-5.9334547e-05, dtype=float32), 'training/total_loss': Array(122879.36, dtype=float32), 'training/v_loss': Array(122879.42, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32442093, 0.14096016], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.393734, 12.164798], dtype=float32), 'eval/episode_reward': Array([-25685.75 ,   9559.801], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31937057, 0.14510514], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0129759311676025, 'eval/sps': 31896.528211360972}
I0728 08:30:17.280495 140143444526912 train.py:379] starting iteration 63, 123863040 steps, 1639.4964830875397
I0728 08:30:42.737129 140143444526912 train.py:394] {'eval/walltime': 272.33957076072693, 'training/sps': 91734.06636660494, 'training/walltime': 1383.451301574707, 'training/entropy_loss': Array(-0.05444507, dtype=float32), 'training/policy_loss': Array(-4.5241275e-05, dtype=float32), 'training/total_loss': Array(122045.67, dtype=float32), 'training/v_loss': Array(122045.73, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27316952, 0.13631445], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.01329 , 11.787657], dtype=float32), 'eval/episode_reward': Array([-22806.95 ,   9894.258], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2666832, 0.1415432], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019582271575928, 'eval/sps': 31844.10502184248}
I0728 08:30:42.739004 140143444526912 train.py:379] starting iteration 64, 125829120 steps, 1664.9549922943115
I0728 08:31:08.168896 140143444526912 train.py:394] {'eval/walltime': 276.3587064743042, 'training/sps': 91845.83873468028, 'training/walltime': 1404.8576061725616, 'training/entropy_loss': Array(-0.0544962, dtype=float32), 'training/policy_loss': Array(-2.6377722e-05, dtype=float32), 'training/total_loss': Array(121652.94, dtype=float32), 'training/v_loss': Array(121652.99, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30166602, 0.13089257], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.445768, 11.254917], dtype=float32), 'eval/episode_reward': Array([-25012.387,   9432.782], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29673925, 0.13388588], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0191357135772705, 'eval/sps': 31847.643155615755}
I0728 08:31:08.170767 140143444526912 train.py:379] starting iteration 65, 127795200 steps, 1690.386754989624
I0728 08:31:33.593022 140143444526912 train.py:394] {'eval/walltime': 280.35212659835815, 'training/sps': 91767.11598242591, 'training/walltime': 1426.2822742462158, 'training/entropy_loss': Array(-0.05450324, dtype=float32), 'training/policy_loss': Array(-9.338296e-05, dtype=float32), 'training/total_loss': Array(118890.05, dtype=float32), 'training/v_loss': Array(118890.09, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30117217, 0.13285525], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.413923, 11.402433], dtype=float32), 'eval/episode_reward': Array([-24331.441,   9370.837], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29596004, 0.1364464 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.993420124053955, 'eval/sps': 32052.72573977508}
I0728 08:31:33.594874 140143444526912 train.py:379] starting iteration 66, 129761280 steps, 1715.8108620643616
I0728 08:31:59.027282 140143444526912 train.py:394] {'eval/walltime': 284.3547420501709, 'training/sps': 91771.00489715062, 'training/walltime': 1447.7060344219208, 'training/entropy_loss': Array(-0.05452071, dtype=float32), 'training/policy_loss': Array(-4.927683e-05, dtype=float32), 'training/total_loss': Array(118480.08, dtype=float32), 'training/v_loss': Array(118480.13, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2889515 , 0.13142103], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.389923, 11.295869], dtype=float32), 'eval/episode_reward': Array([-23515.203,   9104.65 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2835191 , 0.13478616], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.002615451812744, 'eval/sps': 31979.090057734647}
I0728 08:31:59.031200 140143444526912 train.py:379] starting iteration 67, 131727360 steps, 1741.2471873760223
I0728 08:32:24.487410 140143444526912 train.py:394] {'eval/walltime': 288.38111877441406, 'training/sps': 91764.91942232261, 'training/walltime': 1469.1312153339386, 'training/entropy_loss': Array(-0.05448514, dtype=float32), 'training/policy_loss': Array(-4.969719e-05, dtype=float32), 'training/total_loss': Array(118202.836, dtype=float32), 'training/v_loss': Array(118202.89, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31554067, 0.13360888], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.641117, 11.491433], dtype=float32), 'eval/episode_reward': Array([-25621.658,   9894.319], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3108794 , 0.13716164], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.026376724243164, 'eval/sps': 31790.36855376718}
I0728 08:32:24.489286 140143444526912 train.py:379] starting iteration 68, 133693440 steps, 1766.7052736282349
I0728 08:32:49.967069 140143444526912 train.py:394] {'eval/walltime': 292.4016766548157, 'training/sps': 91646.00483456806, 'training/walltime': 1490.5841963291168, 'training/entropy_loss': Array(-0.05446532, dtype=float32), 'training/policy_loss': Array(9.981923e-07, dtype=float32), 'training/total_loss': Array(117854.63, dtype=float32), 'training/v_loss': Array(117854.69, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30424285, 0.13685086], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.704277, 11.764252], dtype=float32), 'eval/episode_reward': Array([-24462.32 ,  10266.613], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29930001, 0.14009072], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020557880401611, 'eval/sps': 31836.37788774083}
I0728 08:32:49.968914 140143444526912 train.py:379] starting iteration 69, 135659520 steps, 1792.1849014759064
I0728 08:33:15.407067 140143444526912 train.py:394] {'eval/walltime': 296.407598733902, 'training/sps': 91754.67938708822, 'training/walltime': 1512.0117683410645, 'training/entropy_loss': Array(-0.05451041, dtype=float32), 'training/policy_loss': Array(-1.897473e-05, dtype=float32), 'training/total_loss': Array(114874.516, dtype=float32), 'training/v_loss': Array(114874.58, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3031736 , 0.13244888], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.561018, 11.365496], dtype=float32), 'eval/episode_reward': Array([-24612.71 ,   9417.742], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29873544, 0.1348422 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.005922079086304, 'eval/sps': 31952.69340565782}
I0728 08:33:15.408936 140143444526912 train.py:379] starting iteration 70, 137625600 steps, 1817.6249248981476
I0728 08:33:40.834613 140143444526912 train.py:394] {'eval/walltime': 300.4105565547943, 'training/sps': 91793.44305841197, 'training/walltime': 1533.4302916526794, 'training/entropy_loss': Array(-0.05450039, dtype=float32), 'training/policy_loss': Array(-6.028804e-05, dtype=float32), 'training/total_loss': Array(113605.06, dtype=float32), 'training/v_loss': Array(113605.11, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30725852, 0.14027417], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.95913 , 12.015612], dtype=float32), 'eval/episode_reward': Array([-24880.871,   9969.976], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30268273, 0.14315844], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.002957820892334, 'eval/sps': 31976.3549173412}
I0728 08:33:40.836482 140143444526912 train.py:379] starting iteration 71, 139591680 steps, 1843.0524697303772
I0728 08:34:06.276831 140143444526912 train.py:394] {'eval/walltime': 304.4315536022186, 'training/sps': 91810.04914999072, 'training/walltime': 1554.8449409008026, 'training/entropy_loss': Array(-0.05449644, dtype=float32), 'training/policy_loss': Array(-3.083342e-05, dtype=float32), 'training/total_loss': Array(111519.805, dtype=float32), 'training/v_loss': Array(111519.86, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29097345, 0.122932  ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.558987, 10.581469], dtype=float32), 'eval/episode_reward': Array([-23814.998,   9105.232], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28564578, 0.12644194], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020997047424316, 'eval/sps': 31832.900768228985}
I0728 08:34:06.278705 140143444526912 train.py:379] starting iteration 72, 141557760 steps, 1868.4946916103363
I0728 08:34:31.700041 140143444526912 train.py:394] {'eval/walltime': 308.4592933654785, 'training/sps': 91921.35826612152, 'training/walltime': 1576.2336587905884, 'training/entropy_loss': Array(-0.05441108, dtype=float32), 'training/policy_loss': Array(-3.7459035e-05, dtype=float32), 'training/total_loss': Array(110980.15, dtype=float32), 'training/v_loss': Array(110980.2, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29951593, 0.13403207], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.305706, 11.503598], dtype=float32), 'eval/episode_reward': Array([-23989.264,   9590.151], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2945236 , 0.13726933], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.027739763259888, 'eval/sps': 31779.610283560633}
I0728 08:34:31.701894 140143444526912 train.py:379] starting iteration 73, 143523840 steps, 1893.9178822040558
I0728 08:34:57.102530 140143444526912 train.py:394] {'eval/walltime': 312.47768235206604, 'training/sps': 91967.62974770862, 'training/walltime': 1597.6116154193878, 'training/entropy_loss': Array(-0.05448134, dtype=float32), 'training/policy_loss': Array(-8.829432e-05, dtype=float32), 'training/total_loss': Array(113262.41, dtype=float32), 'training/v_loss': Array(113262.46, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30281383, 0.13035372], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.544611, 11.170733], dtype=float32), 'eval/episode_reward': Array([-24258.633,   9950.466], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29786736, 0.13371095], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.018388986587524, 'eval/sps': 31853.561322021116}
I0728 08:34:57.104387 140143444526912 train.py:379] starting iteration 74, 145489920 steps, 1919.3203752040863
I0728 08:35:22.499870 140143444526912 train.py:394] {'eval/walltime': 316.5073170661926, 'training/sps': 92039.25181108229, 'training/walltime': 1618.9729363918304, 'training/entropy_loss': Array(-0.05453429, dtype=float32), 'training/policy_loss': Array(-5.8372316e-06, dtype=float32), 'training/total_loss': Array(113353., dtype=float32), 'training/v_loss': Array(113353.055, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29672924, 0.14254238], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.01643 , 12.269678], dtype=float32), 'eval/episode_reward': Array([-23681.887,  10352.029], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29121584, 0.14644633], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.029634714126587, 'eval/sps': 31764.66580240479}
I0728 08:35:22.501714 140143444526912 train.py:379] starting iteration 75, 147456000 steps, 1944.7177021503448
I0728 08:35:47.901142 140143444526912 train.py:394] {'eval/walltime': 320.52478408813477, 'training/sps': 91971.05767443615, 'training/walltime': 1640.3500962257385, 'training/entropy_loss': Array(-0.05453099, dtype=float32), 'training/policy_loss': Array(-6.619627e-05, dtype=float32), 'training/total_loss': Array(108662.56, dtype=float32), 'training/v_loss': Array(108662.625, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3007686 , 0.11511036], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.399662 ,  9.8920555], dtype=float32), 'eval/episode_reward': Array([-24033.488,   8478.15 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2962432 , 0.11748514], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.017467021942139, 'eval/sps': 31860.87136519213}
I0728 08:35:47.945992 140143444526912 train.py:379] starting iteration 76, 149422080 steps, 1970.1619794368744
I0728 08:36:13.343408 140143444526912 train.py:394] {'eval/walltime': 324.51786708831787, 'training/sps': 91875.16710123797, 'training/walltime': 1661.7495675086975, 'training/entropy_loss': Array(-0.05455689, dtype=float32), 'training/policy_loss': Array(-4.8301896e-05, dtype=float32), 'training/total_loss': Array(107324.96, dtype=float32), 'training/v_loss': Array(107325.016, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31285116, 0.12091734], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.4506  , 10.348824], dtype=float32), 'eval/episode_reward': Array([-25337.809,   9476.049], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30864814, 0.12307302], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9930830001831055, 'eval/sps': 32055.431854066264}
I0728 08:36:13.345446 140143444526912 train.py:379] starting iteration 77, 151388160 steps, 1995.5614340305328
I0728 08:36:38.767621 140143444526912 train.py:394] {'eval/walltime': 328.51193404197693, 'training/sps': 91772.77380880533, 'training/walltime': 1683.1729147434235, 'training/entropy_loss': Array(-0.05455349, dtype=float32), 'training/policy_loss': Array(-4.2987645e-05, dtype=float32), 'training/total_loss': Array(105617.22, dtype=float32), 'training/v_loss': Array(105617.27, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29740453, 0.13510726], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.136518, 11.563293], dtype=float32), 'eval/episode_reward': Array([-23989.45 ,   9941.647], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2927022 , 0.13798936], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9940669536590576, 'eval/sps': 32047.534877385122}
I0728 08:36:38.769561 140143444526912 train.py:379] starting iteration 78, 153354240 steps, 2020.985549211502
I0728 08:37:04.154716 140143444526912 train.py:394] {'eval/walltime': 332.5006868839264, 'training/sps': 91914.52134455259, 'training/walltime': 1704.5632236003876, 'training/entropy_loss': Array(-0.0545317, dtype=float32), 'training/policy_loss': Array(-7.58806e-05, dtype=float32), 'training/total_loss': Array(105624.266, dtype=float32), 'training/v_loss': Array(105624.31, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32639518, 0.1374957 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.614826, 11.816913], dtype=float32), 'eval/episode_reward': Array([-25973.605,   9684.932], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32220978, 0.13985918], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.988752841949463, 'eval/sps': 32090.230974913273}
I0728 08:37:04.156918 140143444526912 train.py:379] starting iteration 79, 155320320 steps, 2046.3729050159454
I0728 08:37:29.591221 140143444526912 train.py:394] {'eval/walltime': 336.50188302993774, 'training/sps': 91750.515208478, 'training/walltime': 1725.9917681217194, 'training/entropy_loss': Array(-0.05450781, dtype=float32), 'training/policy_loss': Array(-5.4291577e-05, dtype=float32), 'training/total_loss': Array(102869.39, dtype=float32), 'training/v_loss': Array(102869.43, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31150293, 0.14437433], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.307255, 12.42563 ], dtype=float32), 'eval/episode_reward': Array([-25463.635,   9979.787], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30698752, 0.14707717], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0011961460113525, 'eval/sps': 31990.433692584294}
I0728 08:37:29.593098 140143444526912 train.py:379] starting iteration 80, 157286400 steps, 2071.809086561203
I0728 08:37:54.936452 140143444526912 train.py:394] {'eval/walltime': 340.49341082572937, 'training/sps': 92100.82137282488, 'training/walltime': 1747.3388090133667, 'training/entropy_loss': Array(-0.05455002, dtype=float32), 'training/policy_loss': Array(-5.809538e-05, dtype=float32), 'training/total_loss': Array(103026.55, dtype=float32), 'training/v_loss': Array(103026.59, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30180216, 0.11419492], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.49242 ,  9.843981], dtype=float32), 'eval/episode_reward': Array([-24488.229,   8297.745], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2971431 , 0.11728215], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.991527795791626, 'eval/sps': 32067.921494860642}
I0728 08:37:54.938410 140143444526912 train.py:379] starting iteration 81, 159252480 steps, 2097.1543979644775
I0728 08:38:20.321089 140143444526912 train.py:394] {'eval/walltime': 344.4752643108368, 'training/sps': 91888.85585101206, 'training/walltime': 1768.7350924015045, 'training/entropy_loss': Array(-0.05457971, dtype=float32), 'training/policy_loss': Array(-7.568025e-05, dtype=float32), 'training/total_loss': Array(102855.516, dtype=float32), 'training/v_loss': Array(102855.58, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31508592, 0.13649492], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.631687, 11.718393], dtype=float32), 'eval/episode_reward': Array([-25411.312,   9422.787], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31023562, 0.1395938 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.981853485107422, 'eval/sps': 32145.833712549782}
I0728 08:38:20.323026 140143444526912 train.py:379] starting iteration 82, 161218560 steps, 2122.5390145778656
I0728 08:38:45.698009 140143444526912 train.py:394] {'eval/walltime': 348.4810519218445, 'training/sps': 92025.7523673092, 'training/walltime': 1790.0995469093323, 'training/entropy_loss': Array(-0.05455002, dtype=float32), 'training/policy_loss': Array(-0.00017919, dtype=float32), 'training/total_loss': Array(101450.16, dtype=float32), 'training/v_loss': Array(101450.22, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29548672, 0.12282766], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.91564 , 10.548038], dtype=float32), 'eval/episode_reward': Array([-24243.416,   9066.543], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29061896, 0.12587845], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.00578761100769, 'eval/sps': 31953.76600802869}
I0728 08:38:45.699969 140143444526912 train.py:379] starting iteration 83, 163184640 steps, 2147.915956735611
I0728 08:39:11.079591 140143444526912 train.py:394] {'eval/walltime': 352.4711275100708, 'training/sps': 91938.96702220781, 'training/walltime': 1811.484168291092, 'training/entropy_loss': Array(-0.05455592, dtype=float32), 'training/policy_loss': Array(-8.557745e-05, dtype=float32), 'training/total_loss': Array(98641.15, dtype=float32), 'training/v_loss': Array(98641.21, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30306247, 0.1257509 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.635544, 10.780074], dtype=float32), 'eval/episode_reward': Array([-24814.586,   8797.169], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29823124, 0.12896968], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9900755882263184, 'eval/sps': 32079.592772050462}
I0728 08:39:11.081582 140143444526912 train.py:379] starting iteration 84, 165150720 steps, 2173.297569513321
I0728 08:39:36.439692 140143444526912 train.py:394] {'eval/walltime': 356.4607172012329, 'training/sps': 92029.32944208558, 'training/walltime': 1832.8477923870087, 'training/entropy_loss': Array(-0.05458156, dtype=float32), 'training/policy_loss': Array(-9.997583e-05, dtype=float32), 'training/total_loss': Array(96169.72, dtype=float32), 'training/v_loss': Array(96169.766, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3195393 , 0.12493744], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.995424 , 10.7996645], dtype=float32), 'eval/episode_reward': Array([-25599.02 ,   9458.572], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31490445, 0.12862907], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9895896911621094, 'eval/sps': 32083.49978534145}
I0728 08:39:36.441670 140143444526912 train.py:379] starting iteration 85, 167116800 steps, 2198.6576578617096
I0728 08:40:01.832390 140143444526912 train.py:394] {'eval/walltime': 360.4836506843567, 'training/sps': 92034.3304184182, 'training/walltime': 1854.2102556228638, 'training/entropy_loss': Array(-0.05459856, dtype=float32), 'training/policy_loss': Array(-0.00014532, dtype=float32), 'training/total_loss': Array(95930.734, dtype=float32), 'training/v_loss': Array(95930.79, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30606169, 0.12684013], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.863321, 10.939069], dtype=float32), 'eval/episode_reward': Array([-24702.97 ,   9598.958], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3012735 , 0.12993178], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022933483123779, 'eval/sps': 31817.578027814892}
I0728 08:40:01.834397 140143444526912 train.py:379] starting iteration 86, 169082880 steps, 2224.050384759903
I0728 08:40:27.205499 140143444526912 train.py:394] {'eval/walltime': 364.49657344818115, 'training/sps': 92073.85200397868, 'training/walltime': 1875.5635492801666, 'training/entropy_loss': Array(-0.05452685, dtype=float32), 'training/policy_loss': Array(-6.4706684e-05, dtype=float32), 'training/total_loss': Array(95335.586, dtype=float32), 'training/v_loss': Array(95335.64, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29156882, 0.12637116], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.636349 , 10.8663645], dtype=float32), 'eval/episode_reward': Array([-23561.992,   9473.68 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28622273, 0.12986962], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.012922763824463, 'eval/sps': 31896.950809492108}
I0728 08:40:27.207493 140143444526912 train.py:379] starting iteration 87, 171048960 steps, 2249.423481941223
I0728 08:40:52.622946 140143444526912 train.py:394] {'eval/walltime': 368.51724767684937, 'training/sps': 91916.10420684233, 'training/walltime': 1896.953489780426, 'training/entropy_loss': Array(-0.05458491, dtype=float32), 'training/policy_loss': Array(-0.0001525, dtype=float32), 'training/total_loss': Array(94245.016, dtype=float32), 'training/v_loss': Array(94245.06, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31353378, 0.13624021], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.470993, 11.726318], dtype=float32), 'eval/episode_reward': Array([-25605.441,  10317.891], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30890033, 0.13924158], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020674228668213, 'eval/sps': 31835.45662250733}
I0728 08:40:52.625005 140143444526912 train.py:379] starting iteration 88, 173015040 steps, 2274.8409926891327
I0728 08:41:18.032282 140143444526912 train.py:394] {'eval/walltime': 372.54027366638184, 'training/sps': 91960.1501233226, 'training/walltime': 1918.3331851959229, 'training/entropy_loss': Array(-0.05460868, dtype=float32), 'training/policy_loss': Array(-0.00012713, dtype=float32), 'training/total_loss': Array(93676.36, dtype=float32), 'training/v_loss': Array(93676.42, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28802955, 0.1288621 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.307793, 11.05975 ], dtype=float32), 'eval/episode_reward': Array([-23809.031,   9733.948], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28284308, 0.1322959 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023025989532471, 'eval/sps': 31816.84640691951}
I0728 08:41:18.034296 140143444526912 train.py:379] starting iteration 89, 174981120 steps, 2300.250284433365
I0728 08:41:43.424285 140143444526912 train.py:394] {'eval/walltime': 376.55759048461914, 'training/sps': 92010.99615113858, 'training/walltime': 1939.7010660171509, 'training/entropy_loss': Array(-0.05457573, dtype=float32), 'training/policy_loss': Array(-9.734276e-05, dtype=float32), 'training/total_loss': Array(91332.09, dtype=float32), 'training/v_loss': Array(91332.15, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2885515 , 0.11679761], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.352741, 10.088434], dtype=float32), 'eval/episode_reward': Array([-23619.89 ,   8655.568], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28351974, 0.1197526 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.017316818237305, 'eval/sps': 31862.062613265116}
I0728 08:41:43.426185 140143444526912 train.py:379] starting iteration 90, 176947200 steps, 2325.642173051834
I0728 08:42:08.823501 140143444526912 train.py:394] {'eval/walltime': 380.57670879364014, 'training/sps': 91987.56494454194, 'training/walltime': 1961.0743896961212, 'training/entropy_loss': Array(-0.05458022, dtype=float32), 'training/policy_loss': Array(-0.00013939, dtype=float32), 'training/total_loss': Array(90861.984, dtype=float32), 'training/v_loss': Array(90862.03, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30857402, 0.12771352], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.050749 , 10.9829235], dtype=float32), 'eval/episode_reward': Array([-24386.078,   9306.731], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.303912  , 0.13067162], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019118309020996, 'eval/sps': 31847.78106996783}
I0728 08:42:08.825464 140143444526912 train.py:379] starting iteration 91, 178913280 steps, 2351.0414519309998
I0728 08:42:34.212213 140143444526912 train.py:394] {'eval/walltime': 384.5957889556885, 'training/sps': 92034.18250743696, 'training/walltime': 1982.4368872642517, 'training/entropy_loss': Array(-0.05458202, dtype=float32), 'training/policy_loss': Array(-9.866015e-05, dtype=float32), 'training/total_loss': Array(90595.1, dtype=float32), 'training/v_loss': Array(90595.16, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32185915, 0.1293941 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.202612, 11.085014], dtype=float32), 'eval/episode_reward': Array([-24670.512,   9390.653], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31727967, 0.13251698], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.01908016204834, 'eval/sps': 31848.08335217785}
I0728 08:42:34.214163 140143444526912 train.py:379] starting iteration 92, 180879360 steps, 2376.430151462555
I0728 08:42:59.617207 140143444526912 train.py:394] {'eval/walltime': 388.61972165107727, 'training/sps': 91983.67817514343, 'training/walltime': 2003.8111140727997, 'training/entropy_loss': Array(-0.0545389, dtype=float32), 'training/policy_loss': Array(-0.00010279, dtype=float32), 'training/total_loss': Array(86923.625, dtype=float32), 'training/v_loss': Array(86923.69, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32388428, 0.14265317], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.328863, 12.280182], dtype=float32), 'eval/episode_reward': Array([-26261.803,  10335.178], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31903267, 0.14597256], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023932695388794, 'eval/sps': 31809.677171459894}
I0728 08:42:59.619158 140143444526912 train.py:379] starting iteration 93, 182845440 steps, 2401.835146188736
I0728 08:43:25.013778 140143444526912 train.py:394] {'eval/walltime': 392.63404083251953, 'training/sps': 91977.05254466708, 'training/walltime': 2025.1868805885315, 'training/entropy_loss': Array(-0.05459644, dtype=float32), 'training/policy_loss': Array(-0.00012051, dtype=float32), 'training/total_loss': Array(86326.72, dtype=float32), 'training/v_loss': Array(86326.78, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29612547, 0.12046532], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.024134, 10.367553], dtype=float32), 'eval/episode_reward': Array([-24174.195,   9047.684], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2914949 , 0.12324483], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.014319181442261, 'eval/sps': 31885.855163617627}
I0728 08:43:25.015760 140143444526912 train.py:379] starting iteration 94, 184811520 steps, 2427.231747865677
I0728 08:43:50.392832 140143444526912 train.py:394] {'eval/walltime': 396.6220555305481, 'training/sps': 91940.60710540715, 'training/walltime': 2046.5711205005646, 'training/entropy_loss': Array(-0.05457729, dtype=float32), 'training/policy_loss': Array(-9.7801305e-05, dtype=float32), 'training/total_loss': Array(85868.22, dtype=float32), 'training/v_loss': Array(85868.266, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30128542, 0.14406118], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.404463, 12.396122], dtype=float32), 'eval/episode_reward': Array([-24139.25 ,  10028.389], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29609683, 0.14754255], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9880146980285645, 'eval/sps': 32096.170574114367}
I0728 08:43:50.394788 140143444526912 train.py:379] starting iteration 95, 186777600 steps, 2452.610775947571
I0728 08:44:15.827721 140143444526912 train.py:394] {'eval/walltime': 400.6366636753082, 'training/sps': 91815.55075069616, 'training/walltime': 2067.984486579895, 'training/entropy_loss': Array(-0.05456822, dtype=float32), 'training/policy_loss': Array(-6.842433e-05, dtype=float32), 'training/total_loss': Array(84278.125, dtype=float32), 'training/v_loss': Array(84278.18, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31388462, 0.11478551], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.491877,  9.903048], dtype=float32), 'eval/episode_reward': Array([-25146.93 ,   8481.656], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30960777, 0.11728088], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.014608144760132, 'eval/sps': 31883.56008470357}
I0728 08:44:15.831797 140143444526912 train.py:379] starting iteration 96, 188743680 steps, 2478.0477702617645
I0728 08:44:41.240663 140143444526912 train.py:394] {'eval/walltime': 404.6344017982483, 'training/sps': 91847.20849610968, 'training/walltime': 2089.390471935272, 'training/entropy_loss': Array(-0.05457204, dtype=float32), 'training/policy_loss': Array(-0.00018775, dtype=float32), 'training/total_loss': Array(82870.64, dtype=float32), 'training/v_loss': Array(82870.69, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28794712, 0.13364832], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.279324, 11.493384], dtype=float32), 'eval/episode_reward': Array([-23585.664,   9765.512], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28259563, 0.13654597], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9977381229400635, 'eval/sps': 32018.105254444414}
I0728 08:44:41.242619 140143444526912 train.py:379] starting iteration 97, 190709760 steps, 2503.458607196808
I0728 08:45:06.658869 140143444526912 train.py:394] {'eval/walltime': 408.6490089893341, 'training/sps': 91884.47574445416, 'training/walltime': 2110.7877752780914, 'training/entropy_loss': Array(-0.05454106, dtype=float32), 'training/policy_loss': Array(-7.610278e-05, dtype=float32), 'training/total_loss': Array(83121.66, dtype=float32), 'training/v_loss': Array(83121.72, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31372124, 0.11529117], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.437168,  9.963548], dtype=float32), 'eval/episode_reward': Array([-25034.938 ,   8804.9795], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30900455, 0.11837343], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.014607191085815, 'eval/sps': 31883.567658678043}
I0728 08:45:06.660826 140143444526912 train.py:379] starting iteration 98, 192675840 steps, 2528.876813650131
I0728 08:45:32.071176 140143444526912 train.py:394] {'eval/walltime': 412.64496517181396, 'training/sps': 91833.03512930995, 'training/walltime': 2132.1970643997192, 'training/entropy_loss': Array(-0.05456762, dtype=float32), 'training/policy_loss': Array(-0.00012505, dtype=float32), 'training/total_loss': Array(81324.16, dtype=float32), 'training/v_loss': Array(81324.2, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3005426 , 0.11792889], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.358795, 10.155442], dtype=float32), 'eval/episode_reward': Array([-24878.182,   8810.676], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29588467, 0.12085318], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9959561824798584, 'eval/sps': 32032.383278178047}
I0728 08:45:32.073240 140143444526912 train.py:379] starting iteration 99, 194641920 steps, 2554.2892282009125
I0728 08:45:57.564587 140143444526912 train.py:394] {'eval/walltime': 416.67455101013184, 'training/sps': 91628.64250150233, 'training/walltime': 2153.654110431671, 'training/entropy_loss': Array(-0.05452169, dtype=float32), 'training/policy_loss': Array(-0.00014434, dtype=float32), 'training/total_loss': Array(78984.14, dtype=float32), 'training/v_loss': Array(78984.2, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3035712 , 0.12537362], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.614336, 10.809314], dtype=float32), 'eval/episode_reward': Array([-25199.643,   9203.154], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2987318 , 0.12856606], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.029585838317871, 'eval/sps': 31765.051083620274}
I0728 08:45:57.566587 140143444526912 train.py:379] starting iteration 100, 196608000 steps, 2579.782575368881
I0728 08:46:22.957822 140143444526912 train.py:394] {'eval/walltime': 420.7066743373871, 'training/sps': 92069.43162870986, 'training/walltime': 2175.008429288864, 'training/entropy_loss': Array(-0.05443947, dtype=float32), 'training/policy_loss': Array(-0.00010227, dtype=float32), 'training/total_loss': Array(77877.68, dtype=float32), 'training/v_loss': Array(77877.734, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32131946, 0.13198628], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.167807, 11.314637], dtype=float32), 'eval/episode_reward': Array([-26101.717,   9413.417], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3167373 , 0.13544756], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.032123327255249, 'eval/sps': 31745.060756148123}
I0728 08:46:23.012600 140143444526912 train.py:379] starting iteration 101, 198574080 steps, 2605.228588104248
I0728 08:46:48.454131 140143444526912 train.py:394] {'eval/walltime': 424.71202540397644, 'training/sps': 91736.9482641456, 'training/walltime': 2196.4401428699493, 'training/entropy_loss': Array(-0.05445761, dtype=float32), 'training/policy_loss': Array(-8.902284e-05, dtype=float32), 'training/total_loss': Array(77200.01, dtype=float32), 'training/v_loss': Array(77200.06, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30200797, 0.12772624], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.480984, 11.032485], dtype=float32), 'eval/episode_reward': Array([-24276.707,   9432.037], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29703513, 0.1311819 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0053510665893555, 'eval/sps': 31957.248658603807}
I0728 08:46:48.456247 140143444526912 train.py:379] starting iteration 102, 200540160 steps, 2630.6722350120544
I0728 08:47:13.923239 140143444526912 train.py:394] {'eval/walltime': 428.72863602638245, 'training/sps': 91677.82093821354, 'training/walltime': 2217.885678768158, 'training/entropy_loss': Array(-0.05446145, dtype=float32), 'training/policy_loss': Array(-0.00014458, dtype=float32), 'training/total_loss': Array(75613.47, dtype=float32), 'training/v_loss': Array(75613.52, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30207393, 0.12866138], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.499184, 11.065849], dtype=float32), 'eval/episode_reward': Array([-24824.031,   9234.253], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2971369 , 0.13163082], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016610622406006, 'eval/sps': 31867.66456424054}
I0728 08:47:13.925247 140143444526912 train.py:379] starting iteration 103, 202506240 steps, 2656.1412353515625
I0728 08:47:39.371757 140143444526912 train.py:394] {'eval/walltime': 432.7503261566162, 'training/sps': 91787.73670837191, 'training/walltime': 2239.3055336475372, 'training/entropy_loss': Array(-0.05446711, dtype=float32), 'training/policy_loss': Array(-9.374954e-05, dtype=float32), 'training/total_loss': Array(73789.734, dtype=float32), 'training/v_loss': Array(73789.8, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31249595, 0.13080885], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.39507 , 11.284638], dtype=float32), 'eval/episode_reward': Array([-25031.729,   9558.226], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30778673, 0.13390219], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021690130233765, 'eval/sps': 31827.414806958255}
I0728 08:47:39.373797 140143444526912 train.py:379] starting iteration 104, 204472320 steps, 2681.589785337448
I0728 08:48:04.828033 140143444526912 train.py:394] {'eval/walltime': 436.74352049827576, 'training/sps': 91631.6929077503, 'training/walltime': 2260.761865377426, 'training/entropy_loss': Array(-0.05455142, dtype=float32), 'training/policy_loss': Array(-6.968346e-05, dtype=float32), 'training/total_loss': Array(73401.6, dtype=float32), 'training/v_loss': Array(73401.66, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3096212 , 0.12724012], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.125286, 10.929893], dtype=float32), 'eval/episode_reward': Array([-25161.65 ,   9100.195], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30473885, 0.1305259 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.993194341659546, 'eval/sps': 32054.538058571932}
I0728 08:48:04.830046 140143444526912 train.py:379] starting iteration 105, 206438400 steps, 2707.0460340976715
I0728 08:48:30.262069 140143444526912 train.py:394] {'eval/walltime': 440.74400901794434, 'training/sps': 91758.537638356, 'training/walltime': 2282.1885364055634, 'training/entropy_loss': Array(-0.05456442, dtype=float32), 'training/policy_loss': Array(-8.887278e-05, dtype=float32), 'training/total_loss': Array(71716.64, dtype=float32), 'training/v_loss': Array(71716.695, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31888533, 0.12846115], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.935257, 11.008807], dtype=float32), 'eval/episode_reward': Array([-25823.113,   9689.24 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31464493, 0.1309856 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.000488519668579, 'eval/sps': 31996.092319896015}
I0728 08:48:30.264165 140143444526912 train.py:379] starting iteration 106, 208404480 steps, 2732.4801528453827
I0728 08:48:55.688545 140143444526912 train.py:394] {'eval/walltime': 444.7545018196106, 'training/sps': 91834.4024634579, 'training/walltime': 2303.597506761551, 'training/entropy_loss': Array(-0.05454532, dtype=float32), 'training/policy_loss': Array(-0.00020964, dtype=float32), 'training/total_loss': Array(69766.086, dtype=float32), 'training/v_loss': Array(69766.14, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2862414 , 0.13035958], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.161509, 11.288311], dtype=float32), 'eval/episode_reward': Array([-23258.242,   9159.746], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28047252, 0.13456134], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.01049280166626, 'eval/sps': 31916.277208331903}
I0728 08:48:55.690540 140143444526912 train.py:379] starting iteration 107, 210370560 steps, 2757.9065279960632
I0728 08:49:21.164937 140143444526912 train.py:394] {'eval/walltime': 448.77871584892273, 'training/sps': 91678.05841683559, 'training/walltime': 2325.0429871082306, 'training/entropy_loss': Array(-0.05452092, dtype=float32), 'training/policy_loss': Array(-0.00010372, dtype=float32), 'training/total_loss': Array(68801.016, dtype=float32), 'training/v_loss': Array(68801.07, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30491686, 0.12585029], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.812508, 10.812381], dtype=float32), 'eval/episode_reward': Array([-24258.057,   9076.573], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30061007, 0.12827118], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024214029312134, 'eval/sps': 31807.45334807137}
I0728 08:49:21.166966 140143444526912 train.py:379] starting iteration 108, 212336640 steps, 2783.3829538822174
I0728 08:49:46.609162 140143444526912 train.py:394] {'eval/walltime': 452.7861268520355, 'training/sps': 91743.55261522904, 'training/walltime': 2346.4731578826904, 'training/entropy_loss': Array(-0.05450715, dtype=float32), 'training/policy_loss': Array(-0.00016928, dtype=float32), 'training/total_loss': Array(68658.6, dtype=float32), 'training/v_loss': Array(68658.66, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30565268, 0.1346322 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.810337, 11.513887], dtype=float32), 'eval/episode_reward': Array([-24782.26 ,   9965.227], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30121768, 0.13708216], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.007411003112793, 'eval/sps': 31940.82161789116}
I0728 08:49:46.611321 140143444526912 train.py:379] starting iteration 109, 214302720 steps, 2808.8273079395294
I0728 08:50:12.068632 140143444526912 train.py:394] {'eval/walltime': 456.8024916648865, 'training/sps': 91718.29465491729, 'training/walltime': 2367.9092302322388, 'training/entropy_loss': Array(-0.05442212, dtype=float32), 'training/policy_loss': Array(-0.00010855, dtype=float32), 'training/total_loss': Array(67463.35, dtype=float32), 'training/v_loss': Array(67463.41, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31137395, 0.12750438], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.303814, 10.985983], dtype=float32), 'eval/episode_reward': Array([-25169.443 ,   9831.4375], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30653706, 0.13077015], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016364812850952, 'eval/sps': 31869.614929013696}
I0728 08:50:12.070696 140143444526912 train.py:379] starting iteration 110, 216268800 steps, 2834.2866835594177
I0728 08:50:37.491619 140143444526912 train.py:394] {'eval/walltime': 460.818941116333, 'training/sps': 91874.68293550804, 'training/walltime': 2389.3088142871857, 'training/entropy_loss': Array(-0.05445419, dtype=float32), 'training/policy_loss': Array(-2.7532247e-05, dtype=float32), 'training/total_loss': Array(65686.5, dtype=float32), 'training/v_loss': Array(65686.55, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2861182 , 0.11897923], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.166773, 10.235531], dtype=float32), 'eval/episode_reward': Array([-23541.516,   9384.006], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28133953, 0.12172429], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016449451446533, 'eval/sps': 31868.943340965117}
I0728 08:50:37.493636 140143444526912 train.py:379] starting iteration 111, 218234880 steps, 2859.7096235752106
I0728 08:51:02.958029 140143444526912 train.py:394] {'eval/walltime': 464.8448238372803, 'training/sps': 91727.27972340384, 'training/walltime': 2410.742786884308, 'training/entropy_loss': Array(-0.05447835, dtype=float32), 'training/policy_loss': Array(2.8862762e-05, dtype=float32), 'training/total_loss': Array(64350.324, dtype=float32), 'training/v_loss': Array(64350.375, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3010323 , 0.11893237], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.42268 , 10.215079], dtype=float32), 'eval/episode_reward': Array([-24332.65 ,   8676.196], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2961806 , 0.12190641], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.025882720947266, 'eval/sps': 31794.269449032134}
I0728 08:51:02.959997 140143444526912 train.py:379] starting iteration 112, 220200960 steps, 2885.175985097885
I0728 08:51:28.435499 140143444526912 train.py:394] {'eval/walltime': 468.88300013542175, 'training/sps': 91732.44078685595, 'training/walltime': 2432.175553560257, 'training/entropy_loss': Array(-0.0544896, dtype=float32), 'training/policy_loss': Array(-0.00012847, dtype=float32), 'training/total_loss': Array(62814.812, dtype=float32), 'training/v_loss': Array(62814.867, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3084719, 0.1268159], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.07093 , 10.899544], dtype=float32), 'eval/episode_reward': Array([-24955.592,   9258.61 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30353492, 0.12991826], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0381762981414795, 'eval/sps': 31697.47691771417}
I0728 08:51:28.437525 140143444526912 train.py:379] starting iteration 113, 222167040 steps, 2910.653512239456
I0728 08:51:53.900853 140143444526912 train.py:394] {'eval/walltime': 472.9153325557709, 'training/sps': 91759.54641169638, 'training/walltime': 2453.601989030838, 'training/entropy_loss': Array(-0.05445409, dtype=float32), 'training/policy_loss': Array(-7.910945e-05, dtype=float32), 'training/total_loss': Array(63661.375, dtype=float32), 'training/v_loss': Array(63661.42, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31020015, 0.13448362], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.18721 , 11.545634], dtype=float32), 'eval/episode_reward': Array([-24693.844,   9552.071], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3054039 , 0.13774282], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.032332420349121, 'eval/sps': 31743.414643606615}
I0728 08:51:53.902842 140143444526912 train.py:379] starting iteration 114, 224133120 steps, 2936.1188294887543
I0728 08:52:19.347772 140143444526912 train.py:394] {'eval/walltime': 476.9454302787781, 'training/sps': 91829.7309917613, 'training/walltime': 2475.012048482895, 'training/entropy_loss': Array(-0.05448102, dtype=float32), 'training/policy_loss': Array(-0.00010954, dtype=float32), 'training/total_loss': Array(63627.047, dtype=float32), 'training/v_loss': Array(63627.1, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2977497 , 0.12201568], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.1517  , 10.497288], dtype=float32), 'eval/episode_reward': Array([-24182.34 ,   9109.055], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29316336, 0.1249082 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.030097723007202, 'eval/sps': 31761.016431256212}
I0728 08:52:19.349657 140143444526912 train.py:379] starting iteration 115, 226099200 steps, 2961.5656447410583
I0728 08:52:44.782241 140143444526912 train.py:394] {'eval/walltime': 480.94349670410156, 'training/sps': 91743.72613132182, 'training/walltime': 2496.4421787261963, 'training/entropy_loss': Array(-0.0544662, dtype=float32), 'training/policy_loss': Array(-8.710916e-05, dtype=float32), 'training/total_loss': Array(61641.883, dtype=float32), 'training/v_loss': Array(61641.938, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31919706, 0.13689604], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.961012, 11.782647], dtype=float32), 'eval/episode_reward': Array([-25645.576,   9992.923], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31461698, 0.14009273], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9980664253234863, 'eval/sps': 32015.476078450454}
I0728 08:52:44.784192 140143444526912 train.py:379] starting iteration 116, 228065280 steps, 2987.0001797676086
I0728 08:53:10.213501 140143444526912 train.py:394] {'eval/walltime': 484.9519863128662, 'training/sps': 91804.24159799823, 'training/walltime': 2517.858182668686, 'training/entropy_loss': Array(-0.054484, dtype=float32), 'training/policy_loss': Array(-5.911349e-05, dtype=float32), 'training/total_loss': Array(60386.56, dtype=float32), 'training/v_loss': Array(60386.613, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2879972 , 0.12575307], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.29342 , 10.817306], dtype=float32), 'eval/episode_reward': Array([-23429.031,   9183.248], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.282938  , 0.12866609], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.008489608764648, 'eval/sps': 31932.22697150699}
I0728 08:53:10.215466 140143444526912 train.py:379] starting iteration 117, 230031360 steps, 3012.4314539432526
I0728 08:53:35.647814 140143444526912 train.py:394] {'eval/walltime': 488.96595764160156, 'training/sps': 91814.44363089788, 'training/walltime': 2539.2718069553375, 'training/entropy_loss': Array(-0.05447824, dtype=float32), 'training/policy_loss': Array(4.1151707e-06, dtype=float32), 'training/total_loss': Array(58855.082, dtype=float32), 'training/v_loss': Array(58855.133, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31564265, 0.13197716], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.677378, 11.303162], dtype=float32), 'eval/episode_reward': Array([-25742.773,   9276.365], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3109038 , 0.13551097], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.013971328735352, 'eval/sps': 31888.618407328755}
I0728 08:53:35.649780 140143444526912 train.py:379] starting iteration 118, 231997440 steps, 3037.8657686710358
I0728 08:54:01.101157 140143444526912 train.py:394] {'eval/walltime': 492.99378204345703, 'training/sps': 91792.50608529041, 'training/walltime': 2560.6905488967896, 'training/entropy_loss': Array(-0.05449552, dtype=float32), 'training/policy_loss': Array(-7.956227e-05, dtype=float32), 'training/total_loss': Array(58724.047, dtype=float32), 'training/v_loss': Array(58724.1, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30142626, 0.13365088], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.460255, 11.505225], dtype=float32), 'eval/episode_reward': Array([-24482.607,   9533.691], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2966261, 0.1370074], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.027824401855469, 'eval/sps': 31778.942483449668}
I0728 08:54:01.103121 140143444526912 train.py:379] starting iteration 119, 233963520 steps, 3063.31910943985
I0728 08:54:26.511407 140143444526912 train.py:394] {'eval/walltime': 497.01696729660034, 'training/sps': 91955.32947180604, 'training/walltime': 2582.0713651180267, 'training/entropy_loss': Array(-0.05442535, dtype=float32), 'training/policy_loss': Array(-0.00011499, dtype=float32), 'training/total_loss': Array(55430.266, dtype=float32), 'training/v_loss': Array(55430.32, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30270004, 0.13217868], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.552341, 11.329086], dtype=float32), 'eval/episode_reward': Array([-24394.34 ,   9391.879], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2979517 , 0.13503838], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0231852531433105, 'eval/sps': 31815.58689100725}
I0728 08:54:26.513327 140143444526912 train.py:379] starting iteration 120, 235929600 steps, 3088.7293150424957
I0728 08:54:51.944588 140143444526912 train.py:394] {'eval/walltime': 501.0336365699768, 'training/sps': 91830.0684508039, 'training/walltime': 2603.4813458919525, 'training/entropy_loss': Array(-0.05431702, dtype=float32), 'training/policy_loss': Array(-9.79204e-05, dtype=float32), 'training/total_loss': Array(54549.43, dtype=float32), 'training/v_loss': Array(54549.484, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3310603 , 0.12884192], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.03613 , 11.030693], dtype=float32), 'eval/episode_reward': Array([-26524.77 ,   9235.406], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32708347, 0.13139232], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.016669273376465, 'eval/sps': 31867.19923604801}
I0728 08:54:51.946442 140143444526912 train.py:379] starting iteration 121, 237895680 steps, 3114.1624295711517
I0728 08:55:17.391680 140143444526912 train.py:394] {'eval/walltime': 505.04666900634766, 'training/sps': 91755.15207944768, 'training/walltime': 2624.908807516098, 'training/entropy_loss': Array(-0.05444764, dtype=float32), 'training/policy_loss': Array(2.9494266e-05, dtype=float32), 'training/total_loss': Array(53509.348, dtype=float32), 'training/v_loss': Array(53509.4, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30275035, 0.13379376], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.59153 , 11.507545], dtype=float32), 'eval/episode_reward': Array([-23935.111,   9844.177], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29812813, 0.13624081], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.01303243637085, 'eval/sps': 31896.07909467975}
I0728 08:55:17.393562 140143444526912 train.py:379] starting iteration 122, 239861760 steps, 3139.6095495224
I0728 08:55:42.842960 140143444526912 train.py:394] {'eval/walltime': 509.07345724105835, 'training/sps': 91796.52590650623, 'training/walltime': 2646.32661151886, 'training/entropy_loss': Array(-0.05446357, dtype=float32), 'training/policy_loss': Array(3.3391402e-06, dtype=float32), 'training/total_loss': Array(53576.46, dtype=float32), 'training/v_loss': Array(53576.516, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31475675, 0.14346099], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.60759 , 12.290467], dtype=float32), 'eval/episode_reward': Array([-25096.062,  10110.29 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31014776, 0.14617175], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.026788234710693, 'eval/sps': 31787.119793548376}
I0728 08:55:42.844811 140143444526912 train.py:379] starting iteration 123, 241827840 steps, 3165.060798883438
I0728 08:56:08.251391 140143444526912 train.py:394] {'eval/walltime': 513.0906603336334, 'training/sps': 91939.39343821064, 'training/walltime': 2667.7111337184906, 'training/entropy_loss': Array(-0.05448581, dtype=float32), 'training/policy_loss': Array(-8.948066e-05, dtype=float32), 'training/total_loss': Array(51768.465, dtype=float32), 'training/v_loss': Array(51768.516, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2870075 , 0.13693927], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.202667, 11.774246], dtype=float32), 'eval/episode_reward': Array([-23669.926,   9763.5  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28195578, 0.13963191], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.017203092575073, 'eval/sps': 31862.96461749225}
I0728 08:56:08.253387 140143444526912 train.py:379] starting iteration 124, 243793920 steps, 3190.4693751335144
I0728 08:56:33.653629 140143444526912 train.py:394] {'eval/walltime': 517.1159014701843, 'training/sps': 92000.77502764178, 'training/walltime': 2689.0813884735107, 'training/entropy_loss': Array(-0.0544171, dtype=float32), 'training/policy_loss': Array(-8.4952044e-05, dtype=float32), 'training/total_loss': Array(50771.79, dtype=float32), 'training/v_loss': Array(50771.844, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31511572, 0.12056492], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.653795, 10.360185], dtype=float32), 'eval/episode_reward': Array([-25409.137,   8659.422], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3106885, 0.1234865], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.025241136550903, 'eval/sps': 31799.337147209768}
I0728 08:56:33.655570 140143444526912 train.py:379] starting iteration 125, 245760000 steps, 3215.871557712555
I0728 08:56:59.071107 140143444526912 train.py:394] {'eval/walltime': 521.0999779701233, 'training/sps': 91757.52786620363, 'training/walltime': 2710.5082952976227, 'training/entropy_loss': Array(-0.05444145, dtype=float32), 'training/policy_loss': Array(-4.060979e-06, dtype=float32), 'training/total_loss': Array(49054.543, dtype=float32), 'training/v_loss': Array(49054.594, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31375718, 0.14277068], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.489496, 12.258453], dtype=float32), 'eval/episode_reward': Array([-25127.852,   9260.847], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30843186, 0.147026  ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.984076499938965, 'eval/sps': 32127.89714302949}
I0728 08:56:59.134844 140143444526912 train.py:379] starting iteration 126, 247726080 steps, 3241.3508303165436
I0728 08:57:24.519706 140143444526912 train.py:394] {'eval/walltime': 525.0827648639679, 'training/sps': 91883.93209977086, 'training/walltime': 2731.9057252407074, 'training/entropy_loss': Array(-0.05451027, dtype=float32), 'training/policy_loss': Array(-0.00014138, dtype=float32), 'training/total_loss': Array(47698.18, dtype=float32), 'training/v_loss': Array(47698.234, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3021514 , 0.12207816], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.504478, 10.514908], dtype=float32), 'eval/episode_reward': Array([-24024.031,   9033.8  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2972083 , 0.12559006], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9827868938446045, 'eval/sps': 32138.29999235559}
I0728 08:57:24.521795 140143444526912 train.py:379] starting iteration 127, 249692160 steps, 3266.737782716751
I0728 08:57:49.937675 140143444526912 train.py:394] {'eval/walltime': 529.0831367969513, 'training/sps': 91826.52014191261, 'training/walltime': 2753.3165333271027, 'training/entropy_loss': Array(-0.05452928, dtype=float32), 'training/policy_loss': Array(-5.1289448e-05, dtype=float32), 'training/total_loss': Array(46699.49, dtype=float32), 'training/v_loss': Array(46699.54, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30625874, 0.12612043], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.914946, 10.832056], dtype=float32), 'eval/episode_reward': Array([-24470.209,   9822.596], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30185178, 0.12900509], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.000371932983398, 'eval/sps': 31997.024812775377}
I0728 08:57:49.939645 140143444526912 train.py:379] starting iteration 128, 251658240 steps, 3292.1556327342987
I0728 08:58:15.377820 140143444526912 train.py:394] {'eval/walltime': 533.0861284732819, 'training/sps': 91742.03590838485, 'training/walltime': 2774.747058391571, 'training/entropy_loss': Array(-0.05452181, dtype=float32), 'training/policy_loss': Array(-2.0430805e-05, dtype=float32), 'training/total_loss': Array(47263.1, dtype=float32), 'training/v_loss': Array(47263.156, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3013302, 0.1355296], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.437046, 11.654538], dtype=float32), 'eval/episode_reward': Array([-24467.54,   9771.92], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2962153 , 0.13933995], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.002991676330566, 'eval/sps': 31976.084476232067}
I0728 08:58:15.379678 140143444526912 train.py:379] starting iteration 129, 253624320 steps, 3317.5956661701202
I0728 08:58:40.830253 140143444526912 train.py:394] {'eval/walltime': 537.1071891784668, 'training/sps': 91765.73329123217, 'training/walltime': 2796.1720492839813, 'training/entropy_loss': Array(-0.05447013, dtype=float32), 'training/policy_loss': Array(-8.4375824e-05, dtype=float32), 'training/total_loss': Array(45718.902, dtype=float32), 'training/v_loss': Array(45718.953, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31845722, 0.12847011], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.934494, 11.006143], dtype=float32), 'eval/episode_reward': Array([-25407.752,   8903.327], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3143292 , 0.13066947], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0210607051849365, 'eval/sps': 31832.396818817244}
I0728 08:58:40.832106 140143444526912 train.py:379] starting iteration 130, 255590400 steps, 3343.048094034195
I0728 08:59:06.266423 140143444526912 train.py:394] {'eval/walltime': 541.1363019943237, 'training/sps': 91870.82002851265, 'training/walltime': 2817.5725331306458, 'training/entropy_loss': Array(-0.05446856, dtype=float32), 'training/policy_loss': Array(-9.6278585e-05, dtype=float32), 'training/total_loss': Array(44686.973, dtype=float32), 'training/v_loss': Array(44687.027, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28374368, 0.11196326], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.96344 ,  9.631924], dtype=float32), 'eval/episode_reward': Array([-23331.262,   8320.283], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2788615 , 0.11491492], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.029112815856934, 'eval/sps': 31768.780337012297}
I0728 08:59:06.268288 140143444526912 train.py:379] starting iteration 131, 257556480 steps, 3368.4842760562897
I0728 08:59:31.659680 140143444526912 train.py:394] {'eval/walltime': 545.1514317989349, 'training/sps': 91995.37538903717, 'training/walltime': 2838.9440422058105, 'training/entropy_loss': Array(-0.0545076, dtype=float32), 'training/policy_loss': Array(4.9040253e-05, dtype=float32), 'training/total_loss': Array(43347.83, dtype=float32), 'training/v_loss': Array(43347.883, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28874433, 0.12809855], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.317364, 11.055182], dtype=float32), 'eval/episode_reward': Array([-24091.584,   9348.908], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28336754, 0.1317282 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.015129804611206, 'eval/sps': 31879.41765992159}
I0728 08:59:31.661560 140143444526912 train.py:379] starting iteration 132, 259522560 steps, 3393.8775475025177
I0728 08:59:57.106245 140143444526912 train.py:394] {'eval/walltime': 549.1485161781311, 'training/sps': 91688.84307732622, 'training/walltime': 2860.3870000839233, 'training/entropy_loss': Array(-0.05453642, dtype=float32), 'training/policy_loss': Array(-1.9022387e-05, dtype=float32), 'training/total_loss': Array(41647.46, dtype=float32), 'training/v_loss': Array(41647.516, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31357878, 0.11849996], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.50747 , 10.217922], dtype=float32), 'eval/episode_reward': Array([-25484.234,   8985.174], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30903035, 0.12156186], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.997084379196167, 'eval/sps': 32023.341980521665}
I0728 08:59:57.108240 140143444526912 train.py:379] starting iteration 133, 261488640 steps, 3419.3242275714874
I0728 09:00:22.582650 140143444526912 train.py:394] {'eval/walltime': 553.1609289646149, 'training/sps': 91625.79387426059, 'training/walltime': 2881.8447132110596, 'training/entropy_loss': Array(-0.05452958, dtype=float32), 'training/policy_loss': Array(-4.011644e-05, dtype=float32), 'training/total_loss': Array(40775.41, dtype=float32), 'training/v_loss': Array(40775.465, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31056678, 0.13542585], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.265726, 11.636141], dtype=float32), 'eval/episode_reward': Array([-24901.695,   9666.163], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30578828, 0.13826655], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.012412786483765, 'eval/sps': 31901.004909360643}
I0728 09:00:22.584491 140143444526912 train.py:379] starting iteration 134, 263454720 steps, 3444.8004791736603
I0728 09:00:48.048905 140143444526912 train.py:394] {'eval/walltime': 557.1914401054382, 'training/sps': 91745.60831276012, 'training/walltime': 2903.274403810501, 'training/entropy_loss': Array(-0.05448069, dtype=float32), 'training/policy_loss': Array(-0.0001097, dtype=float32), 'training/total_loss': Array(40707.258, dtype=float32), 'training/v_loss': Array(40707.312, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30239943, 0.13318586], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.500252, 11.429607], dtype=float32), 'eval/episode_reward': Array([-23931.74 ,   9903.567], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2973412 , 0.13621761], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.030511140823364, 'eval/sps': 31757.75863848668}
I0728 09:00:48.050758 140143444526912 train.py:379] starting iteration 135, 265420800 steps, 3470.266745567322
I0728 09:01:13.473896 140143444526912 train.py:394] {'eval/walltime': 561.1937112808228, 'training/sps': 91803.67641789801, 'training/walltime': 2924.690539598465, 'training/entropy_loss': Array(-0.05455741, dtype=float32), 'training/policy_loss': Array(-0.00015269, dtype=float32), 'training/total_loss': Array(41499.133, dtype=float32), 'training/v_loss': Array(41499.188, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28094178, 0.12962925], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.72015 , 11.180556], dtype=float32), 'eval/episode_reward': Array([-23192.371,   9425.678], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2757933 , 0.13261004], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0022711753845215, 'eval/sps': 31981.840907544774}
I0728 09:01:13.475732 140143444526912 train.py:379] starting iteration 136, 267386880 steps, 3495.6917192935944
I0728 09:01:38.905454 140143444526912 train.py:394] {'eval/walltime': 565.1920595169067, 'training/sps': 91759.16352446667, 'training/walltime': 2946.117064476013, 'training/entropy_loss': Array(-0.05451549, dtype=float32), 'training/policy_loss': Array(-0.0001208, dtype=float32), 'training/total_loss': Array(40169.824, dtype=float32), 'training/v_loss': Array(40169.88, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32318056, 0.14648174], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.25914 , 12.573977], dtype=float32), 'eval/episode_reward': Array([-25505.457,  10270.794], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31847003, 0.14947984], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9983482360839844, 'eval/sps': 32013.219570230398}
I0728 09:01:38.907421 140143444526912 train.py:379] starting iteration 137, 269352960 steps, 3521.1234092712402
I0728 09:02:04.419445 140143444526912 train.py:394] {'eval/walltime': 569.2176787853241, 'training/sps': 91524.40434982633, 'training/walltime': 2967.5985481739044, 'training/entropy_loss': Array(-0.05454404, dtype=float32), 'training/policy_loss': Array(2.8049182e-05, dtype=float32), 'training/total_loss': Array(38998.766, dtype=float32), 'training/v_loss': Array(38998.82, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31573528, 0.15170968], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.654852, 13.059231], dtype=float32), 'eval/episode_reward': Array([-25043.846,  10596.778], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31070682, 0.15540548], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.025619268417358, 'eval/sps': 31796.35019243194}
I0728 09:02:04.421279 140143444526912 train.py:379] starting iteration 138, 271319040 steps, 3546.6372668743134
I0728 09:02:29.891302 140143444526912 train.py:394] {'eval/walltime': 573.2199656963348, 'training/sps': 91602.92677400848, 'training/walltime': 2989.0616178512573, 'training/entropy_loss': Array(-0.05452243, dtype=float32), 'training/policy_loss': Array(-0.00019143, dtype=float32), 'training/total_loss': Array(37736.055, dtype=float32), 'training/v_loss': Array(37736.11, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31901944, 0.13012221], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.961727, 11.201062], dtype=float32), 'eval/episode_reward': Array([-26031.412,   9791.935], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31434247, 0.13380143], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.002286911010742, 'eval/sps': 31981.715165861195}
I0728 09:02:29.893283 140143444526912 train.py:379] starting iteration 139, 273285120 steps, 3572.1092705726624
I0728 09:02:55.318672 140143444526912 train.py:394] {'eval/walltime': 577.2190976142883, 'training/sps': 91779.76535391205, 'training/walltime': 3010.4833331108093, 'training/entropy_loss': Array(-0.05452534, dtype=float32), 'training/policy_loss': Array(-4.2476837e-05, dtype=float32), 'training/total_loss': Array(36144.51, dtype=float32), 'training/v_loss': Array(36144.566, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29604805, 0.11265417], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.016996,  9.676041], dtype=float32), 'eval/episode_reward': Array([-24838.693,   8294.762], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29131976, 0.11537252], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.999131917953491, 'eval/sps': 32006.946163832097}
I0728 09:02:55.320683 140143444526912 train.py:379] starting iteration 140, 275251200 steps, 3597.536670923233
I0728 09:03:20.739440 140143444526912 train.py:394] {'eval/walltime': 581.2358319759369, 'training/sps': 91884.08669487368, 'training/walltime': 3031.8807270526886, 'training/entropy_loss': Array(-0.05451486, dtype=float32), 'training/policy_loss': Array(-9.037003e-05, dtype=float32), 'training/total_loss': Array(35515.91, dtype=float32), 'training/v_loss': Array(35515.97, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2664799 , 0.13291597], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.434141, 11.510629], dtype=float32), 'eval/episode_reward': Array([-22430.49 ,  10234.952], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2598719 , 0.13774051], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.01673436164856, 'eval/sps': 31866.682851157195}
I0728 09:03:20.741291 140143444526912 train.py:379] starting iteration 141, 277217280 steps, 3622.9572784900665
I0728 09:03:46.195049 140143444526912 train.py:394] {'eval/walltime': 585.2638494968414, 'training/sps': 91782.00859394399, 'training/walltime': 3053.301918745041, 'training/entropy_loss': Array(-0.05444987, dtype=float32), 'training/policy_loss': Array(-0.00013824, dtype=float32), 'training/total_loss': Array(34425.266, dtype=float32), 'training/v_loss': Array(34425.32, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2946403, 0.1322587], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.86206 , 11.385693], dtype=float32), 'eval/episode_reward': Array([-24061.406,   9946.016], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28989607, 0.135417  ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.028017520904541, 'eval/sps': 31777.4188755902}
I0728 09:03:46.196906 140143444526912 train.py:379] starting iteration 142, 279183360 steps, 3648.412894010544
I0728 09:04:11.656080 140143444526912 train.py:394] {'eval/walltime': 589.2671518325806, 'training/sps': 91653.75330746097, 'training/walltime': 3074.753086090088, 'training/entropy_loss': Array(-0.05431088, dtype=float32), 'training/policy_loss': Array(-7.4723275e-06, dtype=float32), 'training/total_loss': Array(32984.383, dtype=float32), 'training/v_loss': Array(32984.434, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30712426, 0.1163587 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.977415, 10.009766], dtype=float32), 'eval/episode_reward': Array([-25261.693,   8788.368], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30276015, 0.11882191], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.003302335739136, 'eval/sps': 31973.603106937753}
I0728 09:04:11.657938 140143444526912 train.py:379] starting iteration 143, 281149440 steps, 3673.873925447464
I0728 09:04:37.090634 140143444526912 train.py:394] {'eval/walltime': 593.2751863002777, 'training/sps': 91787.09715095529, 'training/walltime': 3096.1730902194977, 'training/entropy_loss': Array(-0.05435638, dtype=float32), 'training/policy_loss': Array(-4.779601e-05, dtype=float32), 'training/total_loss': Array(33986.586, dtype=float32), 'training/v_loss': Array(33986.645, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29883224, 0.12220862], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.293547, 10.441594], dtype=float32), 'eval/episode_reward': Array([-24417.844,   8766.111], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2948857 , 0.12413189], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0080344676971436, 'eval/sps': 31935.853104961865}
I0728 09:04:37.092566 140143444526912 train.py:379] starting iteration 144, 283115520 steps, 3699.30855345726
I0728 09:05:02.544593 140143444526912 train.py:394] {'eval/walltime': 597.306435585022, 'training/sps': 91804.14041696256, 'training/walltime': 3117.5891177654266, 'training/entropy_loss': Array(-0.05433512, dtype=float32), 'training/policy_loss': Array(-7.9240956e-05, dtype=float32), 'training/total_loss': Array(31491.107, dtype=float32), 'training/v_loss': Array(31491.164, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32621294, 0.11513922], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.624882,  9.835157], dtype=float32), 'eval/episode_reward': Array([-26251.754,   8693.239], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3224163 , 0.11729898], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.031249284744263, 'eval/sps': 31751.943618172983}
I0728 09:05:02.546449 140143444526912 train.py:379] starting iteration 145, 285081600 steps, 3724.7624366283417
I0728 09:05:27.999696 140143444526912 train.py:394] {'eval/walltime': 601.3306469917297, 'training/sps': 91768.53547980278, 'training/walltime': 3139.013454437256, 'training/entropy_loss': Array(-0.05433819, dtype=float32), 'training/policy_loss': Array(-6.615854e-05, dtype=float32), 'training/total_loss': Array(31774.95, dtype=float32), 'training/v_loss': Array(31775.006, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3057276 , 0.13505232], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.81947 , 11.632106], dtype=float32), 'eval/episode_reward': Array([-24765.906,  10224.762], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30065483, 0.13890755], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024211406707764, 'eval/sps': 31807.47407719261}
I0728 09:05:28.001574 140143444526912 train.py:379] starting iteration 146, 287047680 steps, 3750.2175617218018
I0728 09:05:53.431266 140143444526912 train.py:394] {'eval/walltime': 605.3318600654602, 'training/sps': 91771.10191998306, 'training/walltime': 3160.437191963196, 'training/entropy_loss': Array(-0.0543576, dtype=float32), 'training/policy_loss': Array(-0.00010566, dtype=float32), 'training/total_loss': Array(31086.11, dtype=float32), 'training/v_loss': Array(31086.164, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29748762, 0.12131616], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.13519 , 10.424259], dtype=float32), 'eval/episode_reward': Array([-24524.988,   8607.632], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29264283, 0.12423183], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.001213073730469, 'eval/sps': 31990.298352359725}
I0728 09:05:53.433110 140143444526912 train.py:379] starting iteration 147, 289013760 steps, 3775.649097919464
I0728 09:06:18.870726 140143444526912 train.py:394] {'eval/walltime': 609.3382062911987, 'training/sps': 91758.78064043229, 'training/walltime': 3181.863806247711, 'training/entropy_loss': Array(-0.0543424, dtype=float32), 'training/policy_loss': Array(-3.469325e-05, dtype=float32), 'training/total_loss': Array(29865.863, dtype=float32), 'training/v_loss': Array(29865.916, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3117611 , 0.11876646], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.322685, 10.160477], dtype=float32), 'eval/episode_reward': Array([-25165.434,   8476.438], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30771866, 0.12079954], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.006346225738525, 'eval/sps': 31949.31061566069}
I0728 09:06:18.872729 140143444526912 train.py:379] starting iteration 148, 290979840 steps, 3801.088716506958
I0728 09:06:44.296271 140143444526912 train.py:394] {'eval/walltime': 613.3432805538177, 'training/sps': 91813.43773910627, 'training/walltime': 3203.2776651382446, 'training/entropy_loss': Array(-0.05442283, dtype=float32), 'training/policy_loss': Array(-4.652114e-05, dtype=float32), 'training/total_loss': Array(29636.373, dtype=float32), 'training/v_loss': Array(29636.426, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3078999 , 0.12229694], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.024258, 10.518382], dtype=float32), 'eval/episode_reward': Array([-24955.96 ,   8584.459], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30333766, 0.12525967], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0050742626190186, 'eval/sps': 31959.457330086458}
I0728 09:06:44.298225 140143444526912 train.py:379] starting iteration 149, 292945920 steps, 3826.514212369919
I0728 09:07:09.755493 140143444526912 train.py:394] {'eval/walltime': 617.3700551986694, 'training/sps': 91762.46258568078, 'training/walltime': 3224.7034196853638, 'training/entropy_loss': Array(-0.05446318, dtype=float32), 'training/policy_loss': Array(-5.8132267e-05, dtype=float32), 'training/total_loss': Array(28480.791, dtype=float32), 'training/v_loss': Array(28480.848, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32082197, 0.13431303], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.105852, 11.557188], dtype=float32), 'eval/episode_reward': Array([-26092.186,   9661.024], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31622216, 0.13751967], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.026774644851685, 'eval/sps': 31787.227071087942}
I0728 09:07:09.757456 140143444526912 train.py:379] starting iteration 150, 294912000 steps, 3851.9734437465668
I0728 09:07:35.179014 140143444526912 train.py:394] {'eval/walltime': 621.3835670948029, 'training/sps': 91858.25605666026, 'training/walltime': 3246.106830596924, 'training/entropy_loss': Array(-0.05445544, dtype=float32), 'training/policy_loss': Array(-9.106083e-05, dtype=float32), 'training/total_loss': Array(28934.635, dtype=float32), 'training/v_loss': Array(28934.69, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28882343, 0.1383677 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.394798, 11.974988], dtype=float32), 'eval/episode_reward': Array([-23978.082,  10020.305], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2835816 , 0.14178076], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.013511896133423, 'eval/sps': 31892.268744316894}
I0728 09:07:35.278516 140143444526912 train.py:379] starting iteration 151, 296878080 steps, 3877.4944875240326
I0728 09:08:00.726886 140143444526912 train.py:394] {'eval/walltime': 625.3937950134277, 'training/sps': 91731.27342551672, 'training/walltime': 3267.5398700237274, 'training/entropy_loss': Array(-0.05449919, dtype=float32), 'training/policy_loss': Array(-0.00011422, dtype=float32), 'training/total_loss': Array(27449.035, dtype=float32), 'training/v_loss': Array(27449.09, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3005512 , 0.12427743], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.411293 , 10.6212635], dtype=float32), 'eval/episode_reward': Array([-23958.21 ,   9162.333], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29620755, 0.126768  ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.010227918624878, 'eval/sps': 31918.38533803128}
I0728 09:08:00.728940 140143444526912 train.py:379] starting iteration 152, 298844160 steps, 3902.9449276924133
I0728 09:08:26.172394 140143444526912 train.py:394] {'eval/walltime': 629.4012644290924, 'training/sps': 91737.08501590838, 'training/walltime': 3288.971551656723, 'training/entropy_loss': Array(-0.05442602, dtype=float32), 'training/policy_loss': Array(-0.00016512, dtype=float32), 'training/total_loss': Array(27554.64, dtype=float32), 'training/v_loss': Array(27554.695, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29171753, 0.125892  ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.590212, 10.820602], dtype=float32), 'eval/episode_reward': Array([-23831.512,   9068.645], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28728515, 0.12811992], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.007469415664673, 'eval/sps': 31940.356051044277}
I0728 09:08:26.174247 140143444526912 train.py:379] starting iteration 153, 300810240 steps, 3928.3902354240417
I0728 09:08:51.617452 140143444526912 train.py:394] {'eval/walltime': 633.3957197666168, 'training/sps': 91684.08344698223, 'training/walltime': 3310.4156227111816, 'training/entropy_loss': Array(-0.05443554, dtype=float32), 'training/policy_loss': Array(-0.00012529, dtype=float32), 'training/total_loss': Array(26376.373, dtype=float32), 'training/v_loss': Array(26376.428, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32958   , 0.13767265], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.882355 , 11.8598995], dtype=float32), 'eval/episode_reward': Array([-26747.955,   9650.052], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32523215, 0.1409448 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.994455337524414, 'eval/sps': 32044.41887171749}
I0728 09:08:51.619310 140143444526912 train.py:379] starting iteration 154, 302776320 steps, 3953.8352978229523
I0728 09:09:17.047223 140143444526912 train.py:394] {'eval/walltime': 637.39533162117, 'training/sps': 91773.9565256522, 'training/walltime': 3331.838693857193, 'training/entropy_loss': Array(-0.05436369, dtype=float32), 'training/policy_loss': Array(-0.00011938, dtype=float32), 'training/total_loss': Array(26657.033, dtype=float32), 'training/v_loss': Array(26657.086, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29980215, 0.13884978], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.307575, 11.941118], dtype=float32), 'eval/episode_reward': Array([-24269.37 ,   9831.178], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29455915, 0.14239748], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9996118545532227, 'eval/sps': 32003.105464917237}
I0728 09:09:17.049172 140143444526912 train.py:379] starting iteration 155, 304742400 steps, 3979.2651600837708
I0728 09:09:42.469043 140143444526912 train.py:394] {'eval/walltime': 641.3975875377655, 'training/sps': 91820.5694152378, 'training/walltime': 3353.2508895397186, 'training/entropy_loss': Array(-0.0544198, dtype=float32), 'training/policy_loss': Array(-0.00015022, dtype=float32), 'training/total_loss': Array(26128.652, dtype=float32), 'training/v_loss': Array(26128.705, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3159798 , 0.13246784], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.622463 , 11.3278885], dtype=float32), 'eval/episode_reward': Array([-25419.139,   9234.936], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31117052, 0.13576424], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.002255916595459, 'eval/sps': 31981.962839818574}
I0728 09:09:42.471889 140143444526912 train.py:379] starting iteration 156, 306708480 steps, 4004.687870979309
I0728 09:10:07.990333 140143444526912 train.py:394] {'eval/walltime': 645.4361207485199, 'training/sps': 91552.81014799752, 'training/walltime': 3374.725708246231, 'training/entropy_loss': Array(-0.054371, dtype=float32), 'training/policy_loss': Array(-0.00012088, dtype=float32), 'training/total_loss': Array(25629.977, dtype=float32), 'training/v_loss': Array(25630.031, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2875506 , 0.12256816], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.231537, 10.557972], dtype=float32), 'eval/episode_reward': Array([-23659.889,   9304.099], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28244376, 0.12583753], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0385332107543945, 'eval/sps': 31694.675596363293}
I0728 09:10:07.992366 140143444526912 train.py:379] starting iteration 157, 308674560 steps, 4030.208353996277
I0728 09:10:33.382951 140143444526912 train.py:394] {'eval/walltime': 649.4434714317322, 'training/sps': 91964.18464136461, 'training/walltime': 3396.1044657230377, 'training/entropy_loss': Array(-0.0544267, dtype=float32), 'training/policy_loss': Array(-0.00012346, dtype=float32), 'training/total_loss': Array(25642.875, dtype=float32), 'training/v_loss': Array(25642.93, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3000949 , 0.14183745], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.345419, 12.164839], dtype=float32), 'eval/episode_reward': Array([-24553.059,   9956.568], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2952567 , 0.14487137], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.00735068321228, 'eval/sps': 31941.302401165347}
I0728 09:10:33.384911 140143444526912 train.py:379] starting iteration 158, 310640640 steps, 4055.6008989810944
I0728 09:10:58.821895 140143444526912 train.py:394] {'eval/walltime': 653.4509062767029, 'training/sps': 91767.21912428096, 'training/walltime': 3417.5291097164154, 'training/entropy_loss': Array(-0.05449411, dtype=float32), 'training/policy_loss': Array(-9.0250105e-05, dtype=float32), 'training/total_loss': Array(26060.89, dtype=float32), 'training/v_loss': Array(26060.945, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29552156, 0.13201882], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.870213, 11.35445 ], dtype=float32), 'eval/episode_reward': Array([-24197.07 ,   9653.221], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28953663, 0.13650335], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.007434844970703, 'eval/sps': 31940.631588967422}
I0728 09:10:58.823942 140143444526912 train.py:379] starting iteration 159, 312606720 steps, 4081.0399298667908
I0728 09:11:24.225440 140143444526912 train.py:394] {'eval/walltime': 657.4451072216034, 'training/sps': 91861.43945044198, 'training/walltime': 3438.931778907776, 'training/entropy_loss': Array(-0.05442707, dtype=float32), 'training/policy_loss': Array(-0.0001416, dtype=float32), 'training/total_loss': Array(25261.012, dtype=float32), 'training/v_loss': Array(25261.068, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30751806, 0.13513307], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.97742, 11.65703], dtype=float32), 'eval/episode_reward': Array([-24882.207,  10132.165], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30263203, 0.13811493], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9942009449005127, 'eval/sps': 32046.45979652589}
I0728 09:11:24.227402 140143444526912 train.py:379] starting iteration 160, 314572800 steps, 4106.443390130997
I0728 09:11:49.758810 140143444526912 train.py:394] {'eval/walltime': 661.4828405380249, 'training/sps': 91491.96273666766, 'training/walltime': 3460.4208796024323, 'training/entropy_loss': Array(-0.05439355, dtype=float32), 'training/policy_loss': Array(-7.720057e-05, dtype=float32), 'training/total_loss': Array(24874.043, dtype=float32), 'training/v_loss': Array(24874.098, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30054215, 0.1277342 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.379517, 10.946911], dtype=float32), 'eval/episode_reward': Array([-25140.266,   9047.945], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2958653 , 0.13062213], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.037733316421509, 'eval/sps': 31700.954463590377}
I0728 09:11:49.760759 140143444526912 train.py:379] starting iteration 161, 316538880 steps, 4131.97674703598
I0728 09:12:15.208481 140143444526912 train.py:394] {'eval/walltime': 665.4885849952698, 'training/sps': 91720.51856834107, 'training/walltime': 3481.856432199478, 'training/entropy_loss': Array(-0.05446826, dtype=float32), 'training/policy_loss': Array(-5.4283282e-05, dtype=float32), 'training/total_loss': Array(23985.14, dtype=float32), 'training/v_loss': Array(23985.195, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3151018 , 0.14060754], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.632915, 12.029232], dtype=float32), 'eval/episode_reward': Array([-25656.793,   9491.549], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31103006, 0.14258152], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.005744457244873, 'eval/sps': 31954.110244974945}
I0728 09:12:15.212611 140143444526912 train.py:379] starting iteration 162, 318504960 steps, 4157.428582668304
I0728 09:12:40.667231 140143444526912 train.py:394] {'eval/walltime': 669.5181932449341, 'training/sps': 91787.84091825227, 'training/walltime': 3503.2762627601624, 'training/entropy_loss': Array(-0.05445842, dtype=float32), 'training/policy_loss': Array(-9.0472975e-05, dtype=float32), 'training/total_loss': Array(23648.941, dtype=float32), 'training/v_loss': Array(23648.996, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30301893, 0.13660088], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.60516 , 11.725155], dtype=float32), 'eval/episode_reward': Array([-23943.395,   9976.147], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29781887, 0.1398823 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.029608249664307, 'eval/sps': 31764.87441692707}
I0728 09:12:40.669196 140143444526912 train.py:379] starting iteration 163, 320471040 steps, 4182.885183811188
I0728 09:13:06.108370 140143444526912 train.py:394] {'eval/walltime': 673.5446772575378, 'training/sps': 91839.11429997373, 'training/walltime': 3524.684134721756, 'training/entropy_loss': Array(-0.05446179, dtype=float32), 'training/policy_loss': Array(-8.872042e-05, dtype=float32), 'training/total_loss': Array(23924.29, dtype=float32), 'training/v_loss': Array(23924.344, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29980522, 0.13846083], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.277506, 11.925886], dtype=float32), 'eval/episode_reward': Array([-24149.055,   9882.279], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29471684, 0.14192791], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.02648401260376, 'eval/sps': 31789.52147812645}
I0728 09:13:06.110232 140143444526912 train.py:379] starting iteration 164, 322437120 steps, 4208.326219797134
I0728 09:13:31.561045 140143444526912 train.py:394] {'eval/walltime': 677.5479266643524, 'training/sps': 91688.67792445513, 'training/walltime': 3546.1271312236786, 'training/entropy_loss': Array(-0.05437417, dtype=float32), 'training/policy_loss': Array(-0.00015692, dtype=float32), 'training/total_loss': Array(23763.576, dtype=float32), 'training/v_loss': Array(23763.629, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32090813, 0.13924289], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.104359, 11.998061], dtype=float32), 'eval/episode_reward': Array([-25299.59 ,   9422.924], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31625658, 0.1421835 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.003249406814575, 'eval/sps': 31974.02584563195}
I0728 09:13:31.562906 140143444526912 train.py:379] starting iteration 165, 324403200 steps, 4233.7788944244385
I0728 09:13:56.967679 140143444526912 train.py:394] {'eval/walltime': 681.5350005626678, 'training/sps': 91818.34575916409, 'training/walltime': 3567.5398454666138, 'training/entropy_loss': Array(-0.05433588, dtype=float32), 'training/policy_loss': Array(-7.538443e-05, dtype=float32), 'training/total_loss': Array(23010.922, dtype=float32), 'training/v_loss': Array(23010.977, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32431936, 0.12283605], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.417934, 10.521549], dtype=float32), 'eval/episode_reward': Array([-25952.83 ,   9353.112], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32000762, 0.1253533 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9870738983154297, 'eval/sps': 32103.7440650601}
I0728 09:13:56.969611 140143444526912 train.py:379] starting iteration 166, 326369280 steps, 4259.18559885025
I0728 09:14:22.389298 140143444526912 train.py:394] {'eval/walltime': 685.5259222984314, 'training/sps': 91770.37067926794, 'training/walltime': 3588.9637537002563, 'training/entropy_loss': Array(-0.05439637, dtype=float32), 'training/policy_loss': Array(-0.00010458, dtype=float32), 'training/total_loss': Array(22603.05, dtype=float32), 'training/v_loss': Array(22603.107, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32584423, 0.12839521], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.550686, 11.049233], dtype=float32), 'eval/episode_reward': Array([-26061.355,   9309.774], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3215623 , 0.13101137], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.99092173576355, 'eval/sps': 32072.79131859769}
I0728 09:14:22.391243 140143444526912 train.py:379] starting iteration 167, 328335360 steps, 4284.607231378555
I0728 09:14:47.832816 140143444526912 train.py:394] {'eval/walltime': 689.5437834262848, 'training/sps': 91791.7254592596, 'training/walltime': 3610.382677793503, 'training/entropy_loss': Array(-0.05427388, dtype=float32), 'training/policy_loss': Array(-0.00011453, dtype=float32), 'training/total_loss': Array(22966.375, dtype=float32), 'training/v_loss': Array(22966.432, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30812946, 0.13420857], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.068108, 11.579878], dtype=float32), 'eval/episode_reward': Array([-25281.7  ,   9598.717], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30271608, 0.13848376], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0178611278533936, 'eval/sps': 31857.746180586895}
I0728 09:14:47.834679 140143444526912 train.py:379] starting iteration 168, 330301440 steps, 4310.050666809082
I0728 09:15:13.293267 140143444526912 train.py:394] {'eval/walltime': 693.5782580375671, 'training/sps': 91789.93742611037, 'training/walltime': 3631.8020191192627, 'training/entropy_loss': Array(-0.0542446, dtype=float32), 'training/policy_loss': Array(-8.19428e-06, dtype=float32), 'training/total_loss': Array(22858.078, dtype=float32), 'training/v_loss': Array(22858.13, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30737126, 0.13663946], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.953426, 11.716549], dtype=float32), 'eval/episode_reward': Array([-24839.07 ,   9758.996], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30264255, 0.13990988], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.034474611282349, 'eval/sps': 31726.559795927304}
I0728 09:15:13.295316 140143444526912 train.py:379] starting iteration 169, 332267520 steps, 4335.511303424835
I0728 09:15:38.714681 140143444526912 train.py:394] {'eval/walltime': 697.6006236076355, 'training/sps': 91905.56306411406, 'training/walltime': 3653.194412946701, 'training/entropy_loss': Array(-0.05425075, dtype=float32), 'training/policy_loss': Array(-0.00010874, dtype=float32), 'training/total_loss': Array(36126.703, dtype=float32), 'training/v_loss': Array(36126.758, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28368565, 0.12806782], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.941744, 11.048116], dtype=float32), 'eval/episode_reward': Array([-23230.3  ,   9540.182], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27831262, 0.13150457], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022365570068359, 'eval/sps': 31822.07031416706}
I0728 09:15:38.716670 140143444526912 train.py:379] starting iteration 170, 334233600 steps, 4360.932657718658
I0728 09:16:04.160367 140143444526912 train.py:394] {'eval/walltime': 701.6294014453888, 'training/sps': 91836.58905548627, 'training/walltime': 3674.6028735637665, 'training/entropy_loss': Array(-0.05361086, dtype=float32), 'training/policy_loss': Array(-0.00044503, dtype=float32), 'training/total_loss': Array(71800.125, dtype=float32), 'training/v_loss': Array(71800.19, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.285874  , 0.12688412], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.114605, 10.86307 ], dtype=float32), 'eval/episode_reward': Array([-23432.96 ,   9473.434], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2808247 , 0.12959655], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.028777837753296, 'eval/sps': 31771.42179460086}
I0728 09:16:04.163922 140143444526912 train.py:379] starting iteration 171, 336199680 steps, 4386.379909753799
I0728 09:16:29.652058 140143444526912 train.py:394] {'eval/walltime': 705.652184009552, 'training/sps': 91613.00370546448, 'training/walltime': 3696.063582420349, 'training/entropy_loss': Array(-0.0524622, dtype=float32), 'training/policy_loss': Array(-0.00046896, dtype=float32), 'training/total_loss': Array(89081.5, dtype=float32), 'training/v_loss': Array(89081.56, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29910806, 0.11855285], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.24197 , 10.138209], dtype=float32), 'eval/episode_reward': Array([-24761.867,   8663.468], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2945553 , 0.12051299], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022782564163208, 'eval/sps': 31818.771698048684}
I0728 09:16:29.653901 140143444526912 train.py:379] starting iteration 172, 338165760 steps, 4411.86988902092
I0728 09:16:55.088108 140143444526912 train.py:394] {'eval/walltime': 709.6810622215271, 'training/sps': 91875.69324060752, 'training/walltime': 3717.4629311561584, 'training/entropy_loss': Array(-0.05153877, dtype=float32), 'training/policy_loss': Array(-0.00013552, dtype=float32), 'training/total_loss': Array(94084.71, dtype=float32), 'training/v_loss': Array(94084.766, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30556116, 0.1342482 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.75555 , 11.522152], dtype=float32), 'eval/episode_reward': Array([-24451.238,   9766.206], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30063564, 0.13736296], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.028878211975098, 'eval/sps': 31770.63025125545}
I0728 09:16:55.090059 140143444526912 train.py:379] starting iteration 173, 340131840 steps, 4437.306046485901
I0728 09:17:20.510876 140143444526912 train.py:394] {'eval/walltime': 713.6760790348053, 'training/sps': 91781.41406336501, 'training/walltime': 3738.884261608124, 'training/entropy_loss': Array(-0.0514858, dtype=float32), 'training/policy_loss': Array(0.00017352, dtype=float32), 'training/total_loss': Array(104515.6, dtype=float32), 'training/v_loss': Array(104515.66, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28028387, 0.14118414], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.616133, 12.211095], dtype=float32), 'eval/episode_reward': Array([-23007.55 ,  10155.918], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27394354, 0.14573601], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9950168132781982, 'eval/sps': 32039.915220023016}
I0728 09:17:20.512859 140143444526912 train.py:379] starting iteration 174, 342097920 steps, 4462.728846549988
I0728 09:17:45.954089 140143444526912 train.py:394] {'eval/walltime': 717.6958863735199, 'training/sps': 91800.38052429144, 'training/walltime': 3760.3011662960052, 'training/entropy_loss': Array(-0.05196131, dtype=float32), 'training/policy_loss': Array(2.3614297e-05, dtype=float32), 'training/total_loss': Array(121390.836, dtype=float32), 'training/v_loss': Array(121390.89, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31232733, 0.13207516], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.392342, 11.375405], dtype=float32), 'eval/episode_reward': Array([-25427.262,   9697.707], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3075031 , 0.13495694], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0198073387146, 'eval/sps': 31842.32208524952}
I0728 09:17:45.956078 140143444526912 train.py:379] starting iteration 175, 344064000 steps, 4488.172065496445
I0728 09:18:11.418035 140143444526912 train.py:394] {'eval/walltime': 721.713015794754, 'training/sps': 91706.48421759358, 'training/walltime': 3781.739999294281, 'training/entropy_loss': Array(-0.05259903, dtype=float32), 'training/policy_loss': Array(2.3702076e-05, dtype=float32), 'training/total_loss': Array(131387.16, dtype=float32), 'training/v_loss': Array(131387.19, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28820157, 0.13599342], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.26472 , 11.780088], dtype=float32), 'eval/episode_reward': Array([-23721.092,  10120.242], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.282332  , 0.13980016], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.017129421234131, 'eval/sps': 31863.548961954082}
I0728 09:18:11.508354 140143444526912 train.py:379] starting iteration 176, 346030080 steps, 4513.724340677261
I0728 09:18:37.003674 140143444526912 train.py:394] {'eval/walltime': 725.7266252040863, 'training/sps': 91544.2596266659, 'training/walltime': 3803.2168238162994, 'training/entropy_loss': Array(-0.05321951, dtype=float32), 'training/policy_loss': Array(6.504725e-05, dtype=float32), 'training/total_loss': Array(137971.05, dtype=float32), 'training/v_loss': Array(137971.1, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3240558 , 0.14423329], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.326826, 12.418529], dtype=float32), 'eval/episode_reward': Array([-25449.809,   9909.249], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31915253, 0.1477999 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.013609409332275, 'eval/sps': 31891.493901319795}
I0728 09:18:37.005752 140143444526912 train.py:379] starting iteration 177, 347996160 steps, 4539.221739530563
I0728 09:19:02.425173 140143444526912 train.py:394] {'eval/walltime': 729.748083114624, 'training/sps': 91901.05538930731, 'training/walltime': 3824.6102669239044, 'training/entropy_loss': Array(-0.05367828, dtype=float32), 'training/policy_loss': Array(5.218379e-05, dtype=float32), 'training/total_loss': Array(152264.23, dtype=float32), 'training/v_loss': Array(152264.28, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30061308, 0.11701479], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.346668, 10.116323], dtype=float32), 'eval/episode_reward': Array([-24209.436,   8365.976], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29596907, 0.11960946], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.02145791053772, 'eval/sps': 31829.25268584616}
I0728 09:19:02.427114 140143444526912 train.py:379] starting iteration 178, 349962240 steps, 4564.643101930618
I0728 09:19:27.887448 140143444526912 train.py:394] {'eval/walltime': 733.7450623512268, 'training/sps': 91621.73809307205, 'training/walltime': 3846.06892991066, 'training/entropy_loss': Array(-0.05401562, dtype=float32), 'training/policy_loss': Array(4.7165246e-05, dtype=float32), 'training/total_loss': Array(161890.6, dtype=float32), 'training/v_loss': Array(161890.62, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31916016, 0.12513082], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.942616, 10.726937], dtype=float32), 'eval/episode_reward': Array([-25584.775,   9349.481], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31436995, 0.12825054], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.996979236602783, 'eval/sps': 32024.184370993404}
I0728 09:19:27.889425 140143444526912 train.py:379] starting iteration 179, 351928320 steps, 4590.105413198471
I0728 09:19:53.294286 140143444526912 train.py:394] {'eval/walltime': 737.7499258518219, 'training/sps': 91894.28497775421, 'training/walltime': 3867.463949203491, 'training/entropy_loss': Array(-0.054075, dtype=float32), 'training/policy_loss': Array(-6.888986e-05, dtype=float32), 'training/total_loss': Array(165557.12, dtype=float32), 'training/v_loss': Array(165557.16, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31466478, 0.13824873], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.564919, 11.893896], dtype=float32), 'eval/episode_reward': Array([-25385.25 ,   9848.903], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30968794, 0.14118826], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.004863500595093, 'eval/sps': 31961.139245065446}
I0728 09:19:53.296256 140143444526912 train.py:379] starting iteration 180, 353894400 steps, 4615.512244224548
I0728 09:20:18.744917 140143444526912 train.py:394] {'eval/walltime': 741.7492477893829, 'training/sps': 91688.08154404064, 'training/walltime': 3888.9070851802826, 'training/entropy_loss': Array(-0.05404816, dtype=float32), 'training/policy_loss': Array(-3.392187e-05, dtype=float32), 'training/total_loss': Array(174846.2, dtype=float32), 'training/v_loss': Array(174846.25, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2946686 , 0.11802448], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.85933 , 10.164779], dtype=float32), 'eval/episode_reward': Array([-23561.527,   9099.068], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28969198, 0.12119589], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.999321937561035, 'eval/sps': 32005.425419204963}
I0728 09:20:18.746884 140143444526912 train.py:379] starting iteration 181, 355860480 steps, 4640.962872266769
I0728 09:20:44.233845 140143444526912 train.py:394] {'eval/walltime': 745.7777488231659, 'training/sps': 91643.16327527851, 'training/walltime': 3910.3607313632965, 'training/entropy_loss': Array(-0.05416001, dtype=float32), 'training/policy_loss': Array(6.4631895e-05, dtype=float32), 'training/total_loss': Array(175777.97, dtype=float32), 'training/v_loss': Array(175778.02, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30158612, 0.13431562], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.428635, 11.547659], dtype=float32), 'eval/episode_reward': Array([-24567.117,  10138.167], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.296191  , 0.13808683], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.028501033782959, 'eval/sps': 31773.604853664827}
I0728 09:20:44.235821 140143444526912 train.py:379] starting iteration 182, 357826560 steps, 4666.451808214188
I0728 09:21:09.677606 140143444526912 train.py:394] {'eval/walltime': 749.8102955818176, 'training/sps': 91852.38408752704, 'training/walltime': 3931.765510559082, 'training/entropy_loss': Array(-0.0541828, dtype=float32), 'training/policy_loss': Array(-1.2240677e-05, dtype=float32), 'training/total_loss': Array(180087.1, dtype=float32), 'training/v_loss': Array(180087.12, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30349633, 0.13021247], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.686615, 11.182685], dtype=float32), 'eval/episode_reward': Array([-24297.11 ,   9377.918], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2990644 , 0.13267085], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.032546758651733, 'eval/sps': 31741.727414661476}
I0728 09:21:09.679616 140143444526912 train.py:379] starting iteration 183, 359792640 steps, 4691.895603179932
I0728 09:21:35.071559 140143444526912 train.py:394] {'eval/walltime': 753.7958691120148, 'training/sps': 91866.06400180419, 'training/walltime': 3953.1671023368835, 'training/entropy_loss': Array(-0.05425577, dtype=float32), 'training/policy_loss': Array(2.0133371e-05, dtype=float32), 'training/total_loss': Array(186746.5, dtype=float32), 'training/v_loss': Array(186746.55, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31142375, 0.12157673], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.295238, 10.450101], dtype=float32), 'eval/episode_reward': Array([-25166.002,   9245.706], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3068527 , 0.12439175], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9855735301971436, 'eval/sps': 32115.829511159107}
I0728 09:21:35.073414 140143444526912 train.py:379] starting iteration 184, 361758720 steps, 4717.289401769638
I0728 09:22:00.570733 140143444526912 train.py:394] {'eval/walltime': 757.8183381557465, 'training/sps': 91573.23805841802, 'training/walltime': 3974.637130498886, 'training/entropy_loss': Array(-0.05427965, dtype=float32), 'training/policy_loss': Array(4.031189e-05, dtype=float32), 'training/total_loss': Array(192970.61, dtype=float32), 'training/v_loss': Array(192970.66, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30663478, 0.12217005], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.888268, 10.48944 ], dtype=float32), 'eval/episode_reward': Array([-25454.85 ,   9196.773], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30258048, 0.12422174], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0224690437316895, 'eval/sps': 31821.25172584373}
I0728 09:22:00.572603 140143444526912 train.py:379] starting iteration 185, 363724800 steps, 4742.788591146469
I0728 09:22:26.043398 140143444526912 train.py:394] {'eval/walltime': 761.8141896724701, 'training/sps': 91572.53233882331, 'training/walltime': 3996.1073241233826, 'training/entropy_loss': Array(-0.05419929, dtype=float32), 'training/policy_loss': Array(0.00011017, dtype=float32), 'training/total_loss': Array(201084.62, dtype=float32), 'training/v_loss': Array(201084.66, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30964833, 0.12935673], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.136898, 11.121799], dtype=float32), 'eval/episode_reward': Array([-25909.57 ,   9674.661], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30492598, 0.13217863], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.995851516723633, 'eval/sps': 32033.222321772508}
I0728 09:22:26.045346 140143444526912 train.py:379] starting iteration 186, 365690880 steps, 4768.261334180832
I0728 09:22:51.496946 140143444526912 train.py:394] {'eval/walltime': 765.827306509018, 'training/sps': 91727.38889765785, 'training/walltime': 4017.541271209717, 'training/entropy_loss': Array(-0.05418482, dtype=float32), 'training/policy_loss': Array(9.1698625e-05, dtype=float32), 'training/total_loss': Array(204523.4, dtype=float32), 'training/v_loss': Array(204523.44, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29837954, 0.12267905], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.207802, 10.53291 ], dtype=float32), 'eval/episode_reward': Array([-24532.42 ,   9269.941], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29387677, 0.12518394], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.013116836547852, 'eval/sps': 31895.408285722297}
I0728 09:22:51.498925 140143444526912 train.py:379] starting iteration 187, 367656960 steps, 4793.714912652969
I0728 09:23:16.897129 140143444526912 train.py:394] {'eval/walltime': 769.8298060894012, 'training/sps': 91910.53113432207, 'training/walltime': 4038.9325087070465, 'training/entropy_loss': Array(-0.05410627, dtype=float32), 'training/policy_loss': Array(9.249854e-05, dtype=float32), 'training/total_loss': Array(206426.55, dtype=float32), 'training/v_loss': Array(206426.6, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30687   , 0.12431228], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.91144 , 10.691179], dtype=float32), 'eval/episode_reward': Array([-25325.467,   9560.012], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30198634, 0.12751852], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.002499580383301, 'eval/sps': 31980.01584493409}
I0728 09:23:16.899106 140143444526912 train.py:379] starting iteration 188, 369623040 steps, 4819.115093946457
I0728 09:23:42.422863 140143444526912 train.py:394] {'eval/walltime': 773.8696205615997, 'training/sps': 91548.63276520142, 'training/walltime': 4060.408307313919, 'training/entropy_loss': Array(-0.05411969, dtype=float32), 'training/policy_loss': Array(0.00021188, dtype=float32), 'training/total_loss': Array(209517.16, dtype=float32), 'training/v_loss': Array(209517.19, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29601675, 0.12791066], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.95839 , 11.016042], dtype=float32), 'eval/episode_reward': Array([-24016.648,   9413.018], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2912246 , 0.13043755], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.039814472198486, 'eval/sps': 31684.623361018308}
I0728 09:23:42.426990 140143444526912 train.py:379] starting iteration 189, 371589120 steps, 4844.642963171005
I0728 09:24:07.903260 140143444526912 train.py:394] {'eval/walltime': 777.8723380565643, 'training/sps': 91578.86590042892, 'training/walltime': 4081.877016067505, 'training/entropy_loss': Array(-0.05395059, dtype=float32), 'training/policy_loss': Array(0.00020675, dtype=float32), 'training/total_loss': Array(212730.94, dtype=float32), 'training/v_loss': Array(212730.98, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2902998 , 0.12386972], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.490965, 10.61168 ], dtype=float32), 'eval/episode_reward': Array([-23633.309,   8967.933], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2853697 , 0.12666881], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0027174949646, 'eval/sps': 31978.27479981373}
I0728 09:24:07.905137 140143444526912 train.py:379] starting iteration 190, 373555200 steps, 4870.121124505997
I0728 09:24:33.382928 140143444526912 train.py:394] {'eval/walltime': 781.8928604125977, 'training/sps': 91646.2319632204, 'training/walltime': 4103.32994389534, 'training/entropy_loss': Array(-0.05390539, dtype=float32), 'training/policy_loss': Array(0.000205, dtype=float32), 'training/total_loss': Array(212630., dtype=float32), 'training/v_loss': Array(212630.03, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3005725 , 0.12228113], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.369799, 10.507899], dtype=float32), 'eval/episode_reward': Array([-24107.367,   9375.714], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2959629 , 0.12507682], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020522356033325, 'eval/sps': 31836.659186316694}
I0728 09:24:33.384846 140143444526912 train.py:379] starting iteration 191, 375521280 steps, 4895.600834608078
I0728 09:24:58.849119 140143444526912 train.py:394] {'eval/walltime': 785.8993813991547, 'training/sps': 91645.96613114316, 'training/walltime': 4124.782933950424, 'training/entropy_loss': Array(-0.05383221, dtype=float32), 'training/policy_loss': Array(0.00024846, dtype=float32), 'training/total_loss': Array(209057.03, dtype=float32), 'training/v_loss': Array(209057.06, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30161333, 0.11621006], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.377712, 10.00094 ], dtype=float32), 'eval/episode_reward': Array([-24450.307,   8658.069], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29699647, 0.11879068], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.006520986557007, 'eval/sps': 31947.917015654137}
I0728 09:24:58.851091 140143444526912 train.py:379] starting iteration 192, 377487360 steps, 4921.067078828812
I0728 09:25:24.380469 140143444526912 train.py:394] {'eval/walltime': 789.9282398223877, 'training/sps': 91462.66848426592, 'training/walltime': 4146.278917312622, 'training/entropy_loss': Array(-0.05385024, dtype=float32), 'training/policy_loss': Array(0.00022561, dtype=float32), 'training/total_loss': Array(209919.31, dtype=float32), 'training/v_loss': Array(209919.36, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29079634, 0.13289143], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.500599, 11.534699], dtype=float32), 'eval/episode_reward': Array([-24045.195,   9873.085], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2847377 , 0.13743697], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.028858423233032, 'eval/sps': 31770.786300622603}
I0728 09:25:24.382452 140143444526912 train.py:379] starting iteration 193, 379453440 steps, 4946.598440408707
I0728 09:25:49.865577 140143444526912 train.py:394] {'eval/walltime': 793.9399435520172, 'training/sps': 91588.26005937222, 'training/walltime': 4167.745424032211, 'training/entropy_loss': Array(-0.05383618, dtype=float32), 'training/policy_loss': Array(0.00016131, dtype=float32), 'training/total_loss': Array(210954.19, dtype=float32), 'training/v_loss': Array(210954.23, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30468857, 0.10599333], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.70868 ,  9.111019], dtype=float32), 'eval/episode_reward': Array([-24628.176,   8697.99 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30022436, 0.10843743], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.011703729629517, 'eval/sps': 31906.643318304286}
I0728 09:25:49.867549 140143444526912 train.py:379] starting iteration 194, 381419520 steps, 4972.083537101746
I0728 09:26:15.385905 140143444526912 train.py:394] {'eval/walltime': 797.9543046951294, 'training/sps': 91448.8772145088, 'training/walltime': 4189.244649171829, 'training/entropy_loss': Array(-0.05386227, dtype=float32), 'training/policy_loss': Array(0.00021905, dtype=float32), 'training/total_loss': Array(211546.69, dtype=float32), 'training/v_loss': Array(211546.73, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2845022 , 0.12160178], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.909788, 10.518674], dtype=float32), 'eval/episode_reward': Array([-22923.594,   9197.251], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2788704 , 0.12548651], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.014361143112183, 'eval/sps': 31885.52186432495}
I0728 09:26:15.387888 140143444526912 train.py:379] starting iteration 195, 383385600 steps, 4997.603875398636
I0728 09:26:40.842360 140143444526912 train.py:394] {'eval/walltime': 801.946038722992, 'training/sps': 91625.00691876069, 'training/walltime': 4210.702546596527, 'training/entropy_loss': Array(-0.05390666, dtype=float32), 'training/policy_loss': Array(0.00015095, dtype=float32), 'training/total_loss': Array(215029.33, dtype=float32), 'training/v_loss': Array(215029.38, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29972368, 0.12092455], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.302547, 10.427474], dtype=float32), 'eval/episode_reward': Array([-24300.758,   9018.824], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29521236, 0.12353238], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.991734027862549, 'eval/sps': 32066.264712666758}
I0728 09:26:40.844301 140143444526912 train.py:379] starting iteration 196, 385351680 steps, 5023.060289621353
I0728 09:27:06.270618 140143444526912 train.py:394] {'eval/walltime': 805.9455111026764, 'training/sps': 91778.3434661934, 'training/walltime': 4232.124593734741, 'training/entropy_loss': Array(-0.05402027, dtype=float32), 'training/policy_loss': Array(0.00021062, dtype=float32), 'training/total_loss': Array(211901.38, dtype=float32), 'training/v_loss': Array(211901.42, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3043856 , 0.11825765], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.7075  , 10.184826], dtype=float32), 'eval/episode_reward': Array([-25101.78 ,   9434.483], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.299636  , 0.12078245], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9994723796844482, 'eval/sps': 32004.22151936426}
I0728 09:27:06.272570 140143444526912 train.py:379] starting iteration 197, 387317760 steps, 5048.488557815552
I0728 09:27:31.737220 140143444526912 train.py:394] {'eval/walltime': 809.9655661582947, 'training/sps': 91700.76318047543, 'training/walltime': 4253.564764261246, 'training/entropy_loss': Array(-0.0539834, dtype=float32), 'training/policy_loss': Array(0.00017198, dtype=float32), 'training/total_loss': Array(210450.77, dtype=float32), 'training/v_loss': Array(210450.8, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29643258, 0.13309571], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.018005, 11.516146], dtype=float32), 'eval/episode_reward': Array([-24349.027,   9363.251], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29129434, 0.1362584 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020055055618286, 'eval/sps': 31840.359952561284}
I0728 09:27:31.739163 140143444526912 train.py:379] starting iteration 198, 389283840 steps, 5073.955151796341
I0728 09:27:57.217568 140143444526912 train.py:394] {'eval/walltime': 813.9979996681213, 'training/sps': 91697.31050776738, 'training/walltime': 4275.005742073059, 'training/entropy_loss': Array(-0.05397174, dtype=float32), 'training/policy_loss': Array(0.00013837, dtype=float32), 'training/total_loss': Array(212876.23, dtype=float32), 'training/v_loss': Array(212876.27, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30359507, 0.12980175], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.642147, 11.121587], dtype=float32), 'eval/episode_reward': Array([-24747.738,   9519.053], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29906535, 0.13223532], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.03243350982666, 'eval/sps': 31742.618864781296}
I0728 09:27:57.219476 140143444526912 train.py:379] starting iteration 199, 391249920 steps, 5099.435463428497
I0728 09:28:22.638509 140143444526912 train.py:394] {'eval/walltime': 817.9948189258575, 'training/sps': 91796.74560693279, 'training/walltime': 4296.423494815826, 'training/entropy_loss': Array(-0.05401837, dtype=float32), 'training/policy_loss': Array(0.00017739, dtype=float32), 'training/total_loss': Array(209535.34, dtype=float32), 'training/v_loss': Array(209535.38, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29767054, 0.12475691], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.155037, 10.686784], dtype=float32), 'eval/episode_reward': Array([-24846.11 ,   9055.336], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29326236, 0.1268966 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.996819257736206, 'eval/sps': 32025.46618845583}
I0728 09:28:22.640383 140143444526912 train.py:379] starting iteration 200, 393216000 steps, 5124.856369972229
I0728 09:28:48.084975 140143444526912 train.py:394] {'eval/walltime': 822.0054247379303, 'training/sps': 91746.00537716858, 'training/walltime': 4317.853092670441, 'training/entropy_loss': Array(-0.05407181, dtype=float32), 'training/policy_loss': Array(0.00012009, dtype=float32), 'training/total_loss': Array(211428.1, dtype=float32), 'training/v_loss': Array(211428.12, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2853735 , 0.11528716], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.030704,  9.951455], dtype=float32), 'eval/episode_reward': Array([-23644.89 ,   8682.421], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28015882, 0.11849709], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.010605812072754, 'eval/sps': 31915.37787500669}
I0728 09:28:48.169890 140143444526912 train.py:379] starting iteration 201, 395182080 steps, 5150.38587641716
I0728 09:29:13.589037 140143444526912 train.py:394] {'eval/walltime': 826.0007357597351, 'training/sps': 91796.85903407627, 'training/walltime': 4339.270818948746, 'training/entropy_loss': Array(-0.05398177, dtype=float32), 'training/policy_loss': Array(0.00019174, dtype=float32), 'training/total_loss': Array(210861.31, dtype=float32), 'training/v_loss': Array(210861.36, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2962852 , 0.11914293], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.011665 , 10.2574835], dtype=float32), 'eval/episode_reward': Array([-24144.824,   9483.827], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2912253 , 0.12270835], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9953110218048096, 'eval/sps': 32037.5558502022}
I0728 09:29:13.592128 140143444526912 train.py:379] starting iteration 202, 397148160 steps, 5175.808115720749
I0728 09:29:39.062633 140143444526912 train.py:394] {'eval/walltime': 830.0210258960724, 'training/sps': 91676.24117611961, 'training/walltime': 4360.716724395752, 'training/entropy_loss': Array(-0.0539121, dtype=float32), 'training/policy_loss': Array(0.0001341, dtype=float32), 'training/total_loss': Array(211716.62, dtype=float32), 'training/v_loss': Array(211716.66, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30257803, 0.11998778], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.605465, 10.277841], dtype=float32), 'eval/episode_reward': Array([-24377.09  ,   9267.5205], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2979719 , 0.12263954], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.02029013633728, 'eval/sps': 31838.49813302666}
I0728 09:29:39.064471 140143444526912 train.py:379] starting iteration 203, 399114240 steps, 5201.280459403992
I0728 09:30:04.523365 140143444526912 train.py:394] {'eval/walltime': 834.0354263782501, 'training/sps': 91702.09904504832, 'training/walltime': 4382.156582593918, 'training/entropy_loss': Array(-0.05388229, dtype=float32), 'training/policy_loss': Array(0.00026227, dtype=float32), 'training/total_loss': Array(209997.22, dtype=float32), 'training/v_loss': Array(209997.25, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30269396, 0.12132008], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.541035, 10.500297], dtype=float32), 'eval/episode_reward': Array([-24986.502,   9161.649], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29773137, 0.12478527], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.014400482177734, 'eval/sps': 31885.209402566255}
I0728 09:30:04.525203 140143444526912 train.py:379] starting iteration 204, 401080320 steps, 5226.741190433502
I0728 09:30:29.973835 140143444526912 train.py:394] {'eval/walltime': 838.0645740032196, 'training/sps': 91809.19565340536, 'training/walltime': 4403.571430921555, 'training/entropy_loss': Array(-0.05389298, dtype=float32), 'training/policy_loss': Array(0.00025977, dtype=float32), 'training/total_loss': Array(209772.08, dtype=float32), 'training/v_loss': Array(209772.12, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3109312 , 0.13269588], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.260727, 11.415883], dtype=float32), 'eval/episode_reward': Array([-25688.332,  10062.985], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3061484 , 0.13595721], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.029147624969482, 'eval/sps': 31768.505876219788}
I0728 09:30:29.975709 140143444526912 train.py:379] starting iteration 205, 403046400 steps, 5252.191696405411
I0728 09:30:55.372495 140143444526912 train.py:394] {'eval/walltime': 842.0824000835419, 'training/sps': 91982.44695450552, 'training/walltime': 4424.9459438323975, 'training/entropy_loss': Array(-0.05384298, dtype=float32), 'training/policy_loss': Array(0.00022201, dtype=float32), 'training/total_loss': Array(208279.86, dtype=float32), 'training/v_loss': Array(208279.9, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31271946, 0.13027456], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.40706  , 11.2395935], dtype=float32), 'eval/episode_reward': Array([-24772.201,   9452.434], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30811816, 0.1333846 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.017826080322266, 'eval/sps': 31858.024075978235}
I0728 09:30:55.374353 140143444526912 train.py:379] starting iteration 206, 405012480 steps, 5277.590331315994
I0728 09:31:20.785199 140143444526912 train.py:394] {'eval/walltime': 846.1065950393677, 'training/sps': 91951.58794456262, 'training/walltime': 4446.32763004303, 'training/entropy_loss': Array(-0.05377344, dtype=float32), 'training/policy_loss': Array(0.00029099, dtype=float32), 'training/total_loss': Array(207261.3, dtype=float32), 'training/v_loss': Array(207261.34, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29567456, 0.12648024], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.936867, 10.886557], dtype=float32), 'eval/episode_reward': Array([-24605.137,   9439.936], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29087263, 0.12928018], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024194955825806, 'eval/sps': 31807.60410593306}
I0728 09:31:20.787178 140143444526912 train.py:379] starting iteration 207, 406978560 steps, 5303.003164768219
I0728 09:31:46.174851 140143444526912 train.py:394] {'eval/walltime': 850.1261723041534, 'training/sps': 92029.87994397366, 'training/walltime': 4467.691126346588, 'training/entropy_loss': Array(-0.05375556, dtype=float32), 'training/policy_loss': Array(0.00038439, dtype=float32), 'training/total_loss': Array(205403.75, dtype=float32), 'training/v_loss': Array(205403.81, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28647608, 0.12340591], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.206675, 10.669764], dtype=float32), 'eval/episode_reward': Array([-23650.414,   9607.384], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28142434, 0.12670025], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.019577264785767, 'eval/sps': 31844.14468689709}
I0728 09:31:46.176693 140143444526912 train.py:379] starting iteration 208, 408944640 steps, 5328.392680883408
I0728 09:32:11.564244 140143444526912 train.py:394] {'eval/walltime': 854.1487028598785, 'training/sps': 92042.1066860119, 'training/walltime': 4489.051784753799, 'training/entropy_loss': Array(-0.05385349, dtype=float32), 'training/policy_loss': Array(0.00035575, dtype=float32), 'training/total_loss': Array(201945.72, dtype=float32), 'training/v_loss': Array(201945.77, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2875537, 0.1230005], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.242525, 10.63844 ], dtype=float32), 'eval/episode_reward': Array([-23210.57 ,   8922.404], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28190982, 0.12699117], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022530555725098, 'eval/sps': 31820.765119564603}
I0728 09:32:11.566107 140143444526912 train.py:379] starting iteration 209, 410910720 steps, 5353.782095193863
I0728 09:32:36.966597 140143444526912 train.py:394] {'eval/walltime': 858.1818187236786, 'training/sps': 92032.78456671306, 'training/walltime': 4510.414606809616, 'training/entropy_loss': Array(-0.05385319, dtype=float32), 'training/policy_loss': Array(0.00043352, dtype=float32), 'training/total_loss': Array(200097.61, dtype=float32), 'training/v_loss': Array(200097.66, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28818655, 0.13124819], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.331068, 11.312908], dtype=float32), 'eval/episode_reward': Array([-24113.3  ,   9934.952], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28312287, 0.13469718], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.033115863800049, 'eval/sps': 31737.248401139885}
I0728 09:32:36.968459 140143444526912 train.py:379] starting iteration 210, 412876800 steps, 5379.184447288513
I0728 09:33:02.410520 140143444526912 train.py:394] {'eval/walltime': 862.1879749298096, 'training/sps': 91739.22002913464, 'training/walltime': 4531.845789670944, 'training/entropy_loss': Array(-0.05368555, dtype=float32), 'training/policy_loss': Array(0.00060353, dtype=float32), 'training/total_loss': Array(200991.75, dtype=float32), 'training/v_loss': Array(200991.78, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2805577 , 0.11816882], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.698149, 10.185072], dtype=float32), 'eval/episode_reward': Array([-23796.146,   8652.226], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2758252 , 0.12096302], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0061562061309814, 'eval/sps': 31950.826032222627}
I0728 09:33:02.412394 140143444526912 train.py:379] starting iteration 211, 414842880 steps, 5404.628381490707
I0728 09:33:27.827063 140143444526912 train.py:394] {'eval/walltime': 866.1912322044373, 'training/sps': 91844.66337077426, 'training/walltime': 4553.252368211746, 'training/entropy_loss': Array(-0.0534846, dtype=float32), 'training/policy_loss': Array(0.00085367, dtype=float32), 'training/total_loss': Array(197742.62, dtype=float32), 'training/v_loss': Array(197742.69, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26815337, 0.12647134], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.670101, 10.843235], dtype=float32), 'eval/episode_reward': Array([-22757.254,   8553.797], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26285443, 0.129882  ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0032572746276855, 'eval/sps': 31973.963005388996}
I0728 09:33:27.829066 140143444526912 train.py:379] starting iteration 212, 416808960 steps, 5430.04505443573
I0728 09:33:53.213853 140143444526912 train.py:394] {'eval/walltime': 870.1903595924377, 'training/sps': 91956.5979073693, 'training/walltime': 4574.632889509201, 'training/entropy_loss': Array(-0.05323789, dtype=float32), 'training/policy_loss': Array(0.00108105, dtype=float32), 'training/total_loss': Array(195510.62, dtype=float32), 'training/v_loss': Array(195510.66, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29261982, 0.14521816], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.69812 , 12.489776], dtype=float32), 'eval/episode_reward': Array([-24285.875,  10083.228], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28703704, 0.14917158], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9991273880004883, 'eval/sps': 32006.982419231794}
I0728 09:33:53.215794 140143444526912 train.py:379] starting iteration 213, 418775040 steps, 5455.431781768799
I0728 09:34:18.619602 140143444526912 train.py:394] {'eval/walltime': 874.1908340454102, 'training/sps': 91878.5256936325, 'training/walltime': 4596.031578540802, 'training/entropy_loss': Array(-0.05293261, dtype=float32), 'training/policy_loss': Array(0.00136211, dtype=float32), 'training/total_loss': Array(193798.78, dtype=float32), 'training/v_loss': Array(193798.81, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28942   , 0.14100982], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.46334 , 12.056767], dtype=float32), 'eval/episode_reward': Array([-23394.145,  10217.087], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28489026, 0.14355494], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.000474452972412, 'eval/sps': 31996.204826378555}
I0728 09:34:18.621478 140143444526912 train.py:379] starting iteration 214, 420741120 steps, 5480.837466001511
I0728 09:34:44.060003 140143444526912 train.py:394] {'eval/walltime': 878.2133965492249, 'training/sps': 91824.39435563853, 'training/walltime': 4617.442882299423, 'training/entropy_loss': Array(-0.05258405, dtype=float32), 'training/policy_loss': Array(0.00155574, dtype=float32), 'training/total_loss': Array(190582.1, dtype=float32), 'training/v_loss': Array(190582.16, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.29196173, 0.12392387], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.669863, 10.717338], dtype=float32), 'eval/episode_reward': Array([-24558.355,   8726.453], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.28733853, 0.12681122], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.022562503814697, 'eval/sps': 31820.512391942793}
I0728 09:34:44.061939 140143444526912 train.py:379] starting iteration 215, 422707200 steps, 5506.277926206589
I0728 09:35:09.469807 140143444526912 train.py:394] {'eval/walltime': 882.2282485961914, 'training/sps': 91930.52971904086, 'training/walltime': 4638.829466342926, 'training/entropy_loss': Array(-0.05216305, dtype=float32), 'training/policy_loss': Array(0.00181221, dtype=float32), 'training/total_loss': Array(188010.23, dtype=float32), 'training/v_loss': Array(188010.28, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27336326, 0.11580232], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.03314  ,  9.9731045], dtype=float32), 'eval/episode_reward': Array([-23027.234,   9369.631], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26765704, 0.11995322], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.014852046966553, 'eval/sps': 31881.62315886864}
I0728 09:35:09.473970 140143444526912 train.py:379] starting iteration 216, 424673280 steps, 5531.689943313599
I0728 09:35:34.899886 140143444526912 train.py:394] {'eval/walltime': 886.2528636455536, 'training/sps': 91889.78455278437, 'training/walltime': 4660.225533485413, 'training/entropy_loss': Array(-0.05157096, dtype=float32), 'training/policy_loss': Array(0.00230926, dtype=float32), 'training/total_loss': Array(182270.3, dtype=float32), 'training/v_loss': Array(182270.34, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27112618, 0.124487  ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.89098 , 10.745187], dtype=float32), 'eval/episode_reward': Array([-22757.672,   9444.294], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26591527, 0.12818249], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024615049362183, 'eval/sps': 31804.283994884263}
I0728 09:35:34.901911 140143444526912 train.py:379] starting iteration 217, 426639360 steps, 5557.11789894104
I0728 09:36:00.309268 140143444526912 train.py:394] {'eval/walltime': 890.2687711715698, 'training/sps': 91930.13310550991, 'training/walltime': 4681.6122097969055, 'training/entropy_loss': Array(-0.05071079, dtype=float32), 'training/policy_loss': Array(0.00292968, dtype=float32), 'training/total_loss': Array(176977., dtype=float32), 'training/v_loss': Array(176977.05, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26607105, 0.1212285 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.476423, 10.405018], dtype=float32), 'eval/episode_reward': Array([-22688.605,   9335.166], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26054564, 0.12485809], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.015907526016235, 'eval/sps': 31873.243885915745}
I0728 09:36:00.311302 140143444526912 train.py:379] starting iteration 218, 428605440 steps, 5582.52729010582
I0728 09:36:25.706412 140143444526912 train.py:394] {'eval/walltime': 894.2652065753937, 'training/sps': 91898.5574613802, 'training/walltime': 4703.006234407425, 'training/entropy_loss': Array(-0.04994768, dtype=float32), 'training/policy_loss': Array(0.00316477, dtype=float32), 'training/total_loss': Array(173864.58, dtype=float32), 'training/v_loss': Array(173864.62, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26707622, 0.12188525], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.551378, 10.471081], dtype=float32), 'eval/episode_reward': Array([-23337.168,   8898.655], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2623244 , 0.12463329], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9964354038238525, 'eval/sps': 32028.542204767673}
I0728 09:36:25.708426 140143444526912 train.py:379] starting iteration 219, 430571520 steps, 5607.924413919449
I0728 09:36:51.099447 140143444526912 train.py:394] {'eval/walltime': 898.2859916687012, 'training/sps': 92023.36061685761, 'training/walltime': 4724.371244192123, 'training/entropy_loss': Array(-0.04894365, dtype=float32), 'training/policy_loss': Array(0.0037696, dtype=float32), 'training/total_loss': Array(170394.12, dtype=float32), 'training/v_loss': Array(170394.16, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25318396, 0.11044358], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.375095,  9.482746], dtype=float32), 'eval/episode_reward': Array([-21551.736,   8979.285], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2479783 , 0.11318266], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020785093307495, 'eval/sps': 31834.578827168127}
I0728 09:36:51.101477 140143444526912 train.py:379] starting iteration 220, 432537600 steps, 5633.317465305328
I0728 09:37:16.566040 140143444526912 train.py:394] {'eval/walltime': 902.3003375530243, 'training/sps': 91685.52993903795, 'training/walltime': 4745.814976930618, 'training/entropy_loss': Array(-0.04832219, dtype=float32), 'training/policy_loss': Array(0.00354877, dtype=float32), 'training/total_loss': Array(166420.97, dtype=float32), 'training/v_loss': Array(166421.03, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28164837, 0.11916332], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.796253, 10.24205 ], dtype=float32), 'eval/episode_reward': Array([-23651.05 ,   8852.171], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27655214, 0.12223358], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.01434588432312, 'eval/sps': 31885.643063261537}
I0728 09:37:16.568135 140143444526912 train.py:379] starting iteration 221, 434503680 steps, 5658.784122467041
I0728 09:37:41.943490 140143444526912 train.py:394] {'eval/walltime': 906.3087990283966, 'training/sps': 92036.44333760186, 'training/walltime': 4767.176949739456, 'training/entropy_loss': Array(-0.04786298, dtype=float32), 'training/policy_loss': Array(0.00368763, dtype=float32), 'training/total_loss': Array(159762.44, dtype=float32), 'training/v_loss': Array(159762.47, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26558453, 0.12064545], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.437244, 10.367442], dtype=float32), 'eval/episode_reward': Array([-23038.043,   8729.376], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26044694, 0.12382293], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0084614753723145, 'eval/sps': 31932.451087885554}
I0728 09:37:41.945510 140143444526912 train.py:379] starting iteration 222, 436469760 steps, 5684.161498308182
I0728 09:38:07.363538 140143444526912 train.py:394] {'eval/walltime': 910.3144261837006, 'training/sps': 91841.6263861006, 'training/walltime': 4788.5842361450195, 'training/entropy_loss': Array(-0.04703025, dtype=float32), 'training/policy_loss': Array(0.00428848, dtype=float32), 'training/total_loss': Array(154161.69, dtype=float32), 'training/v_loss': Array(154161.73, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27981216, 0.11324064], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.65247 ,  9.753019], dtype=float32), 'eval/episode_reward': Array([-23825.887,   8237.797], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27478898, 0.11596171], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.005627155303955, 'eval/sps': 31955.045998355556}
I0728 09:38:07.365562 140143444526912 train.py:379] starting iteration 223, 438435840 steps, 5709.581550359726
I0728 09:38:32.843987 140143444526912 train.py:394] {'eval/walltime': 914.3356862068176, 'training/sps': 91648.13662895861, 'training/walltime': 4810.036718130112, 'training/entropy_loss': Array(-0.04628485, dtype=float32), 'training/policy_loss': Array(0.00465905, dtype=float32), 'training/total_loss': Array(150750.44, dtype=float32), 'training/v_loss': Array(150750.48, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27178386, 0.11723752], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.06155 , 10.012203], dtype=float32), 'eval/episode_reward': Array([-22997.441,   9081.291], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26712212, 0.12020751], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021260023117065, 'eval/sps': 31830.81901298719}
I0728 09:38:32.845955 140143444526912 train.py:379] starting iteration 224, 440401920 steps, 5735.061943054199
I0728 09:38:58.285568 140143444526912 train.py:394] {'eval/walltime': 918.3656644821167, 'training/sps': 91851.47046475568, 'training/walltime': 4831.441710233688, 'training/entropy_loss': Array(-0.04530051, dtype=float32), 'training/policy_loss': Array(0.00549294, dtype=float32), 'training/total_loss': Array(145682.84, dtype=float32), 'training/v_loss': Array(145682.9, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2702844 , 0.12038349], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.802696, 10.422883], dtype=float32), 'eval/episode_reward': Array([-22769.984,   9595.422], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26438817, 0.12430618], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.029978275299072, 'eval/sps': 31761.957821100383}
I0728 09:38:58.287655 140143444526912 train.py:379] starting iteration 225, 442368000 steps, 5760.503643512726
I0728 09:39:23.684779 140143444526912 train.py:394] {'eval/walltime': 922.3960251808167, 'training/sps': 92037.50856504805, 'training/walltime': 4852.80343580246, 'training/entropy_loss': Array(-0.0440256, dtype=float32), 'training/policy_loss': Array(0.00689954, dtype=float32), 'training/total_loss': Array(143966.44, dtype=float32), 'training/v_loss': Array(143966.47, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.26439196, 0.10940981], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.355421,  9.458477], dtype=float32), 'eval/episode_reward': Array([-22960.896,   8477.279], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.25921082, 0.11256161], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.030360698699951, 'eval/sps': 31758.944067038014}
I0728 09:39:23.775912 140143444526912 train.py:379] starting iteration 226, 444334080 steps, 5785.991897821426
I0728 09:39:49.218564 140143444526912 train.py:394] {'eval/walltime': 926.4077603816986, 'training/sps': 91760.8768420318, 'training/walltime': 4874.229560613632, 'training/entropy_loss': Array(-0.04224895, dtype=float32), 'training/policy_loss': Array(0.0082249, dtype=float32), 'training/total_loss': Array(140728.86, dtype=float32), 'training/v_loss': Array(140728.9, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.24518815, 0.11282753], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.67802 ,  9.743898], dtype=float32), 'eval/episode_reward': Array([-21433.535,   8727.313], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2390592 , 0.11680774], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.011735200881958, 'eval/sps': 31906.393017131315}
I0728 09:39:49.220649 140143444526912 train.py:379] starting iteration 227, 446300160 steps, 5811.4366364479065
I0728 09:40:14.684249 140143444526912 train.py:394] {'eval/walltime': 930.4344961643219, 'training/sps': 91749.11158059309, 'training/walltime': 4895.65843296051, 'training/entropy_loss': Array(-0.04022022, dtype=float32), 'training/policy_loss': Array(0.00930539, dtype=float32), 'training/total_loss': Array(137700.08, dtype=float32), 'training/v_loss': Array(137700.11, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25695145, 0.11173811], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.749294,  9.638753], dtype=float32), 'eval/episode_reward': Array([-22710.719,   8736.143], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2508877 , 0.11604585], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.026735782623291, 'eval/sps': 31787.533851205913}
I0728 09:40:14.688009 140143444526912 train.py:379] starting iteration 228, 448266240 steps, 5836.9039833545685
I0728 09:40:40.105005 140143444526912 train.py:394] {'eval/walltime': 934.4352579116821, 'training/sps': 91824.51603098513, 'training/walltime': 4917.069708347321, 'training/entropy_loss': Array(-0.03796282, dtype=float32), 'training/policy_loss': Array(0.01062788, dtype=float32), 'training/total_loss': Array(134650.73, dtype=float32), 'training/v_loss': Array(134650.75, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23102063, 0.10291646], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.529852,  8.814966], dtype=float32), 'eval/episode_reward': Array([-20677.53 ,   8116.394], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22478662, 0.10689879], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0007617473602295, 'eval/sps': 31993.90718141528}
I0728 09:40:40.106866 140143444526912 train.py:379] starting iteration 229, 450232320 steps, 5862.322854042053
I0728 09:41:05.485535 140143444526912 train.py:394] {'eval/walltime': 938.4222357273102, 'training/sps': 91928.91971482108, 'training/walltime': 4938.456666946411, 'training/entropy_loss': Array(-0.03558366, dtype=float32), 'training/policy_loss': Array(0.0121752, dtype=float32), 'training/total_loss': Array(132242.36, dtype=float32), 'training/v_loss': Array(132242.38, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.22236465, 0.10608646], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.769276,  9.106719], dtype=float32), 'eval/episode_reward': Array([-20072.977,   8775.956], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.21561669, 0.11016027], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9869778156280518, 'eval/sps': 32104.517737286857}
I0728 09:41:05.487386 140143444526912 train.py:379] starting iteration 230, 452198400 steps, 5887.703373908997
I0728 09:41:30.886894 140143444526912 train.py:394] {'eval/walltime': 942.4220893383026, 'training/sps': 91893.36540116461, 'training/walltime': 4959.851900339127, 'training/entropy_loss': Array(-0.03292149, dtype=float32), 'training/policy_loss': Array(0.01370415, dtype=float32), 'training/total_loss': Array(129656.086, dtype=float32), 'training/v_loss': Array(129656.1, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.24242379, 0.11630636], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.460201, 10.034448], dtype=float32), 'eval/episode_reward': Array([-21803.555,   8799.346], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.23616923, 0.11991876], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9998536109924316, 'eval/sps': 32001.171154921598}
I0728 09:41:30.889365 140143444526912 train.py:379] starting iteration 231, 454164480 steps, 5913.105345726013
I0728 09:41:56.344244 140143444526912 train.py:394] {'eval/walltime': 946.4136619567871, 'training/sps': 91622.60846766054, 'training/walltime': 4981.310359477997, 'training/entropy_loss': Array(-0.02992055, dtype=float32), 'training/policy_loss': Array(0.01469488, dtype=float32), 'training/total_loss': Array(126378.36, dtype=float32), 'training/v_loss': Array(126378.375, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2306114 , 0.11238807], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.490532,  9.641355], dtype=float32), 'eval/episode_reward': Array([-21104.84,   8645.91], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22400564, 0.11596658], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.991572618484497, 'eval/sps': 32067.56139353378}
I0728 09:41:56.346160 140143444526912 train.py:379] starting iteration 232, 456130560 steps, 5938.56214761734
I0728 09:42:21.759656 140143444526912 train.py:394] {'eval/walltime': 950.4181079864502, 'training/sps': 91852.73808287026, 'training/walltime': 5002.715056180954, 'training/entropy_loss': Array(-0.02641291, dtype=float32), 'training/policy_loss': Array(0.01672309, dtype=float32), 'training/total_loss': Array(124258.37, dtype=float32), 'training/v_loss': Array(124258.375, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23510645, 0.11293229], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.911314,  9.68834 ], dtype=float32), 'eval/episode_reward': Array([-21785.703,   9250.797], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22861624, 0.11712258], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.004446029663086, 'eval/sps': 31964.471253160897}
I0728 09:42:21.761594 140143444526912 train.py:379] starting iteration 233, 458096640 steps, 5963.977581739426
I0728 09:42:47.226571 140143444526912 train.py:394] {'eval/walltime': 954.4338283538818, 'training/sps': 91682.30062422132, 'training/walltime': 5024.159544229507, 'training/entropy_loss': Array(-0.02194306, dtype=float32), 'training/policy_loss': Array(0.01760987, dtype=float32), 'training/total_loss': Array(122265.555, dtype=float32), 'training/v_loss': Array(122265.555, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.20858897, 0.09706314], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([17.592995,  8.35486 ], dtype=float32), 'eval/episode_reward': Array([-19861.102 ,   7823.0864], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20156321, 0.10067935], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.015720367431641, 'eval/sps': 31874.729385568688}
I0728 09:42:47.228483 140143444526912 train.py:379] starting iteration 234, 460062720 steps, 5989.444471120834
I0728 09:43:12.633217 140143444526912 train.py:394] {'eval/walltime': 958.4555540084839, 'training/sps': 91964.59385602825, 'training/walltime': 5045.538206577301, 'training/entropy_loss': Array(-0.0174626, dtype=float32), 'training/policy_loss': Array(0.01827138, dtype=float32), 'training/total_loss': Array(119993.87, dtype=float32), 'training/v_loss': Array(119993.87, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23573194, 0.12741128], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.922081, 10.949134], dtype=float32), 'eval/episode_reward': Array([-21635.56,   9725.42], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22874749, 0.13127665], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.021725654602051, 'eval/sps': 31827.133671718733}
I0728 09:43:12.635168 140143444526912 train.py:379] starting iteration 235, 462028800 steps, 6014.851155757904
I0728 09:43:38.068354 140143444526912 train.py:394] {'eval/walltime': 962.451961517334, 'training/sps': 91734.77457818524, 'training/walltime': 5066.97042798996, 'training/entropy_loss': Array(-0.01214443, dtype=float32), 'training/policy_loss': Array(0.02020679, dtype=float32), 'training/total_loss': Array(118234.125, dtype=float32), 'training/v_loss': Array(118234.12, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.22101668, 0.11773649], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.720917, 10.09793 ], dtype=float32), 'eval/episode_reward': Array([-21465.734,   8988.126], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.21413846, 0.12132221], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9964075088500977, 'eval/sps': 32028.765764387714}
I0728 09:43:38.070297 140143444526912 train.py:379] starting iteration 236, 463994880 steps, 6040.2862849235535
I0728 09:44:03.540499 140143444526912 train.py:394] {'eval/walltime': 966.4702513217926, 'training/sps': 91671.12617808866, 'training/walltime': 5088.417530059814, 'training/entropy_loss': Array(-0.00544978, dtype=float32), 'training/policy_loss': Array(0.0222474, dtype=float32), 'training/total_loss': Array(116110.75, dtype=float32), 'training/v_loss': Array(116110.734, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.22149026, 0.10678176], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.765545,  9.212662], dtype=float32), 'eval/episode_reward': Array([-20406.48 ,   8390.789], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.21487983, 0.11007637], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.018289804458618, 'eval/sps': 31854.347553024578}
I0728 09:44:03.542469 140143444526912 train.py:379] starting iteration 237, 465960960 steps, 6065.758456707001
I0728 09:44:29.028364 140143444526912 train.py:394] {'eval/walltime': 970.4956474304199, 'training/sps': 91633.78127026319, 'training/walltime': 5109.873372793198, 'training/entropy_loss': Array(0.00145706, dtype=float32), 'training/policy_loss': Array(0.02319559, dtype=float32), 'training/total_loss': Array(115321.34, dtype=float32), 'training/v_loss': Array(115321.32, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23855957, 0.11970314], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.202541, 10.292135], dtype=float32), 'eval/episode_reward': Array([-21347.8 ,   9103.01], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.23194234, 0.12347188], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.025396108627319, 'eval/sps': 31798.11291755053}
I0728 09:44:29.030400 140143444526912 train.py:379] starting iteration 238, 467927040 steps, 6091.246387481689
I0728 09:44:54.438845 140143444526912 train.py:394] {'eval/walltime': 974.5233554840088, 'training/sps': 91977.5265051137, 'training/walltime': 5131.249029159546, 'training/entropy_loss': Array(0.00923709, dtype=float32), 'training/policy_loss': Array(0.02417721, dtype=float32), 'training/total_loss': Array(114551., dtype=float32), 'training/v_loss': Array(114550.97, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23921232, 0.11582455], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.20865 ,  9.976013], dtype=float32), 'eval/episode_reward': Array([-21804.959,   8467.877], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.232108  , 0.12014935], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.027708053588867, 'eval/sps': 31779.86048068859}
I0728 09:44:54.440842 140143444526912 train.py:379] starting iteration 239, 469893120 steps, 6116.65683054924
I0728 09:45:19.895464 140143444526912 train.py:394] {'eval/walltime': 978.5376477241516, 'training/sps': 91721.34593246425, 'training/walltime': 5152.684388399124, 'training/entropy_loss': Array(0.01756177, dtype=float32), 'training/policy_loss': Array(0.02593483, dtype=float32), 'training/total_loss': Array(113148.38, dtype=float32), 'training/v_loss': Array(113148.34, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.22332281, 0.10575304], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.874954,  9.136747], dtype=float32), 'eval/episode_reward': Array([-20040.508,   8395.456], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2159955 , 0.11030535], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.014292240142822, 'eval/sps': 31886.069160586565}
I0728 09:45:19.897452 140143444526912 train.py:379] starting iteration 240, 471859200 steps, 6142.113440513611
I0728 09:45:45.302880 140143444526912 train.py:394] {'eval/walltime': 982.5399150848389, 'training/sps': 91881.52007402098, 'training/walltime': 5174.082380056381, 'training/entropy_loss': Array(0.02665226, dtype=float32), 'training/policy_loss': Array(0.02880981, dtype=float32), 'training/total_loss': Array(112160.76, dtype=float32), 'training/v_loss': Array(112160.7, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.24019568, 0.11818436], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.347227, 10.197071], dtype=float32), 'eval/episode_reward': Array([-21523.75 ,   9021.305], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.23424795, 0.12130366], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.002267360687256, 'eval/sps': 31981.871390526063}
I0728 09:45:45.305022 140143444526912 train.py:379] starting iteration 241, 473825280 steps, 6167.521008729935
I0728 09:46:10.755174 140143444526912 train.py:394] {'eval/walltime': 986.5645451545715, 'training/sps': 91785.80784664441, 'training/walltime': 5195.502685070038, 'training/entropy_loss': Array(0.03669319, dtype=float32), 'training/policy_loss': Array(0.02964479, dtype=float32), 'training/total_loss': Array(110672., dtype=float32), 'training/v_loss': Array(110671.94, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23088007, 0.1160876 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.49852 , 10.074717], dtype=float32), 'eval/episode_reward': Array([-20696.273,   9140.667], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.22388119, 0.11983662], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.024630069732666, 'eval/sps': 31804.165297731906}
I0728 09:46:10.757240 140143444526912 train.py:379] starting iteration 242, 475791360 steps, 6192.973227739334
I0728 09:46:36.200592 140143444526912 train.py:394] {'eval/walltime': 990.5973551273346, 'training/sps': 91848.20694392612, 'training/walltime': 5216.908437728882, 'training/entropy_loss': Array(0.04754602, dtype=float32), 'training/policy_loss': Array(0.0298037, dtype=float32), 'training/total_loss': Array(109985.93, dtype=float32), 'training/v_loss': Array(109985.84, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2408621, 0.1187909], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.371412, 10.1956  ], dtype=float32), 'eval/episode_reward': Array([-21452.531,   9288.112], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.23472571, 0.12193817], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0328099727630615, 'eval/sps': 31739.65569032289}
I0728 09:46:36.202568 140143444526912 train.py:379] starting iteration 243, 477757440 steps, 6218.418555259705
I0728 09:47:01.644215 140143444526912 train.py:394] {'eval/walltime': 994.6024763584137, 'training/sps': 91737.43608200204, 'training/walltime': 5238.340037345886, 'training/entropy_loss': Array(0.06031543, dtype=float32), 'training/policy_loss': Array(0.0317431, dtype=float32), 'training/total_loss': Array(108325.52, dtype=float32), 'training/v_loss': Array(108325.43, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23738322, 0.1176502 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.071722, 10.196736], dtype=float32), 'eval/episode_reward': Array([-21097.219,   8712.557], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.23004624, 0.12266992], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.005121231079102, 'eval/sps': 31959.082538311308}
I0728 09:47:01.646276 140143444526912 train.py:379] starting iteration 244, 479723520 steps, 6243.862263202667
I0728 09:47:27.093200 140143444526912 train.py:394] {'eval/walltime': 998.6258883476257, 'training/sps': 91800.45308250221, 'training/walltime': 5259.756925106049, 'training/entropy_loss': Array(0.07261758, dtype=float32), 'training/policy_loss': Array(0.03188996, dtype=float32), 'training/total_loss': Array(106651.484, dtype=float32), 'training/v_loss': Array(106651.375, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2509151 , 0.10779212], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.210562 ,  9.3303585], dtype=float32), 'eval/episode_reward': Array([-21844.408,   8659.285], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24471335, 0.11140164], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023411989212036, 'eval/sps': 31813.79394981326}
I0728 09:47:27.095954 140143444526912 train.py:379] starting iteration 245, 481689600 steps, 6269.311942100525
I0728 09:47:52.593710 140143444526912 train.py:394] {'eval/walltime': 1002.6554155349731, 'training/sps': 91600.84592265828, 'training/walltime': 5281.220482349396, 'training/entropy_loss': Array(0.08624405, dtype=float32), 'training/policy_loss': Array(0.03491441, dtype=float32), 'training/total_loss': Array(105170.83, dtype=float32), 'training/v_loss': Array(105170.7, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2487878 , 0.10427928], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.100086,  9.007306], dtype=float32), 'eval/episode_reward': Array([-22130.01 ,   8384.989], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24277087, 0.10748402], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.029527187347412, 'eval/sps': 31765.513433416196}
I0728 09:47:52.595702 140143444526912 train.py:379] starting iteration 246, 483655680 steps, 6294.81168961525
I0728 09:48:17.994019 140143444526912 train.py:394] {'eval/walltime': 1006.6625199317932, 'training/sps': 91930.93760939415, 'training/walltime': 5302.606971502304, 'training/entropy_loss': Array(0.10008311, dtype=float32), 'training/policy_loss': Array(0.03562636, dtype=float32), 'training/total_loss': Array(104345.9, dtype=float32), 'training/v_loss': Array(104345.75, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2789441 , 0.10749416], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.706333,  9.220771], dtype=float32), 'eval/episode_reward': Array([-24131.816,   8796.306], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2741632 , 0.10987806], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.007104396820068, 'eval/sps': 31943.265591377505}
I0728 09:48:17.996067 140143444526912 train.py:379] starting iteration 247, 485621760 steps, 6320.212054014206
I0728 09:48:43.374128 140143444526912 train.py:394] {'eval/walltime': 1010.6555731296539, 'training/sps': 91958.68162202058, 'training/walltime': 5323.987008333206, 'training/entropy_loss': Array(0.11446951, dtype=float32), 'training/policy_loss': Array(0.04000292, dtype=float32), 'training/total_loss': Array(101427.11, dtype=float32), 'training/v_loss': Array(101426.96, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.25342435, 0.10444444], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([21.487057,  9.076167], dtype=float32), 'eval/episode_reward': Array([-22418.488,   8440.87 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24784787, 0.10734256], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9930531978607178, 'eval/sps': 32055.67110114539}
I0728 09:48:43.376103 140143444526912 train.py:379] starting iteration 248, 487587840 steps, 6345.592091083527
I0728 09:49:08.807458 140143444526912 train.py:394] {'eval/walltime': 1014.649252653122, 'training/sps': 91732.5897704235, 'training/walltime': 5345.419740200043, 'training/entropy_loss': Array(0.1313972, dtype=float32), 'training/policy_loss': Array(0.04939244, dtype=float32), 'training/total_loss': Array(99061.01, dtype=float32), 'training/v_loss': Array(99060.83, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2804939 , 0.11042207], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.823275,  9.570212], dtype=float32), 'eval/episode_reward': Array([-23468.283,   8956.066], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27541485, 0.11317676], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9936795234680176, 'eval/sps': 32050.64383554938}
I0728 09:49:08.809417 140143444526912 train.py:379] starting iteration 249, 489553920 steps, 6371.025404930115
I0728 09:49:34.223930 140143444526912 train.py:394] {'eval/walltime': 1018.6521987915039, 'training/sps': 91844.42502823744, 'training/walltime': 5366.826374292374, 'training/entropy_loss': Array(0.1505704, dtype=float32), 'training/policy_loss': Array(0.04616294, dtype=float32), 'training/total_loss': Array(95653.234, dtype=float32), 'training/v_loss': Array(95653.05, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27041346, 0.10555096], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.989151,  9.136882], dtype=float32), 'eval/episode_reward': Array([-22960.053,   8701.014], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26522046, 0.10813948], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.002946138381958, 'eval/sps': 31976.448239630634}
I0728 09:49:34.225878 140143444526912 train.py:379] starting iteration 250, 491520000 steps, 6396.441864967346
I0728 09:49:59.645015 140143444526912 train.py:394] {'eval/walltime': 1022.6546363830566, 'training/sps': 91822.56006217579, 'training/walltime': 5388.238105773926, 'training/entropy_loss': Array(0.1675662, dtype=float32), 'training/policy_loss': Array(0.05022044, dtype=float32), 'training/total_loss': Array(95376.21, dtype=float32), 'training/v_loss': Array(95375.99, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2718273 , 0.10274401], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.127708,  8.935501], dtype=float32), 'eval/episode_reward': Array([-22643.262,   8541.7  ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26614922, 0.10654014], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.002437591552734, 'eval/sps': 31980.511144045788}
I0728 09:49:59.746595 140143444526912 train.py:379] starting iteration 251, 493486080 steps, 6421.962581157684
I0728 09:50:25.158957 140143444526912 train.py:394] {'eval/walltime': 1026.6764199733734, 'training/sps': 91934.17216457025, 'training/walltime': 5409.6238424777985, 'training/entropy_loss': Array(0.19205084, dtype=float32), 'training/policy_loss': Array(0.05284587, dtype=float32), 'training/total_loss': Array(93440.68, dtype=float32), 'training/v_loss': Array(93440.43, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28490776, 0.10002635], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.21356,  8.70029], dtype=float32), 'eval/episode_reward': Array([-23469.148,   8197.635], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2798594 , 0.10313172], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.0217835903167725, 'eval/sps': 31826.675186647273}
I0728 09:50:25.161058 140143444526912 train.py:379] starting iteration 252, 495452160 steps, 6447.377045631409
I0728 09:50:50.627767 140143444526912 train.py:394] {'eval/walltime': 1030.6805322170258, 'training/sps': 91625.43551768654, 'training/walltime': 5431.0816395282745, 'training/entropy_loss': Array(0.21671057, dtype=float32), 'training/policy_loss': Array(0.05166342, dtype=float32), 'training/total_loss': Array(91476.445, dtype=float32), 'training/v_loss': Array(91476.18, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.276351  , 0.10209073], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.427746,  8.94966 ], dtype=float32), 'eval/episode_reward': Array([-22695.11 ,   8506.099], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.27083266, 0.10593525], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.004112243652344, 'eval/sps': 31967.135837142527}
I0728 09:50:50.629766 140143444526912 train.py:379] starting iteration 253, 497418240 steps, 6472.84575343132
I0728 09:51:16.093268 140143444526912 train.py:394] {'eval/walltime': 1034.7009136676788, 'training/sps': 91708.75549390385, 'training/walltime': 5452.519941568375, 'training/entropy_loss': Array(0.24108128, dtype=float32), 'training/policy_loss': Array(0.05177938, dtype=float32), 'training/total_loss': Array(90609.086, dtype=float32), 'training/v_loss': Array(90608.8, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.27488098, 0.10236735], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([23.348879,  8.885176], dtype=float32), 'eval/episode_reward': Array([-22914.05 ,   8811.915], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2692697 , 0.10562333], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.020381450653076, 'eval/sps': 31837.774990034217}
I0728 09:51:16.095316 140143444526912 train.py:379] starting iteration 254, 499384320 steps, 6498.3113033771515
I0728 09:51:41.553056 140143444526912 train.py:394] {'eval/walltime': 1038.724061012268, 'training/sps': 91747.37828702055, 'training/walltime': 5473.94921875, 'training/entropy_loss': Array(0.26071414, dtype=float32), 'training/policy_loss': Array(0.05262351, dtype=float32), 'training/total_loss': Array(88842.17, dtype=float32), 'training/v_loss': Array(88841.86, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2655393 , 0.09052392], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([22.507607,  7.877664], dtype=float32), 'eval/episode_reward': Array([-22292.547 ,   7806.9854], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.26039666, 0.09317821], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.023147344589233, 'eval/sps': 31815.886676919337}
I0728 09:51:42.333304 140143444526912 train.py:410] total steps: 501350400
