I0727 21:43:06.516632 139877795424064 low_level_env.py:188] Initialising environment...
I0727 21:43:45.878141 139877795424064 low_level_env.py:292] Environment initialised.
I0727 21:43:45.884178 139877795424064 train.py:118] JAX is running on GPU.
I0727 21:43:45.884229 139877795424064 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0727 21:43:54.333153 139877795424064 train.py:367] Running initial eval
I0727 21:44:10.154227 139877795424064 train.py:373] {'eval/walltime': 15.699835538864136, 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5832865, dtype=float32), Array(0.31210408, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(718.5954, dtype=float32), Array(2554.985, dtype=float32)), 'eval/episode_reward': (Array(-12817.479, dtype=float32), Array(4992.8003, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5800035, dtype=float32), Array(0.32137194, dtype=float32)), 'eval/avg_episode_length': (Array(930.1328, dtype=float32), Array(254.05441, dtype=float32)), 'eval/epoch_eval_time': 15.699835538864136, 'eval/sps': 8152.9516460948}
I0727 21:44:10.155925 139877795424064 train.py:379] starting iteration 0, 0 steps, 24.27177858352661
I0727 21:44:45.359758 139877795424064 train.py:394] {'eval/walltime': 19.511312007904053, 'training/sps': 13049.763795197969, 'training/walltime': 31.3875412940979, 'training/entropy_loss': Array(-0.04612562, dtype=float32), 'training/policy_loss': Array(0.00711663, dtype=float32), 'training/total_loss': Array(301082.5, dtype=float32), 'training/v_loss': Array(301082.5, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5216111, dtype=float32), Array(0.25655606, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(560.74835, dtype=float32), Array(2272.6665, dtype=float32)), 'eval/episode_reward': (Array(-12342.52, dtype=float32), Array(4683.577, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5175003, dtype=float32), Array(0.26495147, dtype=float32)), 'eval/avg_episode_length': (Array(945.7031, dtype=float32), Array(225.74544, dtype=float32)), 'eval/epoch_eval_time': 3.811476469039917, 'eval/sps': 33582.78636631391}
I0727 21:44:45.382715 139877795424064 train.py:379] starting iteration 1, 409600 steps, 59.498557806015015
I0727 21:44:58.311368 139877795424064 train.py:394] {'eval/walltime': 23.33882164955139, 'training/sps': 45025.865129576414, 'training/walltime': 40.484534740448, 'training/entropy_loss': Array(-0.04739027, dtype=float32), 'training/policy_loss': Array(0.00176869, dtype=float32), 'training/total_loss': Array(246950.19, dtype=float32), 'training/v_loss': Array(246950.25, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.63103753, dtype=float32), Array(0.3268362, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(641.6997, dtype=float32), Array(2418.9333, dtype=float32)), 'eval/episode_reward': (Array(-13374.249, dtype=float32), Array(5297.1777, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.6289713, dtype=float32), Array(0.33765474, dtype=float32)), 'eval/avg_episode_length': (Array(937.8125, dtype=float32), Array(240.85132, dtype=float32)), 'eval/epoch_eval_time': 3.827509641647339, 'eval/sps': 33442.11040181979}
I0727 21:44:58.314037 139877795424064 train.py:379] starting iteration 2, 819200 steps, 72.42989492416382
I0727 21:45:11.260640 139877795424064 train.py:394] {'eval/walltime': 27.173911333084106, 'training/sps': 44973.32295675515, 'training/walltime': 49.592156171798706, 'training/entropy_loss': Array(-0.04967958, dtype=float32), 'training/policy_loss': Array(0.00247215, dtype=float32), 'training/total_loss': Array(238248.62, dtype=float32), 'training/v_loss': Array(238248.66, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.63378227, dtype=float32), Array(0.36524382, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(798.0576, dtype=float32), Array(2681.539, dtype=float32)), 'eval/episode_reward': (Array(-13334.632, dtype=float32), Array(5305.684, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.6315913, dtype=float32), Array(0.37514666, dtype=float32)), 'eval/avg_episode_length': (Array(922.3828, dtype=float32), Array(266.62405, dtype=float32)), 'eval/epoch_eval_time': 3.835089683532715, 'eval/sps': 33376.012182873405}
I0727 21:45:11.263179 139877795424064 train.py:379] starting iteration 3, 1228800 steps, 85.37903594970703
I0727 21:45:24.237551 139877795424064 train.py:394] {'eval/walltime': 31.019298553466797, 'training/sps': 44885.73028659496, 'training/walltime': 58.71755075454712, 'training/entropy_loss': Array(-0.05041635, dtype=float32), 'training/policy_loss': Array(0.00408329, dtype=float32), 'training/total_loss': Array(235780.66, dtype=float32), 'training/v_loss': Array(235780.7, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5978974, dtype=float32), Array(0.34949386, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(797.0996, dtype=float32), Array(2681.8047, dtype=float32)), 'eval/episode_reward': (Array(-12491.461, dtype=float32), Array(4891.276, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.59399444, dtype=float32), Array(0.3606356, dtype=float32)), 'eval/avg_episode_length': (Array(922.33594, dtype=float32), Array(266.78528, dtype=float32)), 'eval/epoch_eval_time': 3.8453872203826904, 'eval/sps': 33286.63478193531}
I0727 21:45:24.240235 139877795424064 train.py:379] starting iteration 4, 1638400 steps, 98.35609245300293
I0727 21:45:37.288124 139877795424064 train.py:394] {'eval/walltime': 34.863142013549805, 'training/sps': 44520.86781005833, 'training/walltime': 67.91773080825806, 'training/entropy_loss': Array(-0.05017755, dtype=float32), 'training/policy_loss': Array(0.00477828, dtype=float32), 'training/total_loss': Array(229911.62, dtype=float32), 'training/v_loss': Array(229911.66, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5305026, dtype=float32), Array(0.2964187, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1264.1405, dtype=float32), Array(3305.349, dtype=float32)), 'eval/episode_reward': (Array(-11492.918, dtype=float32), Array(4505.638, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5243081, dtype=float32), Array(0.30724016, dtype=float32)), 'eval/avg_episode_length': (Array(875.8125, dtype=float32), Array(328.56976, dtype=float32)), 'eval/epoch_eval_time': 3.843843460083008, 'eval/sps': 33300.00332459841}
I0727 21:45:37.290801 139877795424064 train.py:379] starting iteration 5, 2048000 steps, 111.40665769577026
I0727 21:45:50.422885 139877795424064 train.py:394] {'eval/walltime': 38.72931718826294, 'training/sps': 44222.622246943836, 'training/walltime': 77.17995858192444, 'training/entropy_loss': Array(-0.05132925, dtype=float32), 'training/policy_loss': Array(0.00260029, dtype=float32), 'training/total_loss': Array(335362.7, dtype=float32), 'training/v_loss': Array(335362.75, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5517946, dtype=float32), Array(0.2830106, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1030.3252, dtype=float32), Array(3019.0269, dtype=float32)), 'eval/episode_reward': (Array(-12060.492, dtype=float32), Array(3840.3035, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5461142, dtype=float32), Array(0.2932798, dtype=float32)), 'eval/avg_episode_length': (Array(899.1172, dtype=float32), Array(300.05124, dtype=float32)), 'eval/epoch_eval_time': 3.8661751747131348, 'eval/sps': 33107.65658969331}
I0727 21:45:50.425641 139877795424064 train.py:379] starting iteration 6, 2457600 steps, 124.54149746894836
I0727 21:46:03.600173 139877795424064 train.py:394] {'eval/walltime': 42.6371054649353, 'training/sps': 44220.015619205005, 'training/walltime': 86.44273233413696, 'training/entropy_loss': Array(-0.05214727, dtype=float32), 'training/policy_loss': Array(0.00042065, dtype=float32), 'training/total_loss': Array(258903.62, dtype=float32), 'training/v_loss': Array(258903.67, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.55844265, dtype=float32), Array(0.2658729, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(874.21375, dtype=float32), Array(2801.3816, dtype=float32)), 'eval/episode_reward': (Array(-11633.563, dtype=float32), Array(3717.9263, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5527243, dtype=float32), Array(0.27593312, dtype=float32)), 'eval/avg_episode_length': (Array(914.6172, dtype=float32), Array(278.4631, dtype=float32)), 'eval/epoch_eval_time': 3.9077882766723633, 'eval/sps': 32755.101079579745}
I0727 21:46:03.602895 139877795424064 train.py:379] starting iteration 7, 2867200 steps, 137.7187523841858
I0727 21:46:16.875300 139877795424064 train.py:394] {'eval/walltime': 46.65588402748108, 'training/sps': 44282.34572640148, 'training/walltime': 95.69246816635132, 'training/entropy_loss': Array(-0.05259828, dtype=float32), 'training/policy_loss': Array(0.00045775, dtype=float32), 'training/total_loss': Array(249197.64, dtype=float32), 'training/v_loss': Array(249197.7, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5215206, dtype=float32), Array(0.2513777, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1029.541, dtype=float32), Array(3019.0784, dtype=float32)), 'eval/episode_reward': (Array(-11416.615, dtype=float32), Array(3890.342, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5148382, dtype=float32), Array(0.26020068, dtype=float32)), 'eval/avg_episode_length': (Array(899.0781, dtype=float32), Array(300.1674, dtype=float32)), 'eval/epoch_eval_time': 4.018778562545776, 'eval/sps': 31850.473472943933}
I0727 21:46:16.877824 139877795424064 train.py:379] starting iteration 8, 3276800 steps, 150.9936809539795
I0727 21:46:30.192808 139877795424064 train.py:394] {'eval/walltime': 50.695067167282104, 'training/sps': 44175.63291590042, 'training/walltime': 104.96454811096191, 'training/entropy_loss': Array(-0.05265396, dtype=float32), 'training/policy_loss': Array(0.00016155, dtype=float32), 'training/total_loss': Array(233725.58, dtype=float32), 'training/v_loss': Array(233725.62, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.50810975, dtype=float32), Array(0.24590145, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(951.03705, dtype=float32), Array(2913.5261, dtype=float32)), 'eval/episode_reward': (Array(-11717.53, dtype=float32), Array(3941.1882, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.50162196, dtype=float32), Array(0.25464272, dtype=float32)), 'eval/avg_episode_length': (Array(906.9453, dtype=float32), Array(289.31924, dtype=float32)), 'eval/epoch_eval_time': 4.039183139801025, 'eval/sps': 31689.57573097451}
I0727 21:46:30.195383 139877795424064 train.py:379] starting iteration 9, 3686400 steps, 164.3112404346466
I0727 21:46:43.555630 139877795424064 train.py:394] {'eval/walltime': 54.76997375488281, 'training/sps': 44130.89483365852, 'training/walltime': 114.24602770805359, 'training/entropy_loss': Array(-0.05239068, dtype=float32), 'training/policy_loss': Array(0.00029078, dtype=float32), 'training/total_loss': Array(222313.83, dtype=float32), 'training/v_loss': Array(222313.88, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.52128774, dtype=float32), Array(0.2580322, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1029.5054, dtype=float32), Array(3018.939, dtype=float32)), 'eval/episode_reward': (Array(-11411.66, dtype=float32), Array(3838.4905, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.51481223, dtype=float32), Array(0.26627976, dtype=float32)), 'eval/avg_episode_length': (Array(899.0547, dtype=float32), Array(300.23703, dtype=float32)), 'eval/epoch_eval_time': 4.074906587600708, 'eval/sps': 31411.763005680576}
I0727 21:46:43.558304 139877795424064 train.py:379] starting iteration 10, 4096000 steps, 177.67416143417358
I0727 21:46:56.927231 139877795424064 train.py:394] {'eval/walltime': 58.83995532989502, 'training/sps': 44066.242131275154, 'training/walltime': 123.54112482070923, 'training/entropy_loss': Array(-0.05227053, dtype=float32), 'training/policy_loss': Array(0.00202047, dtype=float32), 'training/total_loss': Array(296418.12, dtype=float32), 'training/v_loss': Array(296418.16, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.51568174, dtype=float32), Array(0.22498174, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(951.29395, dtype=float32), Array(2913.4849, dtype=float32)), 'eval/episode_reward': (Array(-11254.417, dtype=float32), Array(3358.6963, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.50965154, dtype=float32), Array(0.23398961, dtype=float32)), 'eval/avg_episode_length': (Array(906.8828, dtype=float32), Array(289.51382, dtype=float32)), 'eval/epoch_eval_time': 4.069981575012207, 'eval/sps': 31449.773823513216}
I0727 21:46:56.929921 139877795424064 train.py:379] starting iteration 11, 4505600 steps, 191.04577922821045
I0727 21:47:10.345732 139877795424064 train.py:394] {'eval/walltime': 62.9090461730957, 'training/sps': 43841.66562441968, 'training/walltime': 132.88383555412292, 'training/entropy_loss': Array(-0.05128697, dtype=float32), 'training/policy_loss': Array(0.00420888, dtype=float32), 'training/total_loss': Array(234720.75, dtype=float32), 'training/v_loss': Array(234720.81, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.46170187, dtype=float32), Array(0.22551495, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1106.103, dtype=float32), Array(3119.923, dtype=float32)), 'eval/episode_reward': (Array(-10791.692, dtype=float32), Array(3402.4236, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.45311326, dtype=float32), Array(0.23384827, dtype=float32)), 'eval/avg_episode_length': (Array(891.35156, dtype=float32), Array(310.0366, dtype=float32)), 'eval/epoch_eval_time': 4.069090843200684, 'eval/sps': 31456.658239489487}
I0727 21:47:10.348735 139877795424064 train.py:379] starting iteration 12, 4915200 steps, 204.4645926952362
I0727 21:47:23.824799 139877795424064 train.py:394] {'eval/walltime': 66.97024273872375, 'training/sps': 43523.13046274231, 'training/walltime': 142.29492330551147, 'training/entropy_loss': Array(-0.04901376, dtype=float32), 'training/policy_loss': Array(0.00346529, dtype=float32), 'training/total_loss': Array(227579.2, dtype=float32), 'training/v_loss': Array(227579.25, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.46116337, dtype=float32), Array(0.2146788, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1809.1653, dtype=float32), Array(3837.8538, dtype=float32)), 'eval/episode_reward': (Array(-10685.816, dtype=float32), Array(3337.0127, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4535622, dtype=float32), Array(0.22298303, dtype=float32)), 'eval/avg_episode_length': (Array(821.3594, dtype=float32), Array(381.69055, dtype=float32)), 'eval/epoch_eval_time': 4.061196565628052, 'eval/sps': 31517.804649823738}
I0727 21:47:23.827360 139877795424064 train.py:379] starting iteration 13, 5324800 steps, 217.94321751594543
I0727 21:47:37.409485 139877795424064 train.py:394] {'eval/walltime': 71.04164719581604, 'training/sps': 43085.884675814115, 'training/walltime': 151.8015170097351, 'training/entropy_loss': Array(-0.04786716, dtype=float32), 'training/policy_loss': Array(0.00362425, dtype=float32), 'training/total_loss': Array(220479.47, dtype=float32), 'training/v_loss': Array(220479.52, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.50889206, dtype=float32), Array(0.23471832, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1497.9172, dtype=float32), Array(3553.7866, dtype=float32)), 'eval/episode_reward': (Array(-11679.998, dtype=float32), Array(3779.6213, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5039341, dtype=float32), Array(0.24319716, dtype=float32)), 'eval/avg_episode_length': (Array(852.34375, dtype=float32), Array(353.66257, dtype=float32)), 'eval/epoch_eval_time': 4.071404457092285, 'eval/sps': 31438.78269746136}
I0727 21:47:37.412031 139877795424064 train.py:379] starting iteration 14, 5734400 steps, 231.52788949012756
I0727 21:47:51.066011 139877795424064 train.py:394] {'eval/walltime': 75.13502907752991, 'training/sps': 42858.864669010916, 'training/walltime': 161.35846638679504, 'training/entropy_loss': Array(-0.04589045, dtype=float32), 'training/policy_loss': Array(0.00145757, dtype=float32), 'training/total_loss': Array(216594.27, dtype=float32), 'training/v_loss': Array(216594.31, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.46597254, dtype=float32), Array(0.22279103, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1028.1411, dtype=float32), Array(3019.5588, dtype=float32)), 'eval/episode_reward': (Array(-10918.148, dtype=float32), Array(3642.9775, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.45792425, dtype=float32), Array(0.23211335, dtype=float32)), 'eval/avg_episode_length': (Array(899.08594, dtype=float32), Array(300.1439, dtype=float32)), 'eval/epoch_eval_time': 4.093381881713867, 'eval/sps': 31269.987433082446}
I0727 21:47:51.068531 139877795424064 train.py:379] starting iteration 15, 6144000 steps, 245.18438863754272
I0727 21:48:04.772606 139877795424064 train.py:394] {'eval/walltime': 79.19915747642517, 'training/sps': 42506.96188924942, 'training/walltime': 170.99453496932983, 'training/entropy_loss': Array(-0.04072604, dtype=float32), 'training/policy_loss': Array(0.00356905, dtype=float32), 'training/total_loss': Array(263970.3, dtype=float32), 'training/v_loss': Array(263970.34, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.47005707, dtype=float32), Array(0.21818507, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1106.3098, dtype=float32), Array(3120.16, dtype=float32)), 'eval/episode_reward': (Array(-10726.797, dtype=float32), Array(3589.0166, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.46111655, dtype=float32), Array(0.22944516, dtype=float32)), 'eval/avg_episode_length': (Array(891.22656, dtype=float32), Array(310.39288, dtype=float32)), 'eval/epoch_eval_time': 4.064128398895264, 'eval/sps': 31495.06793013571}
I0727 21:48:04.775169 139877795424064 train.py:379] starting iteration 16, 6553600 steps, 258.8910253047943
I0727 21:48:18.503459 139877795424064 train.py:394] {'eval/walltime': 83.26127457618713, 'training/sps': 42391.03750905011, 'training/walltime': 180.65695476531982, 'training/entropy_loss': Array(-0.03985973, dtype=float32), 'training/policy_loss': Array(0.00127644, dtype=float32), 'training/total_loss': Array(219902.58, dtype=float32), 'training/v_loss': Array(219902.6, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4546321, dtype=float32), Array(0.1902411, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(715.2804, dtype=float32), Array(2555.872, dtype=float32)), 'eval/episode_reward': (Array(-10570.122, dtype=float32), Array(3181.6067, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.446271, dtype=float32), Array(0.1996776, dtype=float32)), 'eval/avg_episode_length': (Array(930.1172, dtype=float32), Array(254.11095, dtype=float32)), 'eval/epoch_eval_time': 4.062117099761963, 'eval/sps': 31510.66226217376}
I0727 21:48:18.506137 139877795424064 train.py:379] starting iteration 17, 6963200 steps, 272.62199449539185
I0727 21:48:32.285999 139877795424064 train.py:394] {'eval/walltime': 87.3376681804657, 'training/sps': 42228.323460113905, 'training/walltime': 190.35660576820374, 'training/entropy_loss': Array(-0.03868939, dtype=float32), 'training/policy_loss': Array(0.00104374, dtype=float32), 'training/total_loss': Array(190588.56, dtype=float32), 'training/v_loss': Array(190588.6, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.43417758, dtype=float32), Array(0.18379995, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1105.4026, dtype=float32), Array(3120.3096, dtype=float32)), 'eval/episode_reward': (Array(-10418.116, dtype=float32), Array(3175.634, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.42482403, dtype=float32), Array(0.19376479, dtype=float32)), 'eval/avg_episode_length': (Array(891.28125, dtype=float32), Array(310.23712, dtype=float32)), 'eval/epoch_eval_time': 4.0763936042785645, 'eval/sps': 31400.304392993792}
I0727 21:48:32.288697 139877795424064 train.py:379] starting iteration 18, 7372800 steps, 286.40455412864685
I0727 21:48:46.109603 139877795424064 train.py:394] {'eval/walltime': 91.42951393127441, 'training/sps': 42117.45211205554, 'training/walltime': 200.0817904472351, 'training/entropy_loss': Array(-0.03695919, dtype=float32), 'training/policy_loss': Array(0.00103578, dtype=float32), 'training/total_loss': Array(170597.14, dtype=float32), 'training/v_loss': Array(170597.19, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.45936668, dtype=float32), Array(0.20761532, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1340.4015, dtype=float32), Array(3392.7903, dtype=float32)), 'eval/episode_reward': (Array(-10588.52, dtype=float32), Array(3563.8765, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.44978532, dtype=float32), Array(0.21830016, dtype=float32)), 'eval/avg_episode_length': (Array(868., dtype=float32), Array(337.29633, dtype=float32)), 'eval/epoch_eval_time': 4.091845750808716, 'eval/sps': 31281.72658382882}
I0727 21:48:46.112281 139877795424064 train.py:379] starting iteration 19, 7782400 steps, 300.22813868522644
I0727 21:48:59.928350 139877795424064 train.py:394] {'eval/walltime': 95.50786852836609, 'training/sps': 42080.19204389776, 'training/walltime': 209.81558632850647, 'training/entropy_loss': Array(-0.03488072, dtype=float32), 'training/policy_loss': Array(0.0021814, dtype=float32), 'training/total_loss': Array(163013.84, dtype=float32), 'training/v_loss': Array(163013.88, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.41380563, dtype=float32), Array(0.19234145, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1026.6948, dtype=float32), Array(3020.3086, dtype=float32)), 'eval/episode_reward': (Array(-9951.506, dtype=float32), Array(3364.8936, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.40222234, dtype=float32), Array(0.20190196, dtype=float32)), 'eval/avg_episode_length': (Array(899., dtype=float32), Array(300.39938, dtype=float32)), 'eval/epoch_eval_time': 4.078354597091675, 'eval/sps': 31385.206203324837}
I0727 21:48:59.931072 139877795424064 train.py:379] starting iteration 20, 8192000 steps, 314.0469298362732
I0727 21:49:13.823137 139877795424064 train.py:394] {'eval/walltime': 99.5771484375, 'training/sps': 41715.33395722271, 'training/walltime': 219.63451766967773, 'training/entropy_loss': Array(-0.02716628, dtype=float32), 'training/policy_loss': Array(0.00114755, dtype=float32), 'training/total_loss': Array(199381.5, dtype=float32), 'training/v_loss': Array(199381.53, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.44502032, dtype=float32), Array(0.20187728, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(949.4358, dtype=float32), Array(2914.1775, dtype=float32)), 'eval/episode_reward': (Array(-10088.564, dtype=float32), Array(3347.6826, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.43379012, dtype=float32), Array(0.21219116, dtype=float32)), 'eval/avg_episode_length': (Array(906.75, dtype=float32), Array(289.9265, dtype=float32)), 'eval/epoch_eval_time': 4.069279909133911, 'eval/sps': 31455.196707577434}
I0727 21:49:13.825823 139877795424064 train.py:379] starting iteration 21, 8601600 steps, 327.94168043136597
I0727 21:49:27.725091 139877795424064 train.py:394] {'eval/walltime': 103.64453458786011, 'training/sps': 41675.8091360522, 'training/walltime': 229.46276116371155, 'training/entropy_loss': Array(-0.02613134, dtype=float32), 'training/policy_loss': Array(0.00110958, dtype=float32), 'training/total_loss': Array(180345.5, dtype=float32), 'training/v_loss': Array(180345.56, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.45824215, dtype=float32), Array(0.19252503, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(949.7607, dtype=float32), Array(2913.93, dtype=float32)), 'eval/episode_reward': (Array(-10429.468, dtype=float32), Array(3266.3833, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.44798326, dtype=float32), Array(0.20380661, dtype=float32)), 'eval/avg_episode_length': (Array(906.77344, dtype=float32), Array(289.85336, dtype=float32)), 'eval/epoch_eval_time': 4.067386150360107, 'eval/sps': 31469.842121743834}
I0727 21:49:27.727804 139877795424064 train.py:379] starting iteration 22, 9011200 steps, 341.8436613082886
I0727 21:49:41.667201 139877795424064 train.py:394] {'eval/walltime': 107.71544981002808, 'training/sps': 41522.823150025106, 'training/walltime': 239.3272156715393, 'training/entropy_loss': Array(-0.02955374, dtype=float32), 'training/policy_loss': Array(-0.00305118, dtype=float32), 'training/total_loss': Array(163866.36, dtype=float32), 'training/v_loss': Array(163866.4, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4335061, dtype=float32), Array(0.20167473, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1027.2284, dtype=float32), Array(3019.8635, dtype=float32)), 'eval/episode_reward': (Array(-10167.544, dtype=float32), Array(3369.2654, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4247375, dtype=float32), Array(0.2107216, dtype=float32)), 'eval/avg_episode_length': (Array(899.03906, dtype=float32), Array(300.28336, dtype=float32)), 'eval/epoch_eval_time': 4.070915222167969, 'eval/sps': 31442.56095115474}
I0727 21:49:41.672869 139877795424064 train.py:379] starting iteration 23, 9420800 steps, 355.78871035575867
I0727 21:49:55.614039 139877795424064 train.py:394] {'eval/walltime': 111.79239463806152, 'training/sps': 41541.437945819736, 'training/walltime': 249.18724989891052, 'training/entropy_loss': Array(-0.02318687, dtype=float32), 'training/policy_loss': Array(-0.00066061, dtype=float32), 'training/total_loss': Array(152067.06, dtype=float32), 'training/v_loss': Array(152067.1, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.38353205, dtype=float32), Array(0.16752638, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1494.6875, dtype=float32), Array(3554.9678, dtype=float32)), 'eval/episode_reward': (Array(-9745.754, dtype=float32), Array(3223.1194, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.36909795, dtype=float32), Array(0.17975642, dtype=float32)), 'eval/avg_episode_length': (Array(852.4531, dtype=float32), Array(353.40033, dtype=float32)), 'eval/epoch_eval_time': 4.076944828033447, 'eval/sps': 31396.058911531065}
I0727 21:49:55.616903 139877795424064 train.py:379] starting iteration 24, 9830400 steps, 369.73276019096375
I0727 21:50:09.611840 139877795424064 train.py:394] {'eval/walltime': 115.87144327163696, 'training/sps': 41332.47080775787, 'training/walltime': 259.09713411331177, 'training/entropy_loss': Array(-0.02044607, dtype=float32), 'training/policy_loss': Array(-0.00362551, dtype=float32), 'training/total_loss': Array(139986.12, dtype=float32), 'training/v_loss': Array(139986.14, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.39603627, dtype=float32), Array(0.17386417, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1182.5085, dtype=float32), Array(3216.1218, dtype=float32)), 'eval/episode_reward': (Array(-9733.67, dtype=float32), Array(3450.02, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3847171, dtype=float32), Array(0.18319348, dtype=float32)), 'eval/avg_episode_length': (Array(883.6094, dtype=float32), Array(319.45697, dtype=float32)), 'eval/epoch_eval_time': 4.0790486335754395, 'eval/sps': 31379.866115448393}
I0727 21:50:09.614645 139877795424064 train.py:379] starting iteration 25, 10240000 steps, 383.73050260543823
I0727 21:50:23.628750 139877795424064 train.py:394] {'eval/walltime': 119.97926473617554, 'training/sps': 41363.05344481861, 'training/walltime': 268.99969124794006, 'training/entropy_loss': Array(-0.02150954, dtype=float32), 'training/policy_loss': Array(-0.00196657, dtype=float32), 'training/total_loss': Array(155825.05, dtype=float32), 'training/v_loss': Array(155825.06, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.37392545, dtype=float32), Array(0.13699596, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1025.7037, dtype=float32), Array(3020.631, dtype=float32)), 'eval/episode_reward': (Array(-9341.418, dtype=float32), Array(2787.479, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.36105806, dtype=float32), Array(0.1471137, dtype=float32)), 'eval/avg_episode_length': (Array(899.03906, dtype=float32), Array(300.28342, dtype=float32)), 'eval/epoch_eval_time': 4.107821464538574, 'eval/sps': 31160.068933126833}
I0727 21:50:23.668498 139877795424064 train.py:379] starting iteration 26, 10649600 steps, 397.78435587882996
I0727 21:50:37.679064 139877795424064 train.py:394] {'eval/walltime': 124.05752873420715, 'training/sps': 41256.12451952038, 'training/walltime': 278.92791414260864, 'training/entropy_loss': Array(-0.01906523, dtype=float32), 'training/policy_loss': Array(-5.9311e-05, dtype=float32), 'training/total_loss': Array(136097.17, dtype=float32), 'training/v_loss': Array(136097.19, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.35839987, dtype=float32), Array(0.13644128, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(790.9131, dtype=float32), Array(2683.698, dtype=float32)), 'eval/episode_reward': (Array(-9389.274, dtype=float32), Array(2654.0593, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.34444293, dtype=float32), Array(0.1463521, dtype=float32)), 'eval/avg_episode_length': (Array(922.33594, dtype=float32), Array(266.78546, dtype=float32)), 'eval/epoch_eval_time': 4.078263998031616, 'eval/sps': 31385.903428953963}
I0727 21:50:37.682020 139877795424064 train.py:379] starting iteration 27, 11059200 steps, 411.7978763580322
I0727 21:50:51.691318 139877795424064 train.py:394] {'eval/walltime': 128.16336131095886, 'training/sps': 41375.31232878817, 'training/walltime': 288.8275372982025, 'training/entropy_loss': Array(-0.00958833, dtype=float32), 'training/policy_loss': Array(-0.00256426, dtype=float32), 'training/total_loss': Array(125277.22, dtype=float32), 'training/v_loss': Array(125277.23, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3455481, dtype=float32), Array(0.1180193, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1024.9615, dtype=float32), Array(3020.9246, dtype=float32)), 'eval/episode_reward': (Array(-8981.579, dtype=float32), Array(2527.5884, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3314216, dtype=float32), Array(0.12612544, dtype=float32)), 'eval/avg_episode_length': (Array(898.96875, dtype=float32), Array(300.49234, dtype=float32)), 'eval/epoch_eval_time': 4.105832576751709, 'eval/sps': 31175.163041174466}
I0727 21:50:51.694095 139877795424064 train.py:379] starting iteration 28, 11468800 steps, 425.8099524974823
I0727 21:51:05.688654 139877795424064 train.py:394] {'eval/walltime': 132.24231338500977, 'training/sps': 41325.36502385945, 'training/walltime': 298.7391254901886, 'training/entropy_loss': Array(-0.00017408, dtype=float32), 'training/policy_loss': Array(0.00309846, dtype=float32), 'training/total_loss': Array(113108.15, dtype=float32), 'training/v_loss': Array(113108.14, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.34833786, dtype=float32), Array(0.12634768, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1259.4053, dtype=float32), Array(3307.0938, dtype=float32)), 'eval/episode_reward': (Array(-9144.504, dtype=float32), Array(2646.1501, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.33353904, dtype=float32), Array(0.13658564, dtype=float32)), 'eval/avg_episode_length': (Array(875.78906, dtype=float32), Array(328.6321, dtype=float32)), 'eval/epoch_eval_time': 4.078952074050903, 'eval/sps': 31380.60895941839}
I0727 21:51:05.691466 139877795424064 train.py:379] starting iteration 29, 11878400 steps, 439.8073227405548
I0727 21:51:19.719645 139877795424064 train.py:394] {'eval/walltime': 136.3334014415741, 'training/sps': 41235.08414241401, 'training/walltime': 308.6724143028259, 'training/entropy_loss': Array(0.0095834, dtype=float32), 'training/policy_loss': Array(-0.00071784, dtype=float32), 'training/total_loss': Array(103367.02, dtype=float32), 'training/v_loss': Array(103367.016, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3432743, dtype=float32), Array(0.10945049, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1337.3395, dtype=float32), Array(3394.184, dtype=float32)), 'eval/episode_reward': (Array(-9013.248, dtype=float32), Array(2653.9924, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.32746392, dtype=float32), Array(0.12003054, dtype=float32)), 'eval/avg_episode_length': (Array(867.9531, dtype=float32), Array(337.41608, dtype=float32)), 'eval/epoch_eval_time': 4.091088056564331, 'eval/sps': 31287.520148733627}
I0727 21:51:19.722441 139877795424064 train.py:379] starting iteration 30, 12288000 steps, 453.8382980823517
I0727 21:51:33.745145 139877795424064 train.py:394] {'eval/walltime': 140.409686088562, 'training/sps': 41197.045588211244, 'training/walltime': 318.6148748397827, 'training/entropy_loss': Array(0.02429148, dtype=float32), 'training/policy_loss': Array(0.00180239, dtype=float32), 'training/total_loss': Array(130231.09, dtype=float32), 'training/v_loss': Array(130231.06, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.32765642, dtype=float32), Array(0.09922935, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1024.4601, dtype=float32), Array(3020.7065, dtype=float32)), 'eval/episode_reward': (Array(-8870.989, dtype=float32), Array(2579.399, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30971277, dtype=float32), Array(0.10898566, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.30664, dtype=float32)), 'eval/epoch_eval_time': 4.076284646987915, 'eval/sps': 31401.143709280193}
I0727 21:51:33.747953 139877795424064 train.py:379] starting iteration 31, 12697600 steps, 467.86381006240845
I0727 21:51:47.819384 139877795424064 train.py:394] {'eval/walltime': 144.51714825630188, 'training/sps': 41132.0339821183, 'training/walltime': 328.57305002212524, 'training/entropy_loss': Array(0.0455173, dtype=float32), 'training/policy_loss': Array(0.00264777, dtype=float32), 'training/total_loss': Array(107984.92, dtype=float32), 'training/v_loss': Array(107984.875, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.32956147, dtype=float32), Array(0.08450656, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1180.639, dtype=float32), Array(3217.08, dtype=float32)), 'eval/episode_reward': (Array(-8727.793, dtype=float32), Array(2167.591, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.31207988, dtype=float32), Array(0.09349845, dtype=float32)), 'eval/avg_episode_length': (Array(883.39844, dtype=float32), Array(320.03574, dtype=float32)), 'eval/epoch_eval_time': 4.107462167739868, 'eval/sps': 31162.794633950827}
I0727 21:51:47.822195 139877795424064 train.py:379] starting iteration 32, 13107200 steps, 481.9380524158478
I0727 21:52:01.814827 139877795424064 train.py:394] {'eval/walltime': 148.59327173233032, 'training/sps': 41321.245046974895, 'training/walltime': 338.4856264591217, 'training/entropy_loss': Array(0.06529297, dtype=float32), 'training/policy_loss': Array(0.00256646, dtype=float32), 'training/total_loss': Array(93811., dtype=float32), 'training/v_loss': Array(93810.93, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.32154572, dtype=float32), Array(0.08317855, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(711.75165, dtype=float32), Array(2557.2263, dtype=float32)), 'eval/episode_reward': (Array(-8582.8, dtype=float32), Array(2194.6272, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30123746, dtype=float32), Array(0.09199362, dtype=float32)), 'eval/avg_episode_length': (Array(930.1719, dtype=float32), Array(253.9122, dtype=float32)), 'eval/epoch_eval_time': 4.076123476028442, 'eval/sps': 31402.385318492947}
I0727 21:52:01.817537 139877795424064 train.py:379] starting iteration 33, 13516800 steps, 495.93339443206787
I0727 21:52:15.826698 139877795424064 train.py:394] {'eval/walltime': 152.68799781799316, 'training/sps': 41329.25117827152, 'training/walltime': 348.3962826728821, 'training/entropy_loss': Array(0.07758449, dtype=float32), 'training/policy_loss': Array(0.00681307, dtype=float32), 'training/total_loss': Array(78860.66, dtype=float32), 'training/v_loss': Array(78860.57, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3260951, dtype=float32), Array(0.08803292, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1727.4637, dtype=float32), Array(3772.805, dtype=float32)), 'eval/episode_reward': (Array(-8907.774, dtype=float32), Array(2325.6501, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30601025, dtype=float32), Array(0.09781583, dtype=float32)), 'eval/avg_episode_length': (Array(829.21094, dtype=float32), Array(374.88892, dtype=float32)), 'eval/epoch_eval_time': 4.094726085662842, 'eval/sps': 31259.72221882572}
I0727 21:52:15.829450 139877795424064 train.py:379] starting iteration 34, 13926400 steps, 509.94530725479126
I0727 21:52:29.883345 139877795424064 train.py:394] {'eval/walltime': 156.77041745185852, 'training/sps': 41092.66659184778, 'training/walltime': 358.3639979362488, 'training/entropy_loss': Array(0.10535876, dtype=float32), 'training/policy_loss': Array(0.00210368, dtype=float32), 'training/total_loss': Array(64304.457, dtype=float32), 'training/v_loss': Array(64304.36, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30541295, dtype=float32), Array(0.10730156, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1258.0945, dtype=float32), Array(3307.4458, dtype=float32)), 'eval/episode_reward': (Array(-8251.4795, dtype=float32), Array(2891.1658, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28288266, dtype=float32), Array(0.11799431, dtype=float32)), 'eval/avg_episode_length': (Array(875.78906, dtype=float32), Array(328.63214, dtype=float32)), 'eval/epoch_eval_time': 4.0824196338653564, 'eval/sps': 31353.954634694375}
I0727 21:52:29.886047 139877795424064 train.py:379] starting iteration 35, 14336000 steps, 524.0019042491913
I0727 21:52:43.944760 139877795424064 train.py:394] {'eval/walltime': 160.84587216377258, 'training/sps': 41045.03947091156, 'training/walltime': 368.34327936172485, 'training/entropy_loss': Array(0.12418111, dtype=float32), 'training/policy_loss': Array(0.00858028, dtype=float32), 'training/total_loss': Array(105095.39, dtype=float32), 'training/v_loss': Array(105095.26, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30862394, dtype=float32), Array(0.08352058, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1023.9082, dtype=float32), Array(3021.2146, dtype=float32)), 'eval/episode_reward': (Array(-8318.24, dtype=float32), Array(2266.9404, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28875622, dtype=float32), Array(0.09094443, dtype=float32)), 'eval/avg_episode_length': (Array(898.9844, dtype=float32), Array(300.446, dtype=float32)), 'eval/epoch_eval_time': 4.0754547119140625, 'eval/sps': 31407.538311199638}
I0727 21:52:43.947297 139877795424064 train.py:379] starting iteration 36, 14745600 steps, 538.0631539821625
I0727 21:52:58.034699 139877795424064 train.py:394] {'eval/walltime': 164.94646286964417, 'training/sps': 41029.56717250442, 'training/walltime': 378.32632398605347, 'training/entropy_loss': Array(0.15669385, dtype=float32), 'training/policy_loss': Array(0.01266597, dtype=float32), 'training/total_loss': Array(61530.312, dtype=float32), 'training/v_loss': Array(61530.15, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.32507503, dtype=float32), Array(0.09334678, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1258.6172, dtype=float32), Array(3307.8, dtype=float32)), 'eval/episode_reward': (Array(-8596.34, dtype=float32), Array(2526.9695, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30269933, dtype=float32), Array(0.1058481, dtype=float32)), 'eval/avg_episode_length': (Array(875.6875, dtype=float32), Array(328.90033, dtype=float32)), 'eval/epoch_eval_time': 4.100590705871582, 'eval/sps': 31215.014904247448}
I0727 21:52:58.037229 139877795424064 train.py:379] starting iteration 37, 15155200 steps, 552.1530859470367
I0727 21:53:12.049210 139877795424064 train.py:394] {'eval/walltime': 169.0279393196106, 'training/sps': 41262.16689943462, 'training/walltime': 388.2530930042267, 'training/entropy_loss': Array(0.1957526, dtype=float32), 'training/policy_loss': Array(0.02698256, dtype=float32), 'training/total_loss': Array(44950.65, dtype=float32), 'training/v_loss': Array(44950.426, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.32990253, dtype=float32), Array(0.0885941, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1493.1656, dtype=float32), Array(3555.8118, dtype=float32)), 'eval/episode_reward': (Array(-8805.461, dtype=float32), Array(2271.22, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.31075668, dtype=float32), Array(0.09714352, dtype=float32)), 'eval/avg_episode_length': (Array(852.40625, dtype=float32), Array(353.51285, dtype=float32)), 'eval/epoch_eval_time': 4.081476449966431, 'eval/sps': 31361.200185548732}
I0727 21:53:12.052213 139877795424064 train.py:379] starting iteration 38, 15564800 steps, 566.1680705547333
I0727 21:53:26.137768 139877795424064 train.py:394] {'eval/walltime': 173.10534501075745, 'training/sps': 40942.23036857534, 'training/walltime': 398.25743317604065, 'training/entropy_loss': Array(0.21509583, dtype=float32), 'training/policy_loss': Array(0.04571026, dtype=float32), 'training/total_loss': Array(30272.652, dtype=float32), 'training/v_loss': Array(30272.395, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.32974058, dtype=float32), Array(0.09006429, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1258.8806, dtype=float32), Array(3307.5474, dtype=float32)), 'eval/episode_reward': (Array(-8898.192, dtype=float32), Array(2219.3645, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30944222, dtype=float32), Array(0.0984588, dtype=float32)), 'eval/avg_episode_length': (Array(875.71094, dtype=float32), Array(328.83908, dtype=float32)), 'eval/epoch_eval_time': 4.077405691146851, 'eval/sps': 31392.510261591724}
I0727 21:53:26.140358 139877795424064 train.py:379] starting iteration 39, 15974400 steps, 580.2562155723572
I0727 21:53:40.229689 139877795424064 train.py:394] {'eval/walltime': 177.2163860797882, 'training/sps': 41064.55919895677, 'training/walltime': 408.2319710254669, 'training/entropy_loss': Array(0.25267798, dtype=float32), 'training/policy_loss': Array(0.06305654, dtype=float32), 'training/total_loss': Array(18085.027, dtype=float32), 'training/v_loss': Array(18084.71, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.33393222, dtype=float32), Array(0.0914274, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(2040.1089, dtype=float32), Array(4023.7554, dtype=float32)), 'eval/episode_reward': (Array(-8894.854, dtype=float32), Array(2379.165, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.31421176, dtype=float32), Array(0.10261083, dtype=float32)), 'eval/avg_episode_length': (Array(798.08594, dtype=float32), Array(399.92697, dtype=float32)), 'eval/epoch_eval_time': 4.111041069030762, 'eval/sps': 31135.66560165206}
I0727 21:53:40.232244 139877795424064 train.py:379] starting iteration 40, 16384000 steps, 594.3481018543243
I0727 21:53:54.300117 139877795424064 train.py:394] {'eval/walltime': 181.30279850959778, 'training/sps': 41051.76371476272, 'training/walltime': 418.2096178531647, 'training/entropy_loss': Array(0.15037066, dtype=float32), 'training/policy_loss': Array(0.03190687, dtype=float32), 'training/total_loss': Array(71895.36, dtype=float32), 'training/v_loss': Array(71895.19, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3288356, dtype=float32), Array(0.08228435, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(868.1394, dtype=float32), Array(2803.103, dtype=float32)), 'eval/episode_reward': (Array(-8758.008, dtype=float32), Array(2172.7532, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30945992, dtype=float32), Array(0.09105669, dtype=float32)), 'eval/avg_episode_length': (Array(914.58594, dtype=float32), Array(278.56537, dtype=float32)), 'eval/epoch_eval_time': 4.08641242980957, 'eval/sps': 31323.318974429825}
I0727 21:53:54.302696 139877795424064 train.py:379] starting iteration 41, 16793600 steps, 608.4185543060303
I0727 21:54:08.386098 139877795424064 train.py:394] {'eval/walltime': 185.38227367401123, 'training/sps': 40959.184586545714, 'training/walltime': 428.2098169326782, 'training/entropy_loss': Array(0.15693519, dtype=float32), 'training/policy_loss': Array(0.01229876, dtype=float32), 'training/total_loss': Array(4164.5693, dtype=float32), 'training/v_loss': Array(4164.4, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.33962363, dtype=float32), Array(0.09048561, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1180.8813, dtype=float32), Array(3216.7673, dtype=float32)), 'eval/episode_reward': (Array(-9019.912, dtype=float32), Array(2366.562, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.32111096, dtype=float32), Array(0.09814194, dtype=float32)), 'eval/avg_episode_length': (Array(883.53125, dtype=float32), Array(319.6715, dtype=float32)), 'eval/epoch_eval_time': 4.079475164413452, 'eval/sps': 31376.585183453095}
I0727 21:54:08.388655 139877795424064 train.py:379] starting iteration 42, 17203200 steps, 622.5045123100281
I0727 21:54:22.464580 139877795424064 train.py:394] {'eval/walltime': 189.4540238380432, 'training/sps': 40958.56840941219, 'training/walltime': 438.2101664543152, 'training/entropy_loss': Array(0.18152875, dtype=float32), 'training/policy_loss': Array(0.0167674, dtype=float32), 'training/total_loss': Array(1901.3945, dtype=float32), 'training/v_loss': Array(1901.196, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3232189, dtype=float32), Array(0.08823388, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1727.3875, dtype=float32), Array(3773.2793, dtype=float32)), 'eval/episode_reward': (Array(-8657.998, dtype=float32), Array(2273.471, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30231264, dtype=float32), Array(0.09674542, dtype=float32)), 'eval/avg_episode_length': (Array(829.0625, dtype=float32), Array(375.2147, dtype=float32)), 'eval/epoch_eval_time': 4.071750164031982, 'eval/sps': 31436.113426284028}
I0727 21:54:22.467338 139877795424064 train.py:379] starting iteration 43, 17612800 steps, 636.5831959247589
I0727 21:54:36.584661 139877795424064 train.py:394] {'eval/walltime': 193.5650599002838, 'training/sps': 40949.77599014834, 'training/walltime': 448.21266317367554, 'training/entropy_loss': Array(0.20998307, dtype=float32), 'training/policy_loss': Array(0.02781581, dtype=float32), 'training/total_loss': Array(1339.0293, dtype=float32), 'training/v_loss': Array(1338.7915, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3247857, dtype=float32), Array(0.08700652, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1493.0156, dtype=float32), Array(3555.6494, dtype=float32)), 'eval/episode_reward': (Array(-8731.728, dtype=float32), Array(2299.4246, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30400917, dtype=float32), Array(0.09532414, dtype=float32)), 'eval/avg_episode_length': (Array(852.40625, dtype=float32), Array(353.51285, dtype=float32)), 'eval/epoch_eval_time': 4.111036062240601, 'eval/sps': 31135.703521471256}
I0727 21:54:36.587592 139877795424064 train.py:379] starting iteration 44, 18022400 steps, 650.7034492492676
I0727 21:54:50.686204 139877795424064 train.py:394] {'eval/walltime': 197.66658115386963, 'training/sps': 40987.623108444364, 'training/walltime': 458.2059237957001, 'training/entropy_loss': Array(0.23636907, dtype=float32), 'training/policy_loss': Array(0.04349251, dtype=float32), 'training/total_loss': Array(1294.4961, dtype=float32), 'training/v_loss': Array(1294.2163, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30763465, dtype=float32), Array(0.07992695, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1414.4434, dtype=float32), Array(3476.686, dtype=float32)), 'eval/episode_reward': (Array(-8328.986, dtype=float32), Array(2275.573, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28525722, dtype=float32), Array(0.09034119, dtype=float32)), 'eval/avg_episode_length': (Array(860.21875, dtype=float32), Array(345.54938, dtype=float32)), 'eval/epoch_eval_time': 4.101521253585815, 'eval/sps': 31207.93288297461}
I0727 21:54:50.689246 139877795424064 train.py:379] starting iteration 45, 18432000 steps, 664.8051035404205
I0727 21:55:04.821220 139877795424064 train.py:394] {'eval/walltime': 201.78323578834534, 'training/sps': 40912.16140642127, 'training/walltime': 468.21761679649353, 'training/entropy_loss': Array(0.2554871, dtype=float32), 'training/policy_loss': Array(0.05345443, dtype=float32), 'training/total_loss': Array(52625.234, dtype=float32), 'training/v_loss': Array(52624.926, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31826875, dtype=float32), Array(0.08739772, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1727.2686, dtype=float32), Array(3773.3704, dtype=float32)), 'eval/episode_reward': (Array(-8533.732, dtype=float32), Array(2272.547, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29532185, dtype=float32), Array(0.09760211, dtype=float32)), 'eval/avg_episode_length': (Array(829.125, dtype=float32), Array(375.0772, dtype=float32)), 'eval/epoch_eval_time': 4.116654634475708, 'eval/sps': 31093.20828811813}
I0727 21:55:04.823901 139877795424064 train.py:379] starting iteration 46, 18841600 steps, 678.9397587776184
I0727 21:55:18.907594 139877795424064 train.py:394] {'eval/walltime': 205.87461876869202, 'training/sps': 41007.09510257461, 'training/walltime': 478.2061321735382, 'training/entropy_loss': Array(0.26875353, dtype=float32), 'training/policy_loss': Array(0.07345698, dtype=float32), 'training/total_loss': Array(1396.2297, dtype=float32), 'training/v_loss': Array(1395.8875, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31648713, dtype=float32), Array(0.0852202, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1258.3953, dtype=float32), Array(3307.5205, dtype=float32)), 'eval/episode_reward': (Array(-8494.151, dtype=float32), Array(2338.685, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29554582, dtype=float32), Array(0.0945695, dtype=float32)), 'eval/avg_episode_length': (Array(875.7578, dtype=float32), Array(328.71454, dtype=float32)), 'eval/epoch_eval_time': 4.09138298034668, 'eval/sps': 31285.26481506604}
I0727 21:55:18.910364 139877795424064 train.py:379] starting iteration 47, 19251200 steps, 693.0262215137482
I0727 21:55:33.066802 139877795424064 train.py:394] {'eval/walltime': 209.9807412624359, 'training/sps': 40770.72633863085, 'training/walltime': 488.25255608558655, 'training/entropy_loss': Array(0.2809975, dtype=float32), 'training/policy_loss': Array(0.07319649, dtype=float32), 'training/total_loss': Array(1400.2792, dtype=float32), 'training/v_loss': Array(1399.925, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3235938, dtype=float32), Array(0.08974165, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1102.3295, dtype=float32), Array(3121.7761, dtype=float32)), 'eval/episode_reward': (Array(-8531.004, dtype=float32), Array(2313.9475, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30186215, dtype=float32), Array(0.09917967, dtype=float32)), 'eval/avg_episode_length': (Array(891.375, dtype=float32), Array(309.96997, dtype=float32)), 'eval/epoch_eval_time': 4.1061224937438965, 'eval/sps': 31172.961886797406}
I0727 21:55:33.069607 139877795424064 train.py:379] starting iteration 48, 19660800 steps, 707.1854646205902
I0727 21:55:47.182431 139877795424064 train.py:394] {'eval/walltime': 214.07984519004822, 'training/sps': 40927.75684667862, 'training/walltime': 498.2604341506958, 'training/entropy_loss': Array(0.28561813, dtype=float32), 'training/policy_loss': Array(0.07426468, dtype=float32), 'training/total_loss': Array(1260.1787, dtype=float32), 'training/v_loss': Array(1259.8188, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31273395, dtype=float32), Array(0.08034635, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1102.05, dtype=float32), Array(3121.6094, dtype=float32)), 'eval/episode_reward': (Array(-8333.899, dtype=float32), Array(2181.1235, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29046944, dtype=float32), Array(0.09057988, dtype=float32)), 'eval/avg_episode_length': (Array(891.28906, dtype=float32), Array(310.21475, dtype=float32)), 'eval/epoch_eval_time': 4.099103927612305, 'eval/sps': 31226.336843466903}
I0727 21:55:47.185235 139877795424064 train.py:379] starting iteration 49, 20070400 steps, 721.3010928630829
I0727 21:56:01.304969 139877795424064 train.py:394] {'eval/walltime': 218.1792016029358, 'training/sps': 40894.22298766042, 'training/walltime': 508.2765188217163, 'training/entropy_loss': Array(0.28862345, dtype=float32), 'training/policy_loss': Array(0.07504734, dtype=float32), 'training/total_loss': Array(1242.5686, dtype=float32), 'training/v_loss': Array(1242.2048, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31358534, dtype=float32), Array(0.09123487, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1258.3578, dtype=float32), Array(3307.4292, dtype=float32)), 'eval/episode_reward': (Array(-8488.575, dtype=float32), Array(2513.7979, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29267347, dtype=float32), Array(0.10147358, dtype=float32)), 'eval/avg_episode_length': (Array(875.8203, dtype=float32), Array(328.5492, dtype=float32)), 'eval/epoch_eval_time': 4.099356412887573, 'eval/sps': 31224.41356833309}
I0727 21:56:01.308050 139877795424064 train.py:379] starting iteration 50, 20480000 steps, 735.4239072799683
I0727 21:56:15.459634 139877795424064 train.py:394] {'eval/walltime': 222.28591775894165, 'training/sps': 40794.35424633756, 'training/walltime': 518.3171238899231, 'training/entropy_loss': Array(0.2869178, dtype=float32), 'training/policy_loss': Array(0.0582591, dtype=float32), 'training/total_loss': Array(46472.01, dtype=float32), 'training/v_loss': Array(46471.668, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.32116705, dtype=float32), Array(0.08398873, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1961.6083, dtype=float32), Array(3964.9, dtype=float32)), 'eval/episode_reward': (Array(-8585.004, dtype=float32), Array(2292.9597, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30035532, dtype=float32), Array(0.09313206, dtype=float32)), 'eval/avg_episode_length': (Array(805.9453, dtype=float32), Array(393.88907, dtype=float32)), 'eval/epoch_eval_time': 4.106716156005859, 'eval/sps': 31168.45555853833}
I0727 21:56:15.510113 139877795424064 train.py:379] starting iteration 51, 20889600 steps, 749.6259622573853
I0727 21:56:29.611704 139877795424064 train.py:394] {'eval/walltime': 226.3715569972992, 'training/sps': 40910.15253411749, 'training/walltime': 528.3293085098267, 'training/entropy_loss': Array(0.2856456, dtype=float32), 'training/policy_loss': Array(0.06452391, dtype=float32), 'training/total_loss': Array(1277.9988, dtype=float32), 'training/v_loss': Array(1277.6484, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3180862, dtype=float32), Array(0.08465827, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(2508.4878, dtype=float32), Array(4330.277, dtype=float32)), 'eval/episode_reward': (Array(-8815.077, dtype=float32), Array(2169.2974, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29620925, dtype=float32), Array(0.09415884, dtype=float32)), 'eval/avg_episode_length': (Array(751.4453, dtype=float32), Array(430.51016, dtype=float32)), 'eval/epoch_eval_time': 4.085639238357544, 'eval/sps': 31329.24679161269}
I0727 21:56:29.616161 139877795424064 train.py:379] starting iteration 52, 21299200 steps, 763.7320027351379
I0727 21:56:43.733168 139877795424064 train.py:394] {'eval/walltime': 230.47325611114502, 'training/sps': 40914.41212329459, 'training/walltime': 538.3404507637024, 'training/entropy_loss': Array(0.28677207, dtype=float32), 'training/policy_loss': Array(0.05687491, dtype=float32), 'training/total_loss': Array(1331.6512, dtype=float32), 'training/v_loss': Array(1331.3076, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30951425, dtype=float32), Array(0.08669193, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1414.5084, dtype=float32), Array(3476.8342, dtype=float32)), 'eval/episode_reward': (Array(-8341.172, dtype=float32), Array(2290.7893, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2864411, dtype=float32), Array(0.09566655, dtype=float32)), 'eval/avg_episode_length': (Array(860.25, dtype=float32), Array(345.47195, dtype=float32)), 'eval/epoch_eval_time': 4.101699113845825, 'eval/sps': 31206.579626457522}
I0727 21:56:43.735718 139877795424064 train.py:379] starting iteration 53, 21708800 steps, 777.851574420929
I0727 21:56:57.874575 139877795424064 train.py:394] {'eval/walltime': 234.54329109191895, 'training/sps': 40695.4107094792, 'training/walltime': 548.405467748642, 'training/entropy_loss': Array(0.28821945, dtype=float32), 'training/policy_loss': Array(0.06651691, dtype=float32), 'training/total_loss': Array(1199.8937, dtype=float32), 'training/v_loss': Array(1199.539, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31903517, dtype=float32), Array(0.10467676, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1414.7474, dtype=float32), Array(3476.452, dtype=float32)), 'eval/episode_reward': (Array(-8667.482, dtype=float32), Array(2750.697, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29707193, dtype=float32), Array(0.11647996, dtype=float32)), 'eval/avg_episode_length': (Array(860.25, dtype=float32), Array(345.47202, dtype=float32)), 'eval/epoch_eval_time': 4.070034980773926, 'eval/sps': 31449.3611491419}
I0727 21:56:57.877269 139877795424064 train.py:379] starting iteration 54, 22118400 steps, 791.9931268692017
I0727 21:57:11.985515 139877795424064 train.py:394] {'eval/walltime': 238.65472221374512, 'training/sps': 40988.424985827514, 'training/walltime': 558.3985328674316, 'training/entropy_loss': Array(0.28423482, dtype=float32), 'training/policy_loss': Array(0.05180058, dtype=float32), 'training/total_loss': Array(1155.6958, dtype=float32), 'training/v_loss': Array(1155.3599, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.32451335, dtype=float32), Array(0.09072065, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1492.9443, dtype=float32), Array(3555.6797, dtype=float32)), 'eval/episode_reward': (Array(-8664.189, dtype=float32), Array(2441.1643, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30135012, dtype=float32), Array(0.10311811, dtype=float32)), 'eval/avg_episode_length': (Array(852.39844, dtype=float32), Array(353.53165, dtype=float32)), 'eval/epoch_eval_time': 4.111431121826172, 'eval/sps': 31132.711751023162}
I0727 21:57:11.988415 139877795424064 train.py:379] starting iteration 55, 22528000 steps, 806.1042726039886
I0727 21:57:26.050481 139877795424064 train.py:394] {'eval/walltime': 242.74082350730896, 'training/sps': 41074.653042656784, 'training/walltime': 568.3706195354462, 'training/entropy_loss': Array(0.27879816, dtype=float32), 'training/policy_loss': Array(0.03953887, dtype=float32), 'training/total_loss': Array(44989.137, dtype=float32), 'training/v_loss': Array(44988.82, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.33022127, dtype=float32), Array(0.0986167, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1258.7708, dtype=float32), Array(3307.4504, dtype=float32)), 'eval/episode_reward': (Array(-8843.672, dtype=float32), Array(2601.7886, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.31054312, dtype=float32), Array(0.10696173, dtype=float32)), 'eval/avg_episode_length': (Array(875.7422, dtype=float32), Array(328.75586, dtype=float32)), 'eval/epoch_eval_time': 4.086101293563843, 'eval/sps': 31325.704089034003}
I0727 21:57:26.052990 139877795424064 train.py:379] starting iteration 56, 22937600 steps, 820.168847322464
I0727 21:57:40.176096 139877795424064 train.py:394] {'eval/walltime': 246.82614946365356, 'training/sps': 40821.43325000913, 'training/walltime': 578.4045641422272, 'training/entropy_loss': Array(0.27425426, dtype=float32), 'training/policy_loss': Array(0.04305313, dtype=float32), 'training/total_loss': Array(1230.3264, dtype=float32), 'training/v_loss': Array(1230.009, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.32162905, dtype=float32), Array(0.0846119, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1336.7085, dtype=float32), Array(3394.0654, dtype=float32)), 'eval/episode_reward': (Array(-8654.443, dtype=float32), Array(2168.6992, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29989007, dtype=float32), Array(0.09429961, dtype=float32)), 'eval/avg_episode_length': (Array(867.9219, dtype=float32), Array(337.49628, dtype=float32)), 'eval/epoch_eval_time': 4.0853259563446045, 'eval/sps': 31331.64926563891}
I0727 21:57:40.178609 139877795424064 train.py:379] starting iteration 57, 23347200 steps, 834.2944667339325
I0727 21:57:54.276913 139877795424064 train.py:394] {'eval/walltime': 250.89759135246277, 'training/sps': 40864.963207932604, 'training/walltime': 588.427820444107, 'training/entropy_loss': Array(0.26836765, dtype=float32), 'training/policy_loss': Array(0.03075405, dtype=float32), 'training/total_loss': Array(1403.3552, dtype=float32), 'training/v_loss': Array(1403.0562, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.32962593, dtype=float32), Array(0.08982368, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1493.161, dtype=float32), Array(3555.7048, dtype=float32)), 'eval/episode_reward': (Array(-8838.49, dtype=float32), Array(2301.75, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3087485, dtype=float32), Array(0.10096649, dtype=float32)), 'eval/avg_episode_length': (Array(852.3906, dtype=float32), Array(353.55045, dtype=float32)), 'eval/epoch_eval_time': 4.071441888809204, 'eval/sps': 31438.493657940144}
I0727 21:57:54.279426 139877795424064 train.py:379] starting iteration 58, 23756800 steps, 848.3952827453613
I0727 21:58:08.412678 139877795424064 train.py:394] {'eval/walltime': 254.98291420936584, 'training/sps': 40778.72864675518, 'training/walltime': 598.4722728729248, 'training/entropy_loss': Array(0.26652664, dtype=float32), 'training/policy_loss': Array(0.02457327, dtype=float32), 'training/total_loss': Array(1207.323, dtype=float32), 'training/v_loss': Array(1207.032, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3332578, dtype=float32), Array(0.09774256, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1024.4629, dtype=float32), Array(3021.051, dtype=float32)), 'eval/episode_reward': (Array(-8860.265, dtype=float32), Array(2554.8464, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.31247824, dtype=float32), Array(0.10751792, dtype=float32)), 'eval/avg_episode_length': (Array(899.0469, dtype=float32), Array(300.2601, dtype=float32)), 'eval/epoch_eval_time': 4.085322856903076, 'eval/sps': 31331.67303624855}
I0727 21:58:08.415193 139877795424064 train.py:379] starting iteration 59, 24166400 steps, 862.5310509204865
I0727 21:58:22.485369 139877795424064 train.py:394] {'eval/walltime': 259.05567240715027, 'training/sps': 40985.08077784359, 'training/walltime': 608.466153383255, 'training/entropy_loss': Array(0.2612512, dtype=float32), 'training/policy_loss': Array(0.02724881, dtype=float32), 'training/total_loss': Array(1136.8486, dtype=float32), 'training/v_loss': Array(1136.56, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.33573997, dtype=float32), Array(0.08690926, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1727.7577, dtype=float32), Array(3773.374, dtype=float32)), 'eval/episode_reward': (Array(-8935.893, dtype=float32), Array(2126.9365, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.31622797, dtype=float32), Array(0.09531941, dtype=float32)), 'eval/avg_episode_length': (Array(829.2031, dtype=float32), Array(374.90643, dtype=float32)), 'eval/epoch_eval_time': 4.072758197784424, 'eval/sps': 31428.332786766437}
I0727 21:58:22.488037 139877795424064 train.py:379] starting iteration 60, 24576000 steps, 876.6038942337036
I0727 21:58:36.581950 139877795424064 train.py:394] {'eval/walltime': 263.1249713897705, 'training/sps': 40875.25382821794, 'training/walltime': 618.4868862628937, 'training/entropy_loss': Array(0.25670686, dtype=float32), 'training/policy_loss': Array(0.01936108, dtype=float32), 'training/total_loss': Array(42522.406, dtype=float32), 'training/v_loss': Array(42522.13, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.33174938, dtype=float32), Array(0.09110908, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1805.7332, dtype=float32), Array(3839.7246, dtype=float32)), 'eval/episode_reward': (Array(-8902.652, dtype=float32), Array(2320.221, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.31183386, dtype=float32), Array(0.10159126, dtype=float32)), 'eval/avg_episode_length': (Array(821.46875, dtype=float32), Array(381.4579, dtype=float32)), 'eval/epoch_eval_time': 4.069298982620239, 'eval/sps': 31455.049271798714}
I0727 21:58:36.584630 139877795424064 train.py:379] starting iteration 61, 24985600 steps, 890.7004873752594
I0727 21:58:50.724657 139877795424064 train.py:394] {'eval/walltime': 267.21350741386414, 'training/sps': 40766.214147170474, 'training/walltime': 628.534422159195, 'training/entropy_loss': Array(0.2552147, dtype=float32), 'training/policy_loss': Array(0.0224895, dtype=float32), 'training/total_loss': Array(1189.9265, dtype=float32), 'training/v_loss': Array(1189.6488, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.32395893, dtype=float32), Array(0.09435093, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1571.1794, dtype=float32), Array(3631.1682, dtype=float32)), 'eval/episode_reward': (Array(-8797.711, dtype=float32), Array(2491.9556, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30173156, dtype=float32), Array(0.10392538, dtype=float32)), 'eval/avg_episode_length': (Array(844.6406, dtype=float32), Array(361.02295, dtype=float32)), 'eval/epoch_eval_time': 4.088536024093628, 'eval/sps': 31307.04957610734}
I0727 21:58:50.727363 139877795424064 train.py:379] starting iteration 62, 25395200 steps, 904.843220949173
I0727 21:59:04.879606 139877795424064 train.py:394] {'eval/walltime': 271.32440400123596, 'training/sps': 40806.622390431665, 'training/walltime': 638.5720086097717, 'training/entropy_loss': Array(0.25271776, dtype=float32), 'training/policy_loss': Array(0.01835597, dtype=float32), 'training/total_loss': Array(1389.9471, dtype=float32), 'training/v_loss': Array(1389.676, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.32300878, dtype=float32), Array(0.08510271, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1492.9133, dtype=float32), Array(3555.544, dtype=float32)), 'eval/episode_reward': (Array(-8691.543, dtype=float32), Array(2259.5505, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.303464, dtype=float32), Array(0.09381753, dtype=float32)), 'eval/avg_episode_length': (Array(852.39844, dtype=float32), Array(353.53168, dtype=float32)), 'eval/epoch_eval_time': 4.110896587371826, 'eval/sps': 31136.759896417832}
I0727 21:59:04.882276 139877795424064 train.py:379] starting iteration 63, 25804800 steps, 918.9981331825256
I0727 21:59:18.996216 139877795424064 train.py:394] {'eval/walltime': 275.400132894516, 'training/sps': 40825.13400690882, 'training/walltime': 648.6050436496735, 'training/entropy_loss': Array(0.2511943, dtype=float32), 'training/policy_loss': Array(0.01700183, dtype=float32), 'training/total_loss': Array(1128.2983, dtype=float32), 'training/v_loss': Array(1128.03, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31146944, dtype=float32), Array(0.09244536, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1648.9418, dtype=float32), Array(3704.1401, dtype=float32)), 'eval/episode_reward': (Array(-8295.752, dtype=float32), Array(2373.4668, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2898138, dtype=float32), Array(0.10205191, dtype=float32)), 'eval/avg_episode_length': (Array(836.78125, dtype=float32), Array(368.4285, dtype=float32)), 'eval/epoch_eval_time': 4.075728893280029, 'eval/sps': 31405.42547151346}
I0727 21:59:18.998916 139877795424064 train.py:379] starting iteration 64, 26214400 steps, 933.1147735118866
I0727 21:59:33.091789 139877795424064 train.py:394] {'eval/walltime': 279.50143337249756, 'training/sps': 41010.10712050449, 'training/walltime': 658.5928254127502, 'training/entropy_loss': Array(0.2485736, dtype=float32), 'training/policy_loss': Array(0.01174171, dtype=float32), 'training/total_loss': Array(1117.2845, dtype=float32), 'training/v_loss': Array(1117.0242, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.34002474, dtype=float32), Array(0.08538214, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1962.262, dtype=float32), Array(3964.56, dtype=float32)), 'eval/episode_reward': (Array(-9255.801, dtype=float32), Array(2078.3572, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.31966045, dtype=float32), Array(0.09438661, dtype=float32)), 'eval/avg_episode_length': (Array(805.875, dtype=float32), Array(394.03174, dtype=float32)), 'eval/epoch_eval_time': 4.101300477981567, 'eval/sps': 31209.612825782155}
I0727 21:59:33.094468 139877795424064 train.py:379] starting iteration 65, 26624000 steps, 947.2103254795074
I0727 21:59:47.244424 139877795424064 train.py:394] {'eval/walltime': 283.6094627380371, 'training/sps': 40804.97761813309, 'training/walltime': 668.6308164596558, 'training/entropy_loss': Array(0.24320813, dtype=float32), 'training/policy_loss': Array(0.01335299, dtype=float32), 'training/total_loss': Array(42665.6, dtype=float32), 'training/v_loss': Array(42665.35, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.315722, dtype=float32), Array(0.09205245, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1805.2566, dtype=float32), Array(3839.9475, dtype=float32)), 'eval/episode_reward': (Array(-8418.184, dtype=float32), Array(2408.168, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2947042, dtype=float32), Array(0.10105991, dtype=float32)), 'eval/avg_episode_length': (Array(821.3281, dtype=float32), Array(381.75797, dtype=float32)), 'eval/epoch_eval_time': 4.108029365539551, 'eval/sps': 31158.491970319305}
I0727 21:59:47.247110 139877795424064 train.py:379] starting iteration 66, 27033600 steps, 961.3629677295685
I0727 22:00:01.378280 139877795424064 train.py:394] {'eval/walltime': 287.7151882648468, 'training/sps': 40871.91347554631, 'training/walltime': 678.6523683071136, 'training/entropy_loss': Array(0.24341033, dtype=float32), 'training/policy_loss': Array(0.01572576, dtype=float32), 'training/total_loss': Array(1123.3363, dtype=float32), 'training/v_loss': Array(1123.0771, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.328695, dtype=float32), Array(0.09447708, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(2274.3042, dtype=float32), Array(4186.5903, dtype=float32)), 'eval/episode_reward': (Array(-8767.773, dtype=float32), Array(2485.005, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30639172, dtype=float32), Array(0.10372836, dtype=float32)), 'eval/avg_episode_length': (Array(774.625, dtype=float32), Array(416.4137, dtype=float32)), 'eval/epoch_eval_time': 4.105725526809692, 'eval/sps': 31175.97588152975}
I0727 22:00:01.381091 139877795424064 train.py:379] starting iteration 67, 27443200 steps, 975.4969480037689
I0727 22:00:15.503916 139877795424064 train.py:394] {'eval/walltime': 291.8088264465332, 'training/sps': 40856.51310216282, 'training/walltime': 688.6776976585388, 'training/entropy_loss': Array(0.24290442, dtype=float32), 'training/policy_loss': Array(0.01292603, dtype=float32), 'training/total_loss': Array(1254.0173, dtype=float32), 'training/v_loss': Array(1253.7615, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.34430695, dtype=float32), Array(0.10387512, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1962.2712, dtype=float32), Array(3964.7053, dtype=float32)), 'eval/episode_reward': (Array(-9209.629, dtype=float32), Array(2674.0864, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.32359374, dtype=float32), Array(0.11454799, dtype=float32)), 'eval/avg_episode_length': (Array(805.83594, dtype=float32), Array(394.1106, dtype=float32)), 'eval/epoch_eval_time': 4.093638181686401, 'eval/sps': 31268.029639900797}
I0727 22:00:15.506715 139877795424064 train.py:379] starting iteration 68, 27852800 steps, 989.622572183609
I0727 22:00:29.636883 139877795424064 train.py:394] {'eval/walltime': 295.9132344722748, 'training/sps': 40870.18273607022, 'training/walltime': 698.6996738910675, 'training/entropy_loss': Array(0.23989356, dtype=float32), 'training/policy_loss': Array(0.0102507, dtype=float32), 'training/total_loss': Array(1082.1799, dtype=float32), 'training/v_loss': Array(1081.9298, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3158529, dtype=float32), Array(0.08799715, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1102.2367, dtype=float32), Array(3121.7249, dtype=float32)), 'eval/episode_reward': (Array(-8456.341, dtype=float32), Array(2191.4473, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2938464, dtype=float32), Array(0.09865186, dtype=float32)), 'eval/avg_episode_length': (Array(891.28125, dtype=float32), Array(310.23706, dtype=float32)), 'eval/epoch_eval_time': 4.104408025741577, 'eval/sps': 31185.983264145183}
I0727 22:00:29.639709 139877795424064 train.py:379] starting iteration 69, 28262400 steps, 1003.7555663585663
I0727 22:00:43.785702 139877795424064 train.py:394] {'eval/walltime': 300.03071308135986, 'training/sps': 40859.657552079276, 'training/walltime': 708.7242317199707, 'training/entropy_loss': Array(0.2325542, dtype=float32), 'training/policy_loss': Array(0.01086386, dtype=float32), 'training/total_loss': Array(1084.1526, dtype=float32), 'training/v_loss': Array(1083.9092, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.33588392, dtype=float32), Array(0.09724115, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1727.6659, dtype=float32), Array(3772.6716, dtype=float32)), 'eval/episode_reward': (Array(-9126.234, dtype=float32), Array(2477.3323, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3148382, dtype=float32), Array(0.10802848, dtype=float32)), 'eval/avg_episode_length': (Array(829.15625, dtype=float32), Array(375.0088, dtype=float32)), 'eval/epoch_eval_time': 4.117478609085083, 'eval/sps': 31086.98603013314}
I0727 22:00:43.788611 139877795424064 train.py:379] starting iteration 70, 28672000 steps, 1017.9044692516327
I0727 22:00:57.891914 139877795424064 train.py:394] {'eval/walltime': 304.1357102394104, 'training/sps': 40983.27493689052, 'training/walltime': 718.7185525894165, 'training/entropy_loss': Array(0.21831341, dtype=float32), 'training/policy_loss': Array(0.01185507, dtype=float32), 'training/total_loss': Array(41494.53, dtype=float32), 'training/v_loss': Array(41494.297, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.33596888, dtype=float32), Array(0.09897383, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1180.8015, dtype=float32), Array(3216.612, dtype=float32)), 'eval/episode_reward': (Array(-9004.978, dtype=float32), Array(2534.3955, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.31426424, dtype=float32), Array(0.11104511, dtype=float32)), 'eval/avg_episode_length': (Array(883.60156, dtype=float32), Array(319.47876, dtype=float32)), 'eval/epoch_eval_time': 4.104997158050537, 'eval/sps': 31181.507580089823}
I0727 22:00:57.894779 139877795424064 train.py:379] starting iteration 71, 29081600 steps, 1032.010636806488
I0727 22:01:12.042109 139877795424064 train.py:394] {'eval/walltime': 308.2406949996948, 'training/sps': 40803.30293614539, 'training/walltime': 728.7569556236267, 'training/entropy_loss': Array(0.19857933, dtype=float32), 'training/policy_loss': Array(0.0150402, dtype=float32), 'training/total_loss': Array(1194.7036, dtype=float32), 'training/v_loss': Array(1194.4899, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.32209423, dtype=float32), Array(0.09205597, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1180.3804, dtype=float32), Array(3216.6147, dtype=float32)), 'eval/episode_reward': (Array(-8626.098, dtype=float32), Array(2363.0632, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30000678, dtype=float32), Array(0.10069897, dtype=float32)), 'eval/avg_episode_length': (Array(883.4844, dtype=float32), Array(319.80017, dtype=float32)), 'eval/epoch_eval_time': 4.104984760284424, 'eval/sps': 31181.601753652114}
I0727 22:01:12.044939 139877795424064 train.py:379] starting iteration 72, 29491200 steps, 1046.1607966423035
I0727 22:01:26.160791 139877795424064 train.py:394] {'eval/walltime': 312.3364245891571, 'training/sps': 40894.083787855016, 'training/walltime': 738.773074388504, 'training/entropy_loss': Array(0.17782632, dtype=float32), 'training/policy_loss': Array(0.01777682, dtype=float32), 'training/total_loss': Array(1441.6643, dtype=float32), 'training/v_loss': Array(1441.4688, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.32469764, dtype=float32), Array(0.09893221, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1805.477, dtype=float32), Array(3839.8547, dtype=float32)), 'eval/episode_reward': (Array(-8631.282, dtype=float32), Array(2545.396, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30283624, dtype=float32), Array(0.10879547, dtype=float32)), 'eval/avg_episode_length': (Array(821.27344, dtype=float32), Array(381.87476, dtype=float32)), 'eval/epoch_eval_time': 4.09572958946228, 'eval/sps': 31252.06320488674}
I0727 22:01:26.163578 139877795424064 train.py:379] starting iteration 73, 29900800 steps, 1060.2794351577759
I0727 22:01:40.289502 139877795424064 train.py:394] {'eval/walltime': 316.44521403312683, 'training/sps': 40906.32433733393, 'training/walltime': 748.7861959934235, 'training/entropy_loss': Array(0.1555164, dtype=float32), 'training/policy_loss': Array(0.01877446, dtype=float32), 'training/total_loss': Array(1159.3478, dtype=float32), 'training/v_loss': Array(1159.1735, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31862426, dtype=float32), Array(0.08706007, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1570.9187, dtype=float32), Array(3631.4336, dtype=float32)), 'eval/episode_reward': (Array(-8488.447, dtype=float32), Array(2348.6248, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29778808, dtype=float32), Array(0.09692164, dtype=float32)), 'eval/avg_episode_length': (Array(844.6172, dtype=float32), Array(361.0774, dtype=float32)), 'eval/epoch_eval_time': 4.108789443969727, 'eval/sps': 31152.728010402057}
I0727 22:01:40.292296 139877795424064 train.py:379] starting iteration 74, 30310400 steps, 1074.4081530570984
I0727 22:01:54.434172 139877795424064 train.py:394] {'eval/walltime': 320.5526270866394, 'training/sps': 40835.15318314344, 'training/walltime': 758.816769361496, 'training/entropy_loss': Array(0.13606983, dtype=float32), 'training/policy_loss': Array(0.02201654, dtype=float32), 'training/total_loss': Array(985.4699, dtype=float32), 'training/v_loss': Array(985.3118, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.32435864, dtype=float32), Array(0.09968725, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1649.3113, dtype=float32), Array(3703.8625, dtype=float32)), 'eval/episode_reward': (Array(-8702.395, dtype=float32), Array(2520.5051, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3019395, dtype=float32), Array(0.11093124, dtype=float32)), 'eval/avg_episode_length': (Array(836.96094, dtype=float32), Array(368.0228, dtype=float32)), 'eval/epoch_eval_time': 4.107413053512573, 'eval/sps': 31163.16726182118}
I0727 22:01:54.437072 139877795424064 train.py:379] starting iteration 75, 30720000 steps, 1088.5529289245605
I0727 22:02:08.571186 139877795424064 train.py:394] {'eval/walltime': 324.6535608768463, 'training/sps': 40842.27682354785, 'training/walltime': 768.845593214035, 'training/entropy_loss': Array(0.00291938, dtype=float32), 'training/policy_loss': Array(0.03514373, dtype=float32), 'training/total_loss': Array(66628.734, dtype=float32), 'training/v_loss': Array(66628.7, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.60776174, dtype=float32), Array(0.33882722, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1109.8938, dtype=float32), Array(3118.763, dtype=float32)), 'eval/episode_reward': (Array(-13457.668, dtype=float32), Array(5307.776, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.6049617, dtype=float32), Array(0.348937, dtype=float32)), 'eval/avg_episode_length': (Array(891.2422, dtype=float32), Array(310.34857, dtype=float32)), 'eval/epoch_eval_time': 4.100933790206909, 'eval/sps': 31212.40345446832}
I0727 22:02:08.634479 139877795424064 train.py:379] starting iteration 76, 31129600 steps, 1102.7503280639648
I0727 22:02:22.777541 139877795424064 train.py:394] {'eval/walltime': 328.73008704185486, 'training/sps': 40706.22372808491, 'training/walltime': 778.9079365730286, 'training/entropy_loss': Array(-0.03616435, dtype=float32), 'training/policy_loss': Array(0.00739246, dtype=float32), 'training/total_loss': Array(3956.0715, dtype=float32), 'training/v_loss': Array(3956.1006, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.6150098, dtype=float32), Array(0.3352116, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1110.0544, dtype=float32), Array(3118.9604, dtype=float32)), 'eval/episode_reward': (Array(-14092.899, dtype=float32), Array(5865.7993, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.61249566, dtype=float32), Array(0.34491602, dtype=float32)), 'eval/avg_episode_length': (Array(891.1719, dtype=float32), Array(310.5489, dtype=float32)), 'eval/epoch_eval_time': 4.076526165008545, 'eval/sps': 31399.283315953326}
I0727 22:02:22.780480 139877795424064 train.py:379] starting iteration 77, 31539200 steps, 1116.8963372707367
I0727 22:02:36.884504 139877795424064 train.py:394] {'eval/walltime': 332.80574584007263, 'training/sps': 40859.84024822423, 'training/walltime': 788.9324495792389, 'training/entropy_loss': Array(-0.02222877, dtype=float32), 'training/policy_loss': Array(0.00375697, dtype=float32), 'training/total_loss': Array(2692.291, dtype=float32), 'training/v_loss': Array(2692.3096, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5525895, dtype=float32), Array(0.2761207, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(327.1793, dtype=float32), Array(1739.0409, dtype=float32)), 'eval/episode_reward': (Array(-12891.252, dtype=float32), Array(5046.131, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5492076, dtype=float32), Array(0.28494462, dtype=float32)), 'eval/avg_episode_length': (Array(968.875, dtype=float32), Array(173.2967, dtype=float32)), 'eval/epoch_eval_time': 4.075658798217773, 'eval/sps': 31405.965596524555}
I0727 22:02:36.887287 139877795424064 train.py:379] starting iteration 78, 31948800 steps, 1131.0031445026398
I0727 22:02:50.972056 139877795424064 train.py:394] {'eval/walltime': 336.8797724246979, 'training/sps': 40932.959269632265, 'training/walltime': 798.9390556812286, 'training/entropy_loss': Array(-0.01237629, dtype=float32), 'training/policy_loss': Array(0.00263198, dtype=float32), 'training/total_loss': Array(2304.5356, dtype=float32), 'training/v_loss': Array(2304.5454, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5834917, dtype=float32), Array(0.31441066, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1968.5925, dtype=float32), Array(3961.5266, dtype=float32)), 'eval/episode_reward': (Array(-13634.6045, dtype=float32), Array(5377.9194, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5815491, dtype=float32), Array(0.3226982, dtype=float32)), 'eval/avg_episode_length': (Array(805.9453, dtype=float32), Array(393.88873, dtype=float32)), 'eval/epoch_eval_time': 4.074026584625244, 'eval/sps': 31418.548048520966}
I0727 22:02:50.974885 139877795424064 train.py:379] starting iteration 79, 32358400 steps, 1145.0907430648804
I0727 22:03:05.125782 139877795424064 train.py:394] {'eval/walltime': 340.9639217853546, 'training/sps': 40705.22838946405, 'training/walltime': 809.0016450881958, 'training/entropy_loss': Array(-0.00732831, dtype=float32), 'training/policy_loss': Array(0.00236864, dtype=float32), 'training/total_loss': Array(1790.779, dtype=float32), 'training/v_loss': Array(1790.784, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5660405, dtype=float32), Array(0.29197797, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1265.04, dtype=float32), Array(3305.106, dtype=float32)), 'eval/episode_reward': (Array(-13174.615, dtype=float32), Array(5427.674, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.561736, dtype=float32), Array(0.30223665, dtype=float32)), 'eval/avg_episode_length': (Array(875.875, dtype=float32), Array(328.40445, dtype=float32)), 'eval/epoch_eval_time': 4.084149360656738, 'eval/sps': 31340.675547531242}
I0727 22:03:05.129904 139877795424064 train.py:379] starting iteration 80, 32768000 steps, 1159.2457542419434
I0727 22:03:19.280668 139877795424064 train.py:394] {'eval/walltime': 345.03996419906616, 'training/sps': 40672.93621156603, 'training/walltime': 819.0722236633301, 'training/entropy_loss': Array(-0.01239989, dtype=float32), 'training/policy_loss': Array(0.00270963, dtype=float32), 'training/total_loss': Array(30803.771, dtype=float32), 'training/v_loss': Array(30803.78, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.57750773, dtype=float32), Array(0.32291052, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1187.2351, dtype=float32), Array(3214.415, dtype=float32)), 'eval/episode_reward': (Array(-13318.587, dtype=float32), Array(6035.995, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5748184, dtype=float32), Array(0.33174512, dtype=float32)), 'eval/avg_episode_length': (Array(883.52344, dtype=float32), Array(319.69302, dtype=float32)), 'eval/epoch_eval_time': 4.076042413711548, 'eval/sps': 31403.009833611184}
I0727 22:03:19.283388 139877795424064 train.py:379] starting iteration 81, 33177600 steps, 1173.3992459774017
I0727 22:03:33.402130 139877795424064 train.py:394] {'eval/walltime': 349.11167669296265, 'training/sps': 40785.11126039916, 'training/walltime': 829.1151041984558, 'training/entropy_loss': Array(-0.01629379, dtype=float32), 'training/policy_loss': Array(0.00200869, dtype=float32), 'training/total_loss': Array(3079.8743, dtype=float32), 'training/v_loss': Array(3079.889, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.53835166, dtype=float32), Array(0.24600294, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1264.2935, dtype=float32), Array(3305.3374, dtype=float32)), 'eval/episode_reward': (Array(-12499.131, dtype=float32), Array(4335.223, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.53395474, dtype=float32), Array(0.25437954, dtype=float32)), 'eval/avg_episode_length': (Array(875.78906, dtype=float32), Array(328.63208, dtype=float32)), 'eval/epoch_eval_time': 4.071712493896484, 'eval/sps': 31436.40426279424}
I0727 22:03:33.404911 139877795424064 train.py:379] starting iteration 82, 33587200 steps, 1187.520768404007
I0727 22:03:47.510735 139877795424064 train.py:394] {'eval/walltime': 353.19303011894226, 'training/sps': 40877.29428892263, 'training/walltime': 839.1353368759155, 'training/entropy_loss': Array(-0.01740732, dtype=float32), 'training/policy_loss': Array(0.00211193, dtype=float32), 'training/total_loss': Array(1528.9899, dtype=float32), 'training/v_loss': Array(1529.0051, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5361192, dtype=float32), Array(0.2793613, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(951.69006, dtype=float32), Array(2913.0623, dtype=float32)), 'eval/episode_reward': (Array(-12681.211, dtype=float32), Array(5115.827, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5315637, dtype=float32), Array(0.2885899, dtype=float32)), 'eval/avg_episode_length': (Array(906.83594, dtype=float32), Array(289.65927, dtype=float32)), 'eval/epoch_eval_time': 4.081353425979614, 'eval/sps': 31362.14550428874}
I0727 22:03:47.513477 139877795424064 train.py:379] starting iteration 83, 33996800 steps, 1201.6293346881866
I0727 22:04:01.630502 139877795424064 train.py:394] {'eval/walltime': 357.2700855731964, 'training/sps': 40813.96199730322, 'training/walltime': 849.1711182594299, 'training/entropy_loss': Array(-0.02022675, dtype=float32), 'training/policy_loss': Array(0.00205431, dtype=float32), 'training/total_loss': Array(1311.594, dtype=float32), 'training/v_loss': Array(1311.6122, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5186293, dtype=float32), Array(0.24866198, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1263.8257, dtype=float32), Array(3305.537, dtype=float32)), 'eval/episode_reward': (Array(-12290.921, dtype=float32), Array(4260.392, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5133253, dtype=float32), Array(0.25755915, dtype=float32)), 'eval/avg_episode_length': (Array(875.8047, dtype=float32), Array(328.59082, dtype=float32)), 'eval/epoch_eval_time': 4.07705545425415, 'eval/sps': 31395.207015504307}
I0727 22:04:01.633284 139877795424064 train.py:379] starting iteration 84, 34406400 steps, 1215.7491421699524
I0727 22:04:15.791337 139877795424064 train.py:394] {'eval/walltime': 361.34016966819763, 'training/sps': 40620.30271624453, 'training/walltime': 859.254745721817, 'training/entropy_loss': Array(-0.02239593, dtype=float32), 'training/policy_loss': Array(0.00220013, dtype=float32), 'training/total_loss': Array(1224.0573, dtype=float32), 'training/v_loss': Array(1224.0774, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5574858, dtype=float32), Array(0.29392466, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(483.54288, dtype=float32), Array(2112.624, dtype=float32)), 'eval/episode_reward': (Array(-12744.64, dtype=float32), Array(5043.5767, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.55418515, dtype=float32), Array(0.30256307, dtype=float32)), 'eval/avg_episode_length': (Array(953.4297, dtype=float32), Array(209.99757, dtype=float32)), 'eval/epoch_eval_time': 4.070084095001221, 'eval/sps': 31448.981645663418}
I0727 22:04:15.794114 139877795424064 train.py:379] starting iteration 85, 34816000 steps, 1229.9099712371826
I0727 22:04:29.914233 139877795424064 train.py:394] {'eval/walltime': 365.413028717041, 'training/sps': 40784.45383538571, 'training/walltime': 869.297788143158, 'training/entropy_loss': Array(-0.03104185, dtype=float32), 'training/policy_loss': Array(0.00260598, dtype=float32), 'training/total_loss': Array(33431.055, dtype=float32), 'training/v_loss': Array(33431.086, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5735817, dtype=float32), Array(0.34577915, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(640.2306, dtype=float32), Array(2419.2258, dtype=float32)), 'eval/episode_reward': (Array(-13179.714, dtype=float32), Array(5887.6978, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5694487, dtype=float32), Array(0.35618916, dtype=float32)), 'eval/avg_episode_length': (Array(937.9297, dtype=float32), Array(240.39812, dtype=float32)), 'eval/epoch_eval_time': 4.072859048843384, 'eval/sps': 31427.55456669919}
I0727 22:04:29.917001 139877795424064 train.py:379] starting iteration 86, 35225600 steps, 1244.0328586101532
I0727 22:04:44.024939 139877795424064 train.py:394] {'eval/walltime': 369.4802541732788, 'training/sps': 40809.84641247006, 'training/walltime': 879.3345816135406, 'training/entropy_loss': Array(-0.03607278, dtype=float32), 'training/policy_loss': Array(0.00145832, dtype=float32), 'training/total_loss': Array(2770.09, dtype=float32), 'training/v_loss': Array(2770.1245, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5235065, dtype=float32), Array(0.2746089, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(795.1633, dtype=float32), Array(2682.5696, dtype=float32)), 'eval/episode_reward': (Array(-12503.081, dtype=float32), Array(4922.083, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5185922, dtype=float32), Array(0.28320912, dtype=float32)), 'eval/avg_episode_length': (Array(922.4219, dtype=float32), Array(266.49017, dtype=float32)), 'eval/epoch_eval_time': 4.067225456237793, 'eval/sps': 31471.08548007583}
I0727 22:04:44.027730 139877795424064 train.py:379] starting iteration 87, 35635200 steps, 1258.1435871124268
I0727 22:04:58.192684 139877795424064 train.py:394] {'eval/walltime': 373.55654978752136, 'training/sps': 40617.13641995659, 'training/walltime': 889.418995141983, 'training/entropy_loss': Array(-0.03493383, dtype=float32), 'training/policy_loss': Array(0.00148111, dtype=float32), 'training/total_loss': Array(2269.399, dtype=float32), 'training/v_loss': Array(2269.4324, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5559311, dtype=float32), Array(0.26054612, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(639.71796, dtype=float32), Array(2419.3748, dtype=float32)), 'eval/episode_reward': (Array(-12716.7, dtype=float32), Array(4328.462, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5520556, dtype=float32), Array(0.2689699, dtype=float32)), 'eval/avg_episode_length': (Array(937.9531, dtype=float32), Array(240.30754, dtype=float32)), 'eval/epoch_eval_time': 4.076295614242554, 'eval/sps': 31401.059224647182}
I0727 22:04:58.195473 139877795424064 train.py:379] starting iteration 88, 36044800 steps, 1272.3113300800323
I0727 22:05:12.397364 139877795424064 train.py:394] {'eval/walltime': 377.65089106559753, 'training/sps': 40540.74876312608, 'training/walltime': 899.5224099159241, 'training/entropy_loss': Array(-0.03666922, dtype=float32), 'training/policy_loss': Array(0.00157676, dtype=float32), 'training/total_loss': Array(1523.1953, dtype=float32), 'training/v_loss': Array(1523.2305, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.543108, dtype=float32), Array(0.264048, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(483.19196, dtype=float32), Array(2112.7249, dtype=float32)), 'eval/episode_reward': (Array(-12451.427, dtype=float32), Array(4761.0244, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5391201, dtype=float32), Array(0.27248502, dtype=float32)), 'eval/avg_episode_length': (Array(953.3828, dtype=float32), Array(210.20876, dtype=float32)), 'eval/epoch_eval_time': 4.094341278076172, 'eval/sps': 31262.660170855124}
I0727 22:05:12.400394 139877795424064 train.py:379] starting iteration 89, 36454400 steps, 1286.5162518024445
I0727 22:05:26.572735 139877795424064 train.py:394] {'eval/walltime': 381.75293374061584, 'training/sps': 40691.31128725409, 'training/walltime': 909.5884408950806, 'training/entropy_loss': Array(-0.03700111, dtype=float32), 'training/policy_loss': Array(0.00121434, dtype=float32), 'training/total_loss': Array(1372.9214, dtype=float32), 'training/v_loss': Array(1372.9573, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5420841, dtype=float32), Array(0.2901995, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(248.72337, dtype=float32), Array(1512.1633, dtype=float32)), 'eval/episode_reward': (Array(-12668.233, dtype=float32), Array(5217.636, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.536749, dtype=float32), Array(0.30010852, dtype=float32)), 'eval/avg_episode_length': (Array(976.6719, dtype=float32), Array(150.58241, dtype=float32)), 'eval/epoch_eval_time': 4.1020426750183105, 'eval/sps': 31203.96596055126}
I0727 22:05:26.575544 139877795424064 train.py:379] starting iteration 90, 36864000 steps, 1300.691401720047
I0727 22:05:40.743452 139877795424064 train.py:394] {'eval/walltime': 385.8614444732666, 'training/sps': 40734.226952971505, 'training/walltime': 919.64386677742, 'training/entropy_loss': Array(-0.03837747, dtype=float32), 'training/policy_loss': Array(0.00130956, dtype=float32), 'training/total_loss': Array(24581.191, dtype=float32), 'training/v_loss': Array(24581.229, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.55485296, dtype=float32), Array(0.27625343, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(795.9444, dtype=float32), Array(2682.4282, dtype=float32)), 'eval/episode_reward': (Array(-12600.336, dtype=float32), Array(5328.651, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5496946, dtype=float32), Array(0.28581816, dtype=float32)), 'eval/avg_episode_length': (Array(922.3047, dtype=float32), Array(266.89267, dtype=float32)), 'eval/epoch_eval_time': 4.108510732650757, 'eval/sps': 31154.841335272864}
I0727 22:05:40.746258 139877795424064 train.py:379] starting iteration 91, 37273600 steps, 1314.8621158599854
I0727 22:05:54.885542 139877795424064 train.py:394] {'eval/walltime': 389.9394929409027, 'training/sps': 40725.287293843554, 'training/walltime': 929.7014999389648, 'training/entropy_loss': Array(-0.04247112, dtype=float32), 'training/policy_loss': Array(0.0014625, dtype=float32), 'training/total_loss': Array(3419.6438, dtype=float32), 'training/v_loss': Array(3419.685, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.53411293, dtype=float32), Array(0.28426915, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(951.6925, dtype=float32), Array(2913.1938, dtype=float32)), 'eval/episode_reward': (Array(-12356.266, dtype=float32), Array(4989.797, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5287595, dtype=float32), Array(0.29359347, dtype=float32)), 'eval/avg_episode_length': (Array(906.85156, dtype=float32), Array(289.61087, dtype=float32)), 'eval/epoch_eval_time': 4.078048467636108, 'eval/sps': 31387.562216541482}
I0727 22:05:54.888381 139877795424064 train.py:379] starting iteration 92, 37683200 steps, 1329.0042395591736
I0727 22:06:09.014987 139877795424064 train.py:394] {'eval/walltime': 394.0182342529297, 'training/sps': 40781.19219762023, 'training/walltime': 939.7453455924988, 'training/entropy_loss': Array(-0.04262731, dtype=float32), 'training/policy_loss': Array(0.00108774, dtype=float32), 'training/total_loss': Array(2020.8765, dtype=float32), 'training/v_loss': Array(2020.9177, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5443326, dtype=float32), Array(0.27660528, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(483.1624, dtype=float32), Array(2112.5015, dtype=float32)), 'eval/episode_reward': (Array(-12351.271, dtype=float32), Array(4916.048, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5396596, dtype=float32), Array(0.28582942, dtype=float32)), 'eval/avg_episode_length': (Array(953.4219, dtype=float32), Array(210.03268, dtype=float32)), 'eval/epoch_eval_time': 4.0787413120269775, 'eval/sps': 31382.23049904308}
I0727 22:06:09.017756 139877795424064 train.py:379] starting iteration 93, 38092800 steps, 1343.1336143016815
I0727 22:06:23.108038 139877795424064 train.py:394] {'eval/walltime': 398.09315633773804, 'training/sps': 40913.20391548535, 'training/walltime': 949.7567834854126, 'training/entropy_loss': Array(-0.04021028, dtype=float32), 'training/policy_loss': Array(0.00128709, dtype=float32), 'training/total_loss': Array(1513.4033, dtype=float32), 'training/v_loss': Array(1513.4421, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5122067, dtype=float32), Array(0.22736925, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(794.8241, dtype=float32), Array(2682.2937, dtype=float32)), 'eval/episode_reward': (Array(-11857.768, dtype=float32), Array(4037.9426, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5067901, dtype=float32), Array(0.2357727, dtype=float32)), 'eval/avg_episode_length': (Array(922.375, dtype=float32), Array(266.6514, dtype=float32)), 'eval/epoch_eval_time': 4.07492208480835, 'eval/sps': 31411.643544595543}
I0727 22:06:23.110822 139877795424064 train.py:379] starting iteration 94, 38502400 steps, 1357.226680278778
I0727 22:06:37.248755 139877795424064 train.py:394] {'eval/walltime': 402.17565393447876, 'training/sps': 40748.8983102788, 'training/walltime': 959.8085889816284, 'training/entropy_loss': Array(-0.0387252, dtype=float32), 'training/policy_loss': Array(0.00112286, dtype=float32), 'training/total_loss': Array(1392.7961, dtype=float32), 'training/v_loss': Array(1392.8337, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5752169, dtype=float32), Array(0.27694485, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(562.099, dtype=float32), Array(2272.3755, dtype=float32)), 'eval/episode_reward': (Array(-12955.732, dtype=float32), Array(4489.488, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.57127786, dtype=float32), Array(0.28556937, dtype=float32)), 'eval/avg_episode_length': (Array(945.66406, dtype=float32), Array(225.90816, dtype=float32)), 'eval/epoch_eval_time': 4.082497596740723, 'eval/sps': 31353.355872686683}
I0727 22:06:37.251523 139877795424064 train.py:379] starting iteration 95, 38912000 steps, 1371.3673808574677
I0727 22:06:51.427357 139877795424064 train.py:394] {'eval/walltime': 406.2505900859833, 'training/sps': 40566.82051793787, 'training/walltime': 969.9055104255676, 'training/entropy_loss': Array(-0.03665788, dtype=float32), 'training/policy_loss': Array(0.00169268, dtype=float32), 'training/total_loss': Array(28844.074, dtype=float32), 'training/v_loss': Array(28844.11, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.52587223, dtype=float32), Array(0.27018934, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1029.5654, dtype=float32), Array(3019.31, dtype=float32)), 'eval/episode_reward': (Array(-11952.891, dtype=float32), Array(4756.238, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5204334, dtype=float32), Array(0.27847227, dtype=float32)), 'eval/avg_episode_length': (Array(899.1328, dtype=float32), Array(300.0044, dtype=float32)), 'eval/epoch_eval_time': 4.074936151504517, 'eval/sps': 31411.53511147428}
I0727 22:06:51.430133 139877795424064 train.py:379] starting iteration 96, 39321600 steps, 1385.5459904670715
I0727 22:07:05.560458 139877795424064 train.py:394] {'eval/walltime': 410.32897090911865, 'training/sps': 40764.45462450989, 'training/walltime': 979.9534800052643, 'training/entropy_loss': Array(-0.03945798, dtype=float32), 'training/policy_loss': Array(0.00131963, dtype=float32), 'training/total_loss': Array(3343.018, dtype=float32), 'training/v_loss': Array(3343.0562, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.510563, dtype=float32), Array(0.25700307, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(560.41174, dtype=float32), Array(2272.5918, dtype=float32)), 'eval/episode_reward': (Array(-11841.566, dtype=float32), Array(4132.19, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5047642, dtype=float32), Array(0.2660991, dtype=float32)), 'eval/avg_episode_length': (Array(945.60156, dtype=float32), Array(226.16754, dtype=float32)), 'eval/epoch_eval_time': 4.078380823135376, 'eval/sps': 31385.00438063462}
I0727 22:07:05.563199 139877795424064 train.py:379] starting iteration 97, 39731200 steps, 1399.6790566444397
I0727 22:07:19.703760 139877795424064 train.py:394] {'eval/walltime': 414.4003813266754, 'training/sps': 40693.890559319756, 'training/walltime': 990.0188729763031, 'training/entropy_loss': Array(-0.03903707, dtype=float32), 'training/policy_loss': Array(0.00153905, dtype=float32), 'training/total_loss': Array(2129.7822, dtype=float32), 'training/v_loss': Array(2129.8198, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5740553, dtype=float32), Array(0.29987174, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(249.54315, dtype=float32), Array(1512.1475, dtype=float32)), 'eval/episode_reward': (Array(-12366.669, dtype=float32), Array(4691.5903, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.56948864, dtype=float32), Array(0.30920637, dtype=float32)), 'eval/avg_episode_length': (Array(976.75, dtype=float32), Array(150.07878, dtype=float32)), 'eval/epoch_eval_time': 4.071410417556763, 'eval/sps': 31438.736671704126}
I0727 22:07:19.706539 139877795424064 train.py:379] starting iteration 98, 40140800 steps, 1413.8223960399628
I0727 22:07:33.833179 139877795424064 train.py:394] {'eval/walltime': 418.4830961227417, 'training/sps': 40796.90397109828, 'training/walltime': 1000.0588505268097, 'training/entropy_loss': Array(-0.03895475, dtype=float32), 'training/policy_loss': Array(0.00122039, dtype=float32), 'training/total_loss': Array(1744.974, dtype=float32), 'training/v_loss': Array(1745.0117, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5403125, dtype=float32), Array(0.27743158, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(404.9411, dtype=float32), Array(1936.3203, dtype=float32)), 'eval/episode_reward': (Array(-11986.032, dtype=float32), Array(3801.8223, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5355816, dtype=float32), Array(0.28675395, dtype=float32)), 'eval/avg_episode_length': (Array(961.14844, dtype=float32), Array(192.69783, dtype=float32)), 'eval/epoch_eval_time': 4.082714796066284, 'eval/sps': 31351.687882613947}
I0727 22:07:33.835935 139877795424064 train.py:379] starting iteration 99, 40550400 steps, 1427.95179271698
I0727 22:07:47.965183 139877795424064 train.py:394] {'eval/walltime': 422.5573923587799, 'training/sps': 40750.85947846236, 'training/walltime': 1010.1101722717285, 'training/entropy_loss': Array(-0.0360078, dtype=float32), 'training/policy_loss': Array(0.0013907, dtype=float32), 'training/total_loss': Array(1194.42, dtype=float32), 'training/v_loss': Array(1194.4546, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5615884, dtype=float32), Array(0.31026846, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(483.64667, dtype=float32), Array(2112.8455, dtype=float32)), 'eval/episode_reward': (Array(-12634.695, dtype=float32), Array(5388.3, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.55711937, dtype=float32), Array(0.31987962, dtype=float32)), 'eval/avg_episode_length': (Array(953.4375, dtype=float32), Array(209.96214, dtype=float32)), 'eval/epoch_eval_time': 4.074296236038208, 'eval/sps': 31416.468657287794}
I0727 22:07:47.968043 139877795424064 train.py:379] starting iteration 100, 40960000 steps, 1442.083901166916
I0727 22:08:02.107014 139877795424064 train.py:394] {'eval/walltime': 426.63179779052734, 'training/sps': 40722.34302880919, 'training/walltime': 1020.1685326099396, 'training/entropy_loss': Array(-0.03245047, dtype=float32), 'training/policy_loss': Array(0.00213689, dtype=float32), 'training/total_loss': Array(23558.742, dtype=float32), 'training/v_loss': Array(23558.771, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.56233203, dtype=float32), Array(0.28342232, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(639.92444, dtype=float32), Array(2419.1318, dtype=float32)), 'eval/episode_reward': (Array(-12696.281, dtype=float32), Array(4703.71, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5574423, dtype=float32), Array(0.2930317, dtype=float32)), 'eval/avg_episode_length': (Array(937.8594, dtype=float32), Array(240.67012, dtype=float32)), 'eval/epoch_eval_time': 4.0744054317474365, 'eval/sps': 31415.62668325405}
I0727 22:08:02.185468 139877795424064 train.py:379] starting iteration 101, 41369600 steps, 1456.3013157844543
I0727 22:08:16.376713 139877795424064 train.py:394] {'eval/walltime': 430.7349388599396, 'training/sps': 40618.8256260968, 'training/walltime': 1030.2525267601013, 'training/entropy_loss': Array(-0.04016612, dtype=float32), 'training/policy_loss': Array(0.00143151, dtype=float32), 'training/total_loss': Array(2880.882, dtype=float32), 'training/v_loss': Array(2880.921, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.51286554, dtype=float32), Array(0.27423513, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(716.7124, dtype=float32), Array(2555.6516, dtype=float32)), 'eval/episode_reward': (Array(-11775.801, dtype=float32), Array(4415.205, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5066612, dtype=float32), Array(0.28303748, dtype=float32)), 'eval/avg_episode_length': (Array(930.09375, dtype=float32), Array(254.19614, dtype=float32)), 'eval/epoch_eval_time': 4.1031410694122314, 'eval/sps': 31195.61278411902}
I0727 22:08:16.379527 139877795424064 train.py:379] starting iteration 102, 41779200 steps, 1470.4953844547272
I0727 22:08:30.522387 139877795424064 train.py:394] {'eval/walltime': 434.80816555023193, 'training/sps': 40693.01823527722, 'training/walltime': 1040.3181354999542, 'training/entropy_loss': Array(-0.04347175, dtype=float32), 'training/policy_loss': Array(0.00144812, dtype=float32), 'training/total_loss': Array(1710.3047, dtype=float32), 'training/v_loss': Array(1710.3467, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5262659, dtype=float32), Array(0.24354859, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(638.9066, dtype=float32), Array(2419.6921, dtype=float32)), 'eval/episode_reward': (Array(-11931.223, dtype=float32), Array(4215.3325, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5201807, dtype=float32), Array(0.2535667, dtype=float32)), 'eval/avg_episode_length': (Array(937.91406, dtype=float32), Array(240.4581, dtype=float32)), 'eval/epoch_eval_time': 4.073226690292358, 'eval/sps': 31424.717977288103}
I0727 22:08:30.525049 139877795424064 train.py:379] starting iteration 103, 42188800 steps, 1484.6409068107605
I0727 22:08:44.649340 139877795424064 train.py:394] {'eval/walltime': 438.88032484054565, 'training/sps': 40764.190564123746, 'training/walltime': 1050.366170167923, 'training/entropy_loss': Array(-0.04428088, dtype=float32), 'training/policy_loss': Array(0.00160058, dtype=float32), 'training/total_loss': Array(1981.4199, dtype=float32), 'training/v_loss': Array(1981.4626, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5179919, dtype=float32), Array(0.24865784, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(638.72076, dtype=float32), Array(2419.4985, dtype=float32)), 'eval/episode_reward': (Array(-11673.636, dtype=float32), Array(3892.5125, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.51146746, dtype=float32), Array(0.25884995, dtype=float32)), 'eval/avg_episode_length': (Array(937.8203, dtype=float32), Array(240.82123, dtype=float32)), 'eval/epoch_eval_time': 4.072159290313721, 'eval/sps': 31432.955067467126}
I0727 22:08:44.654939 139877795424064 train.py:379] starting iteration 104, 42598400 steps, 1498.7707810401917
I0727 22:08:58.813070 139877795424064 train.py:394] {'eval/walltime': 442.9506299495697, 'training/sps': 40619.61890042895, 'training/walltime': 1060.4499673843384, 'training/entropy_loss': Array(-0.04398628, dtype=float32), 'training/policy_loss': Array(0.00183588, dtype=float32), 'training/total_loss': Array(1581.8652, dtype=float32), 'training/v_loss': Array(1581.9073, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5383988, dtype=float32), Array(0.3167537, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(248.64044, dtype=float32), Array(1512.244, dtype=float32)), 'eval/episode_reward': (Array(-11917.021, dtype=float32), Array(4764.7007, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.53225684, dtype=float32), Array(0.32667786, dtype=float32)), 'eval/avg_episode_length': (Array(976.6797, dtype=float32), Array(150.53201, dtype=float32)), 'eval/epoch_eval_time': 4.070305109024048, 'eval/sps': 31447.273993347157}
I0727 22:08:58.815843 139877795424064 train.py:379] starting iteration 105, 43008000 steps, 1512.9316999912262
I0727 22:09:12.909611 139877795424064 train.py:394] {'eval/walltime': 447.0251455307007, 'training/sps': 40897.725683486424, 'training/walltime': 1070.4651942253113, 'training/entropy_loss': Array(-0.04585954, dtype=float32), 'training/policy_loss': Array(0.0028363, dtype=float32), 'training/total_loss': Array(22247.406, dtype=float32), 'training/v_loss': Array(22247.45, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.55454993, dtype=float32), Array(0.29406905, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(249.07536, dtype=float32), Array(1512.0411, dtype=float32)), 'eval/episode_reward': (Array(-12543.091, dtype=float32), Array(4972.8413, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5481814, dtype=float32), Array(0.30573532, dtype=float32)), 'eval/avg_episode_length': (Array(976.7031, dtype=float32), Array(150.38083, dtype=float32)), 'eval/epoch_eval_time': 4.0745155811309814, 'eval/sps': 31414.777401457493}
I0727 22:09:12.912507 139877795424064 train.py:379] starting iteration 106, 43417600 steps, 1527.0283637046814
I0727 22:09:27.062383 139877795424064 train.py:394] {'eval/walltime': 451.0967402458191, 'training/sps': 40657.31206262883, 'training/walltime': 1080.5396428108215, 'training/entropy_loss': Array(-0.04681976, dtype=float32), 'training/policy_loss': Array(0.00165378, dtype=float32), 'training/total_loss': Array(2814.4365, dtype=float32), 'training/v_loss': Array(2814.482, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5007472, dtype=float32), Array(0.26169243, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(247.61913, dtype=float32), Array(1512.4852, dtype=float32)), 'eval/episode_reward': (Array(-11518.377, dtype=float32), Array(4284.317, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.49327976, dtype=float32), Array(0.27139622, dtype=float32)), 'eval/avg_episode_length': (Array(976.71094, dtype=float32), Array(150.33037, dtype=float32)), 'eval/epoch_eval_time': 4.071594715118408, 'eval/sps': 31437.313621789974}
I0727 22:09:27.065186 139877795424064 train.py:379] starting iteration 107, 43827200 steps, 1541.1810438632965
I0727 22:09:41.203227 139877795424064 train.py:394] {'eval/walltime': 455.16667675971985, 'training/sps': 40697.97121911369, 'training/walltime': 1090.604026556015, 'training/entropy_loss': Array(-0.04857075, dtype=float32), 'training/policy_loss': Array(0.00165064, dtype=float32), 'training/total_loss': Array(1895.5374, dtype=float32), 'training/v_loss': Array(1895.5842, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5803764, dtype=float32), Array(0.2731332, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(327.86896, dtype=float32), Array(1739.023, dtype=float32)), 'eval/episode_reward': (Array(-12093.652, dtype=float32), Array(3714.3926, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.57774043, dtype=float32), Array(0.28155836, dtype=float32)), 'eval/avg_episode_length': (Array(968.91406, dtype=float32), Array(173.0794, dtype=float32)), 'eval/epoch_eval_time': 4.069936513900757, 'eval/sps': 31450.12202593812}
I0727 22:09:41.206059 139877795424064 train.py:379] starting iteration 108, 44236800 steps, 1555.3219158649445
I0727 22:09:55.355777 139877795424064 train.py:394] {'eval/walltime': 459.2409315109253, 'training/sps': 40669.599959831954, 'training/walltime': 1100.6754312515259, 'training/entropy_loss': Array(-0.05070079, dtype=float32), 'training/policy_loss': Array(0.00203751, dtype=float32), 'training/total_loss': Array(2056.908, dtype=float32), 'training/v_loss': Array(2056.9565, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.56502664, dtype=float32), Array(0.28301504, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(796.21857, dtype=float32), Array(2682.073, dtype=float32)), 'eval/episode_reward': (Array(-12827.506, dtype=float32), Array(4919.0557, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.56030107, dtype=float32), Array(0.29304996, dtype=float32)), 'eval/avg_episode_length': (Array(922.3672, dtype=float32), Array(266.67798, dtype=float32)), 'eval/epoch_eval_time': 4.074254751205444, 'eval/sps': 31416.788545715954}
I0727 22:09:55.358491 139877795424064 train.py:379] starting iteration 109, 44646400 steps, 1569.474348783493
I0727 22:10:09.506706 139877795424064 train.py:394] {'eval/walltime': 463.3186037540436, 'training/sps': 40690.086343931194, 'training/walltime': 1110.7417652606964, 'training/entropy_loss': Array(-0.05233181, dtype=float32), 'training/policy_loss': Array(0.00320197, dtype=float32), 'training/total_loss': Array(1337.1309, dtype=float32), 'training/v_loss': Array(1337.1799, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5615282, dtype=float32), Array(0.26609305, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(327.3911, dtype=float32), Array(1739.0146, dtype=float32)), 'eval/episode_reward': (Array(-12373.062, dtype=float32), Array(4681.1006, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.55715203, dtype=float32), Array(0.2758097, dtype=float32)), 'eval/avg_episode_length': (Array(968.9531, dtype=float32), Array(172.8618, dtype=float32)), 'eval/epoch_eval_time': 4.077672243118286, 'eval/sps': 31390.458175254313}
I0727 22:10:09.509547 139877795424064 train.py:379] starting iteration 110, 45056000 steps, 1583.625403881073
I0727 22:10:23.681268 139877795424064 train.py:394] {'eval/walltime': 467.4020531177521, 'training/sps': 40617.68570864121, 'training/walltime': 1120.8260424137115, 'training/entropy_loss': Array(-0.05219695, dtype=float32), 'training/policy_loss': Array(0.00506011, dtype=float32), 'training/total_loss': Array(22762.36, dtype=float32), 'training/v_loss': Array(22762.404, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.58115184, dtype=float32), Array(0.32729924, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(249.77385, dtype=float32), Array(1511.8207, dtype=float32)), 'eval/episode_reward': (Array(-12634.895, dtype=float32), Array(4857.1445, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5766291, dtype=float32), Array(0.33808458, dtype=float32)), 'eval/avg_episode_length': (Array(976.71094, dtype=float32), Array(150.33026, dtype=float32)), 'eval/epoch_eval_time': 4.083449363708496, 'eval/sps': 31346.04805868177}
I0727 22:10:23.684053 139877795424064 train.py:379] starting iteration 111, 45465600 steps, 1597.7999103069305
I0727 22:10:37.885059 139877795424064 train.py:394] {'eval/walltime': 471.508980512619, 'training/sps': 40594.499833438414, 'training/walltime': 1130.9160792827606, 'training/entropy_loss': Array(-0.05262448, dtype=float32), 'training/policy_loss': Array(0.00113206, dtype=float32), 'training/total_loss': Array(2904.4536, dtype=float32), 'training/v_loss': Array(2904.5054, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.517814, dtype=float32), Array(0.24828728, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(91.88246, dtype=float32), Array(879.9263, dtype=float32)), 'eval/episode_reward': (Array(-11965.838, dtype=float32), Array(4359.4526, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.51275873, dtype=float32), Array(0.25724313, dtype=float32)), 'eval/avg_episode_length': (Array(992.21094, dtype=float32), Array(87.778275, dtype=float32)), 'eval/epoch_eval_time': 4.106927394866943, 'eval/sps': 31166.852416232443}
I0727 22:10:37.887807 139877795424064 train.py:379] starting iteration 112, 45875200 steps, 1612.0036644935608
I0727 22:10:52.055629 139877795424064 train.py:394] {'eval/walltime': 475.5822398662567, 'training/sps': 40592.29472517543, 'training/walltime': 1141.006664276123, 'training/entropy_loss': Array(-0.05228207, dtype=float32), 'training/policy_loss': Array(0.00104905, dtype=float32), 'training/total_loss': Array(1650.5315, dtype=float32), 'training/v_loss': Array(1650.5828, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5399144, dtype=float32), Array(0.287892, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(170.5932, dtype=float32), Array(1239.4884, dtype=float32)), 'eval/episode_reward': (Array(-11978.2295, dtype=float32), Array(4189.4756, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5333761, dtype=float32), Array(0.2978969, dtype=float32)), 'eval/avg_episode_length': (Array(984.4531, dtype=float32), Array(123.39955, dtype=float32)), 'eval/epoch_eval_time': 4.073259353637695, 'eval/sps': 31424.465983411385}
I0727 22:10:52.058356 139877795424064 train.py:379] starting iteration 113, 46284800 steps, 1626.1742024421692
I0727 22:11:06.185209 139877795424064 train.py:394] {'eval/walltime': 479.65496253967285, 'training/sps': 40756.19105964626, 'training/walltime': 1151.0566711425781, 'training/entropy_loss': Array(-0.05181408, dtype=float32), 'training/policy_loss': Array(0.00119415, dtype=float32), 'training/total_loss': Array(1861.2399, dtype=float32), 'training/v_loss': Array(1861.2905, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5432557, dtype=float32), Array(0.2842857, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(326.9214, dtype=float32), Array(1738.9766, dtype=float32)), 'eval/episode_reward': (Array(-12016.131, dtype=float32), Array(4588.879, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.53869045, dtype=float32), Array(0.29325274, dtype=float32)), 'eval/avg_episode_length': (Array(968.96875, dtype=float32), Array(172.77527, dtype=float32)), 'eval/epoch_eval_time': 4.072722673416138, 'eval/sps': 31428.60692074463}
I0727 22:11:06.187966 139877795424064 train.py:379] starting iteration 114, 46694400 steps, 1640.3038234710693
I0727 22:11:20.347030 139877795424064 train.py:394] {'eval/walltime': 483.7295136451721, 'training/sps': 40633.73879553964, 'training/walltime': 1161.1369643211365, 'training/entropy_loss': Array(-0.05127942, dtype=float32), 'training/policy_loss': Array(0.00132046, dtype=float32), 'training/total_loss': Array(1508.0214, dtype=float32), 'training/v_loss': Array(1508.0713, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5890065, dtype=float32), Array(0.2637994, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(93.750465, dtype=float32), Array(880.05597, dtype=float32)), 'eval/episode_reward': (Array(-12253.496, dtype=float32), Array(3542.7092, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5853907, dtype=float32), Array(0.27264574, dtype=float32)), 'eval/avg_episode_length': (Array(992.2656, dtype=float32), Array(87.16198, dtype=float32)), 'eval/epoch_eval_time': 4.074551105499268, 'eval/sps': 31414.503508679332}
I0727 22:11:20.352717 139877795424064 train.py:379] starting iteration 115, 47104000 steps, 1654.4685597419739
I0727 22:11:34.552008 139877795424064 train.py:394] {'eval/walltime': 487.83150339126587, 'training/sps': 40583.255230838746, 'training/walltime': 1171.229796886444, 'training/entropy_loss': Array(-0.04604345, dtype=float32), 'training/policy_loss': Array(0.00087241, dtype=float32), 'training/total_loss': Array(21973.176, dtype=float32), 'training/v_loss': Array(21973.22, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5297383, dtype=float32), Array(0.27499518, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(482.86047, dtype=float32), Array(2112.85, dtype=float32)), 'eval/episode_reward': (Array(-11917.6045, dtype=float32), Array(4542.829, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5227694, dtype=float32), Array(0.28551945, dtype=float32)), 'eval/avg_episode_length': (Array(953.40625, dtype=float32), Array(210.10313, dtype=float32)), 'eval/epoch_eval_time': 4.10198974609375, 'eval/sps': 31204.368592557323}
I0727 22:11:34.554744 139877795424064 train.py:379] starting iteration 116, 47513600 steps, 1668.6706013679504
I0727 22:11:48.707568 139877795424064 train.py:394] {'eval/walltime': 491.9357109069824, 'training/sps': 40778.17305795024, 'training/walltime': 1181.2743861675262, 'training/entropy_loss': Array(-0.04958464, dtype=float32), 'training/policy_loss': Array(0.00100223, dtype=float32), 'training/total_loss': Array(2977.4883, dtype=float32), 'training/v_loss': Array(2977.537, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.49441838, dtype=float32), Array(0.23570669, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(325.7096, dtype=float32), Array(1739.2225, dtype=float32)), 'eval/episode_reward': (Array(-11520.488, dtype=float32), Array(3927.3037, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.48726836, dtype=float32), Array(0.24529795, dtype=float32)), 'eval/avg_episode_length': (Array(968.96875, dtype=float32), Array(172.7749, dtype=float32)), 'eval/epoch_eval_time': 4.104207515716553, 'eval/sps': 31187.506847507077}
I0727 22:11:48.710170 139877795424064 train.py:379] starting iteration 117, 47923200 steps, 1682.8260266780853
I0727 22:12:02.869046 139877795424064 train.py:394] {'eval/walltime': 496.04749941825867, 'training/sps': 40784.79367983769, 'training/walltime': 1191.317344903946, 'training/entropy_loss': Array(-0.04995629, dtype=float32), 'training/policy_loss': Array(0.00277045, dtype=float32), 'training/total_loss': Array(1953.6132, dtype=float32), 'training/v_loss': Array(1953.6605, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.54178894, dtype=float32), Array(0.24312037, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(561.2279, dtype=float32), Array(2272.7476, dtype=float32)), 'eval/episode_reward': (Array(-11808.613, dtype=float32), Array(3820.8562, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5366441, dtype=float32), Array(0.25124794, dtype=float32)), 'eval/avg_episode_length': (Array(945.7422, dtype=float32), Array(225.58301, dtype=float32)), 'eval/epoch_eval_time': 4.111788511276245, 'eval/sps': 31130.005750288572}
I0727 22:12:02.871614 139877795424064 train.py:379] starting iteration 118, 48332800 steps, 1696.987471818924
I0727 22:12:17.028911 139877795424064 train.py:394] {'eval/walltime': 500.1282594203949, 'training/sps': 40664.885849861166, 'training/walltime': 1201.3899171352386, 'training/entropy_loss': Array(-0.05350906, dtype=float32), 'training/policy_loss': Array(0.00240377, dtype=float32), 'training/total_loss': Array(1777.5641, dtype=float32), 'training/v_loss': Array(1777.6151, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5103266, dtype=float32), Array(0.267795, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(872.9602, dtype=float32), Array(2801.66, dtype=float32)), 'eval/episode_reward': (Array(-11416.556, dtype=float32), Array(4241.7515, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5029225, dtype=float32), Array(0.2773777, dtype=float32)), 'eval/avg_episode_length': (Array(914.6406, dtype=float32), Array(278.3866, dtype=float32)), 'eval/epoch_eval_time': 4.0807600021362305, 'eval/sps': 31366.706185365834}
I0727 22:12:17.031700 139877795424064 train.py:379] starting iteration 119, 48742400 steps, 1711.1475574970245
I0727 22:12:31.199728 139877795424064 train.py:394] {'eval/walltime': 504.2045819759369, 'training/sps': 40603.433980038935, 'training/walltime': 1211.4777338504791, 'training/entropy_loss': Array(-0.05291351, dtype=float32), 'training/policy_loss': Array(0.0035173, dtype=float32), 'training/total_loss': Array(1927.3892, dtype=float32), 'training/v_loss': Array(1927.4387, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.52452487, dtype=float32), Array(0.25340804, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(873.3722, dtype=float32), Array(2801.2222, dtype=float32)), 'eval/episode_reward': (Array(-11740.641, dtype=float32), Array(4022.3513, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5182088, dtype=float32), Array(0.26272926, dtype=float32)), 'eval/avg_episode_length': (Array(914.59375, dtype=float32), Array(278.53967, dtype=float32)), 'eval/epoch_eval_time': 4.076322555541992, 'eval/sps': 31400.851688239618}
I0727 22:12:31.202462 139877795424064 train.py:379] starting iteration 120, 49152000 steps, 1725.3183195590973
I0727 22:12:45.365295 139877795424064 train.py:394] {'eval/walltime': 508.28427267074585, 'training/sps': 40638.94267925742, 'training/walltime': 1221.5567362308502, 'training/entropy_loss': Array(-0.05087869, dtype=float32), 'training/policy_loss': Array(0.0074968, dtype=float32), 'training/total_loss': Array(23451.75, dtype=float32), 'training/v_loss': Array(23451.793, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5716095, dtype=float32), Array(0.27228066, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(249.52942, dtype=float32), Array(1512.0476, dtype=float32)), 'eval/episode_reward': (Array(-12187.483, dtype=float32), Array(4168.197, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5664285, dtype=float32), Array(0.28263515, dtype=float32)), 'eval/avg_episode_length': (Array(976.6875, dtype=float32), Array(150.48166, dtype=float32)), 'eval/epoch_eval_time': 4.07969069480896, 'eval/sps': 31374.927555872924}
I0727 22:12:45.368038 139877795424064 train.py:379] starting iteration 121, 49561600 steps, 1739.483895778656
I0727 22:12:59.559324 139877795424064 train.py:394] {'eval/walltime': 512.3798379898071, 'training/sps': 40588.73195299723, 'training/walltime': 1231.648206949234, 'training/entropy_loss': Array(-0.04733211, dtype=float32), 'training/policy_loss': Array(0.00131276, dtype=float32), 'training/total_loss': Array(2812.206, dtype=float32), 'training/v_loss': Array(2812.2522, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.51323235, dtype=float32), Array(0.26181212, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(482.37778, dtype=float32), Array(2113.0776, dtype=float32)), 'eval/episode_reward': (Array(-11338.633, dtype=float32), Array(4032.9802, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.50569844, dtype=float32), Array(0.2719591, dtype=float32)), 'eval/avg_episode_length': (Array(953.46875, dtype=float32), Array(209.82124, dtype=float32)), 'eval/epoch_eval_time': 4.095565319061279, 'eval/sps': 31253.316704356246}
I0727 22:12:59.562055 139877795424064 train.py:379] starting iteration 122, 49971200 steps, 1753.6779124736786
I0727 22:13:13.731232 139877795424064 train.py:394] {'eval/walltime': 516.4872238636017, 'training/sps': 40724.02169037646, 'training/walltime': 1241.706152677536, 'training/entropy_loss': Array(-0.04751163, dtype=float32), 'training/policy_loss': Array(0.00102654, dtype=float32), 'training/total_loss': Array(1872.5254, dtype=float32), 'training/v_loss': Array(1872.5719, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5527958, dtype=float32), Array(0.2619369, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(327.2295, dtype=float32), Array(1738.7562, dtype=float32)), 'eval/episode_reward': (Array(-11815.833, dtype=float32), Array(4272.4644, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5457304, dtype=float32), Array(0.27309385, dtype=float32)), 'eval/avg_episode_length': (Array(968.9375, dtype=float32), Array(172.94945, dtype=float32)), 'eval/epoch_eval_time': 4.107385873794556, 'eval/sps': 31163.373477191428}
I0727 22:13:13.734012 139877795424064 train.py:379] starting iteration 123, 50380800 steps, 1767.849869966507
I0727 22:13:27.916922 139877795424064 train.py:394] {'eval/walltime': 520.560355424881, 'training/sps': 40531.83930923783, 'training/walltime': 1251.8117883205414, 'training/entropy_loss': Array(-0.04671456, dtype=float32), 'training/policy_loss': Array(0.00115392, dtype=float32), 'training/total_loss': Array(1379.1914, dtype=float32), 'training/v_loss': Array(1379.2368, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5628673, dtype=float32), Array(0.30878663, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(327.424, dtype=float32), Array(1738.8318, dtype=float32)), 'eval/episode_reward': (Array(-11718.181, dtype=float32), Array(4330.111, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5572512, dtype=float32), Array(0.3197638, dtype=float32)), 'eval/avg_episode_length': (Array(968.9219, dtype=float32), Array(173.03575, dtype=float32)), 'eval/epoch_eval_time': 4.073131561279297, 'eval/sps': 31425.45190948792}
I0727 22:13:27.919674 139877795424064 train.py:379] starting iteration 124, 50790400 steps, 1782.035531282425
I0727 22:13:42.064323 139877795424064 train.py:394] {'eval/walltime': 524.6402115821838, 'training/sps': 40713.71637064078, 'training/walltime': 1261.872279882431, 'training/entropy_loss': Array(-0.04582337, dtype=float32), 'training/policy_loss': Array(0.00130751, dtype=float32), 'training/total_loss': Array(1359.3214, dtype=float32), 'training/v_loss': Array(1359.366, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5070233, dtype=float32), Array(0.23490933, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(482.2503, dtype=float32), Array(2112.9885, dtype=float32)), 'eval/episode_reward': (Array(-11006.594, dtype=float32), Array(3452.6997, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.49840757, dtype=float32), Array(0.24552417, dtype=float32)), 'eval/avg_episode_length': (Array(953.4375, dtype=float32), Array(209.96237, dtype=float32)), 'eval/epoch_eval_time': 4.0798561573028564, 'eval/sps': 31373.655115483103}
I0727 22:13:42.067242 139877795424064 train.py:379] starting iteration 125, 51200000 steps, 1796.183099269867
I0727 22:13:56.189342 139877795424064 train.py:394] {'eval/walltime': 528.7147512435913, 'training/sps': 40782.091540445566, 'training/walltime': 1271.915904045105, 'training/entropy_loss': Array(-0.04541216, dtype=float32), 'training/policy_loss': Array(0.0036357, dtype=float32), 'training/total_loss': Array(25856.176, dtype=float32), 'training/v_loss': Array(25856.215, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5353178, dtype=float32), Array(0.28640828, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(561.1023, dtype=float32), Array(2272.6511, dtype=float32)), 'eval/episode_reward': (Array(-11988.523, dtype=float32), Array(4288.7915, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.52720165, dtype=float32), Array(0.2986013, dtype=float32)), 'eval/avg_episode_length': (Array(945.72656, dtype=float32), Array(225.6484, dtype=float32)), 'eval/epoch_eval_time': 4.074539661407471, 'eval/sps': 31414.59174207299}
I0727 22:13:56.274892 139877795424064 train.py:379] starting iteration 126, 51609600 steps, 1810.3907470703125
I0727 22:14:10.468836 139877795424064 train.py:394] {'eval/walltime': 532.824786901474, 'training/sps': 40636.2444520046, 'training/walltime': 1281.9955756664276, 'training/entropy_loss': Array(-0.0532017, dtype=float32), 'training/policy_loss': Array(0.00193897, dtype=float32), 'training/total_loss': Array(3216.1787, dtype=float32), 'training/v_loss': Array(3216.23, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5541314, dtype=float32), Array(0.29134512, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(717.84595, dtype=float32), Array(2555.024, dtype=float32)), 'eval/episode_reward': (Array(-12430.33, dtype=float32), Array(4407.311, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.54801494, dtype=float32), Array(0.30222464, dtype=float32)), 'eval/avg_episode_length': (Array(930.10156, dtype=float32), Array(254.16751, dtype=float32)), 'eval/epoch_eval_time': 4.11003565788269, 'eval/sps': 31143.28211593667}
I0727 22:14:10.471855 139877795424064 train.py:379] starting iteration 127, 52019200 steps, 1824.5877120494843
I0727 22:14:24.650417 139877795424064 train.py:394] {'eval/walltime': 536.9041776657104, 'training/sps': 40575.193370728135, 'training/walltime': 1292.090413570404, 'training/entropy_loss': Array(-0.05151572, dtype=float32), 'training/policy_loss': Array(0.00254802, dtype=float32), 'training/total_loss': Array(1378.1816, dtype=float32), 'training/v_loss': Array(1378.2306, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.50346625, dtype=float32), Array(0.23444824, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(482.17288, dtype=float32), Array(2112.8325, dtype=float32)), 'eval/episode_reward': (Array(-11513.67, dtype=float32), Array(3989.3933, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.49350172, dtype=float32), Array(0.24574555, dtype=float32)), 'eval/avg_episode_length': (Array(953.375, dtype=float32), Array(210.24393, dtype=float32)), 'eval/epoch_eval_time': 4.07939076423645, 'eval/sps': 31377.234346403213}
I0727 22:14:24.653239 139877795424064 train.py:379] starting iteration 128, 52428800 steps, 1838.7690968513489
I0727 22:14:38.785552 139877795424064 train.py:394] {'eval/walltime': 540.9776041507721, 'training/sps': 40735.95875346068, 'training/walltime': 1302.1454119682312, 'training/entropy_loss': Array(-0.04924998, dtype=float32), 'training/policy_loss': Array(0.00257427, dtype=float32), 'training/total_loss': Array(1020.14154, dtype=float32), 'training/v_loss': Array(1020.18823, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.51875377, dtype=float32), Array(0.256089, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(248.11655, dtype=float32), Array(1511.8842, dtype=float32)), 'eval/episode_reward': (Array(-11439.559, dtype=float32), Array(3871.014, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5099978, dtype=float32), Array(0.26804942, dtype=float32)), 'eval/avg_episode_length': (Array(976.75, dtype=float32), Array(150.07848, dtype=float32)), 'eval/epoch_eval_time': 4.0734264850616455, 'eval/sps': 31423.176647328866}
I0727 22:14:38.788468 139877795424064 train.py:379] starting iteration 129, 52838400 steps, 1852.9043250083923
I0727 22:14:52.866985 139877795424064 train.py:394] {'eval/walltime': 545.052102804184, 'training/sps': 40960.76075631688, 'training/walltime': 1312.145226240158, 'training/entropy_loss': Array(-0.04745116, dtype=float32), 'training/policy_loss': Array(0.00190753, dtype=float32), 'training/total_loss': Array(1069.773, dtype=float32), 'training/v_loss': Array(1069.8184, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5042825, dtype=float32), Array(0.23409033, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(560.2455, dtype=float32), Array(2272.6316, dtype=float32)), 'eval/episode_reward': (Array(-11335.789, dtype=float32), Array(3703.2383, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.49479204, dtype=float32), Array(0.24450317, dtype=float32)), 'eval/avg_episode_length': (Array(945.64844, dtype=float32), Array(225.97298, dtype=float32)), 'eval/epoch_eval_time': 4.074498653411865, 'eval/sps': 31414.90791580372}
I0727 22:14:52.869854 139877795424064 train.py:379] starting iteration 130, 53248000 steps, 1866.9857113361359
I0727 22:15:06.991933 139877795424064 train.py:394] {'eval/walltime': 549.1156957149506, 'training/sps': 40738.051984948244, 'training/walltime': 1322.1997079849243, 'training/entropy_loss': Array(-0.04470452, dtype=float32), 'training/policy_loss': Array(0.00561935, dtype=float32), 'training/total_loss': Array(23379.984, dtype=float32), 'training/v_loss': Array(23380.023, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.51658475, dtype=float32), Array(0.22720776, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(794.94476, dtype=float32), Array(2682.5315, dtype=float32)), 'eval/episode_reward': (Array(-11481.225, dtype=float32), Array(3724.5513, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5091092, dtype=float32), Array(0.2366033, dtype=float32)), 'eval/avg_episode_length': (Array(922.3047, dtype=float32), Array(266.89252, dtype=float32)), 'eval/epoch_eval_time': 4.063592910766602, 'eval/sps': 31499.21825605623}
I0727 22:15:06.994819 139877795424064 train.py:379] starting iteration 131, 53657600 steps, 1881.1106762886047
I0727 22:15:21.116296 139877795424064 train.py:394] {'eval/walltime': 553.1914989948273, 'training/sps': 40789.95691379601, 'training/walltime': 1332.2413954734802, 'training/entropy_loss': Array(-0.04748761, dtype=float32), 'training/policy_loss': Array(0.00138795, dtype=float32), 'training/total_loss': Array(2301.098, dtype=float32), 'training/v_loss': Array(2301.144, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.52640975, dtype=float32), Array(0.25121734, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(92.14964, dtype=float32), Array(879.8283, dtype=float32)), 'eval/episode_reward': (Array(-11503.534, dtype=float32), Array(3794.639, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5192373, dtype=float32), Array(0.26182747, dtype=float32)), 'eval/avg_episode_length': (Array(992.22656, dtype=float32), Array(87.602196, dtype=float32)), 'eval/epoch_eval_time': 4.075803279876709, 'eval/sps': 31404.852297943078}
I0727 22:15:21.119074 139877795424064 train.py:379] starting iteration 132, 54067200 steps, 1895.2349317073822
I0727 22:15:35.258810 139877795424064 train.py:394] {'eval/walltime': 557.2954585552216, 'training/sps': 40830.52579898081, 'training/walltime': 1342.273105621338, 'training/entropy_loss': Array(-0.04775018, dtype=float32), 'training/policy_loss': Array(0.00111836, dtype=float32), 'training/total_loss': Array(1335.4944, dtype=float32), 'training/v_loss': Array(1335.541, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.512675, dtype=float32), Array(0.23136675, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(169.8949, dtype=float32), Array(1239.7817, dtype=float32)), 'eval/episode_reward': (Array(-11091.898, dtype=float32), Array(3882.4727, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5055475, dtype=float32), Array(0.2405519, dtype=float32)), 'eval/avg_episode_length': (Array(984.4453, dtype=float32), Array(123.46152, dtype=float32)), 'eval/epoch_eval_time': 4.103959560394287, 'eval/sps': 31189.391151725293}
I0727 22:15:35.261616 139877795424064 train.py:379] starting iteration 133, 54476800 steps, 1909.3774738311768
I0727 22:15:49.372584 139877795424064 train.py:394] {'eval/walltime': 561.3659949302673, 'training/sps': 40811.75334233701, 'training/walltime': 1352.3094301223755, 'training/entropy_loss': Array(-0.04648966, dtype=float32), 'training/policy_loss': Array(0.00110075, dtype=float32), 'training/total_loss': Array(824.04675, dtype=float32), 'training/v_loss': Array(824.09216, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.48605993, dtype=float32), Array(0.23439279, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(403.64984, dtype=float32), Array(1936.8959, dtype=float32)), 'eval/episode_reward': (Array(-10845.278, dtype=float32), Array(3817.962, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.47626615, dtype=float32), Array(0.2456591, dtype=float32)), 'eval/avg_episode_length': (Array(961.1953, dtype=float32), Array(192.46558, dtype=float32)), 'eval/epoch_eval_time': 4.070536375045776, 'eval/sps': 31445.487328082294}
I0727 22:15:49.375384 139877795424064 train.py:379] starting iteration 134, 54886400 steps, 1923.4912421703339
I0727 22:16:03.515812 139877795424064 train.py:394] {'eval/walltime': 565.4739005565643, 'training/sps': 40842.24186912258, 'training/walltime': 1362.3382625579834, 'training/entropy_loss': Array(-0.0467167, dtype=float32), 'training/policy_loss': Array(0.00090416, dtype=float32), 'training/total_loss': Array(861.0129, dtype=float32), 'training/v_loss': Array(861.05865, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.46042517, dtype=float32), Array(0.23830651, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(324.67468, dtype=float32), Array(1739.2899, dtype=float32)), 'eval/episode_reward': (Array(-10846.598, dtype=float32), Array(3685.0337, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4478678, dtype=float32), Array(0.25040895, dtype=float32)), 'eval/avg_episode_length': (Array(968.9531, dtype=float32), Array(172.86194, dtype=float32)), 'eval/epoch_eval_time': 4.107905626296997, 'eval/sps': 31159.43053331132}
I0727 22:16:03.518580 139877795424064 train.py:379] starting iteration 135, 55296000 steps, 1937.6344377994537
I0727 22:16:17.625478 139877795424064 train.py:394] {'eval/walltime': 569.5435163974762, 'training/sps': 40822.91541232671, 'training/walltime': 1372.3718428611755, 'training/entropy_loss': Array(-0.04074036, dtype=float32), 'training/policy_loss': Array(0.00138651, dtype=float32), 'training/total_loss': Array(24305.064, dtype=float32), 'training/v_loss': Array(24305.105, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4914332, dtype=float32), Array(0.2576361, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(481.8189, dtype=float32), Array(2112.8738, dtype=float32)), 'eval/episode_reward': (Array(-11003.202, dtype=float32), Array(4105.388, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.48141354, dtype=float32), Array(0.26891014, dtype=float32)), 'eval/avg_episode_length': (Array(953.4297, dtype=float32), Array(209.99753, dtype=float32)), 'eval/epoch_eval_time': 4.069615840911865, 'eval/sps': 31452.600197103486}
I0727 22:16:17.628174 139877795424064 train.py:379] starting iteration 136, 55705600 steps, 1951.7440314292908
I0727 22:16:31.774780 139877795424064 train.py:394] {'eval/walltime': 573.6100153923035, 'training/sps': 40650.84818593262, 'training/walltime': 1382.4478933811188, 'training/entropy_loss': Array(-0.04366619, dtype=float32), 'training/policy_loss': Array(0.00094219, dtype=float32), 'training/total_loss': Array(2428.6304, dtype=float32), 'training/v_loss': Array(2428.6729, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.49080908, dtype=float32), Array(0.2113513, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(247.4561, dtype=float32), Array(1512.4894, dtype=float32)), 'eval/episode_reward': (Array(-10866.773, dtype=float32), Array(3226.0676, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.48035872, dtype=float32), Array(0.22223069, dtype=float32)), 'eval/avg_episode_length': (Array(976.6953, dtype=float32), Array(150.43112, dtype=float32)), 'eval/epoch_eval_time': 4.0664989948272705, 'eval/sps': 31476.707645279268}
I0727 22:16:31.777526 139877795424064 train.py:379] starting iteration 137, 56115200 steps, 1965.8933827877045
I0727 22:16:45.902600 139877795424064 train.py:394] {'eval/walltime': 577.6779437065125, 'training/sps': 40743.28069054268, 'training/walltime': 1392.501084804535, 'training/entropy_loss': Array(-0.04247273, dtype=float32), 'training/policy_loss': Array(0.00102137, dtype=float32), 'training/total_loss': Array(1335.9867, dtype=float32), 'training/v_loss': Array(1336.0281, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4780447, dtype=float32), Array(0.21312934, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(247.11093, dtype=float32), Array(1512.5837, dtype=float32)), 'eval/episode_reward': (Array(-10580.881, dtype=float32), Array(3827.8857, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.46904886, dtype=float32), Array(0.22262931, dtype=float32)), 'eval/avg_episode_length': (Array(976.7031, dtype=float32), Array(150.38074, dtype=float32)), 'eval/epoch_eval_time': 4.067928314208984, 'eval/sps': 31465.64789573727}
I0727 22:16:45.905235 139877795424064 train.py:379] starting iteration 138, 56524800 steps, 1980.0210928916931
I0727 22:17:00.027313 139877795424064 train.py:394] {'eval/walltime': 581.7799859046936, 'training/sps': 40894.14608695147, 'training/walltime': 1402.5171883106232, 'training/entropy_loss': Array(-0.04100232, dtype=float32), 'training/policy_loss': Array(0.00114807, dtype=float32), 'training/total_loss': Array(668.7656, dtype=float32), 'training/v_loss': Array(668.80554, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.45617747, dtype=float32), Array(0.21922477, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(246.53409, dtype=float32), Array(1512.3407, dtype=float32)), 'eval/episode_reward': (Array(-10522.314, dtype=float32), Array(3865.6602, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.44316727, dtype=float32), Array(0.23072895, dtype=float32)), 'eval/avg_episode_length': (Array(976.71875, dtype=float32), Array(150.28009, dtype=float32)), 'eval/epoch_eval_time': 4.102042198181152, 'eval/sps': 31203.969587820247}
I0727 22:17:00.029916 139877795424064 train.py:379] starting iteration 139, 56934400 steps, 1994.1457736492157
I0727 22:17:14.169207 139877795424064 train.py:394] {'eval/walltime': 585.8512313365936, 'training/sps': 40698.15440045038, 'training/walltime': 1412.5815267562866, 'training/entropy_loss': Array(-0.03935328, dtype=float32), 'training/policy_loss': Array(0.00108172, dtype=float32), 'training/total_loss': Array(697.437, dtype=float32), 'training/v_loss': Array(697.4753, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.46397448, dtype=float32), Array(0.20989285, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(324.89093, dtype=float32), Array(1739.5513, dtype=float32)), 'eval/episode_reward': (Array(-10743.828, dtype=float32), Array(3491.539, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.45175007, dtype=float32), Array(0.22117616, dtype=float32)), 'eval/avg_episode_length': (Array(968.9531, dtype=float32), Array(172.86226, dtype=float32)), 'eval/epoch_eval_time': 4.071245431900024, 'eval/sps': 31440.010714427306}
I0727 22:17:14.171701 139877795424064 train.py:379] starting iteration 140, 57344000 steps, 2008.2875587940216
I0727 22:17:28.279061 139877795424064 train.py:394] {'eval/walltime': 589.9526915550232, 'training/sps': 40952.4457294319, 'training/walltime': 1422.5833714008331, 'training/entropy_loss': Array(-0.03097572, dtype=float32), 'training/policy_loss': Array(0.0021739, dtype=float32), 'training/total_loss': Array(22264.547, dtype=float32), 'training/v_loss': Array(22264.578, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.48252493, dtype=float32), Array(0.24387869, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(247.19496, dtype=float32), Array(1512.3607, dtype=float32)), 'eval/episode_reward': (Array(-10432.779, dtype=float32), Array(3605.9211, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.47128484, dtype=float32), Array(0.2553624, dtype=float32)), 'eval/avg_episode_length': (Array(976.6953, dtype=float32), Array(150.43118, dtype=float32)), 'eval/epoch_eval_time': 4.101460218429565, 'eval/sps': 31208.397298318974}
I0727 22:17:28.281650 139877795424064 train.py:379] starting iteration 141, 57753600 steps, 2022.3975076675415
I0727 22:17:42.434550 139877795424064 train.py:394] {'eval/walltime': 594.0225975513458, 'training/sps': 40639.12821357004, 'training/walltime': 1432.6623277664185, 'training/entropy_loss': Array(-0.0398621, dtype=float32), 'training/policy_loss': Array(0.00090713, dtype=float32), 'training/total_loss': Array(2228.853, dtype=float32), 'training/v_loss': Array(2228.892, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4798991, dtype=float32), Array(0.24060549, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(481.52582, dtype=float32), Array(2112.762, dtype=float32)), 'eval/episode_reward': (Array(-11004.298, dtype=float32), Array(4110.343, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.46993384, dtype=float32), Array(0.25023013, dtype=float32)), 'eval/avg_episode_length': (Array(953.4219, dtype=float32), Array(210.03256, dtype=float32)), 'eval/epoch_eval_time': 4.069905996322632, 'eval/sps': 31450.357849949003}
I0727 22:17:42.437129 139877795424064 train.py:379] starting iteration 142, 58163200 steps, 2036.5529868602753
I0727 22:17:56.551177 139877795424064 train.py:394] {'eval/walltime': 598.0910727977753, 'training/sps': 40790.73267385597, 'training/walltime': 1442.7038242816925, 'training/entropy_loss': Array(-0.040086, dtype=float32), 'training/policy_loss': Array(0.00095399, dtype=float32), 'training/total_loss': Array(1463.1548, dtype=float32), 'training/v_loss': Array(1463.1938, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.47921032, dtype=float32), Array(0.2287496, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(325.3296, dtype=float32), Array(1739.2963, dtype=float32)), 'eval/episode_reward': (Array(-10594.042, dtype=float32), Array(3344.8115, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.46921915, dtype=float32), Array(0.23862697, dtype=float32)), 'eval/avg_episode_length': (Array(968.9375, dtype=float32), Array(172.94867, dtype=float32)), 'eval/epoch_eval_time': 4.068475246429443, 'eval/sps': 31461.41791383265}
I0727 22:17:56.553750 139877795424064 train.py:379] starting iteration 143, 58572800 steps, 2050.6696078777313
I0727 22:18:10.694201 139877795424064 train.py:394] {'eval/walltime': 602.1951060295105, 'training/sps': 40826.05857334034, 'training/walltime': 1452.7366321086884, 'training/entropy_loss': Array(-0.04261795, dtype=float32), 'training/policy_loss': Array(0.00392782, dtype=float32), 'training/total_loss': Array(983.5394, dtype=float32), 'training/v_loss': Array(983.5781, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4258235, dtype=float32), Array(0.2119295, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(480.12445, dtype=float32), Array(2113.2373, dtype=float32)), 'eval/episode_reward': (Array(-9914.197, dtype=float32), Array(3440.8013, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4141228, dtype=float32), Array(0.22319338, dtype=float32)), 'eval/avg_episode_length': (Array(953.4219, dtype=float32), Array(210.03256, dtype=float32)), 'eval/epoch_eval_time': 4.1040332317352295, 'eval/sps': 31188.831272177642}
I0727 22:18:10.696707 139877795424064 train.py:379] starting iteration 144, 58982400 steps, 2064.8125653266907
I0727 22:18:24.818642 139877795424064 train.py:394] {'eval/walltime': 606.2635741233826, 'training/sps': 40758.95552364565, 'training/walltime': 1462.7859573364258, 'training/entropy_loss': Array(-0.04771944, dtype=float32), 'training/policy_loss': Array(0.00562909, dtype=float32), 'training/total_loss': Array(734.22534, dtype=float32), 'training/v_loss': Array(734.26733, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.49682957, dtype=float32), Array(0.24837476, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1106.9465, dtype=float32), Array(3119.7175, dtype=float32)), 'eval/episode_reward': (Array(-11485.736, dtype=float32), Array(3857.7676, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.48880413, dtype=float32), Array(0.2574467, dtype=float32)), 'eval/avg_episode_length': (Array(891.4297, dtype=float32), Array(309.8137, dtype=float32)), 'eval/epoch_eval_time': 4.06846809387207, 'eval/sps': 31461.4732244782}
I0727 22:18:24.821208 139877795424064 train.py:379] starting iteration 145, 59392000 steps, 2078.9370663166046
I0727 22:18:38.951376 139877795424064 train.py:394] {'eval/walltime': 610.3360068798065, 'training/sps': 40741.721213282515, 'training/walltime': 1472.8395335674286, 'training/entropy_loss': Array(-0.04987946, dtype=float32), 'training/policy_loss': Array(0.00141256, dtype=float32), 'training/total_loss': Array(32307.742, dtype=float32), 'training/v_loss': Array(32307.79, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.50107574, dtype=float32), Array(0.27323842, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1107.1233, dtype=float32), Array(3120.007, dtype=float32)), 'eval/episode_reward': (Array(-11173.449, dtype=float32), Array(4211.699, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.49249738, dtype=float32), Array(0.28363812, dtype=float32)), 'eval/avg_episode_length': (Array(891.28125, dtype=float32), Array(310.23773, dtype=float32)), 'eval/epoch_eval_time': 4.07243275642395, 'eval/sps': 31430.84432716288}
I0727 22:18:38.953897 139877795424064 train.py:379] starting iteration 146, 59801600 steps, 2093.069754600525
I0727 22:18:53.069000 139877795424064 train.py:394] {'eval/walltime': 614.3996231555939, 'training/sps': 40767.42142814633, 'training/walltime': 1482.8867719173431, 'training/entropy_loss': Array(-0.05134803, dtype=float32), 'training/policy_loss': Array(0.00221239, dtype=float32), 'training/total_loss': Array(2649.6846, dtype=float32), 'training/v_loss': Array(2649.734, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5017345, dtype=float32), Array(0.26120874, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(716.5261, dtype=float32), Array(2555.1646, dtype=float32)), 'eval/episode_reward': (Array(-11196.085, dtype=float32), Array(3896.0151, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4925214, dtype=float32), Array(0.27263203, dtype=float32)), 'eval/avg_episode_length': (Array(930.1328, dtype=float32), Array(254.05432, dtype=float32)), 'eval/epoch_eval_time': 4.0636162757873535, 'eval/sps': 31499.037141541896}
I0727 22:18:53.071664 139877795424064 train.py:379] starting iteration 147, 60211200 steps, 2107.187521457672
I0727 22:19:07.221805 139877795424064 train.py:394] {'eval/walltime': 618.5169134140015, 'training/sps': 40842.4098451022, 'training/walltime': 1492.9155631065369, 'training/entropy_loss': Array(-0.05092429, dtype=float32), 'training/policy_loss': Array(0.00243505, dtype=float32), 'training/total_loss': Array(2192.9038, dtype=float32), 'training/v_loss': Array(2192.9524, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5522083, dtype=float32), Array(0.27759424, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(639.62946, dtype=float32), Array(2419.4915, dtype=float32)), 'eval/episode_reward': (Array(-11903.959, dtype=float32), Array(4207.715, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5454429, dtype=float32), Array(0.28909516, dtype=float32)), 'eval/avg_episode_length': (Array(937.84375, dtype=float32), Array(240.73067, dtype=float32)), 'eval/epoch_eval_time': 4.117290258407593, 'eval/sps': 31088.4081438323}
I0727 22:19:07.227097 139877795424064 train.py:379] starting iteration 148, 60620800 steps, 2121.34295463562
I0727 22:19:21.356815 139877795424064 train.py:394] {'eval/walltime': 622.6125817298889, 'training/sps': 40836.70914374396, 'training/walltime': 1502.945754289627, 'training/entropy_loss': Array(-0.05029958, dtype=float32), 'training/policy_loss': Array(0.0022363, dtype=float32), 'training/total_loss': Array(991.3928, dtype=float32), 'training/v_loss': Array(991.44104, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.53966796, dtype=float32), Array(0.24379933, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(248.71558, dtype=float32), Array(1512.0577, dtype=float32)), 'eval/episode_reward': (Array(-11393.758, dtype=float32), Array(3778.091, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.53159577, dtype=float32), Array(0.2533924, dtype=float32)), 'eval/avg_episode_length': (Array(976.71094, dtype=float32), Array(150.33057, dtype=float32)), 'eval/epoch_eval_time': 4.095668315887451, 'eval/sps': 31252.530753888674}
I0727 22:19:21.359359 139877795424064 train.py:379] starting iteration 149, 61030400 steps, 2135.475216150284
I0727 22:19:35.458437 139877795424064 train.py:394] {'eval/walltime': 626.6862106323242, 'training/sps': 40871.11128874399, 'training/walltime': 1512.9675028324127, 'training/entropy_loss': Array(-0.04862513, dtype=float32), 'training/policy_loss': Array(0.00185586, dtype=float32), 'training/total_loss': Array(885.0205, dtype=float32), 'training/v_loss': Array(885.06726, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.48631603, dtype=float32), Array(0.24229388, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(716.122, dtype=float32), Array(2555.8098, dtype=float32)), 'eval/episode_reward': (Array(-11000.774, dtype=float32), Array(4037.4958, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4777929, dtype=float32), Array(0.25245637, dtype=float32)), 'eval/avg_episode_length': (Array(930.1172, dtype=float32), Array(254.11078, dtype=float32)), 'eval/epoch_eval_time': 4.073628902435303, 'eval/sps': 31421.615239296552}
I0727 22:19:35.460932 139877795424064 train.py:379] starting iteration 150, 61440000 steps, 2149.576790332794
I0727 22:19:49.608770 139877795424064 train.py:394] {'eval/walltime': 630.7834739685059, 'training/sps': 40769.40663212711, 'training/walltime': 1523.014251947403, 'training/entropy_loss': Array(-0.04601816, dtype=float32), 'training/policy_loss': Array(0.0039391, dtype=float32), 'training/total_loss': Array(24324.133, dtype=float32), 'training/v_loss': Array(24324.176, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4966879, dtype=float32), Array(0.25004035, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(169.52339, dtype=float32), Array(1239.63, dtype=float32)), 'eval/episode_reward': (Array(-11092.842, dtype=float32), Array(3744.868, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4878899, dtype=float32), Array(0.26156667, dtype=float32)), 'eval/avg_episode_length': (Array(984.46094, dtype=float32), Array(123.3375, dtype=float32)), 'eval/epoch_eval_time': 4.097263336181641, 'eval/sps': 31240.364481743793}
I0727 22:19:49.706937 139877795424064 train.py:379] starting iteration 151, 61849600 steps, 2163.8227910995483
I0727 22:20:03.790244 139877795424064 train.py:394] {'eval/walltime': 634.8535854816437, 'training/sps': 40922.0244989497, 'training/walltime': 1533.0235319137573, 'training/entropy_loss': Array(-0.04786106, dtype=float32), 'training/policy_loss': Array(0.00125242, dtype=float32), 'training/total_loss': Array(2367.874, dtype=float32), 'training/v_loss': Array(2367.921, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5343822, dtype=float32), Array(0.22157422, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(482.94492, dtype=float32), Array(2112.5981, dtype=float32)), 'eval/episode_reward': (Array(-11201.529, dtype=float32), Array(3283.4766, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5289454, dtype=float32), Array(0.22960816, dtype=float32)), 'eval/avg_episode_length': (Array(953.4375, dtype=float32), Array(209.96214, dtype=float32)), 'eval/epoch_eval_time': 4.070111513137817, 'eval/sps': 31448.76979090912}
I0727 22:20:03.796110 139877795424064 train.py:379] starting iteration 152, 62259200 steps, 2177.911952495575
I0727 22:20:17.920197 139877795424064 train.py:394] {'eval/walltime': 638.9278998374939, 'training/sps': 40774.62016419528, 'training/walltime': 1543.0689964294434, 'training/entropy_loss': Array(-0.04670262, dtype=float32), 'training/policy_loss': Array(0.00103267, dtype=float32), 'training/total_loss': Array(1516.856, dtype=float32), 'training/v_loss': Array(1516.9016, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4947282, dtype=float32), Array(0.2338386, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(325.63617, dtype=float32), Array(1739.4492, dtype=float32)), 'eval/episode_reward': (Array(-10997.229, dtype=float32), Array(3665.4976, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.48349202, dtype=float32), Array(0.24606171, dtype=float32)), 'eval/avg_episode_length': (Array(968.9375, dtype=float32), Array(172.94873, dtype=float32)), 'eval/epoch_eval_time': 4.07431435585022, 'eval/sps': 31416.328937949416}
I0727 22:20:17.922945 139877795424064 train.py:379] starting iteration 153, 62668800 steps, 2192.0388028621674
I0727 22:20:32.054500 139877795424064 train.py:394] {'eval/walltime': 643.0036759376526, 'training/sps': 40757.56695970031, 'training/walltime': 1553.1186640262604, 'training/entropy_loss': Array(-0.04531965, dtype=float32), 'training/policy_loss': Array(0.001019, dtype=float32), 'training/total_loss': Array(862.41943, dtype=float32), 'training/v_loss': Array(862.4637, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.48846275, dtype=float32), Array(0.24210548, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(169.21953, dtype=float32), Array(1239.7487, dtype=float32)), 'eval/episode_reward': (Array(-11154.458, dtype=float32), Array(4092.9902, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.47923884, dtype=float32), Array(0.2543164, dtype=float32)), 'eval/avg_episode_length': (Array(984.46094, dtype=float32), Array(123.3375, dtype=float32)), 'eval/epoch_eval_time': 4.075776100158691, 'eval/sps': 31405.06172432197}
I0727 22:20:32.060086 139877795424064 train.py:379] starting iteration 154, 63078400 steps, 2206.175927400589
I0727 22:20:46.181122 139877795424064 train.py:394] {'eval/walltime': 647.0750231742859, 'training/sps': 40774.731454933004, 'training/walltime': 1563.1641011238098, 'training/entropy_loss': Array(-0.04418563, dtype=float32), 'training/policy_loss': Array(0.00078988, dtype=float32), 'training/total_loss': Array(774.6937, dtype=float32), 'training/v_loss': Array(774.7372, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.46635845, dtype=float32), Array(0.21922483, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(481.09, dtype=float32), Array(2113.124, dtype=float32)), 'eval/episode_reward': (Array(-10684.714, dtype=float32), Array(3498.9807, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.45593795, dtype=float32), Array(0.22921179, dtype=float32)), 'eval/avg_episode_length': (Array(953.5, dtype=float32), Array(209.68053, dtype=float32)), 'eval/epoch_eval_time': 4.071347236633301, 'eval/sps': 31439.2245515876}
I0727 22:20:46.183629 139877795424064 train.py:379] starting iteration 155, 63488000 steps, 2220.2994861602783
I0727 22:21:00.262913 139877795424064 train.py:394] {'eval/walltime': 651.167943239212, 'training/sps': 41031.67598810903, 'training/walltime': 1573.1466326713562, 'training/entropy_loss': Array(-0.03967664, dtype=float32), 'training/policy_loss': Array(0.00144511, dtype=float32), 'training/total_loss': Array(22423.082, dtype=float32), 'training/v_loss': Array(22423.121, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5187549, dtype=float32), Array(0.22715318, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(482.52066, dtype=float32), Array(2112.886, dtype=float32)), 'eval/episode_reward': (Array(-11328.787, dtype=float32), Array(3794.849, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5120959, dtype=float32), Array(0.23640367, dtype=float32)), 'eval/avg_episode_length': (Array(953.4219, dtype=float32), Array(210.03275, dtype=float32)), 'eval/epoch_eval_time': 4.0929200649261475, 'eval/sps': 31273.515722157066}
I0727 22:21:00.268059 139877795424064 train.py:379] starting iteration 156, 63897600 steps, 2234.383901119232
I0727 22:21:14.403556 139877795424064 train.py:394] {'eval/walltime': 655.2380104064941, 'training/sps': 40710.28756800246, 'training/walltime': 1583.207971572876, 'training/entropy_loss': Array(-0.04395221, dtype=float32), 'training/policy_loss': Array(0.00073062, dtype=float32), 'training/total_loss': Array(2090.5925, dtype=float32), 'training/v_loss': Array(2090.6357, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5194198, dtype=float32), Array(0.23579055, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(13.770837, dtype=float32), Array(6.1656566, dtype=float32)), 'eval/episode_reward': (Array(-11217.274, dtype=float32), Array(3591.285, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5108958, dtype=float32), Array(0.24696136, dtype=float32)), 'eval/avg_episode_length': (Array(1000., dtype=float32), Array(0., dtype=float32)), 'eval/epoch_eval_time': 4.0700671672821045, 'eval/sps': 31449.112444371625}
I0727 22:21:14.406141 139877795424064 train.py:379] starting iteration 157, 64307200 steps, 2248.5219979286194
I0727 22:21:28.522457 139877795424064 train.py:394] {'eval/walltime': 659.303738117218, 'training/sps': 40769.96198207693, 'training/walltime': 1593.2545838356018, 'training/entropy_loss': Array(-0.04305125, dtype=float32), 'training/policy_loss': Array(0.00093198, dtype=float32), 'training/total_loss': Array(1369.917, dtype=float32), 'training/v_loss': Array(1369.9592, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.47014987, dtype=float32), Array(0.22927627, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(403.16895, dtype=float32), Array(1936.4563, dtype=float32)), 'eval/episode_reward': (Array(-10929.283, dtype=float32), Array(3808.6775, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.45924562, dtype=float32), Array(0.24135037, dtype=float32)), 'eval/avg_episode_length': (Array(961.125, dtype=float32), Array(192.81383, dtype=float32)), 'eval/epoch_eval_time': 4.065727710723877, 'eval/sps': 31482.678897158712}
I0727 22:21:28.525173 139877795424064 train.py:379] starting iteration 158, 64716800 steps, 2262.6410315036774
I0727 22:21:42.664058 139877795424064 train.py:394] {'eval/walltime': 663.3679485321045, 'training/sps': 40671.30604962349, 'training/walltime': 1603.3255660533905, 'training/entropy_loss': Array(-0.04122326, dtype=float32), 'training/policy_loss': Array(0.00128614, dtype=float32), 'training/total_loss': Array(1244.3933, dtype=float32), 'training/v_loss': Array(1244.4333, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.51426184, dtype=float32), Array(0.22877692, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(482.44727, dtype=float32), Array(2112.585, dtype=float32)), 'eval/episode_reward': (Array(-11274.98, dtype=float32), Array(3614.2092, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5049689, dtype=float32), Array(0.23900382, dtype=float32)), 'eval/avg_episode_length': (Array(953.4453, dtype=float32), Array(209.92691, dtype=float32)), 'eval/epoch_eval_time': 4.064210414886475, 'eval/sps': 31494.43235792097}
I0727 22:21:42.666733 139877795424064 train.py:379] starting iteration 159, 65126400 steps, 2276.7825903892517
I0727 22:21:56.775377 139877795424064 train.py:394] {'eval/walltime': 667.4410178661346, 'training/sps': 40833.149925019025, 'training/walltime': 1613.3566315174103, 'training/entropy_loss': Array(-0.03908681, dtype=float32), 'training/policy_loss': Array(0.00124729, dtype=float32), 'training/total_loss': Array(512.8318, dtype=float32), 'training/v_loss': Array(512.8696, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5056304, dtype=float32), Array(0.20522743, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(404.10065, dtype=float32), Array(1936.7762, dtype=float32)), 'eval/episode_reward': (Array(-10903.635, dtype=float32), Array(3666.5713, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.49688044, dtype=float32), Array(0.21455482, dtype=float32)), 'eval/avg_episode_length': (Array(961.2031, dtype=float32), Array(192.42653, dtype=float32)), 'eval/epoch_eval_time': 4.073069334030151, 'eval/sps': 31425.93201902329}
I0727 22:21:56.781111 139877795424064 train.py:379] starting iteration 160, 65536000 steps, 2290.896954059601
I0727 22:22:10.907192 139877795424064 train.py:394] {'eval/walltime': 671.5070552825928, 'training/sps': 40733.293985967866, 'training/walltime': 1623.4122877120972, 'training/entropy_loss': Array(-0.04595333, dtype=float32), 'training/policy_loss': Array(0.00284082, dtype=float32), 'training/total_loss': Array(28538.75, dtype=float32), 'training/v_loss': Array(28538.793, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.47202992, dtype=float32), Array(0.20377122, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1731.2888, dtype=float32), Array(3771.3716, dtype=float32)), 'eval/episode_reward': (Array(-10884.303, dtype=float32), Array(3434.3936, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4624899, dtype=float32), Array(0.21335346, dtype=float32)), 'eval/avg_episode_length': (Array(829.2031, dtype=float32), Array(374.9065, dtype=float32)), 'eval/epoch_eval_time': 4.06603741645813, 'eval/sps': 31480.280895078202}
I0727 22:22:10.909985 139877795424064 train.py:379] starting iteration 161, 65945600 steps, 2305.025842666626
I0727 22:22:25.044866 139877795424064 train.py:394] {'eval/walltime': 675.5750463008881, 'training/sps': 40703.87048659526, 'training/walltime': 1633.4752128124237, 'training/entropy_loss': Array(-0.04838423, dtype=float32), 'training/policy_loss': Array(0.00062188, dtype=float32), 'training/total_loss': Array(4059.8828, dtype=float32), 'training/v_loss': Array(4059.9307, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5047905, dtype=float32), Array(0.24436906, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1575.9199, dtype=float32), Array(3629.3843, dtype=float32)), 'eval/episode_reward': (Array(-11657.762, dtype=float32), Array(3919.1274, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.49592727, dtype=float32), Array(0.25499532, dtype=float32)), 'eval/avg_episode_length': (Array(844.6719, dtype=float32), Array(360.95062, dtype=float32)), 'eval/epoch_eval_time': 4.067991018295288, 'eval/sps': 31465.16288367791}
I0727 22:22:25.047667 139877795424064 train.py:379] starting iteration 162, 66355200 steps, 2319.163524389267
I0727 22:22:39.188524 139877795424064 train.py:394] {'eval/walltime': 679.6653544902802, 'training/sps': 40770.735046655434, 'training/walltime': 1643.5216345787048, 'training/entropy_loss': Array(-0.04633585, dtype=float32), 'training/policy_loss': Array(0.00140806, dtype=float32), 'training/total_loss': Array(2026.5864, dtype=float32), 'training/v_loss': Array(2026.6313, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4779824, dtype=float32), Array(0.20639002, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(872.09973, dtype=float32), Array(2801.845, dtype=float32)), 'eval/episode_reward': (Array(-10854.35, dtype=float32), Array(3409.1753, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.46680486, dtype=float32), Array(0.21685062, dtype=float32)), 'eval/avg_episode_length': (Array(914.59375, dtype=float32), Array(278.53983, dtype=float32)), 'eval/epoch_eval_time': 4.09030818939209, 'eval/sps': 31293.485496266145}
I0727 22:22:39.191350 139877795424064 train.py:379] starting iteration 163, 66764800 steps, 2333.3072078227997
I0727 22:22:53.312329 139877795424064 train.py:394] {'eval/walltime': 683.7362840175629, 'training/sps': 40772.84443330263, 'training/walltime': 1653.5675365924835, 'training/entropy_loss': Array(-0.04499432, dtype=float32), 'training/policy_loss': Array(0.00234835, dtype=float32), 'training/total_loss': Array(1478.1057, dtype=float32), 'training/v_loss': Array(1478.1483, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.437559, dtype=float32), Array(0.18245403, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(480.36383, dtype=float32), Array(2113.2026, dtype=float32)), 'eval/episode_reward': (Array(-10171.625, dtype=float32), Array(3145.0952, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4256205, dtype=float32), Array(0.19078664, dtype=float32)), 'eval/avg_episode_length': (Array(953.40625, dtype=float32), Array(210.10312, dtype=float32)), 'eval/epoch_eval_time': 4.070929527282715, 'eval/sps': 31442.450463012094}
I0727 22:22:53.315162 139877795424064 train.py:379] starting iteration 164, 67174400 steps, 2347.4310188293457
I0727 22:23:07.449080 139877795424064 train.py:394] {'eval/walltime': 687.8098609447479, 'training/sps': 40730.301246731295, 'training/walltime': 1663.623931646347, 'training/entropy_loss': Array(-0.0439277, dtype=float32), 'training/policy_loss': Array(0.00386092, dtype=float32), 'training/total_loss': Array(522.1399, dtype=float32), 'training/v_loss': Array(522.18, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4455112, dtype=float32), Array(0.19749051, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(324.4432, dtype=float32), Array(1739.3921, dtype=float32)), 'eval/episode_reward': (Array(-10188.433, dtype=float32), Array(3436.2979, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4332028, dtype=float32), Array(0.20855348, dtype=float32)), 'eval/avg_episode_length': (Array(968.9531, dtype=float32), Array(172.8619, dtype=float32)), 'eval/epoch_eval_time': 4.073576927185059, 'eval/sps': 31422.01615140508}
I0727 22:23:07.451893 139877795424064 train.py:379] starting iteration 165, 67584000 steps, 2361.567750930786
I0727 22:23:21.583859 139877795424064 train.py:394] {'eval/walltime': 691.877402305603, 'training/sps': 40714.375376649354, 'training/walltime': 1673.6842603683472, 'training/entropy_loss': Array(-0.04553865, dtype=float32), 'training/policy_loss': Array(0.00657073, dtype=float32), 'training/total_loss': Array(22868.223, dtype=float32), 'training/v_loss': Array(22868.264, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.49026442, dtype=float32), Array(0.23177898, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(403.6786, dtype=float32), Array(1936.6649, dtype=float32)), 'eval/episode_reward': (Array(-11003.018, dtype=float32), Array(3634.9495, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.47895753, dtype=float32), Array(0.24389274, dtype=float32)), 'eval/avg_episode_length': (Array(961.2031, dtype=float32), Array(192.42667, dtype=float32)), 'eval/epoch_eval_time': 4.0675413608551025, 'eval/sps': 31468.641285823604}
I0727 22:23:21.586649 139877795424064 train.py:379] starting iteration 166, 67993600 steps, 2375.702507019043
I0727 22:23:35.761684 139877795424064 train.py:394] {'eval/walltime': 695.9505004882812, 'training/sps': 40562.8580552417, 'training/walltime': 1683.7821681499481, 'training/entropy_loss': Array(-0.04701899, dtype=float32), 'training/policy_loss': Array(0.00299431, dtype=float32), 'training/total_loss': Array(2758.1895, dtype=float32), 'training/v_loss': Array(2758.234, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.47655866, dtype=float32), Array(0.23992136, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(168.91002, dtype=float32), Array(1239.7815, dtype=float32)), 'eval/episode_reward': (Array(-10827.15, dtype=float32), Array(3738.3772, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4642015, dtype=float32), Array(0.25426564, dtype=float32)), 'eval/avg_episode_length': (Array(984.4375, dtype=float32), Array(123.523506, dtype=float32)), 'eval/epoch_eval_time': 4.073098182678223, 'eval/sps': 31425.70943768288}
I0727 22:23:35.764519 139877795424064 train.py:379] starting iteration 167, 68403200 steps, 2389.880376815796
I0727 22:23:49.854329 139877795424064 train.py:394] {'eval/walltime': 700.0165584087372, 'training/sps': 40879.470161864185, 'training/walltime': 1693.8018674850464, 'training/entropy_loss': Array(-0.04494274, dtype=float32), 'training/policy_loss': Array(0.00316596, dtype=float32), 'training/total_loss': Array(1101.0519, dtype=float32), 'training/v_loss': Array(1101.0938, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.47013974, dtype=float32), Array(0.21154061, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(715.7246, dtype=float32), Array(2556.0798, dtype=float32)), 'eval/episode_reward': (Array(-10627.967, dtype=float32), Array(3520.7512, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4598589, dtype=float32), Array(0.22053353, dtype=float32)), 'eval/avg_episode_length': (Array(930.1328, dtype=float32), Array(254.05429, dtype=float32)), 'eval/epoch_eval_time': 4.066057920455933, 'eval/sps': 31480.12214878808}
I0727 22:23:49.857126 139877795424064 train.py:379] starting iteration 168, 68812800 steps, 2403.9729838371277
I0727 22:24:03.961118 139877795424064 train.py:394] {'eval/walltime': 704.087836265564, 'training/sps': 40842.00787020549, 'training/walltime': 1703.8307573795319, 'training/entropy_loss': Array(-0.04406575, dtype=float32), 'training/policy_loss': Array(0.00269174, dtype=float32), 'training/total_loss': Array(1297.539, dtype=float32), 'training/v_loss': Array(1297.5803, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.48850954, dtype=float32), Array(0.26351663, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(13.017429, dtype=float32), Array(6.8766775, dtype=float32)), 'eval/episode_reward': (Array(-10809.609, dtype=float32), Array(4209.971, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.47612837, dtype=float32), Array(0.2765135, dtype=float32)), 'eval/avg_episode_length': (Array(1000., dtype=float32), Array(0., dtype=float32)), 'eval/epoch_eval_time': 4.071277856826782, 'eval/sps': 31439.760316375265}
I0727 22:24:03.963785 139877795424064 train.py:379] starting iteration 169, 69222400 steps, 2418.0796422958374
I0727 22:24:18.092625 139877795424064 train.py:394] {'eval/walltime': 708.1757705211639, 'training/sps': 40809.78436995686, 'training/walltime': 1713.8675661087036, 'training/entropy_loss': Array(-0.04463473, dtype=float32), 'training/policy_loss': Array(0.00243002, dtype=float32), 'training/total_loss': Array(723.5907, dtype=float32), 'training/v_loss': Array(723.63293, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.50908196, dtype=float32), Array(0.23850954, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(326.0678, dtype=float32), Array(1739.3047, dtype=float32)), 'eval/episode_reward': (Array(-10781.84, dtype=float32), Array(3325.32, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.50068045, dtype=float32), Array(0.24949686, dtype=float32)), 'eval/avg_episode_length': (Array(968.9375, dtype=float32), Array(172.94885, dtype=float32)), 'eval/epoch_eval_time': 4.087934255599976, 'eval/sps': 31311.65816198132}
I0727 22:24:18.095333 139877795424064 train.py:379] starting iteration 170, 69632000 steps, 2432.2111909389496
I0727 22:24:32.263504 139877795424064 train.py:394] {'eval/walltime': 712.2530202865601, 'training/sps': 40607.081877004086, 'training/walltime': 1723.954476594925, 'training/entropy_loss': Array(-0.04550959, dtype=float32), 'training/policy_loss': Array(0.00650502, dtype=float32), 'training/total_loss': Array(21920.45, dtype=float32), 'training/v_loss': Array(21920.488, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5199542, dtype=float32), Array(0.23864464, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(326.35132, dtype=float32), Array(1738.9789, dtype=float32)), 'eval/episode_reward': (Array(-11281.877, dtype=float32), Array(3764.9414, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.51147544, dtype=float32), Array(0.24945001, dtype=float32)), 'eval/avg_episode_length': (Array(968.96875, dtype=float32), Array(172.77504, dtype=float32)), 'eval/epoch_eval_time': 4.077249765396118, 'eval/sps': 31393.710801419198}
I0727 22:24:32.266207 139877795424064 train.py:379] starting iteration 171, 70041600 steps, 2446.3820638656616
I0727 22:24:46.356336 139877795424064 train.py:394] {'eval/walltime': 716.3555998802185, 'training/sps': 41027.84657092475, 'training/walltime': 1733.9379398822784, 'training/entropy_loss': Array(-0.04722814, dtype=float32), 'training/policy_loss': Array(0.00232047, dtype=float32), 'training/total_loss': Array(2589.0728, dtype=float32), 'training/v_loss': Array(2589.1177, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4861668, dtype=float32), Array(0.2076711, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(403.55664, dtype=float32), Array(1936.8336, dtype=float32)), 'eval/episode_reward': (Array(-10733.658, dtype=float32), Array(3543.5833, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4760607, dtype=float32), Array(0.21796826, dtype=float32)), 'eval/avg_episode_length': (Array(961.1875, dtype=float32), Array(192.50394, dtype=float32)), 'eval/epoch_eval_time': 4.102579593658447, 'eval/sps': 31199.88219067235}
I0727 22:24:46.359069 139877795424064 train.py:379] starting iteration 172, 70451200 steps, 2460.47492647171
I0727 22:25:00.517121 139877795424064 train.py:394] {'eval/walltime': 720.4242775440216, 'training/sps': 40619.96656795246, 'training/walltime': 1744.0216507911682, 'training/entropy_loss': Array(-0.04450225, dtype=float32), 'training/policy_loss': Array(0.00210451, dtype=float32), 'training/total_loss': Array(1228.8293, dtype=float32), 'training/v_loss': Array(1228.8718, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.49900198, dtype=float32), Array(0.23527467, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(247.59329, dtype=float32), Array(1512.2439, dtype=float32)), 'eval/episode_reward': (Array(-10824.549, dtype=float32), Array(3565.3628, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.48972702, dtype=float32), Array(0.24728239, dtype=float32)), 'eval/avg_episode_length': (Array(976.7031, dtype=float32), Array(150.38083, dtype=float32)), 'eval/epoch_eval_time': 4.068677663803101, 'eval/sps': 31459.852703188833}
I0727 22:25:00.519713 139877795424064 train.py:379] starting iteration 173, 70860800 steps, 2474.63557100296
I0727 22:25:14.669811 139877795424064 train.py:394] {'eval/walltime': 724.492493391037, 'training/sps': 40642.640226574746, 'training/walltime': 1754.099736213684, 'training/entropy_loss': Array(-0.0425687, dtype=float32), 'training/policy_loss': Array(0.0019229, dtype=float32), 'training/total_loss': Array(1099.9474, dtype=float32), 'training/v_loss': Array(1099.988, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.469125, dtype=float32), Array(0.22588508, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(403.1095, dtype=float32), Array(1936.8076, dtype=float32)), 'eval/episode_reward': (Array(-10276.409, dtype=float32), Array(3582.3635, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.45595732, dtype=float32), Array(0.23780195, dtype=float32)), 'eval/avg_episode_length': (Array(961.2344, dtype=float32), Array(192.27159, dtype=float32)), 'eval/epoch_eval_time': 4.068215847015381, 'eval/sps': 31463.42397095433}
I0727 22:25:14.672384 139877795424064 train.py:379] starting iteration 174, 71270400 steps, 2488.788240671158
I0727 22:25:28.767421 139877795424064 train.py:394] {'eval/walltime': 728.5595817565918, 'training/sps': 40862.52451502557, 'training/walltime': 1764.123590707779, 'training/entropy_loss': Array(-0.04049006, dtype=float32), 'training/policy_loss': Array(0.0015702, dtype=float32), 'training/total_loss': Array(796.9852, dtype=float32), 'training/v_loss': Array(797.0242, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4309604, dtype=float32), Array(0.19367054, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(480.2469, dtype=float32), Array(2113.1675, dtype=float32)), 'eval/episode_reward': (Array(-10408.7295, dtype=float32), Array(3387.445, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4161994, dtype=float32), Array(0.20673141, dtype=float32)), 'eval/avg_episode_length': (Array(953.41406, dtype=float32), Array(210.06767, dtype=float32)), 'eval/epoch_eval_time': 4.06708836555481, 'eval/sps': 31472.14628628777}
I0727 22:25:28.770040 139877795424064 train.py:379] starting iteration 175, 71680000 steps, 2502.885897397995
I0727 22:25:42.914296 139877795424064 train.py:394] {'eval/walltime': 732.6302418708801, 'training/sps': 40676.682323914916, 'training/walltime': 1774.1932418346405, 'training/entropy_loss': Array(-0.03693859, dtype=float32), 'training/policy_loss': Array(0.00406663, dtype=float32), 'training/total_loss': Array(21420.855, dtype=float32), 'training/v_loss': Array(21420.889, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4522826, dtype=float32), Array(0.2214754, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(324.60248, dtype=float32), Array(1739.2019, dtype=float32)), 'eval/episode_reward': (Array(-10710.899, dtype=float32), Array(3392.9497, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.43804824, dtype=float32), Array(0.23576353, dtype=float32)), 'eval/avg_episode_length': (Array(968.9219, dtype=float32), Array(173.0357, dtype=float32)), 'eval/epoch_eval_time': 4.07066011428833, 'eval/sps': 31444.53145343925}
I0727 22:25:43.023365 139877795424064 train.py:379] starting iteration 176, 72089600 steps, 2517.1392188072205
I0727 22:25:57.382678 139877795424064 train.py:394] {'eval/walltime': 736.7187793254852, 'training/sps': 39898.18033185878, 'training/walltime': 1784.4593741893768, 'training/entropy_loss': Array(-0.03938684, dtype=float32), 'training/policy_loss': Array(0.00309271, dtype=float32), 'training/total_loss': Array(2183.3684, dtype=float32), 'training/v_loss': Array(2183.4048, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.47668076, dtype=float32), Array(0.20227128, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(325.23257, dtype=float32), Array(1739.5697, dtype=float32)), 'eval/episode_reward': (Array(-10750.123, dtype=float32), Array(3635.8755, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.46409458, dtype=float32), Array(0.21410836, dtype=float32)), 'eval/avg_episode_length': (Array(968.97656, dtype=float32), Array(172.73203, dtype=float32)), 'eval/epoch_eval_time': 4.0885374546051025, 'eval/sps': 31307.038622289707}
I0727 22:25:57.388528 139877795424064 train.py:379] starting iteration 177, 72499200 steps, 2531.504369735718
I0727 22:26:11.532767 139877795424064 train.py:394] {'eval/walltime': 740.8227982521057, 'training/sps': 40813.63330172668, 'training/walltime': 1794.4952363967896, 'training/entropy_loss': Array(-0.03816457, dtype=float32), 'training/policy_loss': Array(0.00358256, dtype=float32), 'training/total_loss': Array(1292.9539, dtype=float32), 'training/v_loss': Array(1292.9885, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.49870276, dtype=float32), Array(0.22967951, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(403.93896, dtype=float32), Array(1936.5999, dtype=float32)), 'eval/episode_reward': (Array(-11134.465, dtype=float32), Array(3695.3855, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.48728502, dtype=float32), Array(0.242009, dtype=float32)), 'eval/avg_episode_length': (Array(961.1719, dtype=float32), Array(192.58136, dtype=float32)), 'eval/epoch_eval_time': 4.104018926620483, 'eval/sps': 31188.939985080317}
I0727 22:26:11.535389 139877795424064 train.py:379] starting iteration 178, 72908800 steps, 2545.651246547699
I0727 22:26:25.687513 139877795424064 train.py:394] {'eval/walltime': 744.8939971923828, 'training/sps': 40646.98180967467, 'training/walltime': 1804.5722453594208, 'training/entropy_loss': Array(-0.03568429, dtype=float32), 'training/policy_loss': Array(0.00416514, dtype=float32), 'training/total_loss': Array(875.73157, dtype=float32), 'training/v_loss': Array(875.7632, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4149617, dtype=float32), Array(0.18484737, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(245.42703, dtype=float32), Array(1512.3276, dtype=float32)), 'eval/episode_reward': (Array(-9952.117, dtype=float32), Array(3469.1638, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3975792, dtype=float32), Array(0.19953331, dtype=float32)), 'eval/avg_episode_length': (Array(976.7422, dtype=float32), Array(150.12866, dtype=float32)), 'eval/epoch_eval_time': 4.0711989402771, 'eval/sps': 31440.369748005458}
I0727 22:26:25.690148 139877795424064 train.py:379] starting iteration 179, 73318400 steps, 2559.8060052394867
I0727 22:26:39.816389 139877795424064 train.py:394] {'eval/walltime': 748.9971022605896, 'training/sps': 40880.708479970395, 'training/walltime': 1814.5916411876678, 'training/entropy_loss': Array(-0.03302045, dtype=float32), 'training/policy_loss': Array(0.00500481, dtype=float32), 'training/total_loss': Array(711.358, dtype=float32), 'training/v_loss': Array(711.3859, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.41026354, dtype=float32), Array(0.1667827, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(401.5718, dtype=float32), Array(1937.2881, dtype=float32)), 'eval/episode_reward': (Array(-9712.762, dtype=float32), Array(3188.9668, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.39563707, dtype=float32), Array(0.17673755, dtype=float32)), 'eval/avg_episode_length': (Array(961.15625, dtype=float32), Array(192.6588, dtype=float32)), 'eval/epoch_eval_time': 4.103105068206787, 'eval/sps': 31195.88649869521}
I0727 22:26:39.819243 139877795424064 train.py:379] starting iteration 180, 73728000 steps, 2573.9351003170013
I0727 22:26:53.972973 139877795424064 train.py:394] {'eval/walltime': 753.0639669895172, 'training/sps': 40623.5569193215, 'training/walltime': 1824.674460887909, 'training/entropy_loss': Array(-0.03450499, dtype=float32), 'training/policy_loss': Array(0.0098711, dtype=float32), 'training/total_loss': Array(25011.898, dtype=float32), 'training/v_loss': Array(25011.924, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4278834, dtype=float32), Array(0.1919725, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(792.64813, dtype=float32), Array(2683.29, dtype=float32)), 'eval/episode_reward': (Array(-10495.4, dtype=float32), Array(3343.809, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.41331372, dtype=float32), Array(0.20341669, dtype=float32)), 'eval/avg_episode_length': (Array(922.40625, dtype=float32), Array(266.54376, dtype=float32)), 'eval/epoch_eval_time': 4.066864728927612, 'eval/sps': 31473.876937567136}
I0727 22:26:53.975830 139877795424064 train.py:379] starting iteration 181, 74137600 steps, 2588.091687440872
I0727 22:27:08.130715 139877795424064 train.py:394] {'eval/walltime': 757.1344287395477, 'training/sps': 40632.303012007105, 'training/walltime': 1834.7551102638245, 'training/entropy_loss': Array(-0.04284232, dtype=float32), 'training/policy_loss': Array(0.00767692, dtype=float32), 'training/total_loss': Array(3289.7412, dtype=float32), 'training/v_loss': Array(3289.7764, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.45436072, dtype=float32), Array(0.211335, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(949.61395, dtype=float32), Array(2913.9753, dtype=float32)), 'eval/episode_reward': (Array(-10291.349, dtype=float32), Array(3510.5168, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.44370303, dtype=float32), Array(0.22075331, dtype=float32)), 'eval/avg_episode_length': (Array(906.8281, dtype=float32), Array(289.68372, dtype=float32)), 'eval/epoch_eval_time': 4.070461750030518, 'eval/sps': 31446.063827780803}
I0727 22:27:08.133557 139877795424064 train.py:379] starting iteration 182, 74547200 steps, 2602.2494144439697
I0727 22:27:22.258323 139877795424064 train.py:394] {'eval/walltime': 761.2120208740234, 'training/sps': 40784.262130757474, 'training/walltime': 1844.798199892044, 'training/entropy_loss': Array(-0.04247552, dtype=float32), 'training/policy_loss': Array(0.01394097, dtype=float32), 'training/total_loss': Array(1624.0874, dtype=float32), 'training/v_loss': Array(1624.116, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.48006344, dtype=float32), Array(0.19840263, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(794.09735, dtype=float32), Array(2682.6042, dtype=float32)), 'eval/episode_reward': (Array(-10851.8125, dtype=float32), Array(3540.4553, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.46946332, dtype=float32), Array(0.20893076, dtype=float32)), 'eval/avg_episode_length': (Array(922.3828, dtype=float32), Array(266.62433, dtype=float32)), 'eval/epoch_eval_time': 4.077592134475708, 'eval/sps': 31391.074874254948}
I0727 22:27:22.261190 139877795424064 train.py:379] starting iteration 183, 74956800 steps, 2616.377047777176
I0727 22:27:36.369030 139877795424064 train.py:394] {'eval/walltime': 765.2881088256836, 'training/sps': 40846.19792468351, 'training/walltime': 1854.8260610103607, 'training/entropy_loss': Array(-0.0417155, dtype=float32), 'training/policy_loss': Array(0.01939564, dtype=float32), 'training/total_loss': Array(934.08716, dtype=float32), 'training/v_loss': Array(934.1096, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5070179, dtype=float32), Array(0.21934892, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(638.4796, dtype=float32), Array(2419.6628, dtype=float32)), 'eval/episode_reward': (Array(-10715.295, dtype=float32), Array(3603.5454, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.49828845, dtype=float32), Array(0.22864147, dtype=float32)), 'eval/avg_episode_length': (Array(937.90625, dtype=float32), Array(240.48856, dtype=float32)), 'eval/epoch_eval_time': 4.076087951660156, 'eval/sps': 31402.659000001873}
I0727 22:27:36.371850 139877795424064 train.py:379] starting iteration 184, 75366400 steps, 2630.48770737648
I0727 22:27:50.521946 139877795424064 train.py:394] {'eval/walltime': 769.3569648265839, 'training/sps': 40645.49316240835, 'training/walltime': 1864.9034390449524, 'training/entropy_loss': Array(-0.04325114, dtype=float32), 'training/policy_loss': Array(0.01888051, dtype=float32), 'training/total_loss': Array(1012.266, dtype=float32), 'training/v_loss': Array(1012.29047, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4754236, dtype=float32), Array(0.22603704, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(481.379, dtype=float32), Array(2113.508, dtype=float32)), 'eval/episode_reward': (Array(-10756.871, dtype=float32), Array(3518.206, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.46691227, dtype=float32), Array(0.23475094, dtype=float32)), 'eval/avg_episode_length': (Array(953.46094, dtype=float32), Array(209.85649, dtype=float32)), 'eval/epoch_eval_time': 4.0688560009002686, 'eval/sps': 31458.47382450471}
I0727 22:27:50.524827 139877795424064 train.py:379] starting iteration 185, 75776000 steps, 2644.640684604645
I0727 22:28:04.656221 139877795424064 train.py:394] {'eval/walltime': 773.4226784706116, 'training/sps': 40709.50714747567, 'training/walltime': 1874.9649708271027, 'training/entropy_loss': Array(-0.04409427, dtype=float32), 'training/policy_loss': Array(0.01299423, dtype=float32), 'training/total_loss': Array(20486.695, dtype=float32), 'training/v_loss': Array(20486.727, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.49642378, dtype=float32), Array(0.22637439, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(716.36755, dtype=float32), Array(2556.0613, dtype=float32)), 'eval/episode_reward': (Array(-11508.789, dtype=float32), Array(3467.3557, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.487999, dtype=float32), Array(0.23579541, dtype=float32)), 'eval/avg_episode_length': (Array(930.08594, dtype=float32), Array(254.2246, dtype=float32)), 'eval/epoch_eval_time': 4.06571364402771, 'eval/sps': 31482.787822016027}
I0727 22:28:04.659112 139877795424064 train.py:379] starting iteration 186, 76185600 steps, 2658.7749693393707
I0727 22:28:18.829198 139877795424064 train.py:394] {'eval/walltime': 777.4975905418396, 'training/sps': 40590.312346485705, 'training/walltime': 1885.056048631668, 'training/entropy_loss': Array(-0.04634765, dtype=float32), 'training/policy_loss': Array(0.07090555, dtype=float32), 'training/total_loss': Array(3166.4368, dtype=float32), 'training/v_loss': Array(3166.412, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.51773775, dtype=float32), Array(0.25997928, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(638.8441, dtype=float32), Array(2419.374, dtype=float32)), 'eval/episode_reward': (Array(-11362.451, dtype=float32), Array(4117.4043, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.51120865, dtype=float32), Array(0.26907867, dtype=float32)), 'eval/avg_episode_length': (Array(937.8828, dtype=float32), Array(240.57896, dtype=float32)), 'eval/epoch_eval_time': 4.074912071228027, 'eval/sps': 31411.72073473123}
I0727 22:28:18.832023 139877795424064 train.py:379] starting iteration 187, 76595200 steps, 2672.947880268097
I0727 22:28:32.973972 139877795424064 train.py:394] {'eval/walltime': 781.5735838413239, 'training/sps': 40707.93385663883, 'training/walltime': 1895.1179692745209, 'training/entropy_loss': Array(-0.04761188, dtype=float32), 'training/policy_loss': Array(0.01028823, dtype=float32), 'training/total_loss': Array(1537.812, dtype=float32), 'training/v_loss': Array(1537.8494, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.52149606, dtype=float32), Array(0.2624428, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(482.59695, dtype=float32), Array(2112.6418, dtype=float32)), 'eval/episode_reward': (Array(-11905.773, dtype=float32), Array(4333.1953, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.51501167, dtype=float32), Array(0.27196658, dtype=float32)), 'eval/avg_episode_length': (Array(953.3906, dtype=float32), Array(210.17383, dtype=float32)), 'eval/epoch_eval_time': 4.075993299484253, 'eval/sps': 31403.388228385018}
I0727 22:28:32.976684 139877795424064 train.py:379] starting iteration 188, 77004800 steps, 2687.092541217804
I0727 22:28:47.122614 139877795424064 train.py:394] {'eval/walltime': 785.6610124111176, 'training/sps': 40737.83463433768, 'training/walltime': 1905.1725046634674, 'training/entropy_loss': Array(-0.04766607, dtype=float32), 'training/policy_loss': Array(0.00785178, dtype=float32), 'training/total_loss': Array(1141.3181, dtype=float32), 'training/v_loss': Array(1141.3579, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.50823355, dtype=float32), Array(0.25495017, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(169.76169, dtype=float32), Array(1239.4464, dtype=float32)), 'eval/episode_reward': (Array(-11123.719, dtype=float32), Array(3829.0776, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5003507, dtype=float32), Array(0.2662197, dtype=float32)), 'eval/avg_episode_length': (Array(984.46875, dtype=float32), Array(123.275734, dtype=float32)), 'eval/epoch_eval_time': 4.087428569793701, 'eval/sps': 31315.53195716405}
I0727 22:28:47.125483 139877795424064 train.py:379] starting iteration 189, 77414400 steps, 2701.2413408756256
I0727 22:29:01.263592 139877795424064 train.py:394] {'eval/walltime': 789.7426598072052, 'training/sps': 40746.58652003476, 'training/walltime': 1915.2248804569244, 'training/entropy_loss': Array(-0.04719239, dtype=float32), 'training/policy_loss': Array(0.00377569, dtype=float32), 'training/total_loss': Array(1176.8109, dtype=float32), 'training/v_loss': Array(1176.8542, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5027279, dtype=float32), Array(0.22170648, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(247.74445, dtype=float32), Array(1512.2175, dtype=float32)), 'eval/episode_reward': (Array(-11297.246, dtype=float32), Array(3852.5552, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.49541253, dtype=float32), Array(0.23031957, dtype=float32)), 'eval/avg_episode_length': (Array(976.7344, dtype=float32), Array(150.1792, dtype=float32)), 'eval/epoch_eval_time': 4.0816473960876465, 'eval/sps': 31359.886726788543}
I0727 22:29:01.266477 139877795424064 train.py:379] starting iteration 190, 77824000 steps, 2715.382334470749
I0727 22:29:15.444248 139877795424064 train.py:394] {'eval/walltime': 793.8179388046265, 'training/sps': 40560.86131337919, 'training/walltime': 1925.3232853412628, 'training/entropy_loss': Array(-0.04273593, dtype=float32), 'training/policy_loss': Array(0.00387566, dtype=float32), 'training/total_loss': Array(19331.078, dtype=float32), 'training/v_loss': Array(19331.117, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5493832, dtype=float32), Array(0.26230755, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(92.72241, dtype=float32), Array(880.0039, dtype=float32)), 'eval/episode_reward': (Array(-11488.776, dtype=float32), Array(4176.0503, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.54319644, dtype=float32), Array(0.27381912, dtype=float32)), 'eval/avg_episode_length': (Array(992.2656, dtype=float32), Array(87.16198, dtype=float32)), 'eval/epoch_eval_time': 4.075278997421265, 'eval/sps': 31408.892515333362}
I0727 22:29:15.447143 139877795424064 train.py:379] starting iteration 191, 78233600 steps, 2729.5630004405975
I0727 22:29:29.588717 139877795424064 train.py:394] {'eval/walltime': 797.8836233615875, 'training/sps': 40668.304120462046, 'training/walltime': 1935.3950109481812, 'training/entropy_loss': Array(-0.04667463, dtype=float32), 'training/policy_loss': Array(0.0025282, dtype=float32), 'training/total_loss': Array(3322.0063, dtype=float32), 'training/v_loss': Array(3322.0503, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5200465, dtype=float32), Array(0.2228161, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(326.3109, dtype=float32), Array(1739.4551, dtype=float32)), 'eval/episode_reward': (Array(-11534.072, dtype=float32), Array(3814.7322, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.51374817, dtype=float32), Array(0.2315137, dtype=float32)), 'eval/avg_episode_length': (Array(968.9219, dtype=float32), Array(173.03593, dtype=float32)), 'eval/epoch_eval_time': 4.06568455696106, 'eval/sps': 31483.013058857425}
I0727 22:29:29.591582 139877795424064 train.py:379] starting iteration 192, 78643200 steps, 2743.707439661026
I0727 22:29:43.717719 139877795424064 train.py:394] {'eval/walltime': 801.9639148712158, 'training/sps': 40788.54590029613, 'training/walltime': 1945.4370458126068, 'training/entropy_loss': Array(-0.0447, dtype=float32), 'training/policy_loss': Array(0.00233049, dtype=float32), 'training/total_loss': Array(1624.5715, dtype=float32), 'training/v_loss': Array(1624.6138, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.49678397, dtype=float32), Array(0.24113032, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(481.9741, dtype=float32), Array(2112.5295, dtype=float32)), 'eval/episode_reward': (Array(-10992.305, dtype=float32), Array(3431.4707, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.49014145, dtype=float32), Array(0.24976636, dtype=float32)), 'eval/avg_episode_length': (Array(953.41406, dtype=float32), Array(210.06772, dtype=float32)), 'eval/epoch_eval_time': 4.080291509628296, 'eval/sps': 31370.307660116294}
I0727 22:29:43.720502 139877795424064 train.py:379] starting iteration 193, 79052800 steps, 2757.8363597393036
I0727 22:29:57.868124 139877795424064 train.py:394] {'eval/walltime': 806.0755758285522, 'training/sps': 40829.82906478541, 'training/walltime': 1955.4689271450043, 'training/entropy_loss': Array(-0.04246009, dtype=float32), 'training/policy_loss': Array(0.00163372, dtype=float32), 'training/total_loss': Array(1007.8893, dtype=float32), 'training/v_loss': Array(1007.9302, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.47427303, dtype=float32), Array(0.22078016, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(481.39758, dtype=float32), Array(2112.8306, dtype=float32)), 'eval/episode_reward': (Array(-10453.52, dtype=float32), Array(3377.8447, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4648458, dtype=float32), Array(0.23077197, dtype=float32)), 'eval/avg_episode_length': (Array(953.40625, dtype=float32), Array(210.10338, dtype=float32)), 'eval/epoch_eval_time': 4.111660957336426, 'eval/sps': 31130.971480420812}
I0727 22:29:57.871000 139877795424064 train.py:379] starting iteration 194, 79462400 steps, 2771.9868578910828
I0727 22:30:11.980679 139877795424064 train.py:394] {'eval/walltime': 810.1500835418701, 'training/sps': 40833.25280073563, 'training/walltime': 1965.4999673366547, 'training/entropy_loss': Array(-0.0415087, dtype=float32), 'training/policy_loss': Array(0.00138217, dtype=float32), 'training/total_loss': Array(529.8262, dtype=float32), 'training/v_loss': Array(529.86633, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.42178535, dtype=float32), Array(0.20632234, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(167.51965, dtype=float32), Array(1240.1594, dtype=float32)), 'eval/episode_reward': (Array(-9751.416, dtype=float32), Array(3201.084, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.40704852, dtype=float32), Array(0.22030598, dtype=float32)), 'eval/avg_episode_length': (Array(984.46875, dtype=float32), Array(123.27554, dtype=float32)), 'eval/epoch_eval_time': 4.074507713317871, 'eval/sps': 31414.83806292015}
I0727 22:30:11.983629 139877795424064 train.py:379] starting iteration 195, 79872000 steps, 2786.0994865894318
I0727 22:30:26.136478 139877795424064 train.py:394] {'eval/walltime': 814.221673488617, 'training/sps': 40646.752927872025, 'training/walltime': 1975.5770330429077, 'training/entropy_loss': Array(-0.03046417, dtype=float32), 'training/policy_loss': Array(0.00184294, dtype=float32), 'training/total_loss': Array(20319.707, dtype=float32), 'training/v_loss': Array(20319.736, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.503265, dtype=float32), Array(0.27070466, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(325.87408, dtype=float32), Array(1739.3066, dtype=float32)), 'eval/episode_reward': (Array(-11143.601, dtype=float32), Array(4354.007, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.49673688, dtype=float32), Array(0.27985686, dtype=float32)), 'eval/avg_episode_length': (Array(968.96094, dtype=float32), Array(172.81825, dtype=float32)), 'eval/epoch_eval_time': 4.071589946746826, 'eval/sps': 31437.350439051745}
I0727 22:30:26.139346 139877795424064 train.py:379] starting iteration 196, 80281600 steps, 2800.255203485489
I0727 22:30:40.272639 139877795424064 train.py:394] {'eval/walltime': 818.2972664833069, 'training/sps': 40740.04882108357, 'training/walltime': 1985.631021976471, 'training/entropy_loss': Array(-0.04192115, dtype=float32), 'training/policy_loss': Array(0.00102013, dtype=float32), 'training/total_loss': Array(2861.055, dtype=float32), 'training/v_loss': Array(2861.0957, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.52099335, dtype=float32), Array(0.23908204, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(248.23874, dtype=float32), Array(1512.1716, dtype=float32)), 'eval/episode_reward': (Array(-11432.818, dtype=float32), Array(3970.892, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.51404107, dtype=float32), Array(0.24918509, dtype=float32)), 'eval/avg_episode_length': (Array(976.7422, dtype=float32), Array(150.12854, dtype=float32)), 'eval/epoch_eval_time': 4.075592994689941, 'eval/sps': 31406.472669564948}
I0727 22:30:40.275496 139877795424064 train.py:379] starting iteration 197, 80691200 steps, 2814.3913536071777
I0727 22:30:54.393966 139877795424064 train.py:394] {'eval/walltime': 822.3719296455383, 'training/sps': 40798.129541127026, 'training/walltime': 1995.670697927475, 'training/entropy_loss': Array(-0.04140031, dtype=float32), 'training/policy_loss': Array(0.00090298, dtype=float32), 'training/total_loss': Array(1369.1714, dtype=float32), 'training/v_loss': Array(1369.2119, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.48650652, dtype=float32), Array(0.24563073, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(481.7375, dtype=float32), Array(2113.0996, dtype=float32)), 'eval/episode_reward': (Array(-10994.272, dtype=float32), Array(3640.1538, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.47763586, dtype=float32), Array(0.2556898, dtype=float32)), 'eval/avg_episode_length': (Array(953.3906, dtype=float32), Array(210.17383, dtype=float32)), 'eval/epoch_eval_time': 4.074663162231445, 'eval/sps': 31413.639582885713}
I0727 22:30:54.396715 139877795424064 train.py:379] starting iteration 198, 81100800 steps, 2828.5125732421875
I0727 22:31:08.541576 139877795424064 train.py:394] {'eval/walltime': 826.4564828872681, 'training/sps': 40731.04480335243, 'training/walltime': 2005.7269093990326, 'training/entropy_loss': Array(-0.04140596, dtype=float32), 'training/policy_loss': Array(0.00093531, dtype=float32), 'training/total_loss': Array(1139.4196, dtype=float32), 'training/v_loss': Array(1139.46, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.48967046, dtype=float32), Array(0.20840286, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(169.31445, dtype=float32), Array(1239.7988, dtype=float32)), 'eval/episode_reward': (Array(-11148.648, dtype=float32), Array(3692.5823, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.48169988, dtype=float32), Array(0.21699937, dtype=float32)), 'eval/avg_episode_length': (Array(984.4922, dtype=float32), Array(123.08946, dtype=float32)), 'eval/epoch_eval_time': 4.084553241729736, 'eval/sps': 31337.576578092114}
I0727 22:31:08.544492 139877795424064 train.py:379] starting iteration 199, 81510400 steps, 2842.660348892212
I0727 22:31:22.680767 139877795424064 train.py:394] {'eval/walltime': 830.534211397171, 'training/sps': 40737.08793191097, 'training/walltime': 2015.7816290855408, 'training/entropy_loss': Array(-0.0417822, dtype=float32), 'training/policy_loss': Array(0.00096254, dtype=float32), 'training/total_loss': Array(660.53516, dtype=float32), 'training/v_loss': Array(660.576, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.49680665, dtype=float32), Array(0.23562744, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(247.62495, dtype=float32), Array(1512.1355, dtype=float32)), 'eval/episode_reward': (Array(-11217.961, dtype=float32), Array(3582.7632, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.48723572, dtype=float32), Array(0.24704029, dtype=float32)), 'eval/avg_episode_length': (Array(976.6953, dtype=float32), Array(150.43112, dtype=float32)), 'eval/epoch_eval_time': 4.077728509902954, 'eval/sps': 31390.025032109428}
I0727 22:31:22.683596 139877795424064 train.py:379] starting iteration 200, 81920000 steps, 2856.7994542121887
I0727 22:31:36.815112 139877795424064 train.py:394] {'eval/walltime': 834.6142659187317, 'training/sps': 40767.17667710572, 'training/walltime': 2025.8289277553558, 'training/entropy_loss': Array(-0.04228938, dtype=float32), 'training/policy_loss': Array(0.00263691, dtype=float32), 'training/total_loss': Array(24215.148, dtype=float32), 'training/v_loss': Array(24215.188, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.51840866, dtype=float32), Array(0.24197465, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1576.2642, dtype=float32), Array(3629.0508, dtype=float32)), 'eval/episode_reward': (Array(-11443.593, dtype=float32), Array(3701.4138, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.51231706, dtype=float32), Array(0.2503276, dtype=float32)), 'eval/avg_episode_length': (Array(844.66406, dtype=float32), Array(360.9685, dtype=float32)), 'eval/epoch_eval_time': 4.080054521560669, 'eval/sps': 31372.12978983391}
I0727 22:31:36.938786 139877795424064 train.py:379] starting iteration 201, 82329600 steps, 2871.0546402931213
I0727 22:31:51.099374 139877795424064 train.py:394] {'eval/walltime': 838.718106508255, 'training/sps': 40746.515972032495, 'training/walltime': 2035.8813209533691, 'training/entropy_loss': Array(-0.05211936, dtype=float32), 'training/policy_loss': Array(0.00109975, dtype=float32), 'training/total_loss': Array(5352.4805, dtype=float32), 'training/v_loss': Array(5352.5312, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5202831, dtype=float32), Array(0.25816533, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(326.37518, dtype=float32), Array(1739.149, dtype=float32)), 'eval/episode_reward': (Array(-11344.065, dtype=float32), Array(3812.8354, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5126928, dtype=float32), Array(0.27046248, dtype=float32)), 'eval/avg_episode_length': (Array(968.9375, dtype=float32), Array(172.94885, dtype=float32)), 'eval/epoch_eval_time': 4.103840589523315, 'eval/sps': 31190.295336220144}
I0727 22:31:51.102580 139877795424064 train.py:379] starting iteration 202, 82739200 steps, 2885.218437194824
I0727 22:32:05.240632 139877795424064 train.py:394] {'eval/walltime': 842.7897598743439, 'training/sps': 40708.48560389603, 'training/walltime': 2045.9431052207947, 'training/entropy_loss': Array(-0.05154668, dtype=float32), 'training/policy_loss': Array(0.00450525, dtype=float32), 'training/total_loss': Array(2285.2637, dtype=float32), 'training/v_loss': Array(2285.311, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.51273906, dtype=float32), Array(0.25247103, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(404.19345, dtype=float32), Array(1936.8441, dtype=float32)), 'eval/episode_reward': (Array(-11223.037, dtype=float32), Array(3694.2166, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.50759006, dtype=float32), Array(0.261438, dtype=float32)), 'eval/avg_episode_length': (Array(961.1406, dtype=float32), Array(192.73633, dtype=float32)), 'eval/epoch_eval_time': 4.071653366088867, 'eval/sps': 31436.86077652375}
I0727 22:32:05.243476 139877795424064 train.py:379] starting iteration 203, 83148800 steps, 2899.3593335151672
I0727 22:32:19.388575 139877795424064 train.py:394] {'eval/walltime': 846.8884632587433, 'training/sps': 40789.286742844284, 'training/walltime': 2055.9849576950073, 'training/entropy_loss': Array(-0.05042234, dtype=float32), 'training/policy_loss': Array(0.00767704, dtype=float32), 'training/total_loss': Array(1092.1115, dtype=float32), 'training/v_loss': Array(1092.1542, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.45379266, dtype=float32), Array(0.21185042, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(637.0781, dtype=float32), Array(2419.7559, dtype=float32)), 'eval/episode_reward': (Array(-10377.016, dtype=float32), Array(3105.6086, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4434815, dtype=float32), Array(0.22151259, dtype=float32)), 'eval/avg_episode_length': (Array(937.90625, dtype=float32), Array(240.48856, dtype=float32)), 'eval/epoch_eval_time': 4.098703384399414, 'eval/sps': 31229.3884176144}
I0727 22:32:19.394468 139877795424064 train.py:379] starting iteration 204, 83558400 steps, 2913.5103101730347
I0727 22:32:33.536166 139877795424064 train.py:394] {'eval/walltime': 850.9667892456055, 'training/sps': 40721.48492807673, 'training/walltime': 2066.043529987335, 'training/entropy_loss': Array(-0.05003029, dtype=float32), 'training/policy_loss': Array(0.00450932, dtype=float32), 'training/total_loss': Array(909.44543, dtype=float32), 'training/v_loss': Array(909.49097, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4875815, dtype=float32), Array(0.24166307, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(794.28125, dtype=float32), Array(2683.1147, dtype=float32)), 'eval/episode_reward': (Array(-10958.746, dtype=float32), Array(3527.1138, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.47627157, dtype=float32), Array(0.25630963, dtype=float32)), 'eval/avg_episode_length': (Array(922.39844, dtype=float32), Array(266.57086, dtype=float32)), 'eval/epoch_eval_time': 4.078325986862183, 'eval/sps': 31385.42637649271}
I0727 22:32:33.539030 139877795424064 train.py:379] starting iteration 205, 83968000 steps, 2927.6548869609833
I0727 22:32:47.666247 139877795424064 train.py:394] {'eval/walltime': 855.0351076126099, 'training/sps': 40737.91964230032, 'training/walltime': 2076.098044395447, 'training/entropy_loss': Array(-0.04985926, dtype=float32), 'training/policy_loss': Array(0.00532081, dtype=float32), 'training/total_loss': Array(20042.398, dtype=float32), 'training/v_loss': Array(20042.443, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.47416368, dtype=float32), Array(0.20786779, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(559.4833, dtype=float32), Array(2272.921, dtype=float32)), 'eval/episode_reward': (Array(-10543.143, dtype=float32), Array(3379.468, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.46504802, dtype=float32), Array(0.21876813, dtype=float32)), 'eval/avg_episode_length': (Array(945.7031, dtype=float32), Array(225.74565, dtype=float32)), 'eval/epoch_eval_time': 4.0683183670043945, 'eval/sps': 31462.631105305958}
I0727 22:32:47.669061 139877795424064 train.py:379] starting iteration 206, 84377600 steps, 2941.784918785095
I0727 22:33:01.839511 139877795424064 train.py:394] {'eval/walltime': 859.1049561500549, 'training/sps': 40569.218293928585, 'training/walltime': 2086.1943690776825, 'training/entropy_loss': Array(-0.04976469, dtype=float32), 'training/policy_loss': Array(0.00253831, dtype=float32), 'training/total_loss': Array(3804.8306, dtype=float32), 'training/v_loss': Array(3804.8774, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.50992024, dtype=float32), Array(0.23874842, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(872.9608, dtype=float32), Array(2801.6143, dtype=float32)), 'eval/episode_reward': (Array(-11137.217, dtype=float32), Array(3631.44, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5014802, dtype=float32), Array(0.24987645, dtype=float32)), 'eval/avg_episode_length': (Array(914.6172, dtype=float32), Array(278.46317, dtype=float32)), 'eval/epoch_eval_time': 4.069848537445068, 'eval/sps': 31450.80187194255}
I0727 22:33:01.842398 139877795424064 train.py:379] starting iteration 207, 84787200 steps, 2955.9582533836365
I0727 22:33:15.994458 139877795424064 train.py:394] {'eval/walltime': 863.1917705535889, 'training/sps': 40712.514197945755, 'training/walltime': 2096.2551577091217, 'training/entropy_loss': Array(-0.04992454, dtype=float32), 'training/policy_loss': Array(0.00232095, dtype=float32), 'training/total_loss': Array(1483.8308, dtype=float32), 'training/v_loss': Array(1483.8784, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.49343595, dtype=float32), Array(0.22222103, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1028.7521, dtype=float32), Array(3019.6477, dtype=float32)), 'eval/episode_reward': (Array(-11413.67, dtype=float32), Array(3480.599, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.48468232, dtype=float32), Array(0.23179415, dtype=float32)), 'eval/avg_episode_length': (Array(899.10156, dtype=float32), Array(300.09744, dtype=float32)), 'eval/epoch_eval_time': 4.0868144035339355, 'eval/sps': 31320.238053706646}
I0727 22:33:15.997262 139877795424064 train.py:379] starting iteration 208, 85196800 steps, 2970.113119363785
I0727 22:33:30.171356 139877795424064 train.py:394] {'eval/walltime': 867.2844552993774, 'training/sps': 40645.8364647979, 'training/walltime': 2106.3324506282806, 'training/entropy_loss': Array(-0.04973869, dtype=float32), 'training/policy_loss': Array(0.00239003, dtype=float32), 'training/total_loss': Array(668.63696, dtype=float32), 'training/v_loss': Array(668.6842, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.47595674, dtype=float32), Array(0.2339298, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(403.3317, dtype=float32), Array(1936.8551, dtype=float32)), 'eval/episode_reward': (Array(-10771.505, dtype=float32), Array(4083.4978, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.46495554, dtype=float32), Array(0.24530058, dtype=float32)), 'eval/avg_episode_length': (Array(961.1953, dtype=float32), Array(192.46521, dtype=float32)), 'eval/epoch_eval_time': 4.092684745788574, 'eval/sps': 31275.31387109981}
I0727 22:33:30.174185 139877795424064 train.py:379] starting iteration 209, 85606400 steps, 2984.29004240036
I0727 22:33:44.360029 139877795424064 train.py:394] {'eval/walltime': 871.3907797336578, 'training/sps': 40653.354990993364, 'training/walltime': 2116.4078798294067, 'training/entropy_loss': Array(-0.04911081, dtype=float32), 'training/policy_loss': Array(0.00255198, dtype=float32), 'training/total_loss': Array(927.18176, dtype=float32), 'training/v_loss': Array(927.2283, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.46355844, dtype=float32), Array(0.20913592, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(246.7557, dtype=float32), Array(1512.6465, dtype=float32)), 'eval/episode_reward': (Array(-10759.744, dtype=float32), Array(3848.2317, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4517438, dtype=float32), Array(0.22136097, dtype=float32)), 'eval/avg_episode_length': (Array(976.6875, dtype=float32), Array(150.48157, dtype=float32)), 'eval/epoch_eval_time': 4.1063244342803955, 'eval/sps': 31171.428865052916}
I0727 22:33:44.362918 139877795424064 train.py:379] starting iteration 210, 86016000 steps, 2998.47877573967
I0727 22:33:58.502994 139877795424064 train.py:394] {'eval/walltime': 875.4586799144745, 'training/sps': 40683.822071004186, 'training/walltime': 2126.47576379776, 'training/entropy_loss': Array(-0.04843873, dtype=float32), 'training/policy_loss': Array(0.00378677, dtype=float32), 'training/total_loss': Array(16751.365, dtype=float32), 'training/v_loss': Array(16751.41, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.51858544, dtype=float32), Array(0.22996998, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(404.43896, dtype=float32), Array(1936.4988, dtype=float32)), 'eval/episode_reward': (Array(-11166.689, dtype=float32), Array(3497.5815, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5121293, dtype=float32), Array(0.2380835, dtype=float32)), 'eval/avg_episode_length': (Array(961.2656, dtype=float32), Array(192.11632, dtype=float32)), 'eval/epoch_eval_time': 4.06790018081665, 'eval/sps': 31465.865510569973}
I0727 22:33:58.505793 139877795424064 train.py:379] starting iteration 211, 86425600 steps, 3012.6216497421265
I0727 22:34:12.652160 139877795424064 train.py:394] {'eval/walltime': 879.5330097675323, 'training/sps': 40685.2123591249, 'training/walltime': 2136.5433037281036, 'training/entropy_loss': Array(-0.04952401, dtype=float32), 'training/policy_loss': Array(0.00143348, dtype=float32), 'training/total_loss': Array(3718.995, dtype=float32), 'training/v_loss': Array(3719.043, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4781692, dtype=float32), Array(0.20469625, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(403.3646, dtype=float32), Array(1936.8219, dtype=float32)), 'eval/episode_reward': (Array(-10558.105, dtype=float32), Array(3637.3071, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.47066325, dtype=float32), Array(0.21359675, dtype=float32)), 'eval/avg_episode_length': (Array(961.22656, dtype=float32), Array(192.31018, dtype=float32)), 'eval/epoch_eval_time': 4.074329853057861, 'eval/sps': 31416.20944213282}
I0727 22:34:12.655047 139877795424064 train.py:379] starting iteration 212, 86835200 steps, 3026.7709045410156
I0727 22:34:26.781363 139877795424064 train.py:394] {'eval/walltime': 883.6123168468475, 'training/sps': 40786.039824618696, 'training/walltime': 2146.585955619812, 'training/entropy_loss': Array(-0.04815887, dtype=float32), 'training/policy_loss': Array(0.00112773, dtype=float32), 'training/total_loss': Array(1539.5338, dtype=float32), 'training/v_loss': Array(1539.5808, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.49947092, dtype=float32), Array(0.22795938, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(91.38737, dtype=float32), Array(880.0126, dtype=float32)), 'eval/episode_reward': (Array(-11492.034, dtype=float32), Array(3747.0823, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.49034333, dtype=float32), Array(0.23927434, dtype=float32)), 'eval/avg_episode_length': (Array(992.25, dtype=float32), Array(87.338066, dtype=float32)), 'eval/epoch_eval_time': 4.0793070793151855, 'eval/sps': 31377.878034494042}
I0727 22:34:26.784250 139877795424064 train.py:379] starting iteration 213, 87244800 steps, 3040.900107383728
I0727 22:34:40.926702 139877795424064 train.py:394] {'eval/walltime': 887.6966269016266, 'training/sps': 40741.2207372116, 'training/walltime': 2156.639655351639, 'training/entropy_loss': Array(-0.04707873, dtype=float32), 'training/policy_loss': Array(0.00112952, dtype=float32), 'training/total_loss': Array(587.84753, dtype=float32), 'training/v_loss': Array(587.89355, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.50260925, dtype=float32), Array(0.22913301, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(169.65009, dtype=float32), Array(1239.5288, dtype=float32)), 'eval/episode_reward': (Array(-10699.366, dtype=float32), Array(3819.4065, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4930625, dtype=float32), Array(0.24016762, dtype=float32)), 'eval/avg_episode_length': (Array(984.5, dtype=float32), Array(123.02769, dtype=float32)), 'eval/epoch_eval_time': 4.084310054779053, 'eval/sps': 31339.4424721079}
I0727 22:34:40.930867 139877795424064 train.py:379] starting iteration 214, 87654400 steps, 3055.046717405319
I0727 22:34:55.055499 139877795424064 train.py:394] {'eval/walltime': 891.774872303009, 'training/sps': 40788.9913710208, 'training/walltime': 2166.681580543518, 'training/entropy_loss': Array(-0.04630252, dtype=float32), 'training/policy_loss': Array(0.00105615, dtype=float32), 'training/total_loss': Array(725.3063, dtype=float32), 'training/v_loss': Array(725.3516, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.47133932, dtype=float32), Array(0.24916261, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(325.08734, dtype=float32), Array(1739.5007, dtype=float32)), 'eval/episode_reward': (Array(-10977.943, dtype=float32), Array(4433.287, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4607547, dtype=float32), Array(0.26010504, dtype=float32)), 'eval/avg_episode_length': (Array(968.9844, dtype=float32), Array(172.68808, dtype=float32)), 'eval/epoch_eval_time': 4.078245401382446, 'eval/sps': 31386.046547520284}
I0727 22:34:55.058316 139877795424064 train.py:379] starting iteration 215, 88064000 steps, 3069.1741733551025
I0727 22:35:09.214399 139877795424064 train.py:394] {'eval/walltime': 895.8519339561462, 'training/sps': 40655.088580374315, 'training/walltime': 2176.7565801143646, 'training/entropy_loss': Array(-0.04310031, dtype=float32), 'training/policy_loss': Array(0.00154566, dtype=float32), 'training/total_loss': Array(17101.215, dtype=float32), 'training/v_loss': Array(17101.258, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.51843685, dtype=float32), Array(0.24418695, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(482.5642, dtype=float32), Array(2113.031, dtype=float32)), 'eval/episode_reward': (Array(-11125.803, dtype=float32), Array(3930.2898, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.51097846, dtype=float32), Array(0.25337988, dtype=float32)), 'eval/avg_episode_length': (Array(953.4531, dtype=float32), Array(209.8922, dtype=float32)), 'eval/epoch_eval_time': 4.077061653137207, 'eval/sps': 31395.15928131891}
I0727 22:35:09.217274 139877795424064 train.py:379] starting iteration 216, 88473600 steps, 3083.3331315517426
I0727 22:35:23.347663 139877795424064 train.py:394] {'eval/walltime': 899.921183347702, 'training/sps': 40728.349781145094, 'training/walltime': 2186.8134570121765, 'training/entropy_loss': Array(-0.04724253, dtype=float32), 'training/policy_loss': Array(0.00077675, dtype=float32), 'training/total_loss': Array(3404.597, dtype=float32), 'training/v_loss': Array(3404.6436, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4838059, dtype=float32), Array(0.22350119, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(247.26047, dtype=float32), Array(1512.2764, dtype=float32)), 'eval/episode_reward': (Array(-10844.564, dtype=float32), Array(3593.8494, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.47331285, dtype=float32), Array(0.2354812, dtype=float32)), 'eval/avg_episode_length': (Array(976.6953, dtype=float32), Array(150.43144, dtype=float32)), 'eval/epoch_eval_time': 4.069249391555786, 'eval/sps': 31455.432607698214}
I0727 22:35:23.350560 139877795424064 train.py:379] starting iteration 217, 88883200 steps, 3097.4664175510406
I0727 22:35:37.523676 139877795424064 train.py:394] {'eval/walltime': 904.0153546333313, 'training/sps': 40656.296985388355, 'training/walltime': 2196.8881571292877, 'training/entropy_loss': Array(-0.04629235, dtype=float32), 'training/policy_loss': Array(0.00078133, dtype=float32), 'training/total_loss': Array(1925.1345, dtype=float32), 'training/v_loss': Array(1925.1799, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.46161944, dtype=float32), Array(0.21817043, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(559.1939, dtype=float32), Array(2273.0764, dtype=float32)), 'eval/episode_reward': (Array(-10446.328, dtype=float32), Array(3635.3904, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4501502, dtype=float32), Array(0.22922151, dtype=float32)), 'eval/avg_episode_length': (Array(945.59375, dtype=float32), Array(226.20009, dtype=float32)), 'eval/epoch_eval_time': 4.0941712856292725, 'eval/sps': 31263.958215252456}
I0727 22:35:37.526515 139877795424064 train.py:379] starting iteration 218, 89292800 steps, 3111.642371892929
I0727 22:35:51.712439 139877795424064 train.py:394] {'eval/walltime': 908.1188697814941, 'training/sps': 40642.53830907774, 'training/walltime': 2206.966267824173, 'training/entropy_loss': Array(-0.04408019, dtype=float32), 'training/policy_loss': Array(0.00089464, dtype=float32), 'training/total_loss': Array(626.8104, dtype=float32), 'training/v_loss': Array(626.85364, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.46611947, dtype=float32), Array(0.1992597, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(481.1197, dtype=float32), Array(2113.0164, dtype=float32)), 'eval/episode_reward': (Array(-10421.01, dtype=float32), Array(3382.398, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.45481294, dtype=float32), Array(0.21125467, dtype=float32)), 'eval/avg_episode_length': (Array(953.47656, dtype=float32), Array(209.78604, dtype=float32)), 'eval/epoch_eval_time': 4.103515148162842, 'eval/sps': 31192.768974498864}
I0727 22:35:51.715276 139877795424064 train.py:379] starting iteration 219, 89702400 steps, 3125.8311331272125
I0727 22:36:05.898288 139877795424064 train.py:394] {'eval/walltime': 912.2357153892517, 'training/sps': 40706.74938682326, 'training/walltime': 2217.028481245041, 'training/entropy_loss': Array(-0.04160962, dtype=float32), 'training/policy_loss': Array(0.00083805, dtype=float32), 'training/total_loss': Array(645.8453, dtype=float32), 'training/v_loss': Array(645.88605, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4374677, dtype=float32), Array(0.18101312, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(636.7566, dtype=float32), Array(2420.2593, dtype=float32)), 'eval/episode_reward': (Array(-10037.98, dtype=float32), Array(3259.9055, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.42626387, dtype=float32), Array(0.1901816, dtype=float32)), 'eval/avg_episode_length': (Array(937.9297, dtype=float32), Array(240.3978, dtype=float32)), 'eval/epoch_eval_time': 4.116845607757568, 'eval/sps': 31091.76592845831}
I0727 22:36:05.901121 139877795424064 train.py:379] starting iteration 220, 90112000 steps, 3140.016977787018
I0727 22:36:20.032698 139877795424064 train.py:394] {'eval/walltime': 916.3036019802094, 'training/sps': 40717.43718801626, 'training/walltime': 2227.0880534648895, 'training/entropy_loss': Array(-0.03580388, dtype=float32), 'training/policy_loss': Array(0.00127342, dtype=float32), 'training/total_loss': Array(17861., dtype=float32), 'training/v_loss': Array(17861.035, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4544561, dtype=float32), Array(0.20879091, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(402.77295, dtype=float32), Array(1936.8778, dtype=float32)), 'eval/episode_reward': (Array(-10086.613, dtype=float32), Array(3154.388, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4434402, dtype=float32), Array(0.21903098, dtype=float32)), 'eval/avg_episode_length': (Array(961.1719, dtype=float32), Array(192.58147, dtype=float32)), 'eval/epoch_eval_time': 4.067886590957642, 'eval/sps': 31465.970630677508}
I0727 22:36:20.035573 139877795424064 train.py:379] starting iteration 221, 90521600 steps, 3154.1514308452606
I0727 22:36:34.204543 139877795424064 train.py:394] {'eval/walltime': 920.381597995758, 'training/sps': 40616.145433197424, 'training/walltime': 2237.1727130413055, 'training/entropy_loss': Array(-0.04299694, dtype=float32), 'training/policy_loss': Array(0.00113426, dtype=float32), 'training/total_loss': Array(3295.9727, dtype=float32), 'training/v_loss': Array(3296.015, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.48430014, dtype=float32), Array(0.23113544, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(91.02521, dtype=float32), Array(880.0703, dtype=float32)), 'eval/episode_reward': (Array(-10734.99, dtype=float32), Array(3382.6777, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.47644436, dtype=float32), Array(0.24006769, dtype=float32)), 'eval/avg_episode_length': (Array(992.22656, dtype=float32), Array(87.60219, dtype=float32)), 'eval/epoch_eval_time': 4.077996015548706, 'eval/sps': 31387.96593031424}
I0727 22:36:34.207195 139877795424064 train.py:379] starting iteration 222, 90931200 steps, 3168.323052406311
I0727 22:36:48.336356 139877795424064 train.py:394] {'eval/walltime': 924.4537305831909, 'training/sps': 40744.895368067, 'training/walltime': 2247.225506067276, 'training/entropy_loss': Array(-0.04328504, dtype=float32), 'training/policy_loss': Array(0.0007774, dtype=float32), 'training/total_loss': Array(1712.277, dtype=float32), 'training/v_loss': Array(1712.3195, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.512837, dtype=float32), Array(0.21981, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(404.23804, dtype=float32), Array(1936.6547, dtype=float32)), 'eval/episode_reward': (Array(-11471.693, dtype=float32), Array(3573.7117, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5054054, dtype=float32), Array(0.22943895, dtype=float32)), 'eval/avg_episode_length': (Array(961.27344, dtype=float32), Array(192.07777, dtype=float32)), 'eval/epoch_eval_time': 4.072132587432861, 'eval/sps': 31433.161188077444}
I0727 22:36:48.338928 139877795424064 train.py:379] starting iteration 223, 91340800 steps, 3182.4547860622406
I0727 22:37:02.485237 139877795424064 train.py:394] {'eval/walltime': 928.5368897914886, 'training/sps': 40720.981088254935, 'training/walltime': 2257.284202814102, 'training/entropy_loss': Array(-0.04470537, dtype=float32), 'training/policy_loss': Array(0.0006408, dtype=float32), 'training/total_loss': Array(848.79346, dtype=float32), 'training/v_loss': Array(848.8376, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4895329, dtype=float32), Array(0.23136045, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(325.54095, dtype=float32), Array(1739.3005, dtype=float32)), 'eval/episode_reward': (Array(-10987.602, dtype=float32), Array(3485.9067, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.47778535, dtype=float32), Array(0.24512616, dtype=float32)), 'eval/avg_episode_length': (Array(968.9219, dtype=float32), Array(173.03607, dtype=float32)), 'eval/epoch_eval_time': 4.0831592082977295, 'eval/sps': 31348.275555819742}
I0727 22:37:02.490801 139877795424064 train.py:379] starting iteration 224, 91750400 steps, 3196.6066439151764
I0727 22:37:16.609885 139877795424064 train.py:394] {'eval/walltime': 932.6143748760223, 'training/sps': 40808.08118276117, 'training/walltime': 2267.3214304447174, 'training/entropy_loss': Array(-0.04463132, dtype=float32), 'training/policy_loss': Array(0.00068902, dtype=float32), 'training/total_loss': Array(759.6293, dtype=float32), 'training/v_loss': Array(759.6732, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.51243716, dtype=float32), Array(0.19996914, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(404.24503, dtype=float32), Array(1936.6064, dtype=float32)), 'eval/episode_reward': (Array(-11259.535, dtype=float32), Array(3337.8662, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.50502187, dtype=float32), Array(0.20877409, dtype=float32)), 'eval/avg_episode_length': (Array(961.21094, dtype=float32), Array(192.38777, dtype=float32)), 'eval/epoch_eval_time': 4.077485084533691, 'eval/sps': 31391.89901282945}
I0727 22:37:16.612422 139877795424064 train.py:379] starting iteration 225, 92160000 steps, 3210.728279352188
I0727 22:37:30.832302 139877795424064 train.py:394] {'eval/walltime': 936.7132244110107, 'training/sps': 40488.03771817015, 'training/walltime': 2277.4379987716675, 'training/entropy_loss': Array(-0.04832745, dtype=float32), 'training/policy_loss': Array(0.00236362, dtype=float32), 'training/total_loss': Array(23663.66, dtype=float32), 'training/v_loss': Array(23663.707, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.51200855, dtype=float32), Array(0.22887427, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1185.4783, dtype=float32), Array(3214.6965, dtype=float32)), 'eval/episode_reward': (Array(-11683.222, dtype=float32), Array(3800.426, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5051924, dtype=float32), Array(0.2394937, dtype=float32)), 'eval/avg_episode_length': (Array(883.58594, dtype=float32), Array(319.52124, dtype=float32)), 'eval/epoch_eval_time': 4.098849534988403, 'eval/sps': 31228.274887226897}
I0727 22:37:30.965872 139877795424064 train.py:379] starting iteration 226, 92569600 steps, 3225.0817246437073
I0727 22:37:45.101955 139877795424064 train.py:394] {'eval/walltime': 940.7902367115021, 'training/sps': 40737.12270657797, 'training/walltime': 2287.492709875107, 'training/entropy_loss': Array(-0.05147713, dtype=float32), 'training/policy_loss': Array(0.00110202, dtype=float32), 'training/total_loss': Array(5273.2266, dtype=float32), 'training/v_loss': Array(5273.2764, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4968987, dtype=float32), Array(0.24634214, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(247.55078, dtype=float32), Array(1512.2985, dtype=float32)), 'eval/episode_reward': (Array(-11225.045, dtype=float32), Array(4057.9585, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.48908064, dtype=float32), Array(0.2562251, dtype=float32)), 'eval/avg_episode_length': (Array(976.7031, dtype=float32), Array(150.38084, dtype=float32)), 'eval/epoch_eval_time': 4.077012300491333, 'eval/sps': 31395.53932289445}
I0727 22:37:45.104678 139877795424064 train.py:379] starting iteration 227, 92979200 steps, 3239.220535516739
I0727 22:37:59.210131 139877795424064 train.py:394] {'eval/walltime': 944.8683876991272, 'training/sps': 40864.57342519449, 'training/walltime': 2297.516061782837, 'training/entropy_loss': Array(-0.050755, dtype=float32), 'training/policy_loss': Array(0.00291926, dtype=float32), 'training/total_loss': Array(2177.5261, dtype=float32), 'training/v_loss': Array(2177.5737, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4867974, dtype=float32), Array(0.22530417, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(247.36423, dtype=float32), Array(1512.5061, dtype=float32)), 'eval/episode_reward': (Array(-11234.99, dtype=float32), Array(3761.4526, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.47969836, dtype=float32), Array(0.23378603, dtype=float32)), 'eval/avg_episode_length': (Array(976.6953, dtype=float32), Array(150.43123, dtype=float32)), 'eval/epoch_eval_time': 4.078150987625122, 'eval/sps': 31386.773169607375}
I0727 22:37:59.212811 139877795424064 train.py:379] starting iteration 228, 93388800 steps, 3253.328668832779
I0727 22:38:13.318911 139877795424064 train.py:394] {'eval/walltime': 948.9450812339783, 'training/sps': 40855.518173079516, 'training/walltime': 2307.541635274887, 'training/entropy_loss': Array(-0.05006089, dtype=float32), 'training/policy_loss': Array(0.00404339, dtype=float32), 'training/total_loss': Array(885.34485, dtype=float32), 'training/v_loss': Array(885.39087, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4722808, dtype=float32), Array(0.24140088, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(481.28867, dtype=float32), Array(2113.01, dtype=float32)), 'eval/episode_reward': (Array(-10572.205, dtype=float32), Array(3028.1484, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.461391, dtype=float32), Array(0.25288078, dtype=float32)), 'eval/avg_episode_length': (Array(953.46875, dtype=float32), Array(209.82188, dtype=float32)), 'eval/epoch_eval_time': 4.076693534851074, 'eval/sps': 31397.994209215427}
I0727 22:38:13.321807 139877795424064 train.py:379] starting iteration 229, 93798400 steps, 3267.4376645088196
I0727 22:38:27.424571 139877795424064 train.py:394] {'eval/walltime': 953.022971868515, 'training/sps': 40874.29785784453, 'training/walltime': 2317.562602519989, 'training/entropy_loss': Array(-0.04954863, dtype=float32), 'training/policy_loss': Array(0.00333998, dtype=float32), 'training/total_loss': Array(1052.8076, dtype=float32), 'training/v_loss': Array(1052.8538, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5295111, dtype=float32), Array(0.2393044, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(326.58405, dtype=float32), Array(1739.3976, dtype=float32)), 'eval/episode_reward': (Array(-11067.487, dtype=float32), Array(3694.1501, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5216379, dtype=float32), Array(0.2507909, dtype=float32)), 'eval/avg_episode_length': (Array(968.96875, dtype=float32), Array(172.775, dtype=float32)), 'eval/epoch_eval_time': 4.077890634536743, 'eval/sps': 31388.777059378168}
I0727 22:38:27.427386 139877795424064 train.py:379] starting iteration 230, 94208000 steps, 3281.543242454529
I0727 22:38:41.565855 139877795424064 train.py:394] {'eval/walltime': 957.0997266769409, 'training/sps': 40725.477479042456, 'training/walltime': 2327.6201887130737, 'training/entropy_loss': Array(-0.04979251, dtype=float32), 'training/policy_loss': Array(0.00676718, dtype=float32), 'training/total_loss': Array(18200.611, dtype=float32), 'training/v_loss': Array(18200.654, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.52366745, dtype=float32), Array(0.23941901, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(404.56915, dtype=float32), Array(1936.7063, dtype=float32)), 'eval/episode_reward': (Array(-11400.068, dtype=float32), Array(4026.573, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.517744, dtype=float32), Array(0.24851486, dtype=float32)), 'eval/avg_episode_length': (Array(961.1406, dtype=float32), Array(192.73633, dtype=float32)), 'eval/epoch_eval_time': 4.076754808425903, 'eval/sps': 31397.5222977471}
I0727 22:38:41.568611 139877795424064 train.py:379] starting iteration 231, 94617600 steps, 3295.684468269348
I0727 22:38:55.692560 139877795424064 train.py:394] {'eval/walltime': 961.1834247112274, 'training/sps': 40812.67051594688, 'training/walltime': 2337.6562876701355, 'training/entropy_loss': Array(-0.0501225, dtype=float32), 'training/policy_loss': Array(0.00200074, dtype=float32), 'training/total_loss': Array(3489.4436, dtype=float32), 'training/v_loss': Array(3489.492, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.50358534, dtype=float32), Array(0.22025034, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(325.84912, dtype=float32), Array(1739.2966, dtype=float32)), 'eval/episode_reward': (Array(-11058.605, dtype=float32), Array(3287.5566, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.49551898, dtype=float32), Array(0.23231946, dtype=float32)), 'eval/avg_episode_length': (Array(968.96875, dtype=float32), Array(172.775, dtype=float32)), 'eval/epoch_eval_time': 4.083698034286499, 'eval/sps': 31344.139288781687}
I0727 22:38:55.695344 139877795424064 train.py:379] starting iteration 232, 95027200 steps, 3309.8112013339996
I0727 22:39:09.834805 139877795424064 train.py:394] {'eval/walltime': 965.2570610046387, 'training/sps': 40707.48822554186, 'training/walltime': 2347.718318462372, 'training/entropy_loss': Array(-0.04956479, dtype=float32), 'training/policy_loss': Array(0.00152224, dtype=float32), 'training/total_loss': Array(1753.522, dtype=float32), 'training/v_loss': Array(1753.5701, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5186503, dtype=float32), Array(0.22405237, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(404.47165, dtype=float32), Array(1936.3898, dtype=float32)), 'eval/episode_reward': (Array(-11469.331, dtype=float32), Array(4007.7305, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5102111, dtype=float32), Array(0.23542061, dtype=float32)), 'eval/avg_episode_length': (Array(961.1797, dtype=float32), Array(192.54266, dtype=float32)), 'eval/epoch_eval_time': 4.073636293411255, 'eval/sps': 31421.55822968993}
I0727 22:39:09.837587 139877795424064 train.py:379] starting iteration 233, 95436800 steps, 3323.9534437656403
I0727 22:39:23.959259 139877795424064 train.py:394] {'eval/walltime': 969.3346767425537, 'training/sps': 40796.640458939575, 'training/walltime': 2357.758360862732, 'training/entropy_loss': Array(-0.04827103, dtype=float32), 'training/policy_loss': Array(0.00157842, dtype=float32), 'training/total_loss': Array(944.38135, dtype=float32), 'training/v_loss': Array(944.4281, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.47079194, dtype=float32), Array(0.22404537, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(325.09308, dtype=float32), Array(1739.1445, dtype=float32)), 'eval/episode_reward': (Array(-10842.773, dtype=float32), Array(3335.9014, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.46134686, dtype=float32), Array(0.23406683, dtype=float32)), 'eval/avg_episode_length': (Array(968.96875, dtype=float32), Array(172.77481, dtype=float32)), 'eval/epoch_eval_time': 4.077615737915039, 'eval/sps': 31390.89316578143}
I0727 22:39:23.961951 139877795424064 train.py:379] starting iteration 234, 95846400 steps, 3338.077808380127
I0727 22:39:38.085339 139877795424064 train.py:394] {'eval/walltime': 973.4090232849121, 'training/sps': 40775.766003992925, 'training/walltime': 2367.8035430908203, 'training/entropy_loss': Array(-0.04671376, dtype=float32), 'training/policy_loss': Array(0.00119975, dtype=float32), 'training/total_loss': Array(823.41895, dtype=float32), 'training/v_loss': Array(823.4645, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4733974, dtype=float32), Array(0.19078535, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(403.24664, dtype=float32), Array(1936.5975, dtype=float32)), 'eval/episode_reward': (Array(-10692.436, dtype=float32), Array(3302.412, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.46325737, dtype=float32), Array(0.20219421, dtype=float32)), 'eval/avg_episode_length': (Array(961.2578, dtype=float32), Array(192.15536, dtype=float32)), 'eval/epoch_eval_time': 4.074346542358398, 'eval/sps': 31416.080755346935}
I0727 22:39:38.088210 139877795424064 train.py:379] starting iteration 235, 96256000 steps, 3352.204067468643
I0727 22:39:52.219878 139877795424064 train.py:394] {'eval/walltime': 977.4882009029388, 'training/sps': 40762.0656312691, 'training/walltime': 2377.8521015644073, 'training/entropy_loss': Array(-0.04030038, dtype=float32), 'training/policy_loss': Array(0.0033984, dtype=float32), 'training/total_loss': Array(16810.734, dtype=float32), 'training/v_loss': Array(16810.77, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.519264, dtype=float32), Array(0.24475107, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(482.56842, dtype=float32), Array(2113.0066, dtype=float32)), 'eval/episode_reward': (Array(-11759.936, dtype=float32), Array(3876.929, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.5118264, dtype=float32), Array(0.2543976, dtype=float32)), 'eval/avg_episode_length': (Array(953.4219, dtype=float32), Array(210.03282, dtype=float32)), 'eval/epoch_eval_time': 4.079177618026733, 'eval/sps': 31378.873877504477}
I0727 22:39:52.222722 139877795424064 train.py:379] starting iteration 236, 96665600 steps, 3366.3385796546936
I0727 22:40:06.361686 139877795424064 train.py:394] {'eval/walltime': 981.5581011772156, 'training/sps': 40694.67520095932, 'training/walltime': 2387.917300462723, 'training/entropy_loss': Array(-0.04497281, dtype=float32), 'training/policy_loss': Array(0.00095399, dtype=float32), 'training/total_loss': Array(3295.3936, dtype=float32), 'training/v_loss': Array(3295.4375, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.5138686, dtype=float32), Array(0.24559924, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(326.18866, dtype=float32), Array(1739.1171, dtype=float32)), 'eval/episode_reward': (Array(-11140.06, dtype=float32), Array(3577.18, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.50637186, dtype=float32), Array(0.25608993, dtype=float32)), 'eval/avg_episode_length': (Array(968.96094, dtype=float32), Array(172.81851, dtype=float32)), 'eval/epoch_eval_time': 4.069900274276733, 'eval/sps': 31450.402067344763}
I0727 22:40:06.364539 139877795424064 train.py:379] starting iteration 237, 97075200 steps, 3380.480396747589
I0727 22:40:20.470052 139877795424064 train.py:394] {'eval/walltime': 985.6370599269867, 'training/sps': 40867.97576972076, 'training/walltime': 2397.939817905426, 'training/entropy_loss': Array(-0.0427998, dtype=float32), 'training/policy_loss': Array(0.00100886, dtype=float32), 'training/total_loss': Array(1917.0154, dtype=float32), 'training/v_loss': Array(1917.0571, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.49153548, dtype=float32), Array(0.24341397, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(247.39642, dtype=float32), Array(1512.266, dtype=float32)), 'eval/episode_reward': (Array(-11017.67, dtype=float32), Array(3824.0798, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4814028, dtype=float32), Array(0.25435236, dtype=float32)), 'eval/avg_episode_length': (Array(976.7344, dtype=float32), Array(150.17903, dtype=float32)), 'eval/epoch_eval_time': 4.078958749771118, 'eval/sps': 31380.55760117271}
I0727 22:40:20.472834 139877795424064 train.py:379] starting iteration 238, 97484800 steps, 3394.588691473007
I0727 22:40:34.590865 139877795424064 train.py:394] {'eval/walltime': 989.7086098194122, 'training/sps': 40786.6169306726, 'training/walltime': 2407.9823276996613, 'training/entropy_loss': Array(-0.04179705, dtype=float32), 'training/policy_loss': Array(0.0009596, dtype=float32), 'training/total_loss': Array(957.2441, dtype=float32), 'training/v_loss': Array(957.28485, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.48762202, dtype=float32), Array(0.25913522, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(325.511, dtype=float32), Array(1739.193, dtype=float32)), 'eval/episode_reward': (Array(-10925.145, dtype=float32), Array(4113.195, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.47903353, dtype=float32), Array(0.2696018, dtype=float32)), 'eval/avg_episode_length': (Array(968.96094, dtype=float32), Array(172.81853, dtype=float32)), 'eval/epoch_eval_time': 4.071549892425537, 'eval/sps': 31437.659707455234}
I0727 22:40:34.593759 139877795424064 train.py:379] starting iteration 239, 97894400 steps, 3408.7096168994904
I0727 22:40:48.717577 139877795424064 train.py:394] {'eval/walltime': 993.8212890625, 'training/sps': 40930.6402005893, 'training/walltime': 2417.989500761032, 'training/entropy_loss': Array(-0.04060875, dtype=float32), 'training/policy_loss': Array(0.00077898, dtype=float32), 'training/total_loss': Array(838.4026, dtype=float32), 'training/v_loss': Array(838.4424, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.47073874, dtype=float32), Array(0.22986591, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(246.99213, dtype=float32), Array(1512.479, dtype=float32)), 'eval/episode_reward': (Array(-10468.215, dtype=float32), Array(3431.0305, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.45998606, dtype=float32), Array(0.24130899, dtype=float32)), 'eval/avg_episode_length': (Array(976.71094, dtype=float32), Array(150.33047, dtype=float32)), 'eval/epoch_eval_time': 4.1126792430877686, 'eval/sps': 31123.263555048987}
I0727 22:40:48.720417 139877795424064 train.py:379] starting iteration 240, 98304000 steps, 3422.8362748622894
I0727 22:41:02.837311 139877795424064 train.py:394] {'eval/walltime': 997.8928124904633, 'training/sps': 40791.57238747578, 'training/walltime': 2428.030790567398, 'training/entropy_loss': Array(-0.02847659, dtype=float32), 'training/policy_loss': Array(0.00260616, dtype=float32), 'training/total_loss': Array(16315.527, dtype=float32), 'training/v_loss': Array(16315.553, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4381965, dtype=float32), Array(0.1882249, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(324.2837, dtype=float32), Array(1739.723, dtype=float32)), 'eval/episode_reward': (Array(-10008.555, dtype=float32), Array(3201.1953, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.4249029, dtype=float32), Array(0.19861323, dtype=float32)), 'eval/avg_episode_length': (Array(968.97656, dtype=float32), Array(172.7312, dtype=float32)), 'eval/epoch_eval_time': 4.071523427963257, 'eval/sps': 31437.864048845928}
I0727 22:41:02.840151 139877795424064 train.py:379] starting iteration 241, 98713600 steps, 3436.956008911133
I0727 22:41:16.970028 139877795424064 train.py:394] {'eval/walltime': 1001.975793838501, 'training/sps': 40785.29329051177, 'training/walltime': 2438.073626279831, 'training/entropy_loss': Array(-0.037952, dtype=float32), 'training/policy_loss': Array(0.0011375, dtype=float32), 'training/total_loss': Array(3218.9766, dtype=float32), 'training/v_loss': Array(3219.0137, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4639846, dtype=float32), Array(0.21842657, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(481.06586, dtype=float32), Array(2113.2598, dtype=float32)), 'eval/episode_reward': (Array(-10542.189, dtype=float32), Array(3423.4631, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.45267242, dtype=float32), Array(0.23069425, dtype=float32)), 'eval/avg_episode_length': (Array(953.46094, dtype=float32), Array(209.85667, dtype=float32)), 'eval/epoch_eval_time': 4.08298134803772, 'eval/sps': 31349.641129640913}
I0727 22:41:16.972796 139877795424064 train.py:379] starting iteration 242, 99123200 steps, 3451.088653564453
I0727 22:41:31.094668 139877795424064 train.py:394] {'eval/walltime': 1006.0447328090668, 'training/sps': 40760.656546716644, 'training/walltime': 2448.1225321292877, 'training/entropy_loss': Array(-0.03684635, dtype=float32), 'training/policy_loss': Array(0.00102884, dtype=float32), 'training/total_loss': Array(1667.1528, dtype=float32), 'training/v_loss': Array(1667.1888, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4466272, dtype=float32), Array(0.20714441, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(324.3742, dtype=float32), Array(1739.5405, dtype=float32)), 'eval/episode_reward': (Array(-10071.053, dtype=float32), Array(3222.84, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.43351182, dtype=float32), Array(0.21837175, dtype=float32)), 'eval/avg_episode_length': (Array(968.9297, dtype=float32), Array(172.99246, dtype=float32)), 'eval/epoch_eval_time': 4.068938970565796, 'eval/sps': 31457.832355298582}
I0727 22:41:31.097499 139877795424064 train.py:379] starting iteration 243, 99532800 steps, 3465.213356733322
I0727 22:41:45.223558 139877795424064 train.py:394] {'eval/walltime': 1010.1274554729462, 'training/sps': 40799.3881280096, 'training/walltime': 2458.1618983745575, 'training/entropy_loss': Array(-0.03556374, dtype=float32), 'training/policy_loss': Array(0.00148607, dtype=float32), 'training/total_loss': Array(823.75, dtype=float32), 'training/v_loss': Array(823.7841, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.40576452, dtype=float32), Array(0.1725064, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(479.62045, dtype=float32), Array(2113.5708, dtype=float32)), 'eval/episode_reward': (Array(-9771.735, dtype=float32), Array(3181.8118, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.39155757, dtype=float32), Array(0.18121587, dtype=float32)), 'eval/avg_episode_length': (Array(953.4453, dtype=float32), Array(209.92683, dtype=float32)), 'eval/epoch_eval_time': 4.0827226638793945, 'eval/sps': 31351.627464789555}
I0727 22:41:45.226349 139877795424064 train.py:379] starting iteration 244, 99942400 steps, 3479.342200279236
I0727 22:41:59.362097 139877795424064 train.py:394] {'eval/walltime': 1014.2003419399261, 'training/sps': 40721.09787760164, 'training/walltime': 2468.2205662727356, 'training/entropy_loss': Array(-0.03443572, dtype=float32), 'training/policy_loss': Array(0.00154426, dtype=float32), 'training/total_loss': Array(805.8588, dtype=float32), 'training/v_loss': Array(805.89166, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4025825, dtype=float32), Array(0.16743535, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1104.5371, dtype=float32), Array(3120.646, dtype=float32)), 'eval/episode_reward': (Array(-9758.123, dtype=float32), Array(2864.2837, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.38822025, dtype=float32), Array(0.17788643, dtype=float32)), 'eval/avg_episode_length': (Array(891.35156, dtype=float32), Array(310.03616, dtype=float32)), 'eval/epoch_eval_time': 4.0728864669799805, 'eval/sps': 31427.343000530822}
I0727 22:41:59.364998 139877795424064 train.py:379] starting iteration 245, 100352000 steps, 3493.480855703354
I0727 22:42:13.491184 139877795424064 train.py:394] {'eval/walltime': 1018.3127999305725, 'training/sps': 40920.08580802493, 'training/walltime': 2478.230320453644, 'training/entropy_loss': Array(-0.03317529, dtype=float32), 'training/policy_loss': Array(0.00362381, dtype=float32), 'training/total_loss': Array(20454.117, dtype=float32), 'training/v_loss': Array(20454.148, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.43858618, dtype=float32), Array(0.19505365, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1027.3362, dtype=float32), Array(3019.7974, dtype=float32)), 'eval/episode_reward': (Array(-10259.366, dtype=float32), Array(3194.821, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.42671978, dtype=float32), Array(0.20706983, dtype=float32)), 'eval/avg_episode_length': (Array(899.15625, dtype=float32), Array(299.93484, dtype=float32)), 'eval/epoch_eval_time': 4.112457990646362, 'eval/sps': 31124.938003289368}
I0727 22:42:13.493918 139877795424064 train.py:379] starting iteration 246, 100761600 steps, 3507.6097757816315
I0727 22:42:27.625025 139877795424064 train.py:394] {'eval/walltime': 1022.384569644928, 'training/sps': 40734.49448866155, 'training/walltime': 2488.285680294037, 'training/entropy_loss': Array(-0.04754759, dtype=float32), 'training/policy_loss': Array(0.00101094, dtype=float32), 'training/total_loss': Array(5972.0083, dtype=float32), 'training/v_loss': Array(5972.0547, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.46814066, dtype=float32), Array(0.20868619, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1184.346, dtype=float32), Array(3215.3196, dtype=float32)), 'eval/episode_reward': (Array(-11125.912, dtype=float32), Array(3323.913, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.45845723, dtype=float32), Array(0.21933867, dtype=float32)), 'eval/avg_episode_length': (Array(883.5078, dtype=float32), Array(319.7358, dtype=float32)), 'eval/epoch_eval_time': 4.071769714355469, 'eval/sps': 31435.962487937868}
I0727 22:42:27.627780 139877795424064 train.py:379] starting iteration 247, 101171200 steps, 3521.743637561798
I0727 22:42:41.746259 139877795424064 train.py:394] {'eval/walltime': 1026.4739170074463, 'training/sps': 40857.9045295708, 'training/walltime': 2498.3106682300568, 'training/entropy_loss': Array(-0.0481054, dtype=float32), 'training/policy_loss': Array(0.00071704, dtype=float32), 'training/total_loss': Array(2695.4263, dtype=float32), 'training/v_loss': Array(2695.4739, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.45374048, dtype=float32), Array(0.19907326, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(793.38257, dtype=float32), Array(2682.82, dtype=float32)), 'eval/episode_reward': (Array(-10364.905, dtype=float32), Array(3465.2158, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.44166464, dtype=float32), Array(0.21119153, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.7582, dtype=float32)), 'eval/epoch_eval_time': 4.0893473625183105, 'eval/sps': 31300.838166307}
I0727 22:42:41.749011 139877795424064 train.py:379] starting iteration 248, 101580800 steps, 3535.8648688793182
I0727 22:42:55.886361 139877795424064 train.py:394] {'eval/walltime': 1030.575819015503, 'training/sps': 40831.496220491, 'training/walltime': 2508.3421399593353, 'training/entropy_loss': Array(-0.04732383, dtype=float32), 'training/policy_loss': Array(0.00223371, dtype=float32), 'training/total_loss': Array(1966.8547, dtype=float32), 'training/v_loss': Array(1966.8997, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.42259574, dtype=float32), Array(0.19144054, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(792.51086, dtype=float32), Array(2683.2039, dtype=float32)), 'eval/episode_reward': (Array(-10068.1875, dtype=float32), Array(3331.3096, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.41104472, dtype=float32), Array(0.20047931, dtype=float32)), 'eval/avg_episode_length': (Array(922.33594, dtype=float32), Array(266.78513, dtype=float32)), 'eval/epoch_eval_time': 4.101902008056641, 'eval/sps': 31205.0360414735}
I0727 22:42:55.889204 139877795424064 train.py:379] starting iteration 249, 101990400 steps, 3550.0050621032715
I0727 22:43:10.028778 139877795424064 train.py:394] {'eval/walltime': 1034.6479580402374, 'training/sps': 40702.022804062355, 'training/walltime': 2518.4055218696594, 'training/entropy_loss': Array(-0.04559751, dtype=float32), 'training/policy_loss': Array(0.00270744, dtype=float32), 'training/total_loss': Array(830.578, dtype=float32), 'training/v_loss': Array(830.6209, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4510036, dtype=float32), Array(0.20243298, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(871.4071, dtype=float32), Array(2801.9543, dtype=float32)), 'eval/episode_reward': (Array(-10546.202, dtype=float32), Array(3613.27, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.43934095, dtype=float32), Array(0.21301056, dtype=float32)), 'eval/avg_episode_length': (Array(914.60156, dtype=float32), Array(278.51398, dtype=float32)), 'eval/epoch_eval_time': 4.072139024734497, 'eval/sps': 31433.11149804017}
I0727 22:43:10.031606 139877795424064 train.py:379] starting iteration 250, 102400000 steps, 3564.147463083267
I0727 22:43:24.159435 139877795424064 train.py:394] {'eval/walltime': 1038.7204792499542, 'training/sps': 40751.5612533008, 'training/walltime': 2528.45667052269, 'training/entropy_loss': Array(-0.0465516, dtype=float32), 'training/policy_loss': Array(0.00608039, dtype=float32), 'training/total_loss': Array(22291.39, dtype=float32), 'training/v_loss': Array(22291.434, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.47595865, dtype=float32), Array(0.21069254, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(481.4227, dtype=float32), Array(2112.8757, dtype=float32)), 'eval/episode_reward': (Array(-10627.264, dtype=float32), Array(3570.0369, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.46618903, dtype=float32), Array(0.22029893, dtype=float32)), 'eval/avg_episode_length': (Array(953.4297, dtype=float32), Array(209.99739, dtype=float32)), 'eval/epoch_eval_time': 4.072521209716797, 'eval/sps': 31430.161663639592}
I0727 22:43:24.305873 139877795424064 train.py:379] starting iteration 251, 102809600 steps, 3578.421727180481
I0727 22:43:38.456445 139877795424064 train.py:394] {'eval/walltime': 1042.8258605003357, 'training/sps': 40791.82227432809, 'training/walltime': 2538.4978988170624, 'training/entropy_loss': Array(-0.04706585, dtype=float32), 'training/policy_loss': Array(0.00569459, dtype=float32), 'training/total_loss': Array(3557.2988, dtype=float32), 'training/v_loss': Array(3557.34, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4760289, dtype=float32), Array(0.2339872, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(403.28748, dtype=float32), Array(1936.9196, dtype=float32)), 'eval/episode_reward': (Array(-10642.559, dtype=float32), Array(3854.854, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.46521312, dtype=float32), Array(0.24572378, dtype=float32)), 'eval/avg_episode_length': (Array(961.16406, dtype=float32), Array(192.62032, dtype=float32)), 'eval/epoch_eval_time': 4.10538125038147, 'eval/sps': 31178.590292462195}
I0727 22:43:38.459460 139877795424064 train.py:379] starting iteration 252, 103219200 steps, 3592.5753180980682
I0727 22:43:52.591252 139877795424064 train.py:394] {'eval/walltime': 1046.8961734771729, 'training/sps': 40725.58077834668, 'training/walltime': 2548.555459499359, 'training/entropy_loss': Array(-0.04761288, dtype=float32), 'training/policy_loss': Array(0.01065967, dtype=float32), 'training/total_loss': Array(1594.9568, dtype=float32), 'training/v_loss': Array(1594.9937, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.47193065, dtype=float32), Array(0.22109763, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(90.70134, dtype=float32), Array(880.05084, dtype=float32)), 'eval/episode_reward': (Array(-10519.9375, dtype=float32), Array(3786.9717, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.46046942, dtype=float32), Array(0.23258291, dtype=float32)), 'eval/avg_episode_length': (Array(992.2344, dtype=float32), Array(87.51415, dtype=float32)), 'eval/epoch_eval_time': 4.070312976837158, 'eval/sps': 31447.213206553606}
I0727 22:43:52.594127 139877795424064 train.py:379] starting iteration 253, 103628800 steps, 3606.7099845409393
I0727 22:44:06.729482 139877795424064 train.py:394] {'eval/walltime': 1051.0048034191132, 'training/sps': 40867.94466000454, 'training/walltime': 2558.577984571457, 'training/entropy_loss': Array(-0.04687278, dtype=float32), 'training/policy_loss': Array(0.00580863, dtype=float32), 'training/total_loss': Array(1443.4215, dtype=float32), 'training/v_loss': Array(1443.4625, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.4759165, dtype=float32), Array(0.20121963, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(325.1958, dtype=float32), Array(1739.2968, dtype=float32)), 'eval/episode_reward': (Array(-10589.055, dtype=float32), Array(3790.3008, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.46480638, dtype=float32), Array(0.21110155, dtype=float32)), 'eval/avg_episode_length': (Array(968.96094, dtype=float32), Array(172.81842, dtype=float32)), 'eval/epoch_eval_time': 4.108629941940308, 'eval/sps': 31153.937397329042}
I0727 22:44:06.732230 139877795424064 train.py:379] starting iteration 254, 104038400 steps, 3620.8480865955353
I0727 22:44:20.878421 139877795424064 train.py:394] {'eval/walltime': 1055.106790304184, 'training/sps': 40796.52904857282, 'training/walltime': 2568.6180543899536, 'training/entropy_loss': Array(-0.04702955, dtype=float32), 'training/policy_loss': Array(0.0031995, dtype=float32), 'training/total_loss': Array(1118.283, dtype=float32), 'training/v_loss': Array(1118.3269, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.48160174, dtype=float32), Array(0.23123841, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(169.04216, dtype=float32), Array(1239.6399, dtype=float32)), 'eval/episode_reward': (Array(-10807.041, dtype=float32), Array(3412.905, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.47317913, dtype=float32), Array(0.24055563, dtype=float32)), 'eval/avg_episode_length': (Array(984.46875, dtype=float32), Array(123.275734, dtype=float32)), 'eval/epoch_eval_time': 4.101986885070801, 'eval/sps': 31204.390356745545}
I0727 22:44:21.920343 139877795424064 train.py:410] total steps: 104448000
