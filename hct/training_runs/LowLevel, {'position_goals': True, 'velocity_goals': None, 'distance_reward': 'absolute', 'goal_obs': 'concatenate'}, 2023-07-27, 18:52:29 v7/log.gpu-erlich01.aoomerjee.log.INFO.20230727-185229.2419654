I0727 18:52:29.794056 139998383736640 low_level_env.py:187] Initialising environment...
I0727 18:53:09.203470 139998383736640 low_level_env.py:289] Environment initialised.
I0727 18:53:09.207936 139998383736640 train.py:118] JAX is running on GPU.
I0727 18:53:09.208012 139998383736640 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0727 18:53:17.373043 139998383736640 train.py:367] Running initial eval
I0727 18:53:34.117995 139998383736640 train.py:373] {'eval/walltime': 16.607625484466553, 'eval/episode_goal_distance': (Array(0.6386247, dtype=float32), Array(0.3701172, dtype=float32)), 'eval/episode_reward': (Array(-12395.547, dtype=float32), Array(6611.037, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.75836, dtype=float32)), 'eval/epoch_eval_time': 16.607625484466553, 'eval/sps': 7707.302896474935}
I0727 18:53:34.119379 139998383736640 train.py:379] starting iteration 0 24.91146445274353
I0727 18:54:43.819950 139998383736640 train.py:394] {'eval/walltime': 20.781572580337524, 'training/sps': 6251.612744240325, 'training/walltime': 65.51909351348877, 'training/entropy_loss': Array(-0.04694344, dtype=float32), 'training/policy_loss': Array(0.02449017, dtype=float32), 'training/total_loss': Array(178.68433, dtype=float32), 'training/v_loss': Array(178.70679, dtype=float32), 'eval/episode_goal_distance': (Array(0.7668441, dtype=float32), Array(0.40274534, dtype=float32)), 'eval/episode_reward': (Array(-14126.15, dtype=float32), Array(6540.137, dtype=float32)), 'eval/avg_episode_length': (Array(945.7031, dtype=float32), Array(225.74544, dtype=float32)), 'eval/epoch_eval_time': 4.173947095870972, 'eval/sps': 30666.416478211355}
I0727 18:54:43.890326 139998383736640 train.py:379] starting iteration 1 94.68239331245422
I0727 18:55:09.382814 139998383736640 train.py:394] {'eval/walltime': 25.121127367019653, 'training/sps': 19370.62749178862, 'training/walltime': 86.66451072692871, 'training/entropy_loss': Array(-0.04486692, dtype=float32), 'training/policy_loss': Array(0.0109196, dtype=float32), 'training/total_loss': Array(221.99117, dtype=float32), 'training/v_loss': Array(222.0251, dtype=float32), 'eval/episode_goal_distance': (Array(0.7693203, dtype=float32), Array(0.41107836, dtype=float32)), 'eval/episode_reward': (Array(-14014.769, dtype=float32), Array(6438.669, dtype=float32)), 'eval/avg_episode_length': (Array(945.625, dtype=float32), Array(226.0705, dtype=float32)), 'eval/epoch_eval_time': 4.339554786682129, 'eval/sps': 29496.11337845657}
I0727 18:55:09.386894 139998383736640 train.py:379] starting iteration 2 120.17898082733154
I0727 18:55:35.050594 139998383736640 train.py:394] {'eval/walltime': 29.548871278762817, 'training/sps': 19293.527828997667, 'training/walltime': 107.89442801475525, 'training/entropy_loss': Array(-0.0409192, dtype=float32), 'training/policy_loss': Array(0.01118998, dtype=float32), 'training/total_loss': Array(254.33444, dtype=float32), 'training/v_loss': Array(254.36417, dtype=float32), 'eval/episode_goal_distance': (Array(0.73668396, dtype=float32), Array(0.403793, dtype=float32)), 'eval/episode_reward': (Array(-13561.088, dtype=float32), Array(6507.7188, dtype=float32)), 'eval/avg_episode_length': (Array(930.1328, dtype=float32), Array(254.054, dtype=float32)), 'eval/epoch_eval_time': 4.427743911743164, 'eval/sps': 28908.627633256125}
I0727 18:55:35.054059 139998383736640 train.py:379] starting iteration 3 145.84614634513855
I0727 18:56:01.283656 139998383736640 train.py:394] {'eval/walltime': 33.987266540527344, 'training/sps': 18801.90995589507, 'training/walltime': 129.67944884300232, 'training/entropy_loss': Array(-0.03756037, dtype=float32), 'training/policy_loss': Array(0.01247831, dtype=float32), 'training/total_loss': Array(322.36646, dtype=float32), 'training/v_loss': Array(322.39154, dtype=float32), 'eval/episode_goal_distance': (Array(0.7399824, dtype=float32), Array(0.41150874, dtype=float32)), 'eval/episode_reward': (Array(-13460.441, dtype=float32), Array(6439.5127, dtype=float32)), 'eval/avg_episode_length': (Array(930.1094, dtype=float32), Array(254.1394, dtype=float32)), 'eval/epoch_eval_time': 4.438395261764526, 'eval/sps': 28839.25212850745}
I0727 18:56:01.287198 139998383736640 train.py:379] starting iteration 4 172.07928466796875
I0727 18:56:28.139315 139998383736640 train.py:394] {'eval/walltime': 38.3931565284729, 'training/sps': 18253.148861087273, 'training/walltime': 152.11941266059875, 'training/entropy_loss': Array(-0.03501093, dtype=float32), 'training/policy_loss': Array(0.01768562, dtype=float32), 'training/total_loss': Array(432.4969, dtype=float32), 'training/v_loss': Array(432.51416, dtype=float32), 'eval/episode_goal_distance': (Array(0.65874344, dtype=float32), Array(0.43043834, dtype=float32)), 'eval/episode_reward': (Array(-12072.039, dtype=float32), Array(6894.202, dtype=float32)), 'eval/avg_episode_length': (Array(906.8281, dtype=float32), Array(289.68344, dtype=float32)), 'eval/epoch_eval_time': 4.405889987945557, 'eval/sps': 29052.01908132203}
I0727 18:56:28.142822 139998383736640 train.py:379] starting iteration 5 198.93490958213806
I0727 18:56:55.250682 139998383736640 train.py:394] {'eval/walltime': 42.80272960662842, 'training/sps': 18050.246852663866, 'training/walltime': 174.81162309646606, 'training/entropy_loss': Array(-0.02890537, dtype=float32), 'training/policy_loss': Array(0.01282102, dtype=float32), 'training/total_loss': Array(145.16891, dtype=float32), 'training/v_loss': Array(145.185, dtype=float32), 'eval/episode_goal_distance': (Array(0.6668564, dtype=float32), Array(0.36994877, dtype=float32)), 'eval/episode_reward': (Array(-12176.254, dtype=float32), Array(5794.8164, dtype=float32)), 'eval/avg_episode_length': (Array(914.64844, dtype=float32), Array(278.36145, dtype=float32)), 'eval/epoch_eval_time': 4.409573078155518, 'eval/sps': 29027.75341996173}
I0727 18:56:55.254259 139998383736640 train.py:379] starting iteration 6 226.04634499549866
I0727 18:57:22.576487 139998383736640 train.py:394] {'eval/walltime': 47.23613142967224, 'training/sps': 17900.051813646955, 'training/walltime': 197.69423842430115, 'training/entropy_loss': Array(-0.02590737, dtype=float32), 'training/policy_loss': Array(0.02573853, dtype=float32), 'training/total_loss': Array(226.66833, dtype=float32), 'training/v_loss': Array(226.66852, dtype=float32), 'eval/episode_goal_distance': (Array(0.43360448, dtype=float32), Array(0.16433422, dtype=float32)), 'eval/episode_reward': (Array(-9476.722, dtype=float32), Array(3710.117, dtype=float32)), 'eval/avg_episode_length': (Array(945.6406, dtype=float32), Array(226.00545, dtype=float32)), 'eval/epoch_eval_time': 4.433401823043823, 'eval/sps': 28871.734417278592}
I0727 18:57:22.580164 139998383736640 train.py:379] starting iteration 7 253.37225127220154
I0727 18:57:50.021420 139998383736640 train.py:394] {'eval/walltime': 51.67667317390442, 'training/sps': 17812.649115585387, 'training/walltime': 220.689133644104, 'training/entropy_loss': Array(-0.02318424, dtype=float32), 'training/policy_loss': Array(0.02704474, dtype=float32), 'training/total_loss': Array(206.32935, dtype=float32), 'training/v_loss': Array(206.32547, dtype=float32), 'eval/episode_goal_distance': (Array(0.41650864, dtype=float32), Array(0.17680566, dtype=float32)), 'eval/episode_reward': (Array(-9087.744, dtype=float32), Array(4122.473, dtype=float32)), 'eval/avg_episode_length': (Array(914.6328, dtype=float32), Array(278.4126, dtype=float32)), 'eval/epoch_eval_time': 4.440541744232178, 'eval/sps': 28825.311723791197}
I0727 18:57:50.024924 139998383736640 train.py:379] starting iteration 8 280.8170111179352
I0727 18:58:17.491817 139998383736640 train.py:394] {'eval/walltime': 56.08007049560547, 'training/sps': 17764.383851510294, 'training/walltime': 243.74650526046753, 'training/entropy_loss': Array(-0.02272272, dtype=float32), 'training/policy_loss': Array(0.01658968, dtype=float32), 'training/total_loss': Array(187.29912, dtype=float32), 'training/v_loss': Array(187.30525, dtype=float32), 'eval/episode_goal_distance': (Array(0.41977823, dtype=float32), Array(0.14825715, dtype=float32)), 'eval/episode_reward': (Array(-9533.346, dtype=float32), Array(3599.7537, dtype=float32)), 'eval/avg_episode_length': (Array(953.41406, dtype=float32), Array(210.06798, dtype=float32)), 'eval/epoch_eval_time': 4.40339732170105, 'eval/sps': 29068.46478948965}
I0727 18:58:17.495287 139998383736640 train.py:379] starting iteration 9 308.2873749732971
I0727 18:58:45.032164 139998383736640 train.py:394] {'eval/walltime': 60.495450019836426, 'training/sps': 17719.85409455931, 'training/walltime': 266.8618197441101, 'training/entropy_loss': Array(-0.02020764, dtype=float32), 'training/policy_loss': Array(0.01533364, dtype=float32), 'training/total_loss': Array(172.97528, dtype=float32), 'training/v_loss': Array(172.98016, dtype=float32), 'eval/episode_goal_distance': (Array(0.3455872, dtype=float32), Array(0.12263467, dtype=float32)), 'eval/episode_reward': (Array(-8013.434, dtype=float32), Array(3780.6675, dtype=float32)), 'eval/avg_episode_length': (Array(914.59375, dtype=float32), Array(278.53973, dtype=float32)), 'eval/epoch_eval_time': 4.415379524230957, 'eval/sps': 28989.580464726696}
I0727 18:58:45.035692 139998383736640 train.py:379] starting iteration 10 335.82777881622314
I0727 18:59:12.637772 139998383736640 train.py:394] {'eval/walltime': 64.98103356361389, 'training/sps': 17723.51295370153, 'training/walltime': 289.97236227989197, 'training/entropy_loss': Array(-0.02024353, dtype=float32), 'training/policy_loss': Array(-0.00151351, dtype=float32), 'training/total_loss': Array(123.44772, dtype=float32), 'training/v_loss': Array(123.46948, dtype=float32), 'eval/episode_goal_distance': (Array(0.34882167, dtype=float32), Array(0.14356413, dtype=float32)), 'eval/episode_reward': (Array(-7832.8896, dtype=float32), Array(3724.1084, dtype=float32)), 'eval/avg_episode_length': (Array(914.59375, dtype=float32), Array(278.53946, dtype=float32)), 'eval/epoch_eval_time': 4.485583543777466, 'eval/sps': 28535.86356173555}
I0727 18:59:12.641289 139998383736640 train.py:379] starting iteration 11 363.43337631225586
I0727 18:59:40.226811 139998383736640 train.py:394] {'eval/walltime': 69.3883752822876, 'training/sps': 17676.561652514694, 'training/walltime': 313.1442894935608, 'training/entropy_loss': Array(-0.0197561, dtype=float32), 'training/policy_loss': Array(-0.00093851, dtype=float32), 'training/total_loss': Array(76.21457, dtype=float32), 'training/v_loss': Array(76.23527, dtype=float32), 'eval/episode_goal_distance': (Array(0.36810005, dtype=float32), Array(0.1359969, dtype=float32)), 'eval/episode_reward': (Array(-8599.051, dtype=float32), Array(3609.7964, dtype=float32)), 'eval/avg_episode_length': (Array(922.33594, dtype=float32), Array(266.78525, dtype=float32)), 'eval/epoch_eval_time': 4.407341718673706, 'eval/sps': 29042.449660227125}
I0727 18:59:40.230421 139998383736640 train.py:379] starting iteration 12 391.0225079059601
I0727 19:00:07.801948 139998383736640 train.py:394] {'eval/walltime': 73.81057238578796, 'training/sps': 17698.511497490807, 'training/walltime': 336.287478685379, 'training/entropy_loss': Array(-0.01749112, dtype=float32), 'training/policy_loss': Array(0.00025971, dtype=float32), 'training/total_loss': Array(64.641106, dtype=float32), 'training/v_loss': Array(64.65834, dtype=float32), 'eval/episode_goal_distance': (Array(0.3717769, dtype=float32), Array(0.12571768, dtype=float32)), 'eval/episode_reward': (Array(-8069.0103, dtype=float32), Array(4175.9443, dtype=float32)), 'eval/avg_episode_length': (Array(852.46875, dtype=float32), Array(353.3632, dtype=float32)), 'eval/epoch_eval_time': 4.422197103500366, 'eval/sps': 28944.88802832472}
I0727 19:00:07.805532 139998383736640 train.py:379] starting iteration 13 418.5976197719574
I0727 19:00:35.391014 139998383736640 train.py:394] {'eval/walltime': 78.23654246330261, 'training/sps': 17690.454259108144, 'training/walltime': 359.4412086009979, 'training/entropy_loss': Array(-0.01484823, dtype=float32), 'training/policy_loss': Array(0.00067168, dtype=float32), 'training/total_loss': Array(61.864525, dtype=float32), 'training/v_loss': Array(61.878696, dtype=float32), 'eval/episode_goal_distance': (Array(0.35686597, dtype=float32), Array(0.12342586, dtype=float32)), 'eval/episode_reward': (Array(-7727.6753, dtype=float32), Array(4038.4998, dtype=float32)), 'eval/avg_episode_length': (Array(867.9453, dtype=float32), Array(337.43604, dtype=float32)), 'eval/epoch_eval_time': 4.425970077514648, 'eval/sps': 28920.213593463086}
I0727 19:00:35.394466 139998383736640 train.py:379] starting iteration 14 446.1865539550781
I0727 19:01:02.842081 139998383736640 train.py:394] {'eval/walltime': 82.65374565124512, 'training/sps': 17789.825000013978, 'training/walltime': 382.4656059741974, 'training/entropy_loss': Array(-0.00909012, dtype=float32), 'training/policy_loss': Array(0.00144626, dtype=float32), 'training/total_loss': Array(61.475014, dtype=float32), 'training/v_loss': Array(61.482655, dtype=float32), 'eval/episode_goal_distance': (Array(0.3517409, dtype=float32), Array(0.11313813, dtype=float32)), 'eval/episode_reward': (Array(-7892.1934, dtype=float32), Array(3976.9678, dtype=float32)), 'eval/avg_episode_length': (Array(868.08594, dtype=float32), Array(337.0769, dtype=float32)), 'eval/epoch_eval_time': 4.417203187942505, 'eval/sps': 28977.61197614758}
I0727 19:01:02.845552 139998383736640 train.py:379] starting iteration 15 473.6376395225525
I0727 19:01:30.416320 139998383736640 train.py:394] {'eval/walltime': 87.09660005569458, 'training/sps': 17715.004390941653, 'training/walltime': 405.5872485637665, 'training/entropy_loss': Array(-0.00207415, dtype=float32), 'training/policy_loss': Array(0.00106162, dtype=float32), 'training/total_loss': Array(90.42801, dtype=float32), 'training/v_loss': Array(90.429016, dtype=float32), 'eval/episode_goal_distance': (Array(0.3511733, dtype=float32), Array(0.10431756, dtype=float32)), 'eval/episode_reward': (Array(-7884.8994, dtype=float32), Array(3508.5234, dtype=float32)), 'eval/avg_episode_length': (Array(898.9922, dtype=float32), Array(300.4225, dtype=float32)), 'eval/epoch_eval_time': 4.442854404449463, 'eval/sps': 28810.307146641946}
I0727 19:01:30.419978 139998383736640 train.py:379] starting iteration 16 501.21206521987915
I0727 19:01:58.013867 139998383736640 train.py:394] {'eval/walltime': 91.53724360466003, 'training/sps': 17695.600378799973, 'training/walltime': 428.7342450618744, 'training/entropy_loss': Array(0.00469417, dtype=float32), 'training/policy_loss': Array(0.0014509, dtype=float32), 'training/total_loss': Array(59.588196, dtype=float32), 'training/v_loss': Array(59.58205, dtype=float32), 'eval/episode_goal_distance': (Array(0.3457908, dtype=float32), Array(0.11048956, dtype=float32)), 'eval/episode_reward': (Array(-8171.095, dtype=float32), Array(3259.667, dtype=float32)), 'eval/avg_episode_length': (Array(930.10156, dtype=float32), Array(254.16763, dtype=float32)), 'eval/epoch_eval_time': 4.440643548965454, 'eval/sps': 28824.650884176557}
I0727 19:01:58.017452 139998383736640 train.py:379] starting iteration 17 528.8095395565033
I0727 19:02:25.600445 139998383736640 train.py:394] {'eval/walltime': 95.95858430862427, 'training/sps': 17689.09980515277, 'training/walltime': 451.8897478580475, 'training/entropy_loss': Array(0.00959009, dtype=float32), 'training/policy_loss': Array(0.00143176, dtype=float32), 'training/total_loss': Array(49.291252, dtype=float32), 'training/v_loss': Array(49.28023, dtype=float32), 'eval/episode_goal_distance': (Array(0.33482316, dtype=float32), Array(0.1086527, dtype=float32)), 'eval/episode_reward': (Array(-7823.334, dtype=float32), Array(3630.023, dtype=float32)), 'eval/avg_episode_length': (Array(906.84375, dtype=float32), Array(289.6351, dtype=float32)), 'eval/epoch_eval_time': 4.421340703964233, 'eval/sps': 28950.494560447125}
I0727 19:02:25.604001 139998383736640 train.py:379] starting iteration 18 556.3960886001587
I0727 19:02:53.208024 139998383736640 train.py:394] {'eval/walltime': 100.38605523109436, 'training/sps': 17677.626419580545, 'training/walltime': 475.06027936935425, 'training/entropy_loss': Array(0.01451528, dtype=float32), 'training/policy_loss': Array(0.00098396, dtype=float32), 'training/total_loss': Array(45.508797, dtype=float32), 'training/v_loss': Array(45.493294, dtype=float32), 'eval/episode_goal_distance': (Array(0.34048447, dtype=float32), Array(0.11480769, dtype=float32)), 'eval/episode_reward': (Array(-7704.8496, dtype=float32), Array(3790.0242, dtype=float32)), 'eval/avg_episode_length': (Array(891.2578, dtype=float32), Array(310.30374, dtype=float32)), 'eval/epoch_eval_time': 4.427470922470093, 'eval/sps': 28910.410083187762}
I0727 19:02:53.211561 139998383736640 train.py:379] starting iteration 19 584.003648519516
I0727 19:03:20.819502 139998383736640 train.py:394] {'eval/walltime': 104.80572628974915, 'training/sps': 17668.609968707708, 'training/walltime': 498.242635011673, 'training/entropy_loss': Array(0.02065904, dtype=float32), 'training/policy_loss': Array(0.00140083, dtype=float32), 'training/total_loss': Array(43.15268, dtype=float32), 'training/v_loss': Array(43.13062, dtype=float32), 'eval/episode_goal_distance': (Array(0.32496154, dtype=float32), Array(0.0968913, dtype=float32)), 'eval/episode_reward': (Array(-7841.404, dtype=float32), Array(3275.1238, dtype=float32)), 'eval/avg_episode_length': (Array(930.0703, dtype=float32), Array(254.28116, dtype=float32)), 'eval/epoch_eval_time': 4.419671058654785, 'eval/sps': 28961.431360224204}
I0727 19:03:20.823047 139998383736640 train.py:379] starting iteration 20 611.6151347160339
I0727 19:03:48.438509 139998383736640 train.py:394] {'eval/walltime': 109.2514181137085, 'training/sps': 17682.882288942412, 'training/walltime': 521.4062795639038, 'training/entropy_loss': Array(0.02761851, dtype=float32), 'training/policy_loss': Array(0.00119511, dtype=float32), 'training/total_loss': Array(73.66098, dtype=float32), 'training/v_loss': Array(73.63217, dtype=float32), 'eval/episode_goal_distance': (Array(0.33676824, dtype=float32), Array(0.08814625, dtype=float32)), 'eval/episode_reward': (Array(-7907.0156, dtype=float32), Array(3351.1206, dtype=float32)), 'eval/avg_episode_length': (Array(906.7344, dtype=float32), Array(289.97485, dtype=float32)), 'eval/epoch_eval_time': 4.445691823959351, 'eval/sps': 28791.91924869023}
I0727 19:03:48.441986 139998383736640 train.py:379] starting iteration 21 639.234073638916
I0727 19:04:16.071282 139998383736640 train.py:394] {'eval/walltime': 113.69408535957336, 'training/sps': 17669.797358279357, 'training/walltime': 544.5870773792267, 'training/entropy_loss': Array(0.03544104, dtype=float32), 'training/policy_loss': Array(0.00204188, dtype=float32), 'training/total_loss': Array(34.998817, dtype=float32), 'training/v_loss': Array(34.961334, dtype=float32), 'eval/episode_goal_distance': (Array(0.32993317, dtype=float32), Array(0.09378651, dtype=float32)), 'eval/episode_reward': (Array(-7924.7275, dtype=float32), Array(3376.0952, dtype=float32)), 'eval/avg_episode_length': (Array(914.53125, dtype=float32), Array(278.74326, dtype=float32)), 'eval/epoch_eval_time': 4.442667245864868, 'eval/sps': 28811.520853635717}
I0727 19:04:16.074816 139998383736640 train.py:379] starting iteration 22 666.8669030666351
I0727 19:04:43.730491 139998383736640 train.py:394] {'eval/walltime': 118.14825129508972, 'training/sps': 17658.737230942344, 'training/walltime': 567.7823939323425, 'training/entropy_loss': Array(0.04471857, dtype=float32), 'training/policy_loss': Array(0.00210711, dtype=float32), 'training/total_loss': Array(28.675797, dtype=float32), 'training/v_loss': Array(28.628971, dtype=float32), 'eval/episode_goal_distance': (Array(0.31403205, dtype=float32), Array(0.0894157, dtype=float32)), 'eval/episode_reward': (Array(-7538.985, dtype=float32), Array(3378.0193, dtype=float32)), 'eval/avg_episode_length': (Array(906.78125, dtype=float32), Array(289.82907, dtype=float32)), 'eval/epoch_eval_time': 4.454165935516357, 'eval/sps': 28737.142228887653}
I0727 19:04:43.734052 139998383736640 train.py:379] starting iteration 23 694.5261399745941
I0727 19:05:11.398917 139998383736640 train.py:394] {'eval/walltime': 122.60270047187805, 'training/sps': 17651.992191471858, 'training/walltime': 590.9865736961365, 'training/entropy_loss': Array(0.05294954, dtype=float32), 'training/policy_loss': Array(0.00216643, dtype=float32), 'training/total_loss': Array(25.70521, dtype=float32), 'training/v_loss': Array(25.650095, dtype=float32), 'eval/episode_goal_distance': (Array(0.3161887, dtype=float32), Array(0.08367714, dtype=float32)), 'eval/episode_reward': (Array(-6821.5723, dtype=float32), Array(3665.2322, dtype=float32)), 'eval/avg_episode_length': (Array(836.9922, dtype=float32), Array(367.952, dtype=float32)), 'eval/epoch_eval_time': 4.45444917678833, 'eval/sps': 28735.314944661317}
I0727 19:05:11.402411 139998383736640 train.py:379] starting iteration 24 722.1944990158081
I0727 19:05:39.047387 139998383736640 train.py:394] {'eval/walltime': 127.03818392753601, 'training/sps': 17652.62501721735, 'training/walltime': 614.1899216175079, 'training/entropy_loss': Array(0.06075617, dtype=float32), 'training/policy_loss': Array(0.00291717, dtype=float32), 'training/total_loss': Array(23.633663, dtype=float32), 'training/v_loss': Array(23.56999, dtype=float32), 'eval/episode_goal_distance': (Array(0.3193537, dtype=float32), Array(0.09199837, dtype=float32)), 'eval/episode_reward': (Array(-7130.8545, dtype=float32), Array(3728.8743, dtype=float32)), 'eval/avg_episode_length': (Array(860.33594, dtype=float32), Array(345.25934, dtype=float32)), 'eval/epoch_eval_time': 4.435483455657959, 'eval/sps': 28858.184520274015}
I0727 19:05:39.050879 139998383736640 train.py:379] starting iteration 25 749.8429665565491
I0727 19:06:06.722321 139998383736640 train.py:394] {'eval/walltime': 131.4921796321869, 'training/sps': 17646.430029661944, 'training/walltime': 637.401415348053, 'training/entropy_loss': Array(0.07049563, dtype=float32), 'training/policy_loss': Array(0.00368057, dtype=float32), 'training/total_loss': Array(59.488518, dtype=float32), 'training/v_loss': Array(59.414345, dtype=float32), 'eval/episode_goal_distance': (Array(0.31949744, dtype=float32), Array(0.08930559, dtype=float32)), 'eval/episode_reward': (Array(-7398.467, dtype=float32), Array(3434.5608, dtype=float32)), 'eval/avg_episode_length': (Array(891.35156, dtype=float32), Array(310.0365, dtype=float32)), 'eval/epoch_eval_time': 4.453995704650879, 'eval/sps': 28738.24055697717}
I0727 19:06:06.775879 139998383736640 train.py:379] starting iteration 26 777.5679485797882
I0727 19:06:34.325289 139998383736640 train.py:394] {'eval/walltime': 135.8967912197113, 'training/sps': 17702.503560079916, 'training/walltime': 660.5393855571747, 'training/entropy_loss': Array(0.07981646, dtype=float32), 'training/policy_loss': Array(0.00431825, dtype=float32), 'training/total_loss': Array(19.967133, dtype=float32), 'training/v_loss': Array(19.882998, dtype=float32), 'eval/episode_goal_distance': (Array(0.32300013, dtype=float32), Array(0.09275701, dtype=float32)), 'eval/episode_reward': (Array(-7706.7837, dtype=float32), Array(3436.0422, dtype=float32)), 'eval/avg_episode_length': (Array(906.7344, dtype=float32), Array(289.97522, dtype=float32)), 'eval/epoch_eval_time': 4.404611587524414, 'eval/sps': 29060.45117861156}
I0727 19:06:34.328934 139998383736640 train.py:379] starting iteration 27 805.1210215091705
I0727 19:07:01.919409 139998383736640 train.py:394] {'eval/walltime': 140.3282549381256, 'training/sps': 17691.432341706583, 'training/walltime': 683.6918354034424, 'training/entropy_loss': Array(0.08840612, dtype=float32), 'training/policy_loss': Array(0.00262508, dtype=float32), 'training/total_loss': Array(18.702257, dtype=float32), 'training/v_loss': Array(18.611225, dtype=float32), 'eval/episode_goal_distance': (Array(0.32267654, dtype=float32), Array(0.08678137, dtype=float32)), 'eval/episode_reward': (Array(-7387.3643, dtype=float32), Array(3566.9766, dtype=float32)), 'eval/avg_episode_length': (Array(875.7031, dtype=float32), Array(328.85913, dtype=float32)), 'eval/epoch_eval_time': 4.431463718414307, 'eval/sps': 28884.361496205984}
I0727 19:07:01.922805 139998383736640 train.py:379] starting iteration 28 832.7148923873901
I0727 19:07:29.531851 139998383736640 train.py:394] {'eval/walltime': 144.75925016403198, 'training/sps': 17676.720068053626, 'training/walltime': 706.8635549545288, 'training/entropy_loss': Array(0.09582746, dtype=float32), 'training/policy_loss': Array(0.00427417, dtype=float32), 'training/total_loss': Array(18.07127, dtype=float32), 'training/v_loss': Array(17.971169, dtype=float32), 'eval/episode_goal_distance': (Array(0.32535097, dtype=float32), Array(0.08941815, dtype=float32)), 'eval/episode_reward': (Array(-7374.2607, dtype=float32), Array(3752.1455, dtype=float32)), 'eval/avg_episode_length': (Array(860.25, dtype=float32), Array(345.472, dtype=float32)), 'eval/epoch_eval_time': 4.430995225906372, 'eval/sps': 28887.415461797806}
I0727 19:07:29.535274 139998383736640 train.py:379] starting iteration 29 860.3273606300354
I0727 19:07:57.175625 139998383736640 train.py:394] {'eval/walltime': 149.19774675369263, 'training/sps': 17658.835791168018, 'training/walltime': 730.0587420463562, 'training/entropy_loss': Array(0.10088819, dtype=float32), 'training/policy_loss': Array(0.00463943, dtype=float32), 'training/total_loss': Array(17.421368, dtype=float32), 'training/v_loss': Array(17.315842, dtype=float32), 'eval/episode_goal_distance': (Array(0.3275866, dtype=float32), Array(0.09215602, dtype=float32)), 'eval/episode_reward': (Array(-7053.5967, dtype=float32), Array(3843.1296, dtype=float32)), 'eval/avg_episode_length': (Array(844.60156, dtype=float32), Array(361.11386, dtype=float32)), 'eval/epoch_eval_time': 4.4384965896606445, 'eval/sps': 28838.59374775064}
I0727 19:07:57.180868 139998383736640 train.py:379] starting iteration 30 887.972948551178
I0727 19:08:24.784775 139998383736640 train.py:394] {'eval/walltime': 153.61774373054504, 'training/sps': 17672.389307637524, 'training/walltime': 753.2361400127411, 'training/entropy_loss': Array(0.10429433, dtype=float32), 'training/policy_loss': Array(0.0044134, dtype=float32), 'training/total_loss': Array(62.37171, dtype=float32), 'training/v_loss': Array(62.263004, dtype=float32), 'eval/episode_goal_distance': (Array(0.32926202, dtype=float32), Array(0.10237737, dtype=float32)), 'eval/episode_reward': (Array(-7638.0146, dtype=float32), Array(4021.945, dtype=float32)), 'eval/avg_episode_length': (Array(867.9922, dtype=float32), Array(337.3163, dtype=float32)), 'eval/epoch_eval_time': 4.419996976852417, 'eval/sps': 28959.295825390313}
I0727 19:08:24.788302 139998383736640 train.py:379] starting iteration 31 915.5803887844086
I0727 19:08:52.463525 139998383736640 train.py:394] {'eval/walltime': 158.05073189735413, 'training/sps': 17627.699117305227, 'training/walltime': 776.4722979068756, 'training/entropy_loss': Array(0.1067636, dtype=float32), 'training/policy_loss': Array(0.00341989, dtype=float32), 'training/total_loss': Array(17.363096, dtype=float32), 'training/v_loss': Array(17.252914, dtype=float32), 'eval/episode_goal_distance': (Array(0.3297841, dtype=float32), Array(0.08469658, dtype=float32)), 'eval/episode_reward': (Array(-7506.536, dtype=float32), Array(3484.6133, dtype=float32)), 'eval/avg_episode_length': (Array(883.3906, dtype=float32), Array(320.05722, dtype=float32)), 'eval/epoch_eval_time': 4.432988166809082, 'eval/sps': 28874.428530707297}
I0727 19:08:52.466999 139998383736640 train.py:379] starting iteration 32 943.2590856552124
I0727 19:09:20.079659 139998383736640 train.py:394] {'eval/walltime': 162.4839789867401, 'training/sps': 17675.7239707061, 'training/walltime': 799.6453232765198, 'training/entropy_loss': Array(0.10979442, dtype=float32), 'training/policy_loss': Array(0.00505993, dtype=float32), 'training/total_loss': Array(16.250433, dtype=float32), 'training/v_loss': Array(16.135578, dtype=float32), 'eval/episode_goal_distance': (Array(0.3218801, dtype=float32), Array(0.08413668, dtype=float32)), 'eval/episode_reward': (Array(-7892.199, dtype=float32), Array(3090.7627, dtype=float32)), 'eval/avg_episode_length': (Array(930.1406, dtype=float32), Array(254.0258, dtype=float32)), 'eval/epoch_eval_time': 4.433247089385986, 'eval/sps': 28872.742127650785}
I0727 19:09:20.083137 139998383736640 train.py:379] starting iteration 33 970.8752243518829
I0727 19:09:47.608335 139998383736640 train.py:394] {'eval/walltime': 166.92888569831848, 'training/sps': 17751.596448459208, 'training/walltime': 822.7193043231964, 'training/entropy_loss': Array(0.11249243, dtype=float32), 'training/policy_loss': Array(0.00469843, dtype=float32), 'training/total_loss': Array(16.141415, dtype=float32), 'training/v_loss': Array(16.024223, dtype=float32), 'eval/episode_goal_distance': (Array(0.33008903, dtype=float32), Array(0.0892185, dtype=float32)), 'eval/episode_reward': (Array(-7009.961, dtype=float32), Array(4102.163, dtype=float32)), 'eval/avg_episode_length': (Array(805.90625, dtype=float32), Array(393.96826, dtype=float32)), 'eval/epoch_eval_time': 4.444906711578369, 'eval/sps': 28797.0048205011}
I0727 19:09:47.611969 139998383736640 train.py:379] starting iteration 34 998.4040563106537
I0727 19:10:15.167621 139998383736640 train.py:394] {'eval/walltime': 171.33897614479065, 'training/sps': 17701.45567452714, 'training/walltime': 845.858644247055, 'training/entropy_loss': Array(0.1159851, dtype=float32), 'training/policy_loss': Array(0.00334755, dtype=float32), 'training/total_loss': Array(15.359009, dtype=float32), 'training/v_loss': Array(15.2396755, dtype=float32), 'eval/episode_goal_distance': (Array(0.30908495, dtype=float32), Array(0.1084656, dtype=float32)), 'eval/episode_reward': (Array(-6943.621, dtype=float32), Array(3978.355, dtype=float32)), 'eval/avg_episode_length': (Array(860.27344, dtype=float32), Array(345.41418, dtype=float32)), 'eval/epoch_eval_time': 4.410090446472168, 'eval/sps': 29024.34803857436}
I0727 19:10:15.171015 139998383736640 train.py:379] starting iteration 35 1025.9631016254425
I0727 19:10:42.755824 139998383736640 train.py:394] {'eval/walltime': 175.75328969955444, 'training/sps': 17682.513188007975, 'training/walltime': 869.0227723121643, 'training/entropy_loss': Array(0.11646301, dtype=float32), 'training/policy_loss': Array(0.00318971, dtype=float32), 'training/total_loss': Array(58.534454, dtype=float32), 'training/v_loss': Array(58.414803, dtype=float32), 'eval/episode_goal_distance': (Array(0.30751562, dtype=float32), Array(0.08325396, dtype=float32)), 'eval/episode_reward': (Array(-7264.2666, dtype=float32), Array(3259.7302, dtype=float32)), 'eval/avg_episode_length': (Array(898.9844, dtype=float32), Array(300.446, dtype=float32)), 'eval/epoch_eval_time': 4.414313554763794, 'eval/sps': 28996.580875381238}
I0727 19:10:42.759187 139998383736640 train.py:379] starting iteration 36 1053.551274061203
I0727 19:11:10.355581 139998383736640 train.py:394] {'eval/walltime': 180.18301844596863, 'training/sps': 17685.525050331813, 'training/walltime': 892.1829555034637, 'training/entropy_loss': Array(0.11526582, dtype=float32), 'training/policy_loss': Array(0.00436175, dtype=float32), 'training/total_loss': Array(16.055603, dtype=float32), 'training/v_loss': Array(15.935977, dtype=float32), 'eval/episode_goal_distance': (Array(0.32603538, dtype=float32), Array(0.09380796, dtype=float32)), 'eval/episode_reward': (Array(-7345.8125, dtype=float32), Array(3705.7083, dtype=float32)), 'eval/avg_episode_length': (Array(875.6875, dtype=float32), Array(328.90033, dtype=float32)), 'eval/epoch_eval_time': 4.429728746414185, 'eval/sps': 28895.674504587794}
I0727 19:11:10.359069 139998383736640 train.py:379] starting iteration 37 1081.151155948639
I0727 19:11:38.009661 139998383736640 train.py:394] {'eval/walltime': 184.6239984035492, 'training/sps': 17652.65875468751, 'training/walltime': 915.3862590789795, 'training/entropy_loss': Array(0.11408436, dtype=float32), 'training/policy_loss': Array(0.00433671, dtype=float32), 'training/total_loss': Array(14.715249, dtype=float32), 'training/v_loss': Array(14.596827, dtype=float32), 'eval/episode_goal_distance': (Array(0.3311796, dtype=float32), Array(0.08883246, dtype=float32)), 'eval/episode_reward': (Array(-7345.616, dtype=float32), Array(3767.0527, dtype=float32)), 'eval/avg_episode_length': (Array(852.40625, dtype=float32), Array(353.5128, dtype=float32)), 'eval/epoch_eval_time': 4.440979957580566, 'eval/sps': 28822.467388421643}
I0727 19:11:38.013212 139998383736640 train.py:379] starting iteration 38 1108.805299282074
I0727 19:12:05.652252 139998383736640 train.py:394] {'eval/walltime': 189.05328750610352, 'training/sps': 17654.14478237386, 'training/walltime': 938.5876095294952, 'training/entropy_loss': Array(0.11174098, dtype=float32), 'training/policy_loss': Array(0.00311541, dtype=float32), 'training/total_loss': Array(14.969544, dtype=float32), 'training/v_loss': Array(14.854689, dtype=float32), 'eval/episode_goal_distance': (Array(0.32923558, dtype=float32), Array(0.08815375, dtype=float32)), 'eval/episode_reward': (Array(-7634.1455, dtype=float32), Array(3574.2117, dtype=float32)), 'eval/avg_episode_length': (Array(875.71094, dtype=float32), Array(328.83908, dtype=float32)), 'eval/epoch_eval_time': 4.429289102554321, 'eval/sps': 28898.542641116797}
I0727 19:12:05.655812 139998383736640 train.py:379] starting iteration 39 1136.4478991031647
I0727 19:12:33.320785 139998383736640 train.py:394] {'eval/walltime': 193.50847554206848, 'training/sps': 17652.53251191234, 'training/walltime': 961.791079044342, 'training/entropy_loss': Array(0.10938647, dtype=float32), 'training/policy_loss': Array(0.00336202, dtype=float32), 'training/total_loss': Array(13.991616, dtype=float32), 'training/v_loss': Array(13.878868, dtype=float32), 'eval/episode_goal_distance': (Array(0.3362294, dtype=float32), Array(0.09579216, dtype=float32)), 'eval/episode_reward': (Array(-7048.51, dtype=float32), Array(4147.0723, dtype=float32)), 'eval/avg_episode_length': (Array(813.6328, dtype=float32), Array(387.9549, dtype=float32)), 'eval/epoch_eval_time': 4.455188035964966, 'eval/sps': 28730.54941041922}
I0727 19:12:33.324255 139998383736640 train.py:379] starting iteration 40 1164.116342306137
I0727 19:13:00.965072 139998383736640 train.py:394] {'eval/walltime': 197.94919109344482, 'training/sps': 17659.871375331342, 'training/walltime': 984.9849059581757, 'training/entropy_loss': Array(0.10676868, dtype=float32), 'training/policy_loss': Array(0.00340433, dtype=float32), 'training/total_loss': Array(58.937187, dtype=float32), 'training/v_loss': Array(58.827007, dtype=float32), 'eval/episode_goal_distance': (Array(0.3316841, dtype=float32), Array(0.08596966, dtype=float32)), 'eval/episode_reward': (Array(-7868.7983, dtype=float32), Array(3414.6833, dtype=float32)), 'eval/avg_episode_length': (Array(899., dtype=float32), Array(300.3995, dtype=float32)), 'eval/epoch_eval_time': 4.440715551376343, 'eval/sps': 28824.183517074864}
I0727 19:13:00.968550 139998383736640 train.py:379] starting iteration 41 1191.7606372833252
I0727 19:13:28.613797 139998383736640 train.py:394] {'eval/walltime': 202.39451956748962, 'training/sps': 17660.114269621812, 'training/walltime': 1008.1784138679504, 'training/entropy_loss': Array(0.10344258, dtype=float32), 'training/policy_loss': Array(0.00226862, dtype=float32), 'training/total_loss': Array(14.610791, dtype=float32), 'training/v_loss': Array(14.505081, dtype=float32), 'eval/episode_goal_distance': (Array(0.34172964, dtype=float32), Array(0.08327134, dtype=float32)), 'eval/episode_reward': (Array(-7612.169, dtype=float32), Array(3713.3325, dtype=float32)), 'eval/avg_episode_length': (Array(860.15625, dtype=float32), Array(345.7035, dtype=float32)), 'eval/epoch_eval_time': 4.4453284740448, 'eval/sps': 28794.272627402253}
I0727 19:13:28.617323 139998383736640 train.py:379] starting iteration 42 1219.409410238266
I0727 19:13:56.258958 139998383736640 train.py:394] {'eval/walltime': 206.83230876922607, 'training/sps': 17657.13718790477, 'training/walltime': 1031.3758323192596, 'training/entropy_loss': Array(0.09854631, dtype=float32), 'training/policy_loss': Array(0.00248764, dtype=float32), 'training/total_loss': Array(13.665104, dtype=float32), 'training/v_loss': Array(13.56407, dtype=float32), 'eval/episode_goal_distance': (Array(0.3236466, dtype=float32), Array(0.08738223, dtype=float32)), 'eval/episode_reward': (Array(-6887.11, dtype=float32), Array(3859.2852, dtype=float32)), 'eval/avg_episode_length': (Array(821.3047, dtype=float32), Array(381.80783, dtype=float32)), 'eval/epoch_eval_time': 4.43778920173645, 'eval/sps': 28843.19064770252}
I0727 19:13:56.262763 139998383736640 train.py:379] starting iteration 43 1247.0548496246338
I0727 19:14:23.906153 139998383736640 train.py:394] {'eval/walltime': 211.27732586860657, 'training/sps': 17661.303785671724, 'training/walltime': 1054.5677781105042, 'training/entropy_loss': Array(0.09464581, dtype=float32), 'training/policy_loss': Array(0.00163253, dtype=float32), 'training/total_loss': Array(13.415535, dtype=float32), 'training/v_loss': Array(13.319257, dtype=float32), 'eval/episode_goal_distance': (Array(0.32423776, dtype=float32), Array(0.08654194, dtype=float32)), 'eval/episode_reward': (Array(-6943.871, dtype=float32), Array(3814.2786, dtype=float32)), 'eval/avg_episode_length': (Array(829.1094, dtype=float32), Array(375.1118, dtype=float32)), 'eval/epoch_eval_time': 4.445017099380493, 'eval/sps': 28796.289674080104}
I0727 19:14:23.909756 139998383736640 train.py:379] starting iteration 44 1274.701843738556
I0727 19:14:51.565805 139998383736640 train.py:394] {'eval/walltime': 215.72122192382812, 'training/sps': 17651.027532391294, 'training/walltime': 1077.7732260227203, 'training/entropy_loss': Array(0.09186792, dtype=float32), 'training/policy_loss': Array(0.00189193, dtype=float32), 'training/total_loss': Array(12.848229, dtype=float32), 'training/v_loss': Array(12.75447, dtype=float32), 'eval/episode_goal_distance': (Array(0.30575007, dtype=float32), Array(0.07846274, dtype=float32)), 'eval/episode_reward': (Array(-6661.8994, dtype=float32), Array(3601.6765, dtype=float32)), 'eval/avg_episode_length': (Array(836.9219, dtype=float32), Array(368.11127, dtype=float32)), 'eval/epoch_eval_time': 4.443896055221558, 'eval/sps': 28803.554000683835}
I0727 19:14:51.569345 139998383736640 train.py:379] starting iteration 45 1302.3614320755005
I0727 19:15:19.205211 139998383736640 train.py:394] {'eval/walltime': 220.16591572761536, 'training/sps': 17666.597004763003, 'training/walltime': 1100.958223104477, 'training/entropy_loss': Array(0.08685777, dtype=float32), 'training/policy_loss': Array(0.00140265, dtype=float32), 'training/total_loss': Array(56.99859, dtype=float32), 'training/v_loss': Array(56.910324, dtype=float32), 'eval/episode_goal_distance': (Array(0.32077426, dtype=float32), Array(0.08876862, dtype=float32)), 'eval/episode_reward': (Array(-6527.4634, dtype=float32), Array(3902.866, dtype=float32)), 'eval/avg_episode_length': (Array(798.03125, dtype=float32), Array(400.03513, dtype=float32)), 'eval/epoch_eval_time': 4.4446938037872314, 'eval/sps': 28798.384242112214}
I0727 19:15:19.208808 139998383736640 train.py:379] starting iteration 46 1330.0008957386017
I0727 19:15:46.866862 139998383736640 train.py:394] {'eval/walltime': 224.60799717903137, 'training/sps': 17647.592504905617, 'training/walltime': 1124.1681878566742, 'training/entropy_loss': Array(0.08185546, dtype=float32), 'training/policy_loss': Array(0.00152799, dtype=float32), 'training/total_loss': Array(13.911448, dtype=float32), 'training/v_loss': Array(13.828065, dtype=float32), 'eval/episode_goal_distance': (Array(0.31945553, dtype=float32), Array(0.0873403, dtype=float32)), 'eval/episode_reward': (Array(-7306.796, dtype=float32), Array(3592.1226, dtype=float32)), 'eval/avg_episode_length': (Array(875.75, dtype=float32), Array(328.73544, dtype=float32)), 'eval/epoch_eval_time': 4.442081451416016, 'eval/sps': 28815.320340242084}
I0727 19:15:46.870479 139998383736640 train.py:379] starting iteration 47 1357.6625661849976
I0727 19:16:14.527574 139998383736640 train.py:394] {'eval/walltime': 229.06292271614075, 'training/sps': 17658.209961497614, 'training/walltime': 1147.3641970157623, 'training/entropy_loss': Array(0.07790413, dtype=float32), 'training/policy_loss': Array(0.00147333, dtype=float32), 'training/total_loss': Array(12.693097, dtype=float32), 'training/v_loss': Array(12.613718, dtype=float32), 'eval/episode_goal_distance': (Array(0.32353523, dtype=float32), Array(0.08990464, dtype=float32)), 'eval/episode_reward': (Array(-7319.71, dtype=float32), Array(3454.1506, dtype=float32)), 'eval/avg_episode_length': (Array(883.5469, dtype=float32), Array(319.62885, dtype=float32)), 'eval/epoch_eval_time': 4.454925537109375, 'eval/sps': 28732.242308825244}
I0727 19:16:14.531322 139998383736640 train.py:379] starting iteration 48 1385.323409795761
I0727 19:16:42.203294 139998383736640 train.py:394] {'eval/walltime': 233.49715662002563, 'training/sps': 17631.208733241372, 'training/walltime': 1170.5957295894623, 'training/entropy_loss': Array(0.07447387, dtype=float32), 'training/policy_loss': Array(0.00111292, dtype=float32), 'training/total_loss': Array(12.691578, dtype=float32), 'training/v_loss': Array(12.61599, dtype=float32), 'eval/episode_goal_distance': (Array(0.31641227, dtype=float32), Array(0.08564071, dtype=float32)), 'eval/episode_reward': (Array(-7253.217, dtype=float32), Array(3444.981, dtype=float32)), 'eval/avg_episode_length': (Array(883.5547, dtype=float32), Array(319.60724, dtype=float32)), 'eval/epoch_eval_time': 4.434233903884888, 'eval/sps': 28866.316656831656}
I0727 19:16:42.206917 139998383736640 train.py:379] starting iteration 49 1412.999004125595
I0727 19:17:09.846452 139998383736640 train.py:394] {'eval/walltime': 237.9129672050476, 'training/sps': 17641.699775313067, 'training/walltime': 1193.8134469985962, 'training/entropy_loss': Array(0.07249454, dtype=float32), 'training/policy_loss': Array(0.00091719, dtype=float32), 'training/total_loss': Array(11.865946, dtype=float32), 'training/v_loss': Array(11.792535, dtype=float32), 'eval/episode_goal_distance': (Array(0.31355113, dtype=float32), Array(0.09092682, dtype=float32)), 'eval/episode_reward': (Array(-7223.544, dtype=float32), Array(3645.701, dtype=float32)), 'eval/avg_episode_length': (Array(875.78906, dtype=float32), Array(328.63193, dtype=float32)), 'eval/epoch_eval_time': 4.415810585021973, 'eval/sps': 28986.750571721608}
I0727 19:17:09.850056 139998383736640 train.py:379] starting iteration 50 1440.6421430110931
I0727 19:17:37.469300 139998383736640 train.py:394] {'eval/walltime': 242.32971906661987, 'training/sps': 17657.990713799594, 'training/walltime': 1217.0097441673279, 'training/entropy_loss': Array(0.06777678, dtype=float32), 'training/policy_loss': Array(0.00103146, dtype=float32), 'training/total_loss': Array(56.177254, dtype=float32), 'training/v_loss': Array(56.108444, dtype=float32), 'eval/episode_goal_distance': (Array(0.32053015, dtype=float32), Array(0.08339216, dtype=float32)), 'eval/episode_reward': (Array(-6554.007, dtype=float32), Array(3928.33, dtype=float32)), 'eval/avg_episode_length': (Array(798.1953, dtype=float32), Array(399.71088, dtype=float32)), 'eval/epoch_eval_time': 4.416751861572266, 'eval/sps': 28980.57305724095}
I0727 19:17:37.523902 139998383736640 train.py:379] starting iteration 51 1468.3159708976746
I0727 19:18:05.148968 139998383736640 train.py:394] {'eval/walltime': 246.75437545776367, 'training/sps': 17659.836702625576, 'training/walltime': 1240.20361661911, 'training/entropy_loss': Array(0.06155667, dtype=float32), 'training/policy_loss': Array(0.00074092, dtype=float32), 'training/total_loss': Array(12.637201, dtype=float32), 'training/v_loss': Array(12.5749035, dtype=float32), 'eval/episode_goal_distance': (Array(0.32124752, dtype=float32), Array(0.08665127, dtype=float32)), 'eval/episode_reward': (Array(-6231.9727, dtype=float32), Array(4263.1763, dtype=float32)), 'eval/avg_episode_length': (Array(735.8594, dtype=float32), Array(439.19803, dtype=float32)), 'eval/epoch_eval_time': 4.424656391143799, 'eval/sps': 28928.800043365914}
I0727 19:18:05.152692 139998383736640 train.py:379] starting iteration 52 1495.9447793960571
I0727 19:18:32.792793 139998383736640 train.py:394] {'eval/walltime': 251.17916083335876, 'training/sps': 17648.19002565191, 'training/walltime': 1263.4127955436707, 'training/entropy_loss': Array(0.05579884, dtype=float32), 'training/policy_loss': Array(0.00085414, dtype=float32), 'training/total_loss': Array(12.056065, dtype=float32), 'training/v_loss': Array(11.999411, dtype=float32), 'eval/episode_goal_distance': (Array(0.31032518, dtype=float32), Array(0.08872432, dtype=float32)), 'eval/episode_reward': (Array(-6770.7305, dtype=float32), Array(3611.6099, dtype=float32)), 'eval/avg_episode_length': (Array(844.66406, dtype=float32), Array(360.96887, dtype=float32)), 'eval/epoch_eval_time': 4.424785375595093, 'eval/sps': 28927.95675604609}
I0727 19:18:32.796279 139998383736640 train.py:379] starting iteration 53 1523.588366508484
I0727 19:19:00.402094 139998383736640 train.py:394] {'eval/walltime': 255.5881791114807, 'training/sps': 17662.482566775925, 'training/walltime': 1286.6031935214996, 'training/entropy_loss': Array(0.05359499, dtype=float32), 'training/policy_loss': Array(0.00093349, dtype=float32), 'training/total_loss': Array(11.852808, dtype=float32), 'training/v_loss': Array(11.79828, dtype=float32), 'eval/episode_goal_distance': (Array(0.320167, dtype=float32), Array(0.10646872, dtype=float32)), 'eval/episode_reward': (Array(-6975.505, dtype=float32), Array(4234.2935, dtype=float32)), 'eval/avg_episode_length': (Array(821.41406, dtype=float32), Array(381.5743, dtype=float32)), 'eval/epoch_eval_time': 4.409018278121948, 'eval/sps': 29031.406069498647}
I0727 19:19:00.405626 139998383736640 train.py:379] starting iteration 54 1551.197713136673
I0727 19:19:28.037253 139998383736640 train.py:394] {'eval/walltime': 260.0327453613281, 'training/sps': 17669.60362879237, 'training/walltime': 1309.7842454910278, 'training/entropy_loss': Array(0.05141189, dtype=float32), 'training/policy_loss': Array(0.00088394, dtype=float32), 'training/total_loss': Array(11.3769, dtype=float32), 'training/v_loss': Array(11.324604, dtype=float32), 'eval/episode_goal_distance': (Array(0.32423854, dtype=float32), Array(0.08954805, dtype=float32)), 'eval/episode_reward': (Array(-7085.6855, dtype=float32), Array(3825.5913, dtype=float32)), 'eval/avg_episode_length': (Array(844.5625, dtype=float32), Array(361.20477, dtype=float32)), 'eval/epoch_eval_time': 4.444566249847412, 'eval/sps': 28799.210722619875}
I0727 19:19:28.040696 139998383736640 train.py:379] starting iteration 55 1578.8327832221985
I0727 19:19:55.675440 139998383736640 train.py:394] {'eval/walltime': 264.47158551216125, 'training/sps': 17663.224742126265, 'training/walltime': 1332.973669052124, 'training/entropy_loss': Array(0.04645852, dtype=float32), 'training/policy_loss': Array(0.00090704, dtype=float32), 'training/total_loss': Array(56.154144, dtype=float32), 'training/v_loss': Array(56.106773, dtype=float32), 'eval/episode_goal_distance': (Array(0.33303463, dtype=float32), Array(0.09787999, dtype=float32)), 'eval/episode_reward': (Array(-7551.4463, dtype=float32), Array(3856.8892, dtype=float32)), 'eval/avg_episode_length': (Array(867.97656, dtype=float32), Array(337.35648, dtype=float32)), 'eval/epoch_eval_time': 4.43884015083313, 'eval/sps': 28836.3616734375}
I0727 19:19:55.678856 139998383736640 train.py:379] starting iteration 56 1606.4709434509277
I0727 19:20:23.298616 139998383736640 train.py:394] {'eval/walltime': 268.9014468193054, 'training/sps': 17667.601703326087, 'training/walltime': 1356.1573476791382, 'training/entropy_loss': Array(0.04313771, dtype=float32), 'training/policy_loss': Array(0.0008981, dtype=float32), 'training/total_loss': Array(11.809132, dtype=float32), 'training/v_loss': Array(11.765096, dtype=float32), 'eval/episode_goal_distance': (Array(0.32268918, dtype=float32), Array(0.08341254, dtype=float32)), 'eval/episode_reward': (Array(-7122.8906, dtype=float32), Array(3723.9565, dtype=float32)), 'eval/avg_episode_length': (Array(836.8828, dtype=float32), Array(368.19946, dtype=float32)), 'eval/epoch_eval_time': 4.429861307144165, 'eval/sps': 28894.809820246675}
I0727 19:20:23.302121 139998383736640 train.py:379] starting iteration 57 1634.0942075252533
I0727 19:20:50.961686 139998383736640 train.py:394] {'eval/walltime': 273.3503041267395, 'training/sps': 17651.779808706884, 'training/walltime': 1379.3618066310883, 'training/entropy_loss': Array(0.03997692, dtype=float32), 'training/policy_loss': Array(0.0009198, dtype=float32), 'training/total_loss': Array(12.157644, dtype=float32), 'training/v_loss': Array(12.116749, dtype=float32), 'eval/episode_goal_distance': (Array(0.32937968, dtype=float32), Array(0.09141758, dtype=float32)), 'eval/episode_reward': (Array(-7252.074, dtype=float32), Array(3921.2507, dtype=float32)), 'eval/avg_episode_length': (Array(836.89844, dtype=float32), Array(368.16418, dtype=float32)), 'eval/epoch_eval_time': 4.448857307434082, 'eval/sps': 28771.43301182324}
I0727 19:20:50.967950 139998383736640 train.py:379] starting iteration 58 1661.760036945343
I0727 19:21:18.589266 139998383736640 train.py:394] {'eval/walltime': 277.7913784980774, 'training/sps': 17675.534293904766, 'training/walltime': 1402.5350806713104, 'training/entropy_loss': Array(0.03427337, dtype=float32), 'training/policy_loss': Array(0.00075654, dtype=float32), 'training/total_loss': Array(11.678739, dtype=float32), 'training/v_loss': Array(11.643709, dtype=float32), 'eval/episode_goal_distance': (Array(0.33468089, dtype=float32), Array(0.09991767, dtype=float32)), 'eval/episode_reward': (Array(-7896.4033, dtype=float32), Array(3686.6853, dtype=float32)), 'eval/avg_episode_length': (Array(898.9922, dtype=float32), Array(300.4228, dtype=float32)), 'eval/epoch_eval_time': 4.441074371337891, 'eval/sps': 28821.85464537481}
I0727 19:21:18.592862 139998383736640 train.py:379] starting iteration 59 1689.3849494457245
I0727 19:21:46.211588 139998383736640 train.py:394] {'eval/walltime': 282.2187714576721, 'training/sps': 17666.79739030828, 'training/walltime': 1425.7198147773743, 'training/entropy_loss': Array(0.02664526, dtype=float32), 'training/policy_loss': Array(0.00079966, dtype=float32), 'training/total_loss': Array(11.740845, dtype=float32), 'training/v_loss': Array(11.713399, dtype=float32), 'eval/episode_goal_distance': (Array(0.33666897, dtype=float32), Array(0.08807556, dtype=float32)), 'eval/episode_reward': (Array(-7241.3286, dtype=float32), Array(3886.6865, dtype=float32)), 'eval/avg_episode_length': (Array(829.1953, dtype=float32), Array(374.92365, dtype=float32)), 'eval/epoch_eval_time': 4.427392959594727, 'eval/sps': 28910.91917255902}
I0727 19:21:46.215092 139998383736640 train.py:379] starting iteration 60 1717.0071802139282
I0727 19:22:13.812633 139998383736640 train.py:394] {'eval/walltime': 286.63819646835327, 'training/sps': 17676.55983375347, 'training/walltime': 1448.8917443752289, 'training/entropy_loss': Array(0.01850068, dtype=float32), 'training/policy_loss': Array(0.00082557, dtype=float32), 'training/total_loss': Array(52.554554, dtype=float32), 'training/v_loss': Array(52.535225, dtype=float32), 'eval/episode_goal_distance': (Array(0.3357765, dtype=float32), Array(0.09072368, dtype=float32)), 'eval/episode_reward': (Array(-6946.5, dtype=float32), Array(4131.366, dtype=float32)), 'eval/avg_episode_length': (Array(798.0625, dtype=float32), Array(399.9739, dtype=float32)), 'eval/epoch_eval_time': 4.419425010681152, 'eval/sps': 28963.043764888265}
I0727 19:22:13.816121 139998383736640 train.py:379] starting iteration 61 1744.6082077026367
I0727 19:22:41.413336 139998383736640 train.py:394] {'eval/walltime': 291.05795550346375, 'training/sps': 17677.124758986567, 'training/walltime': 1472.0629334449768, 'training/entropy_loss': Array(0.01270861, dtype=float32), 'training/policy_loss': Array(0.00080992, dtype=float32), 'training/total_loss': Array(12.452288, dtype=float32), 'training/v_loss': Array(12.438769, dtype=float32), 'eval/episode_goal_distance': (Array(0.3247317, dtype=float32), Array(0.10170066, dtype=float32)), 'eval/episode_reward': (Array(-7073.253, dtype=float32), Array(4171.1406, dtype=float32)), 'eval/avg_episode_length': (Array(821.375, dtype=float32), Array(381.65735, dtype=float32)), 'eval/epoch_eval_time': 4.419759035110474, 'eval/sps': 28960.85487538363}
I0727 19:22:41.416676 139998383736640 train.py:379] starting iteration 62 1772.2087631225586
I0727 19:23:09.036679 139998383736640 train.py:394] {'eval/walltime': 295.5063328742981, 'training/sps': 17681.831993363438, 'training/walltime': 1495.2279539108276, 'training/entropy_loss': Array(0.00894106, dtype=float32), 'training/policy_loss': Array(0.00084782, dtype=float32), 'training/total_loss': Array(11.0567875, dtype=float32), 'training/v_loss': Array(11.046999, dtype=float32), 'eval/episode_goal_distance': (Array(0.32445738, dtype=float32), Array(0.0885175, dtype=float32)), 'eval/episode_reward': (Array(-7211.302, dtype=float32), Array(3821.1946, dtype=float32)), 'eval/avg_episode_length': (Array(844.6094, dtype=float32), Array(361.096, dtype=float32)), 'eval/epoch_eval_time': 4.448377370834351, 'eval/sps': 28774.537169267172}
I0727 19:23:09.040256 139998383736640 train.py:379] starting iteration 63 1799.8323431015015
I0727 19:23:36.654635 139998383736640 train.py:394] {'eval/walltime': 299.9089181423187, 'training/sps': 17651.08193788122, 'training/walltime': 1518.43333029747, 'training/entropy_loss': Array(0.00592485, dtype=float32), 'training/policy_loss': Array(0.00079732, dtype=float32), 'training/total_loss': Array(12.641031, dtype=float32), 'training/v_loss': Array(12.634309, dtype=float32), 'eval/episode_goal_distance': (Array(0.32081684, dtype=float32), Array(0.09466448, dtype=float32)), 'eval/episode_reward': (Array(-6679.742, dtype=float32), Array(3913.335, dtype=float32)), 'eval/avg_episode_length': (Array(813.53125, dtype=float32), Array(388.16672, dtype=float32)), 'eval/epoch_eval_time': 4.40258526802063, 'eval/sps': 29073.826446874897}
I0727 19:23:36.661978 139998383736640 train.py:379] starting iteration 64 1827.454050540924
I0727 19:24:04.300280 139998383736640 train.py:394] {'eval/walltime': 304.35197138786316, 'training/sps': 17663.986957750414, 'training/walltime': 1541.6217532157898, 'training/entropy_loss': Array(0.00230302, dtype=float32), 'training/policy_loss': Array(0.00077558, dtype=float32), 'training/total_loss': Array(11.168814, dtype=float32), 'training/v_loss': Array(11.165735, dtype=float32), 'eval/episode_goal_distance': (Array(0.34174263, dtype=float32), Array(0.0890377, dtype=float32)), 'eval/episode_reward': (Array(-7474.253, dtype=float32), Array(4089.4644, dtype=float32)), 'eval/avg_episode_length': (Array(821.3281, dtype=float32), Array(381.75763, dtype=float32)), 'eval/epoch_eval_time': 4.443053245544434, 'eval/sps': 28809.01779161897}
I0727 19:24:04.303966 139998383736640 train.py:379] starting iteration 65 1855.0960533618927
I0727 19:24:31.878707 139998383736640 train.py:394] {'eval/walltime': 308.77318835258484, 'training/sps': 17695.519087584897, 'training/walltime': 1564.768856048584, 'training/entropy_loss': Array(0.00064686, dtype=float32), 'training/policy_loss': Array(0.00110838, dtype=float32), 'training/total_loss': Array(51.24624, dtype=float32), 'training/v_loss': Array(51.24449, dtype=float32), 'eval/episode_goal_distance': (Array(0.31740114, dtype=float32), Array(0.09478469, dtype=float32)), 'eval/episode_reward': (Array(-6579.1055, dtype=float32), Array(3969.5225, dtype=float32)), 'eval/avg_episode_length': (Array(805.77344, dtype=float32), Array(394.2377, dtype=float32)), 'eval/epoch_eval_time': 4.42121696472168, 'eval/sps': 28951.304815247342}
I0727 19:24:31.882253 139998383736640 train.py:379] starting iteration 66 1882.6743404865265
I0727 19:24:59.486281 139998383736640 train.py:394] {'eval/walltime': 313.1908686161041, 'training/sps': 17670.250076672808, 'training/walltime': 1587.9490599632263, 'training/entropy_loss': Array(0.00211805, dtype=float32), 'training/policy_loss': Array(0.00107341, dtype=float32), 'training/total_loss': Array(12.161064, dtype=float32), 'training/v_loss': Array(12.157872, dtype=float32), 'eval/episode_goal_distance': (Array(0.3311581, dtype=float32), Array(0.09801412, dtype=float32)), 'eval/episode_reward': (Array(-6305.0312, dtype=float32), Array(4333.251, dtype=float32)), 'eval/avg_episode_length': (Array(751.2969, dtype=float32), Array(430.767, dtype=float32)), 'eval/epoch_eval_time': 4.417680263519287, 'eval/sps': 28974.482616365378}
I0727 19:24:59.490046 139998383736640 train.py:379] starting iteration 67 1910.2821335792542
I0727 19:25:27.103268 139998383736640 train.py:394] {'eval/walltime': 317.60985589027405, 'training/sps': 17664.400147156473, 'training/walltime': 1611.1369404792786, 'training/entropy_loss': Array(0.00073289, dtype=float32), 'training/policy_loss': Array(0.00096686, dtype=float32), 'training/total_loss': Array(11.217424, dtype=float32), 'training/v_loss': Array(11.215725, dtype=float32), 'eval/episode_goal_distance': (Array(0.3511948, dtype=float32), Array(0.11221103, dtype=float32)), 'eval/episode_reward': (Array(-6992.2217, dtype=float32), Array(4789.529, dtype=float32)), 'eval/avg_episode_length': (Array(759.27344, dtype=float32), Array(425.8236, dtype=float32)), 'eval/epoch_eval_time': 4.418987274169922, 'eval/sps': 28965.91278915687}
I0727 19:25:27.106839 139998383736640 train.py:379] starting iteration 68 1937.8989262580872
I0727 19:25:54.681972 139998383736640 train.py:394] {'eval/walltime': 322.05166602134705, 'training/sps': 17710.849666647933, 'training/walltime': 1634.2640070915222, 'training/entropy_loss': Array(-0.00222558, dtype=float32), 'training/policy_loss': Array(0.00075478, dtype=float32), 'training/total_loss': Array(11.796577, dtype=float32), 'training/v_loss': Array(11.798048, dtype=float32), 'eval/episode_goal_distance': (Array(0.32651186, dtype=float32), Array(0.09589438, dtype=float32)), 'eval/episode_reward': (Array(-7367.5747, dtype=float32), Array(3554.6392, dtype=float32)), 'eval/avg_episode_length': (Array(875.8203, dtype=float32), Array(328.54953, dtype=float32)), 'eval/epoch_eval_time': 4.441810131072998, 'eval/sps': 28817.08047459456}
I0727 19:25:54.685693 139998383736640 train.py:379] starting iteration 69 1965.4777808189392
I0727 19:26:22.306402 139998383736640 train.py:394] {'eval/walltime': 326.5011775493622, 'training/sps': 17681.853831543838, 'training/walltime': 1657.4289989471436, 'training/entropy_loss': Array(-0.00678196, dtype=float32), 'training/policy_loss': Array(0.00094403, dtype=float32), 'training/total_loss': Array(11.040886, dtype=float32), 'training/v_loss': Array(11.046724, dtype=float32), 'eval/episode_goal_distance': (Array(0.34621644, dtype=float32), Array(0.10448983, dtype=float32)), 'eval/episode_reward': (Array(-7383.2, dtype=float32), Array(4340.073, dtype=float32)), 'eval/avg_episode_length': (Array(805.85156, dtype=float32), Array(394.07904, dtype=float32)), 'eval/epoch_eval_time': 4.449511528015137, 'eval/sps': 28767.202690471277}
I0727 19:26:22.309952 139998383736640 train.py:379] starting iteration 70 1993.102040052414
I0727 19:26:49.958796 139998383736640 train.py:394] {'eval/walltime': 330.947208404541, 'training/sps': 17657.821925820714, 'training/walltime': 1680.6255178451538, 'training/entropy_loss': Array(-0.01028106, dtype=float32), 'training/policy_loss': Array(0.00084647, dtype=float32), 'training/total_loss': Array(51.174675, dtype=float32), 'training/v_loss': Array(51.184105, dtype=float32), 'eval/episode_goal_distance': (Array(0.3514698, dtype=float32), Array(0.11065747, dtype=float32)), 'eval/episode_reward': (Array(-7940.841, dtype=float32), Array(3927.118, dtype=float32)), 'eval/avg_episode_length': (Array(875.8203, dtype=float32), Array(328.54944, dtype=float32)), 'eval/epoch_eval_time': 4.446030855178833, 'eval/sps': 28789.723726479053}
I0727 19:26:49.962352 139998383736640 train.py:379] starting iteration 71 2020.754433631897
I0727 19:27:17.573899 139998383736640 train.py:394] {'eval/walltime': 335.36817622184753, 'training/sps': 17667.278842882737, 'training/walltime': 1703.809620141983, 'training/entropy_loss': Array(-0.01518247, dtype=float32), 'training/policy_loss': Array(0.00067466, dtype=float32), 'training/total_loss': Array(14.808098, dtype=float32), 'training/v_loss': Array(14.822605, dtype=float32), 'eval/episode_goal_distance': (Array(0.35197008, dtype=float32), Array(0.1061354, dtype=float32)), 'eval/episode_reward': (Array(-7783.1777, dtype=float32), Array(3701.3293, dtype=float32)), 'eval/avg_episode_length': (Array(875.71094, dtype=float32), Array(328.8387, dtype=float32)), 'eval/epoch_eval_time': 4.4209678173065186, 'eval/sps': 28952.936390743555}
I0727 19:27:17.577436 139998383736640 train.py:379] starting iteration 72 2048.369524002075
I0727 19:27:45.159677 139998383736640 train.py:394] {'eval/walltime': 339.78416204452515, 'training/sps': 17685.704017751064, 'training/walltime': 1726.9695689678192, 'training/entropy_loss': Array(-0.02051669, dtype=float32), 'training/policy_loss': Array(0.00077249, dtype=float32), 'training/total_loss': Array(16.457922, dtype=float32), 'training/v_loss': Array(16.477667, dtype=float32), 'eval/episode_goal_distance': (Array(0.37037006, dtype=float32), Array(0.12311666, dtype=float32)), 'eval/episode_reward': (Array(-7323.7354, dtype=float32), Array(4393.5547, dtype=float32)), 'eval/avg_episode_length': (Array(805.7656, dtype=float32), Array(394.2538, dtype=float32)), 'eval/epoch_eval_time': 4.415985822677612, 'eval/sps': 28985.600303034444}
I0727 19:27:45.163222 139998383736640 train.py:379] starting iteration 73 2075.9553096294403
I0727 19:28:12.741564 139998383736640 train.py:394] {'eval/walltime': 344.1977252960205, 'training/sps': 17686.874222583636, 'training/walltime': 1750.1279854774475, 'training/entropy_loss': Array(-0.0232004, dtype=float32), 'training/policy_loss': Array(0.00045384, dtype=float32), 'training/total_loss': Array(18.426235, dtype=float32), 'training/v_loss': Array(18.448982, dtype=float32), 'eval/episode_goal_distance': (Array(0.36913574, dtype=float32), Array(0.13314593, dtype=float32)), 'eval/episode_reward': (Array(-7550.504, dtype=float32), Array(4244.219, dtype=float32)), 'eval/avg_episode_length': (Array(829.1406, dtype=float32), Array(375.0433, dtype=float32)), 'eval/epoch_eval_time': 4.413563251495361, 'eval/sps': 29001.51027780836}
I0727 19:28:12.745147 139998383736640 train.py:379] starting iteration 74 2103.537235021591
I0727 19:28:40.351523 139998383736640 train.py:394] {'eval/walltime': 348.63373923301697, 'training/sps': 17682.393979706263, 'training/walltime': 1773.292269706726, 'training/entropy_loss': Array(-0.02601984, dtype=float32), 'training/policy_loss': Array(0.00060914, dtype=float32), 'training/total_loss': Array(19.836224, dtype=float32), 'training/v_loss': Array(19.861633, dtype=float32), 'eval/episode_goal_distance': (Array(0.3864151, dtype=float32), Array(0.15840103, dtype=float32)), 'eval/episode_reward': (Array(-7727.656, dtype=float32), Array(4844.1343, dtype=float32)), 'eval/avg_episode_length': (Array(813.6797, dtype=float32), Array(387.8574, dtype=float32)), 'eval/epoch_eval_time': 4.43601393699646, 'eval/sps': 28854.73351029784}
I0727 19:28:40.355068 139998383736640 train.py:379] starting iteration 75 2131.147155046463
I0727 19:29:07.961750 139998383736640 train.py:394] {'eval/walltime': 353.06283235549927, 'training/sps': 17677.013080626264, 'training/walltime': 1796.4636051654816, 'training/entropy_loss': Array(-0.02698859, dtype=float32), 'training/policy_loss': Array(0.00076577, dtype=float32), 'training/total_loss': Array(59.943466, dtype=float32), 'training/v_loss': Array(59.96969, dtype=float32), 'eval/episode_goal_distance': (Array(0.42139223, dtype=float32), Array(0.16986685, dtype=float32)), 'eval/episode_reward': (Array(-8204.312, dtype=float32), Array(4661.5645, dtype=float32)), 'eval/avg_episode_length': (Array(836.83594, dtype=float32), Array(368.30505, dtype=float32)), 'eval/epoch_eval_time': 4.4290931224823, 'eval/sps': 28899.821354007112}
I0727 19:29:08.029169 139998383736640 train.py:379] starting iteration 76 2158.8212525844574
I0727 19:29:35.635840 139998383736640 train.py:394] {'eval/walltime': 357.4904463291168, 'training/sps': 17676.09369759122, 'training/walltime': 1819.6361458301544, 'training/entropy_loss': Array(-0.0299799, dtype=float32), 'training/policy_loss': Array(0.00044841, dtype=float32), 'training/total_loss': Array(22.587402, dtype=float32), 'training/v_loss': Array(22.616934, dtype=float32), 'eval/episode_goal_distance': (Array(0.4236856, dtype=float32), Array(0.16725205, dtype=float32)), 'eval/episode_reward': (Array(-8449.498, dtype=float32), Array(4758.9663, dtype=float32)), 'eval/avg_episode_length': (Array(836.78906, dtype=float32), Array(368.41068, dtype=float32)), 'eval/epoch_eval_time': 4.427613973617554, 'eval/sps': 28909.47602087777}
I0727 19:29:35.639848 139998383736640 train.py:379] starting iteration 77 2186.431935787201
I0727 19:30:03.296432 139998383736640 train.py:394] {'eval/walltime': 361.93022990226746, 'training/sps': 17647.179738870083, 'training/walltime': 1842.8466534614563, 'training/entropy_loss': Array(-0.03370703, dtype=float32), 'training/policy_loss': Array(0.00040063, dtype=float32), 'training/total_loss': Array(26.195702, dtype=float32), 'training/v_loss': Array(26.229008, dtype=float32), 'eval/episode_goal_distance': (Array(0.4789144, dtype=float32), Array(0.22519419, dtype=float32)), 'eval/episode_reward': (Array(-9600.236, dtype=float32), Array(4757.681, dtype=float32)), 'eval/avg_episode_length': (Array(898.9922, dtype=float32), Array(300.42294, dtype=float32)), 'eval/epoch_eval_time': 4.439783573150635, 'eval/sps': 28830.234152419835}
I0727 19:30:03.299991 139998383736640 train.py:379] starting iteration 78 2214.0920779705048
I0727 19:30:30.906819 139998383736640 train.py:394] {'eval/walltime': 366.35967898368835, 'training/sps': 17677.361216197718, 'training/walltime': 1866.0175325870514, 'training/entropy_loss': Array(-0.03702967, dtype=float32), 'training/policy_loss': Array(0.00031934, dtype=float32), 'training/total_loss': Array(32.28945, dtype=float32), 'training/v_loss': Array(32.326164, dtype=float32), 'eval/episode_goal_distance': (Array(0.43557563, dtype=float32), Array(0.20834197, dtype=float32)), 'eval/episode_reward': (Array(-7901.5635, dtype=float32), Array(5670.0884, dtype=float32)), 'eval/avg_episode_length': (Array(751.4844, dtype=float32), Array(430.44266, dtype=float32)), 'eval/epoch_eval_time': 4.429449081420898, 'eval/sps': 28897.49890948957}
I0727 19:30:30.910368 139998383736640 train.py:379] starting iteration 79 2241.702455997467
I0727 19:30:58.584797 139998383736640 train.py:394] {'eval/walltime': 370.80441522598267, 'training/sps': 17637.437760955603, 'training/walltime': 1889.2408604621887, 'training/entropy_loss': Array(-0.04058726, dtype=float32), 'training/policy_loss': Array(0.00018039, dtype=float32), 'training/total_loss': Array(37.10865, dtype=float32), 'training/v_loss': Array(37.149063, dtype=float32), 'eval/episode_goal_distance': (Array(0.4852511, dtype=float32), Array(0.2390141, dtype=float32)), 'eval/episode_reward': (Array(-9902.528, dtype=float32), Array(5379.824, dtype=float32)), 'eval/avg_episode_length': (Array(860.15625, dtype=float32), Array(345.70367, dtype=float32)), 'eval/epoch_eval_time': 4.4447362422943115, 'eval/sps': 28798.10927406756}
I0727 19:30:58.588480 139998383736640 train.py:379] starting iteration 80 2269.3805677890778
I0727 19:31:26.193617 139998383736640 train.py:394] {'eval/walltime': 375.2330529689789, 'training/sps': 17678.048433138287, 'training/walltime': 1912.410838842392, 'training/entropy_loss': Array(-0.04240604, dtype=float32), 'training/policy_loss': Array(0.00010609, dtype=float32), 'training/total_loss': Array(66.32474, dtype=float32), 'training/v_loss': Array(66.367035, dtype=float32), 'eval/episode_goal_distance': (Array(0.47827202, dtype=float32), Array(0.21898112, dtype=float32)), 'eval/episode_reward': (Array(-9449.68, dtype=float32), Array(5683.681, dtype=float32)), 'eval/avg_episode_length': (Array(821.4531, dtype=float32), Array(381.49088, dtype=float32)), 'eval/epoch_eval_time': 4.428637742996216, 'eval/sps': 28902.793009527348}
I0727 19:31:26.197143 139998383736640 train.py:379] starting iteration 81 2296.9892303943634
I0727 19:31:53.730255 139998383736640 train.py:394] {'eval/walltime': 379.6751482486725, 'training/sps': 17743.511339563716, 'training/walltime': 1935.4953339099884, 'training/entropy_loss': Array(-0.04475045, dtype=float32), 'training/policy_loss': Array(0.00015904, dtype=float32), 'training/total_loss': Array(36.20456, dtype=float32), 'training/v_loss': Array(36.249153, dtype=float32), 'eval/episode_goal_distance': (Array(0.4884374, dtype=float32), Array(0.24797463, dtype=float32)), 'eval/episode_reward': (Array(-9223.229, dtype=float32), Array(6098.3774, dtype=float32)), 'eval/avg_episode_length': (Array(790.2344, dtype=float32), Array(405.70837, dtype=float32)), 'eval/epoch_eval_time': 4.4420952796936035, 'eval/sps': 28815.23063792294}
I0727 19:31:53.733884 139998383736640 train.py:379] starting iteration 82 2324.5259704589844
I0727 19:32:21.323995 139998383736640 train.py:394] {'eval/walltime': 384.11917448043823, 'training/sps': 17701.138141362426, 'training/walltime': 1958.6350889205933, 'training/entropy_loss': Array(-0.04728049, dtype=float32), 'training/policy_loss': Array(-7.3959935e-05, dtype=float32), 'training/total_loss': Array(42.928566, dtype=float32), 'training/v_loss': Array(42.97592, dtype=float32), 'eval/episode_goal_distance': (Array(0.5146775, dtype=float32), Array(0.27352256, dtype=float32)), 'eval/episode_reward': (Array(-10412.916, dtype=float32), Array(5624.0684, dtype=float32)), 'eval/avg_episode_length': (Array(875.7422, dtype=float32), Array(328.75577, dtype=float32)), 'eval/epoch_eval_time': 4.444026231765747, 'eval/sps': 28802.710273188848}
I0727 19:32:21.327713 139998383736640 train.py:379] starting iteration 83 2352.1198003292084
I0727 19:32:48.958604 139998383736640 train.py:394] {'eval/walltime': 388.5605869293213, 'training/sps': 17667.94510751849, 'training/walltime': 1981.818316936493, 'training/entropy_loss': Array(-0.04817335, dtype=float32), 'training/policy_loss': Array(-4.586702e-05, dtype=float32), 'training/total_loss': Array(47.029312, dtype=float32), 'training/v_loss': Array(47.07753, dtype=float32), 'eval/episode_goal_distance': (Array(0.4641313, dtype=float32), Array(0.23396866, dtype=float32)), 'eval/episode_reward': (Array(-8752.202, dtype=float32), Array(5422.31, dtype=float32)), 'eval/avg_episode_length': (Array(805.9453, dtype=float32), Array(393.88928, dtype=float32)), 'eval/epoch_eval_time': 4.441412448883057, 'eval/sps': 28819.660743777564}
I0727 19:32:48.962271 139998383736640 train.py:379] starting iteration 84 2379.754358291626
I0727 19:33:16.576214 139998383736640 train.py:394] {'eval/walltime': 392.9880928993225, 'training/sps': 17670.457815117243, 'training/walltime': 2004.9982483386993, 'training/entropy_loss': Array(-0.04857528, dtype=float32), 'training/policy_loss': Array(8.725896e-05, dtype=float32), 'training/total_loss': Array(48.582764, dtype=float32), 'training/v_loss': Array(48.631256, dtype=float32), 'eval/episode_goal_distance': (Array(0.4888231, dtype=float32), Array(0.23165597, dtype=float32)), 'eval/episode_reward': (Array(-9633.105, dtype=float32), Array(5199.0557, dtype=float32)), 'eval/avg_episode_length': (Array(860.1719, dtype=float32), Array(345.66507, dtype=float32)), 'eval/epoch_eval_time': 4.427505970001221, 'eval/sps': 28910.18123233941}
I0727 19:33:16.579853 139998383736640 train.py:379] starting iteration 85 2407.3719408512115
I0727 19:33:44.247005 139998383736640 train.py:394] {'eval/walltime': 397.42742943763733, 'training/sps': 17638.63201120684, 'training/walltime': 2028.2200038433075, 'training/entropy_loss': Array(-0.04972249, dtype=float32), 'training/policy_loss': Array(-6.9176385e-05, dtype=float32), 'training/total_loss': Array(65.89658, dtype=float32), 'training/v_loss': Array(65.94638, dtype=float32), 'eval/episode_goal_distance': (Array(0.5294492, dtype=float32), Array(0.24724545, dtype=float32)), 'eval/episode_reward': (Array(-10290.345, dtype=float32), Array(5482.304, dtype=float32)), 'eval/avg_episode_length': (Array(868.09375, dtype=float32), Array(337.057, dtype=float32)), 'eval/epoch_eval_time': 4.439336538314819, 'eval/sps': 28833.137315737058}
I0727 19:33:44.250891 139998383736640 train.py:379] starting iteration 86 2435.0429747104645
I0727 19:34:11.914290 139998383736640 train.py:394] {'eval/walltime': 401.86406087875366, 'training/sps': 17639.774984988366, 'training/walltime': 2051.440254688263, 'training/entropy_loss': Array(-0.05034952, dtype=float32), 'training/policy_loss': Array(2.254823e-06, dtype=float32), 'training/total_loss': Array(39.008324, dtype=float32), 'training/v_loss': Array(39.05867, dtype=float32), 'eval/episode_goal_distance': (Array(0.46954766, dtype=float32), Array(0.20426649, dtype=float32)), 'eval/episode_reward': (Array(-9394.7, dtype=float32), Array(4739.778, dtype=float32)), 'eval/avg_episode_length': (Array(868.1094, dtype=float32), Array(337.01743, dtype=float32)), 'eval/epoch_eval_time': 4.436631441116333, 'eval/sps': 28850.717419022978}
I0727 19:34:11.917724 139998383736640 train.py:379] starting iteration 87 2462.7098112106323
I0727 19:34:39.543039 139998383736640 train.py:394] {'eval/walltime': 406.28628373146057, 'training/sps': 17657.75295961816, 'training/walltime': 2074.6368641853333, 'training/entropy_loss': Array(-0.05087626, dtype=float32), 'training/policy_loss': Array(9.10754e-05, dtype=float32), 'training/total_loss': Array(45.673058, dtype=float32), 'training/v_loss': Array(45.72384, dtype=float32), 'eval/episode_goal_distance': (Array(0.5299232, dtype=float32), Array(0.2597638, dtype=float32)), 'eval/episode_reward': (Array(-10148.072, dtype=float32), Array(5715.9307, dtype=float32)), 'eval/avg_episode_length': (Array(836.8672, dtype=float32), Array(368.23438, dtype=float32)), 'eval/epoch_eval_time': 4.422222852706909, 'eval/sps': 28944.71949138639}
I0727 19:34:39.546801 139998383736640 train.py:379] starting iteration 88 2490.338888645172
I0727 19:35:07.185677 139998383736640 train.py:394] {'eval/walltime': 410.71934509277344, 'training/sps': 17655.61799619398, 'training/walltime': 2097.8362786769867, 'training/entropy_loss': Array(-0.05112845, dtype=float32), 'training/policy_loss': Array(-0.00012095, dtype=float32), 'training/total_loss': Array(50.869446, dtype=float32), 'training/v_loss': Array(50.920692, dtype=float32), 'eval/episode_goal_distance': (Array(0.5094881, dtype=float32), Array(0.24279204, dtype=float32)), 'eval/episode_reward': (Array(-9821.744, dtype=float32), Array(5655.188, dtype=float32)), 'eval/avg_episode_length': (Array(836.96875, dtype=float32), Array(368.00497, dtype=float32)), 'eval/epoch_eval_time': 4.433061361312866, 'eval/sps': 28873.95178353506}
I0727 19:35:07.189338 139998383736640 train.py:379] starting iteration 89 2517.981425523758
I0727 19:35:34.801718 139998383736640 train.py:394] {'eval/walltime': 415.1439719200134, 'training/sps': 17669.400090192794, 'training/walltime': 2121.0175976753235, 'training/entropy_loss': Array(-0.05141387, dtype=float32), 'training/policy_loss': Array(-8.998964e-05, dtype=float32), 'training/total_loss': Array(54.746693, dtype=float32), 'training/v_loss': Array(54.7982, dtype=float32), 'eval/episode_goal_distance': (Array(0.5055475, dtype=float32), Array(0.25872964, dtype=float32)), 'eval/episode_reward': (Array(-10312.088, dtype=float32), Array(5185.805, dtype=float32)), 'eval/avg_episode_length': (Array(891.2422, dtype=float32), Array(310.34842, dtype=float32)), 'eval/epoch_eval_time': 4.42462682723999, 'eval/sps': 28928.99333611018}
I0727 19:35:34.805406 139998383736640 train.py:379] starting iteration 90 2545.597493648529
I0727 19:36:02.419663 139998383736640 train.py:394] {'eval/walltime': 419.57444620132446, 'training/sps': 17672.505835983167, 'training/walltime': 2144.194842815399, 'training/entropy_loss': Array(-0.05134529, dtype=float32), 'training/policy_loss': Array(-0.00033679, dtype=float32), 'training/total_loss': Array(64.01117, dtype=float32), 'training/v_loss': Array(64.06284, dtype=float32), 'eval/episode_goal_distance': (Array(0.49041563, dtype=float32), Array(0.24087155, dtype=float32)), 'eval/episode_reward': (Array(-10011.287, dtype=float32), Array(5254.26, dtype=float32)), 'eval/avg_episode_length': (Array(883.4219, dtype=float32), Array(319.9717, dtype=float32)), 'eval/epoch_eval_time': 4.430474281311035, 'eval/sps': 28890.812105588644}
I0727 19:36:02.423120 139998383736640 train.py:379] starting iteration 91 2573.2152078151703
I0727 19:36:30.019510 139998383736640 train.py:394] {'eval/walltime': 423.9856655597687, 'training/sps': 17671.211021529776, 'training/walltime': 2167.373786211014, 'training/entropy_loss': Array(-0.05105532, dtype=float32), 'training/policy_loss': Array(-5.1733823e-05, dtype=float32), 'training/total_loss': Array(42.82531, dtype=float32), 'training/v_loss': Array(42.87642, dtype=float32), 'eval/episode_goal_distance': (Array(0.5102439, dtype=float32), Array(0.27984878, dtype=float32)), 'eval/episode_reward': (Array(-9810.294, dtype=float32), Array(5901.877, dtype=float32)), 'eval/avg_episode_length': (Array(836.83594, dtype=float32), Array(368.30493, dtype=float32)), 'eval/epoch_eval_time': 4.411219358444214, 'eval/sps': 29016.920175364874}
I0727 19:36:30.023240 139998383736640 train.py:379] starting iteration 92 2600.8153269290924
I0727 19:36:57.642524 139998383736640 train.py:394] {'eval/walltime': 428.39533615112305, 'training/sps': 17652.887120846648, 'training/walltime': 2190.5767896175385, 'training/entropy_loss': Array(-0.05079919, dtype=float32), 'training/policy_loss': Array(9.317064e-06, dtype=float32), 'training/total_loss': Array(45.17263, dtype=float32), 'training/v_loss': Array(45.223423, dtype=float32), 'eval/episode_goal_distance': (Array(0.52144265, dtype=float32), Array(0.27134714, dtype=float32)), 'eval/episode_reward': (Array(-10754.417, dtype=float32), Array(5329.8247, dtype=float32)), 'eval/avg_episode_length': (Array(906.85156, dtype=float32), Array(289.61084, dtype=float32)), 'eval/epoch_eval_time': 4.40967059135437, 'eval/sps': 29027.111515077264}
I0727 19:36:57.646150 139998383736640 train.py:379] starting iteration 93 2628.4382371902466
I0727 19:37:25.281751 139998383736640 train.py:394] {'eval/walltime': 432.8449954986572, 'training/sps': 17670.86785388854, 'training/walltime': 2213.7561831474304, 'training/entropy_loss': Array(-0.05107832, dtype=float32), 'training/policy_loss': Array(0.00012233, dtype=float32), 'training/total_loss': Array(49.988346, dtype=float32), 'training/v_loss': Array(50.0393, dtype=float32), 'eval/episode_goal_distance': (Array(0.5104173, dtype=float32), Array(0.25114772, dtype=float32)), 'eval/episode_reward': (Array(-10312.953, dtype=float32), Array(4922.6543, dtype=float32)), 'eval/avg_episode_length': (Array(899.0703, dtype=float32), Array(300.19083, dtype=float32)), 'eval/epoch_eval_time': 4.44965934753418, 'eval/sps': 28766.24703213121}
I0727 19:37:25.285651 139998383736640 train.py:379] starting iteration 94 2656.0777385234833
I0727 19:37:52.929541 139998383736640 train.py:394] {'eval/walltime': 437.29452991485596, 'training/sps': 17664.36418528854, 'training/walltime': 2236.9441108703613, 'training/entropy_loss': Array(-0.05186735, dtype=float32), 'training/policy_loss': Array(5.8213256e-05, dtype=float32), 'training/total_loss': Array(51.971603, dtype=float32), 'training/v_loss': Array(52.02341, dtype=float32), 'eval/episode_goal_distance': (Array(0.5232651, dtype=float32), Array(0.2609143, dtype=float32)), 'eval/episode_reward': (Array(-10577.582, dtype=float32), Array(5072.924, dtype=float32)), 'eval/avg_episode_length': (Array(906.7344, dtype=float32), Array(289.9752, dtype=float32)), 'eval/epoch_eval_time': 4.4495344161987305, 'eval/sps': 28767.054713412315}
I0727 19:37:52.933384 139998383736640 train.py:379] starting iteration 95 2683.725471973419
I0727 19:38:20.597823 139998383736640 train.py:394] {'eval/walltime': 441.72738552093506, 'training/sps': 17636.086886742753, 'training/walltime': 2260.1692175865173, 'training/entropy_loss': Array(-0.05202942, dtype=float32), 'training/policy_loss': Array(0.00406225, dtype=float32), 'training/total_loss': Array(59.990776, dtype=float32), 'training/v_loss': Array(60.038742, dtype=float32), 'eval/episode_goal_distance': (Array(0.5298405, dtype=float32), Array(0.25380528, dtype=float32)), 'eval/episode_reward': (Array(-10243.867, dtype=float32), Array(5510.1304, dtype=float32)), 'eval/avg_episode_length': (Array(860.21875, dtype=float32), Array(345.5491, dtype=float32)), 'eval/epoch_eval_time': 4.432855606079102, 'eval/sps': 28875.291995630123}
I0727 19:38:20.601516 139998383736640 train.py:379] starting iteration 96 2711.393602848053
I0727 19:38:48.241904 139998383736640 train.py:394] {'eval/walltime': 446.1577818393707, 'training/sps': 17652.45125315017, 'training/walltime': 2283.3727939128876, 'training/entropy_loss': Array(-0.05253574, dtype=float32), 'training/policy_loss': Array(-0.00017653, dtype=float32), 'training/total_loss': Array(41.4455, dtype=float32), 'training/v_loss': Array(41.498215, dtype=float32), 'eval/episode_goal_distance': (Array(0.52054465, dtype=float32), Array(0.29527646, dtype=float32)), 'eval/episode_reward': (Array(-10234.915, dtype=float32), Array(5496.239, dtype=float32)), 'eval/avg_episode_length': (Array(891.22656, dtype=float32), Array(310.39307, dtype=float32)), 'eval/epoch_eval_time': 4.430396318435669, 'eval/sps': 28891.320504978117}
I0727 19:38:48.245599 139998383736640 train.py:379] starting iteration 97 2739.037686109543
I0727 19:39:15.890016 139998383736640 train.py:394] {'eval/walltime': 450.6072235107422, 'training/sps': 17663.703820655326, 'training/walltime': 2306.561588525772, 'training/entropy_loss': Array(-0.05255519, dtype=float32), 'training/policy_loss': Array(-0.00014404, dtype=float32), 'training/total_loss': Array(45.609688, dtype=float32), 'training/v_loss': Array(45.662384, dtype=float32), 'eval/episode_goal_distance': (Array(0.5088893, dtype=float32), Array(0.26231936, dtype=float32)), 'eval/episode_reward': (Array(-10003.94, dtype=float32), Array(4941.686, dtype=float32)), 'eval/avg_episode_length': (Array(899.15625, dtype=float32), Array(299.93488, dtype=float32)), 'eval/epoch_eval_time': 4.44944167137146, 'eval/sps': 28767.654338200664}
I0727 19:39:15.893645 139998383736640 train.py:379] starting iteration 98 2766.685732603073
I0727 19:39:43.531209 139998383736640 train.py:394] {'eval/walltime': 455.03359031677246, 'training/sps': 17651.43358678711, 'training/walltime': 2329.7665026187897, 'training/entropy_loss': Array(-0.05293515, dtype=float32), 'training/policy_loss': Array(1.4724606e-05, dtype=float32), 'training/total_loss': Array(51.18108, dtype=float32), 'training/v_loss': Array(51.234, dtype=float32), 'eval/episode_goal_distance': (Array(0.470367, dtype=float32), Array(0.21066083, dtype=float32)), 'eval/episode_reward': (Array(-9942.783, dtype=float32), Array(4868.5938, dtype=float32)), 'eval/avg_episode_length': (Array(883.4922, dtype=float32), Array(319.77872, dtype=float32)), 'eval/epoch_eval_time': 4.426366806030273, 'eval/sps': 28917.62151876316}
I0727 19:39:43.534765 139998383736640 train.py:379] starting iteration 99 2794.326852798462
I0727 19:40:11.191812 139998383736640 train.py:394] {'eval/walltime': 459.4649968147278, 'training/sps': 17640.56307298895, 'training/walltime': 2352.9857161045074, 'training/entropy_loss': Array(-0.05310832, dtype=float32), 'training/policy_loss': Array(-0.00018172, dtype=float32), 'training/total_loss': Array(53.081535, dtype=float32), 'training/v_loss': Array(53.13482, dtype=float32), 'eval/episode_goal_distance': (Array(0.53940654, dtype=float32), Array(0.26326603, dtype=float32)), 'eval/episode_reward': (Array(-11317.353, dtype=float32), Array(5388.767, dtype=float32)), 'eval/avg_episode_length': (Array(922.3594, dtype=float32), Array(266.70483, dtype=float32)), 'eval/epoch_eval_time': 4.431406497955322, 'eval/sps': 28884.73446501917}
I0727 19:40:11.195494 139998383736640 train.py:379] starting iteration 100 2821.9875807762146
I0727 19:40:38.817266 139998383736640 train.py:394] {'eval/walltime': 463.9056272506714, 'training/sps': 17674.237399201556, 'training/walltime': 2376.1606905460358, 'training/entropy_loss': Array(-0.05326112, dtype=float32), 'training/policy_loss': Array(-0.00018506, dtype=float32), 'training/total_loss': Array(57.96437, dtype=float32), 'training/v_loss': Array(58.017815, dtype=float32), 'eval/episode_goal_distance': (Array(0.517928, dtype=float32), Array(0.23042537, dtype=float32)), 'eval/episode_reward': (Array(-10109.271, dtype=float32), Array(5630.108, dtype=float32)), 'eval/avg_episode_length': (Array(844.7969, dtype=float32), Array(360.66052, dtype=float32)), 'eval/epoch_eval_time': 4.4406304359436035, 'eval/sps': 28824.736002333164}
I0727 19:40:38.897477 139998383736640 train.py:379] starting iteration 101 2849.6895475387573
I0727 19:41:06.590681 139998383736640 train.py:394] {'eval/walltime': 468.3561897277832, 'training/sps': 17627.755187811927, 'training/walltime': 2399.3967745304108, 'training/entropy_loss': Array(-0.05311606, dtype=float32), 'training/policy_loss': Array(-0.00018145, dtype=float32), 'training/total_loss': Array(41.11093, dtype=float32), 'training/v_loss': Array(41.164227, dtype=float32), 'eval/episode_goal_distance': (Array(0.52672666, dtype=float32), Array(0.2515977, dtype=float32)), 'eval/episode_reward': (Array(-10276.031, dtype=float32), Array(5397.176, dtype=float32)), 'eval/avg_episode_length': (Array(868., dtype=float32), Array(337.29645, dtype=float32)), 'eval/epoch_eval_time': 4.450562477111816, 'eval/sps': 28760.40964670725}
I0727 19:41:06.594922 139998383736640 train.py:379] starting iteration 102 2877.387009382248
I0727 19:41:34.289698 139998383736640 train.py:394] {'eval/walltime': 472.80136346817017, 'training/sps': 17622.3608495532, 'training/walltime': 2422.639971256256, 'training/entropy_loss': Array(-0.05313589, dtype=float32), 'training/policy_loss': Array(5.017181e-05, dtype=float32), 'training/total_loss': Array(43.260334, dtype=float32), 'training/v_loss': Array(43.313416, dtype=float32), 'eval/episode_goal_distance': (Array(0.53265524, dtype=float32), Array(0.27572733, dtype=float32)), 'eval/episode_reward': (Array(-10307.906, dtype=float32), Array(5291.5366, dtype=float32)), 'eval/avg_episode_length': (Array(883.59375, dtype=float32), Array(319.5002, dtype=float32)), 'eval/epoch_eval_time': 4.445173740386963, 'eval/sps': 28795.274937635462}
I0727 19:41:34.293323 139998383736640 train.py:379] starting iteration 103 2905.0854103565216
I0727 19:42:01.945846 139998383736640 train.py:394] {'eval/walltime': 477.24288988113403, 'training/sps': 17651.587018188402, 'training/walltime': 2445.8446836471558, 'training/entropy_loss': Array(-0.05358612, dtype=float32), 'training/policy_loss': Array(-0.00012272, dtype=float32), 'training/total_loss': Array(46.775097, dtype=float32), 'training/v_loss': Array(46.828804, dtype=float32), 'eval/episode_goal_distance': (Array(0.49140596, dtype=float32), Array(0.26473105, dtype=float32)), 'eval/episode_reward': (Array(-10140.158, dtype=float32), Array(5470.078, dtype=float32)), 'eval/avg_episode_length': (Array(891.21094, dtype=float32), Array(310.4379, dtype=float32)), 'eval/epoch_eval_time': 4.441526412963867, 'eval/sps': 28818.921266885936}
I0727 19:42:01.949534 139998383736640 train.py:379] starting iteration 104 2932.741621494293
I0727 19:42:29.562762 139998383736640 train.py:394] {'eval/walltime': 481.68528723716736, 'training/sps': 17682.139370716275, 'training/walltime': 2469.0093014240265, 'training/entropy_loss': Array(-0.05338486, dtype=float32), 'training/policy_loss': Array(-7.8023404e-05, dtype=float32), 'training/total_loss': Array(49.88229, dtype=float32), 'training/v_loss': Array(49.935753, dtype=float32), 'eval/episode_goal_distance': (Array(0.49375826, dtype=float32), Array(0.23597972, dtype=float32)), 'eval/episode_reward': (Array(-10638.5625, dtype=float32), Array(5075.5327, dtype=float32)), 'eval/avg_episode_length': (Array(899.0547, dtype=float32), Array(300.2368, dtype=float32)), 'eval/epoch_eval_time': 4.442397356033325, 'eval/sps': 28813.27124557198}
I0727 19:42:29.566258 139998383736640 train.py:379] starting iteration 105 2960.358345746994
I0727 19:42:57.212852 139998383736640 train.py:394] {'eval/walltime': 486.129093170166, 'training/sps': 17657.896881804427, 'training/walltime': 2492.2057218551636, 'training/entropy_loss': Array(-0.05312173, dtype=float32), 'training/policy_loss': Array(-8.9426016e-05, dtype=float32), 'training/total_loss': Array(58.18917, dtype=float32), 'training/v_loss': Array(58.242386, dtype=float32), 'eval/episode_goal_distance': (Array(0.58045095, dtype=float32), Array(0.30481124, dtype=float32)), 'eval/episode_reward': (Array(-11430.046, dtype=float32), Array(5906.907, dtype=float32)), 'eval/avg_episode_length': (Array(914.4922, dtype=float32), Array(278.8707, dtype=float32)), 'eval/epoch_eval_time': 4.443805932998657, 'eval/sps': 28804.138148676142}
I0727 19:42:57.216875 139998383736640 train.py:379] starting iteration 106 2988.008962869644
I0727 19:43:24.876499 139998383736640 train.py:394] {'eval/walltime': 490.5693874359131, 'training/sps': 17645.145376306595, 'training/walltime': 2515.4189054965973, 'training/entropy_loss': Array(-0.05323916, dtype=float32), 'training/policy_loss': Array(-0.00025333, dtype=float32), 'training/total_loss': Array(38.89766, dtype=float32), 'training/v_loss': Array(38.951157, dtype=float32), 'eval/episode_goal_distance': (Array(0.47882903, dtype=float32), Array(0.24924293, dtype=float32)), 'eval/episode_reward': (Array(-9919.972, dtype=float32), Array(5275.9673, dtype=float32)), 'eval/avg_episode_length': (Array(875.84375, dtype=float32), Array(328.48706, dtype=float32)), 'eval/epoch_eval_time': 4.44029426574707, 'eval/sps': 28826.918293998307}
I0727 19:43:24.879964 139998383736640 train.py:379] starting iteration 107 3015.672051668167
I0727 19:43:52.530945 139998383736640 train.py:394] {'eval/walltime': 495.0037319660187, 'training/sps': 17647.341616265556, 'training/walltime': 2538.629200220108, 'training/entropy_loss': Array(-0.05339775, dtype=float32), 'training/policy_loss': Array(-4.418352e-05, dtype=float32), 'training/total_loss': Array(41.42581, dtype=float32), 'training/v_loss': Array(41.479256, dtype=float32), 'eval/episode_goal_distance': (Array(0.5292163, dtype=float32), Array(0.26246125, dtype=float32)), 'eval/episode_reward': (Array(-10803.076, dtype=float32), Array(4979.7573, dtype=float32)), 'eval/avg_episode_length': (Array(930.0469, dtype=float32), Array(254.36655, dtype=float32)), 'eval/epoch_eval_time': 4.434344530105591, 'eval/sps': 28865.596511724372}
I0727 19:43:52.534552 139998383736640 train.py:379] starting iteration 108 3043.326639652252
I0727 19:44:20.178013 139998383736640 train.py:394] {'eval/walltime': 499.4380486011505, 'training/sps': 17653.091186154445, 'training/walltime': 2561.831935405731, 'training/entropy_loss': Array(-0.05350919, dtype=float32), 'training/policy_loss': Array(-0.000146, dtype=float32), 'training/total_loss': Array(44.710876, dtype=float32), 'training/v_loss': Array(44.764526, dtype=float32), 'eval/episode_goal_distance': (Array(0.47340304, dtype=float32), Array(0.24048753, dtype=float32)), 'eval/episode_reward': (Array(-10170.843, dtype=float32), Array(5095.188, dtype=float32)), 'eval/avg_episode_length': (Array(906.78125, dtype=float32), Array(289.82928, dtype=float32)), 'eval/epoch_eval_time': 4.434316635131836, 'eval/sps': 28865.778096650163}
I0727 19:44:20.181672 139998383736640 train.py:379] starting iteration 109 3070.973759174347
I0727 19:44:47.824636 139998383736640 train.py:394] {'eval/walltime': 503.8733332157135, 'training/sps': 17654.227145229208, 'training/walltime': 2585.033177614212, 'training/entropy_loss': Array(-0.05355609, dtype=float32), 'training/policy_loss': Array(-0.00018913, dtype=float32), 'training/total_loss': Array(46.944923, dtype=float32), 'training/v_loss': Array(46.998665, dtype=float32), 'eval/episode_goal_distance': (Array(0.4795254, dtype=float32), Array(0.24097243, dtype=float32)), 'eval/episode_reward': (Array(-10311.981, dtype=float32), Array(4852.4756, dtype=float32)), 'eval/avg_episode_length': (Array(914.66406, dtype=float32), Array(278.3103, dtype=float32)), 'eval/epoch_eval_time': 4.435284614562988, 'eval/sps': 28859.478280090472}
I0727 19:44:47.828383 139998383736640 train.py:379] starting iteration 110 3098.6204710006714
I0727 19:45:15.503422 139998383736640 train.py:394] {'eval/walltime': 508.3285517692566, 'training/sps': 17644.805576178976, 'training/walltime': 2608.2468082904816, 'training/entropy_loss': Array(-0.05316114, dtype=float32), 'training/policy_loss': Array(-0.00032284, dtype=float32), 'training/total_loss': Array(56.826756, dtype=float32), 'training/v_loss': Array(56.88024, dtype=float32), 'eval/episode_goal_distance': (Array(0.5008272, dtype=float32), Array(0.23016432, dtype=float32)), 'eval/episode_reward': (Array(-10079.908, dtype=float32), Array(4627.5796, dtype=float32)), 'eval/avg_episode_length': (Array(906.875, dtype=float32), Array(289.53772, dtype=float32)), 'eval/epoch_eval_time': 4.455218553543091, 'eval/sps': 28730.35261046975}
I0727 19:45:15.507007 139998383736640 train.py:379] starting iteration 111 3126.2990942001343
I0727 19:45:43.159653 139998383736640 train.py:394] {'eval/walltime': 512.7678623199463, 'training/sps': 17649.808755982693, 'training/walltime': 2631.453858613968, 'training/entropy_loss': Array(-0.05282997, dtype=float32), 'training/policy_loss': Array(2.6056368e-05, dtype=float32), 'training/total_loss': Array(36.889427, dtype=float32), 'training/v_loss': Array(36.94223, dtype=float32), 'eval/episode_goal_distance': (Array(0.52174526, dtype=float32), Array(0.23217674, dtype=float32)), 'eval/episode_reward': (Array(-10474.595, dtype=float32), Array(4707.5146, dtype=float32)), 'eval/avg_episode_length': (Array(922.2969, dtype=float32), Array(266.91937, dtype=float32)), 'eval/epoch_eval_time': 4.439310550689697, 'eval/sps': 28833.306104280935}
I0727 19:45:43.163442 139998383736640 train.py:379] starting iteration 112 3153.955529689789
I0727 19:46:10.805144 139998383736640 train.py:394] {'eval/walltime': 517.189510345459, 'training/sps': 17644.556760172607, 'training/walltime': 2654.6678166389465, 'training/entropy_loss': Array(-0.05320287, dtype=float32), 'training/policy_loss': Array(-4.5157307e-05, dtype=float32), 'training/total_loss': Array(39.432285, dtype=float32), 'training/v_loss': Array(39.485535, dtype=float32), 'eval/episode_goal_distance': (Array(0.46472323, dtype=float32), Array(0.20259398, dtype=float32)), 'eval/episode_reward': (Array(-9846.811, dtype=float32), Array(4717.8833, dtype=float32)), 'eval/avg_episode_length': (Array(898.96875, dtype=float32), Array(300.49246, dtype=float32)), 'eval/epoch_eval_time': 4.421648025512695, 'eval/sps': 28948.482389698635}
I0727 19:46:10.808663 139998383736640 train.py:379] starting iteration 113 3181.6007509231567
I0727 19:46:38.467199 139998383736640 train.py:394] {'eval/walltime': 521.6089873313904, 'training/sps': 17630.280900950747, 'training/walltime': 2677.90057182312, 'training/entropy_loss': Array(-0.05323296, dtype=float32), 'training/policy_loss': Array(-0.00016178, dtype=float32), 'training/total_loss': Array(40.879734, dtype=float32), 'training/v_loss': Array(40.93313, dtype=float32), 'eval/episode_goal_distance': (Array(0.5258434, dtype=float32), Array(0.26033303, dtype=float32)), 'eval/episode_reward': (Array(-10537.398, dtype=float32), Array(5776.522, dtype=float32)), 'eval/avg_episode_length': (Array(875.78125, dtype=float32), Array(328.6531, dtype=float32)), 'eval/epoch_eval_time': 4.4194769859313965, 'eval/sps': 28962.70314507006}
I0727 19:46:38.470822 139998383736640 train.py:379] starting iteration 114 3209.262909889221
I0727 19:47:06.093088 139998383736640 train.py:394] {'eval/walltime': 526.0381665229797, 'training/sps': 17665.121231673453, 'training/walltime': 2701.0875058174133, 'training/entropy_loss': Array(-0.0535194, dtype=float32), 'training/policy_loss': Array(-0.00016426, dtype=float32), 'training/total_loss': Array(42.276104, dtype=float32), 'training/v_loss': Array(42.32979, dtype=float32), 'eval/episode_goal_distance': (Array(0.5506066, dtype=float32), Array(0.2507303, dtype=float32)), 'eval/episode_reward': (Array(-10684.48, dtype=float32), Array(5182.3486, dtype=float32)), 'eval/avg_episode_length': (Array(906.8672, dtype=float32), Array(289.56216, dtype=float32)), 'eval/epoch_eval_time': 4.4291791915893555, 'eval/sps': 28899.25976421577}
I0727 19:47:06.096607 139998383736640 train.py:379] starting iteration 115 3236.888695001602
I0727 19:47:33.770265 139998383736640 train.py:394] {'eval/walltime': 530.4726083278656, 'training/sps': 17630.15353066216, 'training/walltime': 2724.3204288482666, 'training/entropy_loss': Array(-0.05337675, dtype=float32), 'training/policy_loss': Array(-0.00039029, dtype=float32), 'training/total_loss': Array(55.53542, dtype=float32), 'training/v_loss': Array(55.589188, dtype=float32), 'eval/episode_goal_distance': (Array(0.50484747, dtype=float32), Array(0.2554067, dtype=float32)), 'eval/episode_reward': (Array(-10722.686, dtype=float32), Array(4922.3887, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.75833, dtype=float32)), 'eval/epoch_eval_time': 4.434441804885864, 'eval/sps': 28864.963310369687}
I0727 19:47:33.773941 139998383736640 train.py:379] starting iteration 116 3264.5660285949707
I0727 19:48:01.391666 139998383736640 train.py:394] {'eval/walltime': 534.8950695991516, 'training/sps': 17663.728338261615, 'training/walltime': 2747.509191274643, 'training/entropy_loss': Array(-0.0533823, dtype=float32), 'training/policy_loss': Array(-0.00028504, dtype=float32), 'training/total_loss': Array(33.250076, dtype=float32), 'training/v_loss': Array(33.303738, dtype=float32), 'eval/episode_goal_distance': (Array(0.5258051, dtype=float32), Array(0.24457484, dtype=float32)), 'eval/episode_reward': (Array(-10350.632, dtype=float32), Array(5272.489, dtype=float32)), 'eval/avg_episode_length': (Array(868.0156, dtype=float32), Array(337.2563, dtype=float32)), 'eval/epoch_eval_time': 4.422461271286011, 'eval/sps': 28943.15905739493}
I0727 19:48:01.395319 139998383736640 train.py:379] starting iteration 117 3292.1874063014984
I0727 19:48:29.044990 139998383736640 train.py:394] {'eval/walltime': 539.3366916179657, 'training/sps': 17653.925454201042, 'training/walltime': 2770.710829973221, 'training/entropy_loss': Array(-0.05321561, dtype=float32), 'training/policy_loss': Array(-0.00040087, dtype=float32), 'training/total_loss': Array(35.198387, dtype=float32), 'training/v_loss': Array(35.252003, dtype=float32), 'eval/episode_goal_distance': (Array(0.5354767, dtype=float32), Array(0.28326857, dtype=float32)), 'eval/episode_reward': (Array(-11474.797, dtype=float32), Array(5313.402, dtype=float32)), 'eval/avg_episode_length': (Array(930.1719, dtype=float32), Array(253.91197, dtype=float32)), 'eval/epoch_eval_time': 4.441622018814087, 'eval/sps': 28818.300940019206}
I0727 19:48:29.048833 139998383736640 train.py:379] starting iteration 118 3319.840920686722
I0727 19:48:56.685332 139998383736640 train.py:394] {'eval/walltime': 543.7584285736084, 'training/sps': 17648.694215255367, 'training/walltime': 2793.919345855713, 'training/entropy_loss': Array(-0.05303406, dtype=float32), 'training/policy_loss': Array(-0.00014631, dtype=float32), 'training/total_loss': Array(40.882065, dtype=float32), 'training/v_loss': Array(40.935246, dtype=float32), 'eval/episode_goal_distance': (Array(0.5301114, dtype=float32), Array(0.28972873, dtype=float32)), 'eval/episode_reward': (Array(-10285.967, dtype=float32), Array(5345.8804, dtype=float32)), 'eval/avg_episode_length': (Array(899.09375, dtype=float32), Array(300.12082, dtype=float32)), 'eval/epoch_eval_time': 4.4217369556427, 'eval/sps': 28947.900176797193}
I0727 19:48:56.688945 139998383736640 train.py:379] starting iteration 119 3347.4810321331024
I0727 19:49:24.349032 139998383736640 train.py:394] {'eval/walltime': 548.178626537323, 'training/sps': 17629.609332886605, 'training/walltime': 2817.152986049652, 'training/entropy_loss': Array(-0.0527501, dtype=float32), 'training/policy_loss': Array(-0.00011633, dtype=float32), 'training/total_loss': Array(41.16423, dtype=float32), 'training/v_loss': Array(41.2171, dtype=float32), 'eval/episode_goal_distance': (Array(0.49684167, dtype=float32), Array(0.2481577, dtype=float32)), 'eval/episode_reward': (Array(-10195.799, dtype=float32), Array(5439.897, dtype=float32)), 'eval/avg_episode_length': (Array(875.77344, dtype=float32), Array(328.6733, dtype=float32)), 'eval/epoch_eval_time': 4.4201979637146, 'eval/sps': 28957.979043190342}
I0727 19:49:24.352638 139998383736640 train.py:379] starting iteration 120 3375.1447253227234
I0727 19:49:51.980445 139998383736640 train.py:394] {'eval/walltime': 552.5917010307312, 'training/sps': 17648.636017142195, 'training/walltime': 2840.361578464508, 'training/entropy_loss': Array(-0.05278362, dtype=float32), 'training/policy_loss': Array(-0.00021671, dtype=float32), 'training/total_loss': Array(52.23326, dtype=float32), 'training/v_loss': Array(52.28626, dtype=float32), 'eval/episode_goal_distance': (Array(0.4876005, dtype=float32), Array(0.23168595, dtype=float32)), 'eval/episode_reward': (Array(-10370.61, dtype=float32), Array(4551.2837, dtype=float32)), 'eval/avg_episode_length': (Array(945.625, dtype=float32), Array(226.07024, dtype=float32)), 'eval/epoch_eval_time': 4.413074493408203, 'eval/sps': 29004.7222613607}
I0727 19:49:51.984018 139998383736640 train.py:379] starting iteration 121 3402.7761054039
I0727 19:50:19.633408 139998383736640 train.py:394] {'eval/walltime': 557.0323960781097, 'training/sps': 17653.474478903074, 'training/walltime': 2863.5638098716736, 'training/entropy_loss': Array(-0.05263697, dtype=float32), 'training/policy_loss': Array(-0.00019584, dtype=float32), 'training/total_loss': Array(31.506454, dtype=float32), 'training/v_loss': Array(31.559288, dtype=float32), 'eval/episode_goal_distance': (Array(0.50801885, dtype=float32), Array(0.23857096, dtype=float32)), 'eval/episode_reward': (Array(-9851.573, dtype=float32), Array(5152.0796, dtype=float32)), 'eval/avg_episode_length': (Array(867.9375, dtype=float32), Array(337.45612, dtype=float32)), 'eval/epoch_eval_time': 4.44069504737854, 'eval/sps': 28824.316606825276}
I0727 19:50:19.637264 139998383736640 train.py:379] starting iteration 122 3430.429351568222
I0727 19:50:47.266081 139998383736640 train.py:394] {'eval/walltime': 561.4390416145325, 'training/sps': 17643.207331940186, 'training/walltime': 2886.779543399811, 'training/entropy_loss': Array(-0.05301677, dtype=float32), 'training/policy_loss': Array(-0.00016357, dtype=float32), 'training/total_loss': Array(33.817978, dtype=float32), 'training/v_loss': Array(33.87116, dtype=float32), 'eval/episode_goal_distance': (Array(0.50172806, dtype=float32), Array(0.23924391, dtype=float32)), 'eval/episode_reward': (Array(-9974.677, dtype=float32), Array(5407.4414, dtype=float32)), 'eval/avg_episode_length': (Array(891.2656, dtype=float32), Array(310.28156, dtype=float32)), 'eval/epoch_eval_time': 4.4066455364227295, 'eval/sps': 29047.037920801115}
I0727 19:50:47.269714 139998383736640 train.py:379] starting iteration 123 3458.061801671982
I0727 19:51:14.918751 139998383736640 train.py:394] {'eval/walltime': 565.8977031707764, 'training/sps': 17667.339889310086, 'training/walltime': 2909.9635655879974, 'training/entropy_loss': Array(-0.05312119, dtype=float32), 'training/policy_loss': Array(-9.423659e-05, dtype=float32), 'training/total_loss': Array(37.858356, dtype=float32), 'training/v_loss': Array(37.911568, dtype=float32), 'eval/episode_goal_distance': (Array(0.48964572, dtype=float32), Array(0.23617491, dtype=float32)), 'eval/episode_reward': (Array(-9774.778, dtype=float32), Array(5299.5034, dtype=float32)), 'eval/avg_episode_length': (Array(860.1719, dtype=float32), Array(345.665, dtype=float32)), 'eval/epoch_eval_time': 4.4586615562438965, 'eval/sps': 28708.166875942665}
I0727 19:51:14.922326 139998383736640 train.py:379] starting iteration 124 3485.7144129276276
I0727 19:51:42.555105 139998383736640 train.py:394] {'eval/walltime': 570.3409242630005, 'training/sps': 17668.067391705365, 'training/walltime': 2933.1466331481934, 'training/entropy_loss': Array(-0.05291159, dtype=float32), 'training/policy_loss': Array(-0.00014948, dtype=float32), 'training/total_loss': Array(40.004322, dtype=float32), 'training/v_loss': Array(40.057384, dtype=float32), 'eval/episode_goal_distance': (Array(0.49699506, dtype=float32), Array(0.23994114, dtype=float32)), 'eval/episode_reward': (Array(-9718.48, dtype=float32), Array(5281.841, dtype=float32)), 'eval/avg_episode_length': (Array(867.9375, dtype=float32), Array(337.45605, dtype=float32)), 'eval/epoch_eval_time': 4.443221092224121, 'eval/sps': 28807.929505017648}
I0727 19:51:42.558675 139998383736640 train.py:379] starting iteration 125 3513.35076212883
I0727 19:52:10.165501 139998383736640 train.py:394] {'eval/walltime': 574.7742516994476, 'training/sps': 17680.49450764326, 'training/walltime': 2956.3134059906006, 'training/entropy_loss': Array(-0.05280173, dtype=float32), 'training/policy_loss': Array(-0.00016393, dtype=float32), 'training/total_loss': Array(51.234108, dtype=float32), 'training/v_loss': Array(51.28707, dtype=float32), 'eval/episode_goal_distance': (Array(0.51248574, dtype=float32), Array(0.23745544, dtype=float32)), 'eval/episode_reward': (Array(-10495.145, dtype=float32), Array(5160.236, dtype=float32)), 'eval/avg_episode_length': (Array(906.7969, dtype=float32), Array(289.78052, dtype=float32)), 'eval/epoch_eval_time': 4.4333274364471436, 'eval/sps': 28872.218854779392}
I0727 19:52:10.481856 139998383736640 train.py:379] starting iteration 126 3541.2739219665527
I0727 19:52:38.085450 139998383736640 train.py:394] {'eval/walltime': 579.1872165203094, 'training/sps': 17667.51939732033, 'training/walltime': 2979.497192621231, 'training/entropy_loss': Array(-0.05302227, dtype=float32), 'training/policy_loss': Array(-0.00015352, dtype=float32), 'training/total_loss': Array(32.94532, dtype=float32), 'training/v_loss': Array(32.998497, dtype=float32), 'eval/episode_goal_distance': (Array(0.48961774, dtype=float32), Array(0.22678441, dtype=float32)), 'eval/episode_reward': (Array(-10232.061, dtype=float32), Array(5387.7285, dtype=float32)), 'eval/avg_episode_length': (Array(860.21094, dtype=float32), Array(345.5681, dtype=float32)), 'eval/epoch_eval_time': 4.412964820861816, 'eval/sps': 29005.443096870786}
I0727 19:52:38.089892 139998383736640 train.py:379] starting iteration 127 3568.8819789886475
I0727 19:53:05.714391 139998383736640 train.py:394] {'eval/walltime': 583.6327095031738, 'training/sps': 17676.038774075274, 'training/walltime': 3002.669805288315, 'training/entropy_loss': Array(-0.0528974, dtype=float32), 'training/policy_loss': Array(-0.00023814, dtype=float32), 'training/total_loss': Array(34.280647, dtype=float32), 'training/v_loss': Array(34.333782, dtype=float32), 'eval/episode_goal_distance': (Array(0.5192229, dtype=float32), Array(0.29053378, dtype=float32)), 'eval/episode_reward': (Array(-10385.662, dtype=float32), Array(5977.58, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.30685, dtype=float32)), 'eval/epoch_eval_time': 4.44549298286438, 'eval/sps': 28793.20707363378}
I0727 19:53:05.717911 139998383736640 train.py:379] starting iteration 128 3596.509997844696
I0727 19:53:33.354732 139998383736640 train.py:394] {'eval/walltime': 588.0785818099976, 'training/sps': 17666.90621446282, 'training/walltime': 3025.85439658165, 'training/entropy_loss': Array(-0.05314372, dtype=float32), 'training/policy_loss': Array(-6.26144e-05, dtype=float32), 'training/total_loss': Array(38.084984, dtype=float32), 'training/v_loss': Array(38.138187, dtype=float32), 'eval/episode_goal_distance': (Array(0.5002847, dtype=float32), Array(0.24560753, dtype=float32)), 'eval/episode_reward': (Array(-11039.455, dtype=float32), Array(5007.054, dtype=float32)), 'eval/avg_episode_length': (Array(945.6328, dtype=float32), Array(226.03806, dtype=float32)), 'eval/epoch_eval_time': 4.4458723068237305, 'eval/sps': 28790.750423384783}
I0727 19:53:33.358698 139998383736640 train.py:379] starting iteration 129 3624.1507847309113
I0727 19:54:00.981993 139998383736640 train.py:394] {'eval/walltime': 592.5252425670624, 'training/sps': 17677.801952944832, 'training/walltime': 3049.0246980190277, 'training/entropy_loss': Array(-0.05323232, dtype=float32), 'training/policy_loss': Array(-8.9372814e-05, dtype=float32), 'training/total_loss': Array(40.556908, dtype=float32), 'training/v_loss': Array(40.61023, dtype=float32), 'eval/episode_goal_distance': (Array(0.52721775, dtype=float32), Array(0.27664152, dtype=float32)), 'eval/episode_reward': (Array(-10691.47, dtype=float32), Array(5671.9033, dtype=float32)), 'eval/avg_episode_length': (Array(883.53906, dtype=float32), Array(319.65042, dtype=float32)), 'eval/epoch_eval_time': 4.446660757064819, 'eval/sps': 28785.645452407094}
I0727 19:54:00.985578 139998383736640 train.py:379] starting iteration 130 3651.7776651382446
I0727 19:54:28.632419 139998383736640 train.py:394] {'eval/walltime': 596.9696569442749, 'training/sps': 17658.21613245531, 'training/walltime': 3072.220699071884, 'training/entropy_loss': Array(-0.05310895, dtype=float32), 'training/policy_loss': Array(-0.00053919, dtype=float32), 'training/total_loss': Array(53.073074, dtype=float32), 'training/v_loss': Array(53.126717, dtype=float32), 'eval/episode_goal_distance': (Array(0.4963712, dtype=float32), Array(0.23790808, dtype=float32)), 'eval/episode_reward': (Array(-9176.737, dtype=float32), Array(5414.866, dtype=float32)), 'eval/avg_episode_length': (Array(829.09375, dtype=float32), Array(375.1458, dtype=float32)), 'eval/epoch_eval_time': 4.444414377212524, 'eval/sps': 28800.19483698094}
I0727 19:54:28.636147 139998383736640 train.py:379] starting iteration 131 3679.4282336235046
I0727 19:54:56.247709 139998383736640 train.py:394] {'eval/walltime': 601.407995223999, 'training/sps': 17680.33092974025, 'training/walltime': 3095.387686252594, 'training/entropy_loss': Array(-0.05280305, dtype=float32), 'training/policy_loss': Array(-0.00040778, dtype=float32), 'training/total_loss': Array(31.420877, dtype=float32), 'training/v_loss': Array(31.474087, dtype=float32), 'eval/episode_goal_distance': (Array(0.50499326, dtype=float32), Array(0.22376488, dtype=float32)), 'eval/episode_reward': (Array(-10666.527, dtype=float32), Array(4521.0454, dtype=float32)), 'eval/avg_episode_length': (Array(937.90625, dtype=float32), Array(240.48856, dtype=float32)), 'eval/epoch_eval_time': 4.438338279724121, 'eval/sps': 28839.622384068534}
I0727 19:54:56.251181 139998383736640 train.py:379] starting iteration 132 3707.0432686805725
I0727 19:55:23.891062 139998383736640 train.py:394] {'eval/walltime': 605.8420722484589, 'training/sps': 17655.66626080592, 'training/walltime': 3118.5870373249054, 'training/entropy_loss': Array(-0.05320556, dtype=float32), 'training/policy_loss': Array(-0.00034932, dtype=float32), 'training/total_loss': Array(33.407314, dtype=float32), 'training/v_loss': Array(33.46087, dtype=float32), 'eval/episode_goal_distance': (Array(0.4919501, dtype=float32), Array(0.24371842, dtype=float32)), 'eval/episode_reward': (Array(-10406.357, dtype=float32), Array(4780.6885, dtype=float32)), 'eval/avg_episode_length': (Array(922.3047, dtype=float32), Array(266.89276, dtype=float32)), 'eval/epoch_eval_time': 4.434077024459839, 'eval/sps': 28867.33795870247}
I0727 19:55:23.894781 139998383736640 train.py:379] starting iteration 133 3734.686867952347
I0727 19:55:51.528467 139998383736640 train.py:394] {'eval/walltime': 610.2695808410645, 'training/sps': 17655.300290991985, 'training/walltime': 3141.786869287491, 'training/entropy_loss': Array(-0.05340516, dtype=float32), 'training/policy_loss': Array(-0.00016255, dtype=float32), 'training/total_loss': Array(37.88771, dtype=float32), 'training/v_loss': Array(37.941277, dtype=float32), 'eval/episode_goal_distance': (Array(0.53134865, dtype=float32), Array(0.27273226, dtype=float32)), 'eval/episode_reward': (Array(-10549.68, dtype=float32), Array(5468.488, dtype=float32)), 'eval/avg_episode_length': (Array(891.3047, dtype=float32), Array(310.17084, dtype=float32)), 'eval/epoch_eval_time': 4.427508592605591, 'eval/sps': 28910.1641075917}
I0727 19:55:51.532021 139998383736640 train.py:379] starting iteration 134 3762.3241078853607
I0727 19:56:19.103009 139998383736640 train.py:394] {'eval/walltime': 614.6884217262268, 'training/sps': 17696.518328747035, 'training/walltime': 3164.9326651096344, 'training/entropy_loss': Array(-0.05344106, dtype=float32), 'training/policy_loss': Array(-0.00015535, dtype=float32), 'training/total_loss': Array(38.486763, dtype=float32), 'training/v_loss': Array(38.54036, dtype=float32), 'eval/episode_goal_distance': (Array(0.5441827, dtype=float32), Array(0.2658291, dtype=float32)), 'eval/episode_reward': (Array(-10294.761, dtype=float32), Array(5686.0396, dtype=float32)), 'eval/avg_episode_length': (Array(883.5781, dtype=float32), Array(319.54266, dtype=float32)), 'eval/epoch_eval_time': 4.4188408851623535, 'eval/sps': 28966.872382710182}
I0727 19:56:19.106673 139998383736640 train.py:379] starting iteration 135 3789.8987607955933
I0727 19:56:46.751582 139998383736640 train.py:394] {'eval/walltime': 619.0994641780853, 'training/sps': 17634.29943752931, 'training/walltime': 3188.1601259708405, 'training/entropy_loss': Array(-0.0535598, dtype=float32), 'training/policy_loss': Array(-0.00036498, dtype=float32), 'training/total_loss': Array(53.722435, dtype=float32), 'training/v_loss': Array(53.77636, dtype=float32), 'eval/episode_goal_distance': (Array(0.55638075, dtype=float32), Array(0.27687472, dtype=float32)), 'eval/episode_reward': (Array(-10610.788, dtype=float32), Array(5711.693, dtype=float32)), 'eval/avg_episode_length': (Array(868.0781, dtype=float32), Array(337.09708, dtype=float32)), 'eval/epoch_eval_time': 4.4110424518585205, 'eval/sps': 29018.08391031678}
I0727 19:56:46.755149 139998383736640 train.py:379] starting iteration 136 3817.547236919403
I0727 19:57:14.330267 139998383736640 train.py:394] {'eval/walltime': 623.5020496845245, 'training/sps': 17680.864070200685, 'training/walltime': 3211.3264145851135, 'training/entropy_loss': Array(-0.0532076, dtype=float32), 'training/policy_loss': Array(-0.00014148, dtype=float32), 'training/total_loss': Array(32.071198, dtype=float32), 'training/v_loss': Array(32.12455, dtype=float32), 'eval/episode_goal_distance': (Array(0.51829714, dtype=float32), Array(0.25441852, dtype=float32)), 'eval/episode_reward': (Array(-10626.715, dtype=float32), Array(5078.927, dtype=float32)), 'eval/avg_episode_length': (Array(899.0469, dtype=float32), Array(300.26016, dtype=float32)), 'eval/epoch_eval_time': 4.402585506439209, 'eval/sps': 29073.824872404537}
I0727 19:57:14.334027 139998383736640 train.py:379] starting iteration 137 3845.12611413002
I0727 19:57:41.947979 139998383736640 train.py:394] {'eval/walltime': 627.9381432533264, 'training/sps': 17676.859025008645, 'training/walltime': 3234.4979519844055, 'training/entropy_loss': Array(-0.05322572, dtype=float32), 'training/policy_loss': Array(-0.00019912, dtype=float32), 'training/total_loss': Array(32.88136, dtype=float32), 'training/v_loss': Array(32.934784, dtype=float32), 'eval/episode_goal_distance': (Array(0.48204857, dtype=float32), Array(0.23779178, dtype=float32)), 'eval/episode_reward': (Array(-10247.33, dtype=float32), Array(5325.025, dtype=float32)), 'eval/avg_episode_length': (Array(906.7578, dtype=float32), Array(289.90192, dtype=float32)), 'eval/epoch_eval_time': 4.43609356880188, 'eval/sps': 28854.215542295427}
I0727 19:57:41.951700 139998383736640 train.py:379] starting iteration 138 3872.7437872886658
I0727 19:58:09.588474 139998383736640 train.py:394] {'eval/walltime': 632.3737778663635, 'training/sps': 17659.109514839718, 'training/walltime': 3257.6927795410156, 'training/entropy_loss': Array(-0.05328235, dtype=float32), 'training/policy_loss': Array(-0.00029633, dtype=float32), 'training/total_loss': Array(35.787567, dtype=float32), 'training/v_loss': Array(35.841145, dtype=float32), 'eval/episode_goal_distance': (Array(0.53104925, dtype=float32), Array(0.2617261, dtype=float32)), 'eval/episode_reward': (Array(-10744.004, dtype=float32), Array(6057.908, dtype=float32)), 'eval/avg_episode_length': (Array(875.7578, dtype=float32), Array(328.7146, dtype=float32)), 'eval/epoch_eval_time': 4.435634613037109, 'eval/sps': 28857.201092214746}
I0727 19:58:09.592156 139998383736640 train.py:379] starting iteration 139 3900.3842437267303
I0727 19:58:37.247159 139998383736640 train.py:394] {'eval/walltime': 636.823944568634, 'training/sps': 17656.424920825542, 'training/walltime': 3280.891133785248, 'training/entropy_loss': Array(-0.05311761, dtype=float32), 'training/policy_loss': Array(-0.00027989, dtype=float32), 'training/total_loss': Array(38.08381, dtype=float32), 'training/v_loss': Array(38.137207, dtype=float32), 'eval/episode_goal_distance': (Array(0.5077499, dtype=float32), Array(0.24584024, dtype=float32)), 'eval/episode_reward': (Array(-10323.771, dtype=float32), Array(4769.4824, dtype=float32)), 'eval/avg_episode_length': (Array(914.6406, dtype=float32), Array(278.3871, dtype=float32)), 'eval/epoch_eval_time': 4.450166702270508, 'eval/sps': 28762.967448993193}
I0727 19:58:37.250794 139998383736640 train.py:379] starting iteration 140 3928.0428814888
I0727 19:59:04.893232 139998383736640 train.py:394] {'eval/walltime': 641.2647929191589, 'training/sps': 17658.7742589644, 'training/walltime': 3304.0864017009735, 'training/entropy_loss': Array(-0.05291814, dtype=float32), 'training/policy_loss': Array(-0.00033707, dtype=float32), 'training/total_loss': Array(49.456066, dtype=float32), 'training/v_loss': Array(49.509323, dtype=float32), 'eval/episode_goal_distance': (Array(0.48962408, dtype=float32), Array(0.2050777, dtype=float32)), 'eval/episode_reward': (Array(-9779.957, dtype=float32), Array(4323.709, dtype=float32)), 'eval/avg_episode_length': (Array(906.78906, dtype=float32), Array(289.8049, dtype=float32)), 'eval/epoch_eval_time': 4.440848350524902, 'eval/sps': 28823.321558564498}
I0727 19:59:04.897255 139998383736640 train.py:379] starting iteration 141 3955.6893424987793
I0727 19:59:32.471084 139998383736640 train.py:394] {'eval/walltime': 645.6842360496521, 'training/sps': 17694.78276172261, 'training/walltime': 3327.2344677448273, 'training/entropy_loss': Array(-0.05308923, dtype=float32), 'training/policy_loss': Array(-0.0002986, dtype=float32), 'training/total_loss': Array(32.66164, dtype=float32), 'training/v_loss': Array(32.715027, dtype=float32), 'eval/episode_goal_distance': (Array(0.4985525, dtype=float32), Array(0.26099524, dtype=float32)), 'eval/episode_reward': (Array(-10064.946, dtype=float32), Array(5379.4575, dtype=float32)), 'eval/avg_episode_length': (Array(891.3125, dtype=float32), Array(310.14804, dtype=float32)), 'eval/epoch_eval_time': 4.419443130493164, 'eval/sps': 28962.925015785084}
I0727 19:59:32.474718 139998383736640 train.py:379] starting iteration 142 3983.2668051719666
I0727 20:00:00.060402 139998383736640 train.py:394] {'eval/walltime': 650.1006555557251, 'training/sps': 17683.477651372545, 'training/walltime': 3350.397332429886, 'training/entropy_loss': Array(-0.0531109, dtype=float32), 'training/policy_loss': Array(-0.00027907, dtype=float32), 'training/total_loss': Array(33.032337, dtype=float32), 'training/v_loss': Array(33.08573, dtype=float32), 'eval/episode_goal_distance': (Array(0.47603726, dtype=float32), Array(0.23809443, dtype=float32)), 'eval/episode_reward': (Array(-9837.732, dtype=float32), Array(5399.918, dtype=float32)), 'eval/avg_episode_length': (Array(860.2578, dtype=float32), Array(345.4525, dtype=float32)), 'eval/epoch_eval_time': 4.416419506072998, 'eval/sps': 28982.753976153712}
I0727 20:00:00.064214 139998383736640 train.py:379] starting iteration 143 4010.856302022934
I0727 20:00:27.674042 139998383736640 train.py:394] {'eval/walltime': 654.5464632511139, 'training/sps': 17687.3425663955, 'training/walltime': 3373.5551357269287, 'training/entropy_loss': Array(-0.05299719, dtype=float32), 'training/policy_loss': Array(-0.0001111, dtype=float32), 'training/total_loss': Array(34.38427, dtype=float32), 'training/v_loss': Array(34.437378, dtype=float32), 'eval/episode_goal_distance': (Array(0.49147487, dtype=float32), Array(0.22366935, dtype=float32)), 'eval/episode_reward': (Array(-9943.631, dtype=float32), Array(5220.4937, dtype=float32)), 'eval/avg_episode_length': (Array(867.97656, dtype=float32), Array(337.35626, dtype=float32)), 'eval/epoch_eval_time': 4.445807695388794, 'eval/sps': 28791.16884267442}
I0727 20:00:27.677636 139998383736640 train.py:379] starting iteration 144 4038.4697234630585
I0727 20:00:55.274287 139998383736640 train.py:394] {'eval/walltime': 658.9855296611786, 'training/sps': 17692.627720427, 'training/walltime': 3396.706021308899, 'training/entropy_loss': Array(-0.05319298, dtype=float32), 'training/policy_loss': Array(-0.00019433, dtype=float32), 'training/total_loss': Array(36.42497, dtype=float32), 'training/v_loss': Array(36.478355, dtype=float32), 'eval/episode_goal_distance': (Array(0.5432494, dtype=float32), Array(0.25779337, dtype=float32)), 'eval/episode_reward': (Array(-10609.777, dtype=float32), Array(5571.4053, dtype=float32)), 'eval/avg_episode_length': (Array(868.0625, dtype=float32), Array(337.1368, dtype=float32)), 'eval/epoch_eval_time': 4.439066410064697, 'eval/sps': 28834.89188397487}
I0727 20:00:55.278052 139998383736640 train.py:379] starting iteration 145 4066.0701401233673
I0727 20:01:22.931493 139998383736640 train.py:394] {'eval/walltime': 663.4316017627716, 'training/sps': 17654.668180554003, 'training/walltime': 3419.906683921814, 'training/entropy_loss': Array(-0.0532681, dtype=float32), 'training/policy_loss': Array(-0.00036618, dtype=float32), 'training/total_loss': Array(48.604855, dtype=float32), 'training/v_loss': Array(48.65849, dtype=float32), 'eval/episode_goal_distance': (Array(0.48575616, dtype=float32), Array(0.20088282, dtype=float32)), 'eval/episode_reward': (Array(-9921.926, dtype=float32), Array(4851.946, dtype=float32)), 'eval/avg_episode_length': (Array(883.4297, dtype=float32), Array(319.94998, dtype=float32)), 'eval/epoch_eval_time': 4.446072101593018, 'eval/sps': 28789.456642895624}
I0727 20:01:22.935205 139998383736640 train.py:379] starting iteration 146 4093.727292776108
I0727 20:01:50.566864 139998383736640 train.py:394] {'eval/walltime': 667.884265422821, 'training/sps': 17676.243011258794, 'training/walltime': 3443.0790288448334, 'training/entropy_loss': Array(-0.05300292, dtype=float32), 'training/policy_loss': Array(-0.00032751, dtype=float32), 'training/total_loss': Array(29.575474, dtype=float32), 'training/v_loss': Array(29.628805, dtype=float32), 'eval/episode_goal_distance': (Array(0.48664343, dtype=float32), Array(0.25428566, dtype=float32)), 'eval/episode_reward': (Array(-9695.637, dtype=float32), Array(5176.582, dtype=float32)), 'eval/avg_episode_length': (Array(875.78906, dtype=float32), Array(328.6325, dtype=float32)), 'eval/epoch_eval_time': 4.4526636600494385, 'eval/sps': 28746.837796991564}
I0727 20:01:50.570568 139998383736640 train.py:379] starting iteration 147 4121.362655639648
I0727 20:02:18.205632 139998383736640 train.py:394] {'eval/walltime': 672.3218309879303, 'training/sps': 17663.053311695552, 'training/walltime': 3466.2686774730682, 'training/entropy_loss': Array(-0.05280115, dtype=float32), 'training/policy_loss': Array(-0.00033833, dtype=float32), 'training/total_loss': Array(30.94357, dtype=float32), 'training/v_loss': Array(30.99671, dtype=float32), 'eval/episode_goal_distance': (Array(0.4855316, dtype=float32), Array(0.22086266, dtype=float32)), 'eval/episode_reward': (Array(-10271.495, dtype=float32), Array(4925.495, dtype=float32)), 'eval/avg_episode_length': (Array(906.66406, dtype=float32), Array(290.19354, dtype=float32)), 'eval/epoch_eval_time': 4.437565565109253, 'eval/sps': 28844.64423611252}
I0727 20:02:18.209354 139998383736640 train.py:379] starting iteration 148 4149.001440525055
I0727 20:02:45.852904 139998383736640 train.py:394] {'eval/walltime': 676.754714012146, 'training/sps': 17652.06328914982, 'training/walltime': 3489.472763776779, 'training/entropy_loss': Array(-0.05312272, dtype=float32), 'training/policy_loss': Array(-0.00013033, dtype=float32), 'training/total_loss': Array(35.884705, dtype=float32), 'training/v_loss': Array(35.937954, dtype=float32), 'eval/episode_goal_distance': (Array(0.493622, dtype=float32), Array(0.23935284, dtype=float32)), 'eval/episode_reward': (Array(-10098.063, dtype=float32), Array(4719.2153, dtype=float32)), 'eval/avg_episode_length': (Array(906.9453, dtype=float32), Array(289.31924, dtype=float32)), 'eval/epoch_eval_time': 4.432883024215698, 'eval/sps': 28875.11339703055}
I0727 20:02:45.856471 139998383736640 train.py:379] starting iteration 149 4176.648558139801
I0727 20:03:13.500341 139998383736640 train.py:394] {'eval/walltime': 681.1858115196228, 'training/sps': 17650.441425182285, 'training/walltime': 3512.678982257843, 'training/entropy_loss': Array(-0.05312071, dtype=float32), 'training/policy_loss': Array(-0.00018332, dtype=float32), 'training/total_loss': Array(37.387737, dtype=float32), 'training/v_loss': Array(37.44104, dtype=float32), 'eval/episode_goal_distance': (Array(0.47458017, dtype=float32), Array(0.23587863, dtype=float32)), 'eval/episode_reward': (Array(-9317.594, dtype=float32), Array(5379.9854, dtype=float32)), 'eval/avg_episode_length': (Array(829.0625, dtype=float32), Array(375.21448, dtype=float32)), 'eval/epoch_eval_time': 4.431097507476807, 'eval/sps': 28886.74866306132}
I0727 20:03:13.503859 139998383736640 train.py:379] starting iteration 150 4204.295946359634
I0727 20:03:41.136801 139998383736640 train.py:394] {'eval/walltime': 685.6129567623138, 'training/sps': 17655.785834796985, 'training/walltime': 3535.878176212311, 'training/entropy_loss': Array(-0.05345744, dtype=float32), 'training/policy_loss': Array(-0.00027128, dtype=float32), 'training/total_loss': Array(53.56734, dtype=float32), 'training/v_loss': Array(53.621067, dtype=float32), 'eval/episode_goal_distance': (Array(0.49533173, dtype=float32), Array(0.25640282, dtype=float32)), 'eval/episode_reward': (Array(-10187.972, dtype=float32), Array(5331.9946, dtype=float32)), 'eval/avg_episode_length': (Array(883.59375, dtype=float32), Array(319.49982, dtype=float32)), 'eval/epoch_eval_time': 4.42714524269104, 'eval/sps': 28912.536856865172}
I0727 20:03:41.227850 139998383736640 train.py:379] starting iteration 151 4232.019919395447
I0727 20:04:08.843084 139998383736640 train.py:394] {'eval/walltime': 690.0237390995026, 'training/sps': 17656.863888315023, 'training/walltime': 3559.075953722, 'training/entropy_loss': Array(-0.053236, dtype=float32), 'training/policy_loss': Array(-0.00033338, dtype=float32), 'training/total_loss': Array(30.596626, dtype=float32), 'training/v_loss': Array(30.650196, dtype=float32), 'eval/episode_goal_distance': (Array(0.5018626, dtype=float32), Array(0.23911393, dtype=float32)), 'eval/episode_reward': (Array(-10075.467, dtype=float32), Array(5295.186, dtype=float32)), 'eval/avg_episode_length': (Array(891.21094, dtype=float32), Array(310.43747, dtype=float32)), 'eval/epoch_eval_time': 4.410782337188721, 'eval/sps': 29019.795178009794}
I0727 20:04:08.846895 139998383736640 train.py:379] starting iteration 152 4259.638982057571
I0727 20:04:36.490275 139998383736640 train.py:394] {'eval/walltime': 694.4695491790771, 'training/sps': 17661.933466167575, 'training/walltime': 3582.2670726776123, 'training/entropy_loss': Array(-0.05331157, dtype=float32), 'training/policy_loss': Array(-0.00030932, dtype=float32), 'training/total_loss': Array(31.58122, dtype=float32), 'training/v_loss': Array(31.634842, dtype=float32), 'eval/episode_goal_distance': (Array(0.477457, dtype=float32), Array(0.21726994, dtype=float32)), 'eval/episode_reward': (Array(-9898.773, dtype=float32), Array(4988.05, dtype=float32)), 'eval/avg_episode_length': (Array(883.5, dtype=float32), Array(319.75702, dtype=float32)), 'eval/epoch_eval_time': 4.445810079574585, 'eval/sps': 28791.153402632124}
I0727 20:04:36.493931 139998383736640 train.py:379] starting iteration 153 4287.286018371582
I0727 20:05:04.131001 139998383736640 train.py:394] {'eval/walltime': 698.9116997718811, 'training/sps': 17663.998581276053, 'training/walltime': 3605.455480337143, 'training/entropy_loss': Array(-0.05331454, dtype=float32), 'training/policy_loss': Array(-0.00029383, dtype=float32), 'training/total_loss': Array(34.80234, dtype=float32), 'training/v_loss': Array(34.85595, dtype=float32), 'eval/episode_goal_distance': (Array(0.5196505, dtype=float32), Array(0.23574199, dtype=float32)), 'eval/episode_reward': (Array(-10848.038, dtype=float32), Array(4740.0728, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.75885, dtype=float32)), 'eval/epoch_eval_time': 4.442150592803955, 'eval/sps': 28814.871834231173}
I0727 20:05:04.134505 139998383736640 train.py:379] starting iteration 154 4314.926591873169
I0727 20:05:31.756813 139998383736640 train.py:394] {'eval/walltime': 703.3518693447113, 'training/sps': 17674.76762647627, 'training/walltime': 3628.6297595500946, 'training/entropy_loss': Array(-0.05343317, dtype=float32), 'training/policy_loss': Array(-2.2346626e-06, dtype=float32), 'training/total_loss': Array(35.463913, dtype=float32), 'training/v_loss': Array(35.51735, dtype=float32), 'eval/episode_goal_distance': (Array(0.53211415, dtype=float32), Array(0.27020037, dtype=float32)), 'eval/episode_reward': (Array(-10345., dtype=float32), Array(5871.2393, dtype=float32)), 'eval/avg_episode_length': (Array(860.2422, dtype=float32), Array(345.49146, dtype=float32)), 'eval/epoch_eval_time': 4.4401695728302, 'eval/sps': 28827.727837973485}
I0727 20:05:31.760477 139998383736640 train.py:379] starting iteration 155 4342.55256485939
I0727 20:05:59.404946 139998383736640 train.py:394] {'eval/walltime': 707.7926561832428, 'training/sps': 17657.12883998327, 'training/walltime': 3651.8271889686584, 'training/entropy_loss': Array(-0.05345498, dtype=float32), 'training/policy_loss': Array(-0.00034724, dtype=float32), 'training/total_loss': Array(51.77774, dtype=float32), 'training/v_loss': Array(51.831543, dtype=float32), 'eval/episode_goal_distance': (Array(0.5066783, dtype=float32), Array(0.25689083, dtype=float32)), 'eval/episode_reward': (Array(-10461.486, dtype=float32), Array(5131.7217, dtype=float32)), 'eval/avg_episode_length': (Array(899.0547, dtype=float32), Array(300.23697, dtype=float32)), 'eval/epoch_eval_time': 4.440786838531494, 'eval/sps': 28823.72080762332}
I0727 20:05:59.408559 139998383736640 train.py:379] starting iteration 156 4370.200646400452
I0727 20:06:27.060511 139998383736640 train.py:394] {'eval/walltime': 712.2403490543365, 'training/sps': 17656.74411825274, 'training/walltime': 3675.02512383461, 'training/entropy_loss': Array(-0.052858, dtype=float32), 'training/policy_loss': Array(-0.00029652, dtype=float32), 'training/total_loss': Array(29.464693, dtype=float32), 'training/v_loss': Array(29.517847, dtype=float32), 'eval/episode_goal_distance': (Array(0.4586464, dtype=float32), Array(0.19691432, dtype=float32)), 'eval/episode_reward': (Array(-9543.952, dtype=float32), Array(4792.9614, dtype=float32)), 'eval/avg_episode_length': (Array(883.59375, dtype=float32), Array(319.50006, dtype=float32)), 'eval/epoch_eval_time': 4.44769287109375, 'eval/sps': 28778.965569293683}
I0727 20:06:27.064038 139998383736640 train.py:379] starting iteration 157 4397.856126308441
I0727 20:06:54.726179 139998383736640 train.py:394] {'eval/walltime': 716.689713716507, 'training/sps': 17650.321198223883, 'training/walltime': 3698.2315003871918, 'training/entropy_loss': Array(-0.05252974, dtype=float32), 'training/policy_loss': Array(-0.00020946, dtype=float32), 'training/total_loss': Array(30.153366, dtype=float32), 'training/v_loss': Array(30.206108, dtype=float32), 'eval/episode_goal_distance': (Array(0.50200164, dtype=float32), Array(0.23220544, dtype=float32)), 'eval/episode_reward': (Array(-10240.749, dtype=float32), Array(5249.7803, dtype=float32)), 'eval/avg_episode_length': (Array(883.4922, dtype=float32), Array(319.77908, dtype=float32)), 'eval/epoch_eval_time': 4.44936466217041, 'eval/sps': 28768.152246159414}
I0727 20:06:54.732597 139998383736640 train.py:379] starting iteration 158 4425.524684667587
I0727 20:07:22.363310 139998383736640 train.py:394] {'eval/walltime': 721.1316781044006, 'training/sps': 17668.51202618794, 'training/walltime': 3721.4139845371246, 'training/entropy_loss': Array(-0.05278286, dtype=float32), 'training/policy_loss': Array(-0.00018739, dtype=float32), 'training/total_loss': Array(33.411697, dtype=float32), 'training/v_loss': Array(33.464664, dtype=float32), 'eval/episode_goal_distance': (Array(0.5237573, dtype=float32), Array(0.26632112, dtype=float32)), 'eval/episode_reward': (Array(-10017.629, dtype=float32), Array(6101.404, dtype=float32)), 'eval/avg_episode_length': (Array(829.2031, dtype=float32), Array(374.90604, dtype=float32)), 'eval/epoch_eval_time': 4.441964387893677, 'eval/sps': 28816.079739148016}
I0727 20:07:22.366908 139998383736640 train.py:379] starting iteration 159 4453.158995389938
I0727 20:07:50.027446 139998383736640 train.py:394] {'eval/walltime': 725.581746339798, 'training/sps': 17652.189706588677, 'training/walltime': 3744.617904663086, 'training/entropy_loss': Array(-0.0529968, dtype=float32), 'training/policy_loss': Array(-0.00026246, dtype=float32), 'training/total_loss': Array(37.07132, dtype=float32), 'training/v_loss': Array(37.12458, dtype=float32), 'eval/episode_goal_distance': (Array(0.48223448, dtype=float32), Array(0.21725938, dtype=float32)), 'eval/episode_reward': (Array(-9781.017, dtype=float32), Array(5140.2334, dtype=float32)), 'eval/avg_episode_length': (Array(860.33594, dtype=float32), Array(345.25946, dtype=float32)), 'eval/epoch_eval_time': 4.450068235397339, 'eval/sps': 28763.603888552756}
I0727 20:07:50.030983 139998383736640 train.py:379] starting iteration 160 4480.823070526123
I0727 20:08:17.681909 139998383736640 train.py:394] {'eval/walltime': 730.031644821167, 'training/sps': 17659.25999386814, 'training/walltime': 3767.812534570694, 'training/entropy_loss': Array(-0.05302856, dtype=float32), 'training/policy_loss': Array(-0.00011474, dtype=float32), 'training/total_loss': Array(48.177284, dtype=float32), 'training/v_loss': Array(48.23043, dtype=float32), 'eval/episode_goal_distance': (Array(0.5050293, dtype=float32), Array(0.23102468, dtype=float32)), 'eval/episode_reward': (Array(-9859.127, dtype=float32), Array(5683.3936, dtype=float32)), 'eval/avg_episode_length': (Array(836.9922, dtype=float32), Array(367.9526, dtype=float32)), 'eval/epoch_eval_time': 4.4498984813690186, 'eval/sps': 28764.701157995987}
I0727 20:08:17.685497 139998383736640 train.py:379] starting iteration 161 4508.477584123611
I0727 20:08:45.332572 139998383736640 train.py:394] {'eval/walltime': 734.4680032730103, 'training/sps': 17651.829503276716, 'training/walltime': 3791.0169281959534, 'training/entropy_loss': Array(-0.05280242, dtype=float32), 'training/policy_loss': Array(-0.00028318, dtype=float32), 'training/total_loss': Array(29.874264, dtype=float32), 'training/v_loss': Array(29.92735, dtype=float32), 'eval/episode_goal_distance': (Array(0.49715957, dtype=float32), Array(0.2337706, dtype=float32)), 'eval/episode_reward': (Array(-9683.908, dtype=float32), Array(5369.83, dtype=float32)), 'eval/avg_episode_length': (Array(844.6797, dtype=float32), Array(360.93256, dtype=float32)), 'eval/epoch_eval_time': 4.436358451843262, 'eval/sps': 28852.49273462502}
I0727 20:08:45.336117 139998383736640 train.py:379] starting iteration 162 4536.128205060959
I0727 20:09:12.993953 139998383736640 train.py:394] {'eval/walltime': 738.9146175384521, 'training/sps': 17651.46042802179, 'training/walltime': 3814.2218070030212, 'training/entropy_loss': Array(-0.05279174, dtype=float32), 'training/policy_loss': Array(-6.556432e-05, dtype=float32), 'training/total_loss': Array(29.729416, dtype=float32), 'training/v_loss': Array(29.782269, dtype=float32), 'eval/episode_goal_distance': (Array(0.48308206, dtype=float32), Array(0.25491762, dtype=float32)), 'eval/episode_reward': (Array(-10131.789, dtype=float32), Array(4801.928, dtype=float32)), 'eval/avg_episode_length': (Array(914.53906, dtype=float32), Array(278.71802, dtype=float32)), 'eval/epoch_eval_time': 4.4466142654418945, 'eval/sps': 28785.94642103044}
I0727 20:09:12.997515 139998383736640 train.py:379] starting iteration 163 4563.7896027565
I0727 20:09:40.605517 139998383736640 train.py:394] {'eval/walltime': 743.3359432220459, 'training/sps': 17670.04034395088, 'training/walltime': 3837.402286052704, 'training/entropy_loss': Array(-0.05309383, dtype=float32), 'training/policy_loss': Array(-0.0003123, dtype=float32), 'training/total_loss': Array(33.260353, dtype=float32), 'training/v_loss': Array(33.31376, dtype=float32), 'eval/episode_goal_distance': (Array(0.48978877, dtype=float32), Array(0.22353208, dtype=float32)), 'eval/episode_reward': (Array(-10163.9, dtype=float32), Array(4780.075, dtype=float32)), 'eval/avg_episode_length': (Array(922.3047, dtype=float32), Array(266.89233, dtype=float32)), 'eval/epoch_eval_time': 4.42132568359375, 'eval/sps': 28950.592912657547}
I0727 20:09:40.609048 139998383736640 train.py:379] starting iteration 164 4591.40113568306
I0727 20:10:08.263922 139998383736640 train.py:394] {'eval/walltime': 747.7835414409637, 'training/sps': 17654.320212552237, 'training/walltime': 3860.6034059524536, 'training/entropy_loss': Array(-0.05290837, dtype=float32), 'training/policy_loss': Array(-0.00017895, dtype=float32), 'training/total_loss': Array(35.37359, dtype=float32), 'training/v_loss': Array(35.426674, dtype=float32), 'eval/episode_goal_distance': (Array(0.5293374, dtype=float32), Array(0.27092242, dtype=float32)), 'eval/episode_reward': (Array(-10612.247, dtype=float32), Array(5472.0483, dtype=float32)), 'eval/avg_episode_length': (Array(899.125, dtype=float32), Array(300.02798, dtype=float32)), 'eval/epoch_eval_time': 4.447598218917847, 'eval/sps': 28779.578032825077}
I0727 20:10:08.267643 139998383736640 train.py:379] starting iteration 165 4619.059730768204
I0727 20:10:35.903042 139998383736640 train.py:394] {'eval/walltime': 752.1955864429474, 'training/sps': 17642.19362980721, 'training/walltime': 3883.820473432541, 'training/entropy_loss': Array(-0.05320742, dtype=float32), 'training/policy_loss': Array(-0.00023742, dtype=float32), 'training/total_loss': Array(50.93289, dtype=float32), 'training/v_loss': Array(50.986336, dtype=float32), 'eval/episode_goal_distance': (Array(0.5196992, dtype=float32), Array(0.23393093, dtype=float32)), 'eval/episode_reward': (Array(-10217.319, dtype=float32), Array(5174.234, dtype=float32)), 'eval/avg_episode_length': (Array(875.8906, dtype=float32), Array(328.36343, dtype=float32)), 'eval/epoch_eval_time': 4.412045001983643, 'eval/sps': 29011.490123616502}
I0727 20:10:35.906708 139998383736640 train.py:379] starting iteration 166 4646.698794603348
I0727 20:11:03.542623 139998383736640 train.py:394] {'eval/walltime': 756.6409242153168, 'training/sps': 17667.0771746484, 'training/walltime': 3907.004840373993, 'training/entropy_loss': Array(-0.05324485, dtype=float32), 'training/policy_loss': Array(-0.00037839, dtype=float32), 'training/total_loss': Array(29.817669, dtype=float32), 'training/v_loss': Array(29.871294, dtype=float32), 'eval/episode_goal_distance': (Array(0.5111686, dtype=float32), Array(0.26341286, dtype=float32)), 'eval/episode_reward': (Array(-10077.816, dtype=float32), Array(4973.403, dtype=float32)), 'eval/avg_episode_length': (Array(906.77344, dtype=float32), Array(289.85355, dtype=float32)), 'eval/epoch_eval_time': 4.445337772369385, 'eval/sps': 28794.212398347277}
I0727 20:11:03.546220 139998383736640 train.py:379] starting iteration 167 4674.338307380676
I0727 20:11:31.196726 139998383736640 train.py:394] {'eval/walltime': 761.0883417129517, 'training/sps': 17657.62537356363, 'training/walltime': 3930.2016174793243, 'training/entropy_loss': Array(-0.05333405, dtype=float32), 'training/policy_loss': Array(-0.00026301, dtype=float32), 'training/total_loss': Array(30.236515, dtype=float32), 'training/v_loss': Array(30.290115, dtype=float32), 'eval/episode_goal_distance': (Array(0.5336428, dtype=float32), Array(0.2808075, dtype=float32)), 'eval/episode_reward': (Array(-9871.99, dtype=float32), Array(6172.7217, dtype=float32)), 'eval/avg_episode_length': (Array(821.375, dtype=float32), Array(381.6579, dtype=float32)), 'eval/epoch_eval_time': 4.447417497634888, 'eval/sps': 28780.74749403889}
I0727 20:11:31.200340 139998383736640 train.py:379] starting iteration 168 4701.992427110672
I0727 20:11:58.837279 139998383736640 train.py:394] {'eval/walltime': 765.5321371555328, 'training/sps': 17665.28434661271, 'training/walltime': 3953.3883373737335, 'training/entropy_loss': Array(-0.05340559, dtype=float32), 'training/policy_loss': Array(-0.0002577, dtype=float32), 'training/total_loss': Array(31.796051, dtype=float32), 'training/v_loss': Array(31.849712, dtype=float32), 'eval/episode_goal_distance': (Array(0.50299555, dtype=float32), Array(0.25194588, dtype=float32)), 'eval/episode_reward': (Array(-10759.142, dtype=float32), Array(4787.422, dtype=float32)), 'eval/avg_episode_length': (Array(922.33594, dtype=float32), Array(266.78522, dtype=float32)), 'eval/epoch_eval_time': 4.443795442581177, 'eval/sps': 28804.206146278248}
I0727 20:11:58.840861 139998383736640 train.py:379] starting iteration 169 4729.6329481601715
I0727 20:12:26.483719 139998383736640 train.py:394] {'eval/walltime': 769.9710803031921, 'training/sps': 17657.265311342355, 'training/walltime': 3976.585587501526, 'training/entropy_loss': Array(-0.05317721, dtype=float32), 'training/policy_loss': Array(-0.00017644, dtype=float32), 'training/total_loss': Array(33.277695, dtype=float32), 'training/v_loss': Array(33.331047, dtype=float32), 'eval/episode_goal_distance': (Array(0.5176694, dtype=float32), Array(0.27274835, dtype=float32)), 'eval/episode_reward': (Array(-9913.694, dtype=float32), Array(5168.6875, dtype=float32)), 'eval/avg_episode_length': (Array(875.78125, dtype=float32), Array(328.65274, dtype=float32)), 'eval/epoch_eval_time': 4.438943147659302, 'eval/sps': 28835.69258315364}
I0727 20:12:26.487355 139998383736640 train.py:379] starting iteration 170 4757.27944278717
I0727 20:12:54.132245 139998383736640 train.py:394] {'eval/walltime': 774.4106528759003, 'training/sps': 17656.220959894443, 'training/walltime': 3999.784209728241, 'training/entropy_loss': Array(-0.05294961, dtype=float32), 'training/policy_loss': Array(-0.00033225, dtype=float32), 'training/total_loss': Array(47.548897, dtype=float32), 'training/v_loss': Array(47.602177, dtype=float32), 'eval/episode_goal_distance': (Array(0.48819783, dtype=float32), Array(0.23881614, dtype=float32)), 'eval/episode_reward': (Array(-10021.791, dtype=float32), Array(5144.8936, dtype=float32)), 'eval/avg_episode_length': (Array(891.21094, dtype=float32), Array(310.4377, dtype=float32)), 'eval/epoch_eval_time': 4.43957257270813, 'eval/sps': 28831.60437265254}
I0727 20:12:54.135891 139998383736640 train.py:379] starting iteration 171 4784.927977323532
I0727 20:13:21.774792 139998383736640 train.py:394] {'eval/walltime': 778.8472988605499, 'training/sps': 17658.603277924263, 'training/walltime': 4022.979702234268, 'training/entropy_loss': Array(-0.05297004, dtype=float32), 'training/policy_loss': Array(-0.00028841, dtype=float32), 'training/total_loss': Array(29.567944, dtype=float32), 'training/v_loss': Array(29.621202, dtype=float32), 'eval/episode_goal_distance': (Array(0.5048294, dtype=float32), Array(0.26061997, dtype=float32)), 'eval/episode_reward': (Array(-10490.599, dtype=float32), Array(5051.7295, dtype=float32)), 'eval/avg_episode_length': (Array(922.2969, dtype=float32), Array(266.91925, dtype=float32)), 'eval/epoch_eval_time': 4.436645984649658, 'eval/sps': 28850.622845020072}
I0727 20:13:21.778380 139998383736640 train.py:379] starting iteration 172 4812.570467472076
I0727 20:13:49.432173 139998383736640 train.py:394] {'eval/walltime': 783.2939355373383, 'training/sps': 17654.524310994315, 'training/walltime': 4046.1805539131165, 'training/entropy_loss': Array(-0.0524957, dtype=float32), 'training/policy_loss': Array(-0.00018285, dtype=float32), 'training/total_loss': Array(28.9469, dtype=float32), 'training/v_loss': Array(28.999578, dtype=float32), 'eval/episode_goal_distance': (Array(0.50278115, dtype=float32), Array(0.258611, dtype=float32)), 'eval/episode_reward': (Array(-10355.55, dtype=float32), Array(4961.595, dtype=float32)), 'eval/avg_episode_length': (Array(914.53125, dtype=float32), Array(278.7434, dtype=float32)), 'eval/epoch_eval_time': 4.44663667678833, 'eval/sps': 28785.80133793402}
I0727 20:13:49.435733 139998383736640 train.py:379] starting iteration 173 4840.2278208732605
I0727 20:14:17.054998 139998383736640 train.py:394] {'eval/walltime': 787.7244112491608, 'training/sps': 17668.55745393778, 'training/walltime': 4069.3629784584045, 'training/entropy_loss': Array(-0.05269094, dtype=float32), 'training/policy_loss': Array(-0.00012033, dtype=float32), 'training/total_loss': Array(30.793694, dtype=float32), 'training/v_loss': Array(30.846502, dtype=float32), 'eval/episode_goal_distance': (Array(0.47787637, dtype=float32), Array(0.22047678, dtype=float32)), 'eval/episode_reward': (Array(-10261.357, dtype=float32), Array(4752.586, dtype=float32)), 'eval/avg_episode_length': (Array(922.2344, dtype=float32), Array(267.13388, dtype=float32)), 'eval/epoch_eval_time': 4.43047571182251, 'eval/sps': 28890.802777326644}
I0727 20:14:17.058823 139998383736640 train.py:379] starting iteration 174 4867.850910663605
I0727 20:14:44.678950 139998383736640 train.py:394] {'eval/walltime': 792.1549847126007, 'training/sps': 17667.944380723035, 'training/walltime': 4092.5462074279785, 'training/entropy_loss': Array(-0.05299497, dtype=float32), 'training/policy_loss': Array(-0.00022701, dtype=float32), 'training/total_loss': Array(33.545456, dtype=float32), 'training/v_loss': Array(33.59868, dtype=float32), 'eval/episode_goal_distance': (Array(0.49152744, dtype=float32), Array(0.24848263, dtype=float32)), 'eval/episode_reward': (Array(-9998.539, dtype=float32), Array(5549.3833, dtype=float32)), 'eval/avg_episode_length': (Array(860.2344, dtype=float32), Array(345.51068, dtype=float32)), 'eval/epoch_eval_time': 4.430573463439941, 'eval/sps': 28890.165360359362}
I0727 20:14:44.682430 139998383736640 train.py:379] starting iteration 175 4895.474517822266
I0727 20:15:12.331158 139998383736640 train.py:394] {'eval/walltime': 796.5822865962982, 'training/sps': 17643.677714801397, 'training/walltime': 4115.761322021484, 'training/entropy_loss': Array(-0.05272484, dtype=float32), 'training/policy_loss': Array(-0.00027835, dtype=float32), 'training/total_loss': Array(48.509506, dtype=float32), 'training/v_loss': Array(48.562508, dtype=float32), 'eval/episode_goal_distance': (Array(0.49281263, dtype=float32), Array(0.22941417, dtype=float32)), 'eval/episode_reward': (Array(-10333.555, dtype=float32), Array(4826.6836, dtype=float32)), 'eval/avg_episode_length': (Array(914.5469, dtype=float32), Array(278.69238, dtype=float32)), 'eval/epoch_eval_time': 4.42730188369751, 'eval/sps': 28911.51391128978}
I0727 20:15:12.442035 139998383736640 train.py:379] starting iteration 176 4923.234102487564
I0727 20:15:40.084371 139998383736640 train.py:394] {'eval/walltime': 801.017781496048, 'training/sps': 17655.17872777704, 'training/walltime': 4138.961313724518, 'training/entropy_loss': Array(-0.05237599, dtype=float32), 'training/policy_loss': Array(-0.00038017, dtype=float32), 'training/total_loss': Array(30.172367, dtype=float32), 'training/v_loss': Array(30.225122, dtype=float32), 'eval/episode_goal_distance': (Array(0.49542183, dtype=float32), Array(0.20228766, dtype=float32)), 'eval/episode_reward': (Array(-10077.087, dtype=float32), Array(4817.3154, dtype=float32)), 'eval/avg_episode_length': (Array(891.2656, dtype=float32), Array(310.2819, dtype=float32)), 'eval/epoch_eval_time': 4.435494899749756, 'eval/sps': 28858.11006280755}
I0727 20:15:40.088300 139998383736640 train.py:379] starting iteration 177 4950.880387067795
I0727 20:16:07.755236 139998383736640 train.py:394] {'eval/walltime': 805.4449827671051, 'training/sps': 17629.743570040817, 'training/walltime': 4162.194777011871, 'training/entropy_loss': Array(-0.05237132, dtype=float32), 'training/policy_loss': Array(-0.00038018, dtype=float32), 'training/total_loss': Array(28.631086, dtype=float32), 'training/v_loss': Array(28.68384, dtype=float32), 'eval/episode_goal_distance': (Array(0.48568806, dtype=float32), Array(0.23621549, dtype=float32)), 'eval/episode_reward': (Array(-9997.249, dtype=float32), Array(5119.2637, dtype=float32)), 'eval/avg_episode_length': (Array(899.0547, dtype=float32), Array(300.23672, dtype=float32)), 'eval/epoch_eval_time': 4.427201271057129, 'eval/sps': 28912.1709547748}
I0727 20:16:07.758731 139998383736640 train.py:379] starting iteration 178 4978.550818443298
I0727 20:16:35.409141 139998383736640 train.py:394] {'eval/walltime': 809.869199514389, 'training/sps': 17640.40910864018, 'training/walltime': 4185.414193153381, 'training/entropy_loss': Array(-0.05247836, dtype=float32), 'training/policy_loss': Array(-0.00013482, dtype=float32), 'training/total_loss': Array(30.264072, dtype=float32), 'training/v_loss': Array(30.316689, dtype=float32), 'eval/episode_goal_distance': (Array(0.5188672, dtype=float32), Array(0.2854887, dtype=float32)), 'eval/episode_reward': (Array(-10787.708, dtype=float32), Array(5657.2246, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.3067, dtype=float32)), 'eval/epoch_eval_time': 4.4242167472839355, 'eval/sps': 28931.674759962945}
I0727 20:16:35.412847 139998383736640 train.py:379] starting iteration 179 5006.204934597015
I0727 20:17:03.057807 139998383736640 train.py:394] {'eval/walltime': 814.2865962982178, 'training/sps': 17639.169702969622, 'training/walltime': 4208.635240793228, 'training/entropy_loss': Array(-0.05268358, dtype=float32), 'training/policy_loss': Array(-0.00014966, dtype=float32), 'training/total_loss': Array(33.49176, dtype=float32), 'training/v_loss': Array(33.544594, dtype=float32), 'eval/episode_goal_distance': (Array(0.49988914, dtype=float32), Array(0.24503045, dtype=float32)), 'eval/episode_reward': (Array(-10091.051, dtype=float32), Array(5076.99, dtype=float32)), 'eval/avg_episode_length': (Array(891.2344, dtype=float32), Array(310.37073, dtype=float32)), 'eval/epoch_eval_time': 4.417396783828735, 'eval/sps': 28976.342009525633}
I0727 20:17:03.061407 139998383736640 train.py:379] starting iteration 180 5033.853494167328
I0727 20:17:30.734486 139998383736640 train.py:394] {'eval/walltime': 818.7175316810608, 'training/sps': 17627.94004213787, 'training/walltime': 4231.871081113815, 'training/entropy_loss': Array(-0.05256937, dtype=float32), 'training/policy_loss': Array(-0.00041397, dtype=float32), 'training/total_loss': Array(47.418213, dtype=float32), 'training/v_loss': Array(47.47119, dtype=float32), 'eval/episode_goal_distance': (Array(0.50634897, dtype=float32), Array(0.22016896, dtype=float32)), 'eval/episode_reward': (Array(-10412.572, dtype=float32), Array(4426.4985, dtype=float32)), 'eval/avg_episode_length': (Array(930.1797, dtype=float32), Array(253.88359, dtype=float32)), 'eval/epoch_eval_time': 4.430935382843018, 'eval/sps': 28887.80560773411}
I0727 20:17:30.738062 139998383736640 train.py:379] starting iteration 181 5061.530150175095
I0727 20:17:58.287965 139998383736640 train.py:394] {'eval/walltime': 823.1333038806915, 'training/sps': 17710.166652184696, 'training/walltime': 4254.999039649963, 'training/entropy_loss': Array(-0.05267245, dtype=float32), 'training/policy_loss': Array(-0.00024796, dtype=float32), 'training/total_loss': Array(27.505112, dtype=float32), 'training/v_loss': Array(27.558031, dtype=float32), 'eval/episode_goal_distance': (Array(0.43427265, dtype=float32), Array(0.1904453, dtype=float32)), 'eval/episode_reward': (Array(-8695.557, dtype=float32), Array(4878.2354, dtype=float32)), 'eval/avg_episode_length': (Array(844.6953, dtype=float32), Array(360.89615, dtype=float32)), 'eval/epoch_eval_time': 4.415772199630737, 'eval/sps': 28987.00254752811}
I0727 20:17:58.291464 139998383736640 train.py:379] starting iteration 182 5089.083552122116
I0727 20:18:25.888388 139998383736640 train.py:394] {'eval/walltime': 827.5773849487305, 'training/sps': 17695.952527845897, 'training/walltime': 4278.1455755233765, 'training/entropy_loss': Array(-0.05285836, dtype=float32), 'training/policy_loss': Array(-0.00016236, dtype=float32), 'training/total_loss': Array(28.394096, dtype=float32), 'training/v_loss': Array(28.447117, dtype=float32), 'eval/episode_goal_distance': (Array(0.48361298, dtype=float32), Array(0.22739236, dtype=float32)), 'eval/episode_reward': (Array(-9368., dtype=float32), Array(4972.085, dtype=float32)), 'eval/avg_episode_length': (Array(875.6953, dtype=float32), Array(328.87985, dtype=float32)), 'eval/epoch_eval_time': 4.44408106803894, 'eval/sps': 28802.354871641244}
I0727 20:18:25.892056 139998383736640 train.py:379] starting iteration 183 5116.684143543243
I0727 20:18:53.529650 139998383736640 train.py:394] {'eval/walltime': 832.014880657196, 'training/sps': 17659.937090466614, 'training/walltime': 4301.339316129684, 'training/entropy_loss': Array(-0.05296296, dtype=float32), 'training/policy_loss': Array(-0.00016438, dtype=float32), 'training/total_loss': Array(30.522425, dtype=float32), 'training/v_loss': Array(30.57555, dtype=float32), 'eval/episode_goal_distance': (Array(0.46319246, dtype=float32), Array(0.2143487, dtype=float32)), 'eval/episode_reward': (Array(-9652.837, dtype=float32), Array(4684.284, dtype=float32)), 'eval/avg_episode_length': (Array(899.10156, dtype=float32), Array(300.09784, dtype=float32)), 'eval/epoch_eval_time': 4.437495708465576, 'eval/sps': 28845.098318812932}
I0727 20:18:53.533267 139998383736640 train.py:379] starting iteration 184 5144.325354099274
I0727 20:19:21.153818 139998383736640 train.py:394] {'eval/walltime': 836.4481554031372, 'training/sps': 17669.774822923697, 'training/walltime': 4324.520143508911, 'training/entropy_loss': Array(-0.05312001, dtype=float32), 'training/policy_loss': Array(-8.841863e-05, dtype=float32), 'training/total_loss': Array(31.956253, dtype=float32), 'training/v_loss': Array(32.00946, dtype=float32), 'eval/episode_goal_distance': (Array(0.46930516, dtype=float32), Array(0.20923883, dtype=float32)), 'eval/episode_reward': (Array(-9626.043, dtype=float32), Array(4698.679, dtype=float32)), 'eval/avg_episode_length': (Array(883.5469, dtype=float32), Array(319.62854, dtype=float32)), 'eval/epoch_eval_time': 4.433274745941162, 'eval/sps': 28872.562007845114}
I0727 20:19:21.157658 139998383736640 train.py:379] starting iteration 185 5171.94974565506
I0727 20:19:48.826553 139998383736640 train.py:394] {'eval/walltime': 840.8899948596954, 'training/sps': 17639.376349042668, 'training/walltime': 4347.740919113159, 'training/entropy_loss': Array(-0.05285661, dtype=float32), 'training/policy_loss': Array(-0.00044981, dtype=float32), 'training/total_loss': Array(46.17287, dtype=float32), 'training/v_loss': Array(46.226173, dtype=float32), 'eval/episode_goal_distance': (Array(0.5114574, dtype=float32), Array(0.2400969, dtype=float32)), 'eval/episode_reward': (Array(-9748.621, dtype=float32), Array(5897.01, dtype=float32)), 'eval/avg_episode_length': (Array(829.125, dtype=float32), Array(375.07776, dtype=float32)), 'eval/epoch_eval_time': 4.4418394565582275, 'eval/sps': 28816.890221237572}
I0727 20:19:48.830062 139998383736640 train.py:379] starting iteration 186 5199.622149467468
I0727 20:20:16.499138 139998383736640 train.py:394] {'eval/walltime': 845.3319494724274, 'training/sps': 17639.355340105994, 'training/walltime': 4370.961722373962, 'training/entropy_loss': Array(-0.05293454, dtype=float32), 'training/policy_loss': Array(-8.05857e-05, dtype=float32), 'training/total_loss': Array(27.544422, dtype=float32), 'training/v_loss': Array(27.597437, dtype=float32), 'eval/episode_goal_distance': (Array(0.46269828, dtype=float32), Array(0.24917106, dtype=float32)), 'eval/episode_reward': (Array(-9021.559, dtype=float32), Array(5540.525, dtype=float32)), 'eval/avg_episode_length': (Array(844.6094, dtype=float32), Array(361.0958, dtype=float32)), 'eval/epoch_eval_time': 4.441954612731934, 'eval/sps': 28816.14315308733}
I0727 20:20:16.502958 139998383736640 train.py:379] starting iteration 187 5227.295045852661
I0727 20:20:44.132314 139998383736640 train.py:394] {'eval/walltime': 849.7754564285278, 'training/sps': 17670.66519493659, 'training/walltime': 4394.14138174057, 'training/entropy_loss': Array(-0.05277864, dtype=float32), 'training/policy_loss': Array(-0.00031219, dtype=float32), 'training/total_loss': Array(27.618023, dtype=float32), 'training/v_loss': Array(27.671116, dtype=float32), 'eval/episode_goal_distance': (Array(0.48640683, dtype=float32), Array(0.21152036, dtype=float32)), 'eval/episode_reward': (Array(-10061.524, dtype=float32), Array(5183.3013, dtype=float32)), 'eval/avg_episode_length': (Array(867.91406, dtype=float32), Array(337.51614, dtype=float32)), 'eval/epoch_eval_time': 4.443506956100464, 'eval/sps': 28806.076206152793}
I0727 20:20:44.139723 139998383736640 train.py:379] starting iteration 188 5254.931795835495
I0727 20:21:11.778524 139998383736640 train.py:394] {'eval/walltime': 854.2093825340271, 'training/sps': 17656.691855462042, 'training/walltime': 4417.339385271072, 'training/entropy_loss': Array(-0.05276812, dtype=float32), 'training/policy_loss': Array(-0.00022516, dtype=float32), 'training/total_loss': Array(29.021152, dtype=float32), 'training/v_loss': Array(29.074146, dtype=float32), 'eval/episode_goal_distance': (Array(0.48622087, dtype=float32), Array(0.24907495, dtype=float32)), 'eval/episode_reward': (Array(-10130.061, dtype=float32), Array(4830.765, dtype=float32)), 'eval/avg_episode_length': (Array(914.5703, dtype=float32), Array(278.61636, dtype=float32)), 'eval/epoch_eval_time': 4.433926105499268, 'eval/sps': 28868.320525514708}
I0727 20:21:11.782086 139998383736640 train.py:379] starting iteration 189 5282.574173927307
I0727 20:21:39.436248 139998383736640 train.py:394] {'eval/walltime': 858.6537380218506, 'training/sps': 17652.414977158598, 'training/walltime': 4440.543009281158, 'training/entropy_loss': Array(-0.05271441, dtype=float32), 'training/policy_loss': Array(-0.0001508, dtype=float32), 'training/total_loss': Array(31.948635, dtype=float32), 'training/v_loss': Array(32.001503, dtype=float32), 'eval/episode_goal_distance': (Array(0.49734747, dtype=float32), Array(0.23745103, dtype=float32)), 'eval/episode_reward': (Array(-10073.835, dtype=float32), Array(4792.559, dtype=float32)), 'eval/avg_episode_length': (Array(914.5781, dtype=float32), Array(278.59042, dtype=float32)), 'eval/epoch_eval_time': 4.444355487823486, 'eval/sps': 28800.576450441604}
I0727 20:21:39.439903 139998383736640 train.py:379] starting iteration 190 5310.231991052628
I0727 20:22:07.073479 139998383736640 train.py:394] {'eval/walltime': 863.0939538478851, 'training/sps': 17664.95285226228, 'training/walltime': 4463.7301642894745, 'training/entropy_loss': Array(-0.05264611, dtype=float32), 'training/policy_loss': Array(-0.00030296, dtype=float32), 'training/total_loss': Array(46.495445, dtype=float32), 'training/v_loss': Array(46.548393, dtype=float32), 'eval/episode_goal_distance': (Array(0.48754194, dtype=float32), Array(0.2368374, dtype=float32)), 'eval/episode_reward': (Array(-10495.792, dtype=float32), Array(4788.174, dtype=float32)), 'eval/avg_episode_length': (Array(922.27344, dtype=float32), Array(266.99973, dtype=float32)), 'eval/epoch_eval_time': 4.440215826034546, 'eval/sps': 28827.427542934067}
I0727 20:22:07.076980 139998383736640 train.py:379] starting iteration 191 5337.869067192078
I0727 20:22:34.721907 139998383736640 train.py:394] {'eval/walltime': 867.5398888587952, 'training/sps': 17660.733153809277, 'training/walltime': 4486.922859430313, 'training/entropy_loss': Array(-0.05268104, dtype=float32), 'training/policy_loss': Array(-0.0003947, dtype=float32), 'training/total_loss': Array(28.667892, dtype=float32), 'training/v_loss': Array(28.720966, dtype=float32), 'eval/episode_goal_distance': (Array(0.4798746, dtype=float32), Array(0.22563346, dtype=float32)), 'eval/episode_reward': (Array(-9767.68, dtype=float32), Array(4845.387, dtype=float32)), 'eval/avg_episode_length': (Array(891.34375, dtype=float32), Array(310.05878, dtype=float32)), 'eval/epoch_eval_time': 4.445935010910034, 'eval/sps': 28790.344367584403}
I0727 20:22:34.725486 139998383736640 train.py:379] starting iteration 192 5365.517573356628
I0727 20:23:02.365518 139998383736640 train.py:394] {'eval/walltime': 871.9855735301971, 'training/sps': 17664.34239028833, 'training/walltime': 4510.1108157634735, 'training/entropy_loss': Array(-0.05249112, dtype=float32), 'training/policy_loss': Array(-0.00012119, dtype=float32), 'training/total_loss': Array(27.60052, dtype=float32), 'training/v_loss': Array(27.653131, dtype=float32), 'eval/episode_goal_distance': (Array(0.48777694, dtype=float32), Array(0.2276039, dtype=float32)), 'eval/episode_reward': (Array(-10078.611, dtype=float32), Array(5455.5225, dtype=float32)), 'eval/avg_episode_length': (Array(875.71875, dtype=float32), Array(328.81766, dtype=float32)), 'eval/epoch_eval_time': 4.4456846714019775, 'eval/sps': 28791.965571331068}
I0727 20:23:02.369132 139998383736640 train.py:379] starting iteration 193 5393.161219596863
I0727 20:23:30.003785 139998383736640 train.py:394] {'eval/walltime': 876.4364678859711, 'training/sps': 17672.242785876857, 'training/walltime': 4533.288405895233, 'training/entropy_loss': Array(-0.05287561, dtype=float32), 'training/policy_loss': Array(-0.00014247, dtype=float32), 'training/total_loss': Array(29.067513, dtype=float32), 'training/v_loss': Array(29.120531, dtype=float32), 'eval/episode_goal_distance': (Array(0.48090568, dtype=float32), Array(0.20366722, dtype=float32)), 'eval/episode_reward': (Array(-9872.377, dtype=float32), Array(4638.137, dtype=float32)), 'eval/avg_episode_length': (Array(898.96094, dtype=float32), Array(300.51575, dtype=float32)), 'eval/epoch_eval_time': 4.450894355773926, 'eval/sps': 28758.265141465763}
I0727 20:23:30.007320 139998383736640 train.py:379] starting iteration 194 5420.799407243729
I0727 20:23:57.651071 139998383736640 train.py:394] {'eval/walltime': 880.8715748786926, 'training/sps': 17653.46794845658, 'training/walltime': 4556.4906458854675, 'training/entropy_loss': Array(-0.05298928, dtype=float32), 'training/policy_loss': Array(-8.742937e-05, dtype=float32), 'training/total_loss': Array(32.07451, dtype=float32), 'training/v_loss': Array(32.127583, dtype=float32), 'eval/episode_goal_distance': (Array(0.50239295, dtype=float32), Array(0.24937004, dtype=float32)), 'eval/episode_reward': (Array(-9837.88, dtype=float32), Array(5060.2607, dtype=float32)), 'eval/avg_episode_length': (Array(875.8203, dtype=float32), Array(328.54953, dtype=float32)), 'eval/epoch_eval_time': 4.435106992721558, 'eval/sps': 28860.63407490743}
I0727 20:23:57.654636 139998383736640 train.py:379] starting iteration 195 5448.446722745895
I0727 20:24:25.308478 139998383736640 train.py:394] {'eval/walltime': 885.3152847290039, 'training/sps': 17652.377975800788, 'training/walltime': 4579.694318532944, 'training/entropy_loss': Array(-0.05297672, dtype=float32), 'training/policy_loss': Array(-0.00035167, dtype=float32), 'training/total_loss': Array(46.98606, dtype=float32), 'training/v_loss': Array(47.03939, dtype=float32), 'eval/episode_goal_distance': (Array(0.50432754, dtype=float32), Array(0.22954239, dtype=float32)), 'eval/episode_reward': (Array(-10398.172, dtype=float32), Array(4960.702, dtype=float32)), 'eval/avg_episode_length': (Array(906.78125, dtype=float32), Array(289.8292, dtype=float32)), 'eval/epoch_eval_time': 4.443709850311279, 'eval/sps': 28804.760956891387}
I0727 20:24:25.312027 139998383736640 train.py:379] starting iteration 196 5476.104114294052
I0727 20:24:52.964906 139998383736640 train.py:394] {'eval/walltime': 889.7550797462463, 'training/sps': 17650.007310220215, 'training/walltime': 4602.901107788086, 'training/entropy_loss': Array(-0.05317572, dtype=float32), 'training/policy_loss': Array(-0.00018854, dtype=float32), 'training/total_loss': Array(26.966148, dtype=float32), 'training/v_loss': Array(27.019512, dtype=float32), 'eval/episode_goal_distance': (Array(0.4932702, dtype=float32), Array(0.24821778, dtype=float32)), 'eval/episode_reward': (Array(-9877.075, dtype=float32), Array(5151.4473, dtype=float32)), 'eval/avg_episode_length': (Array(883.53906, dtype=float32), Array(319.64996, dtype=float32)), 'eval/epoch_eval_time': 4.439795017242432, 'eval/sps': 28830.15983911373}
I0727 20:24:52.968518 139998383736640 train.py:379] starting iteration 197 5503.760605335236
