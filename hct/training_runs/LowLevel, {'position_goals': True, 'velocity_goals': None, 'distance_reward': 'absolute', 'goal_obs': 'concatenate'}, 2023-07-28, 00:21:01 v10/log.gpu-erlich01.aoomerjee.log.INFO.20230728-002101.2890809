I0728 00:21:01.913327 140567846262592 low_level_env.py:188] Initialising environment...
I0728 00:21:41.633551 140567846262592 low_level_env.py:293] Environment initialised.
I0728 00:21:41.659174 140567846262592 train.py:118] JAX is running on GPU.
I0728 00:21:41.659282 140567846262592 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 00:21:50.160946 140567846262592 train.py:367] Running initial eval
I0728 00:22:06.012015 140567846262592 train.py:373] {'eval/walltime': 15.720703363418579, 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31777856, dtype=float32), Array(0.1494122, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(729.9234, dtype=float32), Array(2555.6262, dtype=float32)), 'eval/episode_reward': (Array(-24081.613, dtype=float32), Array(10363.702, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3161031, dtype=float32), Array(0.15067475, dtype=float32)), 'eval/avg_episode_length': (Array(930.1328, dtype=float32), Array(254.05441, dtype=float32)), 'eval/epoch_eval_time': 15.720703363418579, 'eval/sps': 8142.12933359271}
I0728 00:22:06.013755 140567846262592 train.py:379] starting iteration 0, 0 steps, 24.35463047027588
I0728 00:22:41.135890 140567846262592 train.py:394] {'eval/walltime': 19.518263339996338, 'training/sps': 13077.90213475541, 'training/walltime': 31.320008039474487, 'training/entropy_loss': Array(-0.04654335, dtype=float32), 'training/policy_loss': Array(0.00934227, dtype=float32), 'training/total_loss': Array(318757.12, dtype=float32), 'training/v_loss': Array(318757.16, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.32481432, dtype=float32), Array(0.13563018, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(652.41534, dtype=float32), Array(2420.3574, dtype=float32)), 'eval/episode_reward': (Array(-25157.43, dtype=float32), Array(10113.555, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.32345483, dtype=float32), Array(0.1368187, dtype=float32)), 'eval/avg_episode_length': (Array(937.9297, dtype=float32), Array(240.39757, dtype=float32)), 'eval/epoch_eval_time': 3.797559976577759, 'eval/sps': 33705.8534399632}
I0728 00:22:41.175101 140567846262592 train.py:379] starting iteration 1, 409600 steps, 59.51596975326538
I0728 00:22:54.047483 140567846262592 train.py:394] {'eval/walltime': 23.328840494155884, 'training/sps': 45222.093324528556, 'training/walltime': 40.377527713775635, 'training/entropy_loss': Array(-0.04661271, dtype=float32), 'training/policy_loss': Array(0.0023894, dtype=float32), 'training/total_loss': Array(243092.78, dtype=float32), 'training/v_loss': Array(243092.84, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3329169, dtype=float32), Array(0.12984148, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(809.43176, dtype=float32), Array(2683.3176, dtype=float32)), 'eval/episode_reward': (Array(-24857.607, dtype=float32), Array(10211.587, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3314476, dtype=float32), Array(0.13122684, dtype=float32)), 'eval/avg_episode_length': (Array(922.2969, dtype=float32), Array(266.91925, dtype=float32)), 'eval/epoch_eval_time': 3.810577154159546, 'eval/sps': 33590.71206845343}
I0728 00:22:54.050383 140567846262592 train.py:379] starting iteration 2, 819200 steps, 72.39125967025757
I0728 00:23:06.956137 140567846262592 train.py:394] {'eval/walltime': 27.156256437301636, 'training/sps': 45136.55839739091, 'training/walltime': 49.45221161842346, 'training/entropy_loss': Array(-0.04658864, dtype=float32), 'training/policy_loss': Array(0.00240868, dtype=float32), 'training/total_loss': Array(236581.84, dtype=float32), 'training/v_loss': Array(236581.9, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.33116466, dtype=float32), Array(0.13336733, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(809.30383, dtype=float32), Array(2682.3274, dtype=float32)), 'eval/episode_reward': (Array(-24928.498, dtype=float32), Array(10035.845, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3297202, dtype=float32), Array(0.13428539, dtype=float32)), 'eval/avg_episode_length': (Array(922.39844, dtype=float32), Array(266.5704, dtype=float32)), 'eval/epoch_eval_time': 3.827415943145752, 'eval/sps': 33442.929094034356}
I0728 00:23:06.958907 140567846262592 train.py:379] starting iteration 3, 1228800 steps, 85.29978370666504
I0728 00:23:19.875629 140567846262592 train.py:394] {'eval/walltime': 30.9932861328125, 'training/sps': 45131.137251584754, 'training/walltime': 58.52798557281494, 'training/entropy_loss': Array(-0.04719186, dtype=float32), 'training/policy_loss': Array(0.00444986, dtype=float32), 'training/total_loss': Array(237282.22, dtype=float32), 'training/v_loss': Array(237282.28, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3140383, dtype=float32), Array(0.13894068, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(807.78845, dtype=float32), Array(2682.9272, dtype=float32)), 'eval/episode_reward': (Array(-24194.41, dtype=float32), Array(10495.945, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.31249064, dtype=float32), Array(0.14003111, dtype=float32)), 'eval/avg_episode_length': (Array(922.3125, dtype=float32), Array(266.8655, dtype=float32)), 'eval/epoch_eval_time': 3.8370296955108643, 'eval/sps': 33359.137186181724}
I0728 00:23:19.878400 140567846262592 train.py:379] starting iteration 4, 1638400 steps, 98.21927690505981
I0728 00:23:32.794648 140567846262592 train.py:394] {'eval/walltime': 34.82313823699951, 'training/sps': 45097.78769516656, 'training/walltime': 67.61047101020813, 'training/entropy_loss': Array(-0.04651346, dtype=float32), 'training/policy_loss': Array(0.00857454, dtype=float32), 'training/total_loss': Array(231022.5, dtype=float32), 'training/v_loss': Array(231022.53, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29718673, dtype=float32), Array(0.1390176, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1275.1007, dtype=float32), Array(3306.3545, dtype=float32)), 'eval/episode_reward': (Array(-22494.201, dtype=float32), Array(10462.854, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2954299, dtype=float32), Array(0.14014405, dtype=float32)), 'eval/avg_episode_length': (Array(875.8203, dtype=float32), Array(328.5492, dtype=float32)), 'eval/epoch_eval_time': 3.8298521041870117, 'eval/sps': 33421.65611566648}
I0728 00:23:32.800297 140567846262592 train.py:379] starting iteration 5, 2048000 steps, 111.14115786552429
I0728 00:23:45.764020 140567846262592 train.py:394] {'eval/walltime': 38.68082547187805, 'training/sps': 45002.00279821336, 'training/walltime': 76.71228814125061, 'training/entropy_loss': Array(-0.0427791, dtype=float32), 'training/policy_loss': Array(0.0121194, dtype=float32), 'training/total_loss': Array(318238.62, dtype=float32), 'training/v_loss': Array(318238.66, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30848688, dtype=float32), Array(0.11532418, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(885.4561, dtype=float32), Array(2802.8723, dtype=float32)), 'eval/episode_reward': (Array(-23395.52, dtype=float32), Array(9363.444, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30697456, dtype=float32), Array(0.11637963, dtype=float32)), 'eval/avg_episode_length': (Array(914.625, dtype=float32), Array(278.4376, dtype=float32)), 'eval/epoch_eval_time': 3.85768723487854, 'eval/sps': 33180.50225604412}
I0728 00:23:45.766704 140567846262592 train.py:379] starting iteration 6, 2457600 steps, 124.10758018493652
I0728 00:23:58.753381 140567846262592 train.py:394] {'eval/walltime': 42.52971816062927, 'training/sps': 44843.52254137643, 'training/walltime': 85.84627175331116, 'training/entropy_loss': Array(-0.04733559, dtype=float32), 'training/policy_loss': Array(0.00748181, dtype=float32), 'training/total_loss': Array(251981.45, dtype=float32), 'training/v_loss': Array(251981.5, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.33903992, dtype=float32), Array(0.12252838, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(809.92175, dtype=float32), Array(2684.1687, dtype=float32)), 'eval/episode_reward': (Array(-25383.201, dtype=float32), Array(9657.4, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.33754897, dtype=float32), Array(0.12376398, dtype=float32)), 'eval/avg_episode_length': (Array(922.40625, dtype=float32), Array(266.54382, dtype=float32)), 'eval/epoch_eval_time': 3.8488926887512207, 'eval/sps': 33256.31820655665}
I0728 00:23:58.756096 140567846262592 train.py:379] starting iteration 7, 2867200 steps, 137.09697246551514
I0728 00:24:11.800552 140567846262592 train.py:394] {'eval/walltime': 46.38240885734558, 'training/sps': 44578.38319615689, 'training/walltime': 95.03458166122437, 'training/entropy_loss': Array(-0.04728142, dtype=float32), 'training/policy_loss': Array(0.03282148, dtype=float32), 'training/total_loss': Array(237239.3, dtype=float32), 'training/v_loss': Array(237239.31, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29520085, dtype=float32), Array(0.12411758, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1118.7034, dtype=float32), Array(3120.2505, dtype=float32)), 'eval/episode_reward': (Array(-22407.645, dtype=float32), Array(9273.804, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29337063, dtype=float32), Array(0.12538852, dtype=float32)), 'eval/avg_episode_length': (Array(891.3203, dtype=float32), Array(310.1257, dtype=float32)), 'eval/epoch_eval_time': 3.8526906967163086, 'eval/sps': 33223.53390815822}
I0728 00:24:11.803172 140567846262592 train.py:379] starting iteration 8, 3276800 steps, 150.14404821395874
I0728 00:24:24.913256 140567846262592 train.py:394] {'eval/walltime': 50.250683546066284, 'training/sps': 44337.307653526645, 'training/walltime': 104.27285122871399, 'training/entropy_loss': Array(-0.04531035, dtype=float32), 'training/policy_loss': Array(0.00478138, dtype=float32), 'training/total_loss': Array(228352.33, dtype=float32), 'training/v_loss': Array(228352.34, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31232607, dtype=float32), Array(0.12519264, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(651.4282, dtype=float32), Array(2420.6404, dtype=float32)), 'eval/episode_reward': (Array(-24166.902, dtype=float32), Array(9858.964, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3104427, dtype=float32), Array(0.12683986, dtype=float32)), 'eval/avg_episode_length': (Array(937.9453, dtype=float32), Array(240.33714, dtype=float32)), 'eval/epoch_eval_time': 3.868274688720703, 'eval/sps': 33089.68734129156}
I0728 00:24:24.915898 140567846262592 train.py:379] starting iteration 9, 3686400 steps, 163.2567744255066
I0728 00:24:38.082658 140567846262592 train.py:394] {'eval/walltime': 54.15077805519104, 'training/sps': 44219.828955472585, 'training/walltime': 113.53566408157349, 'training/entropy_loss': Array(-0.04515241, dtype=float32), 'training/policy_loss': Array(0.00547833, dtype=float32), 'training/total_loss': Array(230221.72, dtype=float32), 'training/v_loss': Array(230221.78, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2907849, dtype=float32), Array(0.11567299, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1040.2168, dtype=float32), Array(3019.8125, dtype=float32)), 'eval/episode_reward': (Array(-22390.293, dtype=float32), Array(9524.681, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28868088, dtype=float32), Array(0.11737987, dtype=float32)), 'eval/avg_episode_length': (Array(899.09375, dtype=float32), Array(300.12094, dtype=float32)), 'eval/epoch_eval_time': 3.900094509124756, 'eval/sps': 32819.71749672427}
I0728 00:24:38.085184 140567846262592 train.py:379] starting iteration 10, 4096000 steps, 176.42606163024902
I0728 00:24:51.324231 140567846262592 train.py:394] {'eval/walltime': 58.112393856048584, 'training/sps': 44177.285733582656, 'training/walltime': 122.80739712715149, 'training/entropy_loss': Array(-0.04178076, dtype=float32), 'training/policy_loss': Array(0.02483637, dtype=float32), 'training/total_loss': Array(302970.5, dtype=float32), 'training/v_loss': Array(302970.53, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29683173, dtype=float32), Array(0.11918464, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(962.5707, dtype=float32), Array(2914.4636, dtype=float32)), 'eval/episode_reward': (Array(-22209.22, dtype=float32), Array(9278.986, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2948967, dtype=float32), Array(0.12058217, dtype=float32)), 'eval/avg_episode_length': (Array(906.875, dtype=float32), Array(289.53787, dtype=float32)), 'eval/epoch_eval_time': 3.961615800857544, 'eval/sps': 32310.04883721756}
I0728 00:24:51.326821 140567846262592 train.py:379] starting iteration 11, 4505600 steps, 189.66769742965698
I0728 00:25:04.614849 140567846262592 train.py:394] {'eval/walltime': 62.11315894126892, 'training/sps': 44122.00001566626, 'training/walltime': 132.09074783325195, 'training/entropy_loss': Array(-0.04238861, dtype=float32), 'training/policy_loss': Array(0.01478343, dtype=float32), 'training/total_loss': Array(262967.38, dtype=float32), 'training/v_loss': Array(262967.4, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30414554, dtype=float32), Array(0.11723838, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(885.0968, dtype=float32), Array(2801.3135, dtype=float32)), 'eval/episode_reward': (Array(-23796.482, dtype=float32), Array(9698.846, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3022846, dtype=float32), Array(0.11845674, dtype=float32)), 'eval/avg_episode_length': (Array(914.5703, dtype=float32), Array(278.61606, dtype=float32)), 'eval/epoch_eval_time': 4.000765085220337, 'eval/sps': 31993.88048872421}
I0728 00:25:04.617347 140567846262592 train.py:379] starting iteration 12, 4915200 steps, 202.95822429656982
I0728 00:25:17.936900 140567846262592 train.py:394] {'eval/walltime': 66.15340375900269, 'training/sps': 44158.86182571501, 'training/walltime': 141.36634922027588, 'training/entropy_loss': Array(-0.04183293, dtype=float32), 'training/policy_loss': Array(0.01385557, dtype=float32), 'training/total_loss': Array(249620.94, dtype=float32), 'training/v_loss': Array(249620.95, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30520093, dtype=float32), Array(0.114105, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1744.5491, dtype=float32), Array(3771.3784, dtype=float32)), 'eval/episode_reward': (Array(-23160.957, dtype=float32), Array(9573.122, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30348903, dtype=float32), Array(0.11536513, dtype=float32)), 'eval/avg_episode_length': (Array(829.1094, dtype=float32), Array(375.11145, dtype=float32)), 'eval/epoch_eval_time': 4.040244817733765, 'eval/sps': 31681.248482312803}
I0728 00:25:17.939428 140567846262592 train.py:379] starting iteration 13, 5324800 steps, 216.28030514717102
I0728 00:25:31.263530 140567846262592 train.py:394] {'eval/walltime': 70.20472979545593, 'training/sps': 44189.39767386062, 'training/walltime': 150.63554096221924, 'training/entropy_loss': Array(-0.03537088, dtype=float32), 'training/policy_loss': Array(0.00689895, dtype=float32), 'training/total_loss': Array(220253.42, dtype=float32), 'training/v_loss': Array(220253.44, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30461302, dtype=float32), Array(0.10812826, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1510.1184, dtype=float32), Array(3555.6433, dtype=float32)), 'eval/episode_reward': (Array(-22629.305, dtype=float32), Array(9363.756, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3029819, dtype=float32), Array(0.10919193, dtype=float32)), 'eval/avg_episode_length': (Array(852.39844, dtype=float32), Array(353.53168, dtype=float32)), 'eval/epoch_eval_time': 4.051326036453247, 'eval/sps': 31594.59368322235}
I0728 00:25:31.266025 140567846262592 train.py:379] starting iteration 14, 5734400 steps, 229.60690140724182
I0728 00:25:44.637970 140567846262592 train.py:394] {'eval/walltime': 74.28845930099487, 'training/sps': 44117.2990434023, 'training/walltime': 159.9198808670044, 'training/entropy_loss': Array(-0.02519046, dtype=float32), 'training/policy_loss': Array(0.0118407, dtype=float32), 'training/total_loss': Array(214246.5, dtype=float32), 'training/v_loss': Array(214246.5, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30704796, dtype=float32), Array(0.10656505, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1119.7095, dtype=float32), Array(3119.4614, dtype=float32)), 'eval/episode_reward': (Array(-23856.717, dtype=float32), Array(9578.378, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30539578, dtype=float32), Array(0.10741282, dtype=float32)), 'eval/avg_episode_length': (Array(891.3281, dtype=float32), Array(310.10336, dtype=float32)), 'eval/epoch_eval_time': 4.08372950553894, 'eval/sps': 31343.897735241284}
I0728 00:25:44.640606 140567846262592 train.py:379] starting iteration 15, 6144000 steps, 242.98148250579834
I0728 00:25:58.024816 140567846262592 train.py:394] {'eval/walltime': 78.38456225395203, 'training/sps': 44117.786202963805, 'training/walltime': 169.20411825180054, 'training/entropy_loss': Array(-0.02026401, dtype=float32), 'training/policy_loss': Array(0.01378996, dtype=float32), 'training/total_loss': Array(254926.6, dtype=float32), 'training/v_loss': Array(254926.6, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2946018, dtype=float32), Array(0.09517551, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1040.4558, dtype=float32), Array(3021.6716, dtype=float32)), 'eval/episode_reward': (Array(-22472.613, dtype=float32), Array(8884.851, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29248986, dtype=float32), Array(0.09634698, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.3065, dtype=float32)), 'eval/epoch_eval_time': 4.096102952957153, 'eval/sps': 31249.214551013978}
I0728 00:25:58.027339 140567846262592 train.py:379] starting iteration 16, 6553600 steps, 256.3682162761688
I0728 00:26:11.416123 140567846262592 train.py:394] {'eval/walltime': 82.48007082939148, 'training/sps': 44093.39726120427, 'training/walltime': 178.49349093437195, 'training/entropy_loss': Array(0.0040762, dtype=float32), 'training/policy_loss': Array(0.01563215, dtype=float32), 'training/total_loss': Array(227929.2, dtype=float32), 'training/v_loss': Array(227929.19, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2893569, dtype=float32), Array(0.09149982, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(727.5195, dtype=float32), Array(2555.986, dtype=float32)), 'eval/episode_reward': (Array(-23161.695, dtype=float32), Array(8107.5347, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28697208, dtype=float32), Array(0.09282424, dtype=float32)), 'eval/avg_episode_length': (Array(930.1406, dtype=float32), Array(254.02573, dtype=float32)), 'eval/epoch_eval_time': 4.095508575439453, 'eval/sps': 31253.749721733995}
I0728 00:26:11.418702 140567846262592 train.py:379] starting iteration 17, 6963200 steps, 269.7595784664154
I0728 00:26:24.871219 140567846262592 train.py:394] {'eval/walltime': 86.5706582069397, 'training/sps': 43768.382437688175, 'training/walltime': 187.85184454917908, 'training/entropy_loss': Array(0.03521962, dtype=float32), 'training/policy_loss': Array(0.01241257, dtype=float32), 'training/total_loss': Array(210504.4, dtype=float32), 'training/v_loss': Array(210504.34, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2856509, dtype=float32), Array(0.0966749, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1039.7024, dtype=float32), Array(3020.4187, dtype=float32)), 'eval/episode_reward': (Array(-22548.578, dtype=float32), Array(9108.5205, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28346467, dtype=float32), Array(0.0977642, dtype=float32)), 'eval/avg_episode_length': (Array(899.02344, dtype=float32), Array(300.32996, dtype=float32)), 'eval/epoch_eval_time': 4.090587377548218, 'eval/sps': 31291.349673288136}
I0728 00:26:24.873876 140567846262592 train.py:379] starting iteration 18, 7372800 steps, 283.2147524356842
I0728 00:26:38.392573 140567846262592 train.py:394] {'eval/walltime': 90.66305208206177, 'training/sps': 43469.980369536104, 'training/walltime': 197.2744390964508, 'training/entropy_loss': Array(0.07014982, dtype=float32), 'training/policy_loss': Array(0.01019043, dtype=float32), 'training/total_loss': Array(202113.16, dtype=float32), 'training/v_loss': Array(202113.08, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29088318, dtype=float32), Array(0.1099843, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1196.387, dtype=float32), Array(3216.4104, dtype=float32)), 'eval/episode_reward': (Array(-22664.395, dtype=float32), Array(9759.14, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2883634, dtype=float32), Array(0.11158638, dtype=float32)), 'eval/avg_episode_length': (Array(883.60156, dtype=float32), Array(319.47833, dtype=float32)), 'eval/epoch_eval_time': 4.09239387512207, 'eval/sps': 31277.536792858664}
I0728 00:26:38.395132 140567846262592 train.py:379] starting iteration 19, 7782400 steps, 296.736008644104
I0728 00:26:51.945217 140567846262592 train.py:394] {'eval/walltime': 94.75169920921326, 'training/sps': 43309.79280442397, 'training/walltime': 206.7318844795227, 'training/entropy_loss': Array(0.10383295, dtype=float32), 'training/policy_loss': Array(0.00877093, dtype=float32), 'training/total_loss': Array(194710.1, dtype=float32), 'training/v_loss': Array(194709.97, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2849608, dtype=float32), Array(0.10091249, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(648.9907, dtype=float32), Array(2421.0996, dtype=float32)), 'eval/episode_reward': (Array(-22749.818, dtype=float32), Array(9336.571, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2826255, dtype=float32), Array(0.10227517, dtype=float32)), 'eval/avg_episode_length': (Array(937.83594, dtype=float32), Array(240.76064, dtype=float32)), 'eval/epoch_eval_time': 4.088647127151489, 'eval/sps': 31306.198852424823}
I0728 00:26:51.950396 140567846262592 train.py:379] starting iteration 20, 8192000 steps, 310.2912576198578
I0728 00:27:05.533393 140567846262592 train.py:394] {'eval/walltime': 98.83069777488708, 'training/sps': 43117.002501231844, 'training/walltime': 216.23161721229553, 'training/entropy_loss': Array(0.06823079, dtype=float32), 'training/policy_loss': Array(0.01443049, dtype=float32), 'training/total_loss': Array(205464.5, dtype=float32), 'training/v_loss': Array(205464.4, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28591627, dtype=float32), Array(0.09406016, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(727.2318, dtype=float32), Array(2557.1611, dtype=float32)), 'eval/episode_reward': (Array(-22686.082, dtype=float32), Array(8900.061, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28344414, dtype=float32), Array(0.09537625, dtype=float32)), 'eval/avg_episode_length': (Array(930.0547, dtype=float32), Array(254.33809, dtype=float32)), 'eval/epoch_eval_time': 4.078998565673828, 'eval/sps': 31380.25128941302}
I0728 00:27:05.535951 140567846262592 train.py:379] starting iteration 21, 8601600 steps, 323.87682819366455
I0728 00:27:19.166711 140567846262592 train.py:394] {'eval/walltime': 102.91994118690491, 'training/sps': 42945.502951663395, 'training/walltime': 225.76928639411926, 'training/entropy_loss': Array(0.0114218, dtype=float32), 'training/policy_loss': Array(0.02895465, dtype=float32), 'training/total_loss': Array(164238.06, dtype=float32), 'training/v_loss': Array(164238.03, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.32417876, dtype=float32), Array(0.12661077, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(730.4606, dtype=float32), Array(2556.1965, dtype=float32)), 'eval/episode_reward': (Array(-24900.412, dtype=float32), Array(9822.971, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3227156, dtype=float32), Array(0.12781334, dtype=float32)), 'eval/avg_episode_length': (Array(930.0547, dtype=float32), Array(254.3379, dtype=float32)), 'eval/epoch_eval_time': 4.089243412017822, 'eval/sps': 31301.63384840886}
I0728 00:27:19.169217 140567846262592 train.py:379] starting iteration 22, 9011200 steps, 337.5100944042206
I0728 00:27:32.821516 140567846262592 train.py:394] {'eval/walltime': 107.0023181438446, 'training/sps': 42817.63087945317, 'training/walltime': 235.33543920516968, 'training/entropy_loss': Array(-0.0302479, dtype=float32), 'training/policy_loss': Array(0.00276298, dtype=float32), 'training/total_loss': Array(114057.27, dtype=float32), 'training/v_loss': Array(114057.305, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29793936, dtype=float32), Array(0.13084802, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(884.41797, dtype=float32), Array(2802.1736, dtype=float32)), 'eval/episode_reward': (Array(-22685.898, dtype=float32), Array(10582.747, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29616898, dtype=float32), Array(0.13226256, dtype=float32)), 'eval/avg_episode_length': (Array(914.5703, dtype=float32), Array(278.61606, dtype=float32)), 'eval/epoch_eval_time': 4.082376956939697, 'eval/sps': 31354.282407069433}
I0728 00:27:32.824044 140567846262592 train.py:379] starting iteration 23, 9420800 steps, 351.1649205684662
I0728 00:27:46.555027 140567846262592 train.py:394] {'eval/walltime': 111.08061075210571, 'training/sps': 42450.74300979444, 'training/walltime': 244.98426914215088, 'training/entropy_loss': Array(-0.03278133, dtype=float32), 'training/policy_loss': Array(9.608339e-07, dtype=float32), 'training/total_loss': Array(105314.76, dtype=float32), 'training/v_loss': Array(105314.78, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30712387, dtype=float32), Array(0.13471612, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1666.5248, dtype=float32), Array(3702.3982, dtype=float32)), 'eval/episode_reward': (Array(-22429.758, dtype=float32), Array(10391.832, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30571187, dtype=float32), Array(0.13565622, dtype=float32)), 'eval/avg_episode_length': (Array(836.9219, dtype=float32), Array(368.11078, dtype=float32)), 'eval/epoch_eval_time': 4.078292608261108, 'eval/sps': 31385.683249092886}
I0728 00:27:46.557868 140567846262592 train.py:379] starting iteration 24, 9830400 steps, 364.8987443447113
I0728 00:28:00.264011 140567846262592 train.py:394] {'eval/walltime': 115.15890884399414, 'training/sps': 42559.281986250375, 'training/walltime': 254.60849165916443, 'training/entropy_loss': Array(-0.03490213, dtype=float32), 'training/policy_loss': Array(0.00030973, dtype=float32), 'training/total_loss': Array(99664.19, dtype=float32), 'training/v_loss': Array(99664.22, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3049571, dtype=float32), Array(0.12801829, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1041.335, dtype=float32), Array(3019.9941, dtype=float32)), 'eval/episode_reward': (Array(-23490.363, dtype=float32), Array(10041.536, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30333972, dtype=float32), Array(0.12919639, dtype=float32)), 'eval/avg_episode_length': (Array(899.1953, dtype=float32), Array(299.8189, dtype=float32)), 'eval/epoch_eval_time': 4.078298091888428, 'eval/sps': 31385.64104830564}
I0728 00:28:00.266690 140567846262592 train.py:379] starting iteration 25, 10240000 steps, 378.6075665950775
I0728 00:28:14.019956 140567846262592 train.py:394] {'eval/walltime': 119.24671602249146, 'training/sps': 42393.449703169565, 'training/walltime': 264.270361661911, 'training/entropy_loss': Array(-0.03874594, dtype=float32), 'training/policy_loss': Array(0.0005564, dtype=float32), 'training/total_loss': Array(109956.26, dtype=float32), 'training/v_loss': Array(109956.3, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3042531, dtype=float32), Array(0.1382334, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1353.7563, dtype=float32), Array(3393.2808, dtype=float32)), 'eval/episode_reward': (Array(-22744.477, dtype=float32), Array(11136.31, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30248332, dtype=float32), Array(0.13951302, dtype=float32)), 'eval/avg_episode_length': (Array(868.03125, dtype=float32), Array(337.2169, dtype=float32)), 'eval/epoch_eval_time': 4.0878071784973145, 'eval/sps': 31312.631543216048}
I0728 00:28:14.060550 140567846262592 train.py:379] starting iteration 26, 10649600 steps, 392.4014253616333
I0728 00:28:27.882937 140567846262592 train.py:394] {'eval/walltime': 123.36876177787781, 'training/sps': 42242.94531663185, 'training/walltime': 273.966655254364, 'training/entropy_loss': Array(-0.04095006, dtype=float32), 'training/policy_loss': Array(0.00101749, dtype=float32), 'training/total_loss': Array(89251.53, dtype=float32), 'training/v_loss': Array(89251.56, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3040692, dtype=float32), Array(0.13630196, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(885.01855, dtype=float32), Array(2801.8127, dtype=float32)), 'eval/episode_reward': (Array(-23874.46, dtype=float32), Array(10540.94, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30240375, dtype=float32), Array(0.13755019, dtype=float32)), 'eval/avg_episode_length': (Array(914.5547, dtype=float32), Array(278.6672, dtype=float32)), 'eval/epoch_eval_time': 4.1220457553863525, 'eval/sps': 31052.542255927183}
I0728 00:28:27.888767 140567846262592 train.py:379] starting iteration 27, 11059200 steps, 406.22962760925293
I0728 00:28:41.746987 140567846262592 train.py:394] {'eval/walltime': 127.46649074554443, 'training/sps': 41982.71756297804, 'training/walltime': 283.7230508327484, 'training/entropy_loss': Array(-0.03841714, dtype=float32), 'training/policy_loss': Array(-0.00076861, dtype=float32), 'training/total_loss': Array(78877.234, dtype=float32), 'training/v_loss': Array(78877.28, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31397894, dtype=float32), Array(0.11940172, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1042.1534, dtype=float32), Array(3019.955, dtype=float32)), 'eval/episode_reward': (Array(-24209.36, dtype=float32), Array(9983.404, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3124782, dtype=float32), Array(0.12036031, dtype=float32)), 'eval/avg_episode_length': (Array(899., dtype=float32), Array(300.39963, dtype=float32)), 'eval/epoch_eval_time': 4.097728967666626, 'eval/sps': 31236.814589249705}
I0728 00:28:41.749763 140567846262592 train.py:379] starting iteration 28, 11468800 steps, 420.0906400680542
I0728 00:28:55.546491 140567846262592 train.py:394] {'eval/walltime': 131.55878138542175, 'training/sps': 42224.84548489091, 'training/walltime': 293.4235007762909, 'training/entropy_loss': Array(-0.0393402, dtype=float32), 'training/policy_loss': Array(-0.00138152, dtype=float32), 'training/total_loss': Array(67980.7, dtype=float32), 'training/v_loss': Array(67980.75, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30983967, dtype=float32), Array(0.13270946, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1354.3096, dtype=float32), Array(3392.803, dtype=float32)), 'eval/episode_reward': (Array(-23397.676, dtype=float32), Array(10081.011, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3081318, dtype=float32), Array(0.13401343, dtype=float32)), 'eval/avg_episode_length': (Array(868.02344, dtype=float32), Array(337.23657, dtype=float32)), 'eval/epoch_eval_time': 4.092290639877319, 'eval/sps': 31278.325823856256}
I0728 00:28:55.549037 140567846262592 train.py:379] starting iteration 29, 11878400 steps, 433.8899130821228
I0728 00:29:09.379400 140567846262592 train.py:394] {'eval/walltime': 135.65299034118652, 'training/sps': 42086.56484465812, 'training/walltime': 303.15582275390625, 'training/entropy_loss': Array(-0.04219891, dtype=float32), 'training/policy_loss': Array(0.00062598, dtype=float32), 'training/total_loss': Array(58753.02, dtype=float32), 'training/v_loss': Array(58753.062, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3065372, dtype=float32), Array(0.11643723, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1275.874, dtype=float32), Array(3308.304, dtype=float32)), 'eval/episode_reward': (Array(-22954.707, dtype=float32), Array(9780.514, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30488968, dtype=float32), Array(0.117686, dtype=float32)), 'eval/avg_episode_length': (Array(875.6953, dtype=float32), Array(328.87976, dtype=float32)), 'eval/epoch_eval_time': 4.0942089557647705, 'eval/sps': 31263.670560773924}
I0728 00:29:09.381928 140567846262592 train.py:379] starting iteration 30, 12288000 steps, 447.7228055000305
I0728 00:29:23.209307 140567846262592 train.py:394] {'eval/walltime': 139.73994660377502, 'training/sps': 42067.577901091514, 'training/walltime': 312.892537355423, 'training/entropy_loss': Array(-0.04412627, dtype=float32), 'training/policy_loss': Array(-0.00082208, dtype=float32), 'training/total_loss': Array(72175.34, dtype=float32), 'training/v_loss': Array(72175.39, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29921326, dtype=float32), Array(0.12800011, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1118.9928, dtype=float32), Array(3119.7878, dtype=float32)), 'eval/episode_reward': (Array(-23247.578, dtype=float32), Array(10333.485, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29749075, dtype=float32), Array(0.12924497, dtype=float32)), 'eval/avg_episode_length': (Array(891.33594, dtype=float32), Array(310.08112, dtype=float32)), 'eval/epoch_eval_time': 4.086956262588501, 'eval/sps': 31319.15092209241}
I0728 00:29:23.211890 140567846262592 train.py:379] starting iteration 31, 12697600 steps, 461.55276703834534
I0728 00:29:37.079436 140567846262592 train.py:394] {'eval/walltime': 143.8496608734131, 'training/sps': 41993.39003364117, 'training/walltime': 322.6464533805847, 'training/entropy_loss': Array(-0.04608022, dtype=float32), 'training/policy_loss': Array(0.00250839, dtype=float32), 'training/total_loss': Array(50143.547, dtype=float32), 'training/v_loss': Array(50143.586, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30193925, dtype=float32), Array(0.12080681, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1275.4425, dtype=float32), Array(3307.3274, dtype=float32)), 'eval/episode_reward': (Array(-23011.633, dtype=float32), Array(9482.354, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30023068, dtype=float32), Array(0.12202933, dtype=float32)), 'eval/avg_episode_length': (Array(875.71094, dtype=float32), Array(328.8388, dtype=float32)), 'eval/epoch_eval_time': 4.1097142696380615, 'eval/sps': 31145.717585683356}
I0728 00:29:37.082074 140567846262592 train.py:379] starting iteration 32, 13107200 steps, 475.4229509830475
I0728 00:29:51.024729 140567846262592 train.py:394] {'eval/walltime': 147.96907949447632, 'training/sps': 41715.85460034559, 'training/walltime': 332.4652621746063, 'training/entropy_loss': Array(-0.05067483, dtype=float32), 'training/policy_loss': Array(-0.00320404, dtype=float32), 'training/total_loss': Array(42497.4, dtype=float32), 'training/v_loss': Array(42497.45, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31544128, dtype=float32), Array(0.12898368, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(729.79895, dtype=float32), Array(2556.1294, dtype=float32)), 'eval/episode_reward': (Array(-24513.629, dtype=float32), Array(9959.006, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.31372786, dtype=float32), Array(0.13006218, dtype=float32)), 'eval/avg_episode_length': (Array(930.1328, dtype=float32), Array(254.05428, dtype=float32)), 'eval/epoch_eval_time': 4.119418621063232, 'eval/sps': 31072.345827033932}
I0728 00:29:51.029134 140567846262592 train.py:379] starting iteration 33, 13516800 steps, 489.3699986934662
I0728 00:30:04.945398 140567846262592 train.py:394] {'eval/walltime': 152.08437728881836, 'training/sps': 41810.68102135461, 'training/walltime': 342.2618019580841, 'training/entropy_loss': Array(-0.04842682, dtype=float32), 'training/policy_loss': Array(-0.0019586, dtype=float32), 'training/total_loss': Array(33942.117, dtype=float32), 'training/v_loss': Array(33942.168, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29731333, dtype=float32), Array(0.14446124, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(962.5874, dtype=float32), Array(2913.7988, dtype=float32)), 'eval/episode_reward': (Array(-22979.352, dtype=float32), Array(10647.662, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2954399, dtype=float32), Array(0.14580977, dtype=float32)), 'eval/avg_episode_length': (Array(906.8906, dtype=float32), Array(289.489, dtype=float32)), 'eval/epoch_eval_time': 4.115297794342041, 'eval/sps': 31103.459918740777}
I0728 00:30:04.948216 140567846262592 train.py:379] starting iteration 34, 13926400 steps, 503.28909373283386
I0728 00:30:18.861912 140567846262592 train.py:394] {'eval/walltime': 156.2036590576172, 'training/sps': 41837.86414469933, 'training/walltime': 352.0519766807556, 'training/entropy_loss': Array(-0.05002007, dtype=float32), 'training/policy_loss': Array(-0.00335045, dtype=float32), 'training/total_loss': Array(27807.906, dtype=float32), 'training/v_loss': Array(27807.957, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28874964, dtype=float32), Array(0.12980448, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(805.6061, dtype=float32), Array(2681.9666, dtype=float32)), 'eval/episode_reward': (Array(-22540.902, dtype=float32), Array(9834.093, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28691125, dtype=float32), Array(0.13113843, dtype=float32)), 'eval/avg_episode_length': (Array(922.39844, dtype=float32), Array(266.57098, dtype=float32)), 'eval/epoch_eval_time': 4.119281768798828, 'eval/sps': 31073.3781237122}
I0728 00:30:18.864746 140567846262592 train.py:379] starting iteration 35, 14336000 steps, 517.2056221961975
I0728 00:30:32.771714 140567846262592 train.py:394] {'eval/walltime': 160.30922603607178, 'training/sps': 41807.76493402743, 'training/walltime': 361.8491997718811, 'training/entropy_loss': Array(-0.05337532, dtype=float32), 'training/policy_loss': Array(-0.00040484, dtype=float32), 'training/total_loss': Array(47438.926, dtype=float32), 'training/v_loss': Array(47438.98, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2974861, dtype=float32), Array(0.12127291, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(962.64026, dtype=float32), Array(2915.3716, dtype=float32)), 'eval/episode_reward': (Array(-22016.5, dtype=float32), Array(9510.585, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29570293, dtype=float32), Array(0.12247436, dtype=float32)), 'eval/avg_episode_length': (Array(906.7578, dtype=float32), Array(289.902, dtype=float32)), 'eval/epoch_eval_time': 4.10556697845459, 'eval/sps': 31177.17983209752}
I0728 00:30:32.774566 140567846262592 train.py:379] starting iteration 36, 14745600 steps, 531.1154410839081
I0728 00:30:46.697650 140567846262592 train.py:394] {'eval/walltime': 164.40419125556946, 'training/sps': 41693.26821141483, 'training/walltime': 371.67332768440247, 'training/entropy_loss': Array(-0.05322404, dtype=float32), 'training/policy_loss': Array(-0.00018432, dtype=float32), 'training/total_loss': Array(19994.035, dtype=float32), 'training/v_loss': Array(19994.09, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29413718, dtype=float32), Array(0.1176273, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1274.7856, dtype=float32), Array(3308.021, dtype=float32)), 'eval/episode_reward': (Array(-21875.355, dtype=float32), Array(9435.186, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2924719, dtype=float32), Array(0.11884612, dtype=float32)), 'eval/avg_episode_length': (Array(875.75, dtype=float32), Array(328.73508, dtype=float32)), 'eval/epoch_eval_time': 4.094965219497681, 'eval/sps': 31257.89674367526}
I0728 00:30:46.700493 140567846262592 train.py:379] starting iteration 37, 15155200 steps, 545.0413694381714
I0728 00:31:00.647175 140567846262592 train.py:394] {'eval/walltime': 168.516184091568, 'training/sps': 41667.089122750236, 'training/walltime': 381.5036280155182, 'training/entropy_loss': Array(-0.05036853, dtype=float32), 'training/policy_loss': Array(-0.00271672, dtype=float32), 'training/total_loss': Array(15216.449, dtype=float32), 'training/v_loss': Array(15216.502, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30689728, dtype=float32), Array(0.13028492, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1197.742, dtype=float32), Array(3215.445, dtype=float32)), 'eval/episode_reward': (Array(-23738.848, dtype=float32), Array(10617.904, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30521268, dtype=float32), Array(0.13164309, dtype=float32)), 'eval/avg_episode_length': (Array(883.5625, dtype=float32), Array(319.58566, dtype=float32)), 'eval/epoch_eval_time': 4.111992835998535, 'eval/sps': 31128.45890183005}
I0728 00:31:00.652956 140567846262592 train.py:379] starting iteration 38, 15564800 steps, 558.9938178062439
I0728 00:31:14.590672 140567846262592 train.py:394] {'eval/walltime': 172.60932159423828, 'training/sps': 41625.34693767284, 'training/walltime': 391.343786239624, 'training/entropy_loss': Array(-0.05249273, dtype=float32), 'training/policy_loss': Array(-0.00096413, dtype=float32), 'training/total_loss': Array(10477.357, dtype=float32), 'training/v_loss': Array(10477.41, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3016315, dtype=float32), Array(0.13350458, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(962.9326, dtype=float32), Array(2915.623, dtype=float32)), 'eval/episode_reward': (Array(-22929.527, dtype=float32), Array(10176.761, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29957896, dtype=float32), Array(0.13518028, dtype=float32)), 'eval/avg_episode_length': (Array(906.78125, dtype=float32), Array(289.82962, dtype=float32)), 'eval/epoch_eval_time': 4.093137502670288, 'eval/sps': 31271.85439445783}
I0728 00:31:14.593445 140567846262592 train.py:379] starting iteration 39, 15974400 steps, 572.934321641922
I0728 00:31:28.513504 140567846262592 train.py:394] {'eval/walltime': 176.69686245918274, 'training/sps': 41673.463763858985, 'training/walltime': 401.17258286476135, 'training/entropy_loss': Array(-0.05347035, dtype=float32), 'training/policy_loss': Array(-1.3068151e-05, dtype=float32), 'training/total_loss': Array(7743.307, dtype=float32), 'training/v_loss': Array(7743.36, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31023857, dtype=float32), Array(0.12607476, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1198.0962, dtype=float32), Array(3215.6248, dtype=float32)), 'eval/episode_reward': (Array(-23643.371, dtype=float32), Array(9924.725, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30857432, dtype=float32), Array(0.12741551, dtype=float32)), 'eval/avg_episode_length': (Array(883.5547, dtype=float32), Array(319.60724, dtype=float32)), 'eval/epoch_eval_time': 4.087540864944458, 'eval/sps': 31314.671639800054}
I0728 00:31:28.516343 140567846262592 train.py:379] starting iteration 40, 16384000 steps, 586.8572194576263
I0728 00:31:42.434044 140567846262592 train.py:394] {'eval/walltime': 180.77819848060608, 'training/sps': 41658.411143375204, 'training/walltime': 411.004930973053, 'training/entropy_loss': Array(-0.05390729, dtype=float32), 'training/policy_loss': Array(0.00012163, dtype=float32), 'training/total_loss': Array(37253.723, dtype=float32), 'training/v_loss': Array(37253.773, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29934043, dtype=float32), Array(0.136981, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(806.4778, dtype=float32), Array(2683.4133, dtype=float32)), 'eval/episode_reward': (Array(-23294.902, dtype=float32), Array(10169.354, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29726326, dtype=float32), Array(0.13861282, dtype=float32)), 'eval/avg_episode_length': (Array(922.33594, dtype=float32), Array(266.7851, dtype=float32)), 'eval/epoch_eval_time': 4.08133602142334, 'eval/sps': 31362.27924584382}
I0728 00:31:42.436743 140567846262592 train.py:379] starting iteration 41, 16793600 steps, 600.7776200771332
I0728 00:31:56.397657 140567846262592 train.py:394] {'eval/walltime': 184.86170101165771, 'training/sps': 41485.98918129892, 'training/walltime': 420.87814378738403, 'training/entropy_loss': Array(-0.0540688, dtype=float32), 'training/policy_loss': Array(5.299432e-05, dtype=float32), 'training/total_loss': Array(7187.2285, dtype=float32), 'training/v_loss': Array(7187.282, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.32367325, dtype=float32), Array(0.12266705, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(964.8476, dtype=float32), Array(2914.495, dtype=float32)), 'eval/episode_reward': (Array(-24399.93, dtype=float32), Array(9773.53, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3220413, dtype=float32), Array(0.12401465, dtype=float32)), 'eval/avg_episode_length': (Array(906.8047, dtype=float32), Array(289.75632, dtype=float32)), 'eval/epoch_eval_time': 4.083502531051636, 'eval/sps': 31345.63993205994}
I0728 00:31:56.400556 140567846262592 train.py:379] starting iteration 42, 17203200 steps, 614.7414321899414
I0728 00:32:10.358305 140567846262592 train.py:394] {'eval/walltime': 188.93660354614258, 'training/sps': 41463.77904725164, 'training/walltime': 430.7566452026367, 'training/entropy_loss': Array(-0.05405327, dtype=float32), 'training/policy_loss': Array(0.00032104, dtype=float32), 'training/total_loss': Array(5738.162, dtype=float32), 'training/v_loss': Array(5738.216, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30807787, dtype=float32), Array(0.13231556, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1666.6792, dtype=float32), Array(3703.0188, dtype=float32)), 'eval/episode_reward': (Array(-21930.324, dtype=float32), Array(10720.49, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3064686, dtype=float32), Array(0.13336858, dtype=float32)), 'eval/avg_episode_length': (Array(836.8672, dtype=float32), Array(368.23456, dtype=float32)), 'eval/epoch_eval_time': 4.074902534484863, 'eval/sps': 31411.79424949887}
I0728 00:32:10.361063 140567846262592 train.py:379] starting iteration 43, 17612800 steps, 628.7019398212433
I0728 00:32:24.322115 140567846262592 train.py:394] {'eval/walltime': 193.0154845714569, 'training/sps': 41466.53925062297, 'training/walltime': 440.63448905944824, 'training/entropy_loss': Array(-0.05408662, dtype=float32), 'training/policy_loss': Array(0.00020985, dtype=float32), 'training/total_loss': Array(4320.6787, dtype=float32), 'training/v_loss': Array(4320.7324, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28889117, dtype=float32), Array(0.12423053, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1352.5105, dtype=float32), Array(3393.2314, dtype=float32)), 'eval/episode_reward': (Array(-21999.316, dtype=float32), Array(9902.645, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28707558, dtype=float32), Array(0.12533431, dtype=float32)), 'eval/avg_episode_length': (Array(867.96875, dtype=float32), Array(337.3762, dtype=float32)), 'eval/epoch_eval_time': 4.078881025314331, 'eval/sps': 31381.155568305876}
I0728 00:32:24.324841 140567846262592 train.py:379] starting iteration 44, 18022400 steps, 642.6657178401947
I0728 00:32:38.290254 140567846262592 train.py:394] {'eval/walltime': 197.11502361297607, 'training/sps': 41533.38555104403, 'training/walltime': 450.4964349269867, 'training/entropy_loss': Array(-0.05397744, dtype=float32), 'training/policy_loss': Array(0.00024889, dtype=float32), 'training/total_loss': Array(3730.6816, dtype=float32), 'training/v_loss': Array(3730.7354, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30466145, dtype=float32), Array(0.12337354, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1197.6366, dtype=float32), Array(3214.8857, dtype=float32)), 'eval/episode_reward': (Array(-22747.14, dtype=float32), Array(9690.216, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30320993, dtype=float32), Array(0.12436103, dtype=float32)), 'eval/avg_episode_length': (Array(883.53906, dtype=float32), Array(319.65018, dtype=float32)), 'eval/epoch_eval_time': 4.099539041519165, 'eval/sps': 31223.02256513383}
I0728 00:32:38.292964 140567846262592 train.py:379] starting iteration 45, 18432000 steps, 656.6338407993317
I0728 00:32:52.289886 140567846262592 train.py:394] {'eval/walltime': 201.23106598854065, 'training/sps': 41471.17978245763, 'training/walltime': 460.3731734752655, 'training/entropy_loss': Array(-0.05411718, dtype=float32), 'training/policy_loss': Array(6.9470007e-06, dtype=float32), 'training/total_loss': Array(34191.133, dtype=float32), 'training/v_loss': Array(34191.19, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29357302, dtype=float32), Array(0.1260784, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1509.1414, dtype=float32), Array(3556.2942, dtype=float32)), 'eval/episode_reward': (Array(-21380.52, dtype=float32), Array(9906.003, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29182088, dtype=float32), Array(0.12727557, dtype=float32)), 'eval/avg_episode_length': (Array(852.5469, dtype=float32), Array(353.17603, dtype=float32)), 'eval/epoch_eval_time': 4.116042375564575, 'eval/sps': 31097.833384779704}
I0728 00:32:52.292630 140567846262592 train.py:379] starting iteration 46, 18841600 steps, 670.6335072517395
I0728 00:33:06.252653 140567846262592 train.py:394] {'eval/walltime': 205.31457543373108, 'training/sps': 41488.18124571169, 'training/walltime': 470.2458646297455, 'training/entropy_loss': Array(-0.05401274, dtype=float32), 'training/policy_loss': Array(0.00021386, dtype=float32), 'training/total_loss': Array(5577.75, dtype=float32), 'training/v_loss': Array(5577.804, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29792994, dtype=float32), Array(0.11874317, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(884.49506, dtype=float32), Array(2802.2537, dtype=float32)), 'eval/episode_reward': (Array(-23515.963, dtype=float32), Array(9515.095, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29613578, dtype=float32), Array(0.11990827, dtype=float32)), 'eval/avg_episode_length': (Array(914.5781, dtype=float32), Array(278.59082, dtype=float32)), 'eval/epoch_eval_time': 4.08350944519043, 'eval/sps': 31345.586858078364}
I0728 00:33:06.255411 140567846262592 train.py:379] starting iteration 47, 19251200 steps, 684.5962886810303
I0728 00:33:20.276063 140567846262592 train.py:394] {'eval/walltime': 209.41735291481018, 'training/sps': 41316.87449557496, 'training/walltime': 480.15948963165283, 'training/entropy_loss': Array(-0.05405358, dtype=float32), 'training/policy_loss': Array(-6.823071e-05, dtype=float32), 'training/total_loss': Array(4884.466, dtype=float32), 'training/v_loss': Array(4884.5195, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31971392, dtype=float32), Array(0.14669691, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(417.62222, dtype=float32), Array(1937.7246, dtype=float32)), 'eval/episode_reward': (Array(-24710.488, dtype=float32), Array(10427.666, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3182065, dtype=float32), Array(0.14799747, dtype=float32)), 'eval/avg_episode_length': (Array(961.16406, dtype=float32), Array(192.62024, dtype=float32)), 'eval/epoch_eval_time': 4.102777481079102, 'eval/sps': 31198.377340789582}
I0728 00:33:20.278794 140567846262592 train.py:379] starting iteration 48, 19660800 steps, 698.6196713447571
I0728 00:33:34.265982 140567846262592 train.py:394] {'eval/walltime': 213.5363254547119, 'training/sps': 41524.78525020662, 'training/walltime': 490.02347803115845, 'training/entropy_loss': Array(-0.05414864, dtype=float32), 'training/policy_loss': Array(0.00021171, dtype=float32), 'training/total_loss': Array(4315.6035, dtype=float32), 'training/v_loss': Array(4315.657, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28155574, dtype=float32), Array(0.12249143, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(805.038, dtype=float32), Array(2684.7527, dtype=float32)), 'eval/episode_reward': (Array(-22263.041, dtype=float32), Array(9015.679, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2795014, dtype=float32), Array(0.1240555, dtype=float32)), 'eval/avg_episode_length': (Array(922.3594, dtype=float32), Array(266.7047, dtype=float32)), 'eval/epoch_eval_time': 4.118972539901733, 'eval/sps': 31075.71093519689}
I0728 00:33:34.268756 140567846262592 train.py:379] starting iteration 49, 20070400 steps, 712.6096322536469
I0728 00:33:48.265284 140567846262592 train.py:394] {'eval/walltime': 217.61959505081177, 'training/sps': 41335.96146073161, 'training/walltime': 499.93252539634705, 'training/entropy_loss': Array(-0.05418929, dtype=float32), 'training/policy_loss': Array(-9.539908e-06, dtype=float32), 'training/total_loss': Array(3741.6528, dtype=float32), 'training/v_loss': Array(3741.707, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30046892, dtype=float32), Array(0.11873629, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(650.38257, dtype=float32), Array(2419.795, dtype=float32)), 'eval/episode_reward': (Array(-23013.625, dtype=float32), Array(9138.344, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29865858, dtype=float32), Array(0.12012118, dtype=float32)), 'eval/avg_episode_length': (Array(937.875, dtype=float32), Array(240.6095, dtype=float32)), 'eval/epoch_eval_time': 4.0832695960998535, 'eval/sps': 31347.42808122676}
I0728 00:33:48.268043 140567846262592 train.py:379] starting iteration 50, 20480000 steps, 726.6089203357697
I0728 00:34:02.299349 140567846262592 train.py:394] {'eval/walltime': 221.7440643310547, 'training/sps': 41362.884146566816, 'training/walltime': 509.8351230621338, 'training/entropy_loss': Array(-0.05363527, dtype=float32), 'training/policy_loss': Array(-1.6602102e-05, dtype=float32), 'training/total_loss': Array(32979.242, dtype=float32), 'training/v_loss': Array(32979.297, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30786866, dtype=float32), Array(0.11777971, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1510.3767, dtype=float32), Array(3555.004, dtype=float32)), 'eval/episode_reward': (Array(-22326.543, dtype=float32), Array(9737.312, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30639148, dtype=float32), Array(0.11882976, dtype=float32)), 'eval/avg_episode_length': (Array(852.6172, dtype=float32), Array(353.00803, dtype=float32)), 'eval/epoch_eval_time': 4.12446928024292, 'eval/sps': 31034.295882174967}
I0728 00:34:02.354635 140567846262592 train.py:379] starting iteration 51, 20889600 steps, 740.6955034732819
I0728 00:34:16.431387 140567846262592 train.py:394] {'eval/walltime': 225.8572084903717, 'training/sps': 41127.78114433919, 'training/walltime': 519.7943279743195, 'training/entropy_loss': Array(-0.05344875, dtype=float32), 'training/policy_loss': Array(0.00019784, dtype=float32), 'training/total_loss': Array(5586.366, dtype=float32), 'training/v_loss': Array(5586.42, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2724738, dtype=float32), Array(0.12195534, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1819.8933, dtype=float32), Array(3838.5938, dtype=float32)), 'eval/episode_reward': (Array(-20550.98, dtype=float32), Array(9461.683, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2701824, dtype=float32), Array(0.12369572, dtype=float32)), 'eval/avg_episode_length': (Array(821.375, dtype=float32), Array(381.65744, dtype=float32)), 'eval/epoch_eval_time': 4.113144159317017, 'eval/sps': 31119.745635478594}
I0728 00:34:16.434275 140567846262592 train.py:379] starting iteration 52, 21299200 steps, 754.7751517295837
I0728 00:34:30.478951 140567846262592 train.py:394] {'eval/walltime': 229.9459364414215, 'training/sps': 41157.33778034008, 'training/walltime': 529.7463808059692, 'training/entropy_loss': Array(-0.05340989, dtype=float32), 'training/policy_loss': Array(0.00012541, dtype=float32), 'training/total_loss': Array(4425.1772, dtype=float32), 'training/v_loss': Array(4425.2305, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28471515, dtype=float32), Array(0.1193464, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(649.06586, dtype=float32), Array(2421.5115, dtype=float32)), 'eval/episode_reward': (Array(-22159.43, dtype=float32), Array(9265.069, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28278846, dtype=float32), Array(0.12066545, dtype=float32)), 'eval/avg_episode_length': (Array(937.875, dtype=float32), Array(240.60963, dtype=float32)), 'eval/epoch_eval_time': 4.088727951049805, 'eval/sps': 31305.580007379864}
I0728 00:34:30.481659 140567846262592 train.py:379] starting iteration 53, 21708800 steps, 768.8225364685059
I0728 00:34:44.483478 140567846262592 train.py:394] {'eval/walltime': 234.06344866752625, 'training/sps': 41456.96918193821, 'training/walltime': 539.6265048980713, 'training/entropy_loss': Array(-0.05311671, dtype=float32), 'training/policy_loss': Array(9.027724e-05, dtype=float32), 'training/total_loss': Array(3905.723, dtype=float32), 'training/v_loss': Array(3905.776, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29086822, dtype=float32), Array(0.14010718, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(805.84265, dtype=float32), Array(2681.7415, dtype=float32)), 'eval/episode_reward': (Array(-22820.297, dtype=float32), Array(10696.913, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2887587, dtype=float32), Array(0.14178331, dtype=float32)), 'eval/avg_episode_length': (Array(922.3281, dtype=float32), Array(266.81216, dtype=float32)), 'eval/epoch_eval_time': 4.117512226104736, 'eval/sps': 31086.732223522995}
I0728 00:34:44.486215 140567846262592 train.py:379] starting iteration 54, 22118400 steps, 782.8270914554596
I0728 00:34:58.533435 140567846262592 train.py:394] {'eval/walltime': 238.154212474823, 'training/sps': 41156.490828093585, 'training/walltime': 549.5787625312805, 'training/entropy_loss': Array(-0.05263009, dtype=float32), 'training/policy_loss': Array(0.00012132, dtype=float32), 'training/total_loss': Array(4291.713, dtype=float32), 'training/v_loss': Array(4291.7656, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30493543, dtype=float32), Array(0.13036668, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(885.14044, dtype=float32), Array(2801.6604, dtype=float32)), 'eval/episode_reward': (Array(-23898.754, dtype=float32), Array(10525.24, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30307093, dtype=float32), Array(0.13188218, dtype=float32)), 'eval/avg_episode_length': (Array(914.53906, dtype=float32), Array(278.71802, dtype=float32)), 'eval/epoch_eval_time': 4.090763807296753, 'eval/sps': 31290.00011481587}
I0728 00:34:58.536195 140567846262592 train.py:379] starting iteration 55, 22528000 steps, 796.8770723342896
I0728 00:35:12.570590 140567846262592 train.py:394] {'eval/walltime': 242.2782461643219, 'training/sps': 41348.82428368032, 'training/walltime': 559.4847273826599, 'training/entropy_loss': Array(-0.05255482, dtype=float32), 'training/policy_loss': Array(8.094317e-05, dtype=float32), 'training/total_loss': Array(34046.656, dtype=float32), 'training/v_loss': Array(34046.71, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31078506, dtype=float32), Array(0.11656053, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(573.1367, dtype=float32), Array(2272.8083, dtype=float32)), 'eval/episode_reward': (Array(-24310.758, dtype=float32), Array(9269.109, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30905682, dtype=float32), Array(0.11789264, dtype=float32)), 'eval/avg_episode_length': (Array(945.6094, dtype=float32), Array(226.1352, dtype=float32)), 'eval/epoch_eval_time': 4.124033689498901, 'eval/sps': 31037.573802059043}
I0728 00:35:12.576343 140567846262592 train.py:379] starting iteration 56, 22937600 steps, 810.9172043800354
I0728 00:35:26.617476 140567846262592 train.py:394] {'eval/walltime': 246.37361693382263, 'training/sps': 41202.60227832459, 'training/walltime': 569.4258470535278, 'training/entropy_loss': Array(-0.05245996, dtype=float32), 'training/policy_loss': Array(0.00026437, dtype=float32), 'training/total_loss': Array(5024.6914, dtype=float32), 'training/v_loss': Array(5024.7437, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29359, dtype=float32), Array(0.12353448, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(805.9986, dtype=float32), Array(2683.8943, dtype=float32)), 'eval/episode_reward': (Array(-22770.06, dtype=float32), Array(10227.591, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29155803, dtype=float32), Array(0.12505779, dtype=float32)), 'eval/avg_episode_length': (Array(922.3203, dtype=float32), Array(266.83908, dtype=float32)), 'eval/epoch_eval_time': 4.095370769500732, 'eval/sps': 31254.801385322313}
I0728 00:35:26.620325 140567846262592 train.py:379] starting iteration 57, 23347200 steps, 824.96120262146
I0728 00:35:40.656503 140567846262592 train.py:394] {'eval/walltime': 250.4492151737213, 'training/sps': 41137.53571545903, 'training/walltime': 579.3826904296875, 'training/entropy_loss': Array(-0.0525404, dtype=float32), 'training/policy_loss': Array(0.00019966, dtype=float32), 'training/total_loss': Array(4182.0293, dtype=float32), 'training/v_loss': Array(4182.0815, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3142196, dtype=float32), Array(0.12895836, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1120.313, dtype=float32), Array(3119.5344, dtype=float32)), 'eval/episode_reward': (Array(-24296.98, dtype=float32), Array(10105.635, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3126806, dtype=float32), Array(0.1300508, dtype=float32)), 'eval/avg_episode_length': (Array(891.25, dtype=float32), Array(310.32614, dtype=float32)), 'eval/epoch_eval_time': 4.075598239898682, 'eval/sps': 31406.432250098835}
I0728 00:35:40.659265 140567846262592 train.py:379] starting iteration 58, 23756800 steps, 839.0001425743103
I0728 00:35:54.696947 140567846262592 train.py:394] {'eval/walltime': 254.56184315681458, 'training/sps': 41285.930469638006, 'training/walltime': 589.3037457466125, 'training/entropy_loss': Array(-0.05268935, dtype=float32), 'training/policy_loss': Array(0.00090632, dtype=float32), 'training/total_loss': Array(3808.4487, dtype=float32), 'training/v_loss': Array(3808.5, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31493303, dtype=float32), Array(0.13917056, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(495.39062, dtype=float32), Array(2113.8506, dtype=float32)), 'eval/episode_reward': (Array(-24706.035, dtype=float32), Array(10155.238, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.31304142, dtype=float32), Array(0.14085193, dtype=float32)), 'eval/avg_episode_length': (Array(953.3672, dtype=float32), Array(210.27928, dtype=float32)), 'eval/epoch_eval_time': 4.112627983093262, 'eval/sps': 31123.65147691438}
I0728 00:35:54.699719 140567846262592 train.py:379] starting iteration 59, 24166400 steps, 853.0405964851379
I0728 00:36:08.675588 140567846262592 train.py:394] {'eval/walltime': 258.6344974040985, 'training/sps': 41376.977491379046, 'training/walltime': 599.2029705047607, 'training/entropy_loss': Array(-0.05250285, dtype=float32), 'training/policy_loss': Array(0.00197515, dtype=float32), 'training/total_loss': Array(2688.085, dtype=float32), 'training/v_loss': Array(2688.1357, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30627048, dtype=float32), Array(0.12344716, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(650.8992, dtype=float32), Array(2421.5054, dtype=float32)), 'eval/episode_reward': (Array(-23904.852, dtype=float32), Array(9869.349, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30427, dtype=float32), Array(0.1251132, dtype=float32)), 'eval/avg_episode_length': (Array(937.9219, dtype=float32), Array(240.42798, dtype=float32)), 'eval/epoch_eval_time': 4.0726542472839355, 'eval/sps': 31429.134964099532}
I0728 00:36:08.678349 140567846262592 train.py:379] starting iteration 60, 24576000 steps, 867.0192196369171
I0728 00:36:22.780528 140567846262592 train.py:394] {'eval/walltime': 262.7520213127136, 'training/sps': 41037.85375263574, 'training/walltime': 609.183999300003, 'training/entropy_loss': Array(-0.05202468, dtype=float32), 'training/policy_loss': Array(0.00310476, dtype=float32), 'training/total_loss': Array(25383.176, dtype=float32), 'training/v_loss': Array(25383.223, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3018648, dtype=float32), Array(0.11487558, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(416.1903, dtype=float32), Array(1938.9703, dtype=float32)), 'eval/episode_reward': (Array(-23870.883, dtype=float32), Array(8512.94, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3001364, dtype=float32), Array(0.11624208, dtype=float32)), 'eval/avg_episode_length': (Array(961.15625, dtype=float32), Array(192.65909, dtype=float32)), 'eval/epoch_eval_time': 4.117523908615112, 'eval/sps': 31086.644022196222}
I0728 00:36:22.783266 140567846262592 train.py:379] starting iteration 61, 24985600 steps, 881.1241433620453
I0728 00:36:36.796090 140567846262592 train.py:394] {'eval/walltime': 266.8263928890228, 'training/sps': 41230.65659456615, 'training/walltime': 619.1183547973633, 'training/entropy_loss': Array(-0.0519594, dtype=float32), 'training/policy_loss': Array(0.00394221, dtype=float32), 'training/total_loss': Array(4484.2344, dtype=float32), 'training/v_loss': Array(4484.282, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2901724, dtype=float32), Array(0.11138422, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(493.3475, dtype=float32), Array(2113.8438, dtype=float32)), 'eval/episode_reward': (Array(-22920.617, dtype=float32), Array(8726.506, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2883288, dtype=float32), Array(0.11269952, dtype=float32)), 'eval/avg_episode_length': (Array(953.3594, dtype=float32), Array(210.31438, dtype=float32)), 'eval/epoch_eval_time': 4.074371576309204, 'eval/sps': 31415.887727144815}
I0728 00:36:36.798823 140567846262592 train.py:379] starting iteration 62, 25395200 steps, 895.1396999359131
I0728 00:36:50.852852 140567846262592 train.py:394] {'eval/walltime': 270.9454483985901, 'training/sps': 41244.19557109349, 'training/walltime': 629.0494492053986, 'training/entropy_loss': Array(-0.0515442, dtype=float32), 'training/policy_loss': Array(0.00395208, dtype=float32), 'training/total_loss': Array(3473.6143, dtype=float32), 'training/v_loss': Array(3473.6619, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3065676, dtype=float32), Array(0.12867898, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1119.7052, dtype=float32), Array(3120.0735, dtype=float32)), 'eval/episode_reward': (Array(-23454.959, dtype=float32), Array(9961.25, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30473995, dtype=float32), Array(0.12994564, dtype=float32)), 'eval/avg_episode_length': (Array(891.2578, dtype=float32), Array(310.3039, dtype=float32)), 'eval/epoch_eval_time': 4.119055509567261, 'eval/sps': 31075.084980694373}
I0728 00:36:50.855595 140567846262592 train.py:379] starting iteration 63, 25804800 steps, 909.1964709758759
I0728 00:37:04.944381 140567846262592 train.py:394] {'eval/walltime': 275.04696393013, 'training/sps': 41028.711752883384, 'training/walltime': 639.0327019691467, 'training/entropy_loss': Array(-0.05251158, dtype=float32), 'training/policy_loss': Array(0.00248413, dtype=float32), 'training/total_loss': Array(3794.392, dtype=float32), 'training/v_loss': Array(3794.442, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.27856022, dtype=float32), Array(0.12064368, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1273.6207, dtype=float32), Array(3307.854, dtype=float32)), 'eval/episode_reward': (Array(-20939.268, dtype=float32), Array(9477.067, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27646255, dtype=float32), Array(0.12200756, dtype=float32)), 'eval/avg_episode_length': (Array(875.6875, dtype=float32), Array(328.90033, dtype=float32)), 'eval/epoch_eval_time': 4.101515531539917, 'eval/sps': 31207.976421325973}
I0728 00:37:04.947078 140567846262592 train.py:379] starting iteration 64, 26214400 steps, 923.2879557609558
I0728 00:37:18.994452 140567846262592 train.py:394] {'eval/walltime': 279.12098479270935, 'training/sps': 41084.78820307567, 'training/walltime': 649.0023286342621, 'training/entropy_loss': Array(-0.05199187, dtype=float32), 'training/policy_loss': Array(0.0017821, dtype=float32), 'training/total_loss': Array(2700.7412, dtype=float32), 'training/v_loss': Array(2700.7917, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30908588, dtype=float32), Array(0.13161637, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(573.046, dtype=float32), Array(2273.6519, dtype=float32)), 'eval/episode_reward': (Array(-24284.4, dtype=float32), Array(9507.234, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30721864, dtype=float32), Array(0.13305329, dtype=float32)), 'eval/avg_episode_length': (Array(945.6094, dtype=float32), Array(226.13536, dtype=float32)), 'eval/epoch_eval_time': 4.074020862579346, 'eval/sps': 31418.592176516395}
I0728 00:37:18.997068 140567846262592 train.py:379] starting iteration 65, 26624000 steps, 937.3379447460175
I0728 00:37:33.028213 140567846262592 train.py:394] {'eval/walltime': 283.1965892314911, 'training/sps': 41157.846560031896, 'training/walltime': 658.954258441925, 'training/entropy_loss': Array(-0.0518213, dtype=float32), 'training/policy_loss': Array(0.00310066, dtype=float32), 'training/total_loss': Array(24241.89, dtype=float32), 'training/v_loss': Array(24241.941, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29263502, dtype=float32), Array(0.13353729, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(493.48138, dtype=float32), Array(2113.3394, dtype=float32)), 'eval/episode_reward': (Array(-23132.076, dtype=float32), Array(9470.725, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2907067, dtype=float32), Array(0.13469487, dtype=float32)), 'eval/avg_episode_length': (Array(953.35156, dtype=float32), Array(210.34969, dtype=float32)), 'eval/epoch_eval_time': 4.075604438781738, 'eval/sps': 31406.384481773013}
I0728 00:37:33.030796 140567846262592 train.py:379] starting iteration 66, 27033600 steps, 951.3716733455658
I0728 00:37:47.078517 140567846262592 train.py:394] {'eval/walltime': 287.27981400489807, 'training/sps': 41122.53892775826, 'training/walltime': 668.9147329330444, 'training/entropy_loss': Array(-0.05145404, dtype=float32), 'training/policy_loss': Array(0.00212241, dtype=float32), 'training/total_loss': Array(4062.2065, dtype=float32), 'training/v_loss': Array(4062.2559, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.27738088, dtype=float32), Array(0.12001222, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(335.9655, dtype=float32), Array(1740.2728, dtype=float32)), 'eval/episode_reward': (Array(-22668.535, dtype=float32), Array(9195.329, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2751298, dtype=float32), Array(0.12156211, dtype=float32)), 'eval/avg_episode_length': (Array(968.96875, dtype=float32), Array(172.77472, dtype=float32)), 'eval/epoch_eval_time': 4.083224773406982, 'eval/sps': 31347.772190654763}
I0728 00:37:47.081053 140567846262592 train.py:379] starting iteration 67, 27443200 steps, 965.4219303131104
I0728 00:38:01.108219 140567846262592 train.py:394] {'eval/walltime': 291.354718208313, 'training/sps': 41173.22434504537, 'training/walltime': 678.8629457950592, 'training/entropy_loss': Array(-0.05037023, dtype=float32), 'training/policy_loss': Array(0.00210172, dtype=float32), 'training/total_loss': Array(3364.3374, dtype=float32), 'training/v_loss': Array(3364.3857, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30948734, dtype=float32), Array(0.12651798, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(651.23047, dtype=float32), Array(2420.3223, dtype=float32)), 'eval/episode_reward': (Array(-24063.605, dtype=float32), Array(10376.792, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3075654, dtype=float32), Array(0.12835199, dtype=float32)), 'eval/avg_episode_length': (Array(937.9375, dtype=float32), Array(240.36725, dtype=float32)), 'eval/epoch_eval_time': 4.074904203414917, 'eval/sps': 31411.78138438969}
I0728 00:38:01.110755 140567846262592 train.py:379] starting iteration 68, 27852800 steps, 979.4516320228577
I0728 00:38:15.157271 140567846262592 train.py:394] {'eval/walltime': 295.44178223609924, 'training/sps': 41142.648741746205, 'training/walltime': 688.8185517787933, 'training/entropy_loss': Array(-0.04970703, dtype=float32), 'training/policy_loss': Array(0.00167466, dtype=float32), 'training/total_loss': Array(3067.101, dtype=float32), 'training/v_loss': Array(3067.149, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31913364, dtype=float32), Array(0.13073106, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(183.32518, dtype=float32), Array(1239.052, dtype=float32)), 'eval/episode_reward': (Array(-24736.795, dtype=float32), Array(9571.007, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.31716725, dtype=float32), Array(0.13232553, dtype=float32)), 'eval/avg_episode_length': (Array(984.4453, dtype=float32), Array(123.461525, dtype=float32)), 'eval/epoch_eval_time': 4.087064027786255, 'eval/sps': 31318.325117928427}
I0728 00:38:15.159851 140567846262592 train.py:379] starting iteration 69, 28262400 steps, 993.500727891922
I0728 00:38:29.227913 140567846262592 train.py:394] {'eval/walltime': 299.51908802986145, 'training/sps': 41014.26417736995, 'training/walltime': 698.8053212165833, 'training/entropy_loss': Array(-0.0483242, dtype=float32), 'training/policy_loss': Array(0.00181191, dtype=float32), 'training/total_loss': Array(2257.9216, dtype=float32), 'training/v_loss': Array(2257.9683, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29834226, dtype=float32), Array(0.12589368, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(415.90955, dtype=float32), Array(1937.3152, dtype=float32)), 'eval/episode_reward': (Array(-23734.838, dtype=float32), Array(9630.333, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29635906, dtype=float32), Array(0.12724294, dtype=float32)), 'eval/avg_episode_length': (Array(961.2031, dtype=float32), Array(192.42657, dtype=float32)), 'eval/epoch_eval_time': 4.077305793762207, 'eval/sps': 31393.279404214612}
I0728 00:38:29.230456 140567846262592 train.py:379] starting iteration 70, 28672000 steps, 1007.5713334083557
I0728 00:38:43.303448 140567846262592 train.py:394] {'eval/walltime': 303.5956003665924, 'training/sps': 40990.97555566399, 'training/walltime': 708.7977645397186, 'training/entropy_loss': Array(-0.04683302, dtype=float32), 'training/policy_loss': Array(0.00202834, dtype=float32), 'training/total_loss': Array(21789.916, dtype=float32), 'training/v_loss': Array(21789.959, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31282103, dtype=float32), Array(0.1348218, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(182.78839, dtype=float32), Array(1239.0853, dtype=float32)), 'eval/episode_reward': (Array(-25306.803, dtype=float32), Array(10235.084, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.31093693, dtype=float32), Array(0.13611275, dtype=float32)), 'eval/avg_episode_length': (Array(984.4922, dtype=float32), Array(123.089584, dtype=float32)), 'eval/epoch_eval_time': 4.076512336730957, 'eval/sps': 31399.389828081803}
I0728 00:38:43.306041 140567846262592 train.py:379] starting iteration 71, 29081600 steps, 1021.6469178199768
I0728 00:38:57.422482 140567846262592 train.py:394] {'eval/walltime': 307.70942401885986, 'training/sps': 40965.35421542666, 'training/walltime': 718.796457529068, 'training/entropy_loss': Array(-0.04834954, dtype=float32), 'training/policy_loss': Array(0.00234127, dtype=float32), 'training/total_loss': Array(3414.9912, dtype=float32), 'training/v_loss': Array(3415.037, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.27210215, dtype=float32), Array(0.11211623, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1038.625, dtype=float32), Array(3020.4194, dtype=float32)), 'eval/episode_reward': (Array(-21543.062, dtype=float32), Array(8735.101, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.26946533, dtype=float32), Array(0.11411579, dtype=float32)), 'eval/avg_episode_length': (Array(899.08594, dtype=float32), Array(300.14423, dtype=float32)), 'eval/epoch_eval_time': 4.113823652267456, 'eval/sps': 31114.605491037273}
I0728 00:38:57.425028 140567846262592 train.py:379] starting iteration 72, 29491200 steps, 1035.7659051418304
I0728 00:39:11.540836 140567846262592 train.py:394] {'eval/walltime': 311.78917121887207, 'training/sps': 40829.2012480712, 'training/walltime': 728.8284931182861, 'training/entropy_loss': Array(-0.04676899, dtype=float32), 'training/policy_loss': Array(0.00165888, dtype=float32), 'training/total_loss': Array(3510.5908, dtype=float32), 'training/v_loss': Array(3510.636, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.27505156, dtype=float32), Array(0.11932705, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(648.331, dtype=float32), Array(2420.7402, dtype=float32)), 'eval/episode_reward': (Array(-22051.59, dtype=float32), Array(9636.278, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27227104, dtype=float32), Array(0.12186935, dtype=float32)), 'eval/avg_episode_length': (Array(937.8906, dtype=float32), Array(240.54922, dtype=float32)), 'eval/epoch_eval_time': 4.079747200012207, 'eval/sps': 31374.493007708177}
I0728 00:39:11.543487 140567846262592 train.py:379] starting iteration 73, 29900800 steps, 1049.8843636512756
I0728 00:39:25.642827 140567846262592 train.py:394] {'eval/walltime': 315.89073634147644, 'training/sps': 40984.99277973956, 'training/walltime': 738.8223950862885, 'training/entropy_loss': Array(-0.04339518, dtype=float32), 'training/policy_loss': Array(0.00190893, dtype=float32), 'training/total_loss': Array(2316.0715, dtype=float32), 'training/v_loss': Array(2316.1133, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28393537, dtype=float32), Array(0.11526433, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(570.9237, dtype=float32), Array(2273.5977, dtype=float32)), 'eval/episode_reward': (Array(-23088.479, dtype=float32), Array(9129.507, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28141284, dtype=float32), Array(0.11714908, dtype=float32)), 'eval/avg_episode_length': (Array(945.6875, dtype=float32), Array(225.81062, dtype=float32)), 'eval/epoch_eval_time': 4.10156512260437, 'eval/sps': 31207.59909298328}
I0728 00:39:25.645359 140567846262592 train.py:379] starting iteration 74, 30310400 steps, 1063.9862365722656
I0728 00:39:39.721267 140567846262592 train.py:394] {'eval/walltime': 319.97890877723694, 'training/sps': 41033.02448940246, 'training/walltime': 748.80459856987, 'training/entropy_loss': Array(-0.04045334, dtype=float32), 'training/policy_loss': Array(0.00180726, dtype=float32), 'training/total_loss': Array(1750.2908, dtype=float32), 'training/v_loss': Array(1750.3293, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2761453, dtype=float32), Array(0.11856947, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(257.74423, dtype=float32), Array(1512.4751, dtype=float32)), 'eval/episode_reward': (Array(-22640.17, dtype=float32), Array(9486.211, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27357924, dtype=float32), Array(0.12029255, dtype=float32)), 'eval/avg_episode_length': (Array(976.71094, dtype=float32), Array(150.3303, dtype=float32)), 'eval/epoch_eval_time': 4.088172435760498, 'eval/sps': 31309.833920004145}
I0728 00:39:39.723814 140567846262592 train.py:379] starting iteration 75, 30720000 steps, 1078.0646905899048
I0728 00:39:53.796444 140567846262592 train.py:394] {'eval/walltime': 324.0523669719696, 'training/sps': 40979.17987537703, 'training/walltime': 758.7999181747437, 'training/entropy_loss': Array(-0.03888873, dtype=float32), 'training/policy_loss': Array(0.00280658, dtype=float32), 'training/total_loss': Array(21278.025, dtype=float32), 'training/v_loss': Array(21278.06, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.27267724, dtype=float32), Array(0.10097092, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1351.2239, dtype=float32), Array(3393.6838, dtype=float32)), 'eval/episode_reward': (Array(-21318.197, dtype=float32), Array(8923.89, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27002132, dtype=float32), Array(0.10250394, dtype=float32)), 'eval/avg_episode_length': (Array(867.96875, dtype=float32), Array(337.37668, dtype=float32)), 'eval/epoch_eval_time': 4.073458194732666, 'eval/sps': 31422.932034877656}
I0728 00:39:54.101900 140567846262592 train.py:379] starting iteration 76, 31129600 steps, 1092.4427585601807
I0728 00:40:08.144097 140567846262592 train.py:394] {'eval/walltime': 328.134850025177, 'training/sps': 41142.83988914435, 'training/walltime': 768.7554779052734, 'training/entropy_loss': Array(-0.0384363, dtype=float32), 'training/policy_loss': Array(0.0005171, dtype=float32), 'training/total_loss': Array(4223.8057, dtype=float32), 'training/v_loss': Array(4223.8438, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.27904937, dtype=float32), Array(0.10707809, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1586.1217, dtype=float32), Array(3631.645, dtype=float32)), 'eval/episode_reward': (Array(-21284.96, dtype=float32), Array(9316.525, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27614653, dtype=float32), Array(0.10917231, dtype=float32)), 'eval/avg_episode_length': (Array(844.5781, dtype=float32), Array(361.16843, dtype=float32)), 'eval/epoch_eval_time': 4.0824830532073975, 'eval/sps': 31353.46756661659}
I0728 00:40:08.146794 140567846262592 train.py:379] starting iteration 77, 31539200 steps, 1106.4876701831818
I0728 00:40:22.204777 140567846262592 train.py:394] {'eval/walltime': 332.21101355552673, 'training/sps': 41049.9686675007, 'training/walltime': 778.733561038971, 'training/entropy_loss': Array(-0.03345497, dtype=float32), 'training/policy_loss': Array(0.00066598, dtype=float32), 'training/total_loss': Array(2302.6875, dtype=float32), 'training/v_loss': Array(2302.7202, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.27792746, dtype=float32), Array(0.09608481, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(804.7472, dtype=float32), Array(2684.434, dtype=float32)), 'eval/episode_reward': (Array(-22263.793, dtype=float32), Array(8749.447, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.275287, dtype=float32), Array(0.09775809, dtype=float32)), 'eval/avg_episode_length': (Array(922.3281, dtype=float32), Array(266.81223, dtype=float32)), 'eval/epoch_eval_time': 4.0761635303497314, 'eval/sps': 31402.07674372125}
I0728 00:40:22.207312 140567846262592 train.py:379] starting iteration 78, 31948800 steps, 1120.5481889247894
I0728 00:40:36.328216 140567846262592 train.py:394] {'eval/walltime': 336.32161259651184, 'training/sps': 40934.29153662544, 'training/walltime': 788.7398414611816, 'training/entropy_loss': Array(-0.02853815, dtype=float32), 'training/policy_loss': Array(0.00098146, dtype=float32), 'training/total_loss': Array(1179.7246, dtype=float32), 'training/v_loss': Array(1179.7522, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2928786, dtype=float32), Array(0.10071351, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(2056.0537, dtype=float32), Array(4023.5703, dtype=float32)), 'eval/episode_reward': (Array(-21650.543, dtype=float32), Array(9461.1, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29038244, dtype=float32), Array(0.10209875, dtype=float32)), 'eval/avg_episode_length': (Array(798.1094, dtype=float32), Array(399.8807, dtype=float32)), 'eval/epoch_eval_time': 4.110599040985107, 'eval/sps': 31139.01373589693}
I0728 00:40:36.330771 140567846262592 train.py:379] starting iteration 79, 32358400 steps, 1134.6716487407684
I0728 00:40:50.452331 140567846262592 train.py:394] {'eval/walltime': 340.43685007095337, 'training/sps': 40949.30747948848, 'training/walltime': 798.74245262146, 'training/entropy_loss': Array(-0.02379864, dtype=float32), 'training/policy_loss': Array(0.0011583, dtype=float32), 'training/total_loss': Array(1138.1196, dtype=float32), 'training/v_loss': Array(1138.1423, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2939608, dtype=float32), Array(0.09303908, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1431.1387, dtype=float32), Array(3476.294, dtype=float32)), 'eval/episode_reward': (Array(-22669.043, dtype=float32), Array(9018.875, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29143476, dtype=float32), Array(0.09481733, dtype=float32)), 'eval/avg_episode_length': (Array(860.16406, dtype=float32), Array(345.68433, dtype=float32)), 'eval/epoch_eval_time': 4.115237474441528, 'eval/sps': 31103.915823805684}
I0728 00:40:50.454861 140567846262592 train.py:379] starting iteration 80, 32768000 steps, 1148.795738697052
I0728 00:41:04.542134 140567846262592 train.py:394] {'eval/walltime': 344.53815841674805, 'training/sps': 41033.11465398742, 'training/walltime': 808.7246341705322, 'training/entropy_loss': Array(-0.02130578, dtype=float32), 'training/policy_loss': Array(0.0012347, dtype=float32), 'training/total_loss': Array(39013.75, dtype=float32), 'training/v_loss': Array(39013.766, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2877922, dtype=float32), Array(0.08874032, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1664.9692, dtype=float32), Array(3704.3413, dtype=float32)), 'eval/episode_reward': (Array(-21424.715, dtype=float32), Array(8618.601, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28531498, dtype=float32), Array(0.0902285, dtype=float32)), 'eval/avg_episode_length': (Array(837., dtype=float32), Array(367.93488, dtype=float32)), 'eval/epoch_eval_time': 4.101308345794678, 'eval/sps': 31209.552954302064}
I0728 00:41:04.544765 140567846262592 train.py:379] starting iteration 81, 33177600 steps, 1162.8856422901154
I0728 00:41:18.663698 140567846262592 train.py:394] {'eval/walltime': 348.6398537158966, 'training/sps': 40905.79351164887, 'training/walltime': 818.7378857135773, 'training/entropy_loss': Array(-0.01368814, dtype=float32), 'training/policy_loss': Array(0.0026531, dtype=float32), 'training/total_loss': Array(3611.0127, dtype=float32), 'training/v_loss': Array(3611.0234, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.27730587, dtype=float32), Array(0.10218304, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1898.4785, dtype=float32), Array(3903.9238, dtype=float32)), 'eval/episode_reward': (Array(-20607.482, dtype=float32), Array(9557.226, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27475402, dtype=float32), Array(0.10388801, dtype=float32)), 'eval/avg_episode_length': (Array(813.6094, dtype=float32), Array(388.00406, dtype=float32)), 'eval/epoch_eval_time': 4.10169529914856, 'eval/sps': 31206.60864949441}
I0728 00:41:18.669250 140567846262592 train.py:379] starting iteration 82, 33587200 steps, 1177.010112285614
I0728 00:41:32.779413 140567846262592 train.py:394] {'eval/walltime': 352.734637260437, 'training/sps': 40913.82262589247, 'training/walltime': 828.7491722106934, 'training/entropy_loss': Array(-0.00407182, dtype=float32), 'training/policy_loss': Array(0.00533004, dtype=float32), 'training/total_loss': Array(1988.319, dtype=float32), 'training/v_loss': Array(1988.3177, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.27833718, dtype=float32), Array(0.09559626, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1039.1509, dtype=float32), Array(3019.9946, dtype=float32)), 'eval/episode_reward': (Array(-22401.594, dtype=float32), Array(8849.064, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27578738, dtype=float32), Array(0.09679895, dtype=float32)), 'eval/avg_episode_length': (Array(899.0547, dtype=float32), Array(300.23706, dtype=float32)), 'eval/epoch_eval_time': 4.094783544540405, 'eval/sps': 31259.28357572478}
I0728 00:41:32.781940 140567846262592 train.py:379] starting iteration 83, 33996800 steps, 1191.1228170394897
I0728 00:41:46.876050 140567846262592 train.py:394] {'eval/walltime': 356.8120560646057, 'training/sps': 40907.828255914086, 'training/walltime': 838.7619256973267, 'training/entropy_loss': Array(0.00704143, dtype=float32), 'training/policy_loss': Array(0.00835265, dtype=float32), 'training/total_loss': Array(1606.9905, dtype=float32), 'training/v_loss': Array(1606.975, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.27252525, dtype=float32), Array(0.09419054, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1351.1868, dtype=float32), Array(3393.7932, dtype=float32)), 'eval/episode_reward': (Array(-21406.98, dtype=float32), Array(8563.728, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2697233, dtype=float32), Array(0.09577583, dtype=float32)), 'eval/avg_episode_length': (Array(868.03906, dtype=float32), Array(337.1971, dtype=float32)), 'eval/epoch_eval_time': 4.077418804168701, 'eval/sps': 31392.409302947843}
I0728 00:41:46.878618 140567846262592 train.py:379] starting iteration 84, 34406400 steps, 1205.2194945812225
I0728 00:42:00.951053 140567846262592 train.py:394] {'eval/walltime': 360.8819456100464, 'training/sps': 40964.99181921931, 'training/walltime': 848.7607071399689, 'training/entropy_loss': Array(0.02276359, dtype=float32), 'training/policy_loss': Array(0.01712936, dtype=float32), 'training/total_loss': Array(1923.6694, dtype=float32), 'training/v_loss': Array(1923.6294, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29282695, dtype=float32), Array(0.09779525, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(415.40564, dtype=float32), Array(1938.4186, dtype=float32)), 'eval/episode_reward': (Array(-24015.186, dtype=float32), Array(8646.574, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29000708, dtype=float32), Array(0.09943271, dtype=float32)), 'eval/avg_episode_length': (Array(961.1719, dtype=float32), Array(192.58168, dtype=float32)), 'eval/epoch_eval_time': 4.069889545440674, 'eval/sps': 31450.48497529694}
I0728 00:42:00.955535 140567846262592 train.py:379] starting iteration 85, 34816000 steps, 1219.2963967323303
I0728 00:42:15.073215 140567846262592 train.py:394] {'eval/walltime': 364.96012449264526, 'training/sps': 40825.311543699005, 'training/walltime': 858.7936985492706, 'training/entropy_loss': Array(0.0364261, dtype=float32), 'training/policy_loss': Array(0.02411971, dtype=float32), 'training/total_loss': Array(25797.94, dtype=float32), 'training/v_loss': Array(25797.883, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28119096, dtype=float32), Array(0.10418139, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(258.16064, dtype=float32), Array(1513.3298, dtype=float32)), 'eval/episode_reward': (Array(-23414.322, dtype=float32), Array(9103.354, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27810305, dtype=float32), Array(0.10639521, dtype=float32)), 'eval/avg_episode_length': (Array(976.6953, dtype=float32), Array(150.43134, dtype=float32)), 'eval/epoch_eval_time': 4.078178882598877, 'eval/sps': 31386.558482307228}
I0728 00:42:15.075771 140567846262592 train.py:379] starting iteration 86, 35225600 steps, 1233.416648864746
I0728 00:42:29.184952 140567846262592 train.py:394] {'eval/walltime': 369.0313127040863, 'training/sps': 40821.57777549933, 'training/walltime': 868.8276076316833, 'training/entropy_loss': Array(0.05430584, dtype=float32), 'training/policy_loss': Array(0.03260476, dtype=float32), 'training/total_loss': Array(2548.8723, dtype=float32), 'training/v_loss': Array(2548.7854, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.26144043, dtype=float32), Array(0.10790405, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(412.71948, dtype=float32), Array(1938.9672, dtype=float32)), 'eval/episode_reward': (Array(-21236.61, dtype=float32), Array(9121.441, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.25760543, dtype=float32), Array(0.11088667, dtype=float32)), 'eval/avg_episode_length': (Array(961.25, dtype=float32), Array(192.1942, dtype=float32)), 'eval/epoch_eval_time': 4.07118821144104, 'eval/sps': 31440.452603072616}
I0728 00:42:29.187472 140567846262592 train.py:379] starting iteration 87, 35635200 steps, 1247.5283496379852
I0728 00:42:43.313913 140567846262592 train.py:394] {'eval/walltime': 373.16077280044556, 'training/sps': 40989.87333412355, 'training/walltime': 878.8203196525574, 'training/entropy_loss': Array(0.07376905, dtype=float32), 'training/policy_loss': Array(0.04097347, dtype=float32), 'training/total_loss': Array(1361.7036, dtype=float32), 'training/v_loss': Array(1361.5889, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28351188, dtype=float32), Array(0.09930676, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(570.8662, dtype=float32), Array(2274.1777, dtype=float32)), 'eval/episode_reward': (Array(-23014.633, dtype=float32), Array(8766.231, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2804497, dtype=float32), Array(0.10142318, dtype=float32)), 'eval/avg_episode_length': (Array(945.72656, dtype=float32), Array(225.64867, dtype=float32)), 'eval/epoch_eval_time': 4.129460096359253, 'eval/sps': 30996.788203099833}
I0728 00:42:43.319711 140567846262592 train.py:379] starting iteration 88, 36044800 steps, 1261.6605730056763
I0728 00:42:57.439371 140567846262592 train.py:394] {'eval/walltime': 377.23074769973755, 'training/sps': 40776.77640940613, 'training/walltime': 888.8652529716492, 'training/entropy_loss': Array(0.09501719, dtype=float32), 'training/policy_loss': Array(0.04971931, dtype=float32), 'training/total_loss': Array(1306.862, dtype=float32), 'training/v_loss': Array(1306.7174, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2728172, dtype=float32), Array(0.10954649, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(335.52768, dtype=float32), Array(1740.4589, dtype=float32)), 'eval/episode_reward': (Array(-22472.55, dtype=float32), Array(9472.518, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2686468, dtype=float32), Array(0.11299521, dtype=float32)), 'eval/avg_episode_length': (Array(969.0156, dtype=float32), Array(172.51372, dtype=float32)), 'eval/epoch_eval_time': 4.069974899291992, 'eval/sps': 31449.825408571123}
I0728 00:42:57.442140 140567846262592 train.py:379] starting iteration 89, 36454400 steps, 1275.7830169200897
I0728 00:43:11.564049 140567846262592 train.py:394] {'eval/walltime': 381.30841851234436, 'training/sps': 40796.94466077913, 'training/walltime': 898.9052205085754, 'training/entropy_loss': Array(0.10655878, dtype=float32), 'training/policy_loss': Array(0.06620742, dtype=float32), 'training/total_loss': Array(1208.1539, dtype=float32), 'training/v_loss': Array(1207.9812, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.27185404, dtype=float32), Array(0.09874448, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(179.24396, dtype=float32), Array(1240.9791, dtype=float32)), 'eval/episode_reward': (Array(-22817.805, dtype=float32), Array(8473.61, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.26862687, dtype=float32), Array(0.10068141, dtype=float32)), 'eval/avg_episode_length': (Array(984.4531, dtype=float32), Array(123.39955, dtype=float32)), 'eval/epoch_eval_time': 4.0776708126068115, 'eval/sps': 31390.469187523984}
I0728 00:43:11.566842 140567846262592 train.py:379] starting iteration 90, 36864000 steps, 1289.9077191352844
I0728 00:43:25.695844 140567846262592 train.py:394] {'eval/walltime': 385.4254333972931, 'training/sps': 40928.27264173809, 'training/walltime': 908.9129724502563, 'training/entropy_loss': Array(-0.0071576, dtype=float32), 'training/policy_loss': Array(0.0212252, dtype=float32), 'training/total_loss': Array(25499.672, dtype=float32), 'training/v_loss': Array(25499.656, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.26900363, dtype=float32), Array(0.10353548, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(725.8345, dtype=float32), Array(2558.0063, dtype=float32)), 'eval/episode_reward': (Array(-21627.242, dtype=float32), Array(8840.105, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.26593912, dtype=float32), Array(0.10552811, dtype=float32)), 'eval/avg_episode_length': (Array(930.0781, dtype=float32), Array(254.25305, dtype=float32)), 'eval/epoch_eval_time': 4.1170148849487305, 'eval/sps': 31090.487544252344}
I0728 00:43:25.698670 140567846262592 train.py:379] starting iteration 91, 37273600 steps, 1304.0395469665527
I0728 00:43:39.830043 140567846262592 train.py:394] {'eval/walltime': 389.53238129615784, 'training/sps': 40876.336277950926, 'training/walltime': 918.9334399700165, 'training/entropy_loss': Array(-0.02690845, dtype=float32), 'training/policy_loss': Array(0.00529075, dtype=float32), 'training/total_loss': Array(3287.836, dtype=float32), 'training/v_loss': Array(3287.8574, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.27280885, dtype=float32), Array(0.10481673, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1429.3132, dtype=float32), Array(3477.189, dtype=float32)), 'eval/episode_reward': (Array(-20475.611, dtype=float32), Array(9270.773, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27008718, dtype=float32), Array(0.10642557, dtype=float32)), 'eval/avg_episode_length': (Array(860.1328, dtype=float32), Array(345.7614, dtype=float32)), 'eval/epoch_eval_time': 4.106947898864746, 'eval/sps': 31166.696815263254}
I0728 00:43:39.832813 140567846262592 train.py:379] starting iteration 92, 37683200 steps, 1318.1736905574799
I0728 00:43:53.971341 140567846262592 train.py:394] {'eval/walltime': 393.6043257713318, 'training/sps': 40705.7713830846, 'training/walltime': 928.9958951473236, 'training/entropy_loss': Array(-0.03247774, dtype=float32), 'training/policy_loss': Array(0.00236095, dtype=float32), 'training/total_loss': Array(2061.8887, dtype=float32), 'training/v_loss': Array(2061.9187, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29720682, dtype=float32), Array(0.12405477, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(884.53864, dtype=float32), Array(2802.4688, dtype=float32)), 'eval/episode_reward': (Array(-23368.863, dtype=float32), Array(9843.669, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29456204, dtype=float32), Array(0.12627494, dtype=float32)), 'eval/avg_episode_length': (Array(914.5625, dtype=float32), Array(278.64142, dtype=float32)), 'eval/epoch_eval_time': 4.07194447517395, 'eval/sps': 31434.61331076523}
I0728 00:43:53.973875 140567846262592 train.py:379] starting iteration 93, 38092800 steps, 1332.314752817154
I0728 00:44:08.109296 140567846262592 train.py:394] {'eval/walltime': 397.67684173583984, 'training/sps': 40719.22643309422, 'training/walltime': 939.0550253391266, 'training/entropy_loss': Array(-0.03593095, dtype=float32), 'training/policy_loss': Array(0.00183384, dtype=float32), 'training/total_loss': Array(2084.7634, dtype=float32), 'training/v_loss': Array(2084.7976, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2794183, dtype=float32), Array(0.11103487, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(883.0414, dtype=float32), Array(2801.7607, dtype=float32)), 'eval/episode_reward': (Array(-22331.65, dtype=float32), Array(9229.963, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27694196, dtype=float32), Array(0.11268735, dtype=float32)), 'eval/avg_episode_length': (Array(914.6797, dtype=float32), Array(278.25964, dtype=float32)), 'eval/epoch_eval_time': 4.072515964508057, 'eval/sps': 31430.202144207404}
I0728 00:44:08.111856 140567846262592 train.py:379] starting iteration 94, 38502400 steps, 1346.452733039856
I0728 00:44:22.234152 140567846262592 train.py:394] {'eval/walltime': 401.79281544685364, 'training/sps': 40948.9600076731, 'training/walltime': 949.0577213764191, 'training/entropy_loss': Array(-0.03903117, dtype=float32), 'training/policy_loss': Array(0.00424776, dtype=float32), 'training/total_loss': Array(2576.2842, dtype=float32), 'training/v_loss': Array(2576.319, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28164512, dtype=float32), Array(0.1238511, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(648.85706, dtype=float32), Array(2420.5713, dtype=float32)), 'eval/episode_reward': (Array(-22507.734, dtype=float32), Array(9544.053, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27912384, dtype=float32), Array(0.12533489, dtype=float32)), 'eval/avg_episode_length': (Array(937.84375, dtype=float32), Array(240.7307, dtype=float32)), 'eval/epoch_eval_time': 4.115973711013794, 'eval/sps': 31098.35217302024}
I0728 00:44:22.236726 140567846262592 train.py:379] starting iteration 95, 38912000 steps, 1360.577603340149
I0728 00:44:36.367717 140567846262592 train.py:394] {'eval/walltime': 405.90259861946106, 'training/sps': 40889.84205523717, 'training/walltime': 959.0748791694641, 'training/entropy_loss': Array(-0.04460703, dtype=float32), 'training/policy_loss': Array(0.00443344, dtype=float32), 'training/total_loss': Array(26192.184, dtype=float32), 'training/v_loss': Array(26192.223, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29306024, dtype=float32), Array(0.12156423, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(884.1953, dtype=float32), Array(2802.4436, dtype=float32)), 'eval/episode_reward': (Array(-23255.178, dtype=float32), Array(8740.191, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29077044, dtype=float32), Array(0.12298273, dtype=float32)), 'eval/avg_episode_length': (Array(914.71094, dtype=float32), Array(278.15732, dtype=float32)), 'eval/epoch_eval_time': 4.109783172607422, 'eval/sps': 31145.195409126984}
I0728 00:44:36.370258 140567846262592 train.py:379] starting iteration 96, 39321600 steps, 1374.7111358642578
I0728 00:44:50.524383 140567846262592 train.py:394] {'eval/walltime': 409.98632740974426, 'training/sps': 40690.04104846386, 'training/walltime': 969.1412243843079, 'training/entropy_loss': Array(-0.04550088, dtype=float32), 'training/policy_loss': Array(0.0018901, dtype=float32), 'training/total_loss': Array(3538.641, dtype=float32), 'training/v_loss': Array(3538.685, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3024211, dtype=float32), Array(0.11319381, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(416.28955, dtype=float32), Array(1937.3203, dtype=float32)), 'eval/episode_reward': (Array(-23896.982, dtype=float32), Array(8880.409, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3006575, dtype=float32), Array(0.11436409, dtype=float32)), 'eval/avg_episode_length': (Array(961.1797, dtype=float32), Array(192.54262, dtype=float32)), 'eval/epoch_eval_time': 4.083728790283203, 'eval/sps': 31343.903225053127}
I0728 00:44:50.527056 140567846262592 train.py:379] starting iteration 97, 39731200 steps, 1388.867932319641
I0728 00:45:04.623304 140567846262592 train.py:394] {'eval/walltime': 414.05870485305786, 'training/sps': 40879.70945393787, 'training/walltime': 979.1608650684357, 'training/entropy_loss': Array(-0.04672653, dtype=float32), 'training/policy_loss': Array(0.00089006, dtype=float32), 'training/total_loss': Array(2275.4346, dtype=float32), 'training/v_loss': Array(2275.4805, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2896825, dtype=float32), Array(0.11479501, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(258.8377, dtype=float32), Array(1513.5714, dtype=float32)), 'eval/episode_reward': (Array(-23335.98, dtype=float32), Array(8759.39, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28736722, dtype=float32), Array(0.11617952, dtype=float32)), 'eval/avg_episode_length': (Array(976.7422, dtype=float32), Array(150.12907, dtype=float32)), 'eval/epoch_eval_time': 4.072377443313599, 'eval/sps': 31431.271236943445}
I0728 00:45:04.626016 140567846262592 train.py:379] starting iteration 98, 40140800 steps, 1402.9668934345245
I0728 00:45:18.747813 140567846262592 train.py:394] {'eval/walltime': 418.1533787250519, 'training/sps': 40866.53116223991, 'training/walltime': 989.1837368011475, 'training/entropy_loss': Array(-0.04854593, dtype=float32), 'training/policy_loss': Array(0.00148636, dtype=float32), 'training/total_loss': Array(2417.524, dtype=float32), 'training/v_loss': Array(2417.571, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29283583, dtype=float32), Array(0.12725522, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(337.27878, dtype=float32), Array(1738.9818, dtype=float32)), 'eval/episode_reward': (Array(-23907.555, dtype=float32), Array(8752.973, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29088524, dtype=float32), Array(0.1285851, dtype=float32)), 'eval/avg_episode_length': (Array(968.90625, dtype=float32), Array(173.12317, dtype=float32)), 'eval/epoch_eval_time': 4.0946738719940186, 'eval/sps': 31260.120830494063}
I0728 00:45:18.751601 140567846262592 train.py:379] starting iteration 99, 40550400 steps, 1417.0924713611603
I0728 00:45:32.901712 140567846262592 train.py:394] {'eval/walltime': 422.2293701171875, 'training/sps': 40674.93437328742, 'training/walltime': 999.2538206577301, 'training/entropy_loss': Array(-0.04706047, dtype=float32), 'training/policy_loss': Array(0.00086309, dtype=float32), 'training/total_loss': Array(2247.191, dtype=float32), 'training/v_loss': Array(2247.237, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28860867, dtype=float32), Array(0.12212547, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(258.88422, dtype=float32), Array(1513.6354, dtype=float32)), 'eval/episode_reward': (Array(-23613.074, dtype=float32), Array(9456.588, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28600672, dtype=float32), Array(0.12392078, dtype=float32)), 'eval/avg_episode_length': (Array(976.6953, dtype=float32), Array(150.43117, dtype=float32)), 'eval/epoch_eval_time': 4.07599139213562, 'eval/sps': 31403.402923511636}
I0728 00:45:32.904487 140567846262592 train.py:379] starting iteration 100, 40960000 steps, 1431.245364189148
I0728 00:45:47.019462 140567846262592 train.py:394] {'eval/walltime': 426.34012269973755, 'training/sps': 40959.07716922939, 'training/walltime': 1009.2540459632874, 'training/entropy_loss': Array(-0.04400577, dtype=float32), 'training/policy_loss': Array(0.00084138, dtype=float32), 'training/total_loss': Array(20079.3, dtype=float32), 'training/v_loss': Array(20079.344, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3275633, dtype=float32), Array(0.1289668, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(418.42065, dtype=float32), Array(1936.5687, dtype=float32)), 'eval/episode_reward': (Array(-25331.062, dtype=float32), Array(9880.353, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.32583892, dtype=float32), Array(0.13022067, dtype=float32)), 'eval/avg_episode_length': (Array(961.16406, dtype=float32), Array(192.62044, dtype=float32)), 'eval/epoch_eval_time': 4.110752582550049, 'eval/sps': 31137.850656192244}
I0728 00:45:47.094027 140567846262592 train.py:379] starting iteration 101, 41369600 steps, 1445.4349024295807
I0728 00:46:01.220203 140567846262592 train.py:394] {'eval/walltime': 430.41719007492065, 'training/sps': 40775.16501073062, 'training/walltime': 1019.2993762493134, 'training/entropy_loss': Array(-0.04829709, dtype=float32), 'training/policy_loss': Array(0.00040827, dtype=float32), 'training/total_loss': Array(3220.9622, dtype=float32), 'training/v_loss': Array(3221.0103, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30911425, dtype=float32), Array(0.13936666, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(651.2087, dtype=float32), Array(2420.2634, dtype=float32)), 'eval/episode_reward': (Array(-23274.422, dtype=float32), Array(9898.946, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30692157, dtype=float32), Array(0.14144777, dtype=float32)), 'eval/avg_episode_length': (Array(937.8828, dtype=float32), Array(240.5792, dtype=float32)), 'eval/epoch_eval_time': 4.0770673751831055, 'eval/sps': 31395.11521912276}
I0728 00:46:01.223076 140567846262592 train.py:379] starting iteration 102, 41779200 steps, 1459.563952922821
I0728 00:46:15.342863 140567846262592 train.py:394] {'eval/walltime': 434.4895803928375, 'training/sps': 40782.94251598108, 'training/walltime': 1029.3427908420563, 'training/entropy_loss': Array(-0.04822829, dtype=float32), 'training/policy_loss': Array(0.00061951, dtype=float32), 'training/total_loss': Array(2645.9392, dtype=float32), 'training/v_loss': Array(2645.9868, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2880891, dtype=float32), Array(0.09396931, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(258.76938, dtype=float32), Array(1514.098, dtype=float32)), 'eval/episode_reward': (Array(-23417.71, dtype=float32), Array(7578.815, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28619903, dtype=float32), Array(0.09481449, dtype=float32)), 'eval/avg_episode_length': (Array(976.6797, dtype=float32), Array(150.53197, dtype=float32)), 'eval/epoch_eval_time': 4.07239031791687, 'eval/sps': 31431.171868976257}
I0728 00:46:15.345539 140567846262592 train.py:379] starting iteration 103, 42188800 steps, 1473.6864166259766
I0728 00:46:29.500690 140567846262592 train.py:394] {'eval/walltime': 438.57317662239075, 'training/sps': 40685.22488467498, 'training/walltime': 1039.4103276729584, 'training/entropy_loss': Array(-0.04721071, dtype=float32), 'training/policy_loss': Array(0.00029151, dtype=float32), 'training/total_loss': Array(2240.581, dtype=float32), 'training/v_loss': Array(2240.628, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3068278, dtype=float32), Array(0.13071379, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(416.66373, dtype=float32), Array(1936.6549, dtype=float32)), 'eval/episode_reward': (Array(-24472.594, dtype=float32), Array(9787.113, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30468088, dtype=float32), Array(0.13213822, dtype=float32)), 'eval/avg_episode_length': (Array(961.1328, dtype=float32), Array(192.77527, dtype=float32)), 'eval/epoch_eval_time': 4.083596229553223, 'eval/sps': 31344.920703388005}
I0728 00:46:29.503370 140567846262592 train.py:379] starting iteration 104, 42598400 steps, 1487.8442475795746
I0728 00:46:43.584279 140567846262592 train.py:394] {'eval/walltime': 442.6494333744049, 'training/sps': 40957.024630211636, 'training/walltime': 1049.411054134369, 'training/entropy_loss': Array(-0.04693651, dtype=float32), 'training/policy_loss': Array(0.00038037, dtype=float32), 'training/total_loss': Array(2232.0146, dtype=float32), 'training/v_loss': Array(2232.0615, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30740607, dtype=float32), Array(0.11119466, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(494.80817, dtype=float32), Array(2114.704, dtype=float32)), 'eval/episode_reward': (Array(-24019.291, dtype=float32), Array(8675.437, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3055895, dtype=float32), Array(0.11260448, dtype=float32)), 'eval/avg_episode_length': (Array(953.4297, dtype=float32), Array(209.9975, dtype=float32)), 'eval/epoch_eval_time': 4.07625675201416, 'eval/sps': 31401.358596156297}
I0728 00:46:43.586981 140567846262592 train.py:379] starting iteration 105, 43008000 steps, 1501.9278585910797
I0728 00:46:57.751544 140567846262592 train.py:394] {'eval/walltime': 446.7298753261566, 'training/sps': 40635.24676580118, 'training/walltime': 1059.4909732341766, 'training/entropy_loss': Array(-0.05196624, dtype=float32), 'training/policy_loss': Array(0.00125185, dtype=float32), 'training/total_loss': Array(23676.389, dtype=float32), 'training/v_loss': Array(23676.44, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3045911, dtype=float32), Array(0.12250575, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(807.0477, dtype=float32), Array(2682.2358, dtype=float32)), 'eval/episode_reward': (Array(-24093.562, dtype=float32), Array(9816.158, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30260342, dtype=float32), Array(0.12399685, dtype=float32)), 'eval/avg_episode_length': (Array(922.28906, dtype=float32), Array(266.94614, dtype=float32)), 'eval/epoch_eval_time': 4.080441951751709, 'eval/sps': 31369.151065866867}
I0728 00:46:57.757304 140567846262592 train.py:379] starting iteration 106, 43417600 steps, 1516.0981664657593
I0728 00:47:11.865788 140567846262592 train.py:394] {'eval/walltime': 450.8095648288727, 'training/sps': 40858.63137430718, 'training/walltime': 1069.5157828330994, 'training/entropy_loss': Array(-0.05301352, dtype=float32), 'training/policy_loss': Array(0.00154547, dtype=float32), 'training/total_loss': Array(3598.272, dtype=float32), 'training/v_loss': Array(3598.3237, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28110507, dtype=float32), Array(0.11854184, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(648.76874, dtype=float32), Array(2421.441, dtype=float32)), 'eval/episode_reward': (Array(-22314.479, dtype=float32), Array(9252.399, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27881542, dtype=float32), Array(0.1200876, dtype=float32)), 'eval/avg_episode_length': (Array(937.9219, dtype=float32), Array(240.42783, dtype=float32)), 'eval/epoch_eval_time': 4.0796895027160645, 'eval/sps': 31374.93672368538}
I0728 00:47:11.868587 140567846262592 train.py:379] starting iteration 107, 43827200 steps, 1530.2094650268555
I0728 00:47:26.081595 140567846262592 train.py:394] {'eval/walltime': 454.9063832759857, 'training/sps': 40504.77630866553, 'training/walltime': 1079.628170490265, 'training/entropy_loss': Array(-0.05074954, dtype=float32), 'training/policy_loss': Array(0.00357352, dtype=float32), 'training/total_loss': Array(2479.018, dtype=float32), 'training/v_loss': Array(2479.0654, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2983482, dtype=float32), Array(0.12808758, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(337.7895, dtype=float32), Array(1740.668, dtype=float32)), 'eval/episode_reward': (Array(-23541.926, dtype=float32), Array(9229.755, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29615918, dtype=float32), Array(0.12993038, dtype=float32)), 'eval/avg_episode_length': (Array(968.91406, dtype=float32), Array(173.0794, dtype=float32)), 'eval/epoch_eval_time': 4.096818447113037, 'eval/sps': 31243.756991525843}
I0728 00:47:26.084453 140567846262592 train.py:379] starting iteration 108, 44236800 steps, 1544.4253301620483
I0728 00:47:40.184798 140567846262592 train.py:394] {'eval/walltime': 458.9966788291931, 'training/sps': 40935.36346006817, 'training/walltime': 1089.6341888904572, 'training/entropy_loss': Array(-0.04739537, dtype=float32), 'training/policy_loss': Array(0.00200495, dtype=float32), 'training/total_loss': Array(2249.3623, dtype=float32), 'training/v_loss': Array(2249.4077, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28008464, dtype=float32), Array(0.1252751, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(258.1073, dtype=float32), Array(1513.2332, dtype=float32)), 'eval/episode_reward': (Array(-22811.277, dtype=float32), Array(9696.823, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27765948, dtype=float32), Array(0.12705998, dtype=float32)), 'eval/avg_episode_length': (Array(976.7344, dtype=float32), Array(150.17897, dtype=float32)), 'eval/epoch_eval_time': 4.0902955532073975, 'eval/sps': 31293.582171495906}
I0728 00:47:40.187637 140567846262592 train.py:379] starting iteration 109, 44646400 steps, 1558.5285139083862
I0728 00:47:54.403498 140567846262592 train.py:394] {'eval/walltime': 463.1188259124756, 'training/sps': 40596.40012107177, 'training/walltime': 1099.723753452301, 'training/entropy_loss': Array(-0.04640766, dtype=float32), 'training/policy_loss': Array(0.00108023, dtype=float32), 'training/total_loss': Array(2312.8953, dtype=float32), 'training/v_loss': Array(2312.9404, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28480655, dtype=float32), Array(0.11843498, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(24.110468, dtype=float32), Array(10.127959, dtype=float32)), 'eval/episode_reward': (Array(-24051.035, dtype=float32), Array(8859.459, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2825546, dtype=float32), Array(0.12026754, dtype=float32)), 'eval/avg_episode_length': (Array(1000., dtype=float32), Array(0., dtype=float32)), 'eval/epoch_eval_time': 4.122147083282471, 'eval/sps': 31051.778942849716}
I0728 00:47:54.406324 140567846262592 train.py:379] starting iteration 110, 45056000 steps, 1572.7472012043
I0728 00:48:08.527282 140567846262592 train.py:394] {'eval/walltime': 467.2132215499878, 'training/sps': 40868.07882099405, 'training/walltime': 1109.746245622635, 'training/entropy_loss': Array(-0.04122937, dtype=float32), 'training/policy_loss': Array(0.00031605, dtype=float32), 'training/total_loss': Array(20498.39, dtype=float32), 'training/v_loss': Array(20498.434, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29455894, dtype=float32), Array(0.13151623, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(259.32553, dtype=float32), Array(1511.776, dtype=float32)), 'eval/episode_reward': (Array(-23784.422, dtype=float32), Array(9456.255, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.292251, dtype=float32), Array(0.13323714, dtype=float32)), 'eval/avg_episode_length': (Array(976.71094, dtype=float32), Array(150.33026, dtype=float32)), 'eval/epoch_eval_time': 4.094395637512207, 'eval/sps': 31262.245110678654}
I0728 00:48:08.530112 140567846262592 train.py:379] starting iteration 111, 45465600 steps, 1586.8709897994995
I0728 00:48:22.700415 140567846262592 train.py:394] {'eval/walltime': 471.28785705566406, 'training/sps': 40587.662763850465, 'training/walltime': 1119.8379821777344, 'training/entropy_loss': Array(-0.04650158, dtype=float32), 'training/policy_loss': Array(0.0002283, dtype=float32), 'training/total_loss': Array(2756.7466, dtype=float32), 'training/v_loss': Array(2756.793, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2908603, dtype=float32), Array(0.11709072, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(102.78856, dtype=float32), Array(879.8276, dtype=float32)), 'eval/episode_reward': (Array(-23848.277, dtype=float32), Array(8579.303, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28875875, dtype=float32), Array(0.11844535, dtype=float32)), 'eval/avg_episode_length': (Array(992.21094, dtype=float32), Array(87.778275, dtype=float32)), 'eval/epoch_eval_time': 4.0746355056762695, 'eval/sps': 31413.852802707508}
I0728 00:48:22.703179 140567846262592 train.py:379] starting iteration 112, 45875200 steps, 1601.0440559387207
I0728 00:48:36.826921 140567846262592 train.py:394] {'eval/walltime': 475.3619558811188, 'training/sps': 40773.272142897404, 'training/walltime': 1129.883778810501, 'training/entropy_loss': Array(-0.04558505, dtype=float32), 'training/policy_loss': Array(0.00033994, dtype=float32), 'training/total_loss': Array(2201.8347, dtype=float32), 'training/v_loss': Array(2201.8801, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29507184, dtype=float32), Array(0.11662991, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(259.38672, dtype=float32), Array(1512.2212, dtype=float32)), 'eval/episode_reward': (Array(-23809.373, dtype=float32), Array(8886.57, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29286218, dtype=float32), Array(0.11838209, dtype=float32)), 'eval/avg_episode_length': (Array(976.6875, dtype=float32), Array(150.48166, dtype=float32)), 'eval/epoch_eval_time': 4.074098825454712, 'eval/sps': 31417.990943239787}
I0728 00:48:36.829734 140567846262592 train.py:379] starting iteration 113, 46284800 steps, 1615.1706116199493
I0728 00:48:50.972915 140567846262592 train.py:394] {'eval/walltime': 479.442179441452, 'training/sps': 40719.66363500379, 'training/walltime': 1139.9428009986877, 'training/entropy_loss': Array(-0.04407363, dtype=float32), 'training/policy_loss': Array(0.00025212, dtype=float32), 'training/total_loss': Array(1860.2047, dtype=float32), 'training/v_loss': Array(1860.2485, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30330068, dtype=float32), Array(0.11549105, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(416.3483, dtype=float32), Array(1936.9023, dtype=float32)), 'eval/episode_reward': (Array(-23775.945, dtype=float32), Array(9169.689, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3010085, dtype=float32), Array(0.11711723, dtype=float32)), 'eval/avg_episode_length': (Array(961.2344, dtype=float32), Array(192.27191, dtype=float32)), 'eval/epoch_eval_time': 4.080223560333252, 'eval/sps': 31370.830080091397}
I0728 00:48:50.975761 140567846262592 train.py:379] starting iteration 114, 46694400 steps, 1629.3166382312775
I0728 00:49:05.072146 140567846262592 train.py:394] {'eval/walltime': 483.52340722084045, 'training/sps': 40914.60992540267, 'training/walltime': 1149.953894853592, 'training/entropy_loss': Array(-0.04380786, dtype=float32), 'training/policy_loss': Array(0.00029737, dtype=float32), 'training/total_loss': Array(1918.0677, dtype=float32), 'training/v_loss': Array(1918.1113, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28856736, dtype=float32), Array(0.12340014, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(180.6807, dtype=float32), Array(1241.3813, dtype=float32)), 'eval/episode_reward': (Array(-23744.457, dtype=float32), Array(9382.777, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28594163, dtype=float32), Array(0.12558873, dtype=float32)), 'eval/avg_episode_length': (Array(984.52344, dtype=float32), Array(122.841415, dtype=float32)), 'eval/epoch_eval_time': 4.081227779388428, 'eval/sps': 31363.11103399889}
I0728 00:49:05.074974 140567846262592 train.py:379] starting iteration 115, 47104000 steps, 1643.4158508777618
I0728 00:49:19.203414 140567846262592 train.py:394] {'eval/walltime': 487.6083393096924, 'training/sps': 40798.08884908261, 'training/walltime': 1159.9935808181763, 'training/entropy_loss': Array(-0.04144339, dtype=float32), 'training/policy_loss': Array(0.00034253, dtype=float32), 'training/total_loss': Array(20437.115, dtype=float32), 'training/v_loss': Array(20437.156, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31037056, dtype=float32), Array(0.11811733, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(573.1603, dtype=float32), Array(2273.3054, dtype=float32)), 'eval/episode_reward': (Array(-24016.465, dtype=float32), Array(9283.996, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30848926, dtype=float32), Array(0.11923129, dtype=float32)), 'eval/avg_episode_length': (Array(945.64844, dtype=float32), Array(225.97281, dtype=float32)), 'eval/epoch_eval_time': 4.084932088851929, 'eval/sps': 31334.67025053394}
I0728 00:49:19.206220 140567846262592 train.py:379] starting iteration 116, 47513600 steps, 1657.54709649086
I0728 00:49:33.332641 140567846262592 train.py:394] {'eval/walltime': 491.68386220932007, 'training/sps': 40769.070913972406, 'training/walltime': 1170.0404126644135, 'training/entropy_loss': Array(-0.04631795, dtype=float32), 'training/policy_loss': Array(0.00106374, dtype=float32), 'training/total_loss': Array(3020.1006, dtype=float32), 'training/v_loss': Array(3020.1458, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28930295, dtype=float32), Array(0.1165044, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(415.1745, dtype=float32), Array(1937.3081, dtype=float32)), 'eval/episode_reward': (Array(-22956.812, dtype=float32), Array(9237.8545, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28713995, dtype=float32), Array(0.11801197, dtype=float32)), 'eval/avg_episode_length': (Array(961.21875, dtype=float32), Array(192.34897, dtype=float32)), 'eval/epoch_eval_time': 4.0755228996276855, 'eval/sps': 31407.01283059734}
I0728 00:49:33.335457 140567846262592 train.py:379] starting iteration 117, 47923200 steps, 1671.6763339042664
I0728 00:49:47.491854 140567846262592 train.py:394] {'eval/walltime': 495.7668616771698, 'training/sps': 40677.333389653795, 'training/walltime': 1180.1099026203156, 'training/entropy_loss': Array(-0.04565381, dtype=float32), 'training/policy_loss': Array(0.00107513, dtype=float32), 'training/total_loss': Array(2408.7441, dtype=float32), 'training/v_loss': Array(2408.7886, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31359953, dtype=float32), Array(0.124412, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(339.1171, dtype=float32), Array(1740.7216, dtype=float32)), 'eval/episode_reward': (Array(-25051.098, dtype=float32), Array(9705.754, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.31157124, dtype=float32), Array(0.1261394, dtype=float32)), 'eval/avg_episode_length': (Array(968.97656, dtype=float32), Array(172.73134, dtype=float32)), 'eval/epoch_eval_time': 4.0829994678497314, 'eval/sps': 31349.502004076883}
I0728 00:49:47.494645 140567846262592 train.py:379] starting iteration 118, 48332800 steps, 1685.8355221748352
I0728 00:50:01.623432 140567846262592 train.py:394] {'eval/walltime': 499.84495663642883, 'training/sps': 40768.46334589421, 'training/walltime': 1190.1568841934204, 'training/entropy_loss': Array(-0.0457392, dtype=float32), 'training/policy_loss': Array(0.00275684, dtype=float32), 'training/total_loss': Array(1705.1115, dtype=float32), 'training/v_loss': Array(1705.1544, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29267776, dtype=float32), Array(0.10150003, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(884.2464, dtype=float32), Array(2803.2422, dtype=float32)), 'eval/episode_reward': (Array(-22255.451, dtype=float32), Array(8440.091, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29081726, dtype=float32), Array(0.10257246, dtype=float32)), 'eval/avg_episode_length': (Array(914.6328, dtype=float32), Array(278.4121, dtype=float32)), 'eval/epoch_eval_time': 4.078094959259033, 'eval/sps': 31387.20438801574}
I0728 00:50:01.626215 140567846262592 train.py:379] starting iteration 119, 48742400 steps, 1699.9670927524567
I0728 00:50:15.753148 140567846262592 train.py:394] {'eval/walltime': 503.9171872138977, 'training/sps': 40752.37808778708, 'training/walltime': 1200.2078313827515, 'training/entropy_loss': Array(-0.04714616, dtype=float32), 'training/policy_loss': Array(0.00240167, dtype=float32), 'training/total_loss': Array(1859.9907, dtype=float32), 'training/v_loss': Array(1860.0354, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29222924, dtype=float32), Array(0.10267152, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1040.3912, dtype=float32), Array(3019.3914, dtype=float32)), 'eval/episode_reward': (Array(-23360.066, dtype=float32), Array(8976.94, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29038504, dtype=float32), Array(0.10362297, dtype=float32)), 'eval/avg_episode_length': (Array(899.0703, dtype=float32), Array(300.1905, dtype=float32)), 'eval/epoch_eval_time': 4.072230577468872, 'eval/sps': 31432.404812293178}
I0728 00:50:15.755975 140567846262592 train.py:379] starting iteration 120, 49152000 steps, 1714.096851825714
I0728 00:50:29.856609 140567846262592 train.py:394] {'eval/walltime': 507.99612522125244, 'training/sps': 40887.97063888155, 'training/walltime': 1210.2254476547241, 'training/entropy_loss': Array(-0.0483655, dtype=float32), 'training/policy_loss': Array(0.00356874, dtype=float32), 'training/total_loss': Array(24222.715, dtype=float32), 'training/v_loss': Array(24222.758, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3086254, dtype=float32), Array(0.11395723, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(416.79358, dtype=float32), Array(1936.6342, dtype=float32)), 'eval/episode_reward': (Array(-24969.307, dtype=float32), Array(8908.908, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3068741, dtype=float32), Array(0.11504842, dtype=float32)), 'eval/avg_episode_length': (Array(961.1719, dtype=float32), Array(192.58144, dtype=float32)), 'eval/epoch_eval_time': 4.078938007354736, 'eval/sps': 31380.71717912925}
I0728 00:50:29.859416 140567846262592 train.py:379] starting iteration 121, 49561600 steps, 1728.2002935409546
I0728 00:50:43.981594 140567846262592 train.py:394] {'eval/walltime': 512.0712251663208, 'training/sps': 40783.127431175395, 'training/walltime': 1220.2688167095184, 'training/entropy_loss': Array(-0.04916936, dtype=float32), 'training/policy_loss': Array(0.00162699, dtype=float32), 'training/total_loss': Array(3302.2437, dtype=float32), 'training/v_loss': Array(3302.2915, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2869131, dtype=float32), Array(0.12603767, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(805.56213, dtype=float32), Array(2685.0188, dtype=float32)), 'eval/episode_reward': (Array(-22027.514, dtype=float32), Array(10065.548, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2845348, dtype=float32), Array(0.1279981, dtype=float32)), 'eval/avg_episode_length': (Array(922.39844, dtype=float32), Array(266.5706, dtype=float32)), 'eval/epoch_eval_time': 4.075099945068359, 'eval/sps': 31410.27256396599}
I0728 00:50:43.984515 140567846262592 train.py:379] starting iteration 122, 49971200 steps, 1742.3253922462463
I0728 00:50:58.122267 140567846262592 train.py:394] {'eval/walltime': 516.1470801830292, 'training/sps': 40724.224413329655, 'training/walltime': 1230.3267123699188, 'training/entropy_loss': Array(-0.04851233, dtype=float32), 'training/policy_loss': Array(0.00320072, dtype=float32), 'training/total_loss': Array(2449.9185, dtype=float32), 'training/v_loss': Array(2449.9639, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3006241, dtype=float32), Array(0.11730621, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(338.0144, dtype=float32), Array(1739.0168, dtype=float32)), 'eval/episode_reward': (Array(-24403.195, dtype=float32), Array(8903.503, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29836076, dtype=float32), Array(0.11901996, dtype=float32)), 'eval/avg_episode_length': (Array(968.9375, dtype=float32), Array(172.94922, dtype=float32)), 'eval/epoch_eval_time': 4.075855016708374, 'eval/sps': 31404.453660712326}
I0728 00:50:58.125055 140567846262592 train.py:379] starting iteration 123, 50380800 steps, 1756.4659323692322
I0728 00:51:12.265989 140567846262592 train.py:394] {'eval/walltime': 520.2241296768188, 'training/sps': 40715.5409914888, 'training/walltime': 1240.3867530822754, 'training/entropy_loss': Array(-0.04692966, dtype=float32), 'training/policy_loss': Array(0.00541725, dtype=float32), 'training/total_loss': Array(1602.6326, dtype=float32), 'training/v_loss': Array(1602.6741, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28066865, dtype=float32), Array(0.12097002, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(258.12268, dtype=float32), Array(1512.6527, dtype=float32)), 'eval/episode_reward': (Array(-22906.277, dtype=float32), Array(8972.382, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2785497, dtype=float32), Array(0.1224078, dtype=float32)), 'eval/avg_episode_length': (Array(976.6875, dtype=float32), Array(150.48161, dtype=float32)), 'eval/epoch_eval_time': 4.077049493789673, 'eval/sps': 31395.252913896384}
I0728 00:51:12.268798 140567846262592 train.py:379] starting iteration 124, 50790400 steps, 1770.609675168991
I0728 00:51:26.399462 140567846262592 train.py:394] {'eval/walltime': 524.3020076751709, 'training/sps': 40761.28805986726, 'training/walltime': 1250.4355032444, 'training/entropy_loss': Array(-0.04467946, dtype=float32), 'training/policy_loss': Array(0.00263679, dtype=float32), 'training/total_loss': Array(1590.7971, dtype=float32), 'training/v_loss': Array(1590.8391, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2838381, dtype=float32), Array(0.10143863, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(492.82465, dtype=float32), Array(2114.4177, dtype=float32)), 'eval/episode_reward': (Array(-22558.916, dtype=float32), Array(8256.829, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28170598, dtype=float32), Array(0.10292107, dtype=float32)), 'eval/avg_episode_length': (Array(953.4297, dtype=float32), Array(209.9975, dtype=float32)), 'eval/epoch_eval_time': 4.077877998352051, 'eval/sps': 31388.87432427534}
I0728 00:51:26.402302 140567846262592 train.py:379] starting iteration 125, 51200000 steps, 1784.7431790828705
I0728 00:51:40.547125 140567846262592 train.py:394] {'eval/walltime': 528.3756070137024, 'training/sps': 40686.5699809359, 'training/walltime': 1260.5027072429657, 'training/entropy_loss': Array(-0.0389142, dtype=float32), 'training/policy_loss': Array(0.0016101, dtype=float32), 'training/total_loss': Array(20974.121, dtype=float32), 'training/v_loss': Array(20974.16, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28226942, dtype=float32), Array(0.10583137, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(492.69342, dtype=float32), Array(2113.6394, dtype=float32)), 'eval/episode_reward': (Array(-22955.504, dtype=float32), Array(8775.396, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27988923, dtype=float32), Array(0.10751634, dtype=float32)), 'eval/avg_episode_length': (Array(953.5156, dtype=float32), Array(209.61009, dtype=float32)), 'eval/epoch_eval_time': 4.073599338531494, 'eval/sps': 31421.843279791763}
I0728 00:51:40.635408 140567846262592 train.py:379] starting iteration 126, 51609600 steps, 1798.9762835502625
I0728 00:51:54.814880 140567846262592 train.py:394] {'eval/walltime': 532.4879562854767, 'training/sps': 40703.653500569286, 'training/walltime': 1270.5656859874725, 'training/entropy_loss': Array(-0.04124522, dtype=float32), 'training/policy_loss': Array(0.00103065, dtype=float32), 'training/total_loss': Array(2394.7244, dtype=float32), 'training/v_loss': Array(2394.7646, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28053337, dtype=float32), Array(0.11995758, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(492.5257, dtype=float32), Array(2113.3662, dtype=float32)), 'eval/episode_reward': (Array(-22492.338, dtype=float32), Array(9838.556, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27774933, dtype=float32), Array(0.12181323, dtype=float32)), 'eval/avg_episode_length': (Array(953.3828, dtype=float32), Array(210.20872, dtype=float32)), 'eval/epoch_eval_time': 4.112349271774292, 'eval/sps': 31125.760858530826}
I0728 00:51:54.817826 140567846262592 train.py:379] starting iteration 127, 52019200 steps, 1813.158703327179
I0728 00:52:08.961802 140567846262592 train.py:394] {'eval/walltime': 536.5646874904633, 'training/sps': 40702.567640814304, 'training/walltime': 1280.6289331912994, 'training/entropy_loss': Array(-0.03798487, dtype=float32), 'training/policy_loss': Array(0.00067209, dtype=float32), 'training/total_loss': Array(2062.9546, dtype=float32), 'training/v_loss': Array(2062.9922, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29646158, dtype=float32), Array(0.11174177, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(493.88574, dtype=float32), Array(2113.4534, dtype=float32)), 'eval/episode_reward': (Array(-23618.35, dtype=float32), Array(9290.936, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29419512, dtype=float32), Array(0.11307526, dtype=float32)), 'eval/avg_episode_length': (Array(953.375, dtype=float32), Array(210.24393, dtype=float32)), 'eval/epoch_eval_time': 4.076731204986572, 'eval/sps': 31397.70408297537}
I0728 00:52:08.964591 140567846262592 train.py:379] starting iteration 128, 52428800 steps, 1827.305468082428
I0728 00:52:23.133487 140567846262592 train.py:394] {'eval/walltime': 540.6453020572662, 'training/sps': 40617.53494086215, 'training/walltime': 1290.7132477760315, 'training/entropy_loss': Array(-0.03540356, dtype=float32), 'training/policy_loss': Array(0.00064571, dtype=float32), 'training/total_loss': Array(1383.0286, dtype=float32), 'training/v_loss': Array(1383.0634, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2830534, dtype=float32), Array(0.11165002, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(180.19702, dtype=float32), Array(1239.0558, dtype=float32)), 'eval/episode_reward': (Array(-23097.576, dtype=float32), Array(9192.433, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28064692, dtype=float32), Array(0.11301456, dtype=float32)), 'eval/avg_episode_length': (Array(984.4844, dtype=float32), Array(123.15171, dtype=float32)), 'eval/epoch_eval_time': 4.0806145668029785, 'eval/sps': 31367.824111916456}
I0728 00:52:23.136320 140567846262592 train.py:379] starting iteration 129, 52838400 steps, 1841.4771971702576
I0728 00:52:37.268469 140567846262592 train.py:394] {'eval/walltime': 544.7266459465027, 'training/sps': 40768.76325780969, 'training/walltime': 1300.7601554393768, 'training/entropy_loss': Array(-0.03350716, dtype=float32), 'training/policy_loss': Array(0.00268498, dtype=float32), 'training/total_loss': Array(1329.7567, dtype=float32), 'training/v_loss': Array(1329.7875, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2877044, dtype=float32), Array(0.1027195, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1118.1223, dtype=float32), Array(3120.333, dtype=float32)), 'eval/episode_reward': (Array(-22768.389, dtype=float32), Array(8933.069, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28521776, dtype=float32), Array(0.10430395, dtype=float32)), 'eval/avg_episode_length': (Array(891.2656, dtype=float32), Array(310.2821, dtype=float32)), 'eval/epoch_eval_time': 4.08134388923645, 'eval/sps': 31362.21878719135}
I0728 00:52:37.271267 140567846262592 train.py:379] starting iteration 130, 53248000 steps, 1855.6121430397034
I0728 00:52:51.355851 140567846262592 train.py:394] {'eval/walltime': 548.8044657707214, 'training/sps': 40948.63499021718, 'training/walltime': 1310.7629308700562, 'training/entropy_loss': Array(-0.03767977, dtype=float32), 'training/policy_loss': Array(0.00629965, dtype=float32), 'training/total_loss': Array(25895.5, dtype=float32), 'training/v_loss': Array(25895.531, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2742647, dtype=float32), Array(0.10534468, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1429.4348, dtype=float32), Array(3478.067, dtype=float32)), 'eval/episode_reward': (Array(-20450.691, dtype=float32), Array(9010.732, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2718472, dtype=float32), Array(0.10668448, dtype=float32)), 'eval/avg_episode_length': (Array(860.1953, dtype=float32), Array(345.60703, dtype=float32)), 'eval/epoch_eval_time': 4.07781982421875, 'eval/sps': 31389.322117615364}
I0728 00:52:51.358648 140567846262592 train.py:379] starting iteration 131, 53657600 steps, 1869.6995255947113
I0728 00:53:05.490675 140567846262592 train.py:394] {'eval/walltime': 552.9205586910248, 'training/sps': 40910.59092298044, 'training/walltime': 1320.7750082015991, 'training/entropy_loss': Array(-0.03874856, dtype=float32), 'training/policy_loss': Array(0.00346374, dtype=float32), 'training/total_loss': Array(3120.5625, dtype=float32), 'training/v_loss': Array(3120.5977, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28264016, dtype=float32), Array(0.10833405, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(258.3165, dtype=float32), Array(1511.6416, dtype=float32)), 'eval/episode_reward': (Array(-23270.89, dtype=float32), Array(8453.083, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27978873, dtype=float32), Array(0.11047852, dtype=float32)), 'eval/avg_episode_length': (Array(976.6953, dtype=float32), Array(150.43117, dtype=float32)), 'eval/epoch_eval_time': 4.116092920303345, 'eval/sps': 31097.45151005161}
I0728 00:53:05.493435 140567846262592 train.py:379] starting iteration 132, 54067200 steps, 1883.8343119621277
I0728 00:53:19.620497 140567846262592 train.py:394] {'eval/walltime': 557.0042848587036, 'training/sps': 40798.490928797306, 'training/walltime': 1330.8145952224731, 'training/entropy_loss': Array(-0.03893049, dtype=float32), 'training/policy_loss': Array(0.00449252, dtype=float32), 'training/total_loss': Array(2254.5437, dtype=float32), 'training/v_loss': Array(2254.578, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29534984, dtype=float32), Array(0.11000168, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(571.8687, dtype=float32), Array(2275.0415, dtype=float32)), 'eval/episode_reward': (Array(-23590.04, dtype=float32), Array(9111.634, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2930333, dtype=float32), Array(0.11179169, dtype=float32)), 'eval/avg_episode_length': (Array(945.6172, dtype=float32), Array(226.10262, dtype=float32)), 'eval/epoch_eval_time': 4.083726167678833, 'eval/sps': 31343.92335437968}
I0728 00:53:19.623292 140567846262592 train.py:379] starting iteration 133, 54476800 steps, 1897.9641697406769
I0728 00:53:33.747236 140567846262592 train.py:394] {'eval/walltime': 561.0786364078522, 'training/sps': 40774.127589007716, 'training/walltime': 1340.860181093216, 'training/entropy_loss': Array(-0.03778742, dtype=float32), 'training/policy_loss': Array(0.00450098, dtype=float32), 'training/total_loss': Array(1213.8507, dtype=float32), 'training/v_loss': Array(1213.884, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28572133, dtype=float32), Array(0.10557477, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(492.97473, dtype=float32), Array(2114.7046, dtype=float32)), 'eval/episode_reward': (Array(-22599.346, dtype=float32), Array(8925.443, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2828819, dtype=float32), Array(0.10812341, dtype=float32)), 'eval/avg_episode_length': (Array(953.4375, dtype=float32), Array(209.96259, dtype=float32)), 'eval/epoch_eval_time': 4.07435154914856, 'eval/sps': 31416.042149516747}
I0728 00:53:33.750032 140567846262592 train.py:379] starting iteration 134, 54886400 steps, 1912.09090924263
I0728 00:53:47.882838 140567846262592 train.py:394] {'eval/walltime': 565.162130355835, 'training/sps': 40774.333714308064, 'training/walltime': 1350.9057161808014, 'training/entropy_loss': Array(-0.03718074, dtype=float32), 'training/policy_loss': Array(0.0055964, dtype=float32), 'training/total_loss': Array(1054.156, dtype=float32), 'training/v_loss': Array(1054.1875, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.27270988, dtype=float32), Array(0.1103703, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(648.08215, dtype=float32), Array(2421.1682, dtype=float32)), 'eval/episode_reward': (Array(-21915.602, dtype=float32), Array(9404.023, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27009213, dtype=float32), Array(0.11188464, dtype=float32)), 'eval/avg_episode_length': (Array(937.9219, dtype=float32), Array(240.42809, dtype=float32)), 'eval/epoch_eval_time': 4.083493947982788, 'eval/sps': 31345.705817252634}
I0728 00:53:47.885718 140567846262592 train.py:379] starting iteration 135, 55296000 steps, 1926.2265949249268
I0728 00:54:02.027230 140567846262592 train.py:394] {'eval/walltime': 569.2408540248871, 'training/sps': 40721.093051582306, 'training/walltime': 1360.9643852710724, 'training/entropy_loss': Array(-0.03926738, dtype=float32), 'training/policy_loss': Array(0.00281074, dtype=float32), 'training/total_loss': Array(23594.95, dtype=float32), 'training/v_loss': Array(23594.984, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.27093446, dtype=float32), Array(0.1129169, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1038.5767, dtype=float32), Array(3020.464, dtype=float32)), 'eval/episode_reward': (Array(-21572.277, dtype=float32), Array(9376.946, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2679968, dtype=float32), Array(0.11539151, dtype=float32)), 'eval/avg_episode_length': (Array(899.0781, dtype=float32), Array(300.16724, dtype=float32)), 'eval/epoch_eval_time': 4.078723669052124, 'eval/sps': 31382.366246386726}
I0728 00:54:02.030086 140567846262592 train.py:379] starting iteration 136, 55705600 steps, 1940.3709635734558
I0728 00:54:16.173460 140567846262592 train.py:394] {'eval/walltime': 573.3161187171936, 'training/sps': 40698.89774795507, 'training/walltime': 1371.0285398960114, 'training/entropy_loss': Array(-0.03992119, dtype=float32), 'training/policy_loss': Array(0.00321517, dtype=float32), 'training/total_loss': Array(3044.3613, dtype=float32), 'training/v_loss': Array(3044.3982, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28724927, dtype=float32), Array(0.1101056, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(414.97174, dtype=float32), Array(1938.6606, dtype=float32)), 'eval/episode_reward': (Array(-23517.373, dtype=float32), Array(9009.922, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2848261, dtype=float32), Array(0.11182944, dtype=float32)), 'eval/avg_episode_length': (Array(961.1406, dtype=float32), Array(192.7363, dtype=float32)), 'eval/epoch_eval_time': 4.0752646923065186, 'eval/sps': 31409.002767757534}
I0728 00:54:16.176247 140567846262592 train.py:379] starting iteration 137, 56115200 steps, 1954.5171246528625
I0728 00:54:30.298913 140567846262592 train.py:394] {'eval/walltime': 577.3923482894897, 'training/sps': 40787.266678676606, 'training/walltime': 1381.07088971138, 'training/entropy_loss': Array(-0.04011087, dtype=float32), 'training/policy_loss': Array(0.00349607, dtype=float32), 'training/total_loss': Array(2188.3555, dtype=float32), 'training/v_loss': Array(2188.3918, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2807848, dtype=float32), Array(0.113997, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(258.17325, dtype=float32), Array(1513.364, dtype=float32)), 'eval/episode_reward': (Array(-22954.543, dtype=float32), Array(9479.443, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27788347, dtype=float32), Array(0.11651623, dtype=float32)), 'eval/avg_episode_length': (Array(976.7031, dtype=float32), Array(150.38074, dtype=float32)), 'eval/epoch_eval_time': 4.076229572296143, 'eval/sps': 31401.567975941434}
I0728 00:54:30.301717 140567846262592 train.py:379] starting iteration 138, 56524800 steps, 1968.6425943374634
I0728 00:54:44.427788 140567846262592 train.py:394] {'eval/walltime': 581.473471403122, 'training/sps': 40791.35155978246, 'training/walltime': 1391.112233877182, 'training/entropy_loss': Array(-0.03895675, dtype=float32), 'training/policy_loss': Array(0.00382504, dtype=float32), 'training/total_loss': Array(1459.3904, dtype=float32), 'training/v_loss': Array(1459.4255, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28876302, dtype=float32), Array(0.11021215, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(258.84317, dtype=float32), Array(1512.1237, dtype=float32)), 'eval/episode_reward': (Array(-24119.363, dtype=float32), Array(9261.403, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28617966, dtype=float32), Array(0.11235338, dtype=float32)), 'eval/avg_episode_length': (Array(976.71875, dtype=float32), Array(150.28009, dtype=float32)), 'eval/epoch_eval_time': 4.081123113632202, 'eval/sps': 31363.91538212625}
I0728 00:54:44.430556 140567846262592 train.py:379] starting iteration 139, 56934400 steps, 1982.771431684494
I0728 00:54:58.589303 140567846262592 train.py:394] {'eval/walltime': 585.5574152469635, 'training/sps': 40672.54912052637, 'training/walltime': 1401.182908296585, 'training/entropy_loss': Array(-0.03607506, dtype=float32), 'training/policy_loss': Array(0.0028422, dtype=float32), 'training/total_loss': Array(1170.2495, dtype=float32), 'training/v_loss': Array(1170.2827, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2946458, dtype=float32), Array(0.10116047, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(337.47974, dtype=float32), Array(1740.3531, dtype=float32)), 'eval/episode_reward': (Array(-24364.82, dtype=float32), Array(8154.3413, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2922787, dtype=float32), Array(0.10277101, dtype=float32)), 'eval/avg_episode_length': (Array(968.96094, dtype=float32), Array(172.81888, dtype=float32)), 'eval/epoch_eval_time': 4.083943843841553, 'eval/sps': 31342.25270825396}
I0728 00:54:58.592145 140567846262592 train.py:379] starting iteration 140, 57344000 steps, 1996.933022260666
I0728 00:55:12.751166 140567846262592 train.py:394] {'eval/walltime': 589.6379327774048, 'training/sps': 40656.9175716134, 'training/walltime': 1411.2574546337128, 'training/entropy_loss': Array(-0.0320473, dtype=float32), 'training/policy_loss': Array(0.00475077, dtype=float32), 'training/total_loss': Array(19649.516, dtype=float32), 'training/v_loss': Array(19649.543, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.25223032, dtype=float32), Array(0.1104005, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(958.8113, dtype=float32), Array(2914.1807, dtype=float32)), 'eval/episode_reward': (Array(-20561.523, dtype=float32), Array(9049.6455, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.24924047, dtype=float32), Array(0.11243742, dtype=float32)), 'eval/avg_episode_length': (Array(906.7656, dtype=float32), Array(289.8777, dtype=float32)), 'eval/epoch_eval_time': 4.080517530441284, 'eval/sps': 31368.570051494804}
I0728 00:55:12.753935 140567846262592 train.py:379] starting iteration 141, 57753600 steps, 2011.0948123931885
I0728 00:55:26.876843 140567846262592 train.py:394] {'eval/walltime': 593.7243113517761, 'training/sps': 40826.82697520766, 'training/walltime': 1421.290073633194, 'training/entropy_loss': Array(-0.03662786, dtype=float32), 'training/policy_loss': Array(0.00314855, dtype=float32), 'training/total_loss': Array(2731.7793, dtype=float32), 'training/v_loss': Array(2731.813, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2786925, dtype=float32), Array(0.11179818, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(726.68085, dtype=float32), Array(2555.94, dtype=float32)), 'eval/episode_reward': (Array(-22685.738, dtype=float32), Array(9402.433, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27581322, dtype=float32), Array(0.11379097, dtype=float32)), 'eval/avg_episode_length': (Array(930.1406, dtype=float32), Array(254.02567, dtype=float32)), 'eval/epoch_eval_time': 4.086378574371338, 'eval/sps': 31323.578486531183}
I0728 00:55:26.879604 140567846262592 train.py:379] starting iteration 142, 58163200 steps, 2025.22048163414
I0728 00:55:40.957491 140567846262592 train.py:394] {'eval/walltime': 597.8008396625519, 'training/sps': 40971.17492296979, 'training/walltime': 1431.287346124649, 'training/entropy_loss': Array(-0.03474436, dtype=float32), 'training/policy_loss': Array(0.00222342, dtype=float32), 'training/total_loss': Array(2488.156, dtype=float32), 'training/v_loss': Array(2488.1885, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28979385, dtype=float32), Array(0.11160699, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(571.44403, dtype=float32), Array(2274.0725, dtype=float32)), 'eval/episode_reward': (Array(-23706.295, dtype=float32), Array(9062.911, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2870767, dtype=float32), Array(0.11365499, dtype=float32)), 'eval/avg_episode_length': (Array(945.6328, dtype=float32), Array(226.03767, dtype=float32)), 'eval/epoch_eval_time': 4.076528310775757, 'eval/sps': 31399.266788274017}
I0728 00:55:40.960293 140567846262592 train.py:379] starting iteration 143, 58572800 steps, 2039.301170349121
I0728 00:55:55.085083 140567846262592 train.py:394] {'eval/walltime': 601.8767514228821, 'training/sps': 40777.371642929254, 'training/walltime': 1441.3321328163147, 'training/entropy_loss': Array(-0.03348299, dtype=float32), 'training/policy_loss': Array(0.00192685, dtype=float32), 'training/total_loss': Array(1518.8961, dtype=float32), 'training/v_loss': Array(1518.9275, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.27374813, dtype=float32), Array(0.09447663, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(804.4308, dtype=float32), Array(2685.614, dtype=float32)), 'eval/episode_reward': (Array(-21610.938, dtype=float32), Array(7914.5015, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27134204, dtype=float32), Array(0.09626024, dtype=float32)), 'eval/avg_episode_length': (Array(922.3672, dtype=float32), Array(266.67795, dtype=float32)), 'eval/epoch_eval_time': 4.0759117603302, 'eval/sps': 31404.016457321537}
I0728 00:55:55.087911 140567846262592 train.py:379] starting iteration 144, 58982400 steps, 2053.428787946701
I0728 00:56:09.216538 140567846262592 train.py:394] {'eval/walltime': 605.9557454586029, 'training/sps': 40773.40568302537, 'training/walltime': 1451.3778965473175, 'training/entropy_loss': Array(-0.03209153, dtype=float32), 'training/policy_loss': Array(0.00201074, dtype=float32), 'training/total_loss': Array(1288.6497, dtype=float32), 'training/v_loss': Array(1288.6798, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.295398, dtype=float32), Array(0.10693295, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(493.76178, dtype=float32), Array(2113.1528, dtype=float32)), 'eval/episode_reward': (Array(-24402.158, dtype=float32), Array(8883.771, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29294345, dtype=float32), Array(0.10868667, dtype=float32)), 'eval/avg_episode_length': (Array(953.4453, dtype=float32), Array(209.92725, dtype=float32)), 'eval/epoch_eval_time': 4.078994035720825, 'eval/sps': 31380.28613895247}
I0728 00:56:09.219307 140567846262592 train.py:379] starting iteration 145, 59392000 steps, 2067.5601840019226
I0728 00:56:23.336740 140567846262592 train.py:394] {'eval/walltime': 610.0694897174835, 'training/sps': 40961.28519657418, 'training/walltime': 1461.3775827884674, 'training/entropy_loss': Array(-0.03351886, dtype=float32), 'training/policy_loss': Array(0.00478545, dtype=float32), 'training/total_loss': Array(21429.424, dtype=float32), 'training/v_loss': Array(21429.455, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2850644, dtype=float32), Array(0.1132853, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(883.546, dtype=float32), Array(2803.2544, dtype=float32)), 'eval/episode_reward': (Array(-22544.164, dtype=float32), Array(9697.865, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28225368, dtype=float32), Array(0.11531159, dtype=float32)), 'eval/avg_episode_length': (Array(914.6094, dtype=float32), Array(278.48892, dtype=float32)), 'eval/epoch_eval_time': 4.113744258880615, 'eval/sps': 31115.205988723734}
I0728 00:56:23.339584 140567846262592 train.py:379] starting iteration 146, 59801600 steps, 2081.6804609298706
I0728 00:56:37.449322 140567846262592 train.py:394] {'eval/walltime': 614.1781389713287, 'training/sps': 40972.234121802845, 'training/walltime': 1471.3745968341827, 'training/entropy_loss': Array(-0.03506245, dtype=float32), 'training/policy_loss': Array(0.00471487, dtype=float32), 'training/total_loss': Array(2924.1309, dtype=float32), 'training/v_loss': Array(2924.1611, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2863469, dtype=float32), Array(0.10406251, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(414.90576, dtype=float32), Array(1936.5039, dtype=float32)), 'eval/episode_reward': (Array(-23639.91, dtype=float32), Array(8616.281, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28349197, dtype=float32), Array(0.10648653, dtype=float32)), 'eval/avg_episode_length': (Array(961.15625, dtype=float32), Array(192.65924, dtype=float32)), 'eval/epoch_eval_time': 4.108649253845215, 'eval/sps': 31153.79096431923}
I0728 00:56:37.452167 140567846262592 train.py:379] starting iteration 147, 60211200 steps, 2095.793044567108
I0728 00:56:51.597470 140567846262592 train.py:394] {'eval/walltime': 618.2644219398499, 'training/sps': 40735.46711191804, 'training/walltime': 1481.4297165870667, 'training/entropy_loss': Array(-0.03175561, dtype=float32), 'training/policy_loss': Array(0.00261062, dtype=float32), 'training/total_loss': Array(1830.4211, dtype=float32), 'training/v_loss': Array(1830.4504, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2824986, dtype=float32), Array(0.09574614, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(648.89825, dtype=float32), Array(2421.7068, dtype=float32)), 'eval/episode_reward': (Array(-22590.01, dtype=float32), Array(8255.602, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28006554, dtype=float32), Array(0.09711479, dtype=float32)), 'eval/avg_episode_length': (Array(937.875, dtype=float32), Array(240.60986, dtype=float32)), 'eval/epoch_eval_time': 4.086282968521118, 'eval/sps': 31324.31135730303}
I0728 00:56:51.600343 140567846262592 train.py:379] starting iteration 148, 60620800 steps, 2109.9412200450897
I0728 00:57:05.736785 140567846262592 train.py:394] {'eval/walltime': 622.3422336578369, 'training/sps': 40737.587340463586, 'training/walltime': 1491.4843130111694, 'training/entropy_loss': Array(-0.02979255, dtype=float32), 'training/policy_loss': Array(0.00216913, dtype=float32), 'training/total_loss': Array(1570.7699, dtype=float32), 'training/v_loss': Array(1570.7975, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28549537, dtype=float32), Array(0.09260806, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(414.81824, dtype=float32), Array(1937.8826, dtype=float32)), 'eval/episode_reward': (Array(-23294.766, dtype=float32), Array(8075.2954, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2830581, dtype=float32), Array(0.09431252, dtype=float32)), 'eval/avg_episode_length': (Array(961.21094, dtype=float32), Array(192.3879, dtype=float32)), 'eval/epoch_eval_time': 4.0778117179870605, 'eval/sps': 31389.38451606219}
I0728 00:57:05.739597 140567846262592 train.py:379] starting iteration 149, 61030400 steps, 2124.08047413826
I0728 00:57:19.850766 140567846262592 train.py:394] {'eval/walltime': 626.4465727806091, 'training/sps': 40948.109897841314, 'training/walltime': 1501.4872167110443, 'training/entropy_loss': Array(-0.02934524, dtype=float32), 'training/policy_loss': Array(0.00183352, dtype=float32), 'training/total_loss': Array(949.1891, dtype=float32), 'training/v_loss': Array(949.2167, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29010394, dtype=float32), Array(0.10798384, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(805.84106, dtype=float32), Array(2684.201, dtype=float32)), 'eval/episode_reward': (Array(-23108.66, dtype=float32), Array(9351.888, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28765333, dtype=float32), Array(0.10969692, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.75836, dtype=float32)), 'eval/epoch_eval_time': 4.104339122772217, 'eval/sps': 31186.506809297047}
I0728 00:57:19.853687 140567846262592 train.py:379] starting iteration 150, 61440000 steps, 2138.1945638656616
I0728 00:57:33.989281 140567846262592 train.py:394] {'eval/walltime': 630.5223498344421, 'training/sps': 40732.93085508628, 'training/walltime': 1511.542962551117, 'training/entropy_loss': Array(-0.02762629, dtype=float32), 'training/policy_loss': Array(0.00380853, dtype=float32), 'training/total_loss': Array(22098.844, dtype=float32), 'training/v_loss': Array(22098.867, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.27662113, dtype=float32), Array(0.10574125, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(335.94067, dtype=float32), Array(1739.649, dtype=float32)), 'eval/episode_reward': (Array(-22607.43, dtype=float32), Array(8910.733, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27420387, dtype=float32), Array(0.10729746, dtype=float32)), 'eval/avg_episode_length': (Array(968.9531, dtype=float32), Array(172.8618, dtype=float32)), 'eval/epoch_eval_time': 4.075777053833008, 'eval/sps': 31405.054375980693}
I0728 00:57:34.093897 140567846262592 train.py:379] starting iteration 151, 61849600 steps, 2152.4347655773163
I0728 00:57:48.161899 140567846262592 train.py:394] {'eval/walltime': 634.6008965969086, 'training/sps': 41021.270225425054, 'training/walltime': 1521.528026342392, 'training/entropy_loss': Array(-0.03529517, dtype=float32), 'training/policy_loss': Array(0.00082613, dtype=float32), 'training/total_loss': Array(2994.939, dtype=float32), 'training/v_loss': Array(2994.9736, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.268184, dtype=float32), Array(0.10456779, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(803.9525, dtype=float32), Array(2684.366, dtype=float32)), 'eval/episode_reward': (Array(-20787.33, dtype=float32), Array(8572.871, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.26540515, dtype=float32), Array(0.10690357, dtype=float32)), 'eval/avg_episode_length': (Array(922.3203, dtype=float32), Array(266.83902, dtype=float32)), 'eval/epoch_eval_time': 4.078546762466431, 'eval/sps': 31383.727453597763}
I0728 00:57:48.164835 140567846262592 train.py:379] starting iteration 152, 62259200 steps, 2166.505712032318
I0728 00:58:02.247997 140567846262592 train.py:394] {'eval/walltime': 638.6839504241943, 'training/sps': 40975.05045002062, 'training/walltime': 1531.5243532657623, 'training/entropy_loss': Array(-0.03739475, dtype=float32), 'training/policy_loss': Array(0.00101997, dtype=float32), 'training/total_loss': Array(2517.4048, dtype=float32), 'training/v_loss': Array(2517.441, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29645544, dtype=float32), Array(0.10664549, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(415.75568, dtype=float32), Array(1938.4677, dtype=float32)), 'eval/episode_reward': (Array(-23612.504, dtype=float32), Array(8469.738, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2939781, dtype=float32), Array(0.10872159, dtype=float32)), 'eval/avg_episode_length': (Array(961.16406, dtype=float32), Array(192.62004, dtype=float32)), 'eval/epoch_eval_time': 4.083053827285767, 'eval/sps': 31349.08463479374}
I0728 00:58:02.250797 140567846262592 train.py:379] starting iteration 153, 62668800 steps, 2180.591673374176
I0728 00:58:16.371679 140567846262592 train.py:394] {'eval/walltime': 642.7992265224457, 'training/sps': 40952.67318583521, 'training/walltime': 1541.52614235878, 'training/entropy_loss': Array(-0.0385268, dtype=float32), 'training/policy_loss': Array(0.00106627, dtype=float32), 'training/total_loss': Array(2226.1099, dtype=float32), 'training/v_loss': Array(2226.1475, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30189922, dtype=float32), Array(0.11245213, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(416.25754, dtype=float32), Array(1937.9156, dtype=float32)), 'eval/episode_reward': (Array(-24651.107, dtype=float32), Array(9259.611, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29972583, dtype=float32), Array(0.11452182, dtype=float32)), 'eval/avg_episode_length': (Array(961.15625, dtype=float32), Array(192.65927, dtype=float32)), 'eval/epoch_eval_time': 4.115276098251343, 'eval/sps': 31103.623898865393}
I0728 00:58:16.374554 140567846262592 train.py:379] starting iteration 154, 63078400 steps, 2194.7154307365417
I0728 00:58:30.504295 140567846262592 train.py:394] {'eval/walltime': 646.9072604179382, 'training/sps': 40887.68843287054, 'training/walltime': 1551.5438277721405, 'training/entropy_loss': Array(-0.03928001, dtype=float32), 'training/policy_loss': Array(0.00306441, dtype=float32), 'training/total_loss': Array(2349.6543, dtype=float32), 'training/v_loss': Array(2349.6907, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28851578, dtype=float32), Array(0.10193908, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(1040.0814, dtype=float32), Array(3021.2954, dtype=float32)), 'eval/episode_reward': (Array(-22207.066, dtype=float32), Array(8353.9, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28639892, dtype=float32), Array(0.10358018, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.30658, dtype=float32)), 'eval/epoch_eval_time': 4.108033895492554, 'eval/sps': 31158.457611667975}
I0728 00:58:30.507048 140567846262592 train.py:379] starting iteration 155, 63488000 steps, 2208.847925424576
I0728 00:58:44.645786 140567846262592 train.py:394] {'eval/walltime': 651.0271224975586, 'training/sps': 40899.74599350251, 'training/walltime': 1561.5585598945618, 'training/entropy_loss': Array(-0.04362252, dtype=float32), 'training/policy_loss': Array(0.00289493, dtype=float32), 'training/total_loss': Array(25276.955, dtype=float32), 'training/v_loss': Array(25276.996, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29677564, dtype=float32), Array(0.1133137, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(806.40704, dtype=float32), Array(2684.375, dtype=float32)), 'eval/episode_reward': (Array(-23083.309, dtype=float32), Array(9177.163, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29447234, dtype=float32), Array(0.11530156, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.75858, dtype=float32)), 'eval/epoch_eval_time': 4.119862079620361, 'eval/sps': 31069.001225350483}
I0728 00:58:44.648652 140567846262592 train.py:379] starting iteration 156, 63897600 steps, 2222.9895288944244
I0728 00:58:58.733779 140567846262592 train.py:394] {'eval/walltime': 655.1036655902863, 'training/sps': 40940.71123612554, 'training/walltime': 1571.5632712841034, 'training/entropy_loss': Array(-0.04261178, dtype=float32), 'training/policy_loss': Array(0.0028668, dtype=float32), 'training/total_loss': Array(3731.8933, dtype=float32), 'training/v_loss': Array(3731.933, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2819013, dtype=float32), Array(0.11322238, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(727.0137, dtype=float32), Array(2555.8945, dtype=float32)), 'eval/episode_reward': (Array(-22252.172, dtype=float32), Array(9047.571, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27911273, dtype=float32), Array(0.11567471, dtype=float32)), 'eval/avg_episode_length': (Array(930.1406, dtype=float32), Array(254.02574, dtype=float32)), 'eval/epoch_eval_time': 4.076543092727661, 'eval/sps': 31399.15293140045}
I0728 00:58:58.736581 140567846262592 train.py:379] starting iteration 157, 64307200 steps, 2237.077458381653
I0728 00:59:12.783266 140567846262592 train.py:394] {'eval/walltime': 659.1806583404541, 'training/sps': 41102.620852733715, 'training/walltime': 1581.5285725593567, 'training/entropy_loss': Array(-0.04101715, dtype=float32), 'training/policy_loss': Array(0.00392152, dtype=float32), 'training/total_loss': Array(2465.8394, dtype=float32), 'training/v_loss': Array(2465.8765, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2757929, dtype=float32), Array(0.11140778, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(414.0206, dtype=float32), Array(1936.0168, dtype=float32)), 'eval/episode_reward': (Array(-22974.18, dtype=float32), Array(9207.675, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2730459, dtype=float32), Array(0.11359224, dtype=float32)), 'eval/avg_episode_length': (Array(961.125, dtype=float32), Array(192.81383, dtype=float32)), 'eval/epoch_eval_time': 4.076992750167847, 'eval/sps': 31395.689873309275}
I0728 00:59:12.786116 140567846262592 train.py:379] starting iteration 158, 64716800 steps, 2251.1269936561584
I0728 00:59:26.899311 140567846262592 train.py:394] {'eval/walltime': 663.2627003192902, 'training/sps': 40850.060527743524, 'training/walltime': 1591.5554854869843, 'training/entropy_loss': Array(-0.04099923, dtype=float32), 'training/policy_loss': Array(0.00365843, dtype=float32), 'training/total_loss': Array(1994.2097, dtype=float32), 'training/v_loss': Array(1994.2471, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29407007, dtype=float32), Array(0.11366907, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(884.28613, dtype=float32), Array(2801.7766, dtype=float32)), 'eval/episode_reward': (Array(-23055.924, dtype=float32), Array(9314.824, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.291902, dtype=float32), Array(0.11526105, dtype=float32)), 'eval/avg_episode_length': (Array(914.6797, dtype=float32), Array(278.2593, dtype=float32)), 'eval/epoch_eval_time': 4.08204197883606, 'eval/sps': 31356.855383564063}
I0728 00:59:26.902133 140567846262592 train.py:379] starting iteration 159, 65126400 steps, 2265.2430098056793
I0728 00:59:40.986014 140567846262592 train.py:394] {'eval/walltime': 667.3347253799438, 'training/sps': 40927.815348300144, 'training/walltime': 1601.5633492469788, 'training/entropy_loss': Array(-0.04131836, dtype=float32), 'training/policy_loss': Array(0.00594849, dtype=float32), 'training/total_loss': Array(1809.8121, dtype=float32), 'training/v_loss': Array(1809.8478, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28987572, dtype=float32), Array(0.11086604, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(649.6079, dtype=float32), Array(2421.3704, dtype=float32)), 'eval/episode_reward': (Array(-22560.332, dtype=float32), Array(9269.165, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28744185, dtype=float32), Array(0.11291908, dtype=float32)), 'eval/avg_episode_length': (Array(937.8672, dtype=float32), Array(240.63956, dtype=float32)), 'eval/epoch_eval_time': 4.0720250606536865, 'eval/sps': 31433.99121896662}
I0728 00:59:40.988823 140567846262592 train.py:379] starting iteration 160, 65536000 steps, 2279.3296999931335
I0728 00:59:55.076398 140567846262592 train.py:394] {'eval/walltime': 671.4054250717163, 'training/sps': 40908.701046937356, 'training/walltime': 1611.5758891105652, 'training/entropy_loss': Array(-0.04238843, dtype=float32), 'training/policy_loss': Array(0.00719698, dtype=float32), 'training/total_loss': Array(21377.16, dtype=float32), 'training/v_loss': Array(21377.195, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29088053, dtype=float32), Array(0.10871049, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(649.6312, dtype=float32), Array(2420.8982, dtype=float32)), 'eval/episode_reward': (Array(-23465.115, dtype=float32), Array(8727.603, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2886049, dtype=float32), Array(0.11069478, dtype=float32)), 'eval/avg_episode_length': (Array(937.84375, dtype=float32), Array(240.73053, dtype=float32)), 'eval/epoch_eval_time': 4.070699691772461, 'eval/sps': 31444.225733160467}
I0728 00:59:55.079201 140567846262592 train.py:379] starting iteration 161, 65945600 steps, 2293.4200773239136
I0728 01:00:09.220145 140567846262592 train.py:394] {'eval/walltime': 675.515881061554, 'training/sps': 40852.94848770174, 'training/walltime': 1621.602093219757, 'training/entropy_loss': Array(-0.04342643, dtype=float32), 'training/policy_loss': Array(0.00634273, dtype=float32), 'training/total_loss': Array(3124.3616, dtype=float32), 'training/v_loss': Array(3124.3984, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29960877, dtype=float32), Array(0.10666288, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(962.9038, dtype=float32), Array(2915.2961, dtype=float32)), 'eval/episode_reward': (Array(-23351.996, dtype=float32), Array(8945.165, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2974151, dtype=float32), Array(0.10867271, dtype=float32)), 'eval/avg_episode_length': (Array(906.7578, dtype=float32), Array(289.90216, dtype=float32)), 'eval/epoch_eval_time': 4.1104559898376465, 'eval/sps': 31140.097428717563}
I0728 01:00:09.223007 140567846262592 train.py:379] starting iteration 162, 66355200 steps, 2307.5638840198517
I0728 01:00:23.307302 140567846262592 train.py:394] {'eval/walltime': 679.6199948787689, 'training/sps': 41058.23012873478, 'training/walltime': 1631.5781686306, 'training/entropy_loss': Array(-0.04471958, dtype=float32), 'training/policy_loss': Array(0.00462253, dtype=float32), 'training/total_loss': Array(2578.379, dtype=float32), 'training/v_loss': Array(2578.419, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28717738, dtype=float32), Array(0.11100744, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(727.46014, dtype=float32), Array(2557.112, dtype=float32)), 'eval/episode_reward': (Array(-22748.451, dtype=float32), Array(9033.915, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28533274, dtype=float32), Array(0.11224283, dtype=float32)), 'eval/avg_episode_length': (Array(930.0547, dtype=float32), Array(254.33806, dtype=float32)), 'eval/epoch_eval_time': 4.104113817214966, 'eval/sps': 31188.21887031882}
I0728 01:00:23.312020 140567846262592 train.py:379] starting iteration 163, 66764800 steps, 2321.6528816223145
I0728 01:00:37.358941 140567846262592 train.py:394] {'eval/walltime': 683.701064825058, 'training/sps': 41118.111895536415, 'training/walltime': 1641.5397155284882, 'training/entropy_loss': Array(-0.04588919, dtype=float32), 'training/policy_loss': Array(0.00341743, dtype=float32), 'training/total_loss': Array(2642.4905, dtype=float32), 'training/v_loss': Array(2642.5327, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2950572, dtype=float32), Array(0.10766266, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(415.63278, dtype=float32), Array(1937.5762, dtype=float32)), 'eval/episode_reward': (Array(-23322.922, dtype=float32), Array(8952.755, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2930708, dtype=float32), Array(0.1090885, dtype=float32)), 'eval/avg_episode_length': (Array(961.1719, dtype=float32), Array(192.58156, dtype=float32)), 'eval/epoch_eval_time': 4.0810699462890625, 'eval/sps': 31364.323984790077}
I0728 01:00:37.361780 140567846262592 train.py:379] starting iteration 164, 67174400 steps, 2335.7026574611664
I0728 01:00:51.460222 140567846262592 train.py:394] {'eval/walltime': 687.7780804634094, 'training/sps': 40890.11163894718, 'training/walltime': 1651.5568072795868, 'training/entropy_loss': Array(-0.04624591, dtype=float32), 'training/policy_loss': Array(0.00379221, dtype=float32), 'training/total_loss': Array(2034.8535, dtype=float32), 'training/v_loss': Array(2034.8961, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29840004, dtype=float32), Array(0.12809311, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(337.79312, dtype=float32), Array(1739.6353, dtype=float32)), 'eval/episode_reward': (Array(-23923.994, dtype=float32), Array(9701.186, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29653072, dtype=float32), Array(0.12920399, dtype=float32)), 'eval/avg_episode_length': (Array(968.97656, dtype=float32), Array(172.73152, dtype=float32)), 'eval/epoch_eval_time': 4.07701563835144, 'eval/sps': 31395.513619309386}
I0728 01:00:51.462996 140567846262592 train.py:379] starting iteration 165, 67584000 steps, 2349.803872823715
I0728 01:01:05.558402 140567846262592 train.py:394] {'eval/walltime': 691.8590683937073, 'training/sps': 40918.103445278146, 'training/walltime': 1661.567046403885, 'training/entropy_loss': Array(-0.04700629, dtype=float32), 'training/policy_loss': Array(0.00381195, dtype=float32), 'training/total_loss': Array(23242.746, dtype=float32), 'training/v_loss': Array(23242.791, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30864322, dtype=float32), Array(0.11460159, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(729.3152, dtype=float32), Array(2557.2466, dtype=float32)), 'eval/episode_reward': (Array(-24059.648, dtype=float32), Array(9534.788, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30670428, dtype=float32), Array(0.11607298, dtype=float32)), 'eval/avg_episode_length': (Array(930.15625, dtype=float32), Array(253.96895, dtype=float32)), 'eval/epoch_eval_time': 4.080987930297852, 'eval/sps': 31364.9543165049}
I0728 01:01:05.561288 140567846262592 train.py:379] starting iteration 166, 67993600 steps, 2363.9021644592285
I0728 01:01:19.655797 140567846262592 train.py:394] {'eval/walltime': 695.9687128067017, 'training/sps': 41039.617349236185, 'training/walltime': 1671.5476462841034, 'training/entropy_loss': Array(-0.04818964, dtype=float32), 'training/policy_loss': Array(0.00214691, dtype=float32), 'training/total_loss': Array(3452.3506, dtype=float32), 'training/v_loss': Array(3452.3967, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30833045, dtype=float32), Array(0.12660785, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(651.15704, dtype=float32), Array(2420.817, dtype=float32)), 'eval/episode_reward': (Array(-24090.361, dtype=float32), Array(9922.436, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30662024, dtype=float32), Array(0.12775086, dtype=float32)), 'eval/avg_episode_length': (Array(937.8594, dtype=float32), Array(240.66977, dtype=float32)), 'eval/epoch_eval_time': 4.109644412994385, 'eval/sps': 31146.247007471906}
I0728 01:01:19.658671 140567846262592 train.py:379] starting iteration 167, 68403200 steps, 2377.999547958374
I0728 01:01:33.783504 140567846262592 train.py:394] {'eval/walltime': 700.0817759037018, 'training/sps': 40929.71869228505, 'training/walltime': 1681.5550446510315, 'training/entropy_loss': Array(-0.0480703, dtype=float32), 'training/policy_loss': Array(0.00288879, dtype=float32), 'training/total_loss': Array(2355.5234, dtype=float32), 'training/v_loss': Array(2355.5686, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2963178, dtype=float32), Array(0.11981419, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(884.5338, dtype=float32), Array(2803.5806, dtype=float32)), 'eval/episode_reward': (Array(-22807.285, dtype=float32), Array(9726.944, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.294042, dtype=float32), Array(0.12180306, dtype=float32)), 'eval/avg_episode_length': (Array(914.60156, dtype=float32), Array(278.51422, dtype=float32)), 'eval/epoch_eval_time': 4.113063097000122, 'eval/sps': 31120.358959082656}
I0728 01:01:33.786350 140567846262592 train.py:379] starting iteration 168, 68812800 steps, 2392.127220392227
I0728 01:01:47.893265 140567846262592 train.py:394] {'eval/walltime': 704.1892490386963, 'training/sps': 40979.441840963205, 'training/walltime': 1691.550300359726, 'training/entropy_loss': Array(-0.04734096, dtype=float32), 'training/policy_loss': Array(0.00339948, dtype=float32), 'training/total_loss': Array(2282.959, dtype=float32), 'training/v_loss': Array(2283.0032, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3133584, dtype=float32), Array(0.13803929, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(104.68421, dtype=float32), Array(879.7835, dtype=float32)), 'eval/episode_reward': (Array(-24927.617, dtype=float32), Array(9872.992, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.31133407, dtype=float32), Array(0.13991219, dtype=float32)), 'eval/avg_episode_length': (Array(992.2578, dtype=float32), Array(87.25002, dtype=float32)), 'eval/epoch_eval_time': 4.107473134994507, 'eval/sps': 31162.71142700272}
I0728 01:01:47.895998 140567846262592 train.py:379] starting iteration 169, 69222400 steps, 2406.2368745803833
I0728 01:02:01.996303 140567846262592 train.py:394] {'eval/walltime': 708.2910256385803, 'training/sps': 40981.40571204041, 'training/walltime': 1701.545077085495, 'training/entropy_loss': Array(-0.0475026, dtype=float32), 'training/policy_loss': Array(0.00327288, dtype=float32), 'training/total_loss': Array(2123.83, dtype=float32), 'training/v_loss': Array(2123.8743, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.27630726, dtype=float32), Array(0.13244186, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(414.05554, dtype=float32), Array(1938.009, dtype=float32)), 'eval/episode_reward': (Array(-22000.629, dtype=float32), Array(9563.924, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.27412546, dtype=float32), Array(0.13423091, dtype=float32)), 'eval/avg_episode_length': (Array(961.1797, dtype=float32), Array(192.54266, dtype=float32)), 'eval/epoch_eval_time': 4.101776599884033, 'eval/sps': 31205.990107705733}
I0728 01:02:01.998843 140567846262592 train.py:379] starting iteration 170, 69632000 steps, 2420.3397200107574
I0728 01:02:16.124650 140567846262592 train.py:394] {'eval/walltime': 712.3904345035553, 'training/sps': 40868.57075215847, 'training/walltime': 1711.5674486160278, 'training/entropy_loss': Array(-0.04531071, dtype=float32), 'training/policy_loss': Array(0.00640028, dtype=float32), 'training/total_loss': Array(19501.54, dtype=float32), 'training/v_loss': Array(19501.578, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.28963333, dtype=float32), Array(0.11807178, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(337.03107, dtype=float32), Array(1739.1846, dtype=float32)), 'eval/episode_reward': (Array(-22783.42, dtype=float32), Array(9113.083, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28774813, dtype=float32), Array(0.11934468, dtype=float32)), 'eval/avg_episode_length': (Array(968.97656, dtype=float32), Array(172.73164, dtype=float32)), 'eval/epoch_eval_time': 4.099408864974976, 'eval/sps': 31224.014050811533}
I0728 01:02:16.127170 140567846262592 train.py:379] starting iteration 171, 70041600 steps, 2434.4680466651917
I0728 01:02:30.179621 140567846262592 train.py:394] {'eval/walltime': 716.4671630859375, 'training/sps': 41076.35891217688, 'training/walltime': 1721.5391211509705, 'training/entropy_loss': Array(-0.04872669, dtype=float32), 'training/policy_loss': Array(0.00252087, dtype=float32), 'training/total_loss': Array(2989.1562, dtype=float32), 'training/v_loss': Array(2989.2024, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29775566, dtype=float32), Array(0.11584059, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(650.2143, dtype=float32), Array(2421.5889, dtype=float32)), 'eval/episode_reward': (Array(-23213.992, dtype=float32), Array(9086.516, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29584295, dtype=float32), Array(0.11719827, dtype=float32)), 'eval/avg_episode_length': (Array(937.85156, dtype=float32), Array(240.70012, dtype=float32)), 'eval/epoch_eval_time': 4.076728582382202, 'eval/sps': 31397.724281464005}
I0728 01:02:30.182166 140567846262592 train.py:379] starting iteration 172, 70451200 steps, 2448.5230417251587
I0728 01:02:44.288913 140567846262592 train.py:394] {'eval/walltime': 720.5411794185638, 'training/sps': 40842.975924617844, 'training/walltime': 1731.5677733421326, 'training/entropy_loss': Array(-0.05029991, dtype=float32), 'training/policy_loss': Array(0.00250835, dtype=float32), 'training/total_loss': Array(2616.3167, dtype=float32), 'training/v_loss': Array(2616.3643, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2826196, dtype=float32), Array(0.12365799, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(336.4281, dtype=float32), Array(1740.4763, dtype=float32)), 'eval/episode_reward': (Array(-22767.236, dtype=float32), Array(9129.41, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.28043613, dtype=float32), Array(0.12509924, dtype=float32)), 'eval/avg_episode_length': (Array(968.9375, dtype=float32), Array(172.94876, dtype=float32)), 'eval/epoch_eval_time': 4.074016332626343, 'eval/sps': 31418.62711126735}
I0728 01:02:44.291517 140567846262592 train.py:379] starting iteration 173, 70860800 steps, 2462.632393836975
I0728 01:02:58.362226 140567846262592 train.py:394] {'eval/walltime': 724.6147046089172, 'training/sps': 40987.86366836491, 'training/walltime': 1741.5609753131866, 'training/entropy_loss': Array(-0.0503186, dtype=float32), 'training/policy_loss': Array(0.00159371, dtype=float32), 'training/total_loss': Array(2409.85, dtype=float32), 'training/v_loss': Array(2409.8987, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.293863, dtype=float32), Array(0.14209485, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(493.6577, dtype=float32), Array(2113.28, dtype=float32)), 'eval/episode_reward': (Array(-23371.895, dtype=float32), Array(10974.813, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2917011, dtype=float32), Array(0.14369693, dtype=float32)), 'eval/avg_episode_length': (Array(953.39844, dtype=float32), Array(210.13914, dtype=float32)), 'eval/epoch_eval_time': 4.0735251903533936, 'eval/sps': 31422.415234627657}
I0728 01:02:58.364867 140567846262592 train.py:379] starting iteration 174, 71270400 steps, 2476.705743789673
I0728 01:03:12.497786 140567846262592 train.py:394] {'eval/walltime': 728.7018799781799, 'training/sps': 40789.773873590006, 'training/walltime': 1751.602707862854, 'training/entropy_loss': Array(-0.0486256, dtype=float32), 'training/policy_loss': Array(0.00155781, dtype=float32), 'training/total_loss': Array(2012.2584, dtype=float32), 'training/v_loss': Array(2012.3055, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30684912, dtype=float32), Array(0.13124506, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(729.1419, dtype=float32), Array(2555.9697, dtype=float32)), 'eval/episode_reward': (Array(-24177.223, dtype=float32), Array(9856.836, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30493033, dtype=float32), Array(0.13262667, dtype=float32)), 'eval/avg_episode_length': (Array(930.0703, dtype=float32), Array(254.2812, dtype=float32)), 'eval/epoch_eval_time': 4.087175369262695, 'eval/sps': 31317.47195449813}
I0728 01:03:12.500450 140567846262592 train.py:379] starting iteration 175, 71680000 steps, 2490.841326236725
I0728 01:03:26.590487 140567846262592 train.py:394] {'eval/walltime': 732.8068709373474, 'training/sps': 41038.135094460806, 'training/walltime': 1761.583668231964, 'training/entropy_loss': Array(-0.04290485, dtype=float32), 'training/policy_loss': Array(0.00201158, dtype=float32), 'training/total_loss': Array(19073.414, dtype=float32), 'training/v_loss': Array(19073.453, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3030572, dtype=float32), Array(0.13329168, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(650.6723, dtype=float32), Array(2420.5552, dtype=float32)), 'eval/episode_reward': (Array(-23895.375, dtype=float32), Array(10069.052, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30113068, dtype=float32), Array(0.1349315, dtype=float32)), 'eval/avg_episode_length': (Array(937.85156, dtype=float32), Array(240.69997, dtype=float32)), 'eval/epoch_eval_time': 4.1049909591674805, 'eval/sps': 31181.554666799864}
I0728 01:03:26.923781 140567846262592 train.py:379] starting iteration 176, 72089600 steps, 2505.264641523361
I0728 01:03:40.973105 140567846262592 train.py:394] {'eval/walltime': 736.884646654129, 'training/sps': 41094.731765969605, 'training/walltime': 1771.5508825778961, 'training/entropy_loss': Array(-0.04872273, dtype=float32), 'training/policy_loss': Array(0.00051984, dtype=float32), 'training/total_loss': Array(2961.6353, dtype=float32), 'training/v_loss': Array(2961.6836, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29535997, dtype=float32), Array(0.13757506, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(415.58813, dtype=float32), Array(1938.3202, dtype=float32)), 'eval/episode_reward': (Array(-23136.887, dtype=float32), Array(9763.447, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29314646, dtype=float32), Array(0.13945498, dtype=float32)), 'eval/avg_episode_length': (Array(961.2031, dtype=float32), Array(192.42665, dtype=float32)), 'eval/epoch_eval_time': 4.077775716781616, 'eval/sps': 31389.66164157355}
I0728 01:03:40.976146 140567846262592 train.py:379] starting iteration 177, 72499200 steps, 2519.3170232772827
I0728 01:03:55.104057 140567846262592 train.py:394] {'eval/walltime': 740.9946119785309, 'training/sps': 40901.644780033064, 'training/walltime': 1781.5651497840881, 'training/entropy_loss': Array(-0.04912666, dtype=float32), 'training/policy_loss': Array(0.00084423, dtype=float32), 'training/total_loss': Array(2605.808, dtype=float32), 'training/v_loss': Array(2605.8564, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3064927, dtype=float32), Array(0.12418693, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(572.775, dtype=float32), Array(2272.864, dtype=float32)), 'eval/episode_reward': (Array(-24092.738, dtype=float32), Array(9660.964, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3047567, dtype=float32), Array(0.1254079, dtype=float32)), 'eval/avg_episode_length': (Array(945.65625, dtype=float32), Array(225.94028, dtype=float32)), 'eval/epoch_eval_time': 4.1099653244018555, 'eval/sps': 31143.815068227737}
I0728 01:03:55.106782 140567846262592 train.py:379] starting iteration 178, 72908800 steps, 2533.4476583004
I0728 01:04:09.214780 140567846262592 train.py:394] {'eval/walltime': 745.1050894260406, 'training/sps': 40985.553040790954, 'training/walltime': 1791.5589151382446, 'training/entropy_loss': Array(-0.04920677, dtype=float32), 'training/policy_loss': Array(0.00102562, dtype=float32), 'training/total_loss': Array(2347.9587, dtype=float32), 'training/v_loss': Array(2348.0068, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31583154, dtype=float32), Array(0.1266835, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(573.5803, dtype=float32), Array(2273.2388, dtype=float32)), 'eval/episode_reward': (Array(-24805.066, dtype=float32), Array(9653.783, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.31420475, dtype=float32), Array(0.12781681, dtype=float32)), 'eval/avg_episode_length': (Array(945.7344, dtype=float32), Array(225.616, dtype=float32)), 'eval/epoch_eval_time': 4.110477447509766, 'eval/sps': 31139.934869985904}
I0728 01:04:09.217513 140567846262592 train.py:379] starting iteration 179, 73318400 steps, 2547.5583901405334
I0728 01:04:23.265205 140567846262592 train.py:394] {'eval/walltime': 749.1885828971863, 'training/sps': 41124.485036976774, 'training/walltime': 1801.5189182758331, 'training/entropy_loss': Array(-0.0495716, dtype=float32), 'training/policy_loss': Array(0.00110312, dtype=float32), 'training/total_loss': Array(2189.3132, dtype=float32), 'training/v_loss': Array(2189.3618, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.30603606, dtype=float32), Array(0.11914604, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(572.7714, dtype=float32), Array(2273.7363, dtype=float32)), 'eval/episode_reward': (Array(-23629.168, dtype=float32), Array(9237.256, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.30424285, dtype=float32), Array(0.12041996, dtype=float32)), 'eval/avg_episode_length': (Array(945.6172, dtype=float32), Array(226.10262, dtype=float32)), 'eval/epoch_eval_time': 4.08349347114563, 'eval/sps': 31345.709477549237}
I0728 01:04:23.267919 140567846262592 train.py:379] starting iteration 180, 73728000 steps, 2561.608795642853
I0728 01:04:37.370119 140567846262592 train.py:394] {'eval/walltime': 753.2786753177643, 'training/sps': 40927.090915007604, 'training/walltime': 1811.526959180832, 'training/entropy_loss': Array(-0.04329072, dtype=float32), 'training/policy_loss': Array(0.00248224, dtype=float32), 'training/total_loss': Array(19627.152, dtype=float32), 'training/v_loss': Array(19627.193, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.31491828, dtype=float32), Array(0.12900697, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(495.41284, dtype=float32), Array(2114.2769, dtype=float32)), 'eval/episode_reward': (Array(-23943.387, dtype=float32), Array(10004.429, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.313317, dtype=float32), Array(0.12998949, dtype=float32)), 'eval/avg_episode_length': (Array(953.4531, dtype=float32), Array(209.89178, dtype=float32)), 'eval/epoch_eval_time': 4.090092420578003, 'eval/sps': 31295.136353400863}
I0728 01:04:37.372851 140567846262592 train.py:379] starting iteration 181, 74137600 steps, 2575.713727712631
I0728 01:04:51.464416 140567846262592 train.py:394] {'eval/walltime': 757.3520267009735, 'training/sps': 40902.900023408634, 'training/walltime': 1821.5409190654755, 'training/entropy_loss': Array(-0.04839499, dtype=float32), 'training/policy_loss': Array(0.0052802, dtype=float32), 'training/total_loss': Array(3041.4312, dtype=float32), 'training/v_loss': Array(3041.474, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.29681987, dtype=float32), Array(0.1357516, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(806.4015, dtype=float32), Array(2683.7273, dtype=float32)), 'eval/episode_reward': (Array(-22629.566, dtype=float32), Array(9922.26, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.29466912, dtype=float32), Array(0.137416, dtype=float32)), 'eval/avg_episode_length': (Array(922.39844, dtype=float32), Array(266.57062, dtype=float32)), 'eval/epoch_eval_time': 4.0733513832092285, 'eval/sps': 31423.75600779965}
I0728 01:04:51.467369 140567846262592 train.py:379] starting iteration 182, 74547200 steps, 2589.808245420456
I0728 01:05:05.597559 140567846262592 train.py:394] {'eval/walltime': 761.4358053207397, 'training/sps': 40788.214708802814, 'training/walltime': 1831.5830354690552, 'training/entropy_loss': Array(-0.04816869, dtype=float32), 'training/policy_loss': Array(0.00157314, dtype=float32), 'training/total_loss': Array(2388.7378, dtype=float32), 'training/v_loss': Array(2388.7847, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.2884013, dtype=float32), Array(0.12227257, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(727.5531, dtype=float32), Array(2555.7039, dtype=float32)), 'eval/episode_reward': (Array(-21869.367, dtype=float32), Array(8882.903, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.2863435, dtype=float32), Array(0.12349141, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.08257, dtype=float32)), 'eval/epoch_eval_time': 4.083778619766235, 'eval/sps': 31343.52077276094}
I0728 01:05:05.600337 140567846262592 train.py:379] starting iteration 183, 74956800 steps, 2603.9412145614624
I0728 01:05:19.683512 140567846262592 train.py:394] {'eval/walltime': 765.5043108463287, 'training/sps': 40916.425311903186, 'training/walltime': 1841.5936851501465, 'training/entropy_loss': Array(-0.04766819, dtype=float32), 'training/policy_loss': Array(0.00101743, dtype=float32), 'training/total_loss': Array(2431.741, dtype=float32), 'training/v_loss': Array(2431.7876, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.3091432, dtype=float32), Array(0.1340748, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(494.90912, dtype=float32), Array(2113.2273, dtype=float32)), 'eval/episode_reward': (Array(-23829.557, dtype=float32), Array(9161.102, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.3072307, dtype=float32), Array(0.1353446, dtype=float32)), 'eval/avg_episode_length': (Array(953.4297, dtype=float32), Array(209.99756, dtype=float32)), 'eval/epoch_eval_time': 4.068505525588989, 'eval/sps': 31461.183767587412}
I0728 01:05:19.686266 140567846262592 train.py:379] starting iteration 184, 75366400 steps, 2618.027143239975
I0728 01:05:33.769889 140567846262592 train.py:394] {'eval/walltime': 769.59943151474, 'training/sps': 41024.631118179015, 'training/walltime': 1851.5779309272766, 'training/entropy_loss': Array(-0.04772766, dtype=float32), 'training/policy_loss': Array(0.0008483, dtype=float32), 'training/total_loss': Array(1725.8994, dtype=float32), 'training/v_loss': Array(1725.9464, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': (Array(0.32344463, dtype=float32), Array(0.1302961, dtype=float32)), 'eval/episode_goal_distance_world_frame': (Array(574.26794, dtype=float32), Array(2274.6077, dtype=float32)), 'eval/episode_reward': (Array(-24773.324, dtype=float32), Array(9775.208, dtype=float32)), 'eval/episode_root_goal_distance_normalised': (Array(0.32179594, dtype=float32), Array(0.13147989, dtype=float32)), 'eval/avg_episode_length': (Array(945.7031, dtype=float32), Array(225.74539, dtype=float32)), 'eval/epoch_eval_time': 4.095120668411255, 'eval/sps': 31256.710208165598}
I0728 01:05:33.772478 140567846262592 train.py:379] starting iteration 185, 75776000 steps, 2632.1133546829224
