I0727 20:35:03.321859 139784370640704 low_level_env.py:187] Initialising environment...
I0727 20:35:42.792809 139784370640704 low_level_env.py:290] Environment initialised.
I0727 20:35:42.797334 139784370640704 train.py:118] JAX is running on GPU.
I0727 20:35:42.797375 139784370640704 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0727 20:35:51.561186 139784370640704 train.py:367] Running initial eval
I0727 20:36:08.825156 139784370640704 train.py:373] {'eval/walltime': 17.12107491493225, 'eval/episode_goal_distance': (Array(0.3322153, dtype=float32), Array(0.11334436, dtype=float32)), 'eval/episode_reward': (Array(-8278.025, dtype=float32), Array(3424.2102, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.30815756, dtype=float32), Array(0.12507045, dtype=float32)), 'eval/avg_episode_length': (Array(961.21094, dtype=float32), Array(192.38777, dtype=float32)), 'eval/epoch_eval_time': 17.12107491493225, 'eval/sps': 7476.166107325657}
I0727 20:36:08.827208 139784370640704 train.py:379] starting iteration 0, 0 steps, 26.029888153076172
I0727 20:37:34.719362 139784370640704 train.py:394] {'eval/walltime': 21.52213978767395, 'training/sps': 5026.86607789581, 'training/walltime': 81.48217868804932, 'training/entropy_loss': Array(-0.01252848, dtype=float32), 'training/policy_loss': Array(0.11785369, dtype=float32), 'training/total_loss': Array(151.72925, dtype=float32), 'training/v_loss': Array(151.62393, dtype=float32), 'eval/episode_goal_distance': (Array(0.36372608, dtype=float32), Array(0.11116397, dtype=float32)), 'eval/episode_reward': (Array(-7921.008, dtype=float32), Array(4039.4395, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.3459655, dtype=float32), Array(0.11913689, dtype=float32)), 'eval/avg_episode_length': (Array(860.3203, dtype=float32), Array(345.29834, dtype=float32)), 'eval/epoch_eval_time': 4.401064872741699, 'eval/sps': 29083.870313472744}
I0727 20:37:34.775645 139784370640704 train.py:379] starting iteration 1, 409600 steps, 111.97831964492798
I0727 20:38:15.860470 139784370640704 train.py:394] {'eval/walltime': 26.123695850372314, 'training/sps': 11228.977539690848, 'training/walltime': 117.95923089981079, 'training/entropy_loss': Array(0.002541, dtype=float32), 'training/policy_loss': Array(0.01320432, dtype=float32), 'training/total_loss': Array(120.500854, dtype=float32), 'training/v_loss': Array(120.48511, dtype=float32), 'eval/episode_goal_distance': (Array(0.33204886, dtype=float32), Array(0.09835334, dtype=float32)), 'eval/episode_reward': (Array(-6939.3857, dtype=float32), Array(4101.8174, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.30948085, dtype=float32), Array(0.11054868, dtype=float32)), 'eval/avg_episode_length': (Array(813.64844, dtype=float32), Array(387.9228, dtype=float32)), 'eval/epoch_eval_time': 4.601556062698364, 'eval/sps': 27816.677283931745}
I0727 20:38:15.864765 139784370640704 train.py:379] starting iteration 2, 819200 steps, 153.06744980812073
I0727 20:38:58.193462 139784370640704 train.py:394] {'eval/walltime': 30.74263906478882, 'training/sps': 10863.686529036791, 'training/walltime': 155.6628224849701, 'training/entropy_loss': Array(0.02583116, dtype=float32), 'training/policy_loss': Array(0.00609135, dtype=float32), 'training/total_loss': Array(97.619736, dtype=float32), 'training/v_loss': Array(97.587814, dtype=float32), 'eval/episode_goal_distance': (Array(0.32561356, dtype=float32), Array(0.09382817, dtype=float32)), 'eval/episode_reward': (Array(-6744.8926, dtype=float32), Array(4143.233, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.30186635, dtype=float32), Array(0.10657736, dtype=float32)), 'eval/avg_episode_length': (Array(798.2031, dtype=float32), Array(399.69556, dtype=float32)), 'eval/epoch_eval_time': 4.618943214416504, 'eval/sps': 27711.96658155275}
I0727 20:38:58.197153 139784370640704 train.py:379] starting iteration 3, 1228800 steps, 195.39983820915222
I0727 20:39:42.514534 139784370640704 train.py:394] {'eval/walltime': 35.36312794685364, 'training/sps': 10319.872025404138, 'training/walltime': 195.3532371520996, 'training/entropy_loss': Array(0.05114984, dtype=float32), 'training/policy_loss': Array(0.00431196, dtype=float32), 'training/total_loss': Array(61.901398, dtype=float32), 'training/v_loss': Array(61.845936, dtype=float32), 'eval/episode_goal_distance': (Array(0.3298486, dtype=float32), Array(0.09565772, dtype=float32)), 'eval/episode_reward': (Array(-7056.346, dtype=float32), Array(4095.5486, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.3069117, dtype=float32), Array(0.1065156, dtype=float32)), 'eval/avg_episode_length': (Array(821.33594, dtype=float32), Array(381.7408, dtype=float32)), 'eval/epoch_eval_time': 4.620488882064819, 'eval/sps': 27702.69624429849}
I0727 20:39:42.518295 139784370640704 train.py:379] starting iteration 4, 1638400 steps, 239.7209792137146
I0727 20:40:27.596911 139784370640704 train.py:394] {'eval/walltime': 40.01042699813843, 'training/sps': 10132.335432633965, 'training/walltime': 235.77827072143555, 'training/entropy_loss': Array(0.07952657, dtype=float32), 'training/policy_loss': Array(0.00398228, dtype=float32), 'training/total_loss': Array(35.800625, dtype=float32), 'training/v_loss': Array(35.717117, dtype=float32), 'eval/episode_goal_distance': (Array(0.32325792, dtype=float32), Array(0.09371337, dtype=float32)), 'eval/episode_reward': (Array(-6587.575, dtype=float32), Array(4174.974, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.300998, dtype=float32), Array(0.1029726, dtype=float32)), 'eval/avg_episode_length': (Array(790.22656, dtype=float32), Array(405.72342, dtype=float32)), 'eval/epoch_eval_time': 4.64729905128479, 'eval/sps': 27542.879979848334}
I0727 20:40:27.600623 139784370640704 train.py:379] starting iteration 5, 2048000 steps, 284.80330777168274
I0727 20:41:12.828285 139784370640704 train.py:394] {'eval/walltime': 44.65890693664551, 'training/sps': 10095.396454761232, 'training/walltime': 276.3512191772461, 'training/entropy_loss': Array(0.11579894, dtype=float32), 'training/policy_loss': Array(0.00383503, dtype=float32), 'training/total_loss': Array(63.489006, dtype=float32), 'training/v_loss': Array(63.369377, dtype=float32), 'eval/episode_goal_distance': (Array(0.3290782, dtype=float32), Array(0.09708688, dtype=float32)), 'eval/episode_reward': (Array(-6806.294, dtype=float32), Array(4268.2856, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.3049448, dtype=float32), Array(0.10920619, dtype=float32)), 'eval/avg_episode_length': (Array(790.28906, dtype=float32), Array(405.6028, dtype=float32)), 'eval/epoch_eval_time': 4.64847993850708, 'eval/sps': 27535.883061401113}
I0727 20:41:12.832003 139784370640704 train.py:379] starting iteration 6, 2457600 steps, 330.03468775749207
I0727 20:41:58.168545 139784370640704 train.py:394] {'eval/walltime': 49.336652994155884, 'training/sps': 10075.703585932677, 'training/walltime': 317.0034670829773, 'training/entropy_loss': Array(0.15426455, dtype=float32), 'training/policy_loss': Array(0.00431057, dtype=float32), 'training/total_loss': Array(16.095406, dtype=float32), 'training/v_loss': Array(15.9368305, dtype=float32), 'eval/episode_goal_distance': (Array(0.34099832, dtype=float32), Array(0.09955539, dtype=float32)), 'eval/episode_reward': (Array(-6494.4756, dtype=float32), Array(4365.069, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.3199276, dtype=float32), Array(0.10934871, dtype=float32)), 'eval/avg_episode_length': (Array(759.14844, dtype=float32), Array(426.04495, dtype=float32)), 'eval/epoch_eval_time': 4.677746057510376, 'eval/sps': 27363.60598166483}
I0727 20:41:58.172367 139784370640704 train.py:379] starting iteration 7, 2867200 steps, 375.37505197525024
I0727 20:42:43.576023 139784370640704 train.py:394] {'eval/walltime': 54.02277898788452, 'training/sps': 10061.099911008367, 'training/walltime': 357.7147216796875, 'training/entropy_loss': Array(0.19815895, dtype=float32), 'training/policy_loss': Array(0.00435706, dtype=float32), 'training/total_loss': Array(35.058346, dtype=float32), 'training/v_loss': Array(34.855827, dtype=float32), 'eval/episode_goal_distance': (Array(0.32608274, dtype=float32), Array(0.09341262, dtype=float32)), 'eval/episode_reward': (Array(-7307.23, dtype=float32), Array(3881.0156, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.30195078, dtype=float32), Array(0.1051104, dtype=float32)), 'eval/avg_episode_length': (Array(844.6719, dtype=float32), Array(360.95084, dtype=float32)), 'eval/epoch_eval_time': 4.686125993728638, 'eval/sps': 27314.673180213296}
I0727 20:42:43.579663 139784370640704 train.py:379] starting iteration 8, 3276800 steps, 420.78234815597534
I0727 20:43:29.032034 139784370640704 train.py:394] {'eval/walltime': 58.71361589431763, 'training/sps': 10050.267682259993, 'training/walltime': 398.46985507011414, 'training/entropy_loss': Array(0.2368133, dtype=float32), 'training/policy_loss': Array(0.00397121, dtype=float32), 'training/total_loss': Array(35.321487, dtype=float32), 'training/v_loss': Array(35.080704, dtype=float32), 'eval/episode_goal_distance': (Array(0.3436438, dtype=float32), Array(0.09976998, dtype=float32)), 'eval/episode_reward': (Array(-6983.799, dtype=float32), Array(4357.314, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.32331198, dtype=float32), Array(0.10967786, dtype=float32)), 'eval/avg_episode_length': (Array(790.3594, dtype=float32), Array(405.4671, dtype=float32)), 'eval/epoch_eval_time': 4.6908369064331055, 'eval/sps': 27287.24160596125}
I0727 20:43:29.035691 139784370640704 train.py:379] starting iteration 9, 3686400 steps, 466.2383759021759
I0727 20:44:14.520691 139784370640704 train.py:394] {'eval/walltime': 63.371708154678345, 'training/sps': 10034.145841532985, 'training/walltime': 439.29046964645386, 'training/entropy_loss': Array(0.26735556, dtype=float32), 'training/policy_loss': Array(0.00375477, dtype=float32), 'training/total_loss': Array(30.88607, dtype=float32), 'training/v_loss': Array(30.614958, dtype=float32), 'eval/episode_goal_distance': (Array(0.32228464, dtype=float32), Array(0.08415013, dtype=float32)), 'eval/episode_reward': (Array(-6358.958, dtype=float32), Array(4035.1677, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.299663, dtype=float32), Array(0.09566892, dtype=float32)), 'eval/avg_episode_length': (Array(774.75, dtype=float32), Array(416.18295, dtype=float32)), 'eval/epoch_eval_time': 4.658092260360718, 'eval/sps': 27479.060706729713}
I0727 20:44:14.524403 139784370640704 train.py:379] starting iteration 10, 4096000 steps, 511.7270874977112
I0727 20:45:00.068079 139784370640704 train.py:394] {'eval/walltime': 68.05429530143738, 'training/sps': 10025.723403322967, 'training/walltime': 480.1453769207001, 'training/entropy_loss': Array(0.29319566, dtype=float32), 'training/policy_loss': Array(0.00348464, dtype=float32), 'training/total_loss': Array(87.36835, dtype=float32), 'training/v_loss': Array(87.07166, dtype=float32), 'eval/episode_goal_distance': (Array(0.3155219, dtype=float32), Array(0.09079268, dtype=float32)), 'eval/episode_reward': (Array(-6261.709, dtype=float32), Array(3846.3892, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.2913043, dtype=float32), Array(0.10064592, dtype=float32)), 'eval/avg_episode_length': (Array(790.2969, dtype=float32), Array(405.58725, dtype=float32)), 'eval/epoch_eval_time': 4.682587146759033, 'eval/sps': 27335.31613791595}
I0727 20:45:00.071601 139784370640704 train.py:379] starting iteration 11, 4505600 steps, 557.2742857933044
I0727 20:45:45.467519 139784370640704 train.py:394] {'eval/walltime': 72.72793984413147, 'training/sps': 10059.918622133866, 'training/walltime': 520.8614120483398, 'training/entropy_loss': Array(0.31456134, dtype=float32), 'training/policy_loss': Array(0.00299163, dtype=float32), 'training/total_loss': Array(29.38181, dtype=float32), 'training/v_loss': Array(29.064259, dtype=float32), 'eval/episode_goal_distance': (Array(0.32828686, dtype=float32), Array(0.0933345, dtype=float32)), 'eval/episode_reward': (Array(-6571.427, dtype=float32), Array(4213.837, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.30594572, dtype=float32), Array(0.10457015, dtype=float32)), 'eval/avg_episode_length': (Array(774.71875, dtype=float32), Array(416.24054, dtype=float32)), 'eval/epoch_eval_time': 4.673644542694092, 'eval/sps': 27387.61983944445}
I0727 20:45:45.472877 139784370640704 train.py:379] starting iteration 12, 4915200 steps, 602.6755557060242
I0727 20:46:31.033483 139784370640704 train.py:394] {'eval/walltime': 77.43982768058777, 'training/sps': 10028.846629023297, 'training/walltime': 561.7035961151123, 'training/entropy_loss': Array(0.33139515, dtype=float32), 'training/policy_loss': Array(0.00335766, dtype=float32), 'training/total_loss': Array(26.8372, dtype=float32), 'training/v_loss': Array(26.502449, dtype=float32), 'eval/episode_goal_distance': (Array(0.34198207, dtype=float32), Array(0.08163641, dtype=float32)), 'eval/episode_reward': (Array(-6549.9834, dtype=float32), Array(4293.3667, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.32089686, dtype=float32), Array(0.09122261, dtype=float32)), 'eval/avg_episode_length': (Array(743.625, dtype=float32), Array(434.99207, dtype=float32)), 'eval/epoch_eval_time': 4.711887836456299, 'eval/sps': 27165.33254668172}
I0727 20:46:31.037189 139784370640704 train.py:379] starting iteration 13, 5324800 steps, 648.2398734092712
I0727 20:47:16.762777 139784370640704 train.py:394] {'eval/walltime': 82.1538770198822, 'training/sps': 9988.95623752621, 'training/walltime': 602.7088813781738, 'training/entropy_loss': Array(0.34266955, dtype=float32), 'training/policy_loss': Array(0.00342503, dtype=float32), 'training/total_loss': Array(24.637383, dtype=float32), 'training/v_loss': Array(24.291286, dtype=float32), 'eval/episode_goal_distance': (Array(0.34127414, dtype=float32), Array(0.08452206, dtype=float32)), 'eval/episode_reward': (Array(-6092.607, dtype=float32), Array(4462.4697, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.32039136, dtype=float32), Array(0.0931381, dtype=float32)), 'eval/avg_episode_length': (Array(704.8125, dtype=float32), Array(454.28525, dtype=float32)), 'eval/epoch_eval_time': 4.714049339294434, 'eval/sps': 27152.876600811767}
I0727 20:47:16.766504 139784370640704 train.py:379] starting iteration 14, 5734400 steps, 693.9691872596741
I0727 20:48:02.458382 139784370640704 train.py:394] {'eval/walltime': 86.85743451118469, 'training/sps': 9994.554836636362, 'training/walltime': 643.6911969184875, 'training/entropy_loss': Array(0.34210753, dtype=float32), 'training/policy_loss': Array(0.00343335, dtype=float32), 'training/total_loss': Array(25.444138, dtype=float32), 'training/v_loss': Array(25.098597, dtype=float32), 'eval/episode_goal_distance': (Array(0.33760792, dtype=float32), Array(0.09979875, dtype=float32)), 'eval/episode_reward': (Array(-6984.7764, dtype=float32), Array(4394.379, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.31639937, dtype=float32), Array(0.10872022, dtype=float32)), 'eval/avg_episode_length': (Array(782.5156, dtype=float32), Array(411.00772, dtype=float32)), 'eval/epoch_eval_time': 4.70355749130249, 'eval/sps': 27213.444342221646}
I0727 20:48:02.462156 139784370640704 train.py:379] starting iteration 15, 6144000 steps, 739.6648406982422
I0727 20:48:48.191729 139784370640704 train.py:394] {'eval/walltime': 91.5452983379364, 'training/sps': 9981.591541448932, 'training/walltime': 684.7267370223999, 'training/entropy_loss': Array(0.32811713, dtype=float32), 'training/policy_loss': Array(0.00441826, dtype=float32), 'training/total_loss': Array(75.438126, dtype=float32), 'training/v_loss': Array(75.10559, dtype=float32), 'eval/episode_goal_distance': (Array(0.33462498, dtype=float32), Array(0.09363273, dtype=float32)), 'eval/episode_reward': (Array(-6472.8115, dtype=float32), Array(4320.116, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.31414196, dtype=float32), Array(0.10221956, dtype=float32)), 'eval/avg_episode_length': (Array(759.1875, dtype=float32), Array(425.9761, dtype=float32)), 'eval/epoch_eval_time': 4.687863826751709, 'eval/sps': 27304.547386713046}
I0727 20:48:48.195472 139784370640704 train.py:379] starting iteration 16, 6553600 steps, 785.398157119751
I0727 20:49:33.850988 139784370640704 train.py:394] {'eval/walltime': 96.22499799728394, 'training/sps': 9997.69200766078, 'training/walltime': 725.696192741394, 'training/entropy_loss': Array(0.3118058, dtype=float32), 'training/policy_loss': Array(0.00466088, dtype=float32), 'training/total_loss': Array(29.745667, dtype=float32), 'training/v_loss': Array(29.4292, dtype=float32), 'eval/episode_goal_distance': (Array(0.33112603, dtype=float32), Array(0.08594766, dtype=float32)), 'eval/episode_reward': (Array(-6944.9062, dtype=float32), Array(4006.3333, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.30977455, dtype=float32), Array(0.09548989, dtype=float32)), 'eval/avg_episode_length': (Array(805.8281, dtype=float32), Array(394.12662, dtype=float32)), 'eval/epoch_eval_time': 4.679699659347534, 'eval/sps': 27352.18268640906}
I0727 20:49:33.854775 139784370640704 train.py:379] starting iteration 17, 6963200 steps, 831.0574595928192
I0727 20:50:19.548381 139784370640704 train.py:394] {'eval/walltime': 100.93579435348511, 'training/sps': 9995.951599466547, 'training/walltime': 766.6727817058563, 'training/entropy_loss': Array(0.28249773, dtype=float32), 'training/policy_loss': Array(0.0047308, dtype=float32), 'training/total_loss': Array(27.934711, dtype=float32), 'training/v_loss': Array(27.64748, dtype=float32), 'eval/episode_goal_distance': (Array(0.33698672, dtype=float32), Array(0.09697718, dtype=float32)), 'eval/episode_reward': (Array(-6649.202, dtype=float32), Array(4152.2046, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.31564417, dtype=float32), Array(0.10473204, dtype=float32)), 'eval/avg_episode_length': (Array(774.7656, dtype=float32), Array(416.15445, dtype=float32)), 'eval/epoch_eval_time': 4.710796356201172, 'eval/sps': 27171.62668929725}
I0727 20:50:19.552122 139784370640704 train.py:379] starting iteration 18, 7372800 steps, 876.7548065185547
I0727 20:51:05.165819 139784370640704 train.py:394] {'eval/walltime': 105.61355638504028, 'training/sps': 10008.23669238238, 'training/walltime': 807.5990719795227, 'training/entropy_loss': Array(0.24692367, dtype=float32), 'training/policy_loss': Array(0.00457253, dtype=float32), 'training/total_loss': Array(27.926758, dtype=float32), 'training/v_loss': Array(27.675262, dtype=float32), 'eval/episode_goal_distance': (Array(0.3702884, dtype=float32), Array(0.15028995, dtype=float32)), 'eval/episode_reward': (Array(-7253.6016, dtype=float32), Array(4884.0557, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.34901464, dtype=float32), Array(0.16486509, dtype=float32)), 'eval/avg_episode_length': (Array(782.4453, dtype=float32), Array(411.1407, dtype=float32)), 'eval/epoch_eval_time': 4.677762031555176, 'eval/sps': 27363.512537948605}
I0727 20:51:05.169558 139784370640704 train.py:379] starting iteration 19, 7782400 steps, 922.3722431659698
I0727 20:51:50.827243 139784370640704 train.py:394] {'eval/walltime': 110.30479550361633, 'training/sps': 9999.986602942712, 'training/walltime': 848.5591268539429, 'training/entropy_loss': Array(0.20716345, dtype=float32), 'training/policy_loss': Array(0.00415702, dtype=float32), 'training/total_loss': Array(34.522602, dtype=float32), 'training/v_loss': Array(34.31128, dtype=float32), 'eval/episode_goal_distance': (Array(0.4663613, dtype=float32), Array(0.22243144, dtype=float32)), 'eval/episode_reward': (Array(-8740.099, dtype=float32), Array(5573.56, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.45266715, dtype=float32), Array(0.23308322, dtype=float32)), 'eval/avg_episode_length': (Array(805.78125, dtype=float32), Array(394.222, dtype=float32)), 'eval/epoch_eval_time': 4.69123911857605, 'eval/sps': 27284.902083365203}
I0727 20:51:50.834910 139784370640704 train.py:379] starting iteration 20, 8192000 steps, 968.037594795227
I0727 20:52:36.587143 139784370640704 train.py:394] {'eval/walltime': 115.00730991363525, 'training/sps': 9979.716034370358, 'training/walltime': 889.6023788452148, 'training/entropy_loss': Array(0.16620865, dtype=float32), 'training/policy_loss': Array(0.00404204, dtype=float32), 'training/total_loss': Array(78.78201, dtype=float32), 'training/v_loss': Array(78.611755, dtype=float32), 'eval/episode_goal_distance': (Array(0.6735436, dtype=float32), Array(0.34469932, dtype=float32)), 'eval/episode_reward': (Array(-11114.727, dtype=float32), Array(6537.033, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.66888356, dtype=float32), Array(0.35579494, dtype=float32)), 'eval/avg_episode_length': (Array(844.6172, dtype=float32), Array(361.07767, dtype=float32)), 'eval/epoch_eval_time': 4.702514410018921, 'eval/sps': 27219.480652157104}
I0727 20:52:36.590936 139784370640704 train.py:379] starting iteration 21, 8601600 steps, 1013.7936208248138
I0727 20:53:22.216466 139784370640704 train.py:394] {'eval/walltime': 119.70899891853333, 'training/sps': 10010.364992631692, 'training/walltime': 930.5199677944183, 'training/entropy_loss': Array(0.1289849, dtype=float32), 'training/policy_loss': Array(0.00302143, dtype=float32), 'training/total_loss': Array(69.140434, dtype=float32), 'training/v_loss': Array(69.00842, dtype=float32), 'eval/episode_goal_distance': (Array(0.8066324, dtype=float32), Array(0.47683516, dtype=float32)), 'eval/episode_reward': (Array(-13828.664, dtype=float32), Array(8123.7954, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.8061935, dtype=float32), Array(0.48985317, dtype=float32)), 'eval/avg_episode_length': (Array(867.9375, dtype=float32), Array(337.4563, dtype=float32)), 'eval/epoch_eval_time': 4.701689004898071, 'eval/sps': 27224.259168705892}
I0727 20:53:22.220155 139784370640704 train.py:379] starting iteration 22, 9011200 steps, 1059.4228394031525
I0727 20:54:07.910204 139784370640704 train.py:394] {'eval/walltime': 124.40311884880066, 'training/sps': 9992.746934508137, 'training/walltime': 971.5096979141235, 'training/entropy_loss': Array(0.09576619, dtype=float32), 'training/policy_loss': Array(0.00251849, dtype=float32), 'training/total_loss': Array(95.828445, dtype=float32), 'training/v_loss': Array(95.73016, dtype=float32), 'eval/episode_goal_distance': (Array(0.72035575, dtype=float32), Array(0.41702348, dtype=float32)), 'eval/episode_reward': (Array(-13596.941, dtype=float32), Array(7806.7686, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.71897876, dtype=float32), Array(0.42833146, dtype=float32)), 'eval/avg_episode_length': (Array(867.97656, dtype=float32), Array(337.3567, dtype=float32)), 'eval/epoch_eval_time': 4.694119930267334, 'eval/sps': 27268.15716289343}
I0727 20:54:07.913904 139784370640704 train.py:379] starting iteration 23, 9420800 steps, 1105.116589307785
I0727 20:54:53.486055 139784370640704 train.py:394] {'eval/walltime': 129.1112003326416, 'training/sps': 10025.066231588706, 'training/walltime': 1012.3672833442688, 'training/entropy_loss': Array(0.06417195, dtype=float32), 'training/policy_loss': Array(0.00189432, dtype=float32), 'training/total_loss': Array(135.21588, dtype=float32), 'training/v_loss': Array(135.14981, dtype=float32), 'eval/episode_goal_distance': (Array(0.62959343, dtype=float32), Array(0.42164227, dtype=float32)), 'eval/episode_reward': (Array(-11505.357, dtype=float32), Array(8613.358, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.6253346, dtype=float32), Array(0.43335217, dtype=float32)), 'eval/avg_episode_length': (Array(782.53125, dtype=float32), Array(410.97787, dtype=float32)), 'eval/epoch_eval_time': 4.708081483840942, 'eval/sps': 27187.29496065883}
I0727 20:54:53.489797 139784370640704 train.py:379] starting iteration 24, 9830400 steps, 1150.6924817562103
I0727 20:55:39.062705 139784370640704 train.py:394] {'eval/walltime': 133.8027994632721, 'training/sps': 10021.108116557994, 'training/walltime': 1053.2410066127777, 'training/entropy_loss': Array(0.03783867, dtype=float32), 'training/policy_loss': Array(0.00151777, dtype=float32), 'training/total_loss': Array(170.92366, dtype=float32), 'training/v_loss': Array(170.88431, dtype=float32), 'eval/episode_goal_distance': (Array(0.6535224, dtype=float32), Array(0.40777484, dtype=float32)), 'eval/episode_reward': (Array(-13641.885, dtype=float32), Array(8243.5205, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.65162754, dtype=float32), Array(0.41848052, dtype=float32)), 'eval/avg_episode_length': (Array(868.16406, dtype=float32), Array(336.87738, dtype=float32)), 'eval/epoch_eval_time': 4.691599130630493, 'eval/sps': 27282.80836363749}
I0727 20:55:39.066380 139784370640704 train.py:379] starting iteration 25, 10240000 steps, 1196.2690641880035
I0727 20:56:24.744172 139784370640704 train.py:394] {'eval/walltime': 138.51032328605652, 'training/sps': 9998.948521657008, 'training/walltime': 1094.2053139209747, 'training/entropy_loss': Array(0.01450993, dtype=float32), 'training/policy_loss': Array(0.0014568, dtype=float32), 'training/total_loss': Array(141.46301, dtype=float32), 'training/v_loss': Array(141.44705, dtype=float32), 'eval/episode_goal_distance': (Array(0.60305893, dtype=float32), Array(0.34517533, dtype=float32)), 'eval/episode_reward': (Array(-12632.025, dtype=float32), Array(7895.4385, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.5994448, dtype=float32), Array(0.35567665, dtype=float32)), 'eval/avg_episode_length': (Array(860.2031, dtype=float32), Array(345.58807, dtype=float32)), 'eval/epoch_eval_time': 4.707523822784424, 'eval/sps': 27190.515612577416}
I0727 20:56:24.807778 139784370640704 train.py:379] starting iteration 26, 10649600 steps, 1242.010451555252
I0727 20:57:10.546403 139784370640704 train.py:394] {'eval/walltime': 143.21802830696106, 'training/sps': 9984.241100220604, 'training/walltime': 1135.2299642562866, 'training/entropy_loss': Array(-0.0031925, dtype=float32), 'training/policy_loss': Array(0.00111178, dtype=float32), 'training/total_loss': Array(129.85913, dtype=float32), 'training/v_loss': Array(129.8612, dtype=float32), 'eval/episode_goal_distance': (Array(0.5693885, dtype=float32), Array(0.32728654, dtype=float32)), 'eval/episode_reward': (Array(-12480.105, dtype=float32), Array(7082.2144, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.5667336, dtype=float32), Array(0.33609992, dtype=float32)), 'eval/avg_episode_length': (Array(906.8047, dtype=float32), Array(289.7567, dtype=float32)), 'eval/epoch_eval_time': 4.707705020904541, 'eval/sps': 27189.469057984013}
I0727 20:57:10.550511 139784370640704 train.py:379] starting iteration 27, 11059200 steps, 1287.7531943321228
I0727 20:57:56.178884 139784370640704 train.py:394] {'eval/walltime': 147.89318370819092, 'training/sps': 10003.20284498379, 'training/walltime': 1176.176849603653, 'training/entropy_loss': Array(-0.01550373, dtype=float32), 'training/policy_loss': Array(0.00190183, dtype=float32), 'training/total_loss': Array(129.63005, dtype=float32), 'training/v_loss': Array(129.64365, dtype=float32), 'eval/episode_goal_distance': (Array(0.5778111, dtype=float32), Array(0.33146983, dtype=float32)), 'eval/episode_reward': (Array(-12666.873, dtype=float32), Array(7089.2866, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.5746356, dtype=float32), Array(0.34043276, dtype=float32)), 'eval/avg_episode_length': (Array(899.0078, dtype=float32), Array(300.37653, dtype=float32)), 'eval/epoch_eval_time': 4.675155401229858, 'eval/sps': 27378.769049329996}
I0727 20:57:56.182640 139784370640704 train.py:379] starting iteration 28, 11468800 steps, 1333.385324716568
I0727 20:58:41.893239 139784370640704 train.py:394] {'eval/walltime': 152.5929310321808, 'training/sps': 9989.108233015342, 'training/walltime': 1217.181510925293, 'training/entropy_loss': Array(-0.02461362, dtype=float32), 'training/policy_loss': Array(0.00169028, dtype=float32), 'training/total_loss': Array(130.6597, dtype=float32), 'training/v_loss': Array(130.68262, dtype=float32), 'eval/episode_goal_distance': (Array(0.53964347, dtype=float32), Array(0.2559763, dtype=float32)), 'eval/episode_reward': (Array(-11859.268, dtype=float32), Array(6692.6807, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.53538746, dtype=float32), Array(0.26505646, dtype=float32)), 'eval/avg_episode_length': (Array(868.03125, dtype=float32), Array(337.2166, dtype=float32)), 'eval/epoch_eval_time': 4.699747323989868, 'eval/sps': 27235.506757272626}
I0727 20:58:41.896994 139784370640704 train.py:379] starting iteration 29, 11878400 steps, 1379.099678516388
I0727 20:59:27.469120 139784370640704 train.py:394] {'eval/walltime': 157.29292488098145, 'training/sps': 10023.103946260258, 'training/walltime': 1258.047095298767, 'training/entropy_loss': Array(-0.03063445, dtype=float32), 'training/policy_loss': Array(0.00162259, dtype=float32), 'training/total_loss': Array(129.9584, dtype=float32), 'training/v_loss': Array(129.98743, dtype=float32), 'eval/episode_goal_distance': (Array(0.5854012, dtype=float32), Array(0.33292812, dtype=float32)), 'eval/episode_reward': (Array(-12168.8125, dtype=float32), Array(7194.199, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.582971, dtype=float32), Array(0.34211454, dtype=float32)), 'eval/avg_episode_length': (Array(867.9375, dtype=float32), Array(337.45602, dtype=float32)), 'eval/epoch_eval_time': 4.699993848800659, 'eval/sps': 27234.078196222097}
I0727 20:59:27.472891 139784370640704 train.py:379] starting iteration 30, 12288000 steps, 1424.6755752563477
I0727 21:00:13.147433 139784370640704 train.py:394] {'eval/walltime': 162.00428652763367, 'training/sps': 10000.764781420445, 'training/walltime': 1299.0039629936218, 'training/entropy_loss': Array(-0.03866929, dtype=float32), 'training/policy_loss': Array(0.00103091, dtype=float32), 'training/total_loss': Array(118.43222, dtype=float32), 'training/v_loss': Array(118.46985, dtype=float32), 'eval/episode_goal_distance': (Array(0.5547482, dtype=float32), Array(0.2614539, dtype=float32)), 'eval/episode_reward': (Array(-11954.092, dtype=float32), Array(6075.766, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.5512466, dtype=float32), Array(0.27028346, dtype=float32)), 'eval/avg_episode_length': (Array(891.3906, dtype=float32), Array(309.9255, dtype=float32)), 'eval/epoch_eval_time': 4.711361646652222, 'eval/sps': 27168.366514795922}
I0727 21:00:13.151208 139784370640704 train.py:379] starting iteration 31, 12697600 steps, 1470.3538928031921
I0727 21:00:58.838092 139784370640704 train.py:394] {'eval/walltime': 166.70728874206543, 'training/sps': 9995.712100097708, 'training/walltime': 1339.9815337657928, 'training/entropy_loss': Array(-0.03953993, dtype=float32), 'training/policy_loss': Array(0.00157413, dtype=float32), 'training/total_loss': Array(98.38881, dtype=float32), 'training/v_loss': Array(98.42677, dtype=float32), 'eval/episode_goal_distance': (Array(0.5291173, dtype=float32), Array(0.2624383, dtype=float32)), 'eval/episode_reward': (Array(-11238.721, dtype=float32), Array(6515.598, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.5250192, dtype=float32), Array(0.27074826, dtype=float32)), 'eval/avg_episode_length': (Array(867.9219, dtype=float32), Array(337.49622, dtype=float32)), 'eval/epoch_eval_time': 4.703002214431763, 'eval/sps': 27216.657395400678}
I0727 21:00:58.841781 139784370640704 train.py:379] starting iteration 32, 13107200 steps, 1516.044466495514
I0727 21:01:44.522030 139784370640704 train.py:394] {'eval/walltime': 171.41717052459717, 'training/sps': 9999.021906621147, 'training/walltime': 1380.9455404281616, 'training/entropy_loss': Array(-0.04192774, dtype=float32), 'training/policy_loss': Array(0.00141573, dtype=float32), 'training/total_loss': Array(93.7474, dtype=float32), 'training/v_loss': Array(93.7879, dtype=float32), 'eval/episode_goal_distance': (Array(0.5897942, dtype=float32), Array(0.35857335, dtype=float32)), 'eval/episode_reward': (Array(-12314.203, dtype=float32), Array(6608.5786, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.5858128, dtype=float32), Array(0.37008226, dtype=float32)), 'eval/avg_episode_length': (Array(930.1328, dtype=float32), Array(254.05428, dtype=float32)), 'eval/epoch_eval_time': 4.709881782531738, 'eval/sps': 27176.90292667923}
I0727 21:01:44.525834 139784370640704 train.py:379] starting iteration 33, 13516800 steps, 1561.7285194396973
I0727 21:02:30.226892 139784370640704 train.py:394] {'eval/walltime': 176.11538696289062, 'training/sps': 9991.05937835318, 'training/walltime': 1421.9421939849854, 'training/entropy_loss': Array(-0.04517379, dtype=float32), 'training/policy_loss': Array(0.00161953, dtype=float32), 'training/total_loss': Array(90.84064, dtype=float32), 'training/v_loss': Array(90.88419, dtype=float32), 'eval/episode_goal_distance': (Array(0.51237303, dtype=float32), Array(0.2926046, dtype=float32)), 'eval/episode_reward': (Array(-11365.095, dtype=float32), Array(5976.5293, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.5060701, dtype=float32), Array(0.30257612, dtype=float32)), 'eval/avg_episode_length': (Array(922.39844, dtype=float32), Array(266.5703, dtype=float32)), 'eval/epoch_eval_time': 4.698216438293457, 'eval/sps': 27244.38128408016}
I0727 21:02:30.230652 139784370640704 train.py:379] starting iteration 34, 13926400 steps, 1607.4333367347717
I0727 21:03:15.926609 139784370640704 train.py:394] {'eval/walltime': 180.82608842849731, 'training/sps': 9995.398407018085, 'training/walltime': 1462.921050786972, 'training/entropy_loss': Array(-0.04595628, dtype=float32), 'training/policy_loss': Array(0.00174674, dtype=float32), 'training/total_loss': Array(87.670265, dtype=float32), 'training/v_loss': Array(87.71448, dtype=float32), 'eval/episode_goal_distance': (Array(0.5355969, dtype=float32), Array(0.31976792, dtype=float32)), 'eval/episode_reward': (Array(-11424.174, dtype=float32), Array(6039.021, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.5301198, dtype=float32), Array(0.33026397, dtype=float32)), 'eval/avg_episode_length': (Array(922.3828, dtype=float32), Array(266.6245, dtype=float32)), 'eval/epoch_eval_time': 4.7107014656066895, 'eval/sps': 27172.174024302116}
I0727 21:03:15.930456 139784370640704 train.py:379] starting iteration 35, 14336000 steps, 1653.1331415176392
I0727 21:04:01.588033 139784370640704 train.py:394] {'eval/walltime': 185.5072958469391, 'training/sps': 9997.556157349394, 'training/walltime': 1503.8910632133484, 'training/entropy_loss': Array(-0.04938681, dtype=float32), 'training/policy_loss': Array(0.00102203, dtype=float32), 'training/total_loss': Array(85.06209, dtype=float32), 'training/v_loss': Array(85.11046, dtype=float32), 'eval/episode_goal_distance': (Array(0.54341793, dtype=float32), Array(0.27141082, dtype=float32)), 'eval/episode_reward': (Array(-11382.448, dtype=float32), Array(5458.0273, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.53981, dtype=float32), Array(0.27900258, dtype=float32)), 'eval/avg_episode_length': (Array(922.28125, dtype=float32), Array(266.97302, dtype=float32)), 'eval/epoch_eval_time': 4.6812074184417725, 'eval/sps': 27343.37288617884}
I0727 21:04:01.591854 139784370640704 train.py:379] starting iteration 36, 14745600 steps, 1698.7945396900177
I0727 21:04:47.205865 139784370640704 train.py:394] {'eval/walltime': 190.18943548202515, 'training/sps': 10008.339482663945, 'training/walltime': 1544.8169331550598, 'training/entropy_loss': Array(-0.04979885, dtype=float32), 'training/policy_loss': Array(0.00159443, dtype=float32), 'training/total_loss': Array(74.76951, dtype=float32), 'training/v_loss': Array(74.81771, dtype=float32), 'eval/episode_goal_distance': (Array(0.5102241, dtype=float32), Array(0.25015855, dtype=float32)), 'eval/episode_reward': (Array(-10801.167, dtype=float32), Array(5184.056, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.5057454, dtype=float32), Array(0.25747207, dtype=float32)), 'eval/avg_episode_length': (Array(906.91406, dtype=float32), Array(289.41705, dtype=float32)), 'eval/epoch_eval_time': 4.68213963508606, 'eval/sps': 27337.928805202606}
I0727 21:04:47.209721 139784370640704 train.py:379] starting iteration 37, 15155200 steps, 1744.4124059677124
I0727 21:05:32.911874 139784370640704 train.py:394] {'eval/walltime': 194.90369820594788, 'training/sps': 9994.727818887404, 'training/walltime': 1585.7985394001007, 'training/entropy_loss': Array(-0.04869188, dtype=float32), 'training/policy_loss': Array(0.00172672, dtype=float32), 'training/total_loss': Array(72.74107, dtype=float32), 'training/v_loss': Array(72.78804, dtype=float32), 'eval/episode_goal_distance': (Array(0.49570823, dtype=float32), Array(0.23645453, dtype=float32)), 'eval/episode_reward': (Array(-10961.211, dtype=float32), Array(5400.9526, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.48964044, dtype=float32), Array(0.24491459, dtype=float32)), 'eval/avg_episode_length': (Array(914.58594, dtype=float32), Array(278.56503, dtype=float32)), 'eval/epoch_eval_time': 4.7142627239227295, 'eval/sps': 27151.647563140355}
I0727 21:05:32.915645 139784370640704 train.py:379] starting iteration 38, 15564800 steps, 1790.118330001831
I0727 21:06:18.711415 139784370640704 train.py:394] {'eval/walltime': 199.60840821266174, 'training/sps': 9969.671357958907, 'training/walltime': 1626.8831434249878, 'training/entropy_loss': Array(-0.04770534, dtype=float32), 'training/policy_loss': Array(0.0020176, dtype=float32), 'training/total_loss': Array(75.47589, dtype=float32), 'training/v_loss': Array(75.521576, dtype=float32), 'eval/episode_goal_distance': (Array(0.48894995, dtype=float32), Array(0.22914536, dtype=float32)), 'eval/episode_reward': (Array(-10419.951, dtype=float32), Array(4993.557, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.48072708, dtype=float32), Array(0.24001136, dtype=float32)), 'eval/avg_episode_length': (Array(922.28125, dtype=float32), Array(266.9733, dtype=float32)), 'eval/epoch_eval_time': 4.704710006713867, 'eval/sps': 27206.77784971599}
I0727 21:06:18.715171 139784370640704 train.py:379] starting iteration 39, 15974400 steps, 1835.9178562164307
I0727 21:07:04.530687 139784370640704 train.py:394] {'eval/walltime': 204.31236791610718, 'training/sps': 9964.675700408061, 'training/walltime': 1667.988344669342, 'training/entropy_loss': Array(-0.04737382, dtype=float32), 'training/policy_loss': Array(0.0024175, dtype=float32), 'training/total_loss': Array(77.38707, dtype=float32), 'training/v_loss': Array(77.43202, dtype=float32), 'eval/episode_goal_distance': (Array(0.49121788, dtype=float32), Array(0.21592554, dtype=float32)), 'eval/episode_reward': (Array(-10224.834, dtype=float32), Array(5132.012, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.48487, dtype=float32), Array(0.2245279, dtype=float32)), 'eval/avg_episode_length': (Array(883.5703, dtype=float32), Array(319.56445, dtype=float32)), 'eval/epoch_eval_time': 4.703959703445435, 'eval/sps': 27211.11745626687}
I0727 21:07:04.534422 139784370640704 train.py:379] starting iteration 40, 16384000 steps, 1881.7371063232422
I0727 21:07:50.380077 139784370640704 train.py:394] {'eval/walltime': 209.01313138008118, 'training/sps': 9956.58939405822, 'training/walltime': 1709.1269297599792, 'training/entropy_loss': Array(-0.04900524, dtype=float32), 'training/policy_loss': Array(0.0014788, dtype=float32), 'training/total_loss': Array(71.201836, dtype=float32), 'training/v_loss': Array(71.24936, dtype=float32), 'eval/episode_goal_distance': (Array(0.5141121, dtype=float32), Array(0.24696738, dtype=float32)), 'eval/episode_reward': (Array(-10965.496, dtype=float32), Array(5082.3203, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.50645554, dtype=float32), Array(0.25711846, dtype=float32)), 'eval/avg_episode_length': (Array(930.10156, dtype=float32), Array(254.1676, dtype=float32)), 'eval/epoch_eval_time': 4.700763463973999, 'eval/sps': 27229.6193971414}
I0727 21:07:50.383859 139784370640704 train.py:379] starting iteration 41, 16793600 steps, 1927.5865440368652
I0727 21:08:36.169193 139784370640704 train.py:394] {'eval/walltime': 213.72750782966614, 'training/sps': 9974.553190609822, 'training/walltime': 1750.1914258003235, 'training/entropy_loss': Array(-0.0484013, dtype=float32), 'training/policy_loss': Array(0.00137662, dtype=float32), 'training/total_loss': Array(57.280624, dtype=float32), 'training/v_loss': Array(57.327644, dtype=float32), 'eval/episode_goal_distance': (Array(0.47839338, dtype=float32), Array(0.20145081, dtype=float32)), 'eval/episode_reward': (Array(-10253.289, dtype=float32), Array(4366.406, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.4685397, dtype=float32), Array(0.21212481, dtype=float32)), 'eval/avg_episode_length': (Array(914.58594, dtype=float32), Array(278.565, dtype=float32)), 'eval/epoch_eval_time': 4.714376449584961, 'eval/sps': 27150.992579574064}
I0727 21:08:36.173073 139784370640704 train.py:379] starting iteration 42, 17203200 steps, 1973.3757576942444
I0727 21:09:22.049885 139784370640704 train.py:394] {'eval/walltime': 218.45167756080627, 'training/sps': 9954.660685650844, 'training/walltime': 1791.3379814624786, 'training/entropy_loss': Array(-0.04754304, dtype=float32), 'training/policy_loss': Array(0.00164203, dtype=float32), 'training/total_loss': Array(59.43033, dtype=float32), 'training/v_loss': Array(59.47623, dtype=float32), 'eval/episode_goal_distance': (Array(0.45444167, dtype=float32), Array(0.1922062, dtype=float32)), 'eval/episode_reward': (Array(-9142.1875, dtype=float32), Array(4886.034, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.4449176, dtype=float32), Array(0.20075816, dtype=float32)), 'eval/avg_episode_length': (Array(852.4375, dtype=float32), Array(353.43823, dtype=float32)), 'eval/epoch_eval_time': 4.724169731140137, 'eval/sps': 27094.708125380654}
I0727 21:09:22.053666 139784370640704 train.py:379] starting iteration 43, 17612800 steps, 2019.2563507556915
I0727 21:10:07.807335 139784370640704 train.py:394] {'eval/walltime': 223.14606618881226, 'training/sps': 9977.355982390378, 'training/walltime': 1832.3909418582916, 'training/entropy_loss': Array(-0.04593296, dtype=float32), 'training/policy_loss': Array(0.01178561, dtype=float32), 'training/total_loss': Array(62.90368, dtype=float32), 'training/v_loss': Array(62.937824, dtype=float32), 'eval/episode_goal_distance': (Array(0.48022592, dtype=float32), Array(0.21661957, dtype=float32)), 'eval/episode_reward': (Array(-9814.646, dtype=float32), Array(4921.077, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.47224545, dtype=float32), Array(0.22584735, dtype=float32)), 'eval/avg_episode_length': (Array(906.77344, dtype=float32), Array(289.8535, dtype=float32)), 'eval/epoch_eval_time': 4.6943886280059814, 'eval/sps': 27266.59638624127}
I0727 21:10:07.811072 139784370640704 train.py:379] starting iteration 44, 18022400 steps, 2065.0137572288513
I0727 21:10:53.609098 139784370640704 train.py:394] {'eval/walltime': 227.83698630332947, 'training/sps': 9965.879760687125, 'training/walltime': 1873.4911768436432, 'training/entropy_loss': Array(-0.04593427, dtype=float32), 'training/policy_loss': Array(0.00571328, dtype=float32), 'training/total_loss': Array(65.18846, dtype=float32), 'training/v_loss': Array(65.22868, dtype=float32), 'eval/episode_goal_distance': (Array(0.44372714, dtype=float32), Array(0.2031883, dtype=float32)), 'eval/episode_reward': (Array(-9460.125, dtype=float32), Array(4568.4106, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.4341508, dtype=float32), Array(0.21255745, dtype=float32)), 'eval/avg_episode_length': (Array(906.8047, dtype=float32), Array(289.75662, dtype=float32)), 'eval/epoch_eval_time': 4.690920114517212, 'eval/sps': 27286.757581710324}
I0727 21:10:53.612942 139784370640704 train.py:379] starting iteration 45, 18432000 steps, 2110.8156266212463
I0727 21:11:39.431078 139784370640704 train.py:394] {'eval/walltime': 232.54052138328552, 'training/sps': 9964.00003964751, 'training/walltime': 1914.5991654396057, 'training/entropy_loss': Array(-0.0455139, dtype=float32), 'training/policy_loss': Array(0.00095256, dtype=float32), 'training/total_loss': Array(60.20573, dtype=float32), 'training/v_loss': Array(60.25029, dtype=float32), 'eval/episode_goal_distance': (Array(0.4519375, dtype=float32), Array(0.21384771, dtype=float32)), 'eval/episode_reward': (Array(-9221.802, dtype=float32), Array(4816.766, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.44177735, dtype=float32), Array(0.22381796, dtype=float32)), 'eval/avg_episode_length': (Array(883.53906, dtype=float32), Array(319.64972, dtype=float32)), 'eval/epoch_eval_time': 4.703535079956055, 'eval/sps': 27213.57400850849}
I0727 21:11:39.434897 139784370640704 train.py:379] starting iteration 46, 18841600 steps, 2156.6375815868378
I0727 21:12:25.234285 139784370640704 train.py:394] {'eval/walltime': 237.24059391021729, 'training/sps': 9967.677259330645, 'training/walltime': 1955.6919887065887, 'training/entropy_loss': Array(-0.04368646, dtype=float32), 'training/policy_loss': Array(0.00121888, dtype=float32), 'training/total_loss': Array(44.471397, dtype=float32), 'training/v_loss': Array(44.513863, dtype=float32), 'eval/episode_goal_distance': (Array(0.45818967, dtype=float32), Array(0.19901592, dtype=float32)), 'eval/episode_reward': (Array(-9918.709, dtype=float32), Array(4303.8916, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.44555888, dtype=float32), Array(0.20979454, dtype=float32)), 'eval/avg_episode_length': (Array(930.1328, dtype=float32), Array(254.05414, dtype=float32)), 'eval/epoch_eval_time': 4.700072526931763, 'eval/sps': 27233.622304028406}
I0727 21:12:25.238145 139784370640704 train.py:379] starting iteration 47, 19251200 steps, 2202.44083070755
I0727 21:13:11.046289 139784370640704 train.py:394] {'eval/walltime': 241.94262409210205, 'training/sps': 9966.08210373877, 'training/walltime': 1996.7913892269135, 'training/entropy_loss': Array(-0.04329618, dtype=float32), 'training/policy_loss': Array(0.00064755, dtype=float32), 'training/total_loss': Array(44.712624, dtype=float32), 'training/v_loss': Array(44.755272, dtype=float32), 'eval/episode_goal_distance': (Array(0.42290646, dtype=float32), Array(0.18569677, dtype=float32)), 'eval/episode_reward': (Array(-9459.011, dtype=float32), Array(3798.1353, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.410644, dtype=float32), Array(0.19682817, dtype=float32)), 'eval/avg_episode_length': (Array(961.16406, dtype=float32), Array(192.62024, dtype=float32)), 'eval/epoch_eval_time': 4.702030181884766, 'eval/sps': 27222.283789912293}
I0727 21:13:11.050071 139784370640704 train.py:379] starting iteration 48, 19660800 steps, 2248.252755880356
I0727 21:13:56.979635 139784370640704 train.py:394] {'eval/walltime': 246.64782786369324, 'training/sps': 9937.521541968948, 'training/walltime': 2038.0089099407196, 'training/entropy_loss': Array(-0.04165452, dtype=float32), 'training/policy_loss': Array(0.00063769, dtype=float32), 'training/total_loss': Array(45.285767, dtype=float32), 'training/v_loss': Array(45.326782, dtype=float32), 'eval/episode_goal_distance': (Array(0.4142323, dtype=float32), Array(0.18542163, dtype=float32)), 'eval/episode_reward': (Array(-9240.211, dtype=float32), Array(3749.5007, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.4016723, dtype=float32), Array(0.19567433, dtype=float32)), 'eval/avg_episode_length': (Array(945.625, dtype=float32), Array(226.0703, dtype=float32)), 'eval/epoch_eval_time': 4.7052037715911865, 'eval/sps': 27203.92276586004}
I0727 21:13:56.983418 139784370640704 train.py:379] starting iteration 49, 20070400 steps, 2294.186102628708
I0727 21:14:43.082443 139784370640704 train.py:394] {'eval/walltime': 251.36701917648315, 'training/sps': 9900.066985584943, 'training/walltime': 2079.382367372513, 'training/entropy_loss': Array(-0.04219808, dtype=float32), 'training/policy_loss': Array(0.00074493, dtype=float32), 'training/total_loss': Array(45.505924, dtype=float32), 'training/v_loss': Array(45.54738, dtype=float32), 'eval/episode_goal_distance': (Array(0.43388003, dtype=float32), Array(0.20912126, dtype=float32)), 'eval/episode_reward': (Array(-9104.047, dtype=float32), Array(4225.6807, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.41944474, dtype=float32), Array(0.22161244, dtype=float32)), 'eval/avg_episode_length': (Array(937.8672, dtype=float32), Array(240.63965, dtype=float32)), 'eval/epoch_eval_time': 4.719191312789917, 'eval/sps': 27123.29115649441}
I0727 21:14:43.086194 139784370640704 train.py:379] starting iteration 50, 20480000 steps, 2340.28887963295
I0727 21:15:28.937330 139784370640704 train.py:394] {'eval/walltime': 256.0732226371765, 'training/sps': 9956.983408409636, 'training/walltime': 2120.519324541092, 'training/entropy_loss': Array(-0.04257954, dtype=float32), 'training/policy_loss': Array(0.00033631, dtype=float32), 'training/total_loss': Array(51.675415, dtype=float32), 'training/v_loss': Array(51.717663, dtype=float32), 'eval/episode_goal_distance': (Array(0.41617674, dtype=float32), Array(0.15803236, dtype=float32)), 'eval/episode_reward': (Array(-8534.852, dtype=float32), Array(4102.29, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.40453947, dtype=float32), Array(0.16634941, dtype=float32)), 'eval/avg_episode_length': (Array(883.66406, dtype=float32), Array(319.30728, dtype=float32)), 'eval/epoch_eval_time': 4.706203460693359, 'eval/sps': 27198.14412382883}
I0727 21:15:29.028116 139784370640704 train.py:379] starting iteration 51, 20889600 steps, 2386.23078417778
I0727 21:16:14.916686 139784370640704 train.py:394] {'eval/walltime': 260.77345299720764, 'training/sps': 9946.273551897084, 'training/walltime': 2161.7005767822266, 'training/entropy_loss': Array(-0.03908969, dtype=float32), 'training/policy_loss': Array(0.00051652, dtype=float32), 'training/total_loss': Array(35.752213, dtype=float32), 'training/v_loss': Array(35.79078, dtype=float32), 'eval/episode_goal_distance': (Array(0.37783638, dtype=float32), Array(0.17235366, dtype=float32)), 'eval/episode_reward': (Array(-8173.736, dtype=float32), Array(4890.215, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.36133882, dtype=float32), Array(0.18474953, dtype=float32)), 'eval/avg_episode_length': (Array(844.625, dtype=float32), Array(361.0593, dtype=float32)), 'eval/epoch_eval_time': 4.700230360031128, 'eval/sps': 27232.707802677207}
I0727 21:16:14.924417 139784370640704 train.py:379] starting iteration 52, 21299200 steps, 2432.1270871162415
I0727 21:17:00.783854 139784370640704 train.py:394] {'eval/walltime': 265.4583842754364, 'training/sps': 9949.600075733364, 'training/walltime': 2202.8680605888367, 'training/entropy_loss': Array(-0.03679899, dtype=float32), 'training/policy_loss': Array(0.00058216, dtype=float32), 'training/total_loss': Array(35.703957, dtype=float32), 'training/v_loss': Array(35.740173, dtype=float32), 'eval/episode_goal_distance': (Array(0.38193175, dtype=float32), Array(0.14684892, dtype=float32)), 'eval/episode_reward': (Array(-8993.057, dtype=float32), Array(3504.0867, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.3667254, dtype=float32), Array(0.15772702, dtype=float32)), 'eval/avg_episode_length': (Array(953.4219, dtype=float32), Array(210.03287, dtype=float32)), 'eval/epoch_eval_time': 4.68493127822876, 'eval/sps': 27321.638760172635}
I0727 21:17:00.787436 139784370640704 train.py:379] starting iteration 53, 21708800 steps, 2477.9901213645935
I0727 21:17:46.671456 139784370640704 train.py:394] {'eval/walltime': 270.1489186286926, 'training/sps': 9944.96087021733, 'training/walltime': 2244.0547485351562, 'training/entropy_loss': Array(-0.03460336, dtype=float32), 'training/policy_loss': Array(0.00059298, dtype=float32), 'training/total_loss': Array(33.33643, dtype=float32), 'training/v_loss': Array(33.370438, dtype=float32), 'eval/episode_goal_distance': (Array(0.38298357, dtype=float32), Array(0.17100456, dtype=float32)), 'eval/episode_reward': (Array(-8732.036, dtype=float32), Array(3865.5984, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.3646976, dtype=float32), Array(0.18511263, dtype=float32)), 'eval/avg_episode_length': (Array(945.64844, dtype=float32), Array(225.97295, dtype=float32)), 'eval/epoch_eval_time': 4.690534353256226, 'eval/sps': 27289.0017128093}
I0727 21:17:46.675042 139784370640704 train.py:379] starting iteration 54, 22118400 steps, 2523.8777260780334
I0727 21:18:32.465949 139784370640704 train.py:394] {'eval/walltime': 274.8344178199768, 'training/sps': 9966.10320570156, 'training/walltime': 2285.1540620326996, 'training/entropy_loss': Array(-0.0337128, dtype=float32), 'training/policy_loss': Array(0.00078031, dtype=float32), 'training/total_loss': Array(33.217903, dtype=float32), 'training/v_loss': Array(33.25084, dtype=float32), 'eval/episode_goal_distance': (Array(0.36113465, dtype=float32), Array(0.13109808, dtype=float32)), 'eval/episode_reward': (Array(-8543.152, dtype=float32), Array(3692.39, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.34315038, dtype=float32), Array(0.14234321, dtype=float32)), 'eval/avg_episode_length': (Array(930.1172, dtype=float32), Array(254.11084, dtype=float32)), 'eval/epoch_eval_time': 4.68549919128418, 'eval/sps': 27318.3271993946}
I0727 21:18:32.469486 139784370640704 train.py:379] starting iteration 55, 22528000 steps, 2569.672170639038
I0727 21:19:18.294372 139784370640704 train.py:394] {'eval/walltime': 279.5471532344818, 'training/sps': 9964.494105046242, 'training/walltime': 2326.2600123882294, 'training/entropy_loss': Array(-0.03030767, dtype=float32), 'training/policy_loss': Array(0.00049787, dtype=float32), 'training/total_loss': Array(41.65686, dtype=float32), 'training/v_loss': Array(41.68667, dtype=float32), 'eval/episode_goal_distance': (Array(0.3415677, dtype=float32), Array(0.11787786, dtype=float32)), 'eval/episode_reward': (Array(-8351.496, dtype=float32), Array(3350.7175, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.32021663, dtype=float32), Array(0.12877971, dtype=float32)), 'eval/avg_episode_length': (Array(945.625, dtype=float32), Array(226.0704, dtype=float32)), 'eval/epoch_eval_time': 4.712735414505005, 'eval/sps': 27160.446904368444}
I0727 21:19:18.297905 139784370640704 train.py:379] starting iteration 56, 22937600 steps, 2615.5005900859833
I0727 21:20:04.289716 139784370640704 train.py:394] {'eval/walltime': 284.2391827106476, 'training/sps': 9919.21785436319, 'training/walltime': 2367.553590774536, 'training/entropy_loss': Array(-0.02617232, dtype=float32), 'training/policy_loss': Array(0.00069408, dtype=float32), 'training/total_loss': Array(25.596313, dtype=float32), 'training/v_loss': Array(25.621794, dtype=float32), 'eval/episode_goal_distance': (Array(0.339799, dtype=float32), Array(0.11783241, dtype=float32)), 'eval/episode_reward': (Array(-8030.617, dtype=float32), Array(3302.6453, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.31805027, dtype=float32), Array(0.12888178, dtype=float32)), 'eval/avg_episode_length': (Array(937.83594, dtype=float32), Array(240.76074, dtype=float32)), 'eval/epoch_eval_time': 4.6920294761657715, 'eval/sps': 27280.30602753138}
I0727 21:20:04.293350 139784370640704 train.py:379] starting iteration 57, 23347200 steps, 2661.496035337448
I0727 21:20:50.216021 139784370640704 train.py:394] {'eval/walltime': 288.9436378479004, 'training/sps': 9938.911147893334, 'training/walltime': 2408.765348672867, 'training/entropy_loss': Array(-0.02465827, dtype=float32), 'training/policy_loss': Array(0.00065756, dtype=float32), 'training/total_loss': Array(23.942104, dtype=float32), 'training/v_loss': Array(23.966105, dtype=float32), 'eval/episode_goal_distance': (Array(0.33908945, dtype=float32), Array(0.10923975, dtype=float32)), 'eval/episode_reward': (Array(-8030.174, dtype=float32), Array(3614.4045, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.31795806, dtype=float32), Array(0.11900835, dtype=float32)), 'eval/avg_episode_length': (Array(906.71094, dtype=float32), Array(290.04782, dtype=float32)), 'eval/epoch_eval_time': 4.704455137252808, 'eval/sps': 27208.25180931501}
I0727 21:20:50.219817 139784370640704 train.py:379] starting iteration 58, 23756800 steps, 2707.4225022792816
I0727 21:21:35.883507 139784370640704 train.py:394] {'eval/walltime': 293.6415948867798, 'training/sps': 10000.137479071875, 'training/walltime': 2449.72478556633, 'training/entropy_loss': Array(-0.02175494, dtype=float32), 'training/policy_loss': Array(0.00068157, dtype=float32), 'training/total_loss': Array(23.958035, dtype=float32), 'training/v_loss': Array(23.979107, dtype=float32), 'eval/episode_goal_distance': (Array(0.3484161, dtype=float32), Array(0.11611002, dtype=float32)), 'eval/episode_reward': (Array(-8408.434, dtype=float32), Array(3494.0354, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.32900393, dtype=float32), Array(0.12814532, dtype=float32)), 'eval/avg_episode_length': (Array(945.64844, dtype=float32), Array(225.97298, dtype=float32)), 'eval/epoch_eval_time': 4.6979570388793945, 'eval/sps': 27245.885592545113}
I0727 21:21:35.887300 139784370640704 train.py:379] starting iteration 59, 24166400 steps, 2753.0899851322174
I0727 21:22:21.427099 139784370640704 train.py:394] {'eval/walltime': 298.3270363807678, 'training/sps': 10027.474663302795, 'training/walltime': 2490.5725576877594, 'training/entropy_loss': Array(-0.01767711, dtype=float32), 'training/policy_loss': Array(0.00076391, dtype=float32), 'training/total_loss': Array(22.14665, dtype=float32), 'training/v_loss': Array(22.163565, dtype=float32), 'eval/episode_goal_distance': (Array(0.35502967, dtype=float32), Array(0.10085718, dtype=float32)), 'eval/episode_reward': (Array(-8294.262, dtype=float32), Array(3486.5342, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.33682862, dtype=float32), Array(0.11040509, dtype=float32)), 'eval/avg_episode_length': (Array(914.66406, dtype=float32), Array(278.3104, dtype=float32)), 'eval/epoch_eval_time': 4.685441493988037, 'eval/sps': 27318.66360176278}
I0727 21:22:21.430924 139784370640704 train.py:379] starting iteration 60, 24576000 steps, 2798.633609056473
I0727 21:23:07.183124 139784370640704 train.py:394] {'eval/walltime': 303.0120360851288, 'training/sps': 9975.418236239258, 'training/walltime': 2531.633492708206, 'training/entropy_loss': Array(-0.01539177, dtype=float32), 'training/policy_loss': Array(0.00077552, dtype=float32), 'training/total_loss': Array(34.684914, dtype=float32), 'training/v_loss': Array(34.69953, dtype=float32), 'eval/episode_goal_distance': (Array(0.3442706, dtype=float32), Array(0.10023083, dtype=float32)), 'eval/episode_reward': (Array(-8522.728, dtype=float32), Array(3189.1328, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.32398885, dtype=float32), Array(0.11102218, dtype=float32)), 'eval/avg_episode_length': (Array(945.6094, dtype=float32), Array(226.1352, dtype=float32)), 'eval/epoch_eval_time': 4.684999704360962, 'eval/sps': 27321.239717657423}
I0727 21:23:07.186924 139784370640704 train.py:379] starting iteration 61, 24985600 steps, 2844.3896090984344
I0727 21:23:52.848408 139784370640704 train.py:394] {'eval/walltime': 307.7233409881592, 'training/sps': 10003.953154496383, 'training/walltime': 2572.577306985855, 'training/entropy_loss': Array(-0.00973018, dtype=float32), 'training/policy_loss': Array(0.00071624, dtype=float32), 'training/total_loss': Array(19.291883, dtype=float32), 'training/v_loss': Array(19.300898, dtype=float32), 'eval/episode_goal_distance': (Array(0.330778, dtype=float32), Array(0.11034896, dtype=float32)), 'eval/episode_reward': (Array(-8195.158, dtype=float32), Array(3374.1233, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.3074656, dtype=float32), Array(0.12324212, dtype=float32)), 'eval/avg_episode_length': (Array(953.375, dtype=float32), Array(210.24397, dtype=float32)), 'eval/epoch_eval_time': 4.7113049030303955, 'eval/sps': 27168.69373444035}
I0727 21:23:52.852205 139784370640704 train.py:379] starting iteration 62, 25395200 steps, 2890.054889202118
I0727 21:24:38.603654 139784370640704 train.py:394] {'eval/walltime': 312.4076817035675, 'training/sps': 9975.46324171451, 'training/walltime': 2613.638056755066, 'training/entropy_loss': Array(-0.0023683, dtype=float32), 'training/policy_loss': Array(0.00082554, dtype=float32), 'training/total_loss': Array(17.907934, dtype=float32), 'training/v_loss': Array(17.909475, dtype=float32), 'eval/episode_goal_distance': (Array(0.32919317, dtype=float32), Array(0.08999268, dtype=float32)), 'eval/episode_reward': (Array(-7888.7983, dtype=float32), Array(3155.3203, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.30471253, dtype=float32), Array(0.10029417, dtype=float32)), 'eval/avg_episode_length': (Array(930.0703, dtype=float32), Array(254.28133, dtype=float32)), 'eval/epoch_eval_time': 4.684340715408325, 'eval/sps': 27325.083245752434}
I0727 21:24:38.607489 139784370640704 train.py:379] starting iteration 63, 25804800 steps, 2935.810172557831
I0727 21:25:24.341970 139784370640704 train.py:394] {'eval/walltime': 317.0982985496521, 'training/sps': 9981.125700959374, 'training/walltime': 2654.675512075424, 'training/entropy_loss': Array(0.00363882, dtype=float32), 'training/policy_loss': Array(0.00094598, dtype=float32), 'training/total_loss': Array(17.745125, dtype=float32), 'training/v_loss': Array(17.74054, dtype=float32), 'eval/episode_goal_distance': (Array(0.3235094, dtype=float32), Array(0.09902405, dtype=float32)), 'eval/episode_reward': (Array(-7491.834, dtype=float32), Array(3474.0015, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.29579073, dtype=float32), Array(0.11196049, dtype=float32)), 'eval/avg_episode_length': (Array(906.7578, dtype=float32), Array(289.90195, dtype=float32)), 'eval/epoch_eval_time': 4.690616846084595, 'eval/sps': 27288.52178724545}
I0727 21:25:24.345757 139784370640704 train.py:379] starting iteration 64, 26214400 steps, 2981.5484426021576
I0727 21:26:10.062365 139784370640704 train.py:394] {'eval/walltime': 321.77783489227295, 'training/sps': 9982.801433652769, 'training/walltime': 2695.7060787677765, 'training/entropy_loss': Array(0.01238441, dtype=float32), 'training/policy_loss': Array(0.00107873, dtype=float32), 'training/total_loss': Array(15.795599, dtype=float32), 'training/v_loss': Array(15.782137, dtype=float32), 'eval/episode_goal_distance': (Array(0.33826134, dtype=float32), Array(0.10489308, dtype=float32)), 'eval/episode_reward': (Array(-8379.528, dtype=float32), Array(3354.5034, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.31376016, dtype=float32), Array(0.11606022, dtype=float32)), 'eval/avg_episode_length': (Array(945.6328, dtype=float32), Array(226.03792, dtype=float32)), 'eval/epoch_eval_time': 4.67953634262085, 'eval/sps': 27353.137282894044}
I0727 21:26:10.066165 139784370640704 train.py:379] starting iteration 65, 26624000 steps, 3027.268850326538
I0727 21:26:55.788542 139784370640704 train.py:394] {'eval/walltime': 326.5083577632904, 'training/sps': 9993.805006011831, 'training/walltime': 2736.691469192505, 'training/entropy_loss': Array(0.0240001, dtype=float32), 'training/policy_loss': Array(0.00130683, dtype=float32), 'training/total_loss': Array(31.345955, dtype=float32), 'training/v_loss': Array(31.320648, dtype=float32), 'eval/episode_goal_distance': (Array(0.32447994, dtype=float32), Array(0.1012651, dtype=float32)), 'eval/episode_reward': (Array(-8000.5703, dtype=float32), Array(3246.0947, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.2978636, dtype=float32), Array(0.11537113, dtype=float32)), 'eval/avg_episode_length': (Array(953.34375, dtype=float32), Array(210.38496, dtype=float32)), 'eval/epoch_eval_time': 4.730522871017456, 'eval/sps': 27058.319659380348}
I0727 21:26:55.792372 139784370640704 train.py:379] starting iteration 66, 27033600 steps, 3072.995057106018
I0727 21:27:41.528843 139784370640704 train.py:394] {'eval/walltime': 331.2250769138336, 'training/sps': 9987.020950115904, 'training/walltime': 2777.7047004699707, 'training/entropy_loss': Array(0.03344378, dtype=float32), 'training/policy_loss': Array(0.00127207, dtype=float32), 'training/total_loss': Array(17.077553, dtype=float32), 'training/v_loss': Array(17.042835, dtype=float32), 'eval/episode_goal_distance': (Array(0.3195177, dtype=float32), Array(0.10509145, dtype=float32)), 'eval/episode_reward': (Array(-8058.276, dtype=float32), Array(3147.3489, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.29777047, dtype=float32), Array(0.11481991, dtype=float32)), 'eval/avg_episode_length': (Array(968.96875, dtype=float32), Array(172.77481, dtype=float32)), 'eval/epoch_eval_time': 4.716719150543213, 'eval/sps': 27137.507219453284}
I0727 21:27:41.532617 139784370640704 train.py:379] starting iteration 67, 27443200 steps, 3118.735301733017
I0727 21:28:27.283666 139784370640704 train.py:394] {'eval/walltime': 335.92940521240234, 'training/sps': 9980.39701717472, 'training/walltime': 2818.7451519966125, 'training/entropy_loss': Array(0.04503775, dtype=float32), 'training/policy_loss': Array(0.00113717, dtype=float32), 'training/total_loss': Array(15.4973345, dtype=float32), 'training/v_loss': Array(15.451159, dtype=float32), 'eval/episode_goal_distance': (Array(0.3439121, dtype=float32), Array(0.10553275, dtype=float32)), 'eval/episode_reward': (Array(-8416.087, dtype=float32), Array(3568.4993, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.32402954, dtype=float32), Array(0.11670072, dtype=float32)), 'eval/avg_episode_length': (Array(930.15625, dtype=float32), Array(253.96881, dtype=float32)), 'eval/epoch_eval_time': 4.704328298568726, 'eval/sps': 27208.985401580823}
I0727 21:28:27.287455 139784370640704 train.py:379] starting iteration 68, 27852800 steps, 3164.4901399612427
I0727 21:29:13.110529 139784370640704 train.py:394] {'eval/walltime': 340.656489610672, 'training/sps': 9968.499813663646, 'training/walltime': 2859.8345844745636, 'training/entropy_loss': Array(0.0600154, dtype=float32), 'training/policy_loss': Array(0.00128084, dtype=float32), 'training/total_loss': Array(15.575043, dtype=float32), 'training/v_loss': Array(15.513744, dtype=float32), 'eval/episode_goal_distance': (Array(0.3220568, dtype=float32), Array(0.10364139, dtype=float32)), 'eval/episode_reward': (Array(-8286.807, dtype=float32), Array(2933.7383, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.2993735, dtype=float32), Array(0.11441589, dtype=float32)), 'eval/avg_episode_length': (Array(984.4453, dtype=float32), Array(123.461525, dtype=float32)), 'eval/epoch_eval_time': 4.727084398269653, 'eval/sps': 27078.00183276912}
I0727 21:29:13.114281 139784370640704 train.py:379] starting iteration 69, 28262400 steps, 3210.3169662952423
I0727 21:29:58.896107 139784370640704 train.py:394] {'eval/walltime': 345.37369418144226, 'training/sps': 9976.108538611088, 'training/walltime': 2900.892678260803, 'training/entropy_loss': Array(0.0782246, dtype=float32), 'training/policy_loss': Array(0.00154831, dtype=float32), 'training/total_loss': Array(14.408104, dtype=float32), 'training/v_loss': Array(14.328331, dtype=float32), 'eval/episode_goal_distance': (Array(0.34188068, dtype=float32), Array(0.1054044, dtype=float32)), 'eval/episode_reward': (Array(-8598.723, dtype=float32), Array(3279.5957, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.3235193, dtype=float32), Array(0.11419928, dtype=float32)), 'eval/avg_episode_length': (Array(953.4453, dtype=float32), Array(209.92694, dtype=float32)), 'eval/epoch_eval_time': 4.717204570770264, 'eval/sps': 27134.714655612046}
I0727 21:29:58.899871 139784370640704 train.py:379] starting iteration 70, 28672000 steps, 3256.1025564670563
I0727 21:30:44.649922 139784370640704 train.py:394] {'eval/walltime': 350.0862965583801, 'training/sps': 9982.760596458565, 'training/walltime': 2941.923412799835, 'training/entropy_loss': Array(0.08865867, dtype=float32), 'training/policy_loss': Array(0.0013146, dtype=float32), 'training/total_loss': Array(27.98559, dtype=float32), 'training/v_loss': Array(27.895618, dtype=float32), 'eval/episode_goal_distance': (Array(0.34304392, dtype=float32), Array(0.11962959, dtype=float32)), 'eval/episode_reward': (Array(-8868.136, dtype=float32), Array(3308.4473, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.32362288, dtype=float32), Array(0.1292424, dtype=float32)), 'eval/avg_episode_length': (Array(984.4922, dtype=float32), Array(123.089584, dtype=float32)), 'eval/epoch_eval_time': 4.712602376937866, 'eval/sps': 27161.213648406992}
I0727 21:30:44.657785 139784370640704 train.py:379] starting iteration 71, 29081600 steps, 3301.8604547977448
I0727 21:31:30.607870 139784370640704 train.py:394] {'eval/walltime': 354.7977931499481, 'training/sps': 9934.1023898267, 'training/walltime': 2983.155119895935, 'training/entropy_loss': Array(0.09979923, dtype=float32), 'training/policy_loss': Array(0.00150501, dtype=float32), 'training/total_loss': Array(15.939663, dtype=float32), 'training/v_loss': Array(15.838358, dtype=float32), 'eval/episode_goal_distance': (Array(0.31854975, dtype=float32), Array(0.09117524, dtype=float32)), 'eval/episode_reward': (Array(-7991.5244, dtype=float32), Array(2901.9072, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.2975952, dtype=float32), Array(0.10272154, dtype=float32)), 'eval/avg_episode_length': (Array(961.16406, dtype=float32), Array(192.62044, dtype=float32)), 'eval/epoch_eval_time': 4.711496591567993, 'eval/sps': 27167.588368646448}
I0727 21:31:30.611693 139784370640704 train.py:379] starting iteration 72, 29491200 steps, 3347.814378261566
I0727 21:32:16.427187 139784370640704 train.py:394] {'eval/walltime': 359.51153111457825, 'training/sps': 9967.137947440515, 'training/walltime': 3024.250166654587, 'training/entropy_loss': Array(0.11520089, dtype=float32), 'training/policy_loss': Array(0.00119735, dtype=float32), 'training/total_loss': Array(14.363411, dtype=float32), 'training/v_loss': Array(14.247011, dtype=float32), 'eval/episode_goal_distance': (Array(0.32042933, dtype=float32), Array(0.10359963, dtype=float32)), 'eval/episode_reward': (Array(-8056.0083, dtype=float32), Array(3189.0393, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.2993772, dtype=float32), Array(0.11321653, dtype=float32)), 'eval/avg_episode_length': (Array(961.1797, dtype=float32), Array(192.54286, dtype=float32)), 'eval/epoch_eval_time': 4.713737964630127, 'eval/sps': 27154.670234208443}
I0727 21:32:16.431023 139784370640704 train.py:379] starting iteration 73, 29900800 steps, 3393.6337077617645
I0727 21:33:02.231601 139784370640704 train.py:394] {'eval/walltime': 364.21090364456177, 'training/sps': 9967.32490151564, 'training/walltime': 3065.3444426059723, 'training/entropy_loss': Array(0.12761968, dtype=float32), 'training/policy_loss': Array(0.00138924, dtype=float32), 'training/total_loss': Array(14.228741, dtype=float32), 'training/v_loss': Array(14.09973, dtype=float32), 'eval/episode_goal_distance': (Array(0.32598072, dtype=float32), Array(0.09914165, dtype=float32)), 'eval/episode_reward': (Array(-8017.498, dtype=float32), Array(3194.303, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.30325294, dtype=float32), Array(0.11155913, dtype=float32)), 'eval/avg_episode_length': (Array(953.40625, dtype=float32), Array(210.10304, dtype=float32)), 'eval/epoch_eval_time': 4.6993725299835205, 'eval/sps': 27237.67889932507}
I0727 21:33:02.235319 139784370640704 train.py:379] starting iteration 74, 30310400 steps, 3439.4380033016205
I0727 21:33:48.075803 139784370640704 train.py:394] {'eval/walltime': 368.92746806144714, 'training/sps': 9961.669594799394, 'training/walltime': 3106.4620480537415, 'training/entropy_loss': Array(0.14247696, dtype=float32), 'training/policy_loss': Array(0.00154309, dtype=float32), 'training/total_loss': Array(13.45681, dtype=float32), 'training/v_loss': Array(13.312789, dtype=float32), 'eval/episode_goal_distance': (Array(0.32668257, dtype=float32), Array(0.11187845, dtype=float32)), 'eval/episode_reward': (Array(-8299.68, dtype=float32), Array(3295.0093, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.306054, dtype=float32), Array(0.12268414, dtype=float32)), 'eval/avg_episode_length': (Array(968.9531, dtype=float32), Array(172.86176, dtype=float32)), 'eval/epoch_eval_time': 4.716564416885376, 'eval/sps': 27138.397504284676}
I0727 21:33:48.079465 139784370640704 train.py:379] starting iteration 75, 30720000 steps, 3485.282149553299
I0727 21:34:33.902185 139784370640704 train.py:394] {'eval/walltime': 373.6343231201172, 'training/sps': 9963.668397775406, 'training/walltime': 3147.5714049339294, 'training/entropy_loss': Array(0.15625396, dtype=float32), 'training/policy_loss': Array(0.00134758, dtype=float32), 'training/total_loss': Array(25.942627, dtype=float32), 'training/v_loss': Array(25.785027, dtype=float32), 'eval/episode_goal_distance': (Array(0.3250129, dtype=float32), Array(0.09587967, dtype=float32)), 'eval/episode_reward': (Array(-8106.504, dtype=float32), Array(3129.9922, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.30347192, dtype=float32), Array(0.10834666, dtype=float32)), 'eval/avg_episode_length': (Array(953.41406, dtype=float32), Array(210.06853, dtype=float32)), 'eval/epoch_eval_time': 4.706855058670044, 'eval/sps': 27194.378922763626}
I0727 21:34:33.983485 139784370640704 train.py:379] starting iteration 76, 31129600 steps, 3531.186147928238
I0727 21:35:19.697262 139784370640704 train.py:394] {'eval/walltime': 378.33954095840454, 'training/sps': 9989.842429969174, 'training/walltime': 3188.5730526447296, 'training/entropy_loss': Array(0.16714665, dtype=float32), 'training/policy_loss': Array(0.00124591, dtype=float32), 'training/total_loss': Array(14.277859, dtype=float32), 'training/v_loss': Array(14.109465, dtype=float32), 'eval/episode_goal_distance': (Array(0.33222383, dtype=float32), Array(0.10308893, dtype=float32)), 'eval/episode_reward': (Array(-8285.594, dtype=float32), Array(3196.625, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.31205237, dtype=float32), Array(0.11391085, dtype=float32)), 'eval/avg_episode_length': (Array(961.15625, dtype=float32), Array(192.65903, dtype=float32)), 'eval/epoch_eval_time': 4.7052178382873535, 'eval/sps': 27203.84143714599}
I0727 21:35:19.701376 139784370640704 train.py:379] starting iteration 77, 31539200 steps, 3576.904061317444
I0727 21:36:05.535111 139784370640704 train.py:394] {'eval/walltime': 383.041960477829, 'training/sps': 9959.835864133564, 'training/walltime': 3229.6982283592224, 'training/entropy_loss': Array(0.1787712, dtype=float32), 'training/policy_loss': Array(0.00152867, dtype=float32), 'training/total_loss': Array(13.495094, dtype=float32), 'training/v_loss': Array(13.314795, dtype=float32), 'eval/episode_goal_distance': (Array(0.32387078, dtype=float32), Array(0.09311709, dtype=float32)), 'eval/episode_reward': (Array(-8253.778, dtype=float32), Array(2818.2117, dtype=float32)), 'eval/episode_root_goal_distance': (Array(0.30312055, dtype=float32), Array(0.10309212, dtype=float32)), 'eval/avg_episode_length': (Array(968.9453, dtype=float32), Array(172.90546, dtype=float32)), 'eval/epoch_eval_time': 4.7024195194244385, 'eval/sps': 27220.029916783522}
I0727 21:36:05.538799 139784370640704 train.py:379] starting iteration 78, 31948800 steps, 3622.7414841651917
