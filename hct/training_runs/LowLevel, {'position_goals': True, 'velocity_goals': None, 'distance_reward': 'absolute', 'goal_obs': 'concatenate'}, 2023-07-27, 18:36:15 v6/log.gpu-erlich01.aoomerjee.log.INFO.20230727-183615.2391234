I0727 18:36:15.175584 139768182101824 low_level_env.py:187] Initialising environment...
I0727 18:36:54.726380 139768182101824 low_level_env.py:289] Environment initialised.
I0727 18:36:54.730930 139768182101824 train.py:118] JAX is running on GPU.
I0727 18:36:54.730985 139768182101824 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0727 18:37:02.915383 139768182101824 train.py:367] Running initial eval
I0727 18:37:19.691655 139768182101824 train.py:373] {'eval/walltime': 16.634554147720337, 'eval/episode_goal_distance': (Array(0.6492345, dtype=float32), Array(0.3859976, dtype=float32)), 'eval/episode_reward': (Array(-12712.256, dtype=float32), Array(6536.798, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.75836, dtype=float32)), 'eval/epoch_eval_time': 16.634554147720337, 'eval/sps': 7694.826014771284}
I0727 18:37:19.692964 139768182101824 train.py:379] starting iteration 0 24.962050676345825
I0727 18:38:29.728960 139768182101824 train.py:394] {'eval/walltime': 20.807289600372314, 'training/sps': 6219.742062725968, 'training/walltime': 65.85482096672058, 'training/entropy_loss': Array(-0.04731461, dtype=float32), 'training/policy_loss': Array(0.03111138, dtype=float32), 'training/total_loss': Array(201.0179, dtype=float32), 'training/v_loss': Array(201.0341, dtype=float32), 'eval/episode_goal_distance': (Array(0.683278, dtype=float32), Array(0.3600415, dtype=float32)), 'eval/episode_reward': (Array(-13300.129, dtype=float32), Array(6200.121, dtype=float32)), 'eval/avg_episode_length': (Array(945.6953, dtype=float32), Array(225.77795, dtype=float32)), 'eval/epoch_eval_time': 4.1727354526519775, 'eval/sps': 30675.321129847744}
I0727 18:38:29.765745 139768182101824 train.py:379] starting iteration 1 95.03482460975647
I0727 18:38:55.344309 139768182101824 train.py:394] {'eval/walltime': 25.182239770889282, 'training/sps': 19324.126862261615, 'training/walltime': 87.05112147331238, 'training/entropy_loss': Array(-0.04582715, dtype=float32), 'training/policy_loss': Array(0.01268209, dtype=float32), 'training/total_loss': Array(306.7712, dtype=float32), 'training/v_loss': Array(306.80438, dtype=float32), 'eval/episode_goal_distance': (Array(0.84363705, dtype=float32), Array(0.45420882, dtype=float32)), 'eval/episode_reward': (Array(-15031.415, dtype=float32), Array(7285.004, dtype=float32)), 'eval/avg_episode_length': (Array(937.83594, dtype=float32), Array(240.76077, dtype=float32)), 'eval/epoch_eval_time': 4.374950170516968, 'eval/sps': 29257.476087979037}
I0727 18:38:55.348128 139768182101824 train.py:379] starting iteration 2 120.61721563339233
I0727 18:39:21.101509 139768182101824 train.py:394] {'eval/walltime': 29.604716062545776, 'training/sps': 19209.963745110465, 'training/walltime': 108.37338972091675, 'training/entropy_loss': Array(-0.04097788, dtype=float32), 'training/policy_loss': Array(0.0150836, dtype=float32), 'training/total_loss': Array(429.51233, dtype=float32), 'training/v_loss': Array(429.5382, dtype=float32), 'eval/episode_goal_distance': (Array(0.72847545, dtype=float32), Array(0.4230541, dtype=float32)), 'eval/episode_reward': (Array(-13549.409, dtype=float32), Array(6524.3774, dtype=float32)), 'eval/avg_episode_length': (Array(930.1406, dtype=float32), Array(254.02562, dtype=float32)), 'eval/epoch_eval_time': 4.422476291656494, 'eval/sps': 28943.06075568717}
I0727 18:39:21.105025 139768182101824 train.py:379] starting iteration 3 146.37411212921143
I0727 18:39:47.572952 139768182101824 train.py:394] {'eval/walltime': 34.025221824645996, 'training/sps': 18583.56260338267, 'training/walltime': 130.41437339782715, 'training/entropy_loss': Array(-0.03390033, dtype=float32), 'training/policy_loss': Array(0.01823584, dtype=float32), 'training/total_loss': Array(554.3279, dtype=float32), 'training/v_loss': Array(554.3436, dtype=float32), 'eval/episode_goal_distance': (Array(0.72449535, dtype=float32), Array(0.3963138, dtype=float32)), 'eval/episode_reward': (Array(-13397.188, dtype=float32), Array(6828.671, dtype=float32)), 'eval/avg_episode_length': (Array(930.10156, dtype=float32), Array(254.1676, dtype=float32)), 'eval/epoch_eval_time': 4.42050576210022, 'eval/sps': 28955.962708481147}
I0727 18:39:47.576471 139768182101824 train.py:379] starting iteration 4 172.8455593585968
I0727 18:40:14.528221 139768182101824 train.py:394] {'eval/walltime': 38.47700071334839, 'training/sps': 18209.56876455604, 'training/walltime': 152.90804171562195, 'training/entropy_loss': Array(-0.025603, dtype=float32), 'training/policy_loss': Array(0.02355633, dtype=float32), 'training/total_loss': Array(669.17584, dtype=float32), 'training/v_loss': Array(669.17786, dtype=float32), 'eval/episode_goal_distance': (Array(0.67574865, dtype=float32), Array(0.38288838, dtype=float32)), 'eval/episode_reward': (Array(-12611.303, dtype=float32), Array(6419.4575, dtype=float32)), 'eval/avg_episode_length': (Array(914.59375, dtype=float32), Array(278.5395, dtype=float32)), 'eval/epoch_eval_time': 4.451778888702393, 'eval/sps': 28752.551103747544}
I0727 18:40:14.531828 139768182101824 train.py:379] starting iteration 5 199.8009159564972
I0727 18:40:41.757273 139768182101824 train.py:394] {'eval/walltime': 42.8895218372345, 'training/sps': 17961.509909967404, 'training/walltime': 175.71236062049866, 'training/entropy_loss': Array(-0.0198346, dtype=float32), 'training/policy_loss': Array(0.01693067, dtype=float32), 'training/total_loss': Array(134.63136, dtype=float32), 'training/v_loss': Array(134.63426, dtype=float32), 'eval/episode_goal_distance': (Array(0.7745255, dtype=float32), Array(0.4200898, dtype=float32)), 'eval/episode_reward': (Array(-13870.026, dtype=float32), Array(7295.56, dtype=float32)), 'eval/avg_episode_length': (Array(914.65625, dtype=float32), Array(278.33582, dtype=float32)), 'eval/epoch_eval_time': 4.412521123886108, 'eval/sps': 29008.359712342946}
I0727 18:40:41.761048 139768182101824 train.py:379] starting iteration 6 227.0301353931427
I0727 18:41:09.142130 139768182101824 train.py:394] {'eval/walltime': 47.31826400756836, 'training/sps': 17850.68971361343, 'training/walltime': 198.65825271606445, 'training/entropy_loss': Array(-0.01405219, dtype=float32), 'training/policy_loss': Array(0.02260965, dtype=float32), 'training/total_loss': Array(180.51277, dtype=float32), 'training/v_loss': Array(180.50421, dtype=float32), 'eval/episode_goal_distance': (Array(0.42266312, dtype=float32), Array(0.1718132, dtype=float32)), 'eval/episode_reward': (Array(-9295.68, dtype=float32), Array(4080.6226, dtype=float32)), 'eval/avg_episode_length': (Array(945.6406, dtype=float32), Array(226.00545, dtype=float32)), 'eval/epoch_eval_time': 4.428742170333862, 'eval/sps': 28902.111497348844}
I0727 18:41:09.145667 139768182101824 train.py:379] starting iteration 7 254.41475462913513
I0727 18:41:36.560217 139768182101824 train.py:394] {'eval/walltime': 51.741491079330444, 'training/sps': 17820.339491669805, 'training/walltime': 221.64322447776794, 'training/entropy_loss': Array(-0.01482938, dtype=float32), 'training/policy_loss': Array(0.01043664, dtype=float32), 'training/total_loss': Array(230.12537, dtype=float32), 'training/v_loss': Array(230.12976, dtype=float32), 'eval/episode_goal_distance': (Array(0.36626336, dtype=float32), Array(0.12899251, dtype=float32)), 'eval/episode_reward': (Array(-8656.45, dtype=float32), Array(3517.3083, dtype=float32)), 'eval/avg_episode_length': (Array(945.66406, dtype=float32), Array(225.90813, dtype=float32)), 'eval/epoch_eval_time': 4.423227071762085, 'eval/sps': 28938.14808133025}
I0727 18:41:36.563787 139768182101824 train.py:379] starting iteration 8 281.83287477493286
I0727 18:42:03.837749 139768182101824 train.py:394] {'eval/walltime': 56.173988580703735, 'training/sps': 17937.302827421776, 'training/walltime': 244.47831869125366, 'training/entropy_loss': Array(-0.00699044, dtype=float32), 'training/policy_loss': Array(0.00968969, dtype=float32), 'training/total_loss': Array(232.43768, dtype=float32), 'training/v_loss': Array(232.43495, dtype=float32), 'eval/episode_goal_distance': (Array(0.33329204, dtype=float32), Array(0.10567396, dtype=float32)), 'eval/episode_reward': (Array(-8225.84, dtype=float32), Array(3282.7942, dtype=float32)), 'eval/avg_episode_length': (Array(953.47656, dtype=float32), Array(209.78629, dtype=float32)), 'eval/epoch_eval_time': 4.432497501373291, 'eval/sps': 28877.624851529552}
I0727 18:42:03.841310 139768182101824 train.py:379] starting iteration 9 309.11039757728577
I0727 18:42:31.241908 139768182101824 train.py:394] {'eval/walltime': 60.60055732727051, 'training/sps': 17834.05625203139, 'training/walltime': 267.44561195373535, 'training/entropy_loss': Array(0.00553773, dtype=float32), 'training/policy_loss': Array(0.00540381, dtype=float32), 'training/total_loss': Array(235.91737, dtype=float32), 'training/v_loss': Array(235.90643, dtype=float32), 'eval/episode_goal_distance': (Array(0.32518396, dtype=float32), Array(0.11685507, dtype=float32)), 'eval/episode_reward': (Array(-8156.0938, dtype=float32), Array(3543.517, dtype=float32)), 'eval/avg_episode_length': (Array(953.40625, dtype=float32), Array(210.10333, dtype=float32)), 'eval/epoch_eval_time': 4.4265687465667725, 'eval/sps': 28916.302293797253}
I0727 18:42:31.245559 139768182101824 train.py:379] starting iteration 10 336.5146470069885
I0727 18:42:58.799117 139768182101824 train.py:394] {'eval/walltime': 65.01149439811707, 'training/sps': 17703.838357407647, 'training/walltime': 290.58183765411377, 'training/entropy_loss': Array(0.01731889, dtype=float32), 'training/policy_loss': Array(0.00436422, dtype=float32), 'training/total_loss': Array(84.48396, dtype=float32), 'training/v_loss': Array(84.46227, dtype=float32), 'eval/episode_goal_distance': (Array(0.32607237, dtype=float32), Array(0.12835893, dtype=float32)), 'eval/episode_reward': (Array(-8099.1484, dtype=float32), Array(3595.4124, dtype=float32)), 'eval/avg_episode_length': (Array(953.3828, dtype=float32), Array(210.20872, dtype=float32)), 'eval/epoch_eval_time': 4.410937070846558, 'eval/sps': 29018.777176849166}
I0727 18:42:58.802687 139768182101824 train.py:379] starting iteration 11 364.0717749595642
I0727 18:43:26.381542 139768182101824 train.py:394] {'eval/walltime': 69.45593738555908, 'training/sps': 17710.141640358797, 'training/walltime': 313.7098288536072, 'training/entropy_loss': Array(0.02767787, dtype=float32), 'training/policy_loss': Array(0.00471348, dtype=float32), 'training/total_loss': Array(76.475914, dtype=float32), 'training/v_loss': Array(76.44353, dtype=float32), 'eval/episode_goal_distance': (Array(0.3408831, dtype=float32), Array(0.12870649, dtype=float32)), 'eval/episode_reward': (Array(-8423.773, dtype=float32), Array(3787.8374, dtype=float32)), 'eval/avg_episode_length': (Array(945.60156, dtype=float32), Array(226.16762, dtype=float32)), 'eval/epoch_eval_time': 4.444442987442017, 'eval/sps': 28800.00944137883}
I0727 18:43:26.385191 139768182101824 train.py:379] starting iteration 12 391.65427899360657
I0727 18:43:54.030013 139768182101824 train.py:394] {'eval/walltime': 73.89724826812744, 'training/sps': 17657.603050730802, 'training/walltime': 336.9066352844238, 'training/entropy_loss': Array(0.04049246, dtype=float32), 'training/policy_loss': Array(0.00381482, dtype=float32), 'training/total_loss': Array(53.954895, dtype=float32), 'training/v_loss': Array(53.910587, dtype=float32), 'eval/episode_goal_distance': (Array(0.37619928, dtype=float32), Array(0.16709743, dtype=float32)), 'eval/episode_reward': (Array(-8595.955, dtype=float32), Array(4937.5835, dtype=float32)), 'eval/avg_episode_length': (Array(883.5, dtype=float32), Array(319.75696, dtype=float32)), 'eval/epoch_eval_time': 4.441310882568359, 'eval/sps': 28820.31980746618}
I0727 18:43:54.033670 139768182101824 train.py:379] starting iteration 13 419.3027572631836
I0727 18:44:21.663579 139768182101824 train.py:394] {'eval/walltime': 78.33267092704773, 'training/sps': 17664.185649496027, 'training/walltime': 360.094797372818, 'training/entropy_loss': Array(0.04603392, dtype=float32), 'training/policy_loss': Array(0.01055279, dtype=float32), 'training/total_loss': Array(84.06624, dtype=float32), 'training/v_loss': Array(84.00966, dtype=float32), 'eval/episode_goal_distance': (Array(0.5389474, dtype=float32), Array(0.37215078, dtype=float32)), 'eval/episode_reward': (Array(-11567.703, dtype=float32), Array(7261.9106, dtype=float32)), 'eval/avg_episode_length': (Array(922.33594, dtype=float32), Array(266.7852, dtype=float32)), 'eval/epoch_eval_time': 4.435422658920288, 'eval/sps': 28858.5800820071}
I0727 18:44:21.667166 139768182101824 train.py:379] starting iteration 14 446.93625354766846
I0727 18:44:49.328123 139768182101824 train.py:394] {'eval/walltime': 82.77686357498169, 'training/sps': 17647.453826415713, 'training/walltime': 383.30494451522827, 'training/entropy_loss': Array(0.0599121, dtype=float32), 'training/policy_loss': Array(0.0030716, dtype=float32), 'training/total_loss': Array(94.849976, dtype=float32), 'training/v_loss': Array(94.786995, dtype=float32), 'eval/episode_goal_distance': (Array(0.54443693, dtype=float32), Array(0.35369506, dtype=float32)), 'eval/episode_reward': (Array(-12163.369, dtype=float32), Array(6670.262, dtype=float32)), 'eval/avg_episode_length': (Array(953.4531, dtype=float32), Array(209.89182, dtype=float32)), 'eval/epoch_eval_time': 4.44419264793396, 'eval/sps': 28801.63173383254}
I0727 18:44:49.331793 139768182101824 train.py:379] starting iteration 15 474.60087966918945
I0727 18:45:16.980778 139768182101824 train.py:394] {'eval/walltime': 87.2166748046875, 'training/sps': 17653.044023975235, 'training/walltime': 406.507741689682, 'training/entropy_loss': Array(-0.00325597, dtype=float32), 'training/policy_loss': Array(0.0060838, dtype=float32), 'training/total_loss': Array(156.49889, dtype=float32), 'training/v_loss': Array(156.49606, dtype=float32), 'eval/episode_goal_distance': (Array(0.5361949, dtype=float32), Array(0.34242165, dtype=float32)), 'eval/episode_reward': (Array(-12005.977, dtype=float32), Array(6147.7183, dtype=float32)), 'eval/avg_episode_length': (Array(961.14844, dtype=float32), Array(192.69771, dtype=float32)), 'eval/epoch_eval_time': 4.4398112297058105, 'eval/sps': 28830.054562585872}
I0727 18:45:16.984375 139768182101824 train.py:379] starting iteration 16 502.2534625530243
I0727 18:45:44.654739 139768182101824 train.py:394] {'eval/walltime': 91.66666436195374, 'training/sps': 17644.648275804837, 'training/walltime': 429.7215793132782, 'training/entropy_loss': Array(0.00057122, dtype=float32), 'training/policy_loss': Array(0.0064706, dtype=float32), 'training/total_loss': Array(154.40508, dtype=float32), 'training/v_loss': Array(154.39803, dtype=float32), 'eval/episode_goal_distance': (Array(0.64218915, dtype=float32), Array(0.35397217, dtype=float32)), 'eval/episode_reward': (Array(-13520.025, dtype=float32), Array(5747.342, dtype=float32)), 'eval/avg_episode_length': (Array(976.6797, dtype=float32), Array(150.53201, dtype=float32)), 'eval/epoch_eval_time': 4.449989557266235, 'eval/sps': 28764.112444037804}
I0727 18:45:44.658452 139768182101824 train.py:379] starting iteration 17 529.9275398254395
I0727 18:46:12.320680 139768182101824 train.py:394] {'eval/walltime': 96.11123847961426, 'training/sps': 17646.656784600575, 'training/walltime': 452.9327747821808, 'training/entropy_loss': Array(0.00221773, dtype=float32), 'training/policy_loss': Array(0.00583936, dtype=float32), 'training/total_loss': Array(171.80075, dtype=float32), 'training/v_loss': Array(171.79268, dtype=float32), 'eval/episode_goal_distance': (Array(0.603918, dtype=float32), Array(0.33445108, dtype=float32)), 'eval/episode_reward': (Array(-12592.201, dtype=float32), Array(5942.9727, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.08266, dtype=float32)), 'eval/epoch_eval_time': 4.4445741176605225, 'eval/sps': 28799.15974207558}
I0727 18:46:12.324232 139768182101824 train.py:379] starting iteration 18 557.5933203697205
I0727 18:46:39.970499 139768182101824 train.py:394] {'eval/walltime': 100.53320574760437, 'training/sps': 17641.715173880042, 'training/walltime': 476.1504719257355, 'training/entropy_loss': Array(0.00609919, dtype=float32), 'training/policy_loss': Array(0.0070666, dtype=float32), 'training/total_loss': Array(185.9205, dtype=float32), 'training/v_loss': Array(185.90735, dtype=float32), 'eval/episode_goal_distance': (Array(0.5472802, dtype=float32), Array(0.2621816, dtype=float32)), 'eval/episode_reward': (Array(-12042.104, dtype=float32), Array(5237.53, dtype=float32)), 'eval/avg_episode_length': (Array(953.40625, dtype=float32), Array(210.10324, dtype=float32)), 'eval/epoch_eval_time': 4.421967267990112, 'eval/sps': 28946.392463501656}
I0727 18:46:39.974292 139768182101824 train.py:379] starting iteration 19 585.2433803081512
I0727 18:47:07.610808 139768182101824 train.py:394] {'eval/walltime': 104.94610142707825, 'training/sps': 17642.251060756123, 'training/walltime': 499.3674638271332, 'training/entropy_loss': Array(0.01186856, dtype=float32), 'training/policy_loss': Array(0.0087108, dtype=float32), 'training/total_loss': Array(199.67532, dtype=float32), 'training/v_loss': Array(199.65472, dtype=float32), 'eval/episode_goal_distance': (Array(0.6009198, dtype=float32), Array(0.334036, dtype=float32)), 'eval/episode_reward': (Array(-12574.244, dtype=float32), Array(5805.0835, dtype=float32)), 'eval/avg_episode_length': (Array(945.6406, dtype=float32), Array(226.00543, dtype=float32)), 'eval/epoch_eval_time': 4.412895679473877, 'eval/sps': 29005.897555063137}
I0727 18:47:07.614617 139768182101824 train.py:379] starting iteration 20 612.8837053775787
I0727 18:47:35.273750 139768182101824 train.py:394] {'eval/walltime': 109.37863230705261, 'training/sps': 17639.9177086439, 'training/walltime': 522.5875267982483, 'training/entropy_loss': Array(-0.04577303, dtype=float32), 'training/policy_loss': Array(0.00396054, dtype=float32), 'training/total_loss': Array(176.67648, dtype=float32), 'training/v_loss': Array(176.71829, dtype=float32), 'eval/episode_goal_distance': (Array(0.65434957, dtype=float32), Array(0.34416673, dtype=float32)), 'eval/episode_reward': (Array(-12870.959, dtype=float32), Array(5926.772, dtype=float32)), 'eval/avg_episode_length': (Array(937.7969, dtype=float32), Array(240.91179, dtype=float32)), 'eval/epoch_eval_time': 4.432530879974365, 'eval/sps': 28877.40739230682}
I0727 18:47:35.277314 139768182101824 train.py:379] starting iteration 21 640.5464022159576
I0727 18:48:02.896320 139768182101824 train.py:394] {'eval/walltime': 113.79426431655884, 'training/sps': 17657.602869244587, 'training/walltime': 545.7843334674835, 'training/entropy_loss': Array(-0.04820353, dtype=float32), 'training/policy_loss': Array(0.00260996, dtype=float32), 'training/total_loss': Array(175.57205, dtype=float32), 'training/v_loss': Array(175.61766, dtype=float32), 'eval/episode_goal_distance': (Array(0.62071145, dtype=float32), Array(0.3168244, dtype=float32)), 'eval/episode_reward': (Array(-12829.779, dtype=float32), Array(5998.073, dtype=float32)), 'eval/avg_episode_length': (Array(945.60156, dtype=float32), Array(226.16757, dtype=float32)), 'eval/epoch_eval_time': 4.415632009506226, 'eval/sps': 28987.922844212168}
I0727 18:48:02.899833 139768182101824 train.py:379] starting iteration 22 668.1689209938049
I0727 18:48:30.549496 139768182101824 train.py:394] {'eval/walltime': 118.20294976234436, 'training/sps': 17628.871607765745, 'training/walltime': 569.0189459323883, 'training/entropy_loss': Array(-0.04913633, dtype=float32), 'training/policy_loss': Array(0.0022093, dtype=float32), 'training/total_loss': Array(182.52509, dtype=float32), 'training/v_loss': Array(182.57202, dtype=float32), 'eval/episode_goal_distance': (Array(0.59329593, dtype=float32), Array(0.3344571, dtype=float32)), 'eval/episode_reward': (Array(-12368.6045, dtype=float32), Array(5699.4614, dtype=float32)), 'eval/avg_episode_length': (Array(961.21094, dtype=float32), Array(192.38773, dtype=float32)), 'eval/epoch_eval_time': 4.4086854457855225, 'eval/sps': 29033.597786469763}
I0727 18:48:30.553021 139768182101824 train.py:379] starting iteration 23 695.822108745575
I0727 18:48:58.208664 139768182101824 train.py:394] {'eval/walltime': 122.6378927230835, 'training/sps': 17644.375543899976, 'training/walltime': 592.233142375946, 'training/entropy_loss': Array(-0.04830311, dtype=float32), 'training/policy_loss': Array(0.00220696, dtype=float32), 'training/total_loss': Array(192.84407, dtype=float32), 'training/v_loss': Array(192.89017, dtype=float32), 'eval/episode_goal_distance': (Array(0.5985304, dtype=float32), Array(0.32587487, dtype=float32)), 'eval/episode_reward': (Array(-13201.094, dtype=float32), Array(6415.9556, dtype=float32)), 'eval/avg_episode_length': (Array(961.2031, dtype=float32), Array(192.42683, dtype=float32)), 'eval/epoch_eval_time': 4.434942960739136, 'eval/sps': 28861.701522011746}
I0727 18:48:58.212249 139768182101824 train.py:379] starting iteration 24 723.4813373088837
I0727 18:49:25.844588 139768182101824 train.py:394] {'eval/walltime': 127.06106519699097, 'training/sps': 17653.218888089257, 'training/walltime': 615.4357097148895, 'training/entropy_loss': Array(-0.04869216, dtype=float32), 'training/policy_loss': Array(0.00154848, dtype=float32), 'training/total_loss': Array(203.08887, dtype=float32), 'training/v_loss': Array(203.136, dtype=float32), 'eval/episode_goal_distance': (Array(0.57893836, dtype=float32), Array(0.30477014, dtype=float32)), 'eval/episode_reward': (Array(-12280.432, dtype=float32), Array(5403.0195, dtype=float32)), 'eval/avg_episode_length': (Array(968.96875, dtype=float32), Array(172.77472, dtype=float32)), 'eval/epoch_eval_time': 4.423172473907471, 'eval/sps': 28938.505282142803}
I0727 18:49:25.848119 139768182101824 train.py:379] starting iteration 25 751.1172068119049
I0727 18:49:53.495639 139768182101824 train.py:394] {'eval/walltime': 131.47536206245422, 'training/sps': 17634.80319579146, 'training/walltime': 638.6625070571899, 'training/entropy_loss': Array(-0.04760517, dtype=float32), 'training/policy_loss': Array(0.00693369, dtype=float32), 'training/total_loss': Array(152.54204, dtype=float32), 'training/v_loss': Array(152.58272, dtype=float32), 'eval/episode_goal_distance': (Array(0.5752348, dtype=float32), Array(0.3022366, dtype=float32)), 'eval/episode_reward': (Array(-12173.307, dtype=float32), Array(5666.473, dtype=float32)), 'eval/avg_episode_length': (Array(953.34375, dtype=float32), Array(210.38475, dtype=float32)), 'eval/epoch_eval_time': 4.414296865463257, 'eval/sps': 28996.69050386059}
I0727 18:49:53.543888 139768182101824 train.py:379] starting iteration 26 778.8129663467407
I0727 18:50:21.175017 139768182101824 train.py:394] {'eval/walltime': 135.88993501663208, 'training/sps': 17647.778681939417, 'training/walltime': 661.8722269535065, 'training/entropy_loss': Array(-0.05033593, dtype=float32), 'training/policy_loss': Array(0.00238166, dtype=float32), 'training/total_loss': Array(159.01425, dtype=float32), 'training/v_loss': Array(159.06223, dtype=float32), 'eval/episode_goal_distance': (Array(0.55506146, dtype=float32), Array(0.29903284, dtype=float32)), 'eval/episode_reward': (Array(-12364.58, dtype=float32), Array(5182.174, dtype=float32)), 'eval/avg_episode_length': (Array(984.4297, dtype=float32), Array(123.58554, dtype=float32)), 'eval/epoch_eval_time': 4.4145729541778564, 'eval/sps': 28994.877042152755}
I0727 18:50:21.178899 139768182101824 train.py:379] starting iteration 27 806.447986125946
I0727 18:50:48.823192 139768182101824 train.py:394] {'eval/walltime': 140.30371046066284, 'training/sps': 17637.18752309573, 'training/walltime': 685.0958843231201, 'training/entropy_loss': Array(-0.05117984, dtype=float32), 'training/policy_loss': Array(0.00165643, dtype=float32), 'training/total_loss': Array(177.8182, dtype=float32), 'training/v_loss': Array(177.86772, dtype=float32), 'eval/episode_goal_distance': (Array(0.6155538, dtype=float32), Array(0.31187293, dtype=float32)), 'eval/episode_reward': (Array(-12271.419, dtype=float32), Array(6210.052, dtype=float32)), 'eval/avg_episode_length': (Array(899., dtype=float32), Array(300.39963, dtype=float32)), 'eval/epoch_eval_time': 4.413775444030762, 'eval/sps': 29000.11602835586}
I0727 18:50:48.826828 139768182101824 train.py:379] starting iteration 28 834.095915555954
