I0726 22:29:35.541993 140267183036224 low_level_env.py:187] Initialising environment...
I0726 22:30:07.740461 140267183036224 low_level_env.py:289] Environment initialised.
I0726 22:30:07.745165 140267183036224 train.py:118] JAX is running on GPU.
I0726 22:30:07.745198 140267183036224 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0726 22:30:12.522317 140267183036224 train.py:367] Running initial eval
I0726 22:30:28.242712 140267183036224 train.py:373] {'eval/walltime': 15.718005180358887, 'eval/episode_goal_distance': (Array(0.3502965, dtype=float32), Array(0.08533146, dtype=float32)), 'eval/episode_reward': (Array(-19181.844, dtype=float32), Array(8938.111, dtype=float32)), 'eval/avg_episode_length': (Array(899.02344, dtype=float32), Array(300.33, dtype=float32)), 'eval/epoch_eval_time': 15.718005180358887, 'eval/sps': 8143.527027204949}
I0726 22:30:28.244026 140267183036224 train.py:379] starting iteration 0 20.498872756958008
I0726 22:30:55.413196 140267183036224 train.py:394] {'eval/walltime': 19.582303285598755, 'training/sps': 5273.7200284733935, 'training/walltime': 23.300440549850464, 'training/entropy_loss': Array(-0.04731485, dtype=float32), 'training/policy_loss': Array(0.02182545, dtype=float32), 'training/total_loss': Array(40577.414, dtype=float32), 'training/v_loss': Array(40577.438, dtype=float32), 'eval/episode_goal_distance': (Array(0.34489623, dtype=float32), Array(0.07305742, dtype=float32)), 'eval/episode_reward': (Array(-18676.008, dtype=float32), Array(7560.9185, dtype=float32)), 'eval/avg_episode_length': (Array(914.6797, dtype=float32), Array(278.25928, dtype=float32)), 'eval/epoch_eval_time': 3.864298105239868, 'eval/sps': 33123.7385196644}
I0726 22:30:55.439957 140267183036224 train.py:379] starting iteration 1 47.69480109214783
I0726 22:31:02.288570 140267183036224 train.py:394] {'eval/walltime': 23.447125911712646, 'training/sps': 41234.96504602046, 'training/walltime': 26.280435800552368, 'training/entropy_loss': Array(-0.04681282, dtype=float32), 'training/policy_loss': Array(0.00467801, dtype=float32), 'training/total_loss': Array(38358.76, dtype=float32), 'training/v_loss': Array(38358.805, dtype=float32), 'eval/episode_goal_distance': (Array(0.33572683, dtype=float32), Array(0.07236488, dtype=float32)), 'eval/episode_reward': (Array(-17414.94, dtype=float32), Array(8225.195, dtype=float32)), 'eval/avg_episode_length': (Array(860.2031, dtype=float32), Array(345.5879, dtype=float32)), 'eval/epoch_eval_time': 3.8648226261138916, 'eval/sps': 33119.243076028295}
I0726 22:31:02.291202 140267183036224 train.py:379] starting iteration 2 54.54604744911194
I0726 22:31:09.136453 140267183036224 train.py:394] {'eval/walltime': 27.31052827835083, 'training/sps': 41257.95580722157, 'training/walltime': 29.25877046585083, 'training/entropy_loss': Array(-0.04639889, dtype=float32), 'training/policy_loss': Array(0.00367867, dtype=float32), 'training/total_loss': Array(34621.25, dtype=float32), 'training/v_loss': Array(34621.29, dtype=float32), 'eval/episode_goal_distance': (Array(0.33666405, dtype=float32), Array(0.07379717, dtype=float32)), 'eval/episode_reward': (Array(-17729.133, dtype=float32), Array(7301.7607, dtype=float32)), 'eval/avg_episode_length': (Array(906.8203, dtype=float32), Array(289.70822, dtype=float32)), 'eval/epoch_eval_time': 3.8634023666381836, 'eval/sps': 33131.41833357154}
I0726 22:31:09.138746 140267183036224 train.py:379] starting iteration 3 61.39359211921692
I0726 22:31:16.004863 140267183036224 train.py:394] {'eval/walltime': 31.174856901168823, 'training/sps': 40982.44067321129, 'training/walltime': 32.25712776184082, 'training/entropy_loss': Array(-0.0455162, dtype=float32), 'training/policy_loss': Array(0.00558563, dtype=float32), 'training/total_loss': Array(29924.316, dtype=float32), 'training/v_loss': Array(29924.36, dtype=float32), 'eval/episode_goal_distance': (Array(0.31885028, dtype=float32), Array(0.06293256, dtype=float32)), 'eval/episode_reward': (Array(-15614.027, dtype=float32), Array(7029.3315, dtype=float32)), 'eval/avg_episode_length': (Array(868., dtype=float32), Array(337.2962, dtype=float32)), 'eval/epoch_eval_time': 3.864328622817993, 'eval/sps': 33123.47693314402}
I0726 22:31:16.007090 140267183036224 train.py:379] starting iteration 4 68.26193594932556
I0726 22:31:22.894762 140267183036224 train.py:394] {'eval/walltime': 35.040881872177124, 'training/sps': 40715.083293156444, 'training/walltime': 35.2751739025116, 'training/entropy_loss': Array(-0.04435461, dtype=float32), 'training/policy_loss': Array(0.00976591, dtype=float32), 'training/total_loss': Array(24411.188, dtype=float32), 'training/v_loss': Array(24411.219, dtype=float32), 'eval/episode_goal_distance': (Array(0.3208256, dtype=float32), Array(0.06364813, dtype=float32)), 'eval/episode_reward': (Array(-16148.655, dtype=float32), Array(7379.433, dtype=float32)), 'eval/avg_episode_length': (Array(868., dtype=float32), Array(337.29633, dtype=float32)), 'eval/epoch_eval_time': 3.866024971008301, 'eval/sps': 33108.94289609729}
I0726 22:31:22.897278 140267183036224 train.py:379] starting iteration 5 75.15212345123291
I0726 22:31:29.780416 140267183036224 train.py:394] {'eval/walltime': 38.912524938583374, 'training/sps': 40853.45495083597, 'training/walltime': 38.282997846603394, 'training/entropy_loss': Array(-0.042825, dtype=float32), 'training/policy_loss': Array(0.01034995, dtype=float32), 'training/total_loss': Array(18369.629, dtype=float32), 'training/v_loss': Array(18369.662, dtype=float32), 'eval/episode_goal_distance': (Array(0.3235037, dtype=float32), Array(0.0614156, dtype=float32)), 'eval/episode_reward': (Array(-16086.618, dtype=float32), Array(7555.243, dtype=float32)), 'eval/avg_episode_length': (Array(868.0156, dtype=float32), Array(337.25632, dtype=float32)), 'eval/epoch_eval_time': 3.87164306640625, 'eval/sps': 33060.89890120285}
I0726 22:31:29.782787 140267183036224 train.py:379] starting iteration 6 82.03763270378113
I0726 22:31:36.695609 140267183036224 train.py:394] {'eval/walltime': 42.79039025306702, 'training/sps': 40536.22707255426, 'training/walltime': 41.31436038017273, 'training/entropy_loss': Array(-0.04069665, dtype=float32), 'training/policy_loss': Array(0.01515267, dtype=float32), 'training/total_loss': Array(12823.378, dtype=float32), 'training/v_loss': Array(12823.404, dtype=float32), 'eval/episode_goal_distance': (Array(0.3174314, dtype=float32), Array(0.06663956, dtype=float32)), 'eval/episode_reward': (Array(-16557.486, dtype=float32), Array(6327.9297, dtype=float32)), 'eval/avg_episode_length': (Array(914.625, dtype=float32), Array(278.43808, dtype=float32)), 'eval/epoch_eval_time': 3.8778653144836426, 'eval/sps': 33007.85087143849}
I0726 22:31:36.698027 140267183036224 train.py:379] starting iteration 7 88.95287275314331
I0726 22:31:43.596130 140267183036224 train.py:394] {'eval/walltime': 46.661664724349976, 'training/sps': 40646.35255119713, 'training/walltime': 44.337509870529175, 'training/entropy_loss': Array(-0.03835919, dtype=float32), 'training/policy_loss': Array(0.01349805, dtype=float32), 'training/total_loss': Array(8588.107, dtype=float32), 'training/v_loss': Array(8588.133, dtype=float32), 'eval/episode_goal_distance': (Array(0.32584375, dtype=float32), Array(0.06561206, dtype=float32)), 'eval/episode_reward': (Array(-16439.162, dtype=float32), Array(6857.132, dtype=float32)), 'eval/avg_episode_length': (Array(891.27344, dtype=float32), Array(310.2594, dtype=float32)), 'eval/epoch_eval_time': 3.871274471282959, 'eval/sps': 33064.04672401856}
I0726 22:31:43.598561 140267183036224 train.py:379] starting iteration 8 95.85340690612793
I0726 22:31:50.534737 140267183036224 train.py:394] {'eval/walltime': 50.56341505050659, 'training/sps': 40542.32701671097, 'training/walltime': 47.36841630935669, 'training/entropy_loss': Array(-0.03675648, dtype=float32), 'training/policy_loss': Array(0.01567153, dtype=float32), 'training/total_loss': Array(6006.2217, dtype=float32), 'training/v_loss': Array(6006.242, dtype=float32), 'eval/episode_goal_distance': (Array(0.30977288, dtype=float32), Array(0.06245931, dtype=float32)), 'eval/episode_reward': (Array(-15317.053, dtype=float32), Array(6813.2583, dtype=float32)), 'eval/avg_episode_length': (Array(875.84375, dtype=float32), Array(328.4871, dtype=float32)), 'eval/epoch_eval_time': 3.901750326156616, 'eval/sps': 32805.78953038372}
I0726 22:31:50.537359 140267183036224 train.py:379] starting iteration 9 102.79220509529114
I0726 22:31:57.483821 140267183036224 train.py:394] {'eval/walltime': 54.4702353477478, 'training/sps': 40477.273849769896, 'training/walltime': 50.40419387817383, 'training/entropy_loss': Array(-0.03444387, dtype=float32), 'training/policy_loss': Array(0.01878464, dtype=float32), 'training/total_loss': Array(4215.62, dtype=float32), 'training/v_loss': Array(4215.6357, dtype=float32), 'eval/episode_goal_distance': (Array(0.31709647, dtype=float32), Array(0.06394827, dtype=float32)), 'eval/episode_reward': (Array(-15023.427, dtype=float32), Array(7301.397, dtype=float32)), 'eval/avg_episode_length': (Array(844.7422, dtype=float32), Array(360.78748, dtype=float32)), 'eval/epoch_eval_time': 3.906820297241211, 'eval/sps': 32763.216698343356}
I0726 22:31:57.486157 140267183036224 train.py:379] starting iteration 10 109.74100375175476
I0726 22:32:04.483420 140267183036224 train.py:394] {'eval/walltime': 58.41731333732605, 'training/sps': 40332.784434963825, 'training/walltime': 53.450846910476685, 'training/entropy_loss': Array(-0.0333324, dtype=float32), 'training/policy_loss': Array(0.02023658, dtype=float32), 'training/total_loss': Array(2878.091, dtype=float32), 'training/v_loss': Array(2878.104, dtype=float32), 'eval/episode_goal_distance': (Array(0.30916807, dtype=float32), Array(0.06291506, dtype=float32)), 'eval/episode_reward': (Array(-15557.053, dtype=float32), Array(6409.4824, dtype=float32)), 'eval/avg_episode_length': (Array(891.33594, dtype=float32), Array(310.081, dtype=float32)), 'eval/epoch_eval_time': 3.947077989578247, 'eval/sps': 32429.052665786585}
I0726 22:32:04.485745 140267183036224 train.py:379] starting iteration 11 116.74059128761292
I0726 22:32:11.507572 140267183036224 train.py:394] {'eval/walltime': 62.38769841194153, 'training/sps': 40317.82606879428, 'training/walltime': 56.49863028526306, 'training/entropy_loss': Array(-0.03358784, dtype=float32), 'training/policy_loss': Array(0.02816494, dtype=float32), 'training/total_loss': Array(2242.9214, dtype=float32), 'training/v_loss': Array(2242.9268, dtype=float32), 'eval/episode_goal_distance': (Array(0.32298058, dtype=float32), Array(0.06311619, dtype=float32)), 'eval/episode_reward': (Array(-15232.239, dtype=float32), Array(6993.6855, dtype=float32)), 'eval/avg_episode_length': (Array(860.21094, dtype=float32), Array(345.56857, dtype=float32)), 'eval/epoch_eval_time': 3.9703850746154785, 'eval/sps': 32238.686574348576}
I0726 22:32:11.510000 140267183036224 train.py:379] starting iteration 12 123.76484632492065
I0726 22:32:18.541077 140267183036224 train.py:394] {'eval/walltime': 66.36015200614929, 'training/sps': 40221.718685409374, 'training/walltime': 59.553696155548096, 'training/entropy_loss': Array(-0.03235968, dtype=float32), 'training/policy_loss': Array(0.02524112, dtype=float32), 'training/total_loss': Array(2017.8212, dtype=float32), 'training/v_loss': Array(2017.8281, dtype=float32), 'eval/episode_goal_distance': (Array(0.30366486, dtype=float32), Array(0.06468663, dtype=float32)), 'eval/episode_reward': (Array(-13842.459, dtype=float32), Array(7648.1694, dtype=float32)), 'eval/avg_episode_length': (Array(798.0625, dtype=float32), Array(399.97327, dtype=float32)), 'eval/epoch_eval_time': 3.9724535942077637, 'eval/sps': 32221.8993789221}
I0726 22:32:18.543392 140267183036224 train.py:379] starting iteration 13 130.79823803901672
I0726 22:32:25.600595 140267183036224 train.py:394] {'eval/walltime': 70.36038136482239, 'training/sps': 40243.50516259234, 'training/walltime': 62.6071081161499, 'training/entropy_loss': Array(-0.03010973, dtype=float32), 'training/policy_loss': Array(0.03505404, dtype=float32), 'training/total_loss': Array(1864.0331, dtype=float32), 'training/v_loss': Array(1864.0281, dtype=float32), 'eval/episode_goal_distance': (Array(0.30795002, dtype=float32), Array(0.06448693, dtype=float32)), 'eval/episode_reward': (Array(-13637.555, dtype=float32), Array(7352.67, dtype=float32)), 'eval/avg_episode_length': (Array(798.0469, dtype=float32), Array(400.0047, dtype=float32)), 'eval/epoch_eval_time': 4.000229358673096, 'eval/sps': 31998.165235820004}
I0726 22:32:25.602996 140267183036224 train.py:379] starting iteration 14 137.85784149169922
I0726 22:32:32.658845 140267183036224 train.py:394] {'eval/walltime': 74.35027289390564, 'training/sps': 40123.20389258764, 'training/walltime': 65.66967511177063, 'training/entropy_loss': Array(-0.02772973, dtype=float32), 'training/policy_loss': Array(0.03877594, dtype=float32), 'training/total_loss': Array(1711.2998, dtype=float32), 'training/v_loss': Array(1711.2888, dtype=float32), 'eval/episode_goal_distance': (Array(0.30624628, dtype=float32), Array(0.05637999, dtype=float32)), 'eval/episode_reward': (Array(-14383.426, dtype=float32), Array(6639.9375, dtype=float32)), 'eval/avg_episode_length': (Array(852.52344, dtype=float32), Array(353.23233, dtype=float32)), 'eval/epoch_eval_time': 3.989891529083252, 'eval/sps': 32081.072647458728}
I0726 22:32:32.661171 140267183036224 train.py:379] starting iteration 15 144.91601729393005
I0726 22:32:39.714614 140267183036224 train.py:394] {'eval/walltime': 78.3478033542633, 'training/sps': 40258.933584751794, 'training/walltime': 68.7219169139862, 'training/entropy_loss': Array(-0.0259521, dtype=float32), 'training/policy_loss': Array(0.04056089, dtype=float32), 'training/total_loss': Array(1592.0142, dtype=float32), 'training/v_loss': Array(1591.9995, dtype=float32), 'eval/episode_goal_distance': (Array(0.3033677, dtype=float32), Array(0.0676507, dtype=float32)), 'eval/episode_reward': (Array(-14675.723, dtype=float32), Array(6269.267, dtype=float32)), 'eval/avg_episode_length': (Array(875.6875, dtype=float32), Array(328.90057, dtype=float32)), 'eval/epoch_eval_time': 3.997530460357666, 'eval/sps': 32019.768521925813}
I0726 22:32:39.717230 140267183036224 train.py:379] starting iteration 16 151.9720757007599
I0726 22:32:46.789417 140267183036224 train.py:394] {'eval/walltime': 82.3692409992218, 'training/sps': 40327.0502924555, 'training/walltime': 71.76900315284729, 'training/entropy_loss': Array(-0.02514413, dtype=float32), 'training/policy_loss': Array(0.04004962, dtype=float32), 'training/total_loss': Array(2077.7021, dtype=float32), 'training/v_loss': Array(2077.6873, dtype=float32), 'eval/episode_goal_distance': (Array(0.29736128, dtype=float32), Array(0.05676177, dtype=float32)), 'eval/episode_reward': (Array(-14792.699, dtype=float32), Array(5772.389, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.30652, dtype=float32)), 'eval/epoch_eval_time': 4.021437644958496, 'eval/sps': 31829.413085757555}
I0726 22:32:46.791769 140267183036224 train.py:379] starting iteration 17 159.04661512374878
I0726 22:32:53.905903 140267183036224 train.py:394] {'eval/walltime': 86.42305421829224, 'training/sps': 40200.02476602456, 'training/walltime': 74.82571768760681, 'training/entropy_loss': Array(-0.02265471, dtype=float32), 'training/policy_loss': Array(0.0392914, dtype=float32), 'training/total_loss': Array(1294.9436, dtype=float32), 'training/v_loss': Array(1294.9268, dtype=float32), 'eval/episode_goal_distance': (Array(0.309326, dtype=float32), Array(0.05765484, dtype=float32)), 'eval/episode_reward': (Array(-14351.965, dtype=float32), Array(6847.6284, dtype=float32)), 'eval/avg_episode_length': (Array(844.7578, dtype=float32), Array(360.75122, dtype=float32)), 'eval/epoch_eval_time': 4.053813219070435, 'eval/sps': 31575.20908902439}
I0726 22:32:53.908256 140267183036224 train.py:379] starting iteration 18 166.1631019115448
I0726 22:33:01.018937 140267183036224 train.py:394] {'eval/walltime': 90.47012376785278, 'training/sps': 40158.462378893535, 'training/walltime': 77.88559579849243, 'training/entropy_loss': Array(-0.02078907, dtype=float32), 'training/policy_loss': Array(0.05315321, dtype=float32), 'training/total_loss': Array(861.7331, dtype=float32), 'training/v_loss': Array(861.7008, dtype=float32), 'eval/episode_goal_distance': (Array(0.31907904, dtype=float32), Array(0.05711763, dtype=float32)), 'eval/episode_reward': (Array(-15241.835, dtype=float32), Array(6876.988, dtype=float32)), 'eval/avg_episode_length': (Array(860.15625, dtype=float32), Array(345.70367, dtype=float32)), 'eval/epoch_eval_time': 4.047069549560547, 'eval/sps': 31627.82315265596}
I0726 22:33:01.021324 140267183036224 train.py:379] starting iteration 19 173.27617001533508
I0726 22:33:08.125954 140267183036224 train.py:394] {'eval/walltime': 94.51549196243286, 'training/sps': 40214.356128846404, 'training/walltime': 80.94122099876404, 'training/entropy_loss': Array(-0.01779918, dtype=float32), 'training/policy_loss': Array(0.07492007, dtype=float32), 'training/total_loss': Array(802.38855, dtype=float32), 'training/v_loss': Array(802.33136, dtype=float32), 'eval/episode_goal_distance': (Array(0.30862868, dtype=float32), Array(0.06387083, dtype=float32)), 'eval/episode_reward': (Array(-15588.403, dtype=float32), Array(6685.571, dtype=float32)), 'eval/avg_episode_length': (Array(883.4844, dtype=float32), Array(319.80002, dtype=float32)), 'eval/epoch_eval_time': 4.045368194580078, 'eval/sps': 31641.124823073565}
I0726 22:33:08.128412 140267183036224 train.py:379] starting iteration 20 180.38325762748718
I0726 22:33:15.230535 140267183036224 train.py:394] {'eval/walltime': 98.5652003288269, 'training/sps': 40305.03460323833, 'training/walltime': 83.98997163772583, 'training/entropy_loss': Array(-0.01547607, dtype=float32), 'training/policy_loss': Array(0.06402224, dtype=float32), 'training/total_loss': Array(795.89154, dtype=float32), 'training/v_loss': Array(795.843, dtype=float32), 'eval/episode_goal_distance': (Array(0.3089579, dtype=float32), Array(0.06348954, dtype=float32)), 'eval/episode_reward': (Array(-15785.021, dtype=float32), Array(7058.4956, dtype=float32)), 'eval/avg_episode_length': (Array(875.6719, dtype=float32), Array(328.94177, dtype=float32)), 'eval/epoch_eval_time': 4.049708366394043, 'eval/sps': 31607.21425330048}
I0726 22:33:15.233089 140267183036224 train.py:379] starting iteration 21 187.487934589386
I0726 22:33:22.344288 140267183036224 train.py:394] {'eval/walltime': 102.61673283576965, 'training/sps': 40208.19135562552, 'training/walltime': 87.04606533050537, 'training/entropy_loss': Array(-0.0147375, dtype=float32), 'training/policy_loss': Array(0.08063048, dtype=float32), 'training/total_loss': Array(770.1909, dtype=float32), 'training/v_loss': Array(770.125, dtype=float32), 'eval/episode_goal_distance': (Array(0.31257325, dtype=float32), Array(0.06106221, dtype=float32)), 'eval/episode_reward': (Array(-16540.273, dtype=float32), Array(6131.26, dtype=float32)), 'eval/avg_episode_length': (Array(914.58594, dtype=float32), Array(278.5651, dtype=float32)), 'eval/epoch_eval_time': 4.051532506942749, 'eval/sps': 31592.983588471237}
I0726 22:33:22.346602 140267183036224 train.py:379] starting iteration 22 194.60144805908203
I0726 22:33:29.463439 140267183036224 train.py:394] {'eval/walltime': 106.66774463653564, 'training/sps': 40129.35512865473, 'training/walltime': 90.10816287994385, 'training/entropy_loss': Array(-0.01592545, dtype=float32), 'training/policy_loss': Array(0.04168668, dtype=float32), 'training/total_loss': Array(760.3777, dtype=float32), 'training/v_loss': Array(760.3519, dtype=float32), 'eval/episode_goal_distance': (Array(0.31593162, dtype=float32), Array(0.06530961, dtype=float32)), 'eval/episode_reward': (Array(-15937.787, dtype=float32), Array(6862.7065, dtype=float32)), 'eval/avg_episode_length': (Array(883.53906, dtype=float32), Array(319.6499, dtype=float32)), 'eval/epoch_eval_time': 4.051011800765991, 'eval/sps': 31597.044465730003}
I0726 22:33:29.465815 140267183036224 train.py:379] starting iteration 23 201.7206609249115
I0726 22:33:36.556801 140267183036224 train.py:394] {'eval/walltime': 110.71142935752869, 'training/sps': 40372.85212245397, 'training/walltime': 93.15179228782654, 'training/entropy_loss': Array(-0.01250286, dtype=float32), 'training/policy_loss': Array(0.04375836, dtype=float32), 'training/total_loss': Array(756.4914, dtype=float32), 'training/v_loss': Array(756.4601, dtype=float32), 'eval/episode_goal_distance': (Array(0.32053858, dtype=float32), Array(0.06356529, dtype=float32)), 'eval/episode_reward': (Array(-15404.6, dtype=float32), Array(7359.2363, dtype=float32)), 'eval/avg_episode_length': (Array(844.6953, dtype=float32), Array(360.89597, dtype=float32)), 'eval/epoch_eval_time': 4.043684720993042, 'eval/sps': 31654.29770908696}
I0726 22:33:36.559183 140267183036224 train.py:379] starting iteration 24 208.81402945518494
I0726 22:33:43.667365 140267183036224 train.py:394] {'eval/walltime': 114.76447439193726, 'training/sps': 40268.571345529, 'training/walltime': 96.20330357551575, 'training/entropy_loss': Array(-0.00526661, dtype=float32), 'training/policy_loss': Array(0.05670985, dtype=float32), 'training/total_loss': Array(755.31934, dtype=float32), 'training/v_loss': Array(755.2678, dtype=float32), 'eval/episode_goal_distance': (Array(0.3234278, dtype=float32), Array(0.05543052, dtype=float32)), 'eval/episode_reward': (Array(-16405.68, dtype=float32), Array(7022.0645, dtype=float32)), 'eval/avg_episode_length': (Array(875.83594, dtype=float32), Array(328.50772, dtype=float32)), 'eval/epoch_eval_time': 4.053045034408569, 'eval/sps': 31581.193624382733}
I0726 22:33:43.669689 140267183036224 train.py:379] starting iteration 25 215.92453455924988
I0726 22:33:50.779408 140267183036224 train.py:394] {'eval/walltime': 118.811448097229, 'training/sps': 40169.2166872957, 'training/walltime': 99.26236248016357, 'training/entropy_loss': Array(-0.00173933, dtype=float32), 'training/policy_loss': Array(0.04328671, dtype=float32), 'training/total_loss': Array(758.2571, dtype=float32), 'training/v_loss': Array(758.2155, dtype=float32), 'eval/episode_goal_distance': (Array(0.32577807, dtype=float32), Array(0.07847597, dtype=float32)), 'eval/episode_reward': (Array(-16575.28, dtype=float32), Array(7716.2725, dtype=float32)), 'eval/avg_episode_length': (Array(875.7031, dtype=float32), Array(328.85904, dtype=float32)), 'eval/epoch_eval_time': 4.046973705291748, 'eval/sps': 31628.57219275469}
I0726 22:33:50.781802 140267183036224 train.py:379] starting iteration 26 223.03664827346802
I0726 22:33:57.893133 140267183036224 train.py:394] {'eval/walltime': 122.85962319374084, 'training/sps': 40162.79658627656, 'training/walltime': 102.32191038131714, 'training/entropy_loss': Array(0.00185895, dtype=float32), 'training/policy_loss': Array(0.06369568, dtype=float32), 'training/total_loss': Array(823.7605, dtype=float32), 'training/v_loss': Array(823.69495, dtype=float32), 'eval/episode_goal_distance': (Array(0.3222509, dtype=float32), Array(0.06498259, dtype=float32)), 'eval/episode_reward': (Array(-16710.062, dtype=float32), Array(6720.807, dtype=float32)), 'eval/avg_episode_length': (Array(906.8047, dtype=float32), Array(289.75668, dtype=float32)), 'eval/epoch_eval_time': 4.048175096511841, 'eval/sps': 31619.185669684288}
I0726 22:33:57.895445 140267183036224 train.py:379] starting iteration 27 230.15029001235962
I0726 22:34:05.000166 140267183036224 train.py:394] {'eval/walltime': 126.91469669342041, 'training/sps': 40342.39127783658, 'training/walltime': 105.36783790588379, 'training/entropy_loss': Array(0.0073659, dtype=float32), 'training/policy_loss': Array(0.05638821, dtype=float32), 'training/total_loss': Array(855.5444, dtype=float32), 'training/v_loss': Array(855.4806, dtype=float32), 'eval/episode_goal_distance': (Array(0.32812756, dtype=float32), Array(0.07403584, dtype=float32)), 'eval/episode_reward': (Array(-18118.043, dtype=float32), Array(8120.0244, dtype=float32)), 'eval/avg_episode_length': (Array(883.4844, dtype=float32), Array(319.80014, dtype=float32)), 'eval/epoch_eval_time': 4.055073499679565, 'eval/sps': 31565.395796183384}
I0726 22:34:05.002607 140267183036224 train.py:379] starting iteration 28 237.2574532032013
I0726 22:34:12.117851 140267183036224 train.py:394] {'eval/walltime': 130.96742010116577, 'training/sps': 40170.55981538549, 'training/walltime': 108.42679452896118, 'training/entropy_loss': Array(0.01018537, dtype=float32), 'training/policy_loss': Array(0.07678934, dtype=float32), 'training/total_loss': Array(848.6898, dtype=float32), 'training/v_loss': Array(848.6028, dtype=float32), 'eval/episode_goal_distance': (Array(0.32141796, dtype=float32), Array(0.05929217, dtype=float32)), 'eval/episode_reward': (Array(-16870.137, dtype=float32), Array(7728.4565, dtype=float32)), 'eval/avg_episode_length': (Array(868., dtype=float32), Array(337.2964, dtype=float32)), 'eval/epoch_eval_time': 4.052723407745361, 'eval/sps': 31583.699927651818}
I0726 22:34:12.120186 140267183036224 train.py:379] starting iteration 29 244.37503170967102
I0726 22:34:19.228190 140267183036224 train.py:394] {'eval/walltime': 135.0115647315979, 'training/sps': 40154.025870639634, 'training/walltime': 111.48701071739197, 'training/entropy_loss': Array(0.00730871, dtype=float32), 'training/policy_loss': Array(0.02685353, dtype=float32), 'training/total_loss': Array(869.76465, dtype=float32), 'training/v_loss': Array(869.73047, dtype=float32), 'eval/episode_goal_distance': (Array(0.31985506, dtype=float32), Array(0.06411993, dtype=float32)), 'eval/episode_reward': (Array(-16337.631, dtype=float32), Array(7418.772, dtype=float32)), 'eval/avg_episode_length': (Array(867.89844, dtype=float32), Array(337.55588, dtype=float32)), 'eval/epoch_eval_time': 4.044144630432129, 'eval/sps': 31650.697909466908}
I0726 22:34:19.230539 140267183036224 train.py:379] starting iteration 30 251.48538541793823
I0726 22:34:26.335142 140267183036224 train.py:394] {'eval/walltime': 139.05820608139038, 'training/sps': 40232.51318592801, 'training/walltime': 114.54125690460205, 'training/entropy_loss': Array(0.00674571, dtype=float32), 'training/policy_loss': Array(0.01350481, dtype=float32), 'training/total_loss': Array(895.31824, dtype=float32), 'training/v_loss': Array(895.298, dtype=float32), 'eval/episode_goal_distance': (Array(0.3164764, dtype=float32), Array(0.06883719, dtype=float32)), 'eval/episode_reward': (Array(-16606.383, dtype=float32), Array(7081.099, dtype=float32)), 'eval/avg_episode_length': (Array(883.6172, dtype=float32), Array(319.4359, dtype=float32)), 'eval/epoch_eval_time': 4.0466413497924805, 'eval/sps': 31631.169885259063}
I0726 22:34:26.337571 140267183036224 train.py:379] starting iteration 31 258.59241676330566
I0726 22:34:33.440985 140267183036224 train.py:394] {'eval/walltime': 143.10974860191345, 'training/sps': 40311.560168590266, 'training/walltime': 117.5895140171051, 'training/entropy_loss': Array(0.0075347, dtype=float32), 'training/policy_loss': Array(0.00845285, dtype=float32), 'training/total_loss': Array(869.24927, dtype=float32), 'training/v_loss': Array(869.2333, dtype=float32), 'eval/episode_goal_distance': (Array(0.32001024, dtype=float32), Array(0.06768621, dtype=float32)), 'eval/episode_reward': (Array(-16119.427, dtype=float32), Array(7826.638, dtype=float32)), 'eval/avg_episode_length': (Array(860.1797, dtype=float32), Array(345.64545, dtype=float32)), 'eval/epoch_eval_time': 4.051542520523071, 'eval/sps': 31592.905504907463}
I0726 22:34:33.443612 140267183036224 train.py:379] starting iteration 32 265.6984586715698
I0726 22:34:40.556724 140267183036224 train.py:394] {'eval/walltime': 147.16145515441895, 'training/sps': 40186.505755158454, 'training/walltime': 120.64725685119629, 'training/entropy_loss': Array(0.00787223, dtype=float32), 'training/policy_loss': Array(0.00639112, dtype=float32), 'training/total_loss': Array(869.0901, dtype=float32), 'training/v_loss': Array(869.0758, dtype=float32), 'eval/episode_goal_distance': (Array(0.32211053, dtype=float32), Array(0.07078269, dtype=float32)), 'eval/episode_reward': (Array(-17293.85, dtype=float32), Array(7095.94, dtype=float32)), 'eval/avg_episode_length': (Array(906.8672, dtype=float32), Array(289.56213, dtype=float32)), 'eval/epoch_eval_time': 4.051706552505493, 'eval/sps': 31591.62647671199}
I0726 22:34:40.559706 140267183036224 train.py:379] starting iteration 33 272.81455063819885
I0726 22:34:47.675183 140267183036224 train.py:394] {'eval/walltime': 151.21095180511475, 'training/sps': 40127.4336411065, 'training/walltime': 123.70950102806091, 'training/entropy_loss': Array(0.00812808, dtype=float32), 'training/policy_loss': Array(0.004611, dtype=float32), 'training/total_loss': Array(1347.0186, dtype=float32), 'training/v_loss': Array(1347.0059, dtype=float32), 'eval/episode_goal_distance': (Array(0.31267136, dtype=float32), Array(0.05853428, dtype=float32)), 'eval/episode_reward': (Array(-16128.822, dtype=float32), Array(7186.278, dtype=float32)), 'eval/avg_episode_length': (Array(875.83594, dtype=float32), Array(328.50757, dtype=float32)), 'eval/epoch_eval_time': 4.049496650695801, 'eval/sps': 31608.866741007558}
I0726 22:34:47.677554 140267183036224 train.py:379] starting iteration 34 279.93239998817444
I0726 22:34:54.775369 140267183036224 train.py:394] {'eval/walltime': 155.25806856155396, 'training/sps': 40328.35350692289, 'training/walltime': 126.75648880004883, 'training/entropy_loss': Array(0.0077288, dtype=float32), 'training/policy_loss': Array(0.00497811, dtype=float32), 'training/total_loss': Array(779.53015, dtype=float32), 'training/v_loss': Array(779.5174, dtype=float32), 'eval/episode_goal_distance': (Array(0.31387335, dtype=float32), Array(0.06546856, dtype=float32)), 'eval/episode_reward': (Array(-16960.73, dtype=float32), Array(6867.8774, dtype=float32)), 'eval/avg_episode_length': (Array(906.9375, dtype=float32), Array(289.3442, dtype=float32)), 'eval/epoch_eval_time': 4.047116756439209, 'eval/sps': 31627.454235498448}
I0726 22:34:54.777596 140267183036224 train.py:379] starting iteration 35 287.03244256973267
I0726 22:35:01.903030 140267183036224 train.py:394] {'eval/walltime': 159.32754230499268, 'training/sps': 40258.24175594134, 'training/walltime': 129.8087830543518, 'training/entropy_loss': Array(0.00695542, dtype=float32), 'training/policy_loss': Array(0.00474505, dtype=float32), 'training/total_loss': Array(618.1046, dtype=float32), 'training/v_loss': Array(618.0929, dtype=float32), 'eval/episode_goal_distance': (Array(0.3183924, dtype=float32), Array(0.05991376, dtype=float32)), 'eval/episode_reward': (Array(-16890.51, dtype=float32), Array(6854.155, dtype=float32)), 'eval/avg_episode_length': (Array(906.75, dtype=float32), Array(289.92624, dtype=float32)), 'eval/epoch_eval_time': 4.069473743438721, 'eval/sps': 31453.698455820362}
I0726 22:35:01.905356 140267183036224 train.py:379] starting iteration 36 294.1602020263672
I0726 22:35:09.023680 140267183036224 train.py:394] {'eval/walltime': 163.38300585746765, 'training/sps': 40166.25210476972, 'training/walltime': 132.86806774139404, 'training/entropy_loss': Array(0.01049545, dtype=float32), 'training/policy_loss': Array(0.00522314, dtype=float32), 'training/total_loss': Array(591.5729, dtype=float32), 'training/v_loss': Array(591.5571, dtype=float32), 'eval/episode_goal_distance': (Array(0.32248428, dtype=float32), Array(0.0629188, dtype=float32)), 'eval/episode_reward': (Array(-16726.945, dtype=float32), Array(7877.081, dtype=float32)), 'eval/avg_episode_length': (Array(860.2578, dtype=float32), Array(345.4527, dtype=float32)), 'eval/epoch_eval_time': 4.055463552474976, 'eval/sps': 31562.359849562432}
I0726 22:35:09.025869 140267183036224 train.py:379] starting iteration 37 301.2807149887085
I0726 22:35:16.135599 140267183036224 train.py:394] {'eval/walltime': 167.4306058883667, 'training/sps': 40177.1922398963, 'training/walltime': 135.9265193939209, 'training/entropy_loss': Array(0.0133885, dtype=float32), 'training/policy_loss': Array(0.00496878, dtype=float32), 'training/total_loss': Array(551.3107, dtype=float32), 'training/v_loss': Array(551.2924, dtype=float32), 'eval/episode_goal_distance': (Array(0.3019951, dtype=float32), Array(0.05911803, dtype=float32)), 'eval/episode_reward': (Array(-16060.953, dtype=float32), Array(6657.6216, dtype=float32)), 'eval/avg_episode_length': (Array(899.09375, dtype=float32), Array(300.12064, dtype=float32)), 'eval/epoch_eval_time': 4.047600030899048, 'eval/sps': 31623.677987661446}
I0726 22:35:16.138105 140267183036224 train.py:379] starting iteration 38 308.3929514884949
I0726 22:35:23.245423 140267183036224 train.py:394] {'eval/walltime': 171.48754978179932, 'training/sps': 40331.75551445487, 'training/walltime': 138.97325015068054, 'training/entropy_loss': Array(0.01575231, dtype=float32), 'training/policy_loss': Array(0.00459306, dtype=float32), 'training/total_loss': Array(544.7849, dtype=float32), 'training/v_loss': Array(544.7645, dtype=float32), 'eval/episode_goal_distance': (Array(0.31460115, dtype=float32), Array(0.05732666, dtype=float32)), 'eval/episode_reward': (Array(-17027.854, dtype=float32), Array(7259.6323, dtype=float32)), 'eval/avg_episode_length': (Array(891.3047, dtype=float32), Array(310.1705, dtype=float32)), 'eval/epoch_eval_time': 4.056943893432617, 'eval/sps': 31550.843039068513}
I0726 22:35:23.247636 140267183036224 train.py:379] starting iteration 39 315.502482175827
I0726 22:35:30.367306 140267183036224 train.py:394] {'eval/walltime': 175.55418014526367, 'training/sps': 40296.14180034118, 'training/walltime': 142.02267360687256, 'training/entropy_loss': Array(0.01606539, dtype=float32), 'training/policy_loss': Array(0.00374156, dtype=float32), 'training/total_loss': Array(551.00244, dtype=float32), 'training/v_loss': Array(550.98267, dtype=float32), 'eval/episode_goal_distance': (Array(0.30566096, dtype=float32), Array(0.05878117, dtype=float32)), 'eval/episode_reward': (Array(-16311.543, dtype=float32), Array(6913.8164, dtype=float32)), 'eval/avg_episode_length': (Array(891.2656, dtype=float32), Array(310.28146, dtype=float32)), 'eval/epoch_eval_time': 4.0666303634643555, 'eval/sps': 31475.690820091408}
I0726 22:35:30.369796 140267183036224 train.py:379] starting iteration 40 322.62464213371277
I0726 22:35:37.495633 140267183036224 train.py:394] {'eval/walltime': 179.61621356010437, 'training/sps': 40155.296026021206, 'training/walltime': 145.08279299736023, 'training/entropy_loss': Array(0.01496088, dtype=float32), 'training/policy_loss': Array(0.00390866, dtype=float32), 'training/total_loss': Array(512.32007, dtype=float32), 'training/v_loss': Array(512.30115, dtype=float32), 'eval/episode_goal_distance': (Array(0.31340843, dtype=float32), Array(0.05993985, dtype=float32)), 'eval/episode_reward': (Array(-17302.225, dtype=float32), Array(6394.4443, dtype=float32)), 'eval/avg_episode_length': (Array(922.3203, dtype=float32), Array(266.83862, dtype=float32)), 'eval/epoch_eval_time': 4.062033414840698, 'eval/sps': 31511.311436373257}
I0726 22:35:37.498097 140267183036224 train.py:379] starting iteration 41 329.75294375419617
I0726 22:35:44.623751 140267183036224 train.py:394] {'eval/walltime': 183.6827518939972, 'training/sps': 40215.18451723894, 'training/walltime': 148.13835525512695, 'training/entropy_loss': Array(0.0150309, dtype=float32), 'training/policy_loss': Array(0.00361587, dtype=float32), 'training/total_loss': Array(499.72086, dtype=float32), 'training/v_loss': Array(499.7022, dtype=float32), 'eval/episode_goal_distance': (Array(0.30558264, dtype=float32), Array(0.07110275, dtype=float32)), 'eval/episode_reward': (Array(-16847.205, dtype=float32), Array(7082.7, dtype=float32)), 'eval/avg_episode_length': (Array(906.84375, dtype=float32), Array(289.63507, dtype=float32)), 'eval/epoch_eval_time': 4.066538333892822, 'eval/sps': 31476.403144457254}
I0726 22:35:44.626041 140267183036224 train.py:379] starting iteration 42 336.88088750839233
I0726 22:35:51.743848 140267183036224 train.py:394] {'eval/walltime': 187.74174547195435, 'training/sps': 40221.116021946815, 'training/walltime': 151.19346690177917, 'training/entropy_loss': Array(0.01387897, dtype=float32), 'training/policy_loss': Array(0.00288784, dtype=float32), 'training/total_loss': Array(482.39203, dtype=float32), 'training/v_loss': Array(482.3753, dtype=float32), 'eval/episode_goal_distance': (Array(0.3092929, dtype=float32), Array(0.06689443, dtype=float32)), 'eval/episode_reward': (Array(-15318.15, dtype=float32), Array(7902.09, dtype=float32)), 'eval/avg_episode_length': (Array(836.875, dtype=float32), Array(368.21667, dtype=float32)), 'eval/epoch_eval_time': 4.058993577957153, 'eval/sps': 31534.910696858256}
I0726 22:35:51.746173 140267183036224 train.py:379] starting iteration 43 344.00101947784424
I0726 22:35:58.875755 140267183036224 train.py:394] {'eval/walltime': 191.82069993019104, 'training/sps': 40328.29039532276, 'training/walltime': 154.24045944213867, 'training/entropy_loss': Array(0.01299461, dtype=float32), 'training/policy_loss': Array(0.00301303, dtype=float32), 'training/total_loss': Array(454.29468, dtype=float32), 'training/v_loss': Array(454.2787, dtype=float32), 'eval/episode_goal_distance': (Array(0.30599564, dtype=float32), Array(0.07058707, dtype=float32)), 'eval/episode_reward': (Array(-16805.262, dtype=float32), Array(8083.6943, dtype=float32)), 'eval/avg_episode_length': (Array(891.21094, dtype=float32), Array(310.43735, dtype=float32)), 'eval/epoch_eval_time': 4.078954458236694, 'eval/sps': 31380.59061716849}
I0726 22:35:58.878678 140267183036224 train.py:379] starting iteration 44 351.13352513313293
I0726 22:36:06.017132 140267183036224 train.py:394] {'eval/walltime': 195.9008858203888, 'training/sps': 40228.44023455373, 'training/walltime': 157.29501485824585, 'training/entropy_loss': Array(0.01180965, dtype=float32), 'training/policy_loss': Array(0.00251646, dtype=float32), 'training/total_loss': Array(482.96707, dtype=float32), 'training/v_loss': Array(482.95273, dtype=float32), 'eval/episode_goal_distance': (Array(0.31119466, dtype=float32), Array(0.06603674, dtype=float32)), 'eval/episode_reward': (Array(-16444.738, dtype=float32), Array(7161.037, dtype=float32)), 'eval/avg_episode_length': (Array(875.7578, dtype=float32), Array(328.7146, dtype=float32)), 'eval/epoch_eval_time': 4.080185890197754, 'eval/sps': 31371.119709890532}
I0726 22:36:06.020137 140267183036224 train.py:379] starting iteration 45 358.2749831676483
I0726 22:36:13.148683 140267183036224 train.py:394] {'eval/walltime': 199.97256684303284, 'training/sps': 40246.855155197554, 'training/walltime': 160.34817266464233, 'training/entropy_loss': Array(0.00921522, dtype=float32), 'training/policy_loss': Array(0.00222818, dtype=float32), 'training/total_loss': Array(464.4168, dtype=float32), 'training/v_loss': Array(464.40533, dtype=float32), 'eval/episode_goal_distance': (Array(0.3067934, dtype=float32), Array(0.06875169, dtype=float32)), 'eval/episode_reward': (Array(-16046.3545, dtype=float32), Array(7253.1494, dtype=float32)), 'eval/avg_episode_length': (Array(875.8203, dtype=float32), Array(328.5491, dtype=float32)), 'eval/epoch_eval_time': 4.071681022644043, 'eval/sps': 31436.64724425789}
I0726 22:36:13.151166 140267183036224 train.py:379] starting iteration 46 365.40601229667664
I0726 22:36:20.289226 140267183036224 train.py:394] {'eval/walltime': 204.04563736915588, 'training/sps': 40139.65931496796, 'training/walltime': 163.4094841480255, 'training/entropy_loss': Array(0.00780658, dtype=float32), 'training/policy_loss': Array(0.00161078, dtype=float32), 'training/total_loss': Array(401.6841, dtype=float32), 'training/v_loss': Array(401.6747, dtype=float32), 'eval/episode_goal_distance': (Array(0.30668432, dtype=float32), Array(0.06448139, dtype=float32)), 'eval/episode_reward': (Array(-17030.305, dtype=float32), Array(6377.0317, dtype=float32)), 'eval/avg_episode_length': (Array(922.33594, dtype=float32), Array(266.78528, dtype=float32)), 'eval/epoch_eval_time': 4.073070526123047, 'eval/sps': 31425.922821384785}
I0726 22:36:20.291646 140267183036224 train.py:379] starting iteration 47 372.5464925765991
I0726 22:36:27.431500 140267183036224 train.py:394] {'eval/walltime': 208.1275053024292, 'training/sps': 40232.5288889555, 'training/walltime': 166.4637291431427, 'training/entropy_loss': Array(0.00395495, dtype=float32), 'training/policy_loss': Array(0.00104677, dtype=float32), 'training/total_loss': Array(389.62924, dtype=float32), 'training/v_loss': Array(389.62427, dtype=float32), 'eval/episode_goal_distance': (Array(0.3133915, dtype=float32), Array(0.06780776, dtype=float32)), 'eval/episode_reward': (Array(-18196.973, dtype=float32), Array(6221.924, dtype=float32)), 'eval/avg_episode_length': (Array(953.3828, dtype=float32), Array(210.20882, dtype=float32)), 'eval/epoch_eval_time': 4.081867933273315, 'eval/sps': 31358.19239927117}
I0726 22:36:27.434006 140267183036224 train.py:379] starting iteration 48 379.68885374069214
I0726 22:36:34.555078 140267183036224 train.py:394] {'eval/walltime': 212.19801139831543, 'training/sps': 40329.49270526726, 'training/walltime': 169.51063084602356, 'training/entropy_loss': Array(0.0016797, dtype=float32), 'training/policy_loss': Array(0.00115849, dtype=float32), 'training/total_loss': Array(360.5026, dtype=float32), 'training/v_loss': Array(360.49976, dtype=float32), 'eval/episode_goal_distance': (Array(0.31487942, dtype=float32), Array(0.06041494, dtype=float32)), 'eval/episode_reward': (Array(-17518.299, dtype=float32), Array(5826.0195, dtype=float32)), 'eval/avg_episode_length': (Array(937.8672, dtype=float32), Array(240.63972, dtype=float32)), 'eval/epoch_eval_time': 4.0705060958862305, 'eval/sps': 31445.721240747054}
I0726 22:36:34.557447 140267183036224 train.py:379] starting iteration 49 386.8122932910919
I0726 22:36:41.714213 140267183036224 train.py:394] {'eval/walltime': 216.3033685684204, 'training/sps': 40319.15392028528, 'training/walltime': 172.55831384658813, 'training/entropy_loss': Array(-0.00159685, dtype=float32), 'training/policy_loss': Array(0.00043916, dtype=float32), 'training/total_loss': Array(361.10205, dtype=float32), 'training/v_loss': Array(361.1032, dtype=float32), 'eval/episode_goal_distance': (Array(0.30939037, dtype=float32), Array(0.05885793, dtype=float32)), 'eval/episode_reward': (Array(-16887.902, dtype=float32), Array(5639.709, dtype=float32)), 'eval/avg_episode_length': (Array(937.8906, dtype=float32), Array(240.54892, dtype=float32)), 'eval/epoch_eval_time': 4.1053571701049805, 'eval/sps': 31178.77317279238}
I0726 22:36:41.716706 140267183036224 train.py:379] starting iteration 50 393.97155261039734
I0726 22:36:48.867785 140267183036224 train.py:394] {'eval/walltime': 220.40285515785217, 'training/sps': 40315.79503525273, 'training/walltime': 175.60625076293945, 'training/entropy_loss': Array(-0.00688597, dtype=float32), 'training/policy_loss': Array(9.979532e-05, dtype=float32), 'training/total_loss': Array(1140.4409, dtype=float32), 'training/v_loss': Array(1140.4478, dtype=float32), 'eval/episode_goal_distance': (Array(0.31064278, dtype=float32), Array(0.06164914, dtype=float32)), 'eval/episode_reward': (Array(-15596.439, dtype=float32), Array(7136.477, dtype=float32)), 'eval/avg_episode_length': (Array(868.03906, dtype=float32), Array(337.19702, dtype=float32)), 'eval/epoch_eval_time': 4.099486589431763, 'eval/sps': 31223.422057283107}
I0726 22:36:48.870143 140267183036224 train.py:379] starting iteration 51 401.1249895095825
I0726 22:36:56.018311 140267183036224 train.py:394] {'eval/walltime': 224.49715113639832, 'training/sps': 40285.63431949471, 'training/walltime': 178.65646958351135, 'training/entropy_loss': Array(-0.01218441, dtype=float32), 'training/policy_loss': Array(0.0002398, dtype=float32), 'training/total_loss': Array(526.371, dtype=float32), 'training/v_loss': Array(526.38293, dtype=float32), 'eval/episode_goal_distance': (Array(0.31124857, dtype=float32), Array(0.06575308, dtype=float32)), 'eval/episode_reward': (Array(-14911.428, dtype=float32), Array(8125.905, dtype=float32)), 'eval/avg_episode_length': (Array(821.3906, dtype=float32), Array(381.624, dtype=float32)), 'eval/epoch_eval_time': 4.094295978546143, 'eval/sps': 31263.00606275464}
I0726 22:36:56.063087 140267183036224 train.py:379] starting iteration 52 408.31793236732483
I0726 22:37:03.193439 140267183036224 train.py:394] {'eval/walltime': 228.57409858703613, 'training/sps': 40292.632397759015, 'training/walltime': 181.7061586380005, 'training/entropy_loss': Array(-0.01533886, dtype=float32), 'training/policy_loss': Array(0.00062206, dtype=float32), 'training/total_loss': Array(421.1137, dtype=float32), 'training/v_loss': Array(421.12842, dtype=float32), 'eval/episode_goal_distance': (Array(0.30569756, dtype=float32), Array(0.06248196, dtype=float32)), 'eval/episode_reward': (Array(-16273.044, dtype=float32), Array(6531.565, dtype=float32)), 'eval/avg_episode_length': (Array(906.78906, dtype=float32), Array(289.80515, dtype=float32)), 'eval/epoch_eval_time': 4.076947450637817, 'eval/sps': 31396.038715185074}
I0726 22:37:03.196035 140267183036224 train.py:379] starting iteration 53 415.4508819580078
I0726 22:37:10.345704 140267183036224 train.py:394] {'eval/walltime': 232.66845297813416, 'training/sps': 40267.548844697725, 'training/walltime': 184.7577474117279, 'training/entropy_loss': Array(-0.01692032, dtype=float32), 'training/policy_loss': Array(-0.00039465, dtype=float32), 'training/total_loss': Array(362.82208, dtype=float32), 'training/v_loss': Array(362.8394, dtype=float32), 'eval/episode_goal_distance': (Array(0.30042845, dtype=float32), Array(0.07057421, dtype=float32)), 'eval/episode_reward': (Array(-15153.788, dtype=float32), Array(6510.3413, dtype=float32)), 'eval/avg_episode_length': (Array(883.52344, dtype=float32), Array(319.69272, dtype=float32)), 'eval/epoch_eval_time': 4.0943543910980225, 'eval/sps': 31262.560045681097}
I0726 22:37:10.348206 140267183036224 train.py:379] starting iteration 54 422.6030526161194
I0726 22:37:17.497732 140267183036224 train.py:394] {'eval/walltime': 236.76150131225586, 'training/sps': 40251.00728014594, 'training/walltime': 187.8105902671814, 'training/entropy_loss': Array(-0.01794274, dtype=float32), 'training/policy_loss': Array(0.00012125, dtype=float32), 'training/total_loss': Array(340.34186, dtype=float32), 'training/v_loss': Array(340.35968, dtype=float32), 'eval/episode_goal_distance': (Array(0.30629802, dtype=float32), Array(0.05910876, dtype=float32)), 'eval/episode_reward': (Array(-16200.852, dtype=float32), Array(6232.277, dtype=float32)), 'eval/avg_episode_length': (Array(906.8203, dtype=float32), Array(289.70807, dtype=float32)), 'eval/epoch_eval_time': 4.093048334121704, 'eval/sps': 31272.535663194543}
I0726 22:37:17.500348 140267183036224 train.py:379] starting iteration 55 429.75519490242004
I0726 22:37:24.655184 140267183036224 train.py:394] {'eval/walltime': 240.85990071296692, 'training/sps': 40251.16445537393, 'training/walltime': 190.86342120170593, 'training/entropy_loss': Array(-0.01822937, dtype=float32), 'training/policy_loss': Array(0.00055859, dtype=float32), 'training/total_loss': Array(315.4408, dtype=float32), 'training/v_loss': Array(315.45844, dtype=float32), 'eval/episode_goal_distance': (Array(0.31126034, dtype=float32), Array(0.06113303, dtype=float32)), 'eval/episode_reward': (Array(-16890.328, dtype=float32), Array(6099.1963, dtype=float32)), 'eval/avg_episode_length': (Array(930.09375, dtype=float32), Array(254.19601, dtype=float32)), 'eval/epoch_eval_time': 4.09839940071106, 'eval/sps': 31231.70474253739}
I0726 22:37:24.657559 140267183036224 train.py:379] starting iteration 56 436.9124059677124
I0726 22:37:31.802662 140267183036224 train.py:394] {'eval/walltime': 244.94591879844666, 'training/sps': 40217.41254928602, 'training/walltime': 193.9188141822815, 'training/entropy_loss': Array(-0.01932367, dtype=float32), 'training/policy_loss': Array(-0.00014769, dtype=float32), 'training/total_loss': Array(302.5156, dtype=float32), 'training/v_loss': Array(302.53506, dtype=float32), 'eval/episode_goal_distance': (Array(0.30963865, dtype=float32), Array(0.0650618, dtype=float32)), 'eval/episode_reward': (Array(-16263.549, dtype=float32), Array(6158.3354, dtype=float32)), 'eval/avg_episode_length': (Array(914.5469, dtype=float32), Array(278.69266, dtype=float32)), 'eval/epoch_eval_time': 4.086018085479736, 'eval/sps': 31326.342008829266}
I0726 22:37:31.805230 140267183036224 train.py:379] starting iteration 57 444.060076713562
I0726 22:37:38.966189 140267183036224 train.py:394] {'eval/walltime': 249.04715132713318, 'training/sps': 40208.0627468994, 'training/walltime': 196.97491765022278, 'training/entropy_loss': Array(-0.02395844, dtype=float32), 'training/policy_loss': Array(0.00025925, dtype=float32), 'training/total_loss': Array(282.68134, dtype=float32), 'training/v_loss': Array(282.70502, dtype=float32), 'eval/episode_goal_distance': (Array(0.30557585, dtype=float32), Array(0.06694701, dtype=float32)), 'eval/episode_reward': (Array(-15511.041, dtype=float32), Array(6877.2627, dtype=float32)), 'eval/avg_episode_length': (Array(875.6875, dtype=float32), Array(328.90048, dtype=float32)), 'eval/epoch_eval_time': 4.101232528686523, 'eval/sps': 31210.129907214447}
I0726 22:37:38.968596 140267183036224 train.py:379] starting iteration 58 451.22344303131104
I0726 22:37:46.122551 140267183036224 train.py:394] {'eval/walltime': 253.1404287815094, 'training/sps': 40196.11513521604, 'training/walltime': 200.03192949295044, 'training/entropy_loss': Array(-0.02367012, dtype=float32), 'training/policy_loss': Array(0.00072469, dtype=float32), 'training/total_loss': Array(269.71588, dtype=float32), 'training/v_loss': Array(269.73883, dtype=float32), 'eval/episode_goal_distance': (Array(0.30845654, dtype=float32), Array(0.06894802, dtype=float32)), 'eval/episode_reward': (Array(-16783.557, dtype=float32), Array(5374.5024, dtype=float32)), 'eval/avg_episode_length': (Array(945.625, dtype=float32), Array(226.07048, dtype=float32)), 'eval/epoch_eval_time': 4.093277454376221, 'eval/sps': 31270.78519027635}
I0726 22:37:46.124964 140267183036224 train.py:379] starting iteration 59 458.3798108100891
I0726 22:37:53.276244 140267183036224 train.py:394] {'eval/walltime': 257.23181796073914, 'training/sps': 40205.340199132625, 'training/walltime': 203.08823990821838, 'training/entropy_loss': Array(-0.02335368, dtype=float32), 'training/policy_loss': Array(0.00027287, dtype=float32), 'training/total_loss': Array(252.777, dtype=float32), 'training/v_loss': Array(252.80006, dtype=float32), 'eval/episode_goal_distance': (Array(0.3138222, dtype=float32), Array(0.06131902, dtype=float32)), 'eval/episode_reward': (Array(-16066.458, dtype=float32), Array(6626.1235, dtype=float32)), 'eval/avg_episode_length': (Array(891.34375, dtype=float32), Array(310.0588, dtype=float32)), 'eval/epoch_eval_time': 4.091389179229736, 'eval/sps': 31285.21741461365}
I0726 22:37:53.278634 140267183036224 train.py:379] starting iteration 60 465.53348088264465
I0726 22:38:00.433062 140267183036224 train.py:394] {'eval/walltime': 261.3241729736328, 'training/sps': 40178.037890501306, 'training/walltime': 206.14662718772888, 'training/entropy_loss': Array(-0.02589472, dtype=float32), 'training/policy_loss': Array(0.00054626, dtype=float32), 'training/total_loss': Array(248.16061, dtype=float32), 'training/v_loss': Array(248.18597, dtype=float32), 'eval/episode_goal_distance': (Array(0.30636865, dtype=float32), Array(0.05839625, dtype=float32)), 'eval/episode_reward': (Array(-15816.729, dtype=float32), Array(6281.0005, dtype=float32)), 'eval/avg_episode_length': (Array(899.0156, dtype=float32), Array(300.35294, dtype=float32)), 'eval/epoch_eval_time': 4.092355012893677, 'eval/sps': 31277.833813712085}
I0726 22:38:00.435635 140267183036224 train.py:379] starting iteration 61 472.6904811859131
I0726 22:38:07.595070 140267183036224 train.py:394] {'eval/walltime': 265.4219129085541, 'training/sps': 40181.67459378505, 'training/walltime': 209.20473766326904, 'training/entropy_loss': Array(-0.02863616, dtype=float32), 'training/policy_loss': Array(-0.00026495, dtype=float32), 'training/total_loss': Array(252.59001, dtype=float32), 'training/v_loss': Array(252.6189, dtype=float32), 'eval/episode_goal_distance': (Array(0.30663273, dtype=float32), Array(0.06347453, dtype=float32)), 'eval/episode_reward': (Array(-16165.622, dtype=float32), Array(5737.7607, dtype=float32)), 'eval/avg_episode_length': (Array(922.35156, dtype=float32), Array(266.73166, dtype=float32)), 'eval/epoch_eval_time': 4.097739934921265, 'eval/sps': 31236.73098655526}
I0726 22:38:07.597445 140267183036224 train.py:379] starting iteration 62 479.85229110717773
I0726 22:38:14.747915 140267183036224 train.py:394] {'eval/walltime': 269.50823879241943, 'training/sps': 40151.07604145081, 'training/walltime': 212.26517868041992, 'training/entropy_loss': Array(-0.02816367, dtype=float32), 'training/policy_loss': Array(-0.00010972, dtype=float32), 'training/total_loss': Array(245.41577, dtype=float32), 'training/v_loss': Array(245.44406, dtype=float32), 'eval/episode_goal_distance': (Array(0.30312094, dtype=float32), Array(0.05684866, dtype=float32)), 'eval/episode_reward': (Array(-15132.449, dtype=float32), Array(5931.31, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.30682, dtype=float32)), 'eval/epoch_eval_time': 4.0863258838653564, 'eval/sps': 31323.982383637413}
I0726 22:38:14.750436 140267183036224 train.py:379] starting iteration 63 487.0052824020386
I0726 22:38:21.893552 140267183036224 train.py:394] {'eval/walltime': 273.601185798645, 'training/sps': 40335.675793237155, 'training/walltime': 215.31161332130432, 'training/entropy_loss': Array(-0.02778576, dtype=float32), 'training/policy_loss': Array(0.00026636, dtype=float32), 'training/total_loss': Array(247.69594, dtype=float32), 'training/v_loss': Array(247.72345, dtype=float32), 'eval/episode_goal_distance': (Array(0.3072921, dtype=float32), Array(0.05908572, dtype=float32)), 'eval/episode_reward': (Array(-15359.168, dtype=float32), Array(6339.25, dtype=float32)), 'eval/avg_episode_length': (Array(891.2422, dtype=float32), Array(310.3484, dtype=float32)), 'eval/epoch_eval_time': 4.092947006225586, 'eval/sps': 31273.30986824538}
I0726 22:38:21.896044 140267183036224 train.py:379] starting iteration 64 494.1508903503418
I0726 22:38:29.030443 140267183036224 train.py:394] {'eval/walltime': 277.68722772598267, 'training/sps': 40359.91194361785, 'training/walltime': 218.35621857643127, 'training/entropy_loss': Array(-0.02971656, dtype=float32), 'training/policy_loss': Array(0.0002606, dtype=float32), 'training/total_loss': Array(224.43378, dtype=float32), 'training/v_loss': Array(224.46324, dtype=float32), 'eval/episode_goal_distance': (Array(0.30750132, dtype=float32), Array(0.0634378, dtype=float32)), 'eval/episode_reward': (Array(-15705.223, dtype=float32), Array(5490.6655, dtype=float32)), 'eval/avg_episode_length': (Array(922.27344, dtype=float32), Array(266.99988, dtype=float32)), 'eval/epoch_eval_time': 4.0860419273376465, 'eval/sps': 31326.159221131955}
I0726 22:38:29.033469 140267183036224 train.py:379] starting iteration 65 501.28831577301025
I0726 22:38:36.205308 140267183036224 train.py:394] {'eval/walltime': 281.80806016921997, 'training/sps': 40323.49764644801, 'training/walltime': 221.40357327461243, 'training/entropy_loss': Array(-0.02837582, dtype=float32), 'training/policy_loss': Array(0.00028695, dtype=float32), 'training/total_loss': Array(214.8111, dtype=float32), 'training/v_loss': Array(214.83917, dtype=float32), 'eval/episode_goal_distance': (Array(0.31563935, dtype=float32), Array(0.0671486, dtype=float32)), 'eval/episode_reward': (Array(-15681.2, dtype=float32), Array(6236.9683, dtype=float32)), 'eval/avg_episode_length': (Array(898.9844, dtype=float32), Array(300.446, dtype=float32)), 'eval/epoch_eval_time': 4.120832443237305, 'eval/sps': 31061.685172388097}
I0726 22:38:36.207744 140267183036224 train.py:379] starting iteration 66 508.4625895023346
I0726 22:38:43.360648 140267183036224 train.py:394] {'eval/walltime': 285.9119634628296, 'training/sps': 40350.416798552884, 'training/walltime': 224.44889497756958, 'training/entropy_loss': Array(-0.0295372, dtype=float32), 'training/policy_loss': Array(0.00027239, dtype=float32), 'training/total_loss': Array(642.01636, dtype=float32), 'training/v_loss': Array(642.0456, dtype=float32), 'eval/episode_goal_distance': (Array(0.29224175, dtype=float32), Array(0.06246387, dtype=float32)), 'eval/episode_reward': (Array(-14473.058, dtype=float32), Array(6127.5063, dtype=float32)), 'eval/avg_episode_length': (Array(883.5, dtype=float32), Array(319.75708, dtype=float32)), 'eval/epoch_eval_time': 4.103903293609619, 'eval/sps': 31189.818775533728}
I0726 22:38:43.363050 140267183036224 train.py:379] starting iteration 67 515.6178970336914
I0726 22:38:50.535201 140267183036224 train.py:394] {'eval/walltime': 290.0309407711029, 'training/sps': 40295.79524326172, 'training/walltime': 227.4983446598053, 'training/entropy_loss': Array(-0.03257939, dtype=float32), 'training/policy_loss': Array(0.00013008, dtype=float32), 'training/total_loss': Array(519.9929, dtype=float32), 'training/v_loss': Array(520.0254, dtype=float32), 'eval/episode_goal_distance': (Array(0.30454916, dtype=float32), Array(0.06687508, dtype=float32)), 'eval/episode_reward': (Array(-14732.284, dtype=float32), Array(7557.1685, dtype=float32)), 'eval/avg_episode_length': (Array(836.96094, dtype=float32), Array(368.0226, dtype=float32)), 'eval/epoch_eval_time': 4.118977308273315, 'eval/sps': 31075.674960117198}
I0726 22:38:50.537616 140267183036224 train.py:379] starting iteration 68 522.7924628257751
I0726 22:38:57.695661 140267183036224 train.py:394] {'eval/walltime': 294.14190220832825, 'training/sps': 40373.351811946166, 'training/walltime': 230.5419363975525, 'training/entropy_loss': Array(-0.0345093, dtype=float32), 'training/policy_loss': Array(-0.00079354, dtype=float32), 'training/total_loss': Array(340.3047, dtype=float32), 'training/v_loss': Array(340.34003, dtype=float32), 'eval/episode_goal_distance': (Array(0.30660707, dtype=float32), Array(0.06764752, dtype=float32)), 'eval/episode_reward': (Array(-15559.617, dtype=float32), Array(5964.218, dtype=float32)), 'eval/avg_episode_length': (Array(914.6172, dtype=float32), Array(278.4634, dtype=float32)), 'eval/epoch_eval_time': 4.110961437225342, 'eval/sps': 31136.26871829586}
I0726 22:38:57.698068 140267183036224 train.py:379] starting iteration 69 529.9529137611389
I0726 22:39:04.867753 140267183036224 train.py:394] {'eval/walltime': 298.2570583820343, 'training/sps': 40277.628250133945, 'training/walltime': 233.59276151657104, 'training/entropy_loss': Array(-0.03539962, dtype=float32), 'training/policy_loss': Array(-0.00066837, dtype=float32), 'training/total_loss': Array(256.79175, dtype=float32), 'training/v_loss': Array(256.82782, dtype=float32), 'eval/episode_goal_distance': (Array(0.2973144, dtype=float32), Array(0.06311802, dtype=float32)), 'eval/episode_reward': (Array(-15689.409, dtype=float32), Array(5916.1396, dtype=float32)), 'eval/avg_episode_length': (Array(914.5781, dtype=float32), Array(278.5904, dtype=float32)), 'eval/epoch_eval_time': 4.115156173706055, 'eval/sps': 31104.530325692333}
I0726 22:39:04.870138 140267183036224 train.py:379] starting iteration 70 537.1249842643738
I0726 22:39:12.025730 140267183036224 train.py:394] {'eval/walltime': 302.3616590499878, 'training/sps': 40324.5576955396, 'training/walltime': 236.64003610610962, 'training/entropy_loss': Array(-0.03445965, dtype=float32), 'training/policy_loss': Array(-0.00044762, dtype=float32), 'training/total_loss': Array(251.33772, dtype=float32), 'training/v_loss': Array(251.37262, dtype=float32), 'eval/episode_goal_distance': (Array(0.3015374, dtype=float32), Array(0.05786945, dtype=float32)), 'eval/episode_reward': (Array(-16412.068, dtype=float32), Array(5171.6426, dtype=float32)), 'eval/avg_episode_length': (Array(945.6953, dtype=float32), Array(225.77802, dtype=float32)), 'eval/epoch_eval_time': 4.104600667953491, 'eval/sps': 31184.5196048802}
I0726 22:39:12.028280 140267183036224 train.py:379] starting iteration 71 544.2831265926361
I0726 22:39:19.193338 140267183036224 train.py:394] {'eval/walltime': 306.47092056274414, 'training/sps': 40260.59407091449, 'training/walltime': 239.69215202331543, 'training/entropy_loss': Array(-0.03310781, dtype=float32), 'training/policy_loss': Array(0.00018777, dtype=float32), 'training/total_loss': Array(219.94669, dtype=float32), 'training/v_loss': Array(219.97961, dtype=float32), 'eval/episode_goal_distance': (Array(0.30933392, dtype=float32), Array(0.06440523, dtype=float32)), 'eval/episode_reward': (Array(-15608.729, dtype=float32), Array(5573.9707, dtype=float32)), 'eval/avg_episode_length': (Array(922.3281, dtype=float32), Array(266.81223, dtype=float32)), 'eval/epoch_eval_time': 4.109261512756348, 'eval/sps': 31149.14920908553}
I0726 22:39:19.195803 140267183036224 train.py:379] starting iteration 72 551.4506492614746
I0726 22:39:26.369754 140267183036224 train.py:394] {'eval/walltime': 310.5920729637146, 'training/sps': 40300.35136950691, 'training/walltime': 242.74125695228577, 'training/entropy_loss': Array(-0.03339054, dtype=float32), 'training/policy_loss': Array(-0.0004872, dtype=float32), 'training/total_loss': Array(216.67723, dtype=float32), 'training/v_loss': Array(216.71112, dtype=float32), 'eval/episode_goal_distance': (Array(0.30910328, dtype=float32), Array(0.06112033, dtype=float32)), 'eval/episode_reward': (Array(-15073.619, dtype=float32), Array(5648.2197, dtype=float32)), 'eval/avg_episode_length': (Array(906.78125, dtype=float32), Array(289.8293, dtype=float32)), 'eval/epoch_eval_time': 4.121152400970459, 'eval/sps': 31059.273607512852}
I0726 22:39:26.372211 140267183036224 train.py:379] starting iteration 73 558.6270573139191
I0726 22:39:33.544940 140267183036224 train.py:394] {'eval/walltime': 314.7124493122101, 'training/sps': 40306.67682917769, 'training/walltime': 245.78988337516785, 'training/entropy_loss': Array(-0.03328614, dtype=float32), 'training/policy_loss': Array(-9.6457006e-05, dtype=float32), 'training/total_loss': Array(199.82422, dtype=float32), 'training/v_loss': Array(199.8576, dtype=float32), 'eval/episode_goal_distance': (Array(0.29614446, dtype=float32), Array(0.05721328, dtype=float32)), 'eval/episode_reward': (Array(-15261.359, dtype=float32), Array(5420.6763, dtype=float32)), 'eval/avg_episode_length': (Array(922.3672, dtype=float32), Array(266.67786, dtype=float32)), 'eval/epoch_eval_time': 4.120376348495483, 'eval/sps': 31065.12346784487}
I0726 22:39:33.547495 140267183036224 train.py:379] starting iteration 74 565.8023414611816
I0726 22:39:40.692800 140267183036224 train.py:394] {'eval/walltime': 318.80824613571167, 'training/sps': 40342.65969110625, 'training/walltime': 248.83579063415527, 'training/entropy_loss': Array(-0.03386688, dtype=float32), 'training/policy_loss': Array(4.6283632e-05, dtype=float32), 'training/total_loss': Array(186.24944, dtype=float32), 'training/v_loss': Array(186.28325, dtype=float32), 'eval/episode_goal_distance': (Array(0.29969022, dtype=float32), Array(0.06541838, dtype=float32)), 'eval/episode_reward': (Array(-14725.172, dtype=float32), Array(5408.554, dtype=float32)), 'eval/avg_episode_length': (Array(914.625, dtype=float32), Array(278.43768, dtype=float32)), 'eval/epoch_eval_time': 4.095796823501587, 'eval/sps': 31251.55019056096}
I0726 22:39:40.695200 140267183036224 train.py:379] starting iteration 75 572.9500467777252
I0726 22:39:47.866069 140267183036224 train.py:394] {'eval/walltime': 322.9215130805969, 'training/sps': 40235.97757073084, 'training/walltime': 251.8897738456726, 'training/entropy_loss': Array(-0.03332769, dtype=float32), 'training/policy_loss': Array(-0.00037467, dtype=float32), 'training/total_loss': Array(180.76224, dtype=float32), 'training/v_loss': Array(180.79594, dtype=float32), 'eval/episode_goal_distance': (Array(0.3014971, dtype=float32), Array(0.05857411, dtype=float32)), 'eval/episode_reward': (Array(-14639.6875, dtype=float32), Array(6223.101, dtype=float32)), 'eval/avg_episode_length': (Array(883.5469, dtype=float32), Array(319.6288, dtype=float32)), 'eval/epoch_eval_time': 4.113266944885254, 'eval/sps': 31118.816676647948}
I0726 22:39:47.868427 140267183036224 train.py:379] starting iteration 76 580.1232743263245
I0726 22:39:55.026143 140267183036224 train.py:394] {'eval/walltime': 327.02660393714905, 'training/sps': 40302.922916280404, 'training/walltime': 254.9386842250824, 'training/entropy_loss': Array(-0.03497421, dtype=float32), 'training/policy_loss': Array(-0.00039379, dtype=float32), 'training/total_loss': Array(194.69562, dtype=float32), 'training/v_loss': Array(194.73099, dtype=float32), 'eval/episode_goal_distance': (Array(0.30322915, dtype=float32), Array(0.07217231, dtype=float32)), 'eval/episode_reward': (Array(-14531.417, dtype=float32), Array(5845.746, dtype=float32)), 'eval/avg_episode_length': (Array(891.2344, dtype=float32), Array(310.37103, dtype=float32)), 'eval/epoch_eval_time': 4.105090856552124, 'eval/sps': 31180.795863677307}
I0726 22:39:55.028597 140267183036224 train.py:379] starting iteration 77 587.2834439277649
I0726 22:40:02.197334 140267183036224 train.py:394] {'eval/walltime': 331.13823318481445, 'training/sps': 40244.108497193585, 'training/walltime': 257.992050409317, 'training/entropy_loss': Array(-0.03603722, dtype=float32), 'training/policy_loss': Array(-0.00017639, dtype=float32), 'training/total_loss': Array(167.94653, dtype=float32), 'training/v_loss': Array(167.98274, dtype=float32), 'eval/episode_goal_distance': (Array(0.3004787, dtype=float32), Array(0.05547273, dtype=float32)), 'eval/episode_reward': (Array(-16185.299, dtype=float32), Array(4054.0483, dtype=float32)), 'eval/avg_episode_length': (Array(968.875, dtype=float32), Array(173.2967, dtype=float32)), 'eval/epoch_eval_time': 4.111629247665405, 'eval/sps': 31131.21156842601}
I0726 22:40:02.199856 140267183036224 train.py:379] starting iteration 78 594.4547030925751
I0726 22:40:09.362334 140267183036224 train.py:394] {'eval/walltime': 335.2472379207611, 'training/sps': 40290.07161274708, 'training/walltime': 261.04193329811096, 'training/entropy_loss': Array(-0.03523586, dtype=float32), 'training/policy_loss': Array(0.00011911, dtype=float32), 'training/total_loss': Array(189.62694, dtype=float32), 'training/v_loss': Array(189.66205, dtype=float32), 'eval/episode_goal_distance': (Array(0.3007514, dtype=float32), Array(0.06294352, dtype=float32)), 'eval/episode_reward': (Array(-14244.135, dtype=float32), Array(6262.5747, dtype=float32)), 'eval/avg_episode_length': (Array(860.2656, dtype=float32), Array(345.43335, dtype=float32)), 'eval/epoch_eval_time': 4.109004735946655, 'eval/sps': 31151.09575810957}
I0726 22:40:09.364736 140267183036224 train.py:379] starting iteration 79 601.6195828914642
I0726 22:40:16.530491 140267183036224 train.py:394] {'eval/walltime': 339.361070394516, 'training/sps': 40311.386756677864, 'training/walltime': 264.09020352363586, 'training/entropy_loss': Array(-0.03435705, dtype=float32), 'training/policy_loss': Array(0.00055293, dtype=float32), 'training/total_loss': Array(176.229, dtype=float32), 'training/v_loss': Array(176.26282, dtype=float32), 'eval/episode_goal_distance': (Array(0.31057927, dtype=float32), Array(0.06291723, dtype=float32)), 'eval/episode_reward': (Array(-15671.494, dtype=float32), Array(5776.08, dtype=float32)), 'eval/avg_episode_length': (Array(914.65625, dtype=float32), Array(278.33575, dtype=float32)), 'eval/epoch_eval_time': 4.113832473754883, 'eval/sps': 31114.538770502862}
I0726 22:40:16.532933 140267183036224 train.py:379] starting iteration 80 608.7877786159515
I0726 22:40:23.695675 140267183036224 train.py:394] {'eval/walltime': 343.4712677001953, 'training/sps': 40303.31056760224, 'training/walltime': 267.1390845775604, 'training/entropy_loss': Array(-0.03284056, dtype=float32), 'training/policy_loss': Array(-0.00019241, dtype=float32), 'training/total_loss': Array(154.80136, dtype=float32), 'training/v_loss': Array(154.8344, dtype=float32), 'eval/episode_goal_distance': (Array(0.29185656, dtype=float32), Array(0.06458227, dtype=float32)), 'eval/episode_reward': (Array(-14436.92, dtype=float32), Array(5561.192, dtype=float32)), 'eval/avg_episode_length': (Array(906.78906, dtype=float32), Array(289.80487, dtype=float32)), 'eval/epoch_eval_time': 4.110197305679321, 'eval/sps': 31142.05729810933}
I0726 22:40:23.698196 140267183036224 train.py:379] starting iteration 81 615.9530427455902
I0726 22:40:30.864344 140267183036224 train.py:394] {'eval/walltime': 347.5827987194061, 'training/sps': 40274.80500446003, 'training/walltime': 270.19012355804443, 'training/entropy_loss': Array(-0.029715, dtype=float32), 'training/policy_loss': Array(2.8459692e-05, dtype=float32), 'training/total_loss': Array(154.49484, dtype=float32), 'training/v_loss': Array(154.52454, dtype=float32), 'eval/episode_goal_distance': (Array(0.29838502, dtype=float32), Array(0.06642723, dtype=float32)), 'eval/episode_reward': (Array(-14370.627, dtype=float32), Array(6016.688, dtype=float32)), 'eval/avg_episode_length': (Array(883.52344, dtype=float32), Array(319.69296, dtype=float32)), 'eval/epoch_eval_time': 4.111531019210815, 'eval/sps': 31131.955323194634}
I0726 22:40:30.866731 140267183036224 train.py:379] starting iteration 82 623.1215779781342
I0726 22:40:38.016472 140267183036224 train.py:394] {'eval/walltime': 351.6784567832947, 'training/sps': 40282.75013070109, 'training/walltime': 273.2405607700348, 'training/entropy_loss': Array(-0.02807601, dtype=float32), 'training/policy_loss': Array(0.00070334, dtype=float32), 'training/total_loss': Array(138.93256, dtype=float32), 'training/v_loss': Array(138.95992, dtype=float32), 'eval/episode_goal_distance': (Array(0.2931494, dtype=float32), Array(0.05414501, dtype=float32)), 'eval/episode_reward': (Array(-13980.639, dtype=float32), Array(5202.5547, dtype=float32)), 'eval/avg_episode_length': (Array(906.8672, dtype=float32), Array(289.56207, dtype=float32)), 'eval/epoch_eval_time': 4.09565806388855, 'eval/sps': 31252.608983297956}
I0726 22:40:38.018880 140267183036224 train.py:379] starting iteration 83 630.2737267017365
I0726 22:40:45.194637 140267183036224 train.py:394] {'eval/walltime': 355.7935209274292, 'training/sps': 40195.84239865407, 'training/walltime': 276.29759335517883, 'training/entropy_loss': Array(-0.02873139, dtype=float32), 'training/policy_loss': Array(5.554771e-05, dtype=float32), 'training/total_loss': Array(618.02496, dtype=float32), 'training/v_loss': Array(618.0536, dtype=float32), 'eval/episode_goal_distance': (Array(0.29263055, dtype=float32), Array(0.06041446, dtype=float32)), 'eval/episode_reward': (Array(-14230.809, dtype=float32), Array(5491.2627, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.30685, dtype=float32)), 'eval/epoch_eval_time': 4.1150641441345215, 'eval/sps': 31105.2259495024}
I0726 22:40:45.197218 140267183036224 train.py:379] starting iteration 84 637.4520647525787
I0726 22:40:52.354313 140267183036224 train.py:394] {'eval/walltime': 359.8950538635254, 'training/sps': 40263.0252948875, 'training/walltime': 279.349524974823, 'training/entropy_loss': Array(-0.02936233, dtype=float32), 'training/policy_loss': Array(0.00042938, dtype=float32), 'training/total_loss': Array(305.74945, dtype=float32), 'training/v_loss': Array(305.77838, dtype=float32), 'eval/episode_goal_distance': (Array(0.292274, dtype=float32), Array(0.05730043, dtype=float32)), 'eval/episode_reward': (Array(-15037.141, dtype=float32), Array(4238.814, dtype=float32)), 'eval/avg_episode_length': (Array(953.4531, dtype=float32), Array(209.89186, dtype=float32)), 'eval/epoch_eval_time': 4.101532936096191, 'eval/sps': 31207.8439925511}
I0726 22:40:52.356687 140267183036224 train.py:379] starting iteration 85 644.6115334033966
I0726 22:40:59.520794 140267183036224 train.py:394] {'eval/walltime': 364.0035798549652, 'training/sps': 40259.39586543636, 'training/walltime': 282.40173172950745, 'training/entropy_loss': Array(-0.02765262, dtype=float32), 'training/policy_loss': Array(-0.00013695, dtype=float32), 'training/total_loss': Array(227.27293, dtype=float32), 'training/v_loss': Array(227.30072, dtype=float32), 'eval/episode_goal_distance': (Array(0.29345128, dtype=float32), Array(0.0607528, dtype=float32)), 'eval/episode_reward': (Array(-14527.612, dtype=float32), Array(5131.2246, dtype=float32)), 'eval/avg_episode_length': (Array(922.3594, dtype=float32), Array(266.70465, dtype=float32)), 'eval/epoch_eval_time': 4.108525991439819, 'eval/sps': 31154.72562828861}
I0726 22:40:59.523334 140267183036224 train.py:379] starting iteration 86 651.7781810760498
I0726 22:41:06.683941 140267183036224 train.py:394] {'eval/walltime': 368.10792779922485, 'training/sps': 40251.90633902063, 'training/walltime': 285.4545063972473, 'training/entropy_loss': Array(-0.02784066, dtype=float32), 'training/policy_loss': Array(-6.965263e-05, dtype=float32), 'training/total_loss': Array(155.27449, dtype=float32), 'training/v_loss': Array(155.3024, dtype=float32), 'eval/episode_goal_distance': (Array(0.29690236, dtype=float32), Array(0.0611505, dtype=float32)), 'eval/episode_reward': (Array(-14865.773, dtype=float32), Array(4838.4805, dtype=float32)), 'eval/avg_episode_length': (Array(930.1172, dtype=float32), Array(254.11082, dtype=float32)), 'eval/epoch_eval_time': 4.1043479442596436, 'eval/sps': 31186.439780043813}
I0726 22:41:06.686374 140267183036224 train.py:379] starting iteration 87 658.941220998764
I0726 22:41:13.855763 140267183036224 train.py:394] {'eval/walltime': 372.21864342689514, 'training/sps': 40222.509708612466, 'training/walltime': 288.5095121860504, 'training/entropy_loss': Array(-0.02714712, dtype=float32), 'training/policy_loss': Array(-0.00070351, dtype=float32), 'training/total_loss': Array(140.88031, dtype=float32), 'training/v_loss': Array(140.90816, dtype=float32), 'eval/episode_goal_distance': (Array(0.29739934, dtype=float32), Array(0.06285045, dtype=float32)), 'eval/episode_reward': (Array(-14802.643, dtype=float32), Array(4846.944, dtype=float32)), 'eval/avg_episode_length': (Array(930.09375, dtype=float32), Array(254.1962, dtype=float32)), 'eval/epoch_eval_time': 4.110715627670288, 'eval/sps': 31138.130582032714}
I0726 22:41:13.858282 140267183036224 train.py:379] starting iteration 88 666.1131281852722
I0726 22:41:21.022017 140267183036224 train.py:394] {'eval/walltime': 376.32413721084595, 'training/sps': 40227.10265248014, 'training/walltime': 291.5641691684723, 'training/entropy_loss': Array(-0.02403435, dtype=float32), 'training/policy_loss': Array(-3.702412e-05, dtype=float32), 'training/total_loss': Array(120.01679, dtype=float32), 'training/v_loss': Array(120.04086, dtype=float32), 'eval/episode_goal_distance': (Array(0.29186732, dtype=float32), Array(0.05344743, dtype=float32)), 'eval/episode_reward': (Array(-14326.86, dtype=float32), Array(4840.2563, dtype=float32)), 'eval/avg_episode_length': (Array(922.3594, dtype=float32), Array(266.70477, dtype=float32)), 'eval/epoch_eval_time': 4.105493783950806, 'eval/sps': 31177.73567222962}
I0726 22:41:21.024414 140267183036224 train.py:379] starting iteration 89 673.2792613506317
I0726 22:41:28.194188 140267183036224 train.py:394] {'eval/walltime': 380.43783259391785, 'training/sps': 40256.03120780026, 'training/walltime': 294.6166310310364, 'training/entropy_loss': Array(-0.02448238, dtype=float32), 'training/policy_loss': Array(0.00016141, dtype=float32), 'training/total_loss': Array(119.585014, dtype=float32), 'training/v_loss': Array(119.60934, dtype=float32), 'eval/episode_goal_distance': (Array(0.28940085, dtype=float32), Array(0.06376889, dtype=float32)), 'eval/episode_reward': (Array(-14157.646, dtype=float32), Array(4778.8203, dtype=float32)), 'eval/avg_episode_length': (Array(930.1172, dtype=float32), Array(254.11107, dtype=float32)), 'eval/epoch_eval_time': 4.113695383071899, 'eval/sps': 31115.575676003526}
I0726 22:41:28.196636 140267183036224 train.py:379] starting iteration 90 680.4514830112457
I0726 22:41:35.366744 140267183036224 train.py:394] {'eval/walltime': 384.548033952713, 'training/sps': 40228.41197485775, 'training/walltime': 297.67118859291077, 'training/entropy_loss': Array(-0.02330685, dtype=float32), 'training/policy_loss': Array(-6.3799744e-05, dtype=float32), 'training/total_loss': Array(123.4049, dtype=float32), 'training/v_loss': Array(123.42826, dtype=float32), 'eval/episode_goal_distance': (Array(0.3032599, dtype=float32), Array(0.06135114, dtype=float32)), 'eval/episode_reward': (Array(-14491.695, dtype=float32), Array(4650.6245, dtype=float32)), 'eval/avg_episode_length': (Array(930.0469, dtype=float32), Array(254.36652, dtype=float32)), 'eval/epoch_eval_time': 4.110201358795166, 'eval/sps': 31142.026588575936}
I0726 22:41:35.369143 140267183036224 train.py:379] starting iteration 91 687.6239898204803
I0726 22:41:42.542969 140267183036224 train.py:394] {'eval/walltime': 388.6623899936676, 'training/sps': 40213.65014294915, 'training/walltime': 300.72686743736267, 'training/entropy_loss': Array(-0.0196133, dtype=float32), 'training/policy_loss': Array(0.00034119, dtype=float32), 'training/total_loss': Array(101.84761, dtype=float32), 'training/v_loss': Array(101.866875, dtype=float32), 'eval/episode_goal_distance': (Array(0.2944035, dtype=float32), Array(0.06135949, dtype=float32)), 'eval/episode_reward': (Array(-13768.152, dtype=float32), Array(5556.5034, dtype=float32)), 'eval/avg_episode_length': (Array(891.2578, dtype=float32), Array(310.30392, dtype=float32)), 'eval/epoch_eval_time': 4.11435604095459, 'eval/sps': 31110.579329032047}
I0726 22:41:42.547345 140267183036224 train.py:379] starting iteration 92 694.8021759986877
I0726 22:41:49.721781 140267183036224 train.py:394] {'eval/walltime': 392.7766742706299, 'training/sps': 40208.674429896884, 'training/walltime': 303.78292441368103, 'training/entropy_loss': Array(-0.02040298, dtype=float32), 'training/policy_loss': Array(0.00041917, dtype=float32), 'training/total_loss': Array(91.19527, dtype=float32), 'training/v_loss': Array(91.21524, dtype=float32), 'eval/episode_goal_distance': (Array(0.29273903, dtype=float32), Array(0.0574168, dtype=float32)), 'eval/episode_reward': (Array(-14667.631, dtype=float32), Array(4435.353, dtype=float32)), 'eval/avg_episode_length': (Array(945.625, dtype=float32), Array(226.0703, dtype=float32)), 'eval/epoch_eval_time': 4.11428427696228, 'eval/sps': 31111.12197976433}
I0726 22:41:49.724295 140267183036224 train.py:379] starting iteration 93 701.9791417121887
I0726 22:41:56.893813 140267183036224 train.py:394] {'eval/walltime': 396.89411449432373, 'training/sps': 40307.799041333405, 'training/walltime': 306.83146595954895, 'training/entropy_loss': Array(-0.02087006, dtype=float32), 'training/policy_loss': Array(0.00017944, dtype=float32), 'training/total_loss': Array(97.25192, dtype=float32), 'training/v_loss': Array(97.27261, dtype=float32), 'eval/episode_goal_distance': (Array(0.29005188, dtype=float32), Array(0.06737058, dtype=float32)), 'eval/episode_reward': (Array(-14271.352, dtype=float32), Array(5153.445, dtype=float32)), 'eval/avg_episode_length': (Array(922.46875, dtype=float32), Array(266.32916, dtype=float32)), 'eval/epoch_eval_time': 4.117440223693848, 'eval/sps': 31087.275842748808}
I0726 22:41:56.896195 140267183036224 train.py:379] starting iteration 94 709.151041507721
I0726 22:42:04.070987 140267183036224 train.py:394] {'eval/walltime': 401.012699842453, 'training/sps': 40253.814614542185, 'training/walltime': 309.8840959072113, 'training/entropy_loss': Array(-0.02454978, dtype=float32), 'training/policy_loss': Array(0.00041703, dtype=float32), 'training/total_loss': Array(109.37838, dtype=float32), 'training/v_loss': Array(109.40251, dtype=float32), 'eval/episode_goal_distance': (Array(0.28930008, dtype=float32), Array(0.06322277, dtype=float32)), 'eval/episode_reward': (Array(-14397.803, dtype=float32), Array(4715.0166, dtype=float32)), 'eval/avg_episode_length': (Array(930.08594, dtype=float32), Array(254.2248, dtype=float32)), 'eval/epoch_eval_time': 4.1185853481292725, 'eval/sps': 31078.632389672257}
I0726 22:42:04.073333 140267183036224 train.py:379] starting iteration 95 716.3281803131104
I0726 22:42:11.246429 140267183036224 train.py:394] {'eval/walltime': 405.1300609111786, 'training/sps': 40284.33701316958, 'training/walltime': 312.9344129562378, 'training/entropy_loss': Array(-0.02492335, dtype=float32), 'training/policy_loss': Array(0.00013044, dtype=float32), 'training/total_loss': Array(113.80179, dtype=float32), 'training/v_loss': Array(113.826584, dtype=float32), 'eval/episode_goal_distance': (Array(0.2887292, dtype=float32), Array(0.06124411, dtype=float32)), 'eval/episode_reward': (Array(-13722.417, dtype=float32), Array(5213.878, dtype=float32)), 'eval/avg_episode_length': (Array(899.0625, dtype=float32), Array(300.21375, dtype=float32)), 'eval/epoch_eval_time': 4.117361068725586, 'eval/sps': 31087.873485824457}
I0726 22:42:11.248931 140267183036224 train.py:379] starting iteration 96 723.5037772655487
I0726 22:42:18.399663 140267183036224 train.py:394] {'eval/walltime': 409.2349100112915, 'training/sps': 40391.15894904132, 'training/walltime': 315.9766628742218, 'training/entropy_loss': Array(-0.02170794, dtype=float32), 'training/policy_loss': Array(0.00051573, dtype=float32), 'training/total_loss': Array(100.57286, dtype=float32), 'training/v_loss': Array(100.59405, dtype=float32), 'eval/episode_goal_distance': (Array(0.2918917, dtype=float32), Array(0.05311546, dtype=float32)), 'eval/episode_reward': (Array(-13926.625, dtype=float32), Array(4748.171, dtype=float32)), 'eval/avg_episode_length': (Array(914.53125, dtype=float32), Array(278.7432, dtype=float32)), 'eval/epoch_eval_time': 4.104849100112915, 'eval/sps': 31182.63226691549}
I0726 22:42:18.402074 140267183036224 train.py:379] starting iteration 97 730.6569204330444
I0726 22:42:25.571631 140267183036224 train.py:394] {'eval/walltime': 413.35799074172974, 'training/sps': 40382.62044772397, 'training/walltime': 319.0195560455322, 'training/entropy_loss': Array(-0.02219783, dtype=float32), 'training/policy_loss': Array(7.913679e-05, dtype=float32), 'training/total_loss': Array(92.7812, dtype=float32), 'training/v_loss': Array(92.803314, dtype=float32), 'eval/episode_goal_distance': (Array(0.2893237, dtype=float32), Array(0.06359238, dtype=float32)), 'eval/episode_reward': (Array(-14373.518, dtype=float32), Array(4290.7217, dtype=float32)), 'eval/avg_episode_length': (Array(945.64844, dtype=float32), Array(225.97287, dtype=float32)), 'eval/epoch_eval_time': 4.123080730438232, 'eval/sps': 31044.747451839292}
I0726 22:42:25.574227 140267183036224 train.py:379] starting iteration 98 737.8290739059448
I0726 22:42:32.754036 140267183036224 train.py:394] {'eval/walltime': 417.4837167263031, 'training/sps': 40283.78284952955, 'training/walltime': 322.06991505622864, 'training/entropy_loss': Array(-0.02308246, dtype=float32), 'training/policy_loss': Array(0.0001973, dtype=float32), 'training/total_loss': Array(87.15463, dtype=float32), 'training/v_loss': Array(87.17752, dtype=float32), 'eval/episode_goal_distance': (Array(0.29645124, dtype=float32), Array(0.05332118, dtype=float32)), 'eval/episode_reward': (Array(-14725.167, dtype=float32), Array(5002.2495, dtype=float32)), 'eval/avg_episode_length': (Array(922.35156, dtype=float32), Array(266.73172, dtype=float32)), 'eval/epoch_eval_time': 4.125725984573364, 'eval/sps': 31024.84277400122}
I0726 22:42:32.756627 140267183036224 train.py:379] starting iteration 99 745.0114738941193
I0726 22:42:39.912628 140267183036224 train.py:394] {'eval/walltime': 421.59216356277466, 'training/sps': 40368.04561482905, 'training/walltime': 325.11390686035156, 'training/entropy_loss': Array(-0.02322673, dtype=float32), 'training/policy_loss': Array(0.00019226, dtype=float32), 'training/total_loss': Array(87.58621, dtype=float32), 'training/v_loss': Array(87.60925, dtype=float32), 'eval/episode_goal_distance': (Array(0.28648376, dtype=float32), Array(0.05785638, dtype=float32)), 'eval/episode_reward': (Array(-14308.896, dtype=float32), Array(4171.6206, dtype=float32)), 'eval/avg_episode_length': (Array(945.6406, dtype=float32), Array(226.00536, dtype=float32)), 'eval/epoch_eval_time': 4.108446836471558, 'eval/sps': 31155.325867604453}
I0726 22:42:39.915168 140267183036224 train.py:379] starting iteration 100 752.1700148582458
I0726 22:42:47.101188 140267183036224 train.py:394] {'eval/walltime': 425.7258174419403, 'training/sps': 40304.92743767489, 'training/walltime': 328.16266560554504, 'training/entropy_loss': Array(-0.02257285, dtype=float32), 'training/policy_loss': Array(9.118804e-05, dtype=float32), 'training/total_loss': Array(631.77484, dtype=float32), 'training/v_loss': Array(631.79736, dtype=float32), 'eval/episode_goal_distance': (Array(0.29886448, dtype=float32), Array(0.06250381, dtype=float32)), 'eval/episode_reward': (Array(-14205.219, dtype=float32), Array(5058.0723, dtype=float32)), 'eval/avg_episode_length': (Array(914.6094, dtype=float32), Array(278.48907, dtype=float32)), 'eval/epoch_eval_time': 4.133653879165649, 'eval/sps': 30965.340529632333}
I0726 22:42:47.103601 140267183036224 train.py:379] starting iteration 101 759.3584470748901
I0726 22:42:54.263124 140267183036224 train.py:394] {'eval/walltime': 429.8369643688202, 'training/sps': 40357.775541404386, 'training/walltime': 331.20743203163147, 'training/entropy_loss': Array(-0.02219944, dtype=float32), 'training/policy_loss': Array(0.00057643, dtype=float32), 'training/total_loss': Array(215.98665, dtype=float32), 'training/v_loss': Array(216.00827, dtype=float32), 'eval/episode_goal_distance': (Array(0.29867578, dtype=float32), Array(0.05643099, dtype=float32)), 'eval/episode_reward': (Array(-14030.412, dtype=float32), Array(5386.6084, dtype=float32)), 'eval/avg_episode_length': (Array(891.28125, dtype=float32), Array(310.23697, dtype=float32)), 'eval/epoch_eval_time': 4.111146926879883, 'eval/sps': 31134.8638899521}
I0726 22:42:54.265672 140267183036224 train.py:379] starting iteration 102 766.5205194950104
I0726 22:43:01.411550 140267183036224 train.py:394] {'eval/walltime': 433.9357714653015, 'training/sps': 40374.66750926121, 'training/walltime': 334.25092458724976, 'training/entropy_loss': Array(-0.02354305, dtype=float32), 'training/policy_loss': Array(0.00024502, dtype=float32), 'training/total_loss': Array(160.17517, dtype=float32), 'training/v_loss': Array(160.19846, dtype=float32), 'eval/episode_goal_distance': (Array(0.2990532, dtype=float32), Array(0.06094112, dtype=float32)), 'eval/episode_reward': (Array(-14935.909, dtype=float32), Array(4337.363, dtype=float32)), 'eval/avg_episode_length': (Array(945.6797, dtype=float32), Array(225.84293, dtype=float32)), 'eval/epoch_eval_time': 4.098807096481323, 'eval/sps': 31228.598220658725}
I0726 22:43:01.489867 140267183036224 train.py:379] starting iteration 103 773.7446973323822
I0726 22:43:08.672045 140267183036224 train.py:394] {'eval/walltime': 438.0480420589447, 'training/sps': 40080.59694881216, 'training/walltime': 337.3167471885681, 'training/entropy_loss': Array(-0.02162599, dtype=float32), 'training/policy_loss': Array(0.00023544, dtype=float32), 'training/total_loss': Array(127.291534, dtype=float32), 'training/v_loss': Array(127.31293, dtype=float32), 'eval/episode_goal_distance': (Array(0.29703745, dtype=float32), Array(0.05700276, dtype=float32)), 'eval/episode_reward': (Array(-14630.686, dtype=float32), Array(4206.8804, dtype=float32)), 'eval/avg_episode_length': (Array(945.5781, dtype=float32), Array(226.26527, dtype=float32)), 'eval/epoch_eval_time': 4.1122705936431885, 'eval/sps': 31126.356373013095}
I0726 22:43:08.674736 140267183036224 train.py:379] starting iteration 104 780.9295823574066
I0726 22:43:15.839642 140267183036224 train.py:394] {'eval/walltime': 442.15127420425415, 'training/sps': 40182.78359036139, 'training/walltime': 340.3747732639313, 'training/entropy_loss': Array(-0.02045987, dtype=float32), 'training/policy_loss': Array(8.9118694e-05, dtype=float32), 'training/total_loss': Array(113.17006, dtype=float32), 'training/v_loss': Array(113.19043, dtype=float32), 'eval/episode_goal_distance': (Array(0.29033184, dtype=float32), Array(0.06057242, dtype=float32)), 'eval/episode_reward': (Array(-14535.074, dtype=float32), Array(4705.6343, dtype=float32)), 'eval/avg_episode_length': (Array(937.8828, dtype=float32), Array(240.57916, dtype=float32)), 'eval/epoch_eval_time': 4.103232145309448, 'eval/sps': 31194.92036206662}
I0726 22:43:15.842115 140267183036224 train.py:379] starting iteration 105 788.0969619750977
I0726 22:43:23.006031 140267183036224 train.py:394] {'eval/walltime': 446.25753569602966, 'training/sps': 40234.966147412546, 'training/walltime': 343.4288332462311, 'training/entropy_loss': Array(-0.02012851, dtype=float32), 'training/policy_loss': Array(0.0002019, dtype=float32), 'training/total_loss': Array(89.455124, dtype=float32), 'training/v_loss': Array(89.47505, dtype=float32), 'eval/episode_goal_distance': (Array(0.2940154, dtype=float32), Array(0.05281986, dtype=float32)), 'eval/episode_reward': (Array(-14431.963, dtype=float32), Array(4345.735, dtype=float32)), 'eval/avg_episode_length': (Array(937.83594, dtype=float32), Array(240.76074, dtype=float32)), 'eval/epoch_eval_time': 4.106261491775513, 'eval/sps': 31171.906673837737}
I0726 22:43:23.008436 140267183036224 train.py:379] starting iteration 106 795.2632830142975
I0726 22:43:30.176849 140267183036224 train.py:394] {'eval/walltime': 450.36536383628845, 'training/sps': 40197.03995915688, 'training/walltime': 346.4857747554779, 'training/entropy_loss': Array(-0.01893863, dtype=float32), 'training/policy_loss': Array(0.00044567, dtype=float32), 'training/total_loss': Array(84.51571, dtype=float32), 'training/v_loss': Array(84.534195, dtype=float32), 'eval/episode_goal_distance': (Array(0.28636417, dtype=float32), Array(0.05553485, dtype=float32)), 'eval/episode_reward': (Array(-13699.57, dtype=float32), Array(4538.7974, dtype=float32)), 'eval/avg_episode_length': (Array(922.3906, dtype=float32), Array(266.59723, dtype=float32)), 'eval/epoch_eval_time': 4.107828140258789, 'eval/sps': 31160.01829422594}
I0726 22:43:30.179453 140267183036224 train.py:379] starting iteration 107 802.434298992157
I0726 22:43:37.353739 140267183036224 train.py:394] {'eval/walltime': 454.4787356853485, 'training/sps': 40192.48522072697, 'training/walltime': 349.54306268692017, 'training/entropy_loss': Array(-0.01608298, dtype=float32), 'training/policy_loss': Array(0.00048502, dtype=float32), 'training/total_loss': Array(78.8621, dtype=float32), 'training/v_loss': Array(78.87769, dtype=float32), 'eval/episode_goal_distance': (Array(0.292919, dtype=float32), Array(0.06282199, dtype=float32)), 'eval/episode_reward': (Array(-14503.688, dtype=float32), Array(4041.088, dtype=float32)), 'eval/avg_episode_length': (Array(961.1328, dtype=float32), Array(192.77544, dtype=float32)), 'eval/epoch_eval_time': 4.113371849060059, 'eval/sps': 31118.023047016555}
I0726 22:43:37.356178 140267183036224 train.py:379] starting iteration 108 809.6110243797302
I0726 22:43:44.523733 140267183036224 train.py:394] {'eval/walltime': 458.58251190185547, 'training/sps': 40155.195912335104, 'training/walltime': 352.60318970680237, 'training/entropy_loss': Array(-0.01280339, dtype=float32), 'training/policy_loss': Array(0.00084086, dtype=float32), 'training/total_loss': Array(79.20134, dtype=float32), 'training/v_loss': Array(79.2133, dtype=float32), 'eval/episode_goal_distance': (Array(0.28840226, dtype=float32), Array(0.05705613, dtype=float32)), 'eval/episode_reward': (Array(-14017.402, dtype=float32), Array(4346.7065, dtype=float32)), 'eval/avg_episode_length': (Array(937.9375, dtype=float32), Array(240.36728, dtype=float32)), 'eval/epoch_eval_time': 4.103776216506958, 'eval/sps': 31190.7845961812}
I0726 22:43:44.526285 140267183036224 train.py:379] starting iteration 109 816.7811307907104
I0726 22:43:51.688602 140267183036224 train.py:394] {'eval/walltime': 462.6862835884094, 'training/sps': 40223.57073303216, 'training/walltime': 355.65811491012573, 'training/entropy_loss': Array(-0.01471728, dtype=float32), 'training/policy_loss': Array(0.00078701, dtype=float32), 'training/total_loss': Array(63.743443, dtype=float32), 'training/v_loss': Array(63.75737, dtype=float32), 'eval/episode_goal_distance': (Array(0.28579858, dtype=float32), Array(0.05783875, dtype=float32)), 'eval/episode_reward': (Array(-14556.889, dtype=float32), Array(4114.019, dtype=float32)), 'eval/avg_episode_length': (Array(953.4453, dtype=float32), Array(209.92691, dtype=float32)), 'eval/epoch_eval_time': 4.103771686553955, 'eval/sps': 31190.819026163943}
I0726 22:43:51.691018 140267183036224 train.py:379] starting iteration 110 823.9458646774292
I0726 22:43:58.851941 140267183036224 train.py:394] {'eval/walltime': 466.78997445106506, 'training/sps': 40239.19436053625, 'training/walltime': 358.71185398101807, 'training/entropy_loss': Array(-0.0121386, dtype=float32), 'training/policy_loss': Array(0.00083899, dtype=float32), 'training/total_loss': Array(62.027817, dtype=float32), 'training/v_loss': Array(62.039116, dtype=float32), 'eval/episode_goal_distance': (Array(0.2888285, dtype=float32), Array(0.0634184, dtype=float32)), 'eval/episode_reward': (Array(-14260.23, dtype=float32), Array(4214.6846, dtype=float32)), 'eval/avg_episode_length': (Array(945.64844, dtype=float32), Array(225.9727, dtype=float32)), 'eval/epoch_eval_time': 4.10369086265564, 'eval/sps': 31191.43334231731}
I0726 22:43:58.854363 140267183036224 train.py:379] starting iteration 111 831.1091985702515
I0726 22:44:06.023271 140267183036224 train.py:394] {'eval/walltime': 470.8966546058655, 'training/sps': 40173.878885355785, 'training/walltime': 361.7705578804016, 'training/entropy_loss': Array(-0.01284494, dtype=float32), 'training/policy_loss': Array(0.00102501, dtype=float32), 'training/total_loss': Array(71.98858, dtype=float32), 'training/v_loss': Array(72.0004, dtype=float32), 'eval/episode_goal_distance': (Array(0.29509726, dtype=float32), Array(0.055554, dtype=float32)), 'eval/episode_reward': (Array(-14736.932, dtype=float32), Array(3920.8123, dtype=float32)), 'eval/avg_episode_length': (Array(961.125, dtype=float32), Array(192.81381, dtype=float32)), 'eval/epoch_eval_time': 4.106680154800415, 'eval/sps': 31168.72879675744}
I0726 22:44:06.025682 140267183036224 train.py:379] starting iteration 112 838.2805287837982
I0726 22:44:13.187199 140267183036224 train.py:394] {'eval/walltime': 474.9958028793335, 'training/sps': 40173.39664679755, 'training/walltime': 364.82929849624634, 'training/entropy_loss': Array(-0.01445447, dtype=float32), 'training/policy_loss': Array(0.00096911, dtype=float32), 'training/total_loss': Array(74.05701, dtype=float32), 'training/v_loss': Array(74.07048, dtype=float32), 'eval/episode_goal_distance': (Array(0.28940147, dtype=float32), Array(0.06125749, dtype=float32)), 'eval/episode_reward': (Array(-14555.885, dtype=float32), Array(4099.9614, dtype=float32)), 'eval/avg_episode_length': (Array(953.3828, dtype=float32), Array(210.2088, dtype=float32)), 'eval/epoch_eval_time': 4.099148273468018, 'eval/sps': 31225.99902728273}
I0726 22:44:13.189762 140267183036224 train.py:379] starting iteration 113 845.4446086883545
I0726 22:44:20.353598 140267183036224 train.py:394] {'eval/walltime': 479.1005208492279, 'training/sps': 40216.442854293644, 'training/walltime': 367.88476514816284, 'training/entropy_loss': Array(-0.01513676, dtype=float32), 'training/policy_loss': Array(0.00096688, dtype=float32), 'training/total_loss': Array(80.382706, dtype=float32), 'training/v_loss': Array(80.39687, dtype=float32), 'eval/episode_goal_distance': (Array(0.28204548, dtype=float32), Array(0.05822743, dtype=float32)), 'eval/episode_reward': (Array(-13866.283, dtype=float32), Array(4580.4194, dtype=float32)), 'eval/avg_episode_length': (Array(930.1172, dtype=float32), Array(254.11145, dtype=float32)), 'eval/epoch_eval_time': 4.104717969894409, 'eval/sps': 31183.628434109618}
I0726 22:44:20.356150 140267183036224 train.py:379] starting iteration 114 852.610996723175
I0726 22:44:27.516935 140267183036224 train.py:394] {'eval/walltime': 483.2010238170624, 'training/sps': 40200.325779556086, 'training/walltime': 370.94145679473877, 'training/entropy_loss': Array(-0.01482089, dtype=float32), 'training/policy_loss': Array(0.00113, dtype=float32), 'training/total_loss': Array(66.0587, dtype=float32), 'training/v_loss': Array(66.07239, dtype=float32), 'eval/episode_goal_distance': (Array(0.2956074, dtype=float32), Array(0.05761339, dtype=float32)), 'eval/episode_reward': (Array(-14856.616, dtype=float32), Array(4308.1514, dtype=float32)), 'eval/avg_episode_length': (Array(953.5, dtype=float32), Array(209.68059, dtype=float32)), 'eval/epoch_eval_time': 4.100502967834473, 'eval/sps': 31215.68280868686}
I0726 22:44:27.519495 140267183036224 train.py:379] starting iteration 115 859.7743399143219
I0726 22:44:34.684382 140267183036224 train.py:394] {'eval/walltime': 487.29760217666626, 'training/sps': 40123.29135250853, 'training/walltime': 374.0040171146393, 'training/entropy_loss': Array(-0.0160307, dtype=float32), 'training/policy_loss': Array(0.00070225, dtype=float32), 'training/total_loss': Array(71.943886, dtype=float32), 'training/v_loss': Array(71.95921, dtype=float32), 'eval/episode_goal_distance': (Array(0.27408463, dtype=float32), Array(0.06377723, dtype=float32)), 'eval/episode_reward': (Array(-13932.452, dtype=float32), Array(4319.8755, dtype=float32)), 'eval/avg_episode_length': (Array(945.6719, dtype=float32), Array(225.87575, dtype=float32)), 'eval/epoch_eval_time': 4.096578359603882, 'eval/sps': 31245.58808936757}
I0726 22:44:34.686829 140267183036224 train.py:379] starting iteration 116 866.9416744709015
I0726 22:44:41.847789 140267183036224 train.py:394] {'eval/walltime': 491.39743995666504, 'training/sps': 40189.20694955748, 'training/walltime': 377.0615544319153, 'training/entropy_loss': Array(-0.01625699, dtype=float32), 'training/policy_loss': Array(0.00092856, dtype=float32), 'training/total_loss': Array(376.3396, dtype=float32), 'training/v_loss': Array(376.35492, dtype=float32), 'eval/episode_goal_distance': (Array(0.29050106, dtype=float32), Array(0.04971114, dtype=float32)), 'eval/episode_reward': (Array(-13673.424, dtype=float32), Array(4923.7266, dtype=float32)), 'eval/avg_episode_length': (Array(906.875, dtype=float32), Array(289.53766, dtype=float32)), 'eval/epoch_eval_time': 4.099837779998779, 'eval/sps': 31220.74747065678}
I0726 22:44:41.850270 140267183036224 train.py:379] starting iteration 117 874.1051161289215
I0726 22:44:49.012271 140267183036224 train.py:394] {'eval/walltime': 495.4971103668213, 'training/sps': 40173.52190245289, 'training/walltime': 380.12028551101685, 'training/entropy_loss': Array(-0.01558695, dtype=float32), 'training/policy_loss': Array(0.00098569, dtype=float32), 'training/total_loss': Array(241.69524, dtype=float32), 'training/v_loss': Array(241.70984, dtype=float32), 'eval/episode_goal_distance': (Array(0.28212976, dtype=float32), Array(0.05641794, dtype=float32)), 'eval/episode_reward': (Array(-14484.093, dtype=float32), Array(4058.575, dtype=float32)), 'eval/avg_episode_length': (Array(953.46875, dtype=float32), Array(209.82117, dtype=float32)), 'eval/epoch_eval_time': 4.09967041015625, 'eval/sps': 31222.022063749646}
I0726 22:44:49.014755 140267183036224 train.py:379] starting iteration 118 881.2696018218994
I0726 22:44:56.171614 140267183036224 train.py:394] {'eval/walltime': 499.5892217159271, 'training/sps': 40142.16037032009, 'training/walltime': 383.18140625953674, 'training/entropy_loss': Array(-0.01545506, dtype=float32), 'training/policy_loss': Array(0.000911, dtype=float32), 'training/total_loss': Array(137.20807, dtype=float32), 'training/v_loss': Array(137.22261, dtype=float32), 'eval/episode_goal_distance': (Array(0.2950572, dtype=float32), Array(0.05510101, dtype=float32)), 'eval/episode_reward': (Array(-14575.059, dtype=float32), Array(4305.3735, dtype=float32)), 'eval/avg_episode_length': (Array(945.65625, dtype=float32), Array(225.94048, dtype=float32)), 'eval/epoch_eval_time': 4.092111349105835, 'eval/sps': 31279.69624481729}
I0726 22:44:56.174063 140267183036224 train.py:379] starting iteration 119 888.4289088249207
I0726 22:45:03.328307 140267183036224 train.py:394] {'eval/walltime': 503.6854901313782, 'training/sps': 40229.33200535458, 'training/walltime': 386.23589396476746, 'training/entropy_loss': Array(-0.01658539, dtype=float32), 'training/policy_loss': Array(0.00098706, dtype=float32), 'training/total_loss': Array(129.34705, dtype=float32), 'training/v_loss': Array(129.36266, dtype=float32), 'eval/episode_goal_distance': (Array(0.2985966, dtype=float32), Array(0.06014344, dtype=float32)), 'eval/episode_reward': (Array(-14109.191, dtype=float32), Array(5126.336, dtype=float32)), 'eval/avg_episode_length': (Array(906.84375, dtype=float32), Array(289.6351, dtype=float32)), 'eval/epoch_eval_time': 4.09626841545105, 'eval/sps': 31247.952286814587}
I0726 22:45:03.330715 140267183036224 train.py:379] starting iteration 120 895.585560798645
I0726 22:45:10.480592 140267183036224 train.py:394] {'eval/walltime': 507.7745780944824, 'training/sps': 40192.42566801116, 'training/walltime': 389.2931864261627, 'training/entropy_loss': Array(-0.01673829, dtype=float32), 'training/policy_loss': Array(0.00160643, dtype=float32), 'training/total_loss': Array(116.59212, dtype=float32), 'training/v_loss': Array(116.607254, dtype=float32), 'eval/episode_goal_distance': (Array(0.29060766, dtype=float32), Array(0.05131647, dtype=float32)), 'eval/episode_reward': (Array(-14986.552, dtype=float32), Array(4019.4958, dtype=float32)), 'eval/avg_episode_length': (Array(961.1719, dtype=float32), Array(192.58144, dtype=float32)), 'eval/epoch_eval_time': 4.089087963104248, 'eval/sps': 31302.823797125722}
I0726 22:45:10.483062 140267183036224 train.py:379] starting iteration 121 902.7379088401794
I0726 22:45:17.638473 140267183036224 train.py:394] {'eval/walltime': 511.87713718414307, 'training/sps': 40298.498546576746, 'training/walltime': 392.34243154525757, 'training/entropy_loss': Array(-0.01693301, dtype=float32), 'training/policy_loss': Array(0.0010851, dtype=float32), 'training/total_loss': Array(85.34158, dtype=float32), 'training/v_loss': Array(85.35743, dtype=float32), 'eval/episode_goal_distance': (Array(0.28742543, dtype=float32), Array(0.0558486, dtype=float32)), 'eval/episode_reward': (Array(-13672.689, dtype=float32), Array(4994.1045, dtype=float32)), 'eval/avg_episode_length': (Array(906.8203, dtype=float32), Array(289.7078, dtype=float32)), 'eval/epoch_eval_time': 4.1025590896606445, 'eval/sps': 31200.038123177383}
I0726 22:45:17.641176 140267183036224 train.py:379] starting iteration 122 909.8960220813751
I0726 22:45:24.782763 140267183036224 train.py:394] {'eval/walltime': 515.9708218574524, 'training/sps': 40364.46993622929, 'training/walltime': 395.38669300079346, 'training/entropy_loss': Array(-0.01783137, dtype=float32), 'training/policy_loss': Array(0.00121256, dtype=float32), 'training/total_loss': Array(107.90796, dtype=float32), 'training/v_loss': Array(107.92458, dtype=float32), 'eval/episode_goal_distance': (Array(0.28626975, dtype=float32), Array(0.0555782, dtype=float32)), 'eval/episode_reward': (Array(-14600.141, dtype=float32), Array(4076.4924, dtype=float32)), 'eval/avg_episode_length': (Array(953.3672, dtype=float32), Array(210.27913, dtype=float32)), 'eval/epoch_eval_time': 4.093684673309326, 'eval/sps': 31267.67453159138}
I0726 22:45:24.785322 140267183036224 train.py:379] starting iteration 123 917.0401692390442
I0726 22:45:31.947579 140267183036224 train.py:394] {'eval/walltime': 520.0910751819611, 'training/sps': 40441.980344451535, 'training/walltime': 398.4251198768616, 'training/entropy_loss': Array(-0.01529225, dtype=float32), 'training/policy_loss': Array(0.00174013, dtype=float32), 'training/total_loss': Array(61.87867, dtype=float32), 'training/v_loss': Array(61.892216, dtype=float32), 'eval/episode_goal_distance': (Array(0.29365236, dtype=float32), Array(0.05439126, dtype=float32)), 'eval/episode_reward': (Array(-14291.344, dtype=float32), Array(4336.887, dtype=float32)), 'eval/avg_episode_length': (Array(937.8828, dtype=float32), Array(240.57932, dtype=float32)), 'eval/epoch_eval_time': 4.120253324508667, 'eval/sps': 31066.051021332234}
I0726 22:45:31.950075 140267183036224 train.py:379] starting iteration 124 924.2049217224121
I0726 22:45:39.088087 140267183036224 train.py:394] {'eval/walltime': 524.1850869655609, 'training/sps': 40415.401625426675, 'training/walltime': 401.46554493904114, 'training/entropy_loss': Array(-0.01455822, dtype=float32), 'training/policy_loss': Array(0.00122922, dtype=float32), 'training/total_loss': Array(57.956085, dtype=float32), 'training/v_loss': Array(57.969414, dtype=float32), 'eval/episode_goal_distance': (Array(0.28896937, dtype=float32), Array(0.06007155, dtype=float32)), 'eval/episode_reward': (Array(-13355.26, dtype=float32), Array(5219.0015, dtype=float32)), 'eval/avg_episode_length': (Array(891.22656, dtype=float32), Array(310.39294, dtype=float32)), 'eval/epoch_eval_time': 4.0940117835998535, 'eval/sps': 31265.17625395058}
I0726 22:45:39.090481 140267183036224 train.py:379] starting iteration 125 931.3453271389008
I0726 22:45:46.259362 140267183036224 train.py:394] {'eval/walltime': 528.3020918369293, 'training/sps': 40311.03678445312, 'training/walltime': 404.5138416290283, 'training/entropy_loss': Array(-0.01399231, dtype=float32), 'training/policy_loss': Array(0.00189189, dtype=float32), 'training/total_loss': Array(61.744286, dtype=float32), 'training/v_loss': Array(61.756386, dtype=float32), 'eval/episode_goal_distance': (Array(0.28884485, dtype=float32), Array(0.05140081, dtype=float32)), 'eval/episode_reward': (Array(-14345.012, dtype=float32), Array(4373.37, dtype=float32)), 'eval/avg_episode_length': (Array(937.8672, dtype=float32), Array(240.63954, dtype=float32)), 'eval/epoch_eval_time': 4.117004871368408, 'eval/sps': 31090.563164054605}
I0726 22:45:46.262092 140267183036224 train.py:379] starting iteration 126 938.5169386863708
I0726 22:45:53.417144 140267183036224 train.py:394] {'eval/walltime': 532.4081461429596, 'training/sps': 40347.85182220788, 'training/walltime': 407.5593569278717, 'training/entropy_loss': Array(-0.01379371, dtype=float32), 'training/policy_loss': Array(0.00246715, dtype=float32), 'training/total_loss': Array(62.600883, dtype=float32), 'training/v_loss': Array(62.612206, dtype=float32), 'eval/episode_goal_distance': (Array(0.2856533, dtype=float32), Array(0.05937557, dtype=float32)), 'eval/episode_reward': (Array(-13505.615, dtype=float32), Array(5282.4707, dtype=float32)), 'eval/avg_episode_length': (Array(899.1094, dtype=float32), Array(300.0741, dtype=float32)), 'eval/epoch_eval_time': 4.106054306030273, 'eval/sps': 31173.479564557972}
I0726 22:45:53.419738 140267183036224 train.py:379] starting iteration 127 945.6745839118958
I0726 22:46:00.596012 140267183036224 train.py:394] {'eval/walltime': 536.5254125595093, 'training/sps': 40230.0825050809, 'training/walltime': 410.613787651062, 'training/entropy_loss': Array(-0.0139559, dtype=float32), 'training/policy_loss': Array(0.00383053, dtype=float32), 'training/total_loss': Array(61.034904, dtype=float32), 'training/v_loss': Array(61.04503, dtype=float32), 'eval/episode_goal_distance': (Array(0.29816192, dtype=float32), Array(0.06230654, dtype=float32)), 'eval/episode_reward': (Array(-14243.596, dtype=float32), Array(5233.7764, dtype=float32)), 'eval/avg_episode_length': (Array(914.5547, dtype=float32), Array(278.66702, dtype=float32)), 'eval/epoch_eval_time': 4.117266416549683, 'eval/sps': 31088.588167502043}
I0726 22:46:00.598424 140267183036224 train.py:379] starting iteration 128 952.8532700538635
I0726 22:46:07.758053 140267183036224 train.py:394] {'eval/walltime': 540.6363346576691, 'training/sps': 40353.60136085218, 'training/walltime': 413.65886902809143, 'training/entropy_loss': Array(-0.0124657, dtype=float32), 'training/policy_loss': Array(0.00386472, dtype=float32), 'training/total_loss': Array(68.448265, dtype=float32), 'training/v_loss': Array(68.45686, dtype=float32), 'eval/episode_goal_distance': (Array(0.29626924, dtype=float32), Array(0.05467714, dtype=float32)), 'eval/episode_reward': (Array(-15131.696, dtype=float32), Array(3526.3484, dtype=float32)), 'eval/avg_episode_length': (Array(976.6719, dtype=float32), Array(150.58241, dtype=float32)), 'eval/epoch_eval_time': 4.11092209815979, 'eval/sps': 31136.56667376349}
I0726 22:46:07.760514 140267183036224 train.py:379] starting iteration 129 960.0153605937958
I0726 22:46:14.924240 140267183036224 train.py:394] {'eval/walltime': 544.7440481185913, 'training/sps': 40255.4212272967, 'training/walltime': 416.71137714385986, 'training/entropy_loss': Array(-0.00967983, dtype=float32), 'training/policy_loss': Array(0.00370103, dtype=float32), 'training/total_loss': Array(65.11902, dtype=float32), 'training/v_loss': Array(65.125, dtype=float32), 'eval/episode_goal_distance': (Array(0.29555643, dtype=float32), Array(0.05139504, dtype=float32)), 'eval/episode_reward': (Array(-13629.404, dtype=float32), Array(5153.433, dtype=float32)), 'eval/avg_episode_length': (Array(899.08594, dtype=float32), Array(300.14444, dtype=float32)), 'eval/epoch_eval_time': 4.107713460922241, 'eval/sps': 31160.88822107425}
I0726 22:46:14.926784 140267183036224 train.py:379] starting iteration 130 967.1816303730011
I0726 22:46:22.094393 140267183036224 train.py:394] {'eval/walltime': 548.8543841838837, 'training/sps': 40240.18086201776, 'training/walltime': 419.76504135131836, 'training/entropy_loss': Array(-0.00940528, dtype=float32), 'training/policy_loss': Array(0.00418577, dtype=float32), 'training/total_loss': Array(63.405956, dtype=float32), 'training/v_loss': Array(63.41118, dtype=float32), 'eval/episode_goal_distance': (Array(0.29477483, dtype=float32), Array(0.06153946, dtype=float32)), 'eval/episode_reward': (Array(-13676.729, dtype=float32), Array(5213.594, dtype=float32)), 'eval/avg_episode_length': (Array(898.96875, dtype=float32), Array(300.49234, dtype=float32)), 'eval/epoch_eval_time': 4.110336065292358, 'eval/sps': 31141.00598265696}
I0726 22:46:22.097010 140267183036224 train.py:379] starting iteration 131 974.351856470108
I0726 22:46:29.259742 140267183036224 train.py:394] {'eval/walltime': 552.9666528701782, 'training/sps': 40327.35321130708, 'training/walltime': 422.81210470199585, 'training/entropy_loss': Array(-0.01205442, dtype=float32), 'training/policy_loss': Array(0.00441815, dtype=float32), 'training/total_loss': Array(61.36077, dtype=float32), 'training/v_loss': Array(61.36841, dtype=float32), 'eval/episode_goal_distance': (Array(0.28871357, dtype=float32), Array(0.05600227, dtype=float32)), 'eval/episode_reward': (Array(-14647.255, dtype=float32), Array(3540.8406, dtype=float32)), 'eval/avg_episode_length': (Array(968.9375, dtype=float32), Array(172.94899, dtype=float32)), 'eval/epoch_eval_time': 4.112268686294556, 'eval/sps': 31126.37081001072}
I0726 22:46:29.262072 140267183036224 train.py:379] starting iteration 132 981.5169184207916
I0726 22:46:36.424072 140267183036224 train.py:394] {'eval/walltime': 557.0756537914276, 'training/sps': 40293.22145585818, 'training/walltime': 425.8617491722107, 'training/entropy_loss': Array(-0.01249164, dtype=float32), 'training/policy_loss': Array(0.00420845, dtype=float32), 'training/total_loss': Array(64.19441, dtype=float32), 'training/v_loss': Array(64.20269, dtype=float32), 'eval/episode_goal_distance': (Array(0.2968583, dtype=float32), Array(0.05410439, dtype=float32)), 'eval/episode_reward': (Array(-14471.757, dtype=float32), Array(4431.997, dtype=float32)), 'eval/avg_episode_length': (Array(937.8594, dtype=float32), Array(240.66978, dtype=float32)), 'eval/epoch_eval_time': 4.10900092124939, 'eval/sps': 31151.1246780349}
I0726 22:46:36.426455 140267183036224 train.py:379] starting iteration 133 988.6813011169434
I0726 22:46:43.590167 140267183036224 train.py:394] {'eval/walltime': 561.185197353363, 'training/sps': 40279.47285850919, 'training/walltime': 428.9124345779419, 'training/entropy_loss': Array(-0.01335912, dtype=float32), 'training/policy_loss': Array(0.00285509, dtype=float32), 'training/total_loss': Array(476.8713, dtype=float32), 'training/v_loss': Array(476.8818, dtype=float32), 'eval/episode_goal_distance': (Array(0.28809988, dtype=float32), Array(0.06294489, dtype=float32)), 'eval/episode_reward': (Array(-13746.551, dtype=float32), Array(5074.7495, dtype=float32)), 'eval/avg_episode_length': (Array(906.7656, dtype=float32), Array(289.87808, dtype=float32)), 'eval/epoch_eval_time': 4.109543561935425, 'eval/sps': 31147.011358049043}
I0726 22:46:43.592700 140267183036224 train.py:379] starting iteration 134 995.8475456237793
I0726 22:46:50.770578 140267183036224 train.py:394] {'eval/walltime': 565.2988681793213, 'training/sps': 40176.343493047076, 'training/walltime': 431.9709508419037, 'training/entropy_loss': Array(-0.01279658, dtype=float32), 'training/policy_loss': Array(0.00443636, dtype=float32), 'training/total_loss': Array(166.58249, dtype=float32), 'training/v_loss': Array(166.59085, dtype=float32), 'eval/episode_goal_distance': (Array(0.2857403, dtype=float32), Array(0.06081204, dtype=float32)), 'eval/episode_reward': (Array(-13149.184, dtype=float32), Array(5201.2876, dtype=float32)), 'eval/avg_episode_length': (Array(891.33594, dtype=float32), Array(310.08096, dtype=float32)), 'eval/epoch_eval_time': 4.113670825958252, 'eval/sps': 31115.761424635442}
I0726 22:46:50.773306 140267183036224 train.py:379] starting iteration 135 1003.0281527042389
I0726 22:46:57.937782 140267183036224 train.py:394] {'eval/walltime': 569.4093861579895, 'training/sps': 40281.77098583703, 'training/walltime': 435.02146220207214, 'training/entropy_loss': Array(-0.01092718, dtype=float32), 'training/policy_loss': Array(0.00482312, dtype=float32), 'training/total_loss': Array(121.13922, dtype=float32), 'training/v_loss': Array(121.145325, dtype=float32), 'eval/episode_goal_distance': (Array(0.29435095, dtype=float32), Array(0.06213539, dtype=float32)), 'eval/episode_reward': (Array(-13725.551, dtype=float32), Array(5013.699, dtype=float32)), 'eval/avg_episode_length': (Array(906.8125, dtype=float32), Array(289.7322, dtype=float32)), 'eval/epoch_eval_time': 4.110517978668213, 'eval/sps': 31139.62781923444}
I0726 22:46:57.940248 140267183036224 train.py:379] starting iteration 136 1010.1950933933258
I0726 22:47:05.095000 140267183036224 train.py:394] {'eval/walltime': 573.5138611793518, 'training/sps': 40331.282102407335, 'training/walltime': 438.06822872161865, 'training/entropy_loss': Array(-0.00895291, dtype=float32), 'training/policy_loss': Array(0.00479668, dtype=float32), 'training/total_loss': Array(106.16292, dtype=float32), 'training/v_loss': Array(106.16707, dtype=float32), 'eval/episode_goal_distance': (Array(0.2928497, dtype=float32), Array(0.05630347, dtype=float32)), 'eval/episode_reward': (Array(-14176.207, dtype=float32), Array(4770.473, dtype=float32)), 'eval/avg_episode_length': (Array(922.3281, dtype=float32), Array(266.8122, dtype=float32)), 'eval/epoch_eval_time': 4.104475021362305, 'eval/sps': 31185.47422844734}
I0726 22:47:05.097439 140267183036224 train.py:379] starting iteration 137 1017.3522853851318
I0726 22:47:12.263325 140267183036224 train.py:394] {'eval/walltime': 577.6253316402435, 'training/sps': 40277.219059569004, 'training/walltime': 441.1190848350525, 'training/entropy_loss': Array(-0.00995512, dtype=float32), 'training/policy_loss': Array(0.00585351, dtype=float32), 'training/total_loss': Array(97.91425, dtype=float32), 'training/v_loss': Array(97.91835, dtype=float32), 'eval/episode_goal_distance': (Array(0.2826655, dtype=float32), Array(0.05760032, dtype=float32)), 'eval/episode_reward': (Array(-14027.174, dtype=float32), Array(4466.4023, dtype=float32)), 'eval/avg_episode_length': (Array(937.83594, dtype=float32), Array(240.76044, dtype=float32)), 'eval/epoch_eval_time': 4.111470460891724, 'eval/sps': 31132.413869328517}
I0726 22:47:12.265747 140267183036224 train.py:379] starting iteration 138 1024.520593881607
I0726 22:47:19.426552 140267183036224 train.py:394] {'eval/walltime': 581.7284502983093, 'training/sps': 40233.65953915013, 'training/walltime': 444.1732439994812, 'training/entropy_loss': Array(-0.01159301, dtype=float32), 'training/policy_loss': Array(0.00532978, dtype=float32), 'training/total_loss': Array(93.15695, dtype=float32), 'training/v_loss': Array(93.16322, dtype=float32), 'eval/episode_goal_distance': (Array(0.28268376, dtype=float32), Array(0.05744978, dtype=float32)), 'eval/episode_reward': (Array(-13803.293, dtype=float32), Array(4737.9624, dtype=float32)), 'eval/avg_episode_length': (Array(922.3672, dtype=float32), Array(266.67792, dtype=float32)), 'eval/epoch_eval_time': 4.103118658065796, 'eval/sps': 31195.783175410044}
I0726 22:47:19.429159 140267183036224 train.py:379] starting iteration 139 1031.684005498886
I0726 22:47:26.590958 140267183036224 train.py:394] {'eval/walltime': 585.8303081989288, 'training/sps': 40210.104899709586, 'training/walltime': 447.2291922569275, 'training/entropy_loss': Array(-0.01128116, dtype=float32), 'training/policy_loss': Array(0.00772552, dtype=float32), 'training/total_loss': Array(85.32439, dtype=float32), 'training/v_loss': Array(85.32794, dtype=float32), 'eval/episode_goal_distance': (Array(0.2841048, dtype=float32), Array(0.05662499, dtype=float32)), 'eval/episode_reward': (Array(-14153.461, dtype=float32), Array(3959.648, dtype=float32)), 'eval/avg_episode_length': (Array(953.39844, dtype=float32), Array(210.13866, dtype=float32)), 'eval/epoch_eval_time': 4.101857900619507, 'eval/sps': 31205.371590436633}
I0726 22:47:26.595981 140267183036224 train.py:379] starting iteration 140 1038.85081076622
I0726 22:47:33.749549 140267183036224 train.py:394] {'eval/walltime': 589.9267082214355, 'training/sps': 40249.50474692404, 'training/walltime': 450.2821490764618, 'training/entropy_loss': Array(-0.00887541, dtype=float32), 'training/policy_loss': Array(0.00947843, dtype=float32), 'training/total_loss': Array(62.80578, dtype=float32), 'training/v_loss': Array(62.805176, dtype=float32), 'eval/episode_goal_distance': (Array(0.28513643, dtype=float32), Array(0.06117152, dtype=float32)), 'eval/episode_reward': (Array(-14053.299, dtype=float32), Array(4289.8896, dtype=float32)), 'eval/avg_episode_length': (Array(945.65625, dtype=float32), Array(225.9402, dtype=float32)), 'eval/epoch_eval_time': 4.096400022506714, 'eval/sps': 31246.948368502555}
I0726 22:47:33.751893 140267183036224 train.py:379] starting iteration 141 1046.0067391395569
I0726 22:47:40.907769 140267183036224 train.py:394] {'eval/walltime': 594.0270638465881, 'training/sps': 40262.311308624136, 'training/walltime': 453.3341348171234, 'training/entropy_loss': Array(-0.01127047, dtype=float32), 'training/policy_loss': Array(0.00878447, dtype=float32), 'training/total_loss': Array(84.35349, dtype=float32), 'training/v_loss': Array(84.35597, dtype=float32), 'eval/episode_goal_distance': (Array(0.28780353, dtype=float32), Array(0.05729776, dtype=float32)), 'eval/episode_reward': (Array(-13972.053, dtype=float32), Array(4845.0713, dtype=float32)), 'eval/avg_episode_length': (Array(914.5625, dtype=float32), Array(278.64148, dtype=float32)), 'eval/epoch_eval_time': 4.100355625152588, 'eval/sps': 31216.80451686107}
I0726 22:47:40.910663 140267183036224 train.py:379] starting iteration 142 1053.1655094623566
I0726 22:47:48.059701 140267183036224 train.py:394] {'eval/walltime': 598.1166875362396, 'training/sps': 40215.5422409591, 'training/walltime': 456.3896698951721, 'training/entropy_loss': Array(-0.01160569, dtype=float32), 'training/policy_loss': Array(0.00987089, dtype=float32), 'training/total_loss': Array(60.39687, dtype=float32), 'training/v_loss': Array(60.398605, dtype=float32), 'eval/episode_goal_distance': (Array(0.2954523, dtype=float32), Array(0.05931707, dtype=float32)), 'eval/episode_reward': (Array(-15119.402, dtype=float32), Array(4209.5234, dtype=float32)), 'eval/avg_episode_length': (Array(953.3906, dtype=float32), Array(210.17342, dtype=float32)), 'eval/epoch_eval_time': 4.089623689651489, 'eval/sps': 31298.723235562033}
I0726 22:47:48.062441 140267183036224 train.py:379] starting iteration 143 1060.3172872066498
I0726 22:47:55.224128 140267183036224 train.py:394] {'eval/walltime': 602.220990896225, 'training/sps': 40237.56391158005, 'training/walltime': 459.443532705307, 'training/entropy_loss': Array(-0.01185177, dtype=float32), 'training/policy_loss': Array(0.00495309, dtype=float32), 'training/total_loss': Array(60.558643, dtype=float32), 'training/v_loss': Array(60.565544, dtype=float32), 'eval/episode_goal_distance': (Array(0.28798646, dtype=float32), Array(0.05410833, dtype=float32)), 'eval/episode_reward': (Array(-13622.41, dtype=float32), Array(4840.0854, dtype=float32)), 'eval/avg_episode_length': (Array(914.58594, dtype=float32), Array(278.56488, dtype=float32)), 'eval/epoch_eval_time': 4.104303359985352, 'eval/sps': 31186.778552464708}
I0726 22:47:55.226540 140267183036224 train.py:379] starting iteration 144 1067.481386423111
I0726 22:48:02.373211 140267183036224 train.py:394] {'eval/walltime': 606.3096077442169, 'training/sps': 40247.719455797334, 'training/walltime': 462.49662494659424, 'training/entropy_loss': Array(-0.01006545, dtype=float32), 'training/policy_loss': Array(0.00849841, dtype=float32), 'training/total_loss': Array(60.99891, dtype=float32), 'training/v_loss': Array(61.000473, dtype=float32), 'eval/episode_goal_distance': (Array(0.29216367, dtype=float32), Array(0.05625225, dtype=float32)), 'eval/episode_reward': (Array(-13940.627, dtype=float32), Array(4868.848, dtype=float32)), 'eval/avg_episode_length': (Array(914.625, dtype=float32), Array(278.43784, dtype=float32)), 'eval/epoch_eval_time': 4.088616847991943, 'eval/sps': 31306.430697428907}
I0726 22:48:02.375642 140267183036224 train.py:379] starting iteration 145 1074.6304879188538
I0726 22:48:09.529711 140267183036224 train.py:394] {'eval/walltime': 610.4028635025024, 'training/sps': 40192.77985254893, 'training/walltime': 465.55389046669006, 'training/entropy_loss': Array(-0.00979872, dtype=float32), 'training/policy_loss': Array(0.00629381, dtype=float32), 'training/total_loss': Array(61.75057, dtype=float32), 'training/v_loss': Array(61.75407, dtype=float32), 'eval/episode_goal_distance': (Array(0.28871718, dtype=float32), Array(0.0586606, dtype=float32)), 'eval/episode_reward': (Array(-13602.328, dtype=float32), Array(4944.556, dtype=float32)), 'eval/avg_episode_length': (Array(906.75, dtype=float32), Array(289.92642, dtype=float32)), 'eval/epoch_eval_time': 4.0932557582855225, 'eval/sps': 31270.95093945787}
I0726 22:48:09.532143 140267183036224 train.py:379] starting iteration 146 1081.7869894504547
I0726 22:48:16.685979 140267183036224 train.py:394] {'eval/walltime': 614.4933140277863, 'training/sps': 40159.90805383713, 'training/walltime': 468.61365842819214, 'training/entropy_loss': Array(-0.00665775, dtype=float32), 'training/policy_loss': Array(0.00698976, dtype=float32), 'training/total_loss': Array(71.11044, dtype=float32), 'training/v_loss': Array(71.11011, dtype=float32), 'eval/episode_goal_distance': (Array(0.27883762, dtype=float32), Array(0.05874358, dtype=float32)), 'eval/episode_reward': (Array(-13593.284, dtype=float32), Array(4851.428, dtype=float32)), 'eval/avg_episode_length': (Array(914.6094, dtype=float32), Array(278.48895, dtype=float32)), 'eval/epoch_eval_time': 4.0904505252838135, 'eval/sps': 31292.396573142465}
I0726 22:48:16.688398 140267183036224 train.py:379] starting iteration 147 1088.9432439804077
I0726 22:48:23.841985 140267183036224 train.py:394] {'eval/walltime': 618.5868334770203, 'training/sps': 40204.07001209095, 'training/walltime': 471.6700654029846, 'training/entropy_loss': Array(-0.00635799, dtype=float32), 'training/policy_loss': Array(0.00727137, dtype=float32), 'training/total_loss': Array(57.29984, dtype=float32), 'training/v_loss': Array(57.298927, dtype=float32), 'eval/episode_goal_distance': (Array(0.28291523, dtype=float32), Array(0.05721411, dtype=float32)), 'eval/episode_reward': (Array(-13646.469, dtype=float32), Array(4736.543, dtype=float32)), 'eval/avg_episode_length': (Array(922.33594, dtype=float32), Array(266.78543, dtype=float32)), 'eval/epoch_eval_time': 4.093519449234009, 'eval/sps': 31268.93656849529}
I0726 22:48:23.844588 140267183036224 train.py:379] starting iteration 148 1096.0994341373444
I0726 22:48:30.999490 140267183036224 train.py:394] {'eval/walltime': 622.6773200035095, 'training/sps': 40152.690104257556, 'training/walltime': 474.7303833961487, 'training/entropy_loss': Array(-0.00397174, dtype=float32), 'training/policy_loss': Array(0.00580713, dtype=float32), 'training/total_loss': Array(61.22769, dtype=float32), 'training/v_loss': Array(61.225857, dtype=float32), 'eval/episode_goal_distance': (Array(0.2871772, dtype=float32), Array(0.05840031, dtype=float32)), 'eval/episode_reward': (Array(-14320.566, dtype=float32), Array(3947.5217, dtype=float32)), 'eval/avg_episode_length': (Array(961.2031, dtype=float32), Array(192.42654, dtype=float32)), 'eval/epoch_eval_time': 4.090486526489258, 'eval/sps': 31292.121162383726}
I0726 22:48:31.004552 140267183036224 train.py:379] starting iteration 149 1103.2593839168549
I0726 22:48:38.158259 140267183036224 train.py:394] {'eval/walltime': 626.7729794979095, 'training/sps': 40242.87356606343, 'training/walltime': 477.7838432788849, 'training/entropy_loss': Array(-0.00094494, dtype=float32), 'training/policy_loss': Array(0.00601437, dtype=float32), 'training/total_loss': Array(49.758217, dtype=float32), 'training/v_loss': Array(49.753147, dtype=float32), 'eval/episode_goal_distance': (Array(0.2902676, dtype=float32), Array(0.06077959, dtype=float32)), 'eval/episode_reward': (Array(-13658.635, dtype=float32), Array(5388.8945, dtype=float32)), 'eval/avg_episode_length': (Array(891.2422, dtype=float32), Array(310.34857, dtype=float32)), 'eval/epoch_eval_time': 4.095659494400024, 'eval/sps': 31252.598067542916}
I0726 22:48:38.160531 140267183036224 train.py:379] starting iteration 150 1110.415377855301
I0726 22:48:45.323325 140267183036224 train.py:394] {'eval/walltime': 630.8706705570221, 'training/sps': 40142.48553040863, 'training/walltime': 480.84493923187256, 'training/entropy_loss': Array(-0.00192007, dtype=float32), 'training/policy_loss': Array(0.0074489, dtype=float32), 'training/total_loss': Array(477.9804, dtype=float32), 'training/v_loss': Array(477.97485, dtype=float32), 'eval/episode_goal_distance': (Array(0.2769798, dtype=float32), Array(0.06276247, dtype=float32)), 'eval/episode_reward': (Array(-13833.936, dtype=float32), Array(4245.0493, dtype=float32)), 'eval/avg_episode_length': (Array(945.64844, dtype=float32), Array(225.97295, dtype=float32)), 'eval/epoch_eval_time': 4.097691059112549, 'eval/sps': 31237.103567227296}
I0726 22:48:45.325819 140267183036224 train.py:379] starting iteration 151 1117.5806651115417
I0726 22:48:52.471734 140267183036224 train.py:394] {'eval/walltime': 634.9610359668732, 'training/sps': 40263.16054623947, 'training/walltime': 483.8968605995178, 'training/entropy_loss': Array(-0.00176768, dtype=float32), 'training/policy_loss': Array(0.01145333, dtype=float32), 'training/total_loss': Array(127.115074, dtype=float32), 'training/v_loss': Array(127.105385, dtype=float32), 'eval/episode_goal_distance': (Array(0.28416303, dtype=float32), Array(0.06035847, dtype=float32)), 'eval/episode_reward': (Array(-13541.864, dtype=float32), Array(4827.1, dtype=float32)), 'eval/avg_episode_length': (Array(914.6172, dtype=float32), Array(278.4633, dtype=float32)), 'eval/epoch_eval_time': 4.090365409851074, 'eval/sps': 31293.047729117272}
I0726 22:48:52.474172 140267183036224 train.py:379] starting iteration 152 1124.7290184497833
I0726 22:48:59.608797 140267183036224 train.py:394] {'eval/walltime': 639.0488662719727, 'training/sps': 40378.45061675537, 'training/walltime': 486.9400680065155, 'training/entropy_loss': Array(-0.00163214, dtype=float32), 'training/policy_loss': Array(0.01221666, dtype=float32), 'training/total_loss': Array(114.900085, dtype=float32), 'training/v_loss': Array(114.889496, dtype=float32), 'eval/episode_goal_distance': (Array(0.28683287, dtype=float32), Array(0.06370148, dtype=float32)), 'eval/episode_reward': (Array(-14082.855, dtype=float32), Array(4426.7144, dtype=float32)), 'eval/avg_episode_length': (Array(937.8125, dtype=float32), Array(240.85129, dtype=float32)), 'eval/epoch_eval_time': 4.087830305099487, 'eval/sps': 31312.45439428406}
I0726 22:48:59.611263 140267183036224 train.py:379] starting iteration 153 1131.8661093711853
I0726 22:49:06.774566 140267183036224 train.py:394] {'eval/walltime': 643.1635119915009, 'training/sps': 40354.505008765074, 'training/walltime': 489.9850811958313, 'training/entropy_loss': Array(-0.00314556, dtype=float32), 'training/policy_loss': Array(0.04822025, dtype=float32), 'training/total_loss': Array(101.236626, dtype=float32), 'training/v_loss': Array(101.19154, dtype=float32), 'eval/episode_goal_distance': (Array(0.28878582, dtype=float32), Array(0.05932489, dtype=float32)), 'eval/episode_reward': (Array(-14567.689, dtype=float32), Array(4267.171, dtype=float32)), 'eval/avg_episode_length': (Array(945.625, dtype=float32), Array(226.07037, dtype=float32)), 'eval/epoch_eval_time': 4.114645719528198, 'eval/sps': 31108.38908742719}
I0726 22:49:06.853007 140267183036224 train.py:379] starting iteration 154 1139.1078515052795
I0726 22:49:14.006137 140267183036224 train.py:394] {'eval/walltime': 647.2530341148376, 'training/sps': 40157.611294270035, 'training/walltime': 493.04502415657043, 'training/entropy_loss': Array(-0.00047684, dtype=float32), 'training/policy_loss': Array(0.00882839, dtype=float32), 'training/total_loss': Array(80.38933, dtype=float32), 'training/v_loss': Array(80.380974, dtype=float32), 'eval/episode_goal_distance': (Array(0.28477818, dtype=float32), Array(0.0550174, dtype=float32)), 'eval/episode_reward': (Array(-13429.882, dtype=float32), Array(4920.792, dtype=float32)), 'eval/avg_episode_length': (Array(906.8203, dtype=float32), Array(289.70798, dtype=float32)), 'eval/epoch_eval_time': 4.089522123336792, 'eval/sps': 31299.500562564528}
I0726 22:49:14.008744 140267183036224 train.py:379] starting iteration 155 1146.263590812683
I0726 22:49:21.170812 140267183036224 train.py:394] {'eval/walltime': 651.3481924533844, 'training/sps': 40147.357309293344, 'training/walltime': 496.10574865341187, 'training/entropy_loss': Array(4.6672867e-05, dtype=float32), 'training/policy_loss': Array(0.01029876, dtype=float32), 'training/total_loss': Array(65.28717, dtype=float32), 'training/v_loss': Array(65.27683, dtype=float32), 'eval/episode_goal_distance': (Array(0.29472578, dtype=float32), Array(0.058147, dtype=float32)), 'eval/episode_reward': (Array(-13780.809, dtype=float32), Array(5256.459, dtype=float32)), 'eval/avg_episode_length': (Array(899.0781, dtype=float32), Array(300.1674, dtype=float32)), 'eval/epoch_eval_time': 4.095158338546753, 'eval/sps': 31256.422687046408}
I0726 22:49:21.175800 140267183036224 train.py:379] starting iteration 156 1153.430630683899
I0726 22:49:28.339607 140267183036224 train.py:394] {'eval/walltime': 655.443589925766, 'training/sps': 40116.97959965222, 'training/walltime': 499.16879081726074, 'training/entropy_loss': Array(-0.00242876, dtype=float32), 'training/policy_loss': Array(0.01262764, dtype=float32), 'training/total_loss': Array(60.910515, dtype=float32), 'training/v_loss': Array(60.90032, dtype=float32), 'eval/episode_goal_distance': (Array(0.28970373, dtype=float32), Array(0.05964838, dtype=float32)), 'eval/episode_reward': (Array(-14577.188, dtype=float32), Array(4136.3784, dtype=float32)), 'eval/avg_episode_length': (Array(953.41406, dtype=float32), Array(210.06808, dtype=float32)), 'eval/epoch_eval_time': 4.095397472381592, 'eval/sps': 31254.59759723012}
I0726 22:49:28.344425 140267183036224 train.py:379] starting iteration 157 1160.5992574691772
I0726 22:49:35.507297 140267183036224 train.py:394] {'eval/walltime': 659.54079413414, 'training/sps': 40139.86251408294, 'training/walltime': 502.2300868034363, 'training/entropy_loss': Array(-0.00347307, dtype=float32), 'training/policy_loss': Array(0.01701026, dtype=float32), 'training/total_loss': Array(59.418617, dtype=float32), 'training/v_loss': Array(59.405083, dtype=float32), 'eval/episode_goal_distance': (Array(0.29184818, dtype=float32), Array(0.06044758, dtype=float32)), 'eval/episode_reward': (Array(-14367.795, dtype=float32), Array(4621.118, dtype=float32)), 'eval/avg_episode_length': (Array(937.8125, dtype=float32), Array(240.85167, dtype=float32)), 'eval/epoch_eval_time': 4.097204208374023, 'eval/sps': 31240.815319477773}
I0726 22:49:35.509529 140267183036224 train.py:379] starting iteration 158 1167.7643761634827
I0726 22:49:42.672755 140267183036224 train.py:394] {'eval/walltime': 663.6354744434357, 'training/sps': 40094.31926072171, 'training/walltime': 505.294860124588, 'training/entropy_loss': Array(-0.00181256, dtype=float32), 'training/policy_loss': Array(0.00727228, dtype=float32), 'training/total_loss': Array(62.63691, dtype=float32), 'training/v_loss': Array(62.631447, dtype=float32), 'eval/episode_goal_distance': (Array(0.29229617, dtype=float32), Array(0.05481016, dtype=float32)), 'eval/episode_reward': (Array(-13210.982, dtype=float32), Array(5588.334, dtype=float32)), 'eval/avg_episode_length': (Array(868.0547, dtype=float32), Array(337.15656, dtype=float32)), 'eval/epoch_eval_time': 4.094680309295654, 'eval/sps': 31260.071686040345}
I0726 22:49:42.674971 140267183036224 train.py:379] starting iteration 159 1174.9298176765442
I0726 22:49:49.834955 140267183036224 train.py:394] {'eval/walltime': 667.7331824302673, 'training/sps': 40177.568080214805, 'training/walltime': 508.3532831668854, 'training/entropy_loss': Array(-0.00275993, dtype=float32), 'training/policy_loss': Array(0.01377859, dtype=float32), 'training/total_loss': Array(57.680996, dtype=float32), 'training/v_loss': Array(57.669975, dtype=float32), 'eval/episode_goal_distance': (Array(0.29461253, dtype=float32), Array(0.0585059, dtype=float32)), 'eval/episode_reward': (Array(-13538.387, dtype=float32), Array(5305.4272, dtype=float32)), 'eval/avg_episode_length': (Array(891.35156, dtype=float32), Array(310.03616, dtype=float32)), 'eval/epoch_eval_time': 4.097707986831665, 'eval/sps': 31236.974526086033}
I0726 22:49:49.837193 140267183036224 train.py:379] starting iteration 160 1182.0920395851135
I0726 22:49:56.988859 140267183036224 train.py:394] {'eval/walltime': 671.8250885009766, 'training/sps': 40214.70128652912, 'training/walltime': 511.4088821411133, 'training/entropy_loss': Array(-0.00506374, dtype=float32), 'training/policy_loss': Array(0.00704342, dtype=float32), 'training/total_loss': Array(73.44359, dtype=float32), 'training/v_loss': Array(73.441605, dtype=float32), 'eval/episode_goal_distance': (Array(0.29131103, dtype=float32), Array(0.05792485, dtype=float32)), 'eval/episode_reward': (Array(-13110.435, dtype=float32), Array(5502.064, dtype=float32)), 'eval/avg_episode_length': (Array(867.89844, dtype=float32), Array(337.5559, dtype=float32)), 'eval/epoch_eval_time': 4.0919060707092285, 'eval/sps': 31281.265451387655}
I0726 22:49:56.991230 140267183036224 train.py:379] starting iteration 161 1189.2460761070251
I0726 22:50:04.138511 140267183036224 train.py:394] {'eval/walltime': 675.9220685958862, 'training/sps': 40338.630709859644, 'training/walltime': 514.4550936222076, 'training/entropy_loss': Array(-0.00576248, dtype=float32), 'training/policy_loss': Array(0.01028945, dtype=float32), 'training/total_loss': Array(79.588554, dtype=float32), 'training/v_loss': Array(79.58402, dtype=float32), 'eval/episode_goal_distance': (Array(0.29878658, dtype=float32), Array(0.06715529, dtype=float32)), 'eval/episode_reward': (Array(-13252.289, dtype=float32), Array(6298.0186, dtype=float32)), 'eval/avg_episode_length': (Array(844.6172, dtype=float32), Array(361.0775, dtype=float32)), 'eval/epoch_eval_time': 4.096980094909668, 'eval/sps': 31242.52425805896}
I0726 22:50:04.143293 140267183036224 train.py:379] starting iteration 162 1196.3981244564056
I0726 22:50:11.285933 140267183036224 train.py:394] {'eval/walltime': 680.0132966041565, 'training/sps': 40327.09762322608, 'training/walltime': 517.50217628479, 'training/entropy_loss': Array(-0.00459501, dtype=float32), 'training/policy_loss': Array(0.00835014, dtype=float32), 'training/total_loss': Array(70.00586, dtype=float32), 'training/v_loss': Array(70.002106, dtype=float32), 'eval/episode_goal_distance': (Array(0.29934502, dtype=float32), Array(0.06357293, dtype=float32)), 'eval/episode_reward': (Array(-14095.427, dtype=float32), Array(5640.4062, dtype=float32)), 'eval/avg_episode_length': (Array(891.25, dtype=float32), Array(310.32645, dtype=float32)), 'eval/epoch_eval_time': 4.091228008270264, 'eval/sps': 31286.449873058362}
I0726 22:50:11.288175 140267183036224 train.py:379] starting iteration 163 1203.543021440506
I0726 22:50:18.451022 140267183036224 train.py:394] {'eval/walltime': 684.1226718425751, 'training/sps': 40294.620144086024, 'training/walltime': 520.5517148971558, 'training/entropy_loss': Array(-0.00630566, dtype=float32), 'training/policy_loss': Array(0.00874663, dtype=float32), 'training/total_loss': Array(81.10252, dtype=float32), 'training/v_loss': Array(81.10008, dtype=float32), 'eval/episode_goal_distance': (Array(0.29522395, dtype=float32), Array(0.06393775, dtype=float32)), 'eval/episode_reward': (Array(-14340.451, dtype=float32), Array(4889.0273, dtype=float32)), 'eval/avg_episode_length': (Array(930.1172, dtype=float32), Array(254.11082, dtype=float32)), 'eval/epoch_eval_time': 4.109375238418579, 'eval/sps': 31148.287166216185}
I0726 22:50:18.453381 140267183036224 train.py:379] starting iteration 164 1210.7082269191742
I0726 22:50:25.623023 140267183036224 train.py:394] {'eval/walltime': 688.2360486984253, 'training/sps': 40286.58531069878, 'training/walltime': 523.6018617153168, 'training/entropy_loss': Array(-0.00628246, dtype=float32), 'training/policy_loss': Array(0.0096864, dtype=float32), 'training/total_loss': Array(88.0373, dtype=float32), 'training/v_loss': Array(88.033905, dtype=float32), 'eval/episode_goal_distance': (Array(0.29717082, dtype=float32), Array(0.05961995, dtype=float32)), 'eval/episode_reward': (Array(-14744.918, dtype=float32), Array(4662.39, dtype=float32)), 'eval/avg_episode_length': (Array(937.8594, dtype=float32), Array(240.66988, dtype=float32)), 'eval/epoch_eval_time': 4.11337685585022, 'eval/sps': 31117.9851702508}
I0726 22:50:25.625495 140267183036224 train.py:379] starting iteration 165 1217.8803415298462
I0726 22:50:32.782180 140267183036224 train.py:394] {'eval/walltime': 692.3421680927277, 'training/sps': 40331.13376891916, 'training/walltime': 526.6486394405365, 'training/entropy_loss': Array(-0.00958068, dtype=float32), 'training/policy_loss': Array(0.0081315, dtype=float32), 'training/total_loss': Array(86.60937, dtype=float32), 'training/v_loss': Array(86.61081, dtype=float32), 'eval/episode_goal_distance': (Array(0.29625642, dtype=float32), Array(0.06007583, dtype=float32)), 'eval/episode_reward': (Array(-14695.594, dtype=float32), Array(5181.2163, dtype=float32)), 'eval/avg_episode_length': (Array(914.65625, dtype=float32), Array(278.33582, dtype=float32)), 'eval/epoch_eval_time': 4.106119394302368, 'eval/sps': 31172.985417231703}
I0726 22:50:32.784399 140267183036224 train.py:379] starting iteration 166 1225.0392456054688
I0726 22:50:39.940643 140267183036224 train.py:394] {'eval/walltime': 696.4457852840424, 'training/sps': 40307.48380688861, 'training/walltime': 529.6972048282623, 'training/entropy_loss': Array(-0.01128325, dtype=float32), 'training/policy_loss': Array(0.00538137, dtype=float32), 'training/total_loss': Array(515.1279, dtype=float32), 'training/v_loss': Array(515.13385, dtype=float32), 'eval/episode_goal_distance': (Array(0.29626256, dtype=float32), Array(0.06228516, dtype=float32)), 'eval/episode_reward': (Array(-14291.939, dtype=float32), Array(5732.079, dtype=float32)), 'eval/avg_episode_length': (Array(891.3125, dtype=float32), Array(310.1477, dtype=float32)), 'eval/epoch_eval_time': 4.103617191314697, 'eval/sps': 31191.993315290692}
I0726 22:50:39.942857 140267183036224 train.py:379] starting iteration 167 1232.1977038383484
I0726 22:50:47.107313 140267183036224 train.py:394] {'eval/walltime': 700.5545330047607, 'training/sps': 40267.221655398556, 'training/walltime': 532.748818397522, 'training/entropy_loss': Array(-0.01424386, dtype=float32), 'training/policy_loss': Array(0.00452349, dtype=float32), 'training/total_loss': Array(309.19104, dtype=float32), 'training/v_loss': Array(309.20074, dtype=float32), 'eval/episode_goal_distance': (Array(0.3024027, dtype=float32), Array(0.06477127, dtype=float32)), 'eval/episode_reward': (Array(-13211.973, dtype=float32), Array(6382.556, dtype=float32)), 'eval/avg_episode_length': (Array(836.96094, dtype=float32), Array(368.02304, dtype=float32)), 'eval/epoch_eval_time': 4.108747720718384, 'eval/sps': 31153.044358153038}
I0726 22:50:47.109714 140267183036224 train.py:379] starting iteration 168 1239.3645603656769
I0726 22:50:54.268188 140267183036224 train.py:394] {'eval/walltime': 704.6578335762024, 'training/sps': 40271.16401115099, 'training/walltime': 535.800133228302, 'training/entropy_loss': Array(-0.0200145, dtype=float32), 'training/policy_loss': Array(0.00655875, dtype=float32), 'training/total_loss': Array(209.359, dtype=float32), 'training/v_loss': Array(209.37244, dtype=float32), 'eval/episode_goal_distance': (Array(0.2960887, dtype=float32), Array(0.06044065, dtype=float32)), 'eval/episode_reward': (Array(-14660.142, dtype=float32), Array(5464.1665, dtype=float32)), 'eval/avg_episode_length': (Array(906.9453, dtype=float32), Array(289.3192, dtype=float32)), 'eval/epoch_eval_time': 4.10330057144165, 'eval/sps': 31194.400159437646}
I0726 22:50:54.270383 140267183036224 train.py:379] starting iteration 169 1246.5252294540405
I0726 22:51:01.428137 140267183036224 train.py:394] {'eval/walltime': 708.7563371658325, 'training/sps': 40218.51410421122, 'training/walltime': 538.8554425239563, 'training/entropy_loss': Array(-0.0263151, dtype=float32), 'training/policy_loss': Array(0.00470857, dtype=float32), 'training/total_loss': Array(230.08266, dtype=float32), 'training/v_loss': Array(230.10426, dtype=float32), 'eval/episode_goal_distance': (Array(0.30800903, dtype=float32), Array(0.05871414, dtype=float32)), 'eval/episode_reward': (Array(-14768.363, dtype=float32), Array(5618.1157, dtype=float32)), 'eval/avg_episode_length': (Array(899.0703, dtype=float32), Array(300.19073, dtype=float32)), 'eval/epoch_eval_time': 4.098503589630127, 'eval/sps': 31230.910794822914}
I0726 22:51:01.430602 140267183036224 train.py:379] starting iteration 170 1253.685448884964
I0726 22:51:08.593516 140267183036224 train.py:394] {'eval/walltime': 712.8562531471252, 'training/sps': 40168.93179288386, 'training/walltime': 541.9145231246948, 'training/entropy_loss': Array(-0.0274179, dtype=float32), 'training/policy_loss': Array(0.00324108, dtype=float32), 'training/total_loss': Array(184.89471, dtype=float32), 'training/v_loss': Array(184.91888, dtype=float32), 'eval/episode_goal_distance': (Array(0.29663485, dtype=float32), Array(0.06251706, dtype=float32)), 'eval/episode_reward': (Array(-14376.266, dtype=float32), Array(5189.8145, dtype=float32)), 'eval/avg_episode_length': (Array(914.53906, dtype=float32), Array(278.71783, dtype=float32)), 'eval/epoch_eval_time': 4.099915981292725, 'eval/sps': 31220.151969953527}
I0726 22:51:08.595926 140267183036224 train.py:379] starting iteration 171 1260.8507721424103
I0726 22:51:15.751316 140267183036224 train.py:394] {'eval/walltime': 716.9539701938629, 'training/sps': 40237.2591987806, 'training/walltime': 544.9684090614319, 'training/entropy_loss': Array(-0.0261218, dtype=float32), 'training/policy_loss': Array(0.00380373, dtype=float32), 'training/total_loss': Array(157.94012, dtype=float32), 'training/v_loss': Array(157.96246, dtype=float32), 'eval/episode_goal_distance': (Array(0.29799885, dtype=float32), Array(0.06003737, dtype=float32)), 'eval/episode_reward': (Array(-15071.496, dtype=float32), Array(4198.637, dtype=float32)), 'eval/avg_episode_length': (Array(953.3906, dtype=float32), Array(210.1736, dtype=float32)), 'eval/epoch_eval_time': 4.097717046737671, 'eval/sps': 31236.90546225125}
I0726 22:51:15.753515 140267183036224 train.py:379] starting iteration 172 1268.0083611011505
I0726 22:51:22.914146 140267183036224 train.py:394] {'eval/walltime': 721.0533804893494, 'training/sps': 40191.37255440186, 'training/walltime': 548.0257816314697, 'training/entropy_loss': Array(-0.02676631, dtype=float32), 'training/policy_loss': Array(0.0023129, dtype=float32), 'training/total_loss': Array(145.23886, dtype=float32), 'training/v_loss': Array(145.26332, dtype=float32), 'eval/episode_goal_distance': (Array(0.29325283, dtype=float32), Array(0.06380189, dtype=float32)), 'eval/episode_reward': (Array(-15061.373, dtype=float32), Array(4438.6494, dtype=float32)), 'eval/avg_episode_length': (Array(953.3672, dtype=float32), Array(210.27924, dtype=float32)), 'eval/epoch_eval_time': 4.09941029548645, 'eval/sps': 31224.003155022343}
I0726 22:51:22.916635 140267183036224 train.py:379] starting iteration 173 1275.171481847763
I0726 22:51:30.070600 140267183036224 train.py:394] {'eval/walltime': 725.1502385139465, 'training/sps': 40246.21716802377, 'training/walltime': 551.0789878368378, 'training/entropy_loss': Array(-0.02442744, dtype=float32), 'training/policy_loss': Array(0.00124073, dtype=float32), 'training/total_loss': Array(105.29123, dtype=float32), 'training/v_loss': Array(105.31441, dtype=float32), 'eval/episode_goal_distance': (Array(0.29365957, dtype=float32), Array(0.0594753, dtype=float32)), 'eval/episode_reward': (Array(-14558.169, dtype=float32), Array(4711.1147, dtype=float32)), 'eval/avg_episode_length': (Array(930.08594, dtype=float32), Array(254.2248, dtype=float32)), 'eval/epoch_eval_time': 4.096858024597168, 'eval/sps': 31243.455162834416}
I0726 22:51:30.073060 140267183036224 train.py:379] starting iteration 174 1282.3279058933258
I0726 22:51:37.230113 140267183036224 train.py:394] {'eval/walltime': 729.2432956695557, 'training/sps': 40155.07077092947, 'training/walltime': 554.1391243934631, 'training/entropy_loss': Array(-0.02307528, dtype=float32), 'training/policy_loss': Array(0.00038542, dtype=float32), 'training/total_loss': Array(102.785, dtype=float32), 'training/v_loss': Array(102.80769, dtype=float32), 'eval/episode_goal_distance': (Array(0.2972545, dtype=float32), Array(0.05841095, dtype=float32)), 'eval/episode_reward': (Array(-14995.117, dtype=float32), Array(4659.405, dtype=float32)), 'eval/avg_episode_length': (Array(937.8203, dtype=float32), Array(240.821, dtype=float32)), 'eval/epoch_eval_time': 4.093057155609131, 'eval/sps': 31272.468263627503}
I0726 22:51:37.232662 140267183036224 train.py:379] starting iteration 175 1289.4875078201294
I0726 22:51:44.384191 140267183036224 train.py:394] {'eval/walltime': 733.3336524963379, 'training/sps': 40192.87388489076, 'training/walltime': 557.1963827610016, 'training/entropy_loss': Array(-0.02355435, dtype=float32), 'training/policy_loss': Array(0.00019144, dtype=float32), 'training/total_loss': Array(97.02347, dtype=float32), 'training/v_loss': Array(97.04683, dtype=float32), 'eval/episode_goal_distance': (Array(0.28649345, dtype=float32), Array(0.05678561, dtype=float32)), 'eval/episode_reward': (Array(-14946.262, dtype=float32), Array(3972.3574, dtype=float32)), 'eval/avg_episode_length': (Array(961.14844, dtype=float32), Array(192.6975, dtype=float32)), 'eval/epoch_eval_time': 4.090356826782227, 'eval/sps': 31293.113393408796}
I0726 22:51:44.386824 140267183036224 train.py:379] starting iteration 176 1296.6416699886322
I0726 22:51:51.539483 140267183036224 train.py:394] {'eval/walltime': 737.4236409664154, 'training/sps': 40173.334019262766, 'training/walltime': 560.2551281452179, 'training/entropy_loss': Array(-0.02047572, dtype=float32), 'training/policy_loss': Array(0.00068381, dtype=float32), 'training/total_loss': Array(86.87538, dtype=float32), 'training/v_loss': Array(86.89518, dtype=float32), 'eval/episode_goal_distance': (Array(0.29773524, dtype=float32), Array(0.05449568, dtype=float32)), 'eval/episode_reward': (Array(-14481.669, dtype=float32), Array(4912.106, dtype=float32)), 'eval/avg_episode_length': (Array(922.33594, dtype=float32), Array(266.78522, dtype=float32)), 'eval/epoch_eval_time': 4.089988470077515, 'eval/sps': 31295.931745639886}
I0726 22:51:51.542105 140267183036224 train.py:379] starting iteration 177 1303.796951532364
I0726 22:51:58.703379 140267183036224 train.py:394] {'eval/walltime': 741.5216608047485, 'training/sps': 40163.901411132094, 'training/walltime': 563.314591884613, 'training/entropy_loss': Array(-0.0221898, dtype=float32), 'training/policy_loss': Array(0.00081524, dtype=float32), 'training/total_loss': Array(92.38986, dtype=float32), 'training/v_loss': Array(92.41124, dtype=float32), 'eval/episode_goal_distance': (Array(0.29638854, dtype=float32), Array(0.0577386, dtype=float32)), 'eval/episode_reward': (Array(-15135.738, dtype=float32), Array(4494.668, dtype=float32)), 'eval/avg_episode_length': (Array(945.65625, dtype=float32), Array(225.94034, dtype=float32)), 'eval/epoch_eval_time': 4.09801983833313, 'eval/sps': 31234.597451842503}
I0726 22:51:58.705845 140267183036224 train.py:379] starting iteration 178 1310.9606909751892
I0726 22:52:05.859763 140267183036224 train.py:394] {'eval/walltime': 745.6119236946106, 'training/sps': 40157.51116903893, 'training/walltime': 566.3745424747467, 'training/entropy_loss': Array(-0.02309301, dtype=float32), 'training/policy_loss': Array(0.00090785, dtype=float32), 'training/total_loss': Array(95.322075, dtype=float32), 'training/v_loss': Array(95.34425, dtype=float32), 'eval/episode_goal_distance': (Array(0.2943055, dtype=float32), Array(0.05363329, dtype=float32)), 'eval/episode_reward': (Array(-14869.33, dtype=float32), Array(4566.3657, dtype=float32)), 'eval/avg_episode_length': (Array(937.8828, dtype=float32), Array(240.57939, dtype=float32)), 'eval/epoch_eval_time': 4.0902628898620605, 'eval/sps': 31293.83207061213}
I0726 22:52:05.862224 140267183036224 train.py:379] starting iteration 179 1318.1170704364777
I0726 22:52:13.022711 140267183036224 train.py:394] {'eval/walltime': 749.7090215682983, 'training/sps': 40159.86737324884, 'training/walltime': 569.4343135356903, 'training/entropy_loss': Array(-0.02300518, dtype=float32), 'training/policy_loss': Array(0.00052739, dtype=float32), 'training/total_loss': Array(88.9689, dtype=float32), 'training/v_loss': Array(88.99138, dtype=float32), 'eval/episode_goal_distance': (Array(0.29301667, dtype=float32), Array(0.05860572, dtype=float32)), 'eval/episode_reward': (Array(-14232.962, dtype=float32), Array(4818.6562, dtype=float32)), 'eval/avg_episode_length': (Array(922.28906, dtype=float32), Array(266.94632, dtype=float32)), 'eval/epoch_eval_time': 4.097097873687744, 'eval/sps': 31241.626132984926}
I0726 22:52:13.025292 140267183036224 train.py:379] starting iteration 180 1325.2801389694214
I0726 22:52:20.185489 140267183036224 train.py:394] {'eval/walltime': 753.8024518489838, 'training/sps': 40115.54638078194, 'training/walltime': 572.497465133667, 'training/entropy_loss': Array(-0.02177749, dtype=float32), 'training/policy_loss': Array(0.00090564, dtype=float32), 'training/total_loss': Array(84.655464, dtype=float32), 'training/v_loss': Array(84.67634, dtype=float32), 'eval/episode_goal_distance': (Array(0.29302, dtype=float32), Array(0.05859961, dtype=float32)), 'eval/episode_reward': (Array(-15117.731, dtype=float32), Array(3787.931, dtype=float32)), 'eval/avg_episode_length': (Array(968.9297, dtype=float32), Array(172.99237, dtype=float32)), 'eval/epoch_eval_time': 4.093430280685425, 'eval/sps': 31269.617710104747}
I0726 22:52:20.187884 140267183036224 train.py:379] starting iteration 181 1332.4427309036255
I0726 22:52:27.353914 140267183036224 train.py:394] {'eval/walltime': 757.8987858295441, 'training/sps': 40078.14719041029, 'training/walltime': 575.5634751319885, 'training/entropy_loss': Array(-0.02053209, dtype=float32), 'training/policy_loss': Array(0.0005531, dtype=float32), 'training/total_loss': Array(104.928314, dtype=float32), 'training/v_loss': Array(104.94829, dtype=float32), 'eval/episode_goal_distance': (Array(0.29038042, dtype=float32), Array(0.06904268, dtype=float32)), 'eval/episode_reward': (Array(-14068.148, dtype=float32), Array(5270.6714, dtype=float32)), 'eval/avg_episode_length': (Array(906.8125, dtype=float32), Array(289.73236, dtype=float32)), 'eval/epoch_eval_time': 4.096333980560303, 'eval/sps': 31247.452138287797}
I0726 22:52:27.356373 140267183036224 train.py:379] starting iteration 182 1339.611219882965
I0726 22:52:34.513201 140267183036224 train.py:394] {'eval/walltime': 761.9882109165192, 'training/sps': 40107.64521972994, 'training/walltime': 578.6272301673889, 'training/entropy_loss': Array(-0.02340735, dtype=float32), 'training/policy_loss': Array(0.00089781, dtype=float32), 'training/total_loss': Array(82.22528, dtype=float32), 'training/v_loss': Array(82.24779, dtype=float32), 'eval/episode_goal_distance': (Array(0.29550225, dtype=float32), Array(0.06183619, dtype=float32)), 'eval/episode_reward': (Array(-13656.894, dtype=float32), Array(5517.4087, dtype=float32)), 'eval/avg_episode_length': (Array(883.52344, dtype=float32), Array(319.69305, dtype=float32)), 'eval/epoch_eval_time': 4.089425086975098, 'eval/sps': 31300.24325611994}
I0726 22:52:34.515712 140267183036224 train.py:379] starting iteration 183 1346.7705583572388
I0726 22:52:41.671847 140267183036224 train.py:394] {'eval/walltime': 766.0788450241089, 'training/sps': 40130.82683136056, 'training/walltime': 581.6892154216766, 'training/entropy_loss': Array(-0.02516151, dtype=float32), 'training/policy_loss': Array(0.00115044, dtype=float32), 'training/total_loss': Array(456.8464, dtype=float32), 'training/v_loss': Array(456.87045, dtype=float32), 'eval/episode_goal_distance': (Array(0.2935674, dtype=float32), Array(0.06030468, dtype=float32)), 'eval/episode_reward': (Array(-14668.711, dtype=float32), Array(4692.1274, dtype=float32)), 'eval/avg_episode_length': (Array(930.1094, dtype=float32), Array(254.13937, dtype=float32)), 'eval/epoch_eval_time': 4.090634107589722, 'eval/sps': 31290.992211332243}
I0726 22:52:41.674240 140267183036224 train.py:379] starting iteration 184 1353.9290862083435
I0726 22:52:48.827144 140267183036224 train.py:394] {'eval/walltime': 770.1693177223206, 'training/sps': 40175.15342870731, 'training/walltime': 584.7478222846985, 'training/entropy_loss': Array(-0.02441224, dtype=float32), 'training/policy_loss': Array(0.00046079, dtype=float32), 'training/total_loss': Array(184.50189, dtype=float32), 'training/v_loss': Array(184.52586, dtype=float32), 'eval/episode_goal_distance': (Array(0.30088496, dtype=float32), Array(0.06529076, dtype=float32)), 'eval/episode_reward': (Array(-14480.047, dtype=float32), Array(5478.779, dtype=float32)), 'eval/avg_episode_length': (Array(906.8281, dtype=float32), Array(289.68347, dtype=float32)), 'eval/epoch_eval_time': 4.09047269821167, 'eval/sps': 31292.22694872425}
I0726 22:52:48.831700 140267183036224 train.py:379] starting iteration 185 1361.086531162262
I0726 22:52:55.985196 140267183036224 train.py:394] {'eval/walltime': 774.2579479217529, 'training/sps': 40145.06511199032, 'training/walltime': 587.8087215423584, 'training/entropy_loss': Array(-0.02710371, dtype=float32), 'training/policy_loss': Array(0.0003739, dtype=float32), 'training/total_loss': Array(142.02005, dtype=float32), 'training/v_loss': Array(142.04678, dtype=float32), 'eval/episode_goal_distance': (Array(0.29658943, dtype=float32), Array(0.05920715, dtype=float32)), 'eval/episode_reward': (Array(-14192.773, dtype=float32), Array(5846.3257, dtype=float32)), 'eval/avg_episode_length': (Array(883.4844, dtype=float32), Array(319.80032, dtype=float32)), 'eval/epoch_eval_time': 4.088630199432373, 'eval/sps': 31306.32846613771}
I0726 22:52:55.987690 140267183036224 train.py:379] starting iteration 186 1368.2425360679626
I0726 22:53:03.131495 140267183036224 train.py:394] {'eval/walltime': 778.3417346477509, 'training/sps': 40203.913204441655, 'training/walltime': 590.8651404380798, 'training/entropy_loss': Array(-0.02919963, dtype=float32), 'training/policy_loss': Array(0.00038833, dtype=float32), 'training/total_loss': Array(174.2199, dtype=float32), 'training/v_loss': Array(174.24872, dtype=float32), 'eval/episode_goal_distance': (Array(0.28813142, dtype=float32), Array(0.05305971, dtype=float32)), 'eval/episode_reward': (Array(-13343.059, dtype=float32), Array(5701.054, dtype=float32)), 'eval/avg_episode_length': (Array(867.9375, dtype=float32), Array(337.45587, dtype=float32)), 'eval/epoch_eval_time': 4.083786725997925, 'eval/sps': 31343.458556524296}
I0726 22:53:03.134021 140267183036224 train.py:379] starting iteration 187 1375.3888671398163
I0726 22:53:10.271882 140267183036224 train.py:394] {'eval/walltime': 782.4294989109039, 'training/sps': 40335.61265871703, 'training/walltime': 593.9115798473358, 'training/entropy_loss': Array(-0.02774123, dtype=float32), 'training/policy_loss': Array(-0.00041054, dtype=float32), 'training/total_loss': Array(110.125305, dtype=float32), 'training/v_loss': Array(110.15346, dtype=float32), 'eval/episode_goal_distance': (Array(0.290447, dtype=float32), Array(0.06551795, dtype=float32)), 'eval/episode_reward': (Array(-13779.6875, dtype=float32), Array(5170.3276, dtype=float32)), 'eval/avg_episode_length': (Array(899.0469, dtype=float32), Array(300.26016, dtype=float32)), 'eval/epoch_eval_time': 4.087764263153076, 'eval/sps': 31312.96027850389}
I0726 22:53:10.274403 140267183036224 train.py:379] starting iteration 188 1382.5292496681213
I0726 22:53:17.425089 140267183036224 train.py:394] {'eval/walltime': 786.5268406867981, 'training/sps': 40292.40559923292, 'training/walltime': 596.9612860679626, 'training/entropy_loss': Array(-0.02924276, dtype=float32), 'training/policy_loss': Array(0.00044096, dtype=float32), 'training/total_loss': Array(118.26543, dtype=float32), 'training/v_loss': Array(118.294235, dtype=float32), 'eval/episode_goal_distance': (Array(0.29902884, dtype=float32), Array(0.05381963, dtype=float32)), 'eval/episode_reward': (Array(-14869.785, dtype=float32), Array(4417.6616, dtype=float32)), 'eval/avg_episode_length': (Array(945.6094, dtype=float32), Array(226.13522, dtype=float32)), 'eval/epoch_eval_time': 4.097341775894165, 'eval/sps': 31239.7664146693}
I0726 22:53:17.427641 140267183036224 train.py:379] starting iteration 189 1389.682487487793
I0726 22:53:24.595880 140267183036224 train.py:394] {'eval/walltime': 790.6345357894897, 'training/sps': 40198.59187799752, 'training/walltime': 600.0181095600128, 'training/entropy_loss': Array(-0.03031158, dtype=float32), 'training/policy_loss': Array(-4.3780718e-05, dtype=float32), 'training/total_loss': Array(139.66821, dtype=float32), 'training/v_loss': Array(139.69855, dtype=float32), 'eval/episode_goal_distance': (Array(0.28799796, dtype=float32), Array(0.059322, dtype=float32)), 'eval/episode_reward': (Array(-14564.254, dtype=float32), Array(4266.486, dtype=float32)), 'eval/avg_episode_length': (Array(945.6797, dtype=float32), Array(225.8429, dtype=float32)), 'eval/epoch_eval_time': 4.10769510269165, 'eval/sps': 31161.027486223455}
I0726 22:53:24.598350 140267183036224 train.py:379] starting iteration 190 1396.8531913757324
I0726 22:53:31.756993 140267183036224 train.py:394] {'eval/walltime': 794.7387914657593, 'training/sps': 40279.3406454622, 'training/walltime': 603.0688049793243, 'training/entropy_loss': Array(-0.02857979, dtype=float32), 'training/policy_loss': Array(-0.0002215, dtype=float32), 'training/total_loss': Array(91.3139, dtype=float32), 'training/v_loss': Array(91.3427, dtype=float32), 'eval/episode_goal_distance': (Array(0.28877556, dtype=float32), Array(0.06272484, dtype=float32)), 'eval/episode_reward': (Array(-14724.497, dtype=float32), Array(4283.262, dtype=float32)), 'eval/avg_episode_length': (Array(953.35156, dtype=float32), Array(210.34958, dtype=float32)), 'eval/epoch_eval_time': 4.104255676269531, 'eval/sps': 31187.140884054927}
I0726 22:53:31.759561 140267183036224 train.py:379] starting iteration 191 1404.0144078731537
I0726 22:53:38.926115 140267183036224 train.py:394] {'eval/walltime': 798.8475682735443, 'training/sps': 40234.19347938806, 'training/walltime': 606.1229236125946, 'training/entropy_loss': Array(-0.02840482, dtype=float32), 'training/policy_loss': Array(0.00022679, dtype=float32), 'training/total_loss': Array(82.688354, dtype=float32), 'training/v_loss': Array(82.71654, dtype=float32), 'eval/episode_goal_distance': (Array(0.2909336, dtype=float32), Array(0.05242285, dtype=float32)), 'eval/episode_reward': (Array(-14161.256, dtype=float32), Array(4954.3057, dtype=float32)), 'eval/avg_episode_length': (Array(914.60156, dtype=float32), Array(278.5142, dtype=float32)), 'eval/epoch_eval_time': 4.108776807785034, 'eval/sps': 31152.823817899818}
I0726 22:53:38.928901 140267183036224 train.py:379] starting iteration 192 1411.183747291565
I0726 22:53:46.092153 140267183036224 train.py:394] {'eval/walltime': 802.9510242938995, 'training/sps': 40206.99940383347, 'training/walltime': 609.1791079044342, 'training/entropy_loss': Array(-0.02599156, dtype=float32), 'training/policy_loss': Array(-1.1650765e-05, dtype=float32), 'training/total_loss': Array(73.74244, dtype=float32), 'training/v_loss': Array(73.76845, dtype=float32), 'eval/episode_goal_distance': (Array(0.2883256, dtype=float32), Array(0.05851808, dtype=float32)), 'eval/episode_reward': (Array(-13986.639, dtype=float32), Array(4706.329, dtype=float32)), 'eval/avg_episode_length': (Array(922.3281, dtype=float32), Array(266.8119, dtype=float32)), 'eval/epoch_eval_time': 4.103456020355225, 'eval/sps': 31193.21843954341}
I0726 22:53:46.094575 140267183036224 train.py:379] starting iteration 193 1418.3494217395782
I0726 22:53:53.261761 140267183036224 train.py:394] {'eval/walltime': 807.0603976249695, 'training/sps': 40234.268860327946, 'training/walltime': 612.2332208156586, 'training/entropy_loss': Array(-0.02375943, dtype=float32), 'training/policy_loss': Array(-1.2051314e-05, dtype=float32), 'training/total_loss': Array(66.92818, dtype=float32), 'training/v_loss': Array(66.95195, dtype=float32), 'eval/episode_goal_distance': (Array(0.2930956, dtype=float32), Array(0.05860581, dtype=float32)), 'eval/episode_reward': (Array(-14248.965, dtype=float32), Array(4926.7095, dtype=float32)), 'eval/avg_episode_length': (Array(914.5, dtype=float32), Array(278.8453, dtype=float32)), 'eval/epoch_eval_time': 4.109373331069946, 'eval/sps': 31148.301623564825}
I0726 22:53:53.264483 140267183036224 train.py:379] starting iteration 194 1425.5193300247192
I0726 22:54:00.426649 140267183036224 train.py:394] {'eval/walltime': 811.1654291152954, 'training/sps': 40243.49887795211, 'training/walltime': 615.2866332530975, 'training/entropy_loss': Array(-0.02344937, dtype=float32), 'training/policy_loss': Array(6.0705337e-05, dtype=float32), 'training/total_loss': Array(68.02657, dtype=float32), 'training/v_loss': Array(68.049965, dtype=float32), 'eval/episode_goal_distance': (Array(0.28779402, dtype=float32), Array(0.06110018, dtype=float32)), 'eval/episode_reward': (Array(-13917.694, dtype=float32), Array(4991.9653, dtype=float32)), 'eval/avg_episode_length': (Array(914.6719, dtype=float32), Array(278.28488, dtype=float32)), 'eval/epoch_eval_time': 4.105031490325928, 'eval/sps': 31181.246794732182}
I0726 22:54:00.429095 140267183036224 train.py:379] starting iteration 195 1432.6839418411255
I0726 22:54:07.598166 140267183036224 train.py:394] {'eval/walltime': 815.2753360271454, 'training/sps': 40216.6719365942, 'training/walltime': 618.3420825004578, 'training/entropy_loss': Array(-0.02409402, dtype=float32), 'training/policy_loss': Array(0.00011049, dtype=float32), 'training/total_loss': Array(69.10876, dtype=float32), 'training/v_loss': Array(69.132744, dtype=float32), 'eval/episode_goal_distance': (Array(0.29394573, dtype=float32), Array(0.05626051, dtype=float32)), 'eval/episode_reward': (Array(-14659.268, dtype=float32), Array(4665.2783, dtype=float32)), 'eval/avg_episode_length': (Array(930.1406, dtype=float32), Array(254.02576, dtype=float32)), 'eval/epoch_eval_time': 4.109906911849976, 'eval/sps': 31144.25770348747}
I0726 22:54:07.600531 140267183036224 train.py:379] starting iteration 196 1439.8553779125214
I0726 22:54:14.769300 140267183036224 train.py:394] {'eval/walltime': 819.3833162784576, 'training/sps': 40194.666852381895, 'training/walltime': 621.399204492569, 'training/entropy_loss': Array(-0.02427641, dtype=float32), 'training/policy_loss': Array(-0.00028007, dtype=float32), 'training/total_loss': Array(76.08588, dtype=float32), 'training/v_loss': Array(76.11043, dtype=float32), 'eval/episode_goal_distance': (Array(0.2826642, dtype=float32), Array(0.0643259, dtype=float32)), 'eval/episode_reward': (Array(-13925.062, dtype=float32), Array(4927.4673, dtype=float32)), 'eval/avg_episode_length': (Array(914.6172, dtype=float32), Array(278.4634, dtype=float32)), 'eval/epoch_eval_time': 4.107980251312256, 'eval/sps': 31158.86449529829}
I0726 22:54:14.771707 140267183036224 train.py:379] starting iteration 197 1447.0265536308289
I0726 22:54:21.949305 140267183036224 train.py:394] {'eval/walltime': 823.5033431053162, 'training/sps': 40237.092708169796, 'training/walltime': 624.4531030654907, 'training/entropy_loss': Array(-0.02227277, dtype=float32), 'training/policy_loss': Array(8.448506e-05, dtype=float32), 'training/total_loss': Array(69.51342, dtype=float32), 'training/v_loss': Array(69.53561, dtype=float32), 'eval/episode_goal_distance': (Array(0.28502193, dtype=float32), Array(0.0595335, dtype=float32)), 'eval/episode_reward': (Array(-13830.703, dtype=float32), Array(4750.126, dtype=float32)), 'eval/avg_episode_length': (Array(914.5469, dtype=float32), Array(278.69244, dtype=float32)), 'eval/epoch_eval_time': 4.1200268268585205, 'eval/sps': 31067.758871268983}
I0726 22:54:21.951868 140267183036224 train.py:379] starting iteration 198 1454.2067139148712
I0726 22:54:29.121359 140267183036224 train.py:394] {'eval/walltime': 827.6166443824768, 'training/sps': 40253.46564124985, 'training/walltime': 627.5057594776154, 'training/entropy_loss': Array(-0.0204386, dtype=float32), 'training/policy_loss': Array(0.00040049, dtype=float32), 'training/total_loss': Array(66.66649, dtype=float32), 'training/v_loss': Array(66.68652, dtype=float32), 'eval/episode_goal_distance': (Array(0.2949725, dtype=float32), Array(0.05681538, dtype=float32)), 'eval/episode_reward': (Array(-14274.486, dtype=float32), Array(4181.6577, dtype=float32)), 'eval/avg_episode_length': (Array(945.6328, dtype=float32), Array(226.03809, dtype=float32)), 'eval/epoch_eval_time': 4.1133012771606445, 'eval/sps': 31118.556938857793}
I0726 22:54:29.123767 140267183036224 train.py:379] starting iteration 199 1461.3786134719849
I0726 22:54:36.287243 140267183036224 train.py:394] {'eval/walltime': 831.7278180122375, 'training/sps': 40306.83443947266, 'training/walltime': 630.5543739795685, 'training/entropy_loss': Array(-0.01985633, dtype=float32), 'training/policy_loss': Array(0.00021072, dtype=float32), 'training/total_loss': Array(57.210857, dtype=float32), 'training/v_loss': Array(57.230503, dtype=float32), 'eval/episode_goal_distance': (Array(0.2968592, dtype=float32), Array(0.06197152, dtype=float32)), 'eval/episode_reward': (Array(-14792.213, dtype=float32), Array(4213.452, dtype=float32)), 'eval/avg_episode_length': (Array(953.35156, dtype=float32), Array(210.34966, dtype=float32)), 'eval/epoch_eval_time': 4.111173629760742, 'eval/sps': 31134.661662890947}
I0726 22:54:36.289767 140267183036224 train.py:379] starting iteration 200 1468.5446135997772
I0726 22:54:43.463082 140267183036224 train.py:394] {'eval/walltime': 835.8463823795319, 'training/sps': 40274.1913077725, 'training/walltime': 633.6054594516754, 'training/entropy_loss': Array(-0.02205854, dtype=float32), 'training/policy_loss': Array(-1.4747551e-05, dtype=float32), 'training/total_loss': Array(486.11447, dtype=float32), 'training/v_loss': Array(486.13654, dtype=float32), 'eval/episode_goal_distance': (Array(0.29269513, dtype=float32), Array(0.06358888, dtype=float32)), 'eval/episode_reward': (Array(-13542.334, dtype=float32), Array(5674.399, dtype=float32)), 'eval/avg_episode_length': (Array(875.8047, dtype=float32), Array(328.59045, dtype=float32)), 'eval/epoch_eval_time': 4.1185643672943115, 'eval/sps': 31078.790710776124}
I0726 22:54:43.465392 140267183036224 train.py:379] starting iteration 201 1475.7202389240265
I0726 22:54:50.619491 140267183036224 train.py:394] {'eval/walltime': 839.9491302967072, 'training/sps': 40319.25169903053, 'training/walltime': 636.653135061264, 'training/entropy_loss': Array(-0.02034977, dtype=float32), 'training/policy_loss': Array(-6.545829e-05, dtype=float32), 'training/total_loss': Array(129.7201, dtype=float32), 'training/v_loss': Array(129.74051, dtype=float32), 'eval/episode_goal_distance': (Array(0.28540796, dtype=float32), Array(0.05732411, dtype=float32)), 'eval/episode_reward': (Array(-14540.482, dtype=float32), Array(3908.117, dtype=float32)), 'eval/avg_episode_length': (Array(961.1719, dtype=float32), Array(192.58153, dtype=float32)), 'eval/epoch_eval_time': 4.102747917175293, 'eval/sps': 31198.60215251219}
I0726 22:54:50.622148 140267183036224 train.py:379] starting iteration 202 1482.8769953250885
I0726 22:54:57.799368 140267183036224 train.py:394] {'eval/walltime': 844.067932844162, 'training/sps': 40225.67725212045, 'training/walltime': 639.7079002857208, 'training/entropy_loss': Array(-0.02040675, dtype=float32), 'training/policy_loss': Array(0.00067135, dtype=float32), 'training/total_loss': Array(135.65392, dtype=float32), 'training/v_loss': Array(135.67366, dtype=float32), 'eval/episode_goal_distance': (Array(0.29179275, dtype=float32), Array(0.0625088, dtype=float32)), 'eval/episode_reward': (Array(-13869.494, dtype=float32), Array(5273.3696, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.3065, dtype=float32)), 'eval/epoch_eval_time': 4.118802547454834, 'eval/sps': 31076.993501204885}
I0726 22:54:57.801889 140267183036224 train.py:379] starting iteration 203 1490.0567362308502
I0726 22:55:04.958114 140267183036224 train.py:394] {'eval/walltime': 848.1740140914917, 'training/sps': 40334.46364498139, 'training/walltime': 642.7544264793396, 'training/entropy_loss': Array(-0.02238676, dtype=float32), 'training/policy_loss': Array(0.0002546, dtype=float32), 'training/total_loss': Array(116.42081, dtype=float32), 'training/v_loss': Array(116.44294, dtype=float32), 'eval/episode_goal_distance': (Array(0.28832835, dtype=float32), Array(0.06126158, dtype=float32)), 'eval/episode_reward': (Array(-13737.193, dtype=float32), Array(5113.349, dtype=float32)), 'eval/avg_episode_length': (Array(906.8203, dtype=float32), Array(289.708, dtype=float32)), 'eval/epoch_eval_time': 4.106081247329712, 'eval/sps': 31173.27502548607}
I0726 22:55:04.960484 140267183036224 train.py:379] starting iteration 204 1497.2153315544128
I0726 22:55:12.126972 140267183036224 train.py:394] {'eval/walltime': 852.2879507541656, 'training/sps': 40302.02472845972, 'training/walltime': 645.8034048080444, 'training/entropy_loss': Array(-0.0245412, dtype=float32), 'training/policy_loss': Array(9.237764e-05, dtype=float32), 'training/total_loss': Array(91.2834, dtype=float32), 'training/v_loss': Array(91.30786, dtype=float32), 'eval/episode_goal_distance': (Array(0.2840358, dtype=float32), Array(0.06213763, dtype=float32)), 'eval/episode_reward': (Array(-13840.266, dtype=float32), Array(5035.4155, dtype=float32)), 'eval/avg_episode_length': (Array(914.66406, dtype=float32), Array(278.3107, dtype=float32)), 'eval/epoch_eval_time': 4.11393666267395, 'eval/sps': 31113.75076854085}
I0726 22:55:12.221426 140267183036224 train.py:379] starting iteration 205 1504.4762694835663
I0726 22:55:19.394472 140267183036224 train.py:394] {'eval/walltime': 856.3939402103424, 'training/sps': 40113.67304648077, 'training/walltime': 648.8666994571686, 'training/entropy_loss': Array(-0.02492871, dtype=float32), 'training/policy_loss': Array(-4.3743916e-05, dtype=float32), 'training/total_loss': Array(99.58365, dtype=float32), 'training/v_loss': Array(99.60862, dtype=float32), 'eval/episode_goal_distance': (Array(0.28612375, dtype=float32), Array(0.05636051, dtype=float32)), 'eval/episode_reward': (Array(-14191.256, dtype=float32), Array(4363.4766, dtype=float32)), 'eval/avg_episode_length': (Array(937.8828, dtype=float32), Array(240.57939, dtype=float32)), 'eval/epoch_eval_time': 4.105989456176758, 'eval/sps': 31173.97191740128}
I0726 22:55:19.397072 140267183036224 train.py:379] starting iteration 206 1511.651918888092
I0726 22:55:26.559752 140267183036224 train.py:394] {'eval/walltime': 860.4931244850159, 'training/sps': 40159.229011734154, 'training/walltime': 651.9265191555023, 'training/entropy_loss': Array(-0.02335565, dtype=float32), 'training/policy_loss': Array(0.00034519, dtype=float32), 'training/total_loss': Array(114.377335, dtype=float32), 'training/v_loss': Array(114.400345, dtype=float32), 'eval/episode_goal_distance': (Array(0.2891702, dtype=float32), Array(0.05708091, dtype=float32)), 'eval/episode_reward': (Array(-13129.96, dtype=float32), Array(5902.846, dtype=float32)), 'eval/avg_episode_length': (Array(860.3047, dtype=float32), Array(345.3366, dtype=float32)), 'eval/epoch_eval_time': 4.099184274673462, 'eval/sps': 31225.72478403557}
I0726 22:55:26.562218 140267183036224 train.py:379] starting iteration 207 1518.8170647621155
I0726 22:55:33.731071 140267183036224 train.py:394] {'eval/walltime': 864.5985832214355, 'training/sps': 40161.13163928016, 'training/walltime': 654.98619389534, 'training/entropy_loss': Array(-0.02283759, dtype=float32), 'training/policy_loss': Array(0.00028526, dtype=float32), 'training/total_loss': Array(66.71861, dtype=float32), 'training/v_loss': Array(66.741165, dtype=float32), 'eval/episode_goal_distance': (Array(0.29299122, dtype=float32), Array(0.06784112, dtype=float32)), 'eval/episode_reward': (Array(-12899.462, dtype=float32), Array(6011.097, dtype=float32)), 'eval/avg_episode_length': (Array(852.375, dtype=float32), Array(353.58725, dtype=float32)), 'eval/epoch_eval_time': 4.105458736419678, 'eval/sps': 31178.001830710713}
I0726 22:55:33.733591 140267183036224 train.py:379] starting iteration 208 1525.9884369373322
I0726 22:55:40.895020 140267183036224 train.py:394] {'eval/walltime': 868.6966886520386, 'training/sps': 40162.94994360078, 'training/walltime': 658.0457301139832, 'training/entropy_loss': Array(-0.02256661, dtype=float32), 'training/policy_loss': Array(0.00056829, dtype=float32), 'training/total_loss': Array(63.907585, dtype=float32), 'training/v_loss': Array(63.92959, dtype=float32), 'eval/episode_goal_distance': (Array(0.2972793, dtype=float32), Array(0.06124896, dtype=float32)), 'eval/episode_reward': (Array(-14232.99, dtype=float32), Array(4147.864, dtype=float32)), 'eval/avg_episode_length': (Array(945.625, dtype=float32), Array(226.07019, dtype=float32)), 'eval/epoch_eval_time': 4.098105430603027, 'eval/sps': 31233.94509183359}
I0726 22:55:40.897481 140267183036224 train.py:379] starting iteration 209 1533.1523277759552
I0726 22:55:48.057646 140267183036224 train.py:394] {'eval/walltime': 872.7934415340424, 'training/sps': 40160.10207008435, 'training/walltime': 661.1054832935333, 'training/entropy_loss': Array(-0.02058661, dtype=float32), 'training/policy_loss': Array(0.0001401, dtype=float32), 'training/total_loss': Array(74.485, dtype=float32), 'training/v_loss': Array(74.50545, dtype=float32), 'eval/episode_goal_distance': (Array(0.29119313, dtype=float32), Array(0.05907046, dtype=float32)), 'eval/episode_reward': (Array(-14371., dtype=float32), Array(4427.7363, dtype=float32)), 'eval/avg_episode_length': (Array(937.8281, dtype=float32), Array(240.79068, dtype=float32)), 'eval/epoch_eval_time': 4.096752882003784, 'eval/sps': 31244.257021769215}
I0726 22:55:48.060207 140267183036224 train.py:379] starting iteration 210 1540.3150532245636
I0726 22:55:55.218332 140267183036224 train.py:394] {'eval/walltime': 876.8902273178101, 'training/sps': 40187.45520553927, 'training/walltime': 664.163153886795, 'training/entropy_loss': Array(-0.01888797, dtype=float32), 'training/policy_loss': Array(0.00057681, dtype=float32), 'training/total_loss': Array(51.36967, dtype=float32), 'training/v_loss': Array(51.38798, dtype=float32), 'eval/episode_goal_distance': (Array(0.29620323, dtype=float32), Array(0.05889778, dtype=float32)), 'eval/episode_reward': (Array(-14791.737, dtype=float32), Array(4173.685, dtype=float32)), 'eval/avg_episode_length': (Array(953.5078, dtype=float32), Array(209.64508, dtype=float32)), 'eval/epoch_eval_time': 4.0967857837677, 'eval/sps': 31244.006095501034}
I0726 22:55:55.220831 140267183036224 train.py:379] starting iteration 211 1547.4756774902344
I0726 22:56:02.381644 140267183036224 train.py:394] {'eval/walltime': 880.990403175354, 'training/sps': 40197.105795704025, 'training/walltime': 667.2200903892517, 'training/entropy_loss': Array(-0.01985593, dtype=float32), 'training/policy_loss': Array(0.00036134, dtype=float32), 'training/total_loss': Array(52.3742, dtype=float32), 'training/v_loss': Array(52.393692, dtype=float32), 'eval/episode_goal_distance': (Array(0.29448137, dtype=float32), Array(0.05616312, dtype=float32)), 'eval/episode_reward': (Array(-14171.52, dtype=float32), Array(4673.0923, dtype=float32)), 'eval/avg_episode_length': (Array(922.40625, dtype=float32), Array(266.5438, dtype=float32)), 'eval/epoch_eval_time': 4.100175857543945, 'eval/sps': 31218.17318261894}
I0726 22:56:02.384097 140267183036224 train.py:379] starting iteration 212 1554.6389436721802
I0726 22:56:09.546218 140267183036224 train.py:394] {'eval/walltime': 885.0846240520477, 'training/sps': 40101.3134268296, 'training/walltime': 670.2843291759491, 'training/entropy_loss': Array(-0.0191722, dtype=float32), 'training/policy_loss': Array(0.00028592, dtype=float32), 'training/total_loss': Array(55.27148, dtype=float32), 'training/v_loss': Array(55.290363, dtype=float32), 'eval/episode_goal_distance': (Array(0.28637415, dtype=float32), Array(0.05671742, dtype=float32)), 'eval/episode_reward': (Array(-14445.592, dtype=float32), Array(4048.9343, dtype=float32)), 'eval/avg_episode_length': (Array(953.4453, dtype=float32), Array(209.92683, dtype=float32)), 'eval/epoch_eval_time': 4.094220876693726, 'eval/sps': 31263.57953197825}
I0726 22:56:09.548701 140267183036224 train.py:379] starting iteration 213 1561.803546667099
I0726 22:56:16.712746 140267183036224 train.py:394] {'eval/walltime': 889.1836442947388, 'training/sps': 40140.06571525523, 'training/walltime': 673.345609664917, 'training/entropy_loss': Array(-0.02111779, dtype=float32), 'training/policy_loss': Array(0.00014969, dtype=float32), 'training/total_loss': Array(61.093117, dtype=float32), 'training/v_loss': Array(61.114082, dtype=float32), 'eval/episode_goal_distance': (Array(0.29278624, dtype=float32), Array(0.05570601, dtype=float32)), 'eval/episode_reward': (Array(-14703.736, dtype=float32), Array(3897.1453, dtype=float32)), 'eval/avg_episode_length': (Array(961.1875, dtype=float32), Array(192.50385, dtype=float32)), 'eval/epoch_eval_time': 4.09902024269104, 'eval/sps': 31226.974355210055}
I0726 22:56:16.715149 140267183036224 train.py:379] starting iteration 214 1568.9699952602386
I0726 22:56:23.878629 140267183036224 train.py:394] {'eval/walltime': 893.2852220535278, 'training/sps': 40180.05506752, 'training/walltime': 676.4038434028625, 'training/entropy_loss': Array(-0.02137434, dtype=float32), 'training/policy_loss': Array(9.037504e-05, dtype=float32), 'training/total_loss': Array(64.65469, dtype=float32), 'training/v_loss': Array(64.67598, dtype=float32), 'eval/episode_goal_distance': (Array(0.28616238, dtype=float32), Array(0.05900691, dtype=float32)), 'eval/episode_reward': (Array(-14471.36, dtype=float32), Array(4303.224, dtype=float32)), 'eval/avg_episode_length': (Array(945.7031, dtype=float32), Array(225.74567, dtype=float32)), 'eval/epoch_eval_time': 4.1015777587890625, 'eval/sps': 31207.50294827772}
I0726 22:56:23.881066 140267183036224 train.py:379] starting iteration 215 1576.1359133720398
I0726 22:56:31.047522 140267183036224 train.py:394] {'eval/walltime': 897.3918311595917, 'training/sps': 40206.75161220238, 'training/walltime': 679.4600465297699, 'training/entropy_loss': Array(-0.02245293, dtype=float32), 'training/policy_loss': Array(0.00038008, dtype=float32), 'training/total_loss': Array(62.357635, dtype=float32), 'training/v_loss': Array(62.37971, dtype=float32), 'eval/episode_goal_distance': (Array(0.29209206, dtype=float32), Array(0.05861009, dtype=float32)), 'eval/episode_reward': (Array(-13742.375, dtype=float32), Array(5516.449, dtype=float32)), 'eval/avg_episode_length': (Array(883.46094, dtype=float32), Array(319.86432, dtype=float32)), 'eval/epoch_eval_time': 4.106609106063843, 'eval/sps': 31169.268049153852}
I0726 22:56:31.049916 140267183036224 train.py:379] starting iteration 216 1583.304762840271
I0726 22:56:38.212935 140267183036224 train.py:394] {'eval/walltime': 901.4953224658966, 'training/sps': 40210.785663411625, 'training/walltime': 682.5159430503845, 'training/entropy_loss': Array(-0.02235509, dtype=float32), 'training/policy_loss': Array(0.00016145, dtype=float32), 'training/total_loss': Array(358.5489, dtype=float32), 'training/v_loss': Array(358.5711, dtype=float32), 'eval/episode_goal_distance': (Array(0.29524198, dtype=float32), Array(0.05750078, dtype=float32)), 'eval/episode_reward': (Array(-14050.683, dtype=float32), Array(4738.882, dtype=float32)), 'eval/avg_episode_length': (Array(922.3281, dtype=float32), Array(266.81204, dtype=float32)), 'eval/epoch_eval_time': 4.103491306304932, 'eval/sps': 31192.950208845472}
I0726 22:56:38.215352 140267183036224 train.py:379] starting iteration 217 1590.4701986312866
I0726 22:56:45.371800 140267183036224 train.py:394] {'eval/walltime': 905.6000101566315, 'training/sps': 40313.568702540855, 'training/walltime': 685.5640482902527, 'training/entropy_loss': Array(-0.02132837, dtype=float32), 'training/policy_loss': Array(0.00027087, dtype=float32), 'training/total_loss': Array(218.69855, dtype=float32), 'training/v_loss': Array(218.7196, dtype=float32), 'eval/episode_goal_distance': (Array(0.29952413, dtype=float32), Array(0.06183973, dtype=float32)), 'eval/episode_reward': (Array(-13606.116, dtype=float32), Array(5503.366, dtype=float32)), 'eval/avg_episode_length': (Array(883.4219, dtype=float32), Array(319.97134, dtype=float32)), 'eval/epoch_eval_time': 4.104687690734863, 'eval/sps': 31183.858467216083}
I0726 22:56:45.374261 140267183036224 train.py:379] starting iteration 218 1597.6291074752808
I0726 22:56:52.528264 140267183036224 train.py:394] {'eval/walltime': 909.7042200565338, 'training/sps': 40339.230585540834, 'training/walltime': 688.610214471817, 'training/entropy_loss': Array(-0.0211509, dtype=float32), 'training/policy_loss': Array(0.00012357, dtype=float32), 'training/total_loss': Array(106.74614, dtype=float32), 'training/v_loss': Array(106.767166, dtype=float32), 'eval/episode_goal_distance': (Array(0.2858585, dtype=float32), Array(0.05760261, dtype=float32)), 'eval/episode_reward': (Array(-14130.404, dtype=float32), Array(4703.803, dtype=float32)), 'eval/avg_episode_length': (Array(922.3594, dtype=float32), Array(266.7046, dtype=float32)), 'eval/epoch_eval_time': 4.104209899902344, 'eval/sps': 31187.488730302426}
I0726 22:56:52.530806 140267183036224 train.py:379] starting iteration 219 1604.785651922226
I0726 22:56:59.687059 140267183036224 train.py:394] {'eval/walltime': 913.8067164421082, 'training/sps': 40285.66265964312, 'training/walltime': 691.6604311466217, 'training/entropy_loss': Array(-0.0198492, dtype=float32), 'training/policy_loss': Array(1.0009659e-05, dtype=float32), 'training/total_loss': Array(144.79294, dtype=float32), 'training/v_loss': Array(144.81279, dtype=float32), 'eval/episode_goal_distance': (Array(0.28357086, dtype=float32), Array(0.06451631, dtype=float32)), 'eval/episode_reward': (Array(-13132.928, dtype=float32), Array(5455.435, dtype=float32)), 'eval/avg_episode_length': (Array(883.4844, dtype=float32), Array(319.8002, dtype=float32)), 'eval/epoch_eval_time': 4.102496385574341, 'eval/sps': 31200.514996207672}
I0726 22:56:59.689432 140267183036224 train.py:379] starting iteration 220 1611.944278717041
I0726 22:57:06.842858 140267183036224 train.py:394] {'eval/walltime': 917.908611536026, 'training/sps': 40316.48253611849, 'training/walltime': 694.7083160877228, 'training/entropy_loss': Array(-0.01879048, dtype=float32), 'training/policy_loss': Array(3.5076435e-05, dtype=float32), 'training/total_loss': Array(105.569885, dtype=float32), 'training/v_loss': Array(105.58864, dtype=float32), 'eval/episode_goal_distance': (Array(0.2831121, dtype=float32), Array(0.05267727, dtype=float32)), 'eval/episode_reward': (Array(-13835.35, dtype=float32), Array(4653.35, dtype=float32)), 'eval/avg_episode_length': (Array(922.39844, dtype=float32), Array(266.57095, dtype=float32)), 'eval/epoch_eval_time': 4.101895093917847, 'eval/sps': 31205.088640563758}
I0726 22:57:06.845335 140267183036224 train.py:379] starting iteration 221 1619.1001811027527
I0726 22:57:13.998783 140267183036224 train.py:394] {'eval/walltime': 922.0150895118713, 'training/sps': 40375.29376070053, 'training/walltime': 697.7517614364624, 'training/entropy_loss': Array(-0.01780508, dtype=float32), 'training/policy_loss': Array(0.00018273, dtype=float32), 'training/total_loss': Array(75.341644, dtype=float32), 'training/v_loss': Array(75.35927, dtype=float32), 'eval/episode_goal_distance': (Array(0.29866886, dtype=float32), Array(0.06104309, dtype=float32)), 'eval/episode_reward': (Array(-14697.055, dtype=float32), Array(4574.4595, dtype=float32)), 'eval/avg_episode_length': (Array(937.8672, dtype=float32), Array(240.63956, dtype=float32)), 'eval/epoch_eval_time': 4.106477975845337, 'eval/sps': 31170.263362644877}
I0726 22:57:14.001203 140267183036224 train.py:379] starting iteration 222 1626.2560501098633
I0726 22:57:21.154010 140267183036224 train.py:394] {'eval/walltime': 926.1165022850037, 'training/sps': 40318.103617220106, 'training/walltime': 700.7995238304138, 'training/entropy_loss': Array(-0.01894641, dtype=float32), 'training/policy_loss': Array(-4.210144e-05, dtype=float32), 'training/total_loss': Array(97.90788, dtype=float32), 'training/v_loss': Array(97.926865, dtype=float32), 'eval/episode_goal_distance': (Array(0.2910953, dtype=float32), Array(0.05997186, dtype=float32)), 'eval/episode_reward': (Array(-14266.971, dtype=float32), Array(4676.979, dtype=float32)), 'eval/avg_episode_length': (Array(930.08594, dtype=float32), Array(254.2244, dtype=float32)), 'eval/epoch_eval_time': 4.101412773132324, 'eval/sps': 31208.758318232878}
I0726 22:57:21.156448 140267183036224 train.py:379] starting iteration 223 1633.4112939834595
I0726 22:57:28.323011 140267183036224 train.py:394] {'eval/walltime': 930.2384600639343, 'training/sps': 40407.76841107667, 'training/walltime': 703.8405232429504, 'training/entropy_loss': Array(-0.01886521, dtype=float32), 'training/policy_loss': Array(0.00018608, dtype=float32), 'training/total_loss': Array(77.21893, dtype=float32), 'training/v_loss': Array(77.23761, dtype=float32), 'eval/episode_goal_distance': (Array(0.29389027, dtype=float32), Array(0.06078108, dtype=float32)), 'eval/episode_reward': (Array(-13754.292, dtype=float32), Array(5127.008, dtype=float32)), 'eval/avg_episode_length': (Array(906.78906, dtype=float32), Array(289.80524, dtype=float32)), 'eval/epoch_eval_time': 4.121957778930664, 'eval/sps': 31053.20502171818}
I0726 22:57:28.325536 140267183036224 train.py:379] starting iteration 224 1640.580382347107
I0726 22:57:35.485901 140267183036224 train.py:394] {'eval/walltime': 934.3535315990448, 'training/sps': 40396.36673847253, 'training/walltime': 706.8823809623718, 'training/entropy_loss': Array(-0.01657196, dtype=float32), 'training/policy_loss': Array(0.00030616, dtype=float32), 'training/total_loss': Array(64.172676, dtype=float32), 'training/v_loss': Array(64.18894, dtype=float32), 'eval/episode_goal_distance': (Array(0.2813433, dtype=float32), Array(0.06276492, dtype=float32)), 'eval/episode_reward': (Array(-13429.4375, dtype=float32), Array(5284.8823, dtype=float32)), 'eval/avg_episode_length': (Array(899.03906, dtype=float32), Array(300.2833, dtype=float32)), 'eval/epoch_eval_time': 4.115071535110474, 'eval/sps': 31105.170082192435}
I0726 22:57:35.488265 140267183036224 train.py:379] starting iteration 225 1647.7431118488312
I0726 22:57:42.666233 140267183036224 train.py:394] {'eval/walltime': 938.4836201667786, 'training/sps': 40361.98219407495, 'training/walltime': 709.9268300533295, 'training/entropy_loss': Array(-0.01762831, dtype=float32), 'training/policy_loss': Array(0.00051654, dtype=float32), 'training/total_loss': Array(69.66804, dtype=float32), 'training/v_loss': Array(69.68515, dtype=float32), 'eval/episode_goal_distance': (Array(0.29181847, dtype=float32), Array(0.06349438, dtype=float32)), 'eval/episode_reward': (Array(-14084.02, dtype=float32), Array(5242.3555, dtype=float32)), 'eval/avg_episode_length': (Array(906.84375, dtype=float32), Array(289.63495, dtype=float32)), 'eval/epoch_eval_time': 4.130088567733765, 'eval/sps': 30992.071453381766}
I0726 22:57:42.668601 140267183036224 train.py:379] starting iteration 226 1654.9234478473663
I0726 22:57:49.833194 140267183036224 train.py:394] {'eval/walltime': 942.5925393104553, 'training/sps': 40289.07006583725, 'training/walltime': 712.9767887592316, 'training/entropy_loss': Array(-0.01756085, dtype=float32), 'training/policy_loss': Array(0.00035753, dtype=float32), 'training/total_loss': Array(60.3605, dtype=float32), 'training/v_loss': Array(60.3777, dtype=float32), 'eval/episode_goal_distance': (Array(0.28728402, dtype=float32), Array(0.05349554, dtype=float32)), 'eval/episode_reward': (Array(-14681.283, dtype=float32), Array(3297.1655, dtype=float32)), 'eval/avg_episode_length': (Array(976.7031, dtype=float32), Array(150.38084, dtype=float32)), 'eval/epoch_eval_time': 4.108919143676758, 'eval/sps': 31151.74466184861}
I0726 22:57:49.835690 140267183036224 train.py:379] starting iteration 227 1662.0905363559723
I0726 22:57:57.025690 140267183036224 train.py:394] {'eval/walltime': 946.7325592041016, 'training/sps': 40336.8248760361, 'training/walltime': 716.0231366157532, 'training/entropy_loss': Array(-0.01652219, dtype=float32), 'training/policy_loss': Array(0.00039872, dtype=float32), 'training/total_loss': Array(56.977516, dtype=float32), 'training/v_loss': Array(56.993633, dtype=float32), 'eval/episode_goal_distance': (Array(0.28095937, dtype=float32), Array(0.06213612, dtype=float32)), 'eval/episode_reward': (Array(-13773.402, dtype=float32), Array(4913.4907, dtype=float32)), 'eval/avg_episode_length': (Array(914.64844, dtype=float32), Array(278.36096, dtype=float32)), 'eval/epoch_eval_time': 4.14001989364624, 'eval/sps': 30917.725829396088}
I0726 22:57:57.028146 140267183036224 train.py:379] starting iteration 228 1669.2829928398132
I0726 22:58:04.185975 140267183036224 train.py:394] {'eval/walltime': 950.8413579463959, 'training/sps': 40350.08194323215, 'training/walltime': 719.0684835910797, 'training/entropy_loss': Array(-0.0156586, dtype=float32), 'training/policy_loss': Array(0.00011115, dtype=float32), 'training/total_loss': Array(56.384525, dtype=float32), 'training/v_loss': Array(56.40007, dtype=float32), 'eval/episode_goal_distance': (Array(0.2848407, dtype=float32), Array(0.06581137, dtype=float32)), 'eval/episode_reward': (Array(-13417.774, dtype=float32), Array(4996.624, dtype=float32)), 'eval/avg_episode_length': (Array(906.71875, dtype=float32), Array(290.0235, dtype=float32)), 'eval/epoch_eval_time': 4.1087987422943115, 'eval/sps': 31152.657510921574}
I0726 22:58:04.188465 140267183036224 train.py:379] starting iteration 229 1676.4433109760284
I0726 22:58:11.350193 140267183036224 train.py:394] {'eval/walltime': 954.9539957046509, 'training/sps': 40348.84365862677, 'training/walltime': 722.1139240264893, 'training/entropy_loss': Array(-0.01413943, dtype=float32), 'training/policy_loss': Array(0.00038471, dtype=float32), 'training/total_loss': Array(62.245453, dtype=float32), 'training/v_loss': Array(62.25921, dtype=float32), 'eval/episode_goal_distance': (Array(0.29330772, dtype=float32), Array(0.06008508, dtype=float32)), 'eval/episode_reward': (Array(-14298.4, dtype=float32), Array(4511.9395, dtype=float32)), 'eval/avg_episode_length': (Array(937.90625, dtype=float32), Array(240.48856, dtype=float32)), 'eval/epoch_eval_time': 4.112637758255005, 'eval/sps': 31123.577500370586}
I0726 22:58:11.352708 140267183036224 train.py:379] starting iteration 230 1683.6075539588928
I0726 22:58:18.506060 140267183036224 train.py:394] {'eval/walltime': 959.0578274726868, 'training/sps': 40340.72403775996, 'training/walltime': 725.1599774360657, 'training/entropy_loss': Array(-0.01324004, dtype=float32), 'training/policy_loss': Array(0.00042861, dtype=float32), 'training/total_loss': Array(68.750015, dtype=float32), 'training/v_loss': Array(68.76283, dtype=float32), 'eval/episode_goal_distance': (Array(0.28826848, dtype=float32), Array(0.0591756, dtype=float32)), 'eval/episode_reward': (Array(-14147.947, dtype=float32), Array(4779.424, dtype=float32)), 'eval/avg_episode_length': (Array(930.0625, dtype=float32), Array(254.30971, dtype=float32)), 'eval/epoch_eval_time': 4.103831768035889, 'eval/sps': 31190.362382047973}
I0726 22:58:18.508435 140267183036224 train.py:379] starting iteration 231 1690.7632813453674
I0726 22:58:25.690465 140267183036224 train.py:394] {'eval/walltime': 963.1910858154297, 'training/sps': 40352.92839062308, 'training/walltime': 728.2051095962524, 'training/entropy_loss': Array(-0.01462866, dtype=float32), 'training/policy_loss': Array(0.00061457, dtype=float32), 'training/total_loss': Array(69.190735, dtype=float32), 'training/v_loss': Array(69.20476, dtype=float32), 'eval/episode_goal_distance': (Array(0.28446454, dtype=float32), Array(0.06023042, dtype=float32)), 'eval/episode_reward': (Array(-14075.655, dtype=float32), Array(4500.1245, dtype=float32)), 'eval/avg_episode_length': (Array(930.0703, dtype=float32), Array(254.28143, dtype=float32)), 'eval/epoch_eval_time': 4.13325834274292, 'eval/sps': 30968.30378985128}
I0726 22:58:25.692831 140267183036224 train.py:379] starting iteration 232 1697.9476766586304
I0726 22:58:32.847722 140267183036224 train.py:394] {'eval/walltime': 967.2975370883942, 'training/sps': 40356.309266048294, 'training/walltime': 731.2499866485596, 'training/entropy_loss': Array(-0.01293344, dtype=float32), 'training/policy_loss': Array(0.00057929, dtype=float32), 'training/total_loss': Array(64.38414, dtype=float32), 'training/v_loss': Array(64.396484, dtype=float32), 'eval/episode_goal_distance': (Array(0.28807372, dtype=float32), Array(0.05914629, dtype=float32)), 'eval/episode_reward': (Array(-14097.037, dtype=float32), Array(4541.4683, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.08235, dtype=float32)), 'eval/epoch_eval_time': 4.1064512729644775, 'eval/sps': 31170.46605245503}
I0726 22:58:32.850133 140267183036224 train.py:379] starting iteration 233 1705.104979276657
I0726 22:58:40.018549 140267183036224 train.py:394] {'eval/walltime': 971.4155828952789, 'training/sps': 40332.13425209498, 'training/walltime': 734.2966887950897, 'training/entropy_loss': Array(-0.01463375, dtype=float32), 'training/policy_loss': Array(0.00054388, dtype=float32), 'training/total_loss': Array(461.63757, dtype=float32), 'training/v_loss': Array(461.65167, dtype=float32), 'eval/episode_goal_distance': (Array(0.2956313, dtype=float32), Array(0.06438033, dtype=float32)), 'eval/episode_reward': (Array(-13986.576, dtype=float32), Array(5139.5703, dtype=float32)), 'eval/avg_episode_length': (Array(914.5781, dtype=float32), Array(278.59036, dtype=float32)), 'eval/epoch_eval_time': 4.118045806884766, 'eval/sps': 31082.704273469437}
I0726 22:58:40.021153 140267183036224 train.py:379] starting iteration 234 1712.2760000228882
I0726 22:58:47.180537 140267183036224 train.py:394] {'eval/walltime': 975.5275721549988, 'training/sps': 40370.790243479074, 'training/walltime': 737.340473651886, 'training/entropy_loss': Array(-0.01553556, dtype=float32), 'training/policy_loss': Array(0.00066387, dtype=float32), 'training/total_loss': Array(183.15088, dtype=float32), 'training/v_loss': Array(183.16576, dtype=float32), 'eval/episode_goal_distance': (Array(0.29854333, dtype=float32), Array(0.06109896, dtype=float32)), 'eval/episode_reward': (Array(-13783.3125, dtype=float32), Array(5655.4976, dtype=float32)), 'eval/avg_episode_length': (Array(883.53125, dtype=float32), Array(319.6715, dtype=float32)), 'eval/epoch_eval_time': 4.111989259719849, 'eval/sps': 31128.48597486869}
I0726 22:58:47.182897 140267183036224 train.py:379] starting iteration 235 1719.4377431869507
I0726 22:58:54.360724 140267183036224 train.py:394] {'eval/walltime': 979.6587932109833, 'training/sps': 40381.07959802891, 'training/walltime': 740.3834829330444, 'training/entropy_loss': Array(-0.01594327, dtype=float32), 'training/policy_loss': Array(0.00047207, dtype=float32), 'training/total_loss': Array(111.2813, dtype=float32), 'training/v_loss': Array(111.29678, dtype=float32), 'eval/episode_goal_distance': (Array(0.2991147, dtype=float32), Array(0.05697769, dtype=float32)), 'eval/episode_reward': (Array(-14378.2, dtype=float32), Array(4998.118, dtype=float32)), 'eval/avg_episode_length': (Array(914.5469, dtype=float32), Array(278.69272, dtype=float32)), 'eval/epoch_eval_time': 4.131221055984497, 'eval/sps': 30983.575622170807}
I0726 22:58:54.363098 140267183036224 train.py:379] starting iteration 236 1726.617943763733
I0726 22:59:01.521722 140267183036224 train.py:394] {'eval/walltime': 983.7670097351074, 'training/sps': 40331.496714491754, 'training/walltime': 743.4302332401276, 'training/entropy_loss': Array(-0.01599881, dtype=float32), 'training/policy_loss': Array(0.00022282, dtype=float32), 'training/total_loss': Array(158.45009, dtype=float32), 'training/v_loss': Array(158.46587, dtype=float32), 'eval/episode_goal_distance': (Array(0.29027626, dtype=float32), Array(0.05781495, dtype=float32)), 'eval/episode_reward': (Array(-14267.834, dtype=float32), Array(3935.1785, dtype=float32)), 'eval/avg_episode_length': (Array(953.3672, dtype=float32), Array(210.27916, dtype=float32)), 'eval/epoch_eval_time': 4.1082165241241455, 'eval/sps': 31157.07247861992}
I0726 22:59:01.524232 140267183036224 train.py:379] starting iteration 237 1733.7790787220001
I0726 22:59:08.685609 140267183036224 train.py:394] {'eval/walltime': 987.8796365261078, 'training/sps': 40351.22868782818, 'training/walltime': 746.4754936695099, 'training/entropy_loss': Array(-0.01584959, dtype=float32), 'training/policy_loss': Array(0.0004151, dtype=float32), 'training/total_loss': Array(98.778076, dtype=float32), 'training/v_loss': Array(98.79351, dtype=float32), 'eval/episode_goal_distance': (Array(0.29165965, dtype=float32), Array(0.05595216, dtype=float32)), 'eval/episode_reward': (Array(-14059.929, dtype=float32), Array(5050.2285, dtype=float32)), 'eval/avg_episode_length': (Array(914.58594, dtype=float32), Array(278.56494, dtype=float32)), 'eval/epoch_eval_time': 4.112626791000366, 'eval/sps': 31123.66049846817}
I0726 22:59:08.687972 140267183036224 train.py:379] starting iteration 238 1740.9428186416626
I0726 22:59:15.841221 140267183036224 train.py:394] {'eval/walltime': 991.9832322597504, 'training/sps': 40340.71772271862, 'training/walltime': 749.5215475559235, 'training/entropy_loss': Array(-0.01657559, dtype=float32), 'training/policy_loss': Array(0.0002561, dtype=float32), 'training/total_loss': Array(98.20375, dtype=float32), 'training/v_loss': Array(98.22008, dtype=float32), 'eval/episode_goal_distance': (Array(0.2935795, dtype=float32), Array(0.05340635, dtype=float32)), 'eval/episode_reward': (Array(-14292.521, dtype=float32), Array(4606.514, dtype=float32)), 'eval/avg_episode_length': (Array(930.0625, dtype=float32), Array(254.30962, dtype=float32)), 'eval/epoch_eval_time': 4.103595733642578, 'eval/sps': 31192.156417995917}
I0726 22:59:15.843807 140267183036224 train.py:379] starting iteration 239 1748.0986533164978
I0726 22:59:23.045834 140267183036224 train.py:394] {'eval/walltime': 996.1322062015533, 'training/sps': 40296.211112472396, 'training/walltime': 752.5709657669067, 'training/entropy_loss': Array(-0.01710782, dtype=float32), 'training/policy_loss': Array(0.00010633, dtype=float32), 'training/total_loss': Array(134.01736, dtype=float32), 'training/v_loss': Array(134.03435, dtype=float32), 'eval/episode_goal_distance': (Array(0.28026104, dtype=float32), Array(0.05849842, dtype=float32)), 'eval/episode_reward': (Array(-13699.05, dtype=float32), Array(4517.1934, dtype=float32)), 'eval/avg_episode_length': (Array(930.1406, dtype=float32), Array(254.02557, dtype=float32)), 'eval/epoch_eval_time': 4.1489739418029785, 'eval/sps': 30851.001186181544}
I0726 22:59:23.048310 140267183036224 train.py:379] starting iteration 240 1755.3031573295593
I0726 22:59:30.212367 140267183036224 train.py:394] {'eval/walltime': 1000.2433152198792, 'training/sps': 40296.88219225465, 'training/walltime': 755.6203331947327, 'training/entropy_loss': Array(-0.01830227, dtype=float32), 'training/policy_loss': Array(6.0267703e-05, dtype=float32), 'training/total_loss': Array(86.06735, dtype=float32), 'training/v_loss': Array(86.085594, dtype=float32), 'eval/episode_goal_distance': (Array(0.27863616, dtype=float32), Array(0.05671055, dtype=float32)), 'eval/episode_reward': (Array(-13689.571, dtype=float32), Array(4521.4336, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.08232, dtype=float32)), 'eval/epoch_eval_time': 4.111109018325806, 'eval/sps': 31135.150984666976}
I0726 22:59:30.214847 140267183036224 train.py:379] starting iteration 241 1762.469693183899
I0726 22:59:37.385560 140267183036224 train.py:394] {'eval/walltime': 1004.3642392158508, 'training/sps': 40339.84310859288, 'training/walltime': 758.6664531230927, 'training/entropy_loss': Array(-0.01708319, dtype=float32), 'training/policy_loss': Array(-0.00050595, dtype=float32), 'training/total_loss': Array(74.027245, dtype=float32), 'training/v_loss': Array(74.04484, dtype=float32), 'eval/episode_goal_distance': (Array(0.29184166, dtype=float32), Array(0.06054225, dtype=float32)), 'eval/episode_reward': (Array(-13443.041, dtype=float32), Array(5059.3413, dtype=float32)), 'eval/avg_episode_length': (Array(899.03906, dtype=float32), Array(300.28394, dtype=float32)), 'eval/epoch_eval_time': 4.12092399597168, 'eval/sps': 31060.99508875282}
I0726 22:59:37.388421 140267183036224 train.py:379] starting iteration 242 1769.6432673931122
I0726 22:59:44.552901 140267183036224 train.py:394] {'eval/walltime': 1008.481121301651, 'training/sps': 40367.214078662626, 'training/walltime': 761.7105076313019, 'training/entropy_loss': Array(-0.0154845, dtype=float32), 'training/policy_loss': Array(0.00015591, dtype=float32), 'training/total_loss': Array(73.26038, dtype=float32), 'training/v_loss': Array(73.27571, dtype=float32), 'eval/episode_goal_distance': (Array(0.28559357, dtype=float32), Array(0.06329836, dtype=float32)), 'eval/episode_reward': (Array(-14122.252, dtype=float32), Array(4565.8657, dtype=float32)), 'eval/avg_episode_length': (Array(937.8906, dtype=float32), Array(240.54889, dtype=float32)), 'eval/epoch_eval_time': 4.116882085800171, 'eval/sps': 31091.49043677832}
I0726 22:59:44.555511 140267183036224 train.py:379] starting iteration 243 1776.8103566169739
I0726 22:59:51.730377 140267183036224 train.py:394] {'eval/walltime': 1012.6066339015961, 'training/sps': 40344.75343719721, 'training/walltime': 764.7562568187714, 'training/entropy_loss': Array(-0.01646791, dtype=float32), 'training/policy_loss': Array(0.00042352, dtype=float32), 'training/total_loss': Array(64.09458, dtype=float32), 'training/v_loss': Array(64.11063, dtype=float32), 'eval/episode_goal_distance': (Array(0.2924621, dtype=float32), Array(0.06409274, dtype=float32)), 'eval/episode_reward': (Array(-13811.526, dtype=float32), Array(5063.869, dtype=float32)), 'eval/avg_episode_length': (Array(906.8047, dtype=float32), Array(289.75638, dtype=float32)), 'eval/epoch_eval_time': 4.125512599945068, 'eval/sps': 31026.447477509664}
I0726 22:59:51.732907 140267183036224 train.py:379] starting iteration 244 1783.9877526760101
I0726 22:59:58.918016 140267183036224 train.py:394] {'eval/walltime': 1016.7321972846985, 'training/sps': 40211.23742716339, 'training/walltime': 767.8121190071106, 'training/entropy_loss': Array(-0.01934656, dtype=float32), 'training/policy_loss': Array(0.00049672, dtype=float32), 'training/total_loss': Array(84.35525, dtype=float32), 'training/v_loss': Array(84.37409, dtype=float32), 'eval/episode_goal_distance': (Array(0.30622676, dtype=float32), Array(0.05799398, dtype=float32)), 'eval/episode_reward': (Array(-13536.764, dtype=float32), Array(5901.608, dtype=float32)), 'eval/avg_episode_length': (Array(860.25, dtype=float32), Array(345.47162, dtype=float32)), 'eval/epoch_eval_time': 4.125563383102417, 'eval/sps': 31026.065560952356}
I0726 22:59:58.920814 140267183036224 train.py:379] starting iteration 245 1791.1756608486176
I0726 23:00:06.104451 140267183036224 train.py:394] {'eval/walltime': 1020.8559143543243, 'training/sps': 40203.55255149023, 'training/walltime': 770.8685653209686, 'training/entropy_loss': Array(-0.02060795, dtype=float32), 'training/policy_loss': Array(-2.7008711e-05, dtype=float32), 'training/total_loss': Array(79.781006, dtype=float32), 'training/v_loss': Array(79.80164, dtype=float32), 'eval/episode_goal_distance': (Array(0.294509, dtype=float32), Array(0.05654525, dtype=float32)), 'eval/episode_reward': (Array(-14335.33, dtype=float32), Array(4519.2886, dtype=float32)), 'eval/avg_episode_length': (Array(930.21875, dtype=float32), Array(253.74178, dtype=float32)), 'eval/epoch_eval_time': 4.1237170696258545, 'eval/sps': 31039.956873572188}
I0726 23:00:06.106774 140267183036224 train.py:379] starting iteration 246 1798.361620426178
I0726 23:00:13.277835 140267183036224 train.py:394] {'eval/walltime': 1024.9678401947021, 'training/sps': 40215.43868870142, 'training/walltime': 773.9241082668304, 'training/entropy_loss': Array(-0.02079945, dtype=float32), 'training/policy_loss': Array(0.00026571, dtype=float32), 'training/total_loss': Array(76.13371, dtype=float32), 'training/v_loss': Array(76.154236, dtype=float32), 'eval/episode_goal_distance': (Array(0.28871742, dtype=float32), Array(0.05928449, dtype=float32)), 'eval/episode_reward': (Array(-13716.305, dtype=float32), Array(5252.42, dtype=float32)), 'eval/avg_episode_length': (Array(899.0781, dtype=float32), Array(300.16724, dtype=float32)), 'eval/epoch_eval_time': 4.111925840377808, 'eval/sps': 31128.96607790943}
I0726 23:00:13.280234 140267183036224 train.py:379] starting iteration 247 1805.5350799560547
I0726 23:00:20.460381 140267183036224 train.py:394] {'eval/walltime': 1029.0869801044464, 'training/sps': 40189.26022502414, 'training/walltime': 776.9816415309906, 'training/entropy_loss': Array(-0.02161638, dtype=float32), 'training/policy_loss': Array(-0.00012732, dtype=float32), 'training/total_loss': Array(83.595955, dtype=float32), 'training/v_loss': Array(83.61771, dtype=float32), 'eval/episode_goal_distance': (Array(0.28634506, dtype=float32), Array(0.06186576, dtype=float32)), 'eval/episode_reward': (Array(-14341.533, dtype=float32), Array(4585.3735, dtype=float32)), 'eval/avg_episode_length': (Array(937.85156, dtype=float32), Array(240.70009, dtype=float32)), 'eval/epoch_eval_time': 4.119139909744263, 'eval/sps': 31074.448259745295}
I0726 23:00:20.462713 140267183036224 train.py:379] starting iteration 248 1812.717559337616
I0726 23:00:27.647077 140267183036224 train.py:394] {'eval/walltime': 1033.2060947418213, 'training/sps': 40134.99880466255, 'training/walltime': 780.0433084964752, 'training/entropy_loss': Array(-0.02263636, dtype=float32), 'training/policy_loss': Array(0.00029791, dtype=float32), 'training/total_loss': Array(88.66524, dtype=float32), 'training/v_loss': Array(88.68758, dtype=float32), 'eval/episode_goal_distance': (Array(0.28732377, dtype=float32), Array(0.05759183, dtype=float32)), 'eval/episode_reward': (Array(-14030.302, dtype=float32), Array(4940.7485, dtype=float32)), 'eval/avg_episode_length': (Array(914.5625, dtype=float32), Array(278.6414, dtype=float32)), 'eval/epoch_eval_time': 4.119114637374878, 'eval/sps': 31074.638913563893}
I0726 23:00:27.649571 140267183036224 train.py:379] starting iteration 249 1819.9044179916382
I0726 23:00:34.840721 140267183036224 train.py:394] {'eval/walltime': 1037.335750579834, 'training/sps': 40211.504098031306, 'training/walltime': 783.0991504192352, 'training/entropy_loss': Array(-0.02234269, dtype=float32), 'training/policy_loss': Array(0.00018665, dtype=float32), 'training/total_loss': Array(82.10214, dtype=float32), 'training/v_loss': Array(82.1243, dtype=float32), 'eval/episode_goal_distance': (Array(0.29683816, dtype=float32), Array(0.05826752, dtype=float32)), 'eval/episode_reward': (Array(-14105.163, dtype=float32), Array(5069.047, dtype=float32)), 'eval/avg_episode_length': (Array(906.8047, dtype=float32), Array(289.75632, dtype=float32)), 'eval/epoch_eval_time': 4.129655838012695, 'eval/sps': 30995.318985612405}
I0726 23:00:34.843142 140267183036224 train.py:379] starting iteration 250 1827.0979888439178
I0726 23:00:42.032301 140267183036224 train.py:394] {'eval/walltime': 1041.464486837387, 'training/sps': 40195.557126440326, 'training/walltime': 786.15620470047, 'training/entropy_loss': Array(-0.02493513, dtype=float32), 'training/policy_loss': Array(0.00050053, dtype=float32), 'training/total_loss': Array(554.4342, dtype=float32), 'training/v_loss': Array(554.4586, dtype=float32), 'eval/episode_goal_distance': (Array(0.29288763, dtype=float32), Array(0.06079399, dtype=float32)), 'eval/episode_reward': (Array(-14020.579, dtype=float32), Array(4912.042, dtype=float32)), 'eval/avg_episode_length': (Array(914.60156, dtype=float32), Array(278.5141, dtype=float32)), 'eval/epoch_eval_time': 4.128736257553101, 'eval/sps': 31002.22247566361}
I0726 23:00:42.034597 140267183036224 train.py:379] starting iteration 251 1834.2894442081451
I0726 23:00:49.226372 140267183036224 train.py:394] {'eval/walltime': 1045.59516787529, 'training/sps': 40189.388713495435, 'training/walltime': 789.2137281894684, 'training/entropy_loss': Array(-0.02569769, dtype=float32), 'training/policy_loss': Array(-0.00010585, dtype=float32), 'training/total_loss': Array(182.27005, dtype=float32), 'training/v_loss': Array(182.29587, dtype=float32), 'eval/episode_goal_distance': (Array(0.2927089, dtype=float32), Array(0.06011513, dtype=float32)), 'eval/episode_reward': (Array(-14320.18, dtype=float32), Array(4495.4805, dtype=float32)), 'eval/avg_episode_length': (Array(937.8594, dtype=float32), Array(240.66989, dtype=float32)), 'eval/epoch_eval_time': 4.130681037902832, 'eval/sps': 30987.626211145624}
I0726 23:00:49.228824 140267183036224 train.py:379] starting iteration 252 1841.4836704730988
I0726 23:00:56.431490 140267183036224 train.py:394] {'eval/walltime': 1049.7419106960297, 'training/sps': 40257.084568171915, 'training/walltime': 792.2661101818085, 'training/entropy_loss': Array(-0.02603345, dtype=float32), 'training/policy_loss': Array(-0.00035611, dtype=float32), 'training/total_loss': Array(166.09576, dtype=float32), 'training/v_loss': Array(166.12216, dtype=float32), 'eval/episode_goal_distance': (Array(0.28547296, dtype=float32), Array(0.06375839, dtype=float32)), 'eval/episode_reward': (Array(-14219.8125, dtype=float32), Array(4051.5957, dtype=float32)), 'eval/avg_episode_length': (Array(953.4297, dtype=float32), Array(209.99736, dtype=float32)), 'eval/epoch_eval_time': 4.146742820739746, 'eval/sps': 30867.600315074716}
I0726 23:00:56.434386 140267183036224 train.py:379] starting iteration 253 1848.6892325878143
I0726 23:01:03.609224 140267183036224 train.py:394] {'eval/walltime': 1053.8677246570587, 'training/sps': 40348.03502398942, 'training/walltime': 795.3116116523743, 'training/entropy_loss': Array(-0.02547701, dtype=float32), 'training/policy_loss': Array(-3.930363e-06, dtype=float32), 'training/total_loss': Array(131.18108, dtype=float32), 'training/v_loss': Array(131.20654, dtype=float32), 'eval/episode_goal_distance': (Array(0.29706296, dtype=float32), Array(0.0586472, dtype=float32)), 'eval/episode_reward': (Array(-13659.931, dtype=float32), Array(5615.702, dtype=float32)), 'eval/avg_episode_length': (Array(883.52344, dtype=float32), Array(319.69287, dtype=float32)), 'eval/epoch_eval_time': 4.125813961029053, 'eval/sps': 31024.18121831031}
I0726 23:01:03.611590 140267183036224 train.py:379] starting iteration 254 1855.8664362430573
I0726 23:01:10.787192 140267183036224 train.py:394] {'eval/walltime': 1057.9860496520996, 'training/sps': 40238.76081786021, 'training/walltime': 798.3653836250305, 'training/entropy_loss': Array(-0.02254715, dtype=float32), 'training/policy_loss': Array(-0.00090229, dtype=float32), 'training/total_loss': Array(91.42887, dtype=float32), 'training/v_loss': Array(91.45232, dtype=float32), 'eval/episode_goal_distance': (Array(0.30045503, dtype=float32), Array(0.0575386, dtype=float32)), 'eval/episode_reward': (Array(-14309.77, dtype=float32), Array(4593.5093, dtype=float32)), 'eval/avg_episode_length': (Array(930.1406, dtype=float32), Array(254.02585, dtype=float32)), 'eval/epoch_eval_time': 4.1183249950408936, 'eval/sps': 31080.597124834003}
I0726 23:01:10.790091 140267183036224 train.py:379] starting iteration 255 1863.0449380874634
I0726 23:01:17.969501 140267183036224 train.py:394] {'eval/walltime': 1062.103613615036, 'training/sps': 40178.99007304774, 'training/walltime': 801.423698425293, 'training/entropy_loss': Array(-0.02128825, dtype=float32), 'training/policy_loss': Array(4.2387124e-05, dtype=float32), 'training/total_loss': Array(108.88603, dtype=float32), 'training/v_loss': Array(108.90728, dtype=float32), 'eval/episode_goal_distance': (Array(0.290415, dtype=float32), Array(0.06297663, dtype=float32)), 'eval/episode_reward': (Array(-12975.414, dtype=float32), Array(5825.1562, dtype=float32)), 'eval/avg_episode_length': (Array(860.1719, dtype=float32), Array(345.6649, dtype=float32)), 'eval/epoch_eval_time': 4.117563962936401, 'eval/sps': 31086.341621446973}
I0726 23:01:18.085256 140267183036224 train.py:379] starting iteration 256 1870.340099811554
I0726 23:01:25.282091 140267183036224 train.py:394] {'eval/walltime': 1066.237850189209, 'training/sps': 40171.398924670335, 'training/walltime': 804.4825911521912, 'training/entropy_loss': Array(-0.02320741, dtype=float32), 'training/policy_loss': Array(-4.221362e-05, dtype=float32), 'training/total_loss': Array(115.70032, dtype=float32), 'training/v_loss': Array(115.72357, dtype=float32), 'eval/episode_goal_distance': (Array(0.29473823, dtype=float32), Array(0.05649498, dtype=float32)), 'eval/episode_reward': (Array(-15048.854, dtype=float32), Array(3798.917, dtype=float32)), 'eval/avg_episode_length': (Array(968.97656, dtype=float32), Array(172.73143, dtype=float32)), 'eval/epoch_eval_time': 4.134236574172974, 'eval/sps': 30960.976156911278}
I0726 23:01:25.284679 140267183036224 train.py:379] starting iteration 257 1877.5395247936249
I0726 23:01:32.475581 140267183036224 train.py:394] {'eval/walltime': 1070.3765261173248, 'training/sps': 40306.97628979201, 'training/walltime': 807.5311949253082, 'training/entropy_loss': Array(-0.02212884, dtype=float32), 'training/policy_loss': Array(-0.00017661, dtype=float32), 'training/total_loss': Array(78.981094, dtype=float32), 'training/v_loss': Array(79.003395, dtype=float32), 'eval/episode_goal_distance': (Array(0.2918109, dtype=float32), Array(0.05962238, dtype=float32)), 'eval/episode_reward': (Array(-14100.588, dtype=float32), Array(4865.2124, dtype=float32)), 'eval/avg_episode_length': (Array(922.3281, dtype=float32), Array(266.81198, dtype=float32)), 'eval/epoch_eval_time': 4.138675928115845, 'eval/sps': 30927.765841833552}
I0726 23:01:32.478009 140267183036224 train.py:379] starting iteration 258 1884.7328550815582
I0726 23:01:39.643668 140267183036224 train.py:394] {'eval/walltime': 1074.4916453361511, 'training/sps': 40328.55862098753, 'training/walltime': 810.5781672000885, 'training/entropy_loss': Array(-0.02260048, dtype=float32), 'training/policy_loss': Array(6.7057845e-06, dtype=float32), 'training/total_loss': Array(104.855194, dtype=float32), 'training/v_loss': Array(104.87779, dtype=float32), 'eval/episode_goal_distance': (Array(0.3036052, dtype=float32), Array(0.06658103, dtype=float32)), 'eval/episode_reward': (Array(-14406.208, dtype=float32), Array(4635.4175, dtype=float32)), 'eval/avg_episode_length': (Array(930.0469, dtype=float32), Array(254.36655, dtype=float32)), 'eval/epoch_eval_time': 4.115119218826294, 'eval/sps': 31104.80965275847}
I0726 23:01:39.646132 140267183036224 train.py:379] starting iteration 259 1891.9009788036346
I0726 23:01:46.819988 140267183036224 train.py:394] {'eval/walltime': 1078.6116559505463, 'training/sps': 40283.12794852036, 'training/walltime': 813.6285758018494, 'training/entropy_loss': Array(-0.02215998, dtype=float32), 'training/policy_loss': Array(-2.4523579e-05, dtype=float32), 'training/total_loss': Array(79.51856, dtype=float32), 'training/v_loss': Array(79.54074, dtype=float32), 'eval/episode_goal_distance': (Array(0.30133772, dtype=float32), Array(0.0616007, dtype=float32)), 'eval/episode_reward': (Array(-13755.889, dtype=float32), Array(5648.6987, dtype=float32)), 'eval/avg_episode_length': (Array(883.4922, dtype=float32), Array(319.77872, dtype=float32)), 'eval/epoch_eval_time': 4.120010614395142, 'eval/sps': 31067.881124571246}
I0726 23:01:46.822397 140267183036224 train.py:379] starting iteration 260 1899.077243566513
I0726 23:01:53.989060 140267183036224 train.py:394] {'eval/walltime': 1082.7241146564484, 'training/sps': 40283.288523239666, 'training/walltime': 816.6789722442627, 'training/entropy_loss': Array(-0.02140078, dtype=float32), 'training/policy_loss': Array(-0.00053484, dtype=float32), 'training/total_loss': Array(84.89366, dtype=float32), 'training/v_loss': Array(84.91559, dtype=float32), 'eval/episode_goal_distance': (Array(0.2853586, dtype=float32), Array(0.06864849, dtype=float32)), 'eval/episode_reward': (Array(-14106.914, dtype=float32), Array(4855.707, dtype=float32)), 'eval/avg_episode_length': (Array(930.03906, dtype=float32), Array(254.39474, dtype=float32)), 'eval/epoch_eval_time': 4.1124587059021, 'eval/sps': 31124.9325899121}
I0726 23:01:53.991964 140267183036224 train.py:379] starting iteration 261 1906.2468101978302
I0726 23:02:01.161556 140267183036224 train.py:394] {'eval/walltime': 1086.8408012390137, 'training/sps': 40298.709658956475, 'training/walltime': 819.7282013893127, 'training/entropy_loss': Array(-0.0211266, dtype=float32), 'training/policy_loss': Array(0.00012759, dtype=float32), 'training/total_loss': Array(82.87895, dtype=float32), 'training/v_loss': Array(82.899956, dtype=float32), 'eval/episode_goal_distance': (Array(0.29857087, dtype=float32), Array(0.05602, dtype=float32)), 'eval/episode_reward': (Array(-14447.266, dtype=float32), Array(4829.3315, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.7586, dtype=float32)), 'eval/epoch_eval_time': 4.116686582565308, 'eval/sps': 31092.96698517111}
I0726 23:02:01.164047 140267183036224 train.py:379] starting iteration 262 1913.4188921451569
I0726 23:02:08.337670 140267183036224 train.py:394] {'eval/walltime': 1090.958547592163, 'training/sps': 40256.47770009258, 'training/walltime': 822.7806293964386, 'training/entropy_loss': Array(-0.02089906, dtype=float32), 'training/policy_loss': Array(0.00043907, dtype=float32), 'training/total_loss': Array(77.944, dtype=float32), 'training/v_loss': Array(77.96446, dtype=float32), 'eval/episode_goal_distance': (Array(0.29153877, dtype=float32), Array(0.05886677, dtype=float32)), 'eval/episode_reward': (Array(-13953.373, dtype=float32), Array(4514.945, dtype=float32)), 'eval/avg_episode_length': (Array(930.10156, dtype=float32), Array(254.16763, dtype=float32)), 'eval/epoch_eval_time': 4.117746353149414, 'eval/sps': 31084.964692422247}
I0726 23:02:08.339843 140267183036224 train.py:379] starting iteration 263 1920.5946896076202
I0726 23:02:15.513817 140267183036224 train.py:394] {'eval/walltime': 1095.079479932785, 'training/sps': 40295.54635594608, 'training/walltime': 825.8300979137421, 'training/entropy_loss': Array(-0.02088243, dtype=float32), 'training/policy_loss': Array(-0.00020494, dtype=float32), 'training/total_loss': Array(76.70736, dtype=float32), 'training/v_loss': Array(76.72844, dtype=float32), 'eval/episode_goal_distance': (Array(0.28403944, dtype=float32), Array(0.0545135, dtype=float32)), 'eval/episode_reward': (Array(-13662.365, dtype=float32), Array(4817.636, dtype=float32)), 'eval/avg_episode_length': (Array(914.6094, dtype=float32), Array(278.48834, dtype=float32)), 'eval/epoch_eval_time': 4.120932340621948, 'eval/sps': 31060.932192029562}
I0726 23:02:15.516280 140267183036224 train.py:379] starting iteration 264 1927.7711265087128
I0726 23:02:22.697170 140267183036224 train.py:394] {'eval/walltime': 1099.1927738189697, 'training/sps': 40105.02987512876, 'training/walltime': 828.8940527439117, 'training/entropy_loss': Array(-0.02233316, dtype=float32), 'training/policy_loss': Array(0.00012016, dtype=float32), 'training/total_loss': Array(93.5788, dtype=float32), 'training/v_loss': Array(93.60101, dtype=float32), 'eval/episode_goal_distance': (Array(0.29911005, dtype=float32), Array(0.06322711, dtype=float32)), 'eval/episode_reward': (Array(-14036.068, dtype=float32), Array(5359.7275, dtype=float32)), 'eval/avg_episode_length': (Array(899.1328, dtype=float32), Array(300.00473, dtype=float32)), 'eval/epoch_eval_time': 4.113293886184692, 'eval/sps': 31118.612854265826}
I0726 23:02:22.699556 140267183036224 train.py:379] starting iteration 265 1934.954402923584
I0726 23:02:29.879671 140267183036224 train.py:394] {'eval/walltime': 1103.3113088607788, 'training/sps': 40184.450333114786, 'training/walltime': 831.9519519805908, 'training/entropy_loss': Array(-0.02113249, dtype=float32), 'training/policy_loss': Array(7.515382e-05, dtype=float32), 'training/total_loss': Array(97.06081, dtype=float32), 'training/v_loss': Array(97.08186, dtype=float32), 'eval/episode_goal_distance': (Array(0.28745598, dtype=float32), Array(0.05762272, dtype=float32)), 'eval/episode_reward': (Array(-13828.084, dtype=float32), Array(4882.027, dtype=float32)), 'eval/avg_episode_length': (Array(914.5625, dtype=float32), Array(278.64136, dtype=float32)), 'eval/epoch_eval_time': 4.118535041809082, 'eval/sps': 31079.012003203818}
I0726 23:02:29.882055 140267183036224 train.py:379] starting iteration 266 1942.1369009017944
I0726 23:02:37.067833 140267183036224 train.py:394] {'eval/walltime': 1107.4346754550934, 'training/sps': 40172.425941950794, 'training/walltime': 835.0107665061951, 'training/entropy_loss': Array(-0.02109627, dtype=float32), 'training/policy_loss': Array(0.00027336, dtype=float32), 'training/total_loss': Array(378.19568, dtype=float32), 'training/v_loss': Array(378.21652, dtype=float32), 'eval/episode_goal_distance': (Array(0.29354006, dtype=float32), Array(0.05555071, dtype=float32)), 'eval/episode_reward': (Array(-14377.891, dtype=float32), Array(4344.055, dtype=float32)), 'eval/avg_episode_length': (Array(937.9297, dtype=float32), Array(240.39769, dtype=float32)), 'eval/epoch_eval_time': 4.123366594314575, 'eval/sps': 31042.595188235347}
I0726 23:02:37.070301 140267183036224 train.py:379] starting iteration 267 1949.3251473903656
I0726 23:02:44.252597 140267183036224 train.py:394] {'eval/walltime': 1111.555410861969, 'training/sps': 40182.61128461683, 'training/walltime': 838.0688056945801, 'training/entropy_loss': Array(-0.0207982, dtype=float32), 'training/policy_loss': Array(0.00022296, dtype=float32), 'training/total_loss': Array(238.50893, dtype=float32), 'training/v_loss': Array(238.5295, dtype=float32), 'eval/episode_goal_distance': (Array(0.29174685, dtype=float32), Array(0.06037369, dtype=float32)), 'eval/episode_reward': (Array(-14194.604, dtype=float32), Array(5109.906, dtype=float32)), 'eval/avg_episode_length': (Array(914.6094, dtype=float32), Array(278.48874, dtype=float32)), 'eval/epoch_eval_time': 4.12073540687561, 'eval/sps': 31062.41662263171}
I0726 23:02:44.254920 140267183036224 train.py:379] starting iteration 268 1956.5097658634186
I0726 23:02:51.442640 140267183036224 train.py:394] {'eval/walltime': 1115.6821191310883, 'training/sps': 40190.21293956918, 'training/walltime': 841.1262664794922, 'training/entropy_loss': Array(-0.0202891, dtype=float32), 'training/policy_loss': Array(0.00014517, dtype=float32), 'training/total_loss': Array(143.82213, dtype=float32), 'training/v_loss': Array(143.84227, dtype=float32), 'eval/episode_goal_distance': (Array(0.28756362, dtype=float32), Array(0.0523067, dtype=float32)), 'eval/episode_reward': (Array(-13570.49, dtype=float32), Array(5158.043, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.30643, dtype=float32)), 'eval/epoch_eval_time': 4.126708269119263, 'eval/sps': 31017.457899275792}
I0726 23:02:51.445096 140267183036224 train.py:379] starting iteration 269 1963.699942111969
I0726 23:02:58.641404 140267183036224 train.py:394] {'eval/walltime': 1119.8196377754211, 'training/sps': 40218.40112144177, 'training/walltime': 844.1815843582153, 'training/entropy_loss': Array(-0.02044835, dtype=float32), 'training/policy_loss': Array(-0.00027503, dtype=float32), 'training/total_loss': Array(155.7196, dtype=float32), 'training/v_loss': Array(155.74033, dtype=float32), 'eval/episode_goal_distance': (Array(0.28805864, dtype=float32), Array(0.0539085, dtype=float32)), 'eval/episode_reward': (Array(-14442.09, dtype=float32), Array(4409.2534, dtype=float32)), 'eval/avg_episode_length': (Array(945.58594, dtype=float32), Array(226.23262, dtype=float32)), 'eval/epoch_eval_time': 4.137518644332886, 'eval/sps': 30936.41648607921}
I0726 23:02:58.643726 140267183036224 train.py:379] starting iteration 270 1970.8985731601715
I0726 23:03:05.824340 140267183036224 train.py:394] {'eval/walltime': 1123.942512512207, 'training/sps': 40234.99441631764, 'training/walltime': 847.2356421947479, 'training/entropy_loss': Array(-0.02097685, dtype=float32), 'training/policy_loss': Array(-0.00063201, dtype=float32), 'training/total_loss': Array(126.669235, dtype=float32), 'training/v_loss': Array(126.69084, dtype=float32), 'eval/episode_goal_distance': (Array(0.29234743, dtype=float32), Array(0.05475982, dtype=float32)), 'eval/episode_reward': (Array(-14231.588, dtype=float32), Array(4768.4106, dtype=float32)), 'eval/avg_episode_length': (Array(922.2578, dtype=float32), Array(267.05347, dtype=float32)), 'eval/epoch_eval_time': 4.122874736785889, 'eval/sps': 31046.298559093808}
I0726 23:03:05.826819 140267183036224 train.py:379] starting iteration 271 1978.081665277481
I0726 23:03:13.035925 140267183036224 train.py:394] {'eval/walltime': 1128.0975904464722, 'training/sps': 40284.041037510324, 'training/walltime': 850.2859816551208, 'training/entropy_loss': Array(-0.02112833, dtype=float32), 'training/policy_loss': Array(-0.00018496, dtype=float32), 'training/total_loss': Array(115.053925, dtype=float32), 'training/v_loss': Array(115.07524, dtype=float32), 'eval/episode_goal_distance': (Array(0.29038948, dtype=float32), Array(0.06145128, dtype=float32)), 'eval/episode_reward': (Array(-13090.595, dtype=float32), Array(5687.071, dtype=float32)), 'eval/avg_episode_length': (Array(867.9922, dtype=float32), Array(337.31644, dtype=float32)), 'eval/epoch_eval_time': 4.155077934265137, 'eval/sps': 30805.6797068568}
I0726 23:03:13.038305 140267183036224 train.py:379] starting iteration 272 1985.2931518554688
I0726 23:03:20.211479 140267183036224 train.py:394] {'eval/walltime': 1132.2126393318176, 'training/sps': 40227.31615726768, 'training/walltime': 853.3406224250793, 'training/entropy_loss': Array(-0.02127525, dtype=float32), 'training/policy_loss': Array(-0.00014141, dtype=float32), 'training/total_loss': Array(116.04352, dtype=float32), 'training/v_loss': Array(116.064926, dtype=float32), 'eval/episode_goal_distance': (Array(0.28620824, dtype=float32), Array(0.06019806, dtype=float32)), 'eval/episode_reward': (Array(-14602.747, dtype=float32), Array(4077.4878, dtype=float32)), 'eval/avg_episode_length': (Array(953.39844, dtype=float32), Array(210.1384, dtype=float32)), 'eval/epoch_eval_time': 4.115048885345459, 'eval/sps': 31105.34128909975}
I0726 23:03:20.213764 140267183036224 train.py:379] starting iteration 273 1992.4686102867126
I0726 23:03:27.379890 140267183036224 train.py:394] {'eval/walltime': 1136.3274166584015, 'training/sps': 40317.96484252953, 'training/walltime': 856.3883953094482, 'training/entropy_loss': Array(-0.02181861, dtype=float32), 'training/policy_loss': Array(0.00034905, dtype=float32), 'training/total_loss': Array(127.1961, dtype=float32), 'training/v_loss': Array(127.21756, dtype=float32), 'eval/episode_goal_distance': (Array(0.29795974, dtype=float32), Array(0.056996, dtype=float32)), 'eval/episode_reward': (Array(-14440.264, dtype=float32), Array(4872.4644, dtype=float32)), 'eval/avg_episode_length': (Array(922.375, dtype=float32), Array(266.6513, dtype=float32)), 'eval/epoch_eval_time': 4.114777326583862, 'eval/sps': 31107.394116577176}
I0726 23:03:27.382199 140267183036224 train.py:379] starting iteration 274 1999.637045621872
I0726 23:03:34.557854 140267183036224 train.py:394] {'eval/walltime': 1140.4371373653412, 'training/sps': 40126.61511205992, 'training/walltime': 859.4507019519806, 'training/entropy_loss': Array(-0.02348002, dtype=float32), 'training/policy_loss': Array(-0.00021672, dtype=float32), 'training/total_loss': Array(93.561485, dtype=float32), 'training/v_loss': Array(93.585175, dtype=float32), 'eval/episode_goal_distance': (Array(0.29140332, dtype=float32), Array(0.05656529, dtype=float32)), 'eval/episode_reward': (Array(-13771.662, dtype=float32), Array(5107.6274, dtype=float32)), 'eval/avg_episode_length': (Array(906.75, dtype=float32), Array(289.92636, dtype=float32)), 'eval/epoch_eval_time': 4.109720706939697, 'eval/sps': 31145.668800280877}
I0726 23:03:34.560385 140267183036224 train.py:379] starting iteration 275 2006.8152315616608
I0726 23:03:41.741437 140267183036224 train.py:394] {'eval/walltime': 1144.5550556182861, 'training/sps': 40162.7214729124, 'training/walltime': 862.51025557518, 'training/entropy_loss': Array(-0.0222713, dtype=float32), 'training/policy_loss': Array(-0.00036232, dtype=float32), 'training/total_loss': Array(85.89598, dtype=float32), 'training/v_loss': Array(85.91861, dtype=float32), 'eval/episode_goal_distance': (Array(0.28080112, dtype=float32), Array(0.06579161, dtype=float32)), 'eval/episode_reward': (Array(-13698.731, dtype=float32), Array(4933.248, dtype=float32)), 'eval/avg_episode_length': (Array(914.4922, dtype=float32), Array(278.87067, dtype=float32)), 'eval/epoch_eval_time': 4.117918252944946, 'eval/sps': 31083.667070967294}
I0726 23:03:41.743808 140267183036224 train.py:379] starting iteration 276 2013.9986550807953
I0726 23:03:48.922919 140267183036224 train.py:394] {'eval/walltime': 1148.6694366931915, 'training/sps': 40138.99346151534, 'training/walltime': 865.5716178417206, 'training/entropy_loss': Array(-0.02214632, dtype=float32), 'training/policy_loss': Array(-0.00013842, dtype=float32), 'training/total_loss': Array(90.93672, dtype=float32), 'training/v_loss': Array(90.959, dtype=float32), 'eval/episode_goal_distance': (Array(0.2906318, dtype=float32), Array(0.05674082, dtype=float32)), 'eval/episode_reward': (Array(-14123.532, dtype=float32), Array(4405.5205, dtype=float32)), 'eval/avg_episode_length': (Array(930.1172, dtype=float32), Array(254.11115, dtype=float32)), 'eval/epoch_eval_time': 4.1143810749053955, 'eval/sps': 31110.39003671851}
I0726 23:03:48.925210 140267183036224 train.py:379] starting iteration 277 2021.180056810379
I0726 23:03:56.110406 140267183036224 train.py:394] {'eval/walltime': 1152.7925095558167, 'training/sps': 40176.28398815562, 'training/walltime': 868.6301386356354, 'training/entropy_loss': Array(-0.0216135, dtype=float32), 'training/policy_loss': Array(0.0002469, dtype=float32), 'training/total_loss': Array(102.01066, dtype=float32), 'training/v_loss': Array(102.03203, dtype=float32), 'eval/episode_goal_distance': (Array(0.29218733, dtype=float32), Array(0.05487154, dtype=float32)), 'eval/episode_reward': (Array(-14612.174, dtype=float32), Array(4669.8706, dtype=float32)), 'eval/avg_episode_length': (Array(930.0547, dtype=float32), Array(254.338, dtype=float32)), 'eval/epoch_eval_time': 4.123072862625122, 'eval/sps': 31044.80669267232}
I0726 23:03:56.114941 140267183036224 train.py:379] starting iteration 278 2028.3697752952576
I0726 23:04:03.296153 140267183036224 train.py:394] {'eval/walltime': 1156.915556192398, 'training/sps': 40232.53202956247, 'training/walltime': 871.684383392334, 'training/entropy_loss': Array(-0.02172272, dtype=float32), 'training/policy_loss': Array(0.00038907, dtype=float32), 'training/total_loss': Array(93.12333, dtype=float32), 'training/v_loss': Array(93.14467, dtype=float32), 'eval/episode_goal_distance': (Array(0.29263842, dtype=float32), Array(0.05191309, dtype=float32)), 'eval/episode_reward': (Array(-14580.984, dtype=float32), Array(4334.747, dtype=float32)), 'eval/avg_episode_length': (Array(945.6875, dtype=float32), Array(225.81049, dtype=float32)), 'eval/epoch_eval_time': 4.123046636581421, 'eval/sps': 31045.004163748632}
I0726 23:04:03.298662 140267183036224 train.py:379] starting iteration 279 2035.5535082817078
I0726 23:04:10.476674 140267183036224 train.py:394] {'eval/walltime': 1161.0398755073547, 'training/sps': 40288.00558314401, 'training/walltime': 874.7344226837158, 'training/entropy_loss': Array(-0.02225155, dtype=float32), 'training/policy_loss': Array(-0.00048293, dtype=float32), 'training/total_loss': Array(102.24586, dtype=float32), 'training/v_loss': Array(102.268585, dtype=float32), 'eval/episode_goal_distance': (Array(0.2911653, dtype=float32), Array(0.05157552, dtype=float32)), 'eval/episode_reward': (Array(-13687.525, dtype=float32), Array(5530.9443, dtype=float32)), 'eval/avg_episode_length': (Array(883.53125, dtype=float32), Array(319.67145, dtype=float32)), 'eval/epoch_eval_time': 4.124319314956665, 'eval/sps': 31035.42432706739}
I0726 23:04:10.479200 140267183036224 train.py:379] starting iteration 280 2042.7340459823608
I0726 23:04:17.657415 140267183036224 train.py:394] {'eval/walltime': 1165.162609577179, 'training/sps': 40262.81455621978, 'training/walltime': 877.7863702774048, 'training/entropy_loss': Array(-0.02066844, dtype=float32), 'training/policy_loss': Array(-0.0003504, dtype=float32), 'training/total_loss': Array(91.75957, dtype=float32), 'training/v_loss': Array(91.78058, dtype=float32), 'eval/episode_goal_distance': (Array(0.28960043, dtype=float32), Array(0.06333768, dtype=float32)), 'eval/episode_reward': (Array(-14208.182, dtype=float32), Array(4869.7026, dtype=float32)), 'eval/avg_episode_length': (Array(930.14844, dtype=float32), Array(253.99751, dtype=float32)), 'eval/epoch_eval_time': 4.122734069824219, 'eval/sps': 31047.357853342586}
I0726 23:04:17.659840 140267183036224 train.py:379] starting iteration 281 2049.9146864414215
I0726 23:04:24.839273 140267183036224 train.py:394] {'eval/walltime': 1169.2876467704773, 'training/sps': 40278.893645874086, 'training/walltime': 880.8370995521545, 'training/entropy_loss': Array(-0.02143358, dtype=float32), 'training/policy_loss': Array(0.00032294, dtype=float32), 'training/total_loss': Array(83.66432, dtype=float32), 'training/v_loss': Array(83.685425, dtype=float32), 'eval/episode_goal_distance': (Array(0.28273517, dtype=float32), Array(0.06235073, dtype=float32)), 'eval/episode_reward': (Array(-13425.676, dtype=float32), Array(5164.993, dtype=float32)), 'eval/avg_episode_length': (Array(899.0703, dtype=float32), Array(300.1904, dtype=float32)), 'eval/epoch_eval_time': 4.12503719329834, 'eval/sps': 31030.02324632434}
I0726 23:04:24.841743 140267183036224 train.py:379] starting iteration 282 2057.0965898036957
I0726 23:04:32.025546 140267183036224 train.py:394] {'eval/walltime': 1173.4158935546875, 'training/sps': 40261.75460562749, 'training/walltime': 883.8891274929047, 'training/entropy_loss': Array(-0.02316116, dtype=float32), 'training/policy_loss': Array(0.00011989, dtype=float32), 'training/total_loss': Array(85.70711, dtype=float32), 'training/v_loss': Array(85.73015, dtype=float32), 'eval/episode_goal_distance': (Array(0.29884684, dtype=float32), Array(0.06060283, dtype=float32)), 'eval/episode_reward': (Array(-13604.093, dtype=float32), Array(5522.4175, dtype=float32)), 'eval/avg_episode_length': (Array(883.4531, dtype=float32), Array(319.88593, dtype=float32)), 'eval/epoch_eval_time': 4.128246784210205, 'eval/sps': 31005.898312469297}
I0726 23:04:32.028057 140267183036224 train.py:379] starting iteration 283 2064.282903432846
I0726 23:04:39.205063 140267183036224 train.py:394] {'eval/walltime': 1177.5407979488373, 'training/sps': 40309.4793241064, 'training/walltime': 886.9375419616699, 'training/entropy_loss': Array(-0.02333779, dtype=float32), 'training/policy_loss': Array(0.00022458, dtype=float32), 'training/total_loss': Array(445.80463, dtype=float32), 'training/v_loss': Array(445.82773, dtype=float32), 'eval/episode_goal_distance': (Array(0.28359857, dtype=float32), Array(0.06327448, dtype=float32)), 'eval/episode_reward': (Array(-13938.638, dtype=float32), Array(4499.245, dtype=float32)), 'eval/avg_episode_length': (Array(937.8672, dtype=float32), Array(240.63991, dtype=float32)), 'eval/epoch_eval_time': 4.12490439414978, 'eval/sps': 31031.02224176112}
I0726 23:04:39.207494 140267183036224 train.py:379] starting iteration 284 2071.462341070175
I0726 23:04:46.382894 140267183036224 train.py:394] {'eval/walltime': 1181.6659882068634, 'training/sps': 40333.00221934934, 'training/walltime': 889.9841785430908, 'training/entropy_loss': Array(-0.02325517, dtype=float32), 'training/policy_loss': Array(0.00013684, dtype=float32), 'training/total_loss': Array(213.20587, dtype=float32), 'training/v_loss': Array(213.22897, dtype=float32), 'eval/episode_goal_distance': (Array(0.29655993, dtype=float32), Array(0.06686065, dtype=float32)), 'eval/episode_reward': (Array(-14091.412, dtype=float32), Array(5139.163, dtype=float32)), 'eval/avg_episode_length': (Array(906.83594, dtype=float32), Array(289.65933, dtype=float32)), 'eval/epoch_eval_time': 4.125190258026123, 'eval/sps': 31028.87188074743}
I0726 23:04:46.385302 140267183036224 train.py:379] starting iteration 285 2078.640148162842
I0726 23:04:53.560261 140267183036224 train.py:394] {'eval/walltime': 1185.794261455536, 'training/sps': 40377.80212407345, 'training/walltime': 893.0274348258972, 'training/entropy_loss': Array(-0.02348233, dtype=float32), 'training/policy_loss': Array(0.00042472, dtype=float32), 'training/total_loss': Array(137.88293, dtype=float32), 'training/v_loss': Array(137.90599, dtype=float32), 'eval/episode_goal_distance': (Array(0.2877295, dtype=float32), Array(0.0595644, dtype=float32)), 'eval/episode_reward': (Array(-14224.598, dtype=float32), Array(4655.536, dtype=float32)), 'eval/avg_episode_length': (Array(930.1094, dtype=float32), Array(254.13928, dtype=float32)), 'eval/epoch_eval_time': 4.128273248672485, 'eval/sps': 31005.69954790675}
I0726 23:04:53.562596 140267183036224 train.py:379] starting iteration 286 2085.817442893982
I0726 23:05:00.720265 140267183036224 train.py:394] {'eval/walltime': 1189.9051756858826, 'training/sps': 40378.906155777426, 'training/walltime': 896.0706079006195, 'training/entropy_loss': Array(-0.02263821, dtype=float32), 'training/policy_loss': Array(-8.298759e-05, dtype=float32), 'training/total_loss': Array(185.05756, dtype=float32), 'training/v_loss': Array(185.08026, dtype=float32), 'eval/episode_goal_distance': (Array(0.29517785, dtype=float32), Array(0.05487231, dtype=float32)), 'eval/episode_reward': (Array(-14521.987, dtype=float32), Array(4225.1724, dtype=float32)), 'eval/avg_episode_length': (Array(945.5469, dtype=float32), Array(226.39487, dtype=float32)), 'eval/epoch_eval_time': 4.11091423034668, 'eval/sps': 31136.62626554132}
I0726 23:05:00.722776 140267183036224 train.py:379] starting iteration 287 2092.977622270584
I0726 23:05:07.902715 140267183036224 train.py:394] {'eval/walltime': 1194.0346286296844, 'training/sps': 40329.96923114483, 'training/walltime': 899.1174736022949, 'training/entropy_loss': Array(-0.02225233, dtype=float32), 'training/policy_loss': Array(1.5135269e-05, dtype=float32), 'training/total_loss': Array(111.38196, dtype=float32), 'training/v_loss': Array(111.4042, dtype=float32), 'eval/episode_goal_distance': (Array(0.2875606, dtype=float32), Array(0.06562448, dtype=float32)), 'eval/episode_reward': (Array(-14543.857, dtype=float32), Array(4461.8096, dtype=float32)), 'eval/avg_episode_length': (Array(945.64844, dtype=float32), Array(225.97287, dtype=float32)), 'eval/epoch_eval_time': 4.12945294380188, 'eval/sps': 30996.84189212572}
I0726 23:05:07.905249 140267183036224 train.py:379] starting iteration 288 2100.1600954532623
I0726 23:05:15.072004 140267183036224 train.py:394] {'eval/walltime': 1198.1499133110046, 'training/sps': 40316.8073726657, 'training/walltime': 902.1653339862823, 'training/entropy_loss': Array(-0.02210852, dtype=float32), 'training/policy_loss': Array(-7.4833195e-05, dtype=float32), 'training/total_loss': Array(113.2413, dtype=float32), 'training/v_loss': Array(113.26349, dtype=float32), 'eval/episode_goal_distance': (Array(0.2913832, dtype=float32), Array(0.06259342, dtype=float32)), 'eval/episode_reward': (Array(-14149.742, dtype=float32), Array(4715.2754, dtype=float32)), 'eval/avg_episode_length': (Array(930.1094, dtype=float32), Array(254.13939, dtype=float32)), 'eval/epoch_eval_time': 4.11528468132019, 'eval/sps': 31103.5590274006}
I0726 23:05:15.074321 140267183036224 train.py:379] starting iteration 289 2107.3291676044464
I0726 23:05:22.242951 140267183036224 train.py:394] {'eval/walltime': 1202.2716619968414, 'training/sps': 40376.79304761795, 'training/walltime': 905.2086663246155, 'training/entropy_loss': Array(-0.02216161, dtype=float32), 'training/policy_loss': Array(0.00045185, dtype=float32), 'training/total_loss': Array(121.0112, dtype=float32), 'training/v_loss': Array(121.03291, dtype=float32), 'eval/episode_goal_distance': (Array(0.29289198, dtype=float32), Array(0.06145623, dtype=float32)), 'eval/episode_reward': (Array(-14120.832, dtype=float32), Array(5148.09, dtype=float32)), 'eval/avg_episode_length': (Array(906.77344, dtype=float32), Array(289.8536, dtype=float32)), 'eval/epoch_eval_time': 4.121748685836792, 'eval/sps': 31054.78032657239}
I0726 23:05:22.245516 140267183036224 train.py:379] starting iteration 290 2114.500362634659
I0726 23:05:29.404521 140267183036224 train.py:394] {'eval/walltime': 1206.3794887065887, 'training/sps': 40318.94890187526, 'training/walltime': 908.2563648223877, 'training/entropy_loss': Array(-0.02337457, dtype=float32), 'training/policy_loss': Array(-0.00020919, dtype=float32), 'training/total_loss': Array(117.43892, dtype=float32), 'training/v_loss': Array(117.462494, dtype=float32), 'eval/episode_goal_distance': (Array(0.28861815, dtype=float32), Array(0.06191809, dtype=float32)), 'eval/episode_reward': (Array(-13492.905, dtype=float32), Array(5034.444, dtype=float32)), 'eval/avg_episode_length': (Array(906.78125, dtype=float32), Array(289.82935, dtype=float32)), 'eval/epoch_eval_time': 4.1078267097473145, 'eval/sps': 31160.029145405137}
I0726 23:05:29.407092 140267183036224 train.py:379] starting iteration 291 2121.661938428879
I0726 23:05:36.577092 140267183036224 train.py:394] {'eval/walltime': 1210.4994497299194, 'training/sps': 40334.22374927542, 'training/walltime': 911.3029091358185, 'training/entropy_loss': Array(-0.02232282, dtype=float32), 'training/policy_loss': Array(-0.00053794, dtype=float32), 'training/total_loss': Array(97.43477, dtype=float32), 'training/v_loss': Array(97.45763, dtype=float32), 'eval/episode_goal_distance': (Array(0.29353273, dtype=float32), Array(0.06189952, dtype=float32)), 'eval/episode_reward': (Array(-13993.373, dtype=float32), Array(5167.9956, dtype=float32)), 'eval/avg_episode_length': (Array(914.5469, dtype=float32), Array(278.6926, dtype=float32)), 'eval/epoch_eval_time': 4.1199610233306885, 'eval/sps': 31068.25508182146}
I0726 23:05:36.579548 140267183036224 train.py:379] starting iteration 292 2128.8343946933746
I0726 23:05:43.739636 140267183036224 train.py:394] {'eval/walltime': 1214.6075611114502, 'training/sps': 40307.934593660626, 'training/walltime': 914.3514404296875, 'training/entropy_loss': Array(-0.02352452, dtype=float32), 'training/policy_loss': Array(0.0002098, dtype=float32), 'training/total_loss': Array(99.734116, dtype=float32), 'training/v_loss': Array(99.75745, dtype=float32), 'eval/episode_goal_distance': (Array(0.2946248, dtype=float32), Array(0.0540559, dtype=float32)), 'eval/episode_reward': (Array(-14445.391, dtype=float32), Array(4703.4272, dtype=float32)), 'eval/avg_episode_length': (Array(930.03906, dtype=float32), Array(254.39476, dtype=float32)), 'eval/epoch_eval_time': 4.108111381530762, 'eval/sps': 31157.869909628575}
I0726 23:05:43.741945 140267183036224 train.py:379] starting iteration 293 2135.9967923164368
I0726 23:05:50.910990 140267183036224 train.py:394] {'eval/walltime': 1218.7259287834167, 'training/sps': 40326.267773151514, 'training/walltime': 917.3985857963562, 'training/entropy_loss': Array(-0.02352942, dtype=float32), 'training/policy_loss': Array(-8.267042e-05, dtype=float32), 'training/total_loss': Array(93.9254, dtype=float32), 'training/v_loss': Array(93.949005, dtype=float32), 'eval/episode_goal_distance': (Array(0.29517257, dtype=float32), Array(0.06711334, dtype=float32)), 'eval/episode_reward': (Array(-13015.915, dtype=float32), Array(6005.1123, dtype=float32)), 'eval/avg_episode_length': (Array(852.3594, dtype=float32), Array(353.62488, dtype=float32)), 'eval/epoch_eval_time': 4.118367671966553, 'eval/sps': 31080.275049575404}
I0726 23:05:50.913705 140267183036224 train.py:379] starting iteration 294 2143.1685512065887
I0726 23:05:58.070443 140267183036224 train.py:394] {'eval/walltime': 1222.8311531543732, 'training/sps': 40316.85467939565, 'training/walltime': 920.4464426040649, 'training/entropy_loss': Array(-0.02275245, dtype=float32), 'training/policy_loss': Array(0.00043293, dtype=float32), 'training/total_loss': Array(94.98324, dtype=float32), 'training/v_loss': Array(95.005554, dtype=float32), 'eval/episode_goal_distance': (Array(0.2905891, dtype=float32), Array(0.05435346, dtype=float32)), 'eval/episode_reward': (Array(-14455.488, dtype=float32), Array(4713.521, dtype=float32)), 'eval/avg_episode_length': (Array(930.1172, dtype=float32), Array(254.11096, dtype=float32)), 'eval/epoch_eval_time': 4.105224370956421, 'eval/sps': 31179.781769194506}
I0726 23:05:58.072718 140267183036224 train.py:379] starting iteration 295 2150.327563762665
I0726 23:06:05.249551 140267183036224 train.py:394] {'eval/walltime': 1226.9596045017242, 'training/sps': 40356.86542691687, 'training/walltime': 923.4912776947021, 'training/entropy_loss': Array(-0.02366479, dtype=float32), 'training/policy_loss': Array(0.00046507, dtype=float32), 'training/total_loss': Array(81.72621, dtype=float32), 'training/v_loss': Array(81.749405, dtype=float32), 'eval/episode_goal_distance': (Array(0.29695833, dtype=float32), Array(0.05978772, dtype=float32)), 'eval/episode_reward': (Array(-13719.472, dtype=float32), Array(5412.115, dtype=float32)), 'eval/avg_episode_length': (Array(891.3594, dtype=float32), Array(310.01422, dtype=float32)), 'eval/epoch_eval_time': 4.128451347351074, 'eval/sps': 31004.36198240007}
I0726 23:06:05.251980 140267183036224 train.py:379] starting iteration 296 2157.5068259239197
I0726 23:06:12.411300 140267183036224 train.py:394] {'eval/walltime': 1231.0670869350433, 'training/sps': 40311.28271024657, 'training/walltime': 926.5395557880402, 'training/entropy_loss': Array(-0.02426163, dtype=float32), 'training/policy_loss': Array(-0.00031532, dtype=float32), 'training/total_loss': Array(81.92719, dtype=float32), 'training/v_loss': Array(81.95177, dtype=float32), 'eval/episode_goal_distance': (Array(0.29437417, dtype=float32), Array(0.05944395, dtype=float32)), 'eval/episode_reward': (Array(-14318.459, dtype=float32), Array(4823.9204, dtype=float32)), 'eval/avg_episode_length': (Array(922.2578, dtype=float32), Array(267.0533, dtype=float32)), 'eval/epoch_eval_time': 4.107482433319092, 'eval/sps': 31162.640882329553}
I0726 23:06:12.413496 140267183036224 train.py:379] starting iteration 297 2164.668343067169
I0726 23:06:19.585251 140267183036224 train.py:394] {'eval/walltime': 1235.189135313034, 'training/sps': 40338.26447874136, 'training/walltime': 929.5857949256897, 'training/entropy_loss': Array(-0.02339683, dtype=float32), 'training/policy_loss': Array(-0.00028821, dtype=float32), 'training/total_loss': Array(114.66069, dtype=float32), 'training/v_loss': Array(114.68437, dtype=float32), 'eval/episode_goal_distance': (Array(0.29505548, dtype=float32), Array(0.06010968, dtype=float32)), 'eval/episode_reward': (Array(-14121.061, dtype=float32), Array(4801.8887, dtype=float32)), 'eval/avg_episode_length': (Array(922.28125, dtype=float32), Array(266.97314, dtype=float32)), 'eval/epoch_eval_time': 4.122048377990723, 'eval/sps': 31052.52249911563}
I0726 23:06:19.587704 140267183036224 train.py:379] starting iteration 298 2171.842549800873
I0726 23:06:26.744150 140267183036224 train.py:394] {'eval/walltime': 1239.2967252731323, 'training/sps': 40350.31886859162, 'training/walltime': 932.6311240196228, 'training/entropy_loss': Array(-0.02357789, dtype=float32), 'training/policy_loss': Array(-0.00018456, dtype=float32), 'training/total_loss': Array(86.042145, dtype=float32), 'training/v_loss': Array(86.06592, dtype=float32), 'eval/episode_goal_distance': (Array(0.29804736, dtype=float32), Array(0.06535177, dtype=float32)), 'eval/episode_reward': (Array(-13974.389, dtype=float32), Array(5510.321, dtype=float32)), 'eval/avg_episode_length': (Array(891.2422, dtype=float32), Array(310.34818, dtype=float32)), 'eval/epoch_eval_time': 4.107589960098267, 'eval/sps': 31161.825119695695}
I0726 23:06:26.746484 140267183036224 train.py:379] starting iteration 299 2179.001330137253
I0726 23:06:33.912770 140267183036224 train.py:394] {'eval/walltime': 1243.4103236198425, 'training/sps': 40298.92077354813, 'training/walltime': 935.680337190628, 'training/entropy_loss': Array(-0.02085844, dtype=float32), 'training/policy_loss': Array(0.0001914, dtype=float32), 'training/total_loss': Array(91.8911, dtype=float32), 'training/v_loss': Array(91.911766, dtype=float32), 'eval/episode_goal_distance': (Array(0.28801087, dtype=float32), Array(0.06892762, dtype=float32)), 'eval/episode_reward': (Array(-13400.462, dtype=float32), Array(5367.332, dtype=float32)), 'eval/avg_episode_length': (Array(891.28125, dtype=float32), Array(310.23706, dtype=float32)), 'eval/epoch_eval_time': 4.113598346710205, 'eval/sps': 31116.309666539582}
I0726 23:06:33.915151 140267183036224 train.py:379] starting iteration 300 2186.169997692108
I0726 23:06:41.074655 140267183036224 train.py:394] {'eval/walltime': 1247.5167615413666, 'training/sps': 40291.523629235424, 'training/walltime': 938.730110168457, 'training/entropy_loss': Array(-0.02186675, dtype=float32), 'training/policy_loss': Array(0.00053733, dtype=float32), 'training/total_loss': Array(508.86743, dtype=float32), 'training/v_loss': Array(508.88876, dtype=float32), 'eval/episode_goal_distance': (Array(0.28404778, dtype=float32), Array(0.05991784, dtype=float32)), 'eval/episode_reward': (Array(-13776.456, dtype=float32), Array(4515.0396, dtype=float32)), 'eval/avg_episode_length': (Array(930.1406, dtype=float32), Array(254.02563, dtype=float32)), 'eval/epoch_eval_time': 4.106437921524048, 'eval/sps': 31170.56739834863}
I0726 23:06:41.076934 140267183036224 train.py:379] starting iteration 301 2193.331779718399
I0726 23:06:48.263992 140267183036224 train.py:394] {'eval/walltime': 1251.6530983448029, 'training/sps': 40324.97100359008, 'training/walltime': 941.7773535251617, 'training/entropy_loss': Array(-0.02236614, dtype=float32), 'training/policy_loss': Array(6.955127e-05, dtype=float32), 'training/total_loss': Array(134.99365, dtype=float32), 'training/v_loss': Array(135.01593, dtype=float32), 'eval/episode_goal_distance': (Array(0.28539515, dtype=float32), Array(0.06297746, dtype=float32)), 'eval/episode_reward': (Array(-13999.043, dtype=float32), Array(4754.98, dtype=float32)), 'eval/avg_episode_length': (Array(930.0469, dtype=float32), Array(254.36627, dtype=float32)), 'eval/epoch_eval_time': 4.136336803436279, 'eval/sps': 30945.25568944566}
I0726 23:06:48.266173 140267183036224 train.py:379] starting iteration 302 2200.5210196971893
I0726 23:06:55.427480 140267183036224 train.py:394] {'eval/walltime': 1255.764196395874, 'training/sps': 40330.55622551948, 'training/walltime': 944.8241748809814, 'training/entropy_loss': Array(-0.02357936, dtype=float32), 'training/policy_loss': Array(0.00024996, dtype=float32), 'training/total_loss': Array(134.82008, dtype=float32), 'training/v_loss': Array(134.8434, dtype=float32), 'eval/episode_goal_distance': (Array(0.29158843, dtype=float32), Array(0.06056609, dtype=float32)), 'eval/episode_reward': (Array(-14530.464, dtype=float32), Array(4655.4644, dtype=float32)), 'eval/avg_episode_length': (Array(937.83594, dtype=float32), Array(240.7605, dtype=float32)), 'eval/epoch_eval_time': 4.111098051071167, 'eval/sps': 31135.234044502773}
I0726 23:06:55.429625 140267183036224 train.py:379] starting iteration 303 2207.684471130371
I0726 23:07:02.590838 140267183036224 train.py:394] {'eval/walltime': 1259.8754813671112, 'training/sps': 40335.48007686809, 'training/walltime': 947.8706243038177, 'training/entropy_loss': Array(-0.02449168, dtype=float32), 'training/policy_loss': Array(1.4599558e-05, dtype=float32), 'training/total_loss': Array(170.41176, dtype=float32), 'training/v_loss': Array(170.43625, dtype=float32), 'eval/episode_goal_distance': (Array(0.29269063, dtype=float32), Array(0.06054024, dtype=float32)), 'eval/episode_reward': (Array(-13968.57, dtype=float32), Array(5089.365, dtype=float32)), 'eval/avg_episode_length': (Array(906.75, dtype=float32), Array(289.92648, dtype=float32)), 'eval/epoch_eval_time': 4.111284971237183, 'eval/sps': 31133.818476582463}
I0726 23:07:02.592979 140267183036224 train.py:379] starting iteration 304 2214.8478257656097
I0726 23:07:09.757053 140267183036224 train.py:394] {'eval/walltime': 1263.9877722263336, 'training/sps': 40305.93607730165, 'training/walltime': 950.9193067550659, 'training/entropy_loss': Array(-0.02522637, dtype=float32), 'training/policy_loss': Array(-0.00016749, dtype=float32), 'training/total_loss': Array(128.55353, dtype=float32), 'training/v_loss': Array(128.57892, dtype=float32), 'eval/episode_goal_distance': (Array(0.29276142, dtype=float32), Array(0.05895616, dtype=float32)), 'eval/episode_reward': (Array(-14332.92, dtype=float32), Array(4648.9062, dtype=float32)), 'eval/avg_episode_length': (Array(930.16406, dtype=float32), Array(253.94022, dtype=float32)), 'eval/epoch_eval_time': 4.112290859222412, 'eval/sps': 31126.202980740363}
I0726 23:07:09.759414 140267183036224 train.py:379] starting iteration 305 2222.0142600536346
I0726 23:07:16.924194 140267183036224 train.py:394] {'eval/walltime': 1268.1033296585083, 'training/sps': 40347.08428800241, 'training/walltime': 953.964879989624, 'training/entropy_loss': Array(-0.02457151, dtype=float32), 'training/policy_loss': Array(-0.00012262, dtype=float32), 'training/total_loss': Array(112.41514, dtype=float32), 'training/v_loss': Array(112.43982, dtype=float32), 'eval/episode_goal_distance': (Array(0.29402828, dtype=float32), Array(0.05788499, dtype=float32)), 'eval/episode_reward': (Array(-13667.958, dtype=float32), Array(5554.382, dtype=float32)), 'eval/avg_episode_length': (Array(883.4844, dtype=float32), Array(319.79987, dtype=float32)), 'eval/epoch_eval_time': 4.115557432174683, 'eval/sps': 31101.497697327508}
I0726 23:07:16.926624 140267183036224 train.py:379] starting iteration 306 2229.181470155716
I0726 23:07:24.088785 140267183036224 train.py:394] {'eval/walltime': 1272.2111597061157, 'training/sps': 40276.967254123025, 'training/walltime': 957.0157551765442, 'training/entropy_loss': Array(-0.02322928, dtype=float32), 'training/policy_loss': Array(6.992996e-05, dtype=float32), 'training/total_loss': Array(96.65239, dtype=float32), 'training/v_loss': Array(96.67554, dtype=float32), 'eval/episode_goal_distance': (Array(0.29367763, dtype=float32), Array(0.06702153, dtype=float32)), 'eval/episode_reward': (Array(-14151.823, dtype=float32), Array(4989.3203, dtype=float32)), 'eval/avg_episode_length': (Array(922.3125, dtype=float32), Array(266.86594, dtype=float32)), 'eval/epoch_eval_time': 4.107830047607422, 'eval/sps': 31160.003825998778}
I0726 23:07:24.261315 140267183036224 train.py:379] starting iteration 307 2236.5161430835724
I0726 23:07:31.452213 140267183036224 train.py:394] {'eval/walltime': 1276.3389556407928, 'training/sps': 40168.956838384416, 'training/walltime': 960.0748338699341, 'training/entropy_loss': Array(-0.02347438, dtype=float32), 'training/policy_loss': Array(8.327274e-05, dtype=float32), 'training/total_loss': Array(103.898834, dtype=float32), 'training/v_loss': Array(103.922226, dtype=float32), 'eval/episode_goal_distance': (Array(0.2961839, dtype=float32), Array(0.05720927, dtype=float32)), 'eval/episode_reward': (Array(-14107.184, dtype=float32), Array(5112.668, dtype=float32)), 'eval/avg_episode_length': (Array(906.7578, dtype=float32), Array(289.90216, dtype=float32)), 'eval/epoch_eval_time': 4.127795934677124, 'eval/sps': 31009.28486427519}
I0726 23:07:31.454677 140267183036224 train.py:379] starting iteration 308 2243.709522485733
I0726 23:07:38.627079 140267183036224 train.py:394] {'eval/walltime': 1280.4617099761963, 'training/sps': 40341.150307621225, 'training/walltime': 963.1208550930023, 'training/entropy_loss': Array(-0.02396375, dtype=float32), 'training/policy_loss': Array(9.965243e-05, dtype=float32), 'training/total_loss': Array(106.09378, dtype=float32), 'training/v_loss': Array(106.11764, dtype=float32), 'eval/episode_goal_distance': (Array(0.28820497, dtype=float32), Array(0.0635609, dtype=float32)), 'eval/episode_reward': (Array(-13569.249, dtype=float32), Array(5620.937, dtype=float32)), 'eval/avg_episode_length': (Array(883.47656, dtype=float32), Array(319.82126, dtype=float32)), 'eval/epoch_eval_time': 4.122754335403442, 'eval/sps': 31047.20523869736}
I0726 23:07:38.629511 140267183036224 train.py:379] starting iteration 309 2250.884357690811
I0726 23:07:45.801600 140267183036224 train.py:394] {'eval/walltime': 1284.5817725658417, 'training/sps': 40311.82817175316, 'training/walltime': 966.1690919399261, 'training/entropy_loss': Array(-0.02323592, dtype=float32), 'training/policy_loss': Array(-0.00031353, dtype=float32), 'training/total_loss': Array(84.51984, dtype=float32), 'training/v_loss': Array(84.54338, dtype=float32), 'eval/episode_goal_distance': (Array(0.288782, dtype=float32), Array(0.06055933, dtype=float32)), 'eval/episode_reward': (Array(-13715.871, dtype=float32), Array(5128.2705, dtype=float32)), 'eval/avg_episode_length': (Array(906.8906, dtype=float32), Array(289.48956, dtype=float32)), 'eval/epoch_eval_time': 4.120062589645386, 'eval/sps': 31067.489198268944}
I0726 23:07:45.804344 140267183036224 train.py:379] starting iteration 310 2258.0591909885406
I0726 23:07:52.967489 140267183036224 train.py:394] {'eval/walltime': 1288.6908445358276, 'training/sps': 40287.09231442431, 'training/walltime': 969.2192003726959, 'training/entropy_loss': Array(-0.02356471, dtype=float32), 'training/policy_loss': Array(-4.4034e-05, dtype=float32), 'training/total_loss': Array(91.90806, dtype=float32), 'training/v_loss': Array(91.93166, dtype=float32), 'eval/episode_goal_distance': (Array(0.2796471, dtype=float32), Array(0.06070862, dtype=float32)), 'eval/episode_reward': (Array(-13508.972, dtype=float32), Array(4976.957, dtype=float32)), 'eval/avg_episode_length': (Array(906.85156, dtype=float32), Array(289.61084, dtype=float32)), 'eval/epoch_eval_time': 4.109071969985962, 'eval/sps': 31150.586053239}
I0726 23:07:52.969874 140267183036224 train.py:379] starting iteration 311 2265.224720478058
I0726 23:08:00.132266 140267183036224 train.py:394] {'eval/walltime': 1292.8010714054108, 'training/sps': 40313.37004740729, 'training/walltime': 972.2673206329346, 'training/entropy_loss': Array(-0.0228586, dtype=float32), 'training/policy_loss': Array(-0.00036077, dtype=float32), 'training/total_loss': Array(101.08057, dtype=float32), 'training/v_loss': Array(101.10379, dtype=float32), 'eval/episode_goal_distance': (Array(0.29310453, dtype=float32), Array(0.05902622, dtype=float32)), 'eval/episode_reward': (Array(-13935.937, dtype=float32), Array(5444.5894, dtype=float32)), 'eval/avg_episode_length': (Array(899.0469, dtype=float32), Array(300.26013, dtype=float32)), 'eval/epoch_eval_time': 4.11022686958313, 'eval/sps': 31141.83330055017}
I0726 23:08:00.134732 140267183036224 train.py:379] starting iteration 312 2272.3895783424377
I0726 23:08:07.303257 140267183036224 train.py:394] {'eval/walltime': 1296.9148962497711, 'training/sps': 40276.82561494327, 'training/walltime': 975.3182065486908, 'training/entropy_loss': Array(-0.02148726, dtype=float32), 'training/policy_loss': Array(7.777967e-05, dtype=float32), 'training/total_loss': Array(94.0818, dtype=float32), 'training/v_loss': Array(94.10321, dtype=float32), 'eval/episode_goal_distance': (Array(0.29215688, dtype=float32), Array(0.05811264, dtype=float32)), 'eval/episode_reward': (Array(-13162.837, dtype=float32), Array(5561.6147, dtype=float32)), 'eval/avg_episode_length': (Array(868.09375, dtype=float32), Array(337.0573, dtype=float32)), 'eval/epoch_eval_time': 4.113824844360352, 'eval/sps': 31114.59647473212}
I0726 23:08:07.305594 140267183036224 train.py:379] starting iteration 313 2279.5604407787323
I0726 23:08:14.467272 140267183036224 train.py:394] {'eval/walltime': 1301.0246407985687, 'training/sps': 40315.47652215745, 'training/walltime': 978.3661675453186, 'training/entropy_loss': Array(-0.02174713, dtype=float32), 'training/policy_loss': Array(0.00028461, dtype=float32), 'training/total_loss': Array(98.70443, dtype=float32), 'training/v_loss': Array(98.72589, dtype=float32), 'eval/episode_goal_distance': (Array(0.29049134, dtype=float32), Array(0.05685161, dtype=float32)), 'eval/episode_reward': (Array(-14316.058, dtype=float32), Array(4790.126, dtype=float32)), 'eval/avg_episode_length': (Array(930.16406, dtype=float32), Array(253.94032, dtype=float32)), 'eval/epoch_eval_time': 4.109744548797607, 'eval/sps': 31145.48811493627}
I0726 23:08:14.469605 140267183036224 train.py:379] starting iteration 314 2286.724451780319
I0726 23:08:21.647480 140267183036224 train.py:394] {'eval/walltime': 1305.1427273750305, 'training/sps': 40227.69293595084, 'training/walltime': 981.4207797050476, 'training/entropy_loss': Array(-0.02211594, dtype=float32), 'training/policy_loss': Array(2.0381729e-05, dtype=float32), 'training/total_loss': Array(89.8632, dtype=float32), 'training/v_loss': Array(89.8853, dtype=float32), 'eval/episode_goal_distance': (Array(0.29343876, dtype=float32), Array(0.05869269, dtype=float32)), 'eval/episode_reward': (Array(-13692.84, dtype=float32), Array(5395.1113, dtype=float32)), 'eval/avg_episode_length': (Array(891.28125, dtype=float32), Array(310.2372, dtype=float32)), 'eval/epoch_eval_time': 4.118086576461792, 'eval/sps': 31082.396550772857}
I0726 23:08:21.649968 140267183036224 train.py:379] starting iteration 315 2293.904814004898
I0726 23:08:28.805522 140267183036224 train.py:394] {'eval/walltime': 1309.248782157898, 'training/sps': 40348.354051752445, 'training/walltime': 984.4662570953369, 'training/entropy_loss': Array(-0.02250879, dtype=float32), 'training/policy_loss': Array(6.767466e-05, dtype=float32), 'training/total_loss': Array(97.25008, dtype=float32), 'training/v_loss': Array(97.27252, dtype=float32), 'eval/episode_goal_distance': (Array(0.3057221, dtype=float32), Array(0.05733631, dtype=float32)), 'eval/episode_reward': (Array(-14638.654, dtype=float32), Array(4803.595, dtype=float32)), 'eval/avg_episode_length': (Array(922.28906, dtype=float32), Array(266.9462, dtype=float32)), 'eval/epoch_eval_time': 4.106054782867432, 'eval/sps': 31173.475944374077}
I0726 23:08:28.808127 140267183036224 train.py:379] starting iteration 316 2301.0629732608795
I0726 23:08:35.971204 140267183036224 train.py:394] {'eval/walltime': 1313.3586211204529, 'training/sps': 40298.92392452894, 'training/walltime': 987.5154700279236, 'training/entropy_loss': Array(-0.02401712, dtype=float32), 'training/policy_loss': Array(0.00037844, dtype=float32), 'training/total_loss': Array(443.814, dtype=float32), 'training/v_loss': Array(443.83762, dtype=float32), 'eval/episode_goal_distance': (Array(0.29519922, dtype=float32), Array(0.06098115, dtype=float32)), 'eval/episode_reward': (Array(-14649.406, dtype=float32), Array(4482.1885, dtype=float32)), 'eval/avg_episode_length': (Array(937.8125, dtype=float32), Array(240.85135, dtype=float32)), 'eval/epoch_eval_time': 4.109838962554932, 'eval/sps': 31144.772621559663}
I0726 23:08:35.973579 140267183036224 train.py:379] starting iteration 317 2308.228425502777
I0726 23:08:43.133356 140267183036224 train.py:394] {'eval/walltime': 1317.4661402702332, 'training/sps': 40312.20968828507, 'training/walltime': 990.5636780261993, 'training/entropy_loss': Array(-0.02419159, dtype=float32), 'training/policy_loss': Array(-4.216221e-05, dtype=float32), 'training/total_loss': Array(235.24344, dtype=float32), 'training/v_loss': Array(235.26768, dtype=float32), 'eval/episode_goal_distance': (Array(0.2897038, dtype=float32), Array(0.0588433, dtype=float32)), 'eval/episode_reward': (Array(-14801.222, dtype=float32), Array(4057.0537, dtype=float32)), 'eval/avg_episode_length': (Array(961.1328, dtype=float32), Array(192.77519, dtype=float32)), 'eval/epoch_eval_time': 4.107519149780273, 'eval/sps': 31162.362324433034}
I0726 23:08:43.135829 140267183036224 train.py:379] starting iteration 318 2315.3906762599945
I0726 23:08:50.331801 140267183036224 train.py:394] {'eval/walltime': 1321.6088376045227, 'training/sps': 40294.2641624198, 'training/walltime': 993.6132435798645, 'training/entropy_loss': Array(-0.02428476, dtype=float32), 'training/policy_loss': Array(-0.00051337, dtype=float32), 'training/total_loss': Array(168.60036, dtype=float32), 'training/v_loss': Array(168.62517, dtype=float32), 'eval/episode_goal_distance': (Array(0.29464853, dtype=float32), Array(0.06213835, dtype=float32)), 'eval/episode_reward': (Array(-14430.828, dtype=float32), Array(4465.2744, dtype=float32)), 'eval/avg_episode_length': (Array(937.8281, dtype=float32), Array(240.79088, dtype=float32)), 'eval/epoch_eval_time': 4.142697334289551, 'eval/sps': 30897.743588586658}
I0726 23:08:50.334339 140267183036224 train.py:379] starting iteration 319 2322.5891847610474
I0726 23:08:57.492953 140267183036224 train.py:394] {'eval/walltime': 1325.717512845993, 'training/sps': 40335.918862984865, 'training/walltime': 996.6596598625183, 'training/entropy_loss': Array(-0.02304643, dtype=float32), 'training/policy_loss': Array(0.00033835, dtype=float32), 'training/total_loss': Array(137.78058, dtype=float32), 'training/v_loss': Array(137.80328, dtype=float32), 'eval/episode_goal_distance': (Array(0.28765494, dtype=float32), Array(0.05337245, dtype=float32)), 'eval/episode_reward': (Array(-14520.565, dtype=float32), Array(4025.487, dtype=float32)), 'eval/avg_episode_length': (Array(953.41406, dtype=float32), Array(210.06812, dtype=float32)), 'eval/epoch_eval_time': 4.108675241470337, 'eval/sps': 31153.593914663776}
I0726 23:08:57.495486 140267183036224 train.py:379] starting iteration 320 2329.750331878662
I0726 23:09:04.653856 140267183036224 train.py:394] {'eval/walltime': 1329.8226823806763, 'training/sps': 40293.97749149082, 'training/walltime': 999.7092471122742, 'training/entropy_loss': Array(-0.02343669, dtype=float32), 'training/policy_loss': Array(0.00018881, dtype=float32), 'training/total_loss': Array(173.76874, dtype=float32), 'training/v_loss': Array(173.79196, dtype=float32), 'eval/episode_goal_distance': (Array(0.28928888, dtype=float32), Array(0.06216513, dtype=float32)), 'eval/episode_reward': (Array(-14593.475, dtype=float32), Array(4191.955, dtype=float32)), 'eval/avg_episode_length': (Array(945.60156, dtype=float32), Array(226.16768, dtype=float32)), 'eval/epoch_eval_time': 4.1051695346832275, 'eval/sps': 31180.198264303115}
I0726 23:09:04.656234 140267183036224 train.py:379] starting iteration 321 2336.9110808372498
I0726 23:09:11.817009 140267183036224 train.py:394] {'eval/walltime': 1333.9344394207, 'training/sps': 40347.82023586197, 'training/walltime': 1002.7547647953033, 'training/entropy_loss': Array(-0.02399397, dtype=float32), 'training/policy_loss': Array(-0.00027737, dtype=float32), 'training/total_loss': Array(153.81914, dtype=float32), 'training/v_loss': Array(153.84341, dtype=float32), 'eval/episode_goal_distance': (Array(0.28529102, dtype=float32), Array(0.05541078, dtype=float32)), 'eval/episode_reward': (Array(-14040.358, dtype=float32), Array(4599.11, dtype=float32)), 'eval/avg_episode_length': (Array(922.375, dtype=float32), Array(266.65106, dtype=float32)), 'eval/epoch_eval_time': 4.111757040023804, 'eval/sps': 31130.244018323367}
I0726 23:09:11.819358 140267183036224 train.py:379] starting iteration 322 2344.0742044448853
I0726 23:09:18.979840 140267183036224 train.py:394] {'eval/walltime': 1338.0425684452057, 'training/sps': 40302.73066920402, 'training/walltime': 1005.8036897182465, 'training/entropy_loss': Array(-0.02417078, dtype=float32), 'training/policy_loss': Array(-0.00010496, dtype=float32), 'training/total_loss': Array(129.8905, dtype=float32), 'training/v_loss': Array(129.9148, dtype=float32), 'eval/episode_goal_distance': (Array(0.28568095, dtype=float32), Array(0.06329639, dtype=float32)), 'eval/episode_reward': (Array(-13410.921, dtype=float32), Array(5170.2227, dtype=float32)), 'eval/avg_episode_length': (Array(899.09375, dtype=float32), Array(300.12103, dtype=float32)), 'eval/epoch_eval_time': 4.108129024505615, 'eval/sps': 31157.736097493653}
I0726 23:09:18.981975 140267183036224 train.py:379] starting iteration 323 2351.2368216514587
I0726 23:09:26.143186 140267183036224 train.py:394] {'eval/walltime': 1342.1537909507751, 'training/sps': 40335.429569726315, 'training/walltime': 1008.85014295578, 'training/entropy_loss': Array(-0.02493766, dtype=float32), 'training/policy_loss': Array(-1.4703473e-06, dtype=float32), 'training/total_loss': Array(126.92682, dtype=float32), 'training/v_loss': Array(126.95175, dtype=float32), 'eval/episode_goal_distance': (Array(0.30305064, dtype=float32), Array(0.05770903, dtype=float32)), 'eval/episode_reward': (Array(-14100.145, dtype=float32), Array(5339.9507, dtype=float32)), 'eval/avg_episode_length': (Array(898.9844, dtype=float32), Array(300.44595, dtype=float32)), 'eval/epoch_eval_time': 4.111222505569458, 'eval/sps': 31134.291521949704}
I0726 23:09:26.145344 140267183036224 train.py:379] starting iteration 324 2358.400190591812
I0726 23:09:33.316200 140267183036224 train.py:394] {'eval/walltime': 1346.2702140808105, 'training/sps': 40277.15296033467, 'training/walltime': 1011.901004076004, 'training/entropy_loss': Array(-0.02428256, dtype=float32), 'training/policy_loss': Array(-5.0893996e-05, dtype=float32), 'training/total_loss': Array(127.647964, dtype=float32), 'training/v_loss': Array(127.67231, dtype=float32), 'eval/episode_goal_distance': (Array(0.28688055, dtype=float32), Array(0.05784473, dtype=float32)), 'eval/episode_reward': (Array(-14092.566, dtype=float32), Array(5002.368, dtype=float32)), 'eval/avg_episode_length': (Array(914.5781, dtype=float32), Array(278.59033, dtype=float32)), 'eval/epoch_eval_time': 4.1164231300354, 'eval/sps': 31094.95694600745}
I0726 23:09:33.318516 140267183036224 train.py:379] starting iteration 325 2365.573362350464
I0726 23:09:40.480880 140267183036224 train.py:394] {'eval/walltime': 1350.3820033073425, 'training/sps': 40327.16704189058, 'training/walltime': 1014.9480814933777, 'training/entropy_loss': Array(-0.02515478, dtype=float32), 'training/policy_loss': Array(0.00037, dtype=float32), 'training/total_loss': Array(131.01007, dtype=float32), 'training/v_loss': Array(131.03485, dtype=float32), 'eval/episode_goal_distance': (Array(0.2948264, dtype=float32), Array(0.06212876, dtype=float32)), 'eval/episode_reward': (Array(-13570.888, dtype=float32), Array(5472.154, dtype=float32)), 'eval/avg_episode_length': (Array(883.52344, dtype=float32), Array(319.6928, dtype=float32)), 'eval/epoch_eval_time': 4.111789226531982, 'eval/sps': 31130.000335148354}
I0726 23:09:40.483082 140267183036224 train.py:379] starting iteration 326 2372.73792719841
I0726 23:09:47.653895 140267183036224 train.py:394] {'eval/walltime': 1354.5008418560028, 'training/sps': 40308.75738432495, 'training/walltime': 1017.9965505599976, 'training/entropy_loss': Array(-0.0250726, dtype=float32), 'training/policy_loss': Array(0.00021112, dtype=float32), 'training/total_loss': Array(88.92679, dtype=float32), 'training/v_loss': Array(88.951645, dtype=float32), 'eval/episode_goal_distance': (Array(0.2978925, dtype=float32), Array(0.05769166, dtype=float32)), 'eval/episode_reward': (Array(-14102.451, dtype=float32), Array(5120.101, dtype=float32)), 'eval/avg_episode_length': (Array(906.8047, dtype=float32), Array(289.75696, dtype=float32)), 'eval/epoch_eval_time': 4.118838548660278, 'eval/sps': 31076.721868992452}
I0726 23:09:47.656125 140267183036224 train.py:379] starting iteration 327 2379.910971879959
I0726 23:09:54.806251 140267183036224 train.py:394] {'eval/walltime': 1358.6028254032135, 'training/sps': 40358.14528714053, 'training/walltime': 1021.0412890911102, 'training/entropy_loss': Array(-0.02570952, dtype=float32), 'training/policy_loss': Array(-0.00030192, dtype=float32), 'training/total_loss': Array(93.789566, dtype=float32), 'training/v_loss': Array(93.81558, dtype=float32), 'eval/episode_goal_distance': (Array(0.30030078, dtype=float32), Array(0.06028118, dtype=float32)), 'eval/episode_reward': (Array(-14224.607, dtype=float32), Array(5367.144, dtype=float32)), 'eval/avg_episode_length': (Array(899.10156, dtype=float32), Array(300.09784, dtype=float32)), 'eval/epoch_eval_time': 4.101983547210693, 'eval/sps': 31204.415748336847}
I0726 23:09:54.808591 140267183036224 train.py:379] starting iteration 328 2387.063437461853
I0726 23:10:02.001912 140267183036224 train.py:394] {'eval/walltime': 1362.7403490543365, 'training/sps': 40257.87383848627, 'training/walltime': 1024.093611240387, 'training/entropy_loss': Array(-0.02485376, dtype=float32), 'training/policy_loss': Array(0.00027888, dtype=float32), 'training/total_loss': Array(96.61998, dtype=float32), 'training/v_loss': Array(96.644554, dtype=float32), 'eval/episode_goal_distance': (Array(0.29636064, dtype=float32), Array(0.06646489, dtype=float32)), 'eval/episode_reward': (Array(-13203.646, dtype=float32), Array(5683.6426, dtype=float32)), 'eval/avg_episode_length': (Array(868.0078, dtype=float32), Array(337.27704, dtype=float32)), 'eval/epoch_eval_time': 4.137523651123047, 'eval/sps': 30936.379050125066}
I0726 23:10:02.004089 140267183036224 train.py:379] starting iteration 329 2394.2589349746704
I0726 23:10:09.166145 140267183036224 train.py:394] {'eval/walltime': 1366.8509511947632, 'training/sps': 40316.60868561223, 'training/walltime': 1027.1414866447449, 'training/entropy_loss': Array(-0.02374873, dtype=float32), 'training/policy_loss': Array(0.00020672, dtype=float32), 'training/total_loss': Array(107.24346, dtype=float32), 'training/v_loss': Array(107.267006, dtype=float32), 'eval/episode_goal_distance': (Array(0.3001248, dtype=float32), Array(0.05512665, dtype=float32)), 'eval/episode_reward': (Array(-14067.41, dtype=float32), Array(5504.015, dtype=float32)), 'eval/avg_episode_length': (Array(891.35156, dtype=float32), Array(310.03625, dtype=float32)), 'eval/epoch_eval_time': 4.110602140426636, 'eval/sps': 31138.990256720637}
I0726 23:10:09.168396 140267183036224 train.py:379] starting iteration 330 2401.423243045807
I0726 23:10:16.327321 140267183036224 train.py:394] {'eval/walltime': 1370.9579923152924, 'training/sps': 40309.8229639161, 'training/walltime': 1030.189875125885, 'training/entropy_loss': Array(-0.02357242, dtype=float32), 'training/policy_loss': Array(0.00014881, dtype=float32), 'training/total_loss': Array(87.48605, dtype=float32), 'training/v_loss': Array(87.509476, dtype=float32), 'eval/episode_goal_distance': (Array(0.2947857, dtype=float32), Array(0.05985907, dtype=float32)), 'eval/episode_reward': (Array(-14968.434, dtype=float32), Array(3867.3972, dtype=float32)), 'eval/avg_episode_length': (Array(968.9375, dtype=float32), Array(172.94936, dtype=float32)), 'eval/epoch_eval_time': 4.107041120529175, 'eval/sps': 31165.989393236887}
I0726 23:10:16.329618 140267183036224 train.py:379] starting iteration 331 2408.5844643115997
I0726 23:10:23.489078 140267183036224 train.py:394] {'eval/walltime': 1375.066466331482, 'training/sps': 40321.746794557104, 'training/walltime': 1033.2373621463776, 'training/entropy_loss': Array(-0.02431318, dtype=float32), 'training/policy_loss': Array(0.00054328, dtype=float32), 'training/total_loss': Array(98.37912, dtype=float32), 'training/v_loss': Array(98.4029, dtype=float32), 'eval/episode_goal_distance': (Array(0.29036307, dtype=float32), Array(0.05406717, dtype=float32)), 'eval/episode_reward': (Array(-14085.673, dtype=float32), Array(4708.4634, dtype=float32)), 'eval/avg_episode_length': (Array(922.4219, dtype=float32), Array(266.48978, dtype=float32)), 'eval/epoch_eval_time': 4.108474016189575, 'eval/sps': 31155.119758725952}
I0726 23:10:23.491221 140267183036224 train.py:379] starting iteration 332 2415.7460675239563
I0726 23:10:30.655284 140267183036224 train.py:394] {'eval/walltime': 1379.1783559322357, 'training/sps': 40306.686285760625, 'training/walltime': 1036.285987854004, 'training/entropy_loss': Array(-0.02482317, dtype=float32), 'training/policy_loss': Array(-3.9720886e-05, dtype=float32), 'training/total_loss': Array(116.09438, dtype=float32), 'training/v_loss': Array(116.11924, dtype=float32), 'eval/episode_goal_distance': (Array(0.2900805, dtype=float32), Array(0.06170754, dtype=float32)), 'eval/episode_reward': (Array(-13555.979, dtype=float32), Array(5434.2427, dtype=float32)), 'eval/avg_episode_length': (Array(891.2344, dtype=float32), Array(310.3706, dtype=float32)), 'eval/epoch_eval_time': 4.111889600753784, 'eval/sps': 31129.240429153368}
I0726 23:10:30.657474 140267183036224 train.py:379] starting iteration 333 2422.9123210906982
I0726 23:10:37.842145 140267183036224 train.py:394] {'eval/walltime': 1383.3117485046387, 'training/sps': 40316.51091968575, 'training/walltime': 1039.3338706493378, 'training/entropy_loss': Array(-0.02463334, dtype=float32), 'training/policy_loss': Array(-8.339749e-05, dtype=float32), 'training/total_loss': Array(482.54816, dtype=float32), 'training/v_loss': Array(482.57288, dtype=float32), 'eval/episode_goal_distance': (Array(0.29590023, dtype=float32), Array(0.05928062, dtype=float32)), 'eval/episode_reward': (Array(-12401.995, dtype=float32), Array(6144.285, dtype=float32)), 'eval/avg_episode_length': (Array(829.2422, dtype=float32), Array(374.82028, dtype=float32)), 'eval/epoch_eval_time': 4.133392572402954, 'eval/sps': 30967.298111146265}
I0726 23:10:37.844518 140267183036224 train.py:379] starting iteration 334 2430.0993645191193
I0726 23:10:45.008141 140267183036224 train.py:394] {'eval/walltime': 1387.4212584495544, 'training/sps': 40277.77304263302, 'training/walltime': 1042.3846848011017, 'training/entropy_loss': Array(-0.02512296, dtype=float32), 'training/policy_loss': Array(7.432746e-05, dtype=float32), 'training/total_loss': Array(175.76979, dtype=float32), 'training/v_loss': Array(175.79485, dtype=float32), 'eval/episode_goal_distance': (Array(0.29502204, dtype=float32), Array(0.06071952, dtype=float32)), 'eval/episode_reward': (Array(-14314.152, dtype=float32), Array(4885.7773, dtype=float32)), 'eval/avg_episode_length': (Array(922.39844, dtype=float32), Array(266.57053, dtype=float32)), 'eval/epoch_eval_time': 4.1095099449157715, 'eval/sps': 31147.26614991158}
I0726 23:10:45.010280 140267183036224 train.py:379] starting iteration 335 2437.2651274204254
I0726 23:10:52.174572 140267183036224 train.py:394] {'eval/walltime': 1391.535713672638, 'training/sps': 40336.32924712928, 'training/walltime': 1045.4310700893402, 'training/entropy_loss': Array(-0.0253808, dtype=float32), 'training/policy_loss': Array(-0.00034594, dtype=float32), 'training/total_loss': Array(161.32816, dtype=float32), 'training/v_loss': Array(161.35388, dtype=float32), 'eval/episode_goal_distance': (Array(0.2995335, dtype=float32), Array(0.06415311, dtype=float32)), 'eval/episode_reward': (Array(-14422.524, dtype=float32), Array(5091.3413, dtype=float32)), 'eval/avg_episode_length': (Array(922.3281, dtype=float32), Array(266.81274, dtype=float32)), 'eval/epoch_eval_time': 4.114455223083496, 'eval/sps': 31109.829384428922}
I0726 23:10:52.176707 140267183036224 train.py:379] starting iteration 336 2444.4315536022186
I0726 23:10:59.331259 140267183036224 train.py:394] {'eval/walltime': 1395.639494419098, 'training/sps': 40323.83521555695, 'training/walltime': 1048.4783992767334, 'training/entropy_loss': Array(-0.02449195, dtype=float32), 'training/policy_loss': Array(-6.1785606e-05, dtype=float32), 'training/total_loss': Array(191.50206, dtype=float32), 'training/v_loss': Array(191.5266, dtype=float32), 'eval/episode_goal_distance': (Array(0.29090422, dtype=float32), Array(0.05528479, dtype=float32)), 'eval/episode_reward': (Array(-13372.169, dtype=float32), Array(5530.0654, dtype=float32)), 'eval/avg_episode_length': (Array(875.7031, dtype=float32), Array(328.85895, dtype=float32)), 'eval/epoch_eval_time': 4.103780746459961, 'eval/sps': 31190.750166274473}
I0726 23:10:59.333645 140267183036224 train.py:379] starting iteration 337 2451.588491678238
I0726 23:11:06.521089 140267183036224 train.py:394] {'eval/walltime': 1399.7770748138428, 'training/sps': 40337.7719715154, 'training/walltime': 1051.5246756076813, 'training/entropy_loss': Array(-0.02421781, dtype=float32), 'training/policy_loss': Array(0.0008687, dtype=float32), 'training/total_loss': Array(150.85974, dtype=float32), 'training/v_loss': Array(150.8831, dtype=float32), 'eval/episode_goal_distance': (Array(0.2906607, dtype=float32), Array(0.06967717, dtype=float32)), 'eval/episode_reward': (Array(-14019.407, dtype=float32), Array(4849.034, dtype=float32)), 'eval/avg_episode_length': (Array(930.08594, dtype=float32), Array(254.22435, dtype=float32)), 'eval/epoch_eval_time': 4.137580394744873, 'eval/sps': 30935.95478231006}
I0726 23:11:06.523305 140267183036224 train.py:379] starting iteration 338 2458.778151035309
I0726 23:11:13.679864 140267183036224 train.py:394] {'eval/walltime': 1403.8818006515503, 'training/sps': 40309.438339933, 'training/walltime': 1054.573093175888, 'training/entropy_loss': Array(-0.024123, dtype=float32), 'training/policy_loss': Array(0.00035829, dtype=float32), 'training/total_loss': Array(122.27948, dtype=float32), 'training/v_loss': Array(122.30324, dtype=float32), 'eval/episode_goal_distance': (Array(0.30426538, dtype=float32), Array(0.06031813, dtype=float32)), 'eval/episode_reward': (Array(-14375.502, dtype=float32), Array(5059.393, dtype=float32)), 'eval/avg_episode_length': (Array(914.5625, dtype=float32), Array(278.64175, dtype=float32)), 'eval/epoch_eval_time': 4.1047258377075195, 'eval/sps': 31183.568662283113}
I0726 23:11:13.682272 140267183036224 train.py:379] starting iteration 339 2465.937118291855
I0726 23:11:20.843389 140267183036224 train.py:394] {'eval/walltime': 1407.993367433548, 'training/sps': 40341.0808408072, 'training/walltime': 1057.619119644165, 'training/entropy_loss': Array(-0.02374387, dtype=float32), 'training/policy_loss': Array(0.00040803, dtype=float32), 'training/total_loss': Array(100.07174, dtype=float32), 'training/v_loss': Array(100.09508, dtype=float32), 'eval/episode_goal_distance': (Array(0.29382712, dtype=float32), Array(0.05785909, dtype=float32)), 'eval/episode_reward': (Array(-14126.151, dtype=float32), Array(4943.0293, dtype=float32)), 'eval/avg_episode_length': (Array(914.5156, dtype=float32), Array(278.79437, dtype=float32)), 'eval/epoch_eval_time': 4.111566781997681, 'eval/sps': 31131.684534577555}
I0726 23:11:20.847184 140267183036224 train.py:379] starting iteration 340 2473.102014064789
I0726 23:11:28.010377 140267183036224 train.py:394] {'eval/walltime': 1412.1017224788666, 'training/sps': 40281.89062156353, 'training/walltime': 1060.6696219444275, 'training/entropy_loss': Array(-0.02242398, dtype=float32), 'training/policy_loss': Array(0.00018507, dtype=float32), 'training/total_loss': Array(101.25519, dtype=float32), 'training/v_loss': Array(101.277435, dtype=float32), 'eval/episode_goal_distance': (Array(0.2860332, dtype=float32), Array(0.05869817, dtype=float32)), 'eval/episode_reward': (Array(-14404.494, dtype=float32), Array(4190.655, dtype=float32)), 'eval/avg_episode_length': (Array(945.65625, dtype=float32), Array(225.94037, dtype=float32)), 'eval/epoch_eval_time': 4.1083550453186035, 'eval/sps': 31156.021957219516}
I0726 23:11:28.015088 140267183036224 train.py:379] starting iteration 341 2480.2699184417725
I0726 23:11:35.190737 140267183036224 train.py:394] {'eval/walltime': 1416.2197649478912, 'training/sps': 40269.54670240375, 'training/walltime': 1063.7210593223572, 'training/entropy_loss': Array(-0.02160164, dtype=float32), 'training/policy_loss': Array(0.00061275, dtype=float32), 'training/total_loss': Array(113.782394, dtype=float32), 'training/v_loss': Array(113.80339, dtype=float32), 'eval/episode_goal_distance': (Array(0.28599504, dtype=float32), Array(0.06031142, dtype=float32)), 'eval/episode_reward': (Array(-13988.6875, dtype=float32), Array(4534.66, dtype=float32)), 'eval/avg_episode_length': (Array(930.1406, dtype=float32), Array(254.02585, dtype=float32)), 'eval/epoch_eval_time': 4.118042469024658, 'eval/sps': 31082.729467410347}
I0726 23:11:35.193137 140267183036224 train.py:379] starting iteration 342 2487.447983980179
I0726 23:11:42.361585 140267183036224 train.py:394] {'eval/walltime': 1420.3358833789825, 'training/sps': 40309.239725504034, 'training/walltime': 1066.7694919109344, 'training/entropy_loss': Array(-0.022099, dtype=float32), 'training/policy_loss': Array(0.00100791, dtype=float32), 'training/total_loss': Array(122.63088, dtype=float32), 'training/v_loss': Array(122.65197, dtype=float32), 'eval/episode_goal_distance': (Array(0.29034656, dtype=float32), Array(0.06105442, dtype=float32)), 'eval/episode_reward': (Array(-13742.844, dtype=float32), Array(5074.081, dtype=float32)), 'eval/avg_episode_length': (Array(906.77344, dtype=float32), Array(289.8534, dtype=float32)), 'eval/epoch_eval_time': 4.116118431091309, 'eval/sps': 31097.258774953007}
I0726 23:11:42.363907 140267183036224 train.py:379] starting iteration 343 2494.6187539100647
I0726 23:11:49.530771 140267183036224 train.py:394] {'eval/walltime': 1424.449597120285, 'training/sps': 40299.475353758826, 'training/walltime': 1069.8186631202698, 'training/entropy_loss': Array(-0.02404908, dtype=float32), 'training/policy_loss': Array(0.00123369, dtype=float32), 'training/total_loss': Array(88.19979, dtype=float32), 'training/v_loss': Array(88.2226, dtype=float32), 'eval/episode_goal_distance': (Array(0.2955284, dtype=float32), Array(0.06058758, dtype=float32)), 'eval/episode_reward': (Array(-13915.773, dtype=float32), Array(5193.069, dtype=float32)), 'eval/avg_episode_length': (Array(906.78906, dtype=float32), Array(289.805, dtype=float32)), 'eval/epoch_eval_time': 4.11371374130249, 'eval/sps': 31115.43681682441}
I0726 23:11:49.535385 140267183036224 train.py:379] starting iteration 344 2501.7902166843414
I0726 23:11:56.700477 140267183036224 train.py:394] {'eval/walltime': 1428.5604929924011, 'training/sps': 40289.87948825366, 'training/walltime': 1072.868560552597, 'training/entropy_loss': Array(-0.02349537, dtype=float32), 'training/policy_loss': Array(0.00063018, dtype=float32), 'training/total_loss': Array(77.51425, dtype=float32), 'training/v_loss': Array(77.53712, dtype=float32), 'eval/episode_goal_distance': (Array(0.2882513, dtype=float32), Array(0.0598115, dtype=float32)), 'eval/episode_reward': (Array(-14247.809, dtype=float32), Array(4309.35, dtype=float32)), 'eval/avg_episode_length': (Array(945.53906, dtype=float32), Array(226.42734, dtype=float32)), 'eval/epoch_eval_time': 4.110895872116089, 'eval/sps': 31136.765313909993}
I0726 23:11:56.702818 140267183036224 train.py:379] starting iteration 345 2508.957664489746
I0726 23:12:03.875058 140267183036224 train.py:394] {'eval/walltime': 1432.6792523860931, 'training/sps': 40296.08194005611, 'training/walltime': 1075.917988538742, 'training/entropy_loss': Array(-0.02256524, dtype=float32), 'training/policy_loss': Array(0.00141245, dtype=float32), 'training/total_loss': Array(80.39461, dtype=float32), 'training/v_loss': Array(80.41576, dtype=float32), 'eval/episode_goal_distance': (Array(0.28725296, dtype=float32), Array(0.05647438, dtype=float32)), 'eval/episode_reward': (Array(-13522.244, dtype=float32), Array(5037.994, dtype=float32)), 'eval/avg_episode_length': (Array(899.1328, dtype=float32), Array(300.00507, dtype=float32)), 'eval/epoch_eval_time': 4.118759393692017, 'eval/sps': 31077.31910633945}
I0726 23:12:03.877309 140267183036224 train.py:379] starting iteration 346 2516.132155418396
I0726 23:12:11.042962 140267183036224 train.py:394] {'eval/walltime': 1436.7926104068756, 'training/sps': 40309.92069740917, 'training/walltime': 1078.9663696289062, 'training/entropy_loss': Array(-0.02219499, dtype=float32), 'training/policy_loss': Array(0.00214432, dtype=float32), 'training/total_loss': Array(101.19888, dtype=float32), 'training/v_loss': Array(101.21893, dtype=float32), 'eval/episode_goal_distance': (Array(0.28620106, dtype=float32), Array(0.05279098, dtype=float32)), 'eval/episode_reward': (Array(-13615.256, dtype=float32), Array(4935.7896, dtype=float32)), 'eval/avg_episode_length': (Array(906.78906, dtype=float32), Array(289.80496, dtype=float32)), 'eval/epoch_eval_time': 4.113358020782471, 'eval/sps': 31118.127659515274}
I0726 23:12:11.045341 140267183036224 train.py:379] starting iteration 347 2523.3001873493195
I0726 23:12:18.218963 140267183036224 train.py:394] {'eval/walltime': 1440.9104404449463, 'training/sps': 40263.15425545877, 'training/walltime': 1082.0182914733887, 'training/entropy_loss': Array(-0.02179986, dtype=float32), 'training/policy_loss': Array(0.00317856, dtype=float32), 'training/total_loss': Array(77.3029, dtype=float32), 'training/v_loss': Array(77.321526, dtype=float32), 'eval/episode_goal_distance': (Array(0.2962735, dtype=float32), Array(0.06291236, dtype=float32)), 'eval/episode_reward': (Array(-14299.154, dtype=float32), Array(4707.9043, dtype=float32)), 'eval/avg_episode_length': (Array(930.10156, dtype=float32), Array(254.16754, dtype=float32)), 'eval/epoch_eval_time': 4.117830038070679, 'eval/sps': 31084.33296580926}
I0726 23:12:18.221150 140267183036224 train.py:379] starting iteration 348 2530.4759969711304
I0726 23:12:25.388328 140267183036224 train.py:394] {'eval/walltime': 1445.0207650661469, 'training/sps': 40246.619443294585, 'training/walltime': 1085.0714671611786, 'training/entropy_loss': Array(-0.02216344, dtype=float32), 'training/policy_loss': Array(0.00234715, dtype=float32), 'training/total_loss': Array(76.834114, dtype=float32), 'training/v_loss': Array(76.85393, dtype=float32), 'eval/episode_goal_distance': (Array(0.2934879, dtype=float32), Array(0.0655426, dtype=float32)), 'eval/episode_reward': (Array(-13313.699, dtype=float32), Array(5666.315, dtype=float32)), 'eval/avg_episode_length': (Array(875.65625, dtype=float32), Array(328.983, dtype=float32)), 'eval/epoch_eval_time': 4.1103246212005615, 'eval/sps': 31141.092686400327}
I0726 23:12:25.390503 140267183036224 train.py:379] starting iteration 349 2537.6453495025635
I0726 23:12:32.562735 140267183036224 train.py:394] {'eval/walltime': 1449.1379692554474, 'training/sps': 40274.60673117738, 'training/walltime': 1088.122521162033, 'training/entropy_loss': Array(-0.02208873, dtype=float32), 'training/policy_loss': Array(0.00208841, dtype=float32), 'training/total_loss': Array(92.121895, dtype=float32), 'training/v_loss': Array(92.14189, dtype=float32), 'eval/episode_goal_distance': (Array(0.30364883, dtype=float32), Array(0.05665383, dtype=float32)), 'eval/episode_reward': (Array(-14381.904, dtype=float32), Array(5301.785, dtype=float32)), 'eval/avg_episode_length': (Array(906.77344, dtype=float32), Array(289.8534, dtype=float32)), 'eval/epoch_eval_time': 4.117204189300537, 'eval/sps': 31089.058039102412}
I0726 23:12:32.565014 140267183036224 train.py:379] starting iteration 350 2544.8198602199554
I0726 23:12:39.746340 140267183036224 train.py:394] {'eval/walltime': 1453.2541599273682, 'training/sps': 40142.91387433474, 'training/walltime': 1091.1835844516754, 'training/entropy_loss': Array(-0.02369615, dtype=float32), 'training/policy_loss': Array(0.00135499, dtype=float32), 'training/total_loss': Array(582.4249, dtype=float32), 'training/v_loss': Array(582.44727, dtype=float32), 'eval/episode_goal_distance': (Array(0.28894165, dtype=float32), Array(0.0621749, dtype=float32)), 'eval/episode_reward': (Array(-14421.621, dtype=float32), Array(4352.1313, dtype=float32)), 'eval/avg_episode_length': (Array(945.5703, dtype=float32), Array(226.29768, dtype=float32)), 'eval/epoch_eval_time': 4.116190671920776, 'eval/sps': 31096.713005345344}
I0726 23:12:39.751089 140267183036224 train.py:379] starting iteration 351 2552.005919933319
I0726 23:12:46.937987 140267183036224 train.py:394] {'eval/walltime': 1457.3751578330994, 'training/sps': 40136.971028035274, 'training/walltime': 1094.2451009750366, 'training/entropy_loss': Array(-0.0235115, dtype=float32), 'training/policy_loss': Array(0.00234491, dtype=float32), 'training/total_loss': Array(149.05316, dtype=float32), 'training/v_loss': Array(149.07433, dtype=float32), 'eval/episode_goal_distance': (Array(0.2952739, dtype=float32), Array(0.06035585, dtype=float32)), 'eval/episode_reward': (Array(-13973.365, dtype=float32), Array(4813.747, dtype=float32)), 'eval/avg_episode_length': (Array(922.3047, dtype=float32), Array(266.89236, dtype=float32)), 'eval/epoch_eval_time': 4.120997905731201, 'eval/sps': 31060.43801235288}
I0726 23:12:46.940447 140267183036224 train.py:379] starting iteration 352 2559.1952934265137
I0726 23:12:54.124326 140267183036224 train.py:394] {'eval/walltime': 1461.493597984314, 'training/sps': 40137.324235885906, 'training/walltime': 1097.3065905570984, 'training/entropy_loss': Array(-0.02295986, dtype=float32), 'training/policy_loss': Array(0.00292609, dtype=float32), 'training/total_loss': Array(152.63997, dtype=float32), 'training/v_loss': Array(152.66, dtype=float32), 'eval/episode_goal_distance': (Array(0.2887894, dtype=float32), Array(0.05814354, dtype=float32)), 'eval/episode_reward': (Array(-13756.018, dtype=float32), Array(4701.3687, dtype=float32)), 'eval/avg_episode_length': (Array(922.3125, dtype=float32), Array(266.86594, dtype=float32)), 'eval/epoch_eval_time': 4.1184401512146, 'eval/sps': 31079.728076721127}
I0726 23:12:54.126839 140267183036224 train.py:379] starting iteration 353 2566.3816859722137
I0726 23:13:01.311563 140267183036224 train.py:394] {'eval/walltime': 1465.6115293502808, 'training/sps': 40132.26426728821, 'training/walltime': 1100.3684661388397, 'training/entropy_loss': Array(-0.02327522, dtype=float32), 'training/policy_loss': Array(0.00308363, dtype=float32), 'training/total_loss': Array(144.11934, dtype=float32), 'training/v_loss': Array(144.13953, dtype=float32), 'eval/episode_goal_distance': (Array(0.29446492, dtype=float32), Array(0.05750751, dtype=float32)), 'eval/episode_reward': (Array(-14225.145, dtype=float32), Array(4667.0522, dtype=float32)), 'eval/avg_episode_length': (Array(930.10156, dtype=float32), Array(254.16772, dtype=float32)), 'eval/epoch_eval_time': 4.117931365966797, 'eval/sps': 31083.568089034554}
I0726 23:13:01.313919 140267183036224 train.py:379] starting iteration 354 2573.56876540184
I0726 23:13:08.497877 140267183036224 train.py:394] {'eval/walltime': 1469.7360904216766, 'training/sps': 40216.20436042483, 'training/walltime': 1103.4239509105682, 'training/entropy_loss': Array(-0.02290321, dtype=float32), 'training/policy_loss': Array(0.00240913, dtype=float32), 'training/total_loss': Array(135.9615, dtype=float32), 'training/v_loss': Array(135.982, dtype=float32), 'eval/episode_goal_distance': (Array(0.29180238, dtype=float32), Array(0.05911494, dtype=float32)), 'eval/episode_reward': (Array(-14396.202, dtype=float32), Array(4434.1562, dtype=float32)), 'eval/avg_episode_length': (Array(937.8203, dtype=float32), Array(240.82149, dtype=float32)), 'eval/epoch_eval_time': 4.124561071395874, 'eval/sps': 31033.605221095924}
I0726 23:13:08.502516 140267183036224 train.py:379] starting iteration 355 2580.7573471069336
I0726 23:13:15.695910 140267183036224 train.py:394] {'eval/walltime': 1473.8630316257477, 'training/sps': 40126.149627611856, 'training/walltime': 1106.4862930774689, 'training/entropy_loss': Array(-0.02282572, dtype=float32), 'training/policy_loss': Array(0.00232958, dtype=float32), 'training/total_loss': Array(109.026855, dtype=float32), 'training/v_loss': Array(109.047356, dtype=float32), 'eval/episode_goal_distance': (Array(0.2975651, dtype=float32), Array(0.05646908, dtype=float32)), 'eval/episode_reward': (Array(-14082.773, dtype=float32), Array(4751.3364, dtype=float32)), 'eval/avg_episode_length': (Array(922.2969, dtype=float32), Array(266.91937, dtype=float32)), 'eval/epoch_eval_time': 4.126941204071045, 'eval/sps': 31015.70719586062}
I0726 23:13:15.698048 140267183036224 train.py:379] starting iteration 356 2587.9528951644897
I0726 23:13:22.881176 140267183036224 train.py:394] {'eval/walltime': 1477.9925682544708, 'training/sps': 40294.83436617322, 'training/walltime': 1109.5358154773712, 'training/entropy_loss': Array(-0.02125003, dtype=float32), 'training/policy_loss': Array(0.00321365, dtype=float32), 'training/total_loss': Array(89.87355, dtype=float32), 'training/v_loss': Array(89.89159, dtype=float32), 'eval/episode_goal_distance': (Array(0.29681104, dtype=float32), Array(0.05771848, dtype=float32)), 'eval/episode_reward': (Array(-14099.893, dtype=float32), Array(5211.829, dtype=float32)), 'eval/avg_episode_length': (Array(906.8125, dtype=float32), Array(289.73212, dtype=float32)), 'eval/epoch_eval_time': 4.1295366287231445, 'eval/sps': 30996.213742164502}
I0726 23:13:22.883416 140267183036224 train.py:379] starting iteration 357 2595.138262987137
I0726 23:13:30.085063 140267183036224 train.py:394] {'eval/walltime': 1482.1393475532532, 'training/sps': 40277.360701515994, 'training/walltime': 1112.586660861969, 'training/entropy_loss': Array(-0.02085531, dtype=float32), 'training/policy_loss': Array(0.00268398, dtype=float32), 'training/total_loss': Array(101.490486, dtype=float32), 'training/v_loss': Array(101.50865, dtype=float32), 'eval/episode_goal_distance': (Array(0.29023272, dtype=float32), Array(0.06418264, dtype=float32)), 'eval/episode_reward': (Array(-13183.719, dtype=float32), Array(5247.383, dtype=float32)), 'eval/avg_episode_length': (Array(891.2031, dtype=float32), Array(310.45978, dtype=float32)), 'eval/epoch_eval_time': 4.146779298782349, 'eval/sps': 30867.328781540327}
I0726 23:13:30.277091 140267183036224 train.py:379] starting iteration 358 2602.531916618347
I0726 23:13:37.449612 140267183036224 train.py:394] {'eval/walltime': 1486.2551138401031, 'training/sps': 40254.924452709674, 'training/walltime': 1115.639206647873, 'training/entropy_loss': Array(-0.02180287, dtype=float32), 'training/policy_loss': Array(0.00227209, dtype=float32), 'training/total_loss': Array(100.033264, dtype=float32), 'training/v_loss': Array(100.052795, dtype=float32), 'eval/episode_goal_distance': (Array(0.2964135, dtype=float32), Array(0.05739101, dtype=float32)), 'eval/episode_reward': (Array(-14296.455, dtype=float32), Array(4714.6562, dtype=float32)), 'eval/avg_episode_length': (Array(922.35156, dtype=float32), Array(266.73203, dtype=float32)), 'eval/epoch_eval_time': 4.115766286849976, 'eval/sps': 31099.919450957335}
I0726 23:13:37.452121 140267183036224 train.py:379] starting iteration 359 2609.7069675922394
I0726 23:13:44.604382 140267183036224 train.py:394] {'eval/walltime': 1490.3590729236603, 'training/sps': 40356.33770572082, 'training/walltime': 1118.6840815544128, 'training/entropy_loss': Array(-0.02218615, dtype=float32), 'training/policy_loss': Array(0.00095576, dtype=float32), 'training/total_loss': Array(107.28121, dtype=float32), 'training/v_loss': Array(107.302444, dtype=float32), 'eval/episode_goal_distance': (Array(0.28715566, dtype=float32), Array(0.05717541, dtype=float32)), 'eval/episode_reward': (Array(-14281.926, dtype=float32), Array(4298.562, dtype=float32)), 'eval/avg_episode_length': (Array(937.89844, dtype=float32), Array(240.51868, dtype=float32)), 'eval/epoch_eval_time': 4.103959083557129, 'eval/sps': 31189.394775606608}
I0726 23:13:44.606525 140267183036224 train.py:379] starting iteration 360 2616.8613710403442
I0726 23:13:51.768596 140267183036224 train.py:394] {'eval/walltime': 1494.467481136322, 'training/sps': 40287.71585203149, 'training/walltime': 1121.734142780304, 'training/entropy_loss': Array(-0.02289675, dtype=float32), 'training/policy_loss': Array(0.00141273, dtype=float32), 'training/total_loss': Array(82.37688, dtype=float32), 'training/v_loss': Array(82.39836, dtype=float32), 'eval/episode_goal_distance': (Array(0.28074244, dtype=float32), Array(0.055909, dtype=float32)), 'eval/episode_reward': (Array(-13818.17, dtype=float32), Array(4359.1294, dtype=float32)), 'eval/avg_episode_length': (Array(937.8281, dtype=float32), Array(240.79097, dtype=float32)), 'eval/epoch_eval_time': 4.108408212661743, 'eval/sps': 31155.618763859824}
I0726 23:13:51.770847 140267183036224 train.py:379] starting iteration 361 2624.025693178177
I0726 23:13:58.935392 140267183036224 train.py:394] {'eval/walltime': 1498.5828342437744, 'training/sps': 40344.39972727792, 'training/walltime': 1124.7799186706543, 'training/entropy_loss': Array(-0.02127586, dtype=float32), 'training/policy_loss': Array(0.00316071, dtype=float32), 'training/total_loss': Array(85.62526, dtype=float32), 'training/v_loss': Array(85.64338, dtype=float32), 'eval/episode_goal_distance': (Array(0.28675747, dtype=float32), Array(0.06422756, dtype=float32)), 'eval/episode_reward': (Array(-13436.445, dtype=float32), Array(4774.167, dtype=float32)), 'eval/avg_episode_length': (Array(914.52344, dtype=float32), Array(278.7688, dtype=float32)), 'eval/epoch_eval_time': 4.115353107452393, 'eval/sps': 31103.04186734497}
I0726 23:13:58.937770 140267183036224 train.py:379] starting iteration 362 2631.192616701126
I0726 23:14:06.109950 140267183036224 train.py:394] {'eval/walltime': 1502.7044041156769, 'training/sps': 40326.24884160912, 'training/walltime': 1127.8270654678345, 'training/entropy_loss': Array(-0.02153672, dtype=float32), 'training/policy_loss': Array(0.00366947, dtype=float32), 'training/total_loss': Array(67.0607, dtype=float32), 'training/v_loss': Array(67.07857, dtype=float32), 'eval/episode_goal_distance': (Array(0.2881906, dtype=float32), Array(0.06041447, dtype=float32)), 'eval/episode_reward': (Array(-12915.218, dtype=float32), Array(5616.958, dtype=float32)), 'eval/avg_episode_length': (Array(867.91406, dtype=float32), Array(337.51587, dtype=float32)), 'eval/epoch_eval_time': 4.121569871902466, 'eval/sps': 31056.127635394612}
I0726 23:14:06.112254 140267183036224 train.py:379] starting iteration 363 2638.3671002388
I0726 23:14:13.269098 140267183036224 train.py:394] {'eval/walltime': 1506.8136596679688, 'training/sps': 40366.537492763106, 'training/walltime': 1130.8711709976196, 'training/entropy_loss': Array(-0.02243824, dtype=float32), 'training/policy_loss': Array(0.00384942, dtype=float32), 'training/total_loss': Array(75.124405, dtype=float32), 'training/v_loss': Array(75.14299, dtype=float32), 'eval/episode_goal_distance': (Array(0.2888595, dtype=float32), Array(0.06320801, dtype=float32)), 'eval/episode_reward': (Array(-13314.058, dtype=float32), Array(5520.0767, dtype=float32)), 'eval/avg_episode_length': (Array(875.71094, dtype=float32), Array(328.83826, dtype=float32)), 'eval/epoch_eval_time': 4.10925555229187, 'eval/sps': 31149.194390845343}
I0726 23:14:13.271547 140267183036224 train.py:379] starting iteration 364 2645.526392221451
I0726 23:14:20.447946 140267183036224 train.py:394] {'eval/walltime': 1510.9381411075592, 'training/sps': 40308.99067208128, 'training/walltime': 1133.9196224212646, 'training/entropy_loss': Array(-0.02312782, dtype=float32), 'training/policy_loss': Array(0.0038349, dtype=float32), 'training/total_loss': Array(101.82063, dtype=float32), 'training/v_loss': Array(101.83993, dtype=float32), 'eval/episode_goal_distance': (Array(0.28704768, dtype=float32), Array(0.06294812, dtype=float32)), 'eval/episode_reward': (Array(-13959.072, dtype=float32), Array(4700.9844, dtype=float32)), 'eval/avg_episode_length': (Array(930.03125, dtype=float32), Array(254.4231, dtype=float32)), 'eval/epoch_eval_time': 4.124481439590454, 'eval/sps': 31034.204390239644}
I0726 23:14:20.450324 140267183036224 train.py:379] starting iteration 365 2652.7051706314087
I0726 23:14:27.607566 140267183036224 train.py:394] {'eval/walltime': 1515.0452558994293, 'training/sps': 40331.7271094186, 'training/walltime': 1136.9663553237915, 'training/entropy_loss': Array(-0.02394806, dtype=float32), 'training/policy_loss': Array(0.00361511, dtype=float32), 'training/total_loss': Array(75.41095, dtype=float32), 'training/v_loss': Array(75.431274, dtype=float32), 'eval/episode_goal_distance': (Array(0.29521984, dtype=float32), Array(0.06237343, dtype=float32)), 'eval/episode_reward': (Array(-14545.127, dtype=float32), Array(4685.1836, dtype=float32)), 'eval/avg_episode_length': (Array(937.89844, dtype=float32), Array(240.51897, dtype=float32)), 'eval/epoch_eval_time': 4.107114791870117, 'eval/sps': 31165.430353534626}
I0726 23:14:27.609799 140267183036224 train.py:379] starting iteration 366 2659.8646450042725
I0726 23:14:34.779927 140267183036224 train.py:394] {'eval/walltime': 1519.1646111011505, 'training/sps': 40323.03073774339, 'training/walltime': 1140.0137453079224, 'training/entropy_loss': Array(-0.02318777, dtype=float32), 'training/policy_loss': Array(0.00227259, dtype=float32), 'training/total_loss': Array(379.17444, dtype=float32), 'training/v_loss': Array(379.19534, dtype=float32), 'eval/episode_goal_distance': (Array(0.28236598, dtype=float32), Array(0.06089909, dtype=float32)), 'eval/episode_reward': (Array(-13813.911, dtype=float32), Array(4707.112, dtype=float32)), 'eval/avg_episode_length': (Array(922.3203, dtype=float32), Array(266.83887, dtype=float32)), 'eval/epoch_eval_time': 4.119355201721191, 'eval/sps': 31072.824199893643}
I0726 23:14:34.782235 140267183036224 train.py:379] starting iteration 367 2667.0370819568634
I0726 23:14:41.978090 140267183036224 train.py:394] {'eval/walltime': 1523.3109531402588, 'training/sps': 40341.89866983434, 'training/walltime': 1143.0597100257874, 'training/entropy_loss': Array(-0.02310153, dtype=float32), 'training/policy_loss': Array(0.00296394, dtype=float32), 'training/total_loss': Array(220.01381, dtype=float32), 'training/v_loss': Array(220.03395, dtype=float32), 'eval/episode_goal_distance': (Array(0.30146465, dtype=float32), Array(0.0530903, dtype=float32)), 'eval/episode_reward': (Array(-14370.473, dtype=float32), Array(5272.8916, dtype=float32)), 'eval/avg_episode_length': (Array(899.0469, dtype=float32), Array(300.25992, dtype=float32)), 'eval/epoch_eval_time': 4.146342039108276, 'eval/sps': 30870.583949106145}
I0726 23:14:41.980386 140267183036224 train.py:379] starting iteration 368 2674.235233068466
I0726 23:14:49.150788 140267183036224 train.py:394] {'eval/walltime': 1527.431999206543, 'training/sps': 40350.07562526065, 'training/walltime': 1146.105057477951, 'training/entropy_loss': Array(-0.02335445, dtype=float32), 'training/policy_loss': Array(0.00378814, dtype=float32), 'training/total_loss': Array(161.6547, dtype=float32), 'training/v_loss': Array(161.67426, dtype=float32), 'eval/episode_goal_distance': (Array(0.2971935, dtype=float32), Array(0.06358434, dtype=float32)), 'eval/episode_reward': (Array(-14003.068, dtype=float32), Array(4966.2583, dtype=float32)), 'eval/avg_episode_length': (Array(906.78125, dtype=float32), Array(289.82922, dtype=float32)), 'eval/epoch_eval_time': 4.12104606628418, 'eval/sps': 31060.07502493503}
I0726 23:14:49.155869 140267183036224 train.py:379] starting iteration 369 2681.410699367523
I0726 23:14:56.337082 140267183036224 train.py:394] {'eval/walltime': 1531.550395488739, 'training/sps': 40176.51887691041, 'training/walltime': 1149.1635603904724, 'training/entropy_loss': Array(-0.02309763, dtype=float32), 'training/policy_loss': Array(0.00504547, dtype=float32), 'training/total_loss': Array(123.57106, dtype=float32), 'training/v_loss': Array(123.58911, dtype=float32), 'eval/episode_goal_distance': (Array(0.2835982, dtype=float32), Array(0.06483439, dtype=float32)), 'eval/episode_reward': (Array(-13557.882, dtype=float32), Array(5425.247, dtype=float32)), 'eval/avg_episode_length': (Array(891.27344, dtype=float32), Array(310.25925, dtype=float32)), 'eval/epoch_eval_time': 4.118396282196045, 'eval/sps': 31080.059136938322}
I0726 23:14:56.339388 140267183036224 train.py:379] starting iteration 370 2688.5942351818085
I0726 23:15:03.525832 140267183036224 train.py:394] {'eval/walltime': 1535.670049905777, 'training/sps': 40118.86261146516, 'training/walltime': 1152.226458787918, 'training/entropy_loss': Array(-0.02433268, dtype=float32), 'training/policy_loss': Array(0.00485116, dtype=float32), 'training/total_loss': Array(158.35101, dtype=float32), 'training/v_loss': Array(158.3705, dtype=float32), 'eval/episode_goal_distance': (Array(0.28400886, dtype=float32), Array(0.06871578, dtype=float32)), 'eval/episode_reward': (Array(-12911.646, dtype=float32), Array(5756.4478, dtype=float32)), 'eval/avg_episode_length': (Array(868.0078, dtype=float32), Array(337.27652, dtype=float32)), 'eval/epoch_eval_time': 4.119654417037964, 'eval/sps': 31070.567344343446}
I0726 23:15:03.528308 140267183036224 train.py:379] starting iteration 371 2695.7831542491913
I0726 23:15:10.714541 140267183036224 train.py:394] {'eval/walltime': 1539.790334701538, 'training/sps': 40130.8143323693, 'training/walltime': 1155.2884449958801, 'training/entropy_loss': Array(-0.02475008, dtype=float32), 'training/policy_loss': Array(0.01274846, dtype=float32), 'training/total_loss': Array(131.74553, dtype=float32), 'training/v_loss': Array(131.75754, dtype=float32), 'eval/episode_goal_distance': (Array(0.301258, dtype=float32), Array(0.05669841, dtype=float32)), 'eval/episode_reward': (Array(-14662.338, dtype=float32), Array(4789.9297, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.08241, dtype=float32)), 'eval/epoch_eval_time': 4.120284795761108, 'eval/sps': 31065.81373493517}
I0726 23:15:10.716934 140267183036224 train.py:379] starting iteration 372 2702.971780538559
I0726 23:15:17.892199 140267183036224 train.py:394] {'eval/walltime': 1543.9047391414642, 'training/sps': 40195.56026127781, 'training/walltime': 1158.3454990386963, 'training/entropy_loss': Array(-0.02397289, dtype=float32), 'training/policy_loss': Array(0.00782293, dtype=float32), 'training/total_loss': Array(110.71951, dtype=float32), 'training/v_loss': Array(110.73566, dtype=float32), 'eval/episode_goal_distance': (Array(0.2913248, dtype=float32), Array(0.05833062, dtype=float32)), 'eval/episode_reward': (Array(-14233.3125, dtype=float32), Array(4730.6177, dtype=float32)), 'eval/avg_episode_length': (Array(922.3594, dtype=float32), Array(266.70474, dtype=float32)), 'eval/epoch_eval_time': 4.1144044399261475, 'eval/sps': 31110.21336597079}
I0726 23:15:17.894533 140267183036224 train.py:379] starting iteration 373 2710.1493792533875
I0726 23:15:25.073072 140267183036224 train.py:394] {'eval/walltime': 1548.020480632782, 'training/sps': 40171.84041041997, 'training/walltime': 1161.4043581485748, 'training/entropy_loss': Array(-0.02387303, dtype=float32), 'training/policy_loss': Array(0.00755008, dtype=float32), 'training/total_loss': Array(103.460236, dtype=float32), 'training/v_loss': Array(103.47656, dtype=float32), 'eval/episode_goal_distance': (Array(0.2956758, dtype=float32), Array(0.06235593, dtype=float32)), 'eval/episode_reward': (Array(-13598.863, dtype=float32), Array(5309.94, dtype=float32)), 'eval/avg_episode_length': (Array(891.28125, dtype=float32), Array(310.23676, dtype=float32)), 'eval/epoch_eval_time': 4.115741491317749, 'eval/sps': 31100.106814293107}
I0726 23:15:25.075534 140267183036224 train.py:379] starting iteration 374 2717.3303802013397
I0726 23:15:32.250792 140267183036224 train.py:394] {'eval/walltime': 1552.132952451706, 'training/sps': 40172.30069312946, 'training/walltime': 1164.4631822109222, 'training/entropy_loss': Array(-0.02264249, dtype=float32), 'training/policy_loss': Array(0.01045786, dtype=float32), 'training/total_loss': Array(107.531685, dtype=float32), 'training/v_loss': Array(107.54386, dtype=float32), 'eval/episode_goal_distance': (Array(0.2858858, dtype=float32), Array(0.06254555, dtype=float32)), 'eval/episode_reward': (Array(-14444.193, dtype=float32), Array(4340.8447, dtype=float32)), 'eval/avg_episode_length': (Array(945.6406, dtype=float32), Array(226.00542, dtype=float32)), 'eval/epoch_eval_time': 4.11247181892395, 'eval/sps': 31124.833344995874}
I0726 23:15:32.253129 140267183036224 train.py:379] starting iteration 375 2724.5079753398895
I0726 23:15:39.426470 140267183036224 train.py:394] {'eval/walltime': 1556.2453663349152, 'training/sps': 40196.112000292, 'training/walltime': 1167.5201942920685, 'training/entropy_loss': Array(-0.02297884, dtype=float32), 'training/policy_loss': Array(0.0061558, dtype=float32), 'training/total_loss': Array(139.52931, dtype=float32), 'training/v_loss': Array(139.54614, dtype=float32), 'eval/episode_goal_distance': (Array(0.2978896, dtype=float32), Array(0.0555269, dtype=float32)), 'eval/episode_reward': (Array(-14616.002, dtype=float32), Array(4309.8936, dtype=float32)), 'eval/avg_episode_length': (Array(945.65625, dtype=float32), Array(225.94037, dtype=float32)), 'eval/epoch_eval_time': 4.1124138832092285, 'eval/sps': 31125.271831859463}
I0726 23:15:39.429086 140267183036224 train.py:379] starting iteration 376 2731.6839323043823
I0726 23:15:46.600188 140267183036224 train.py:394] {'eval/walltime': 1560.3603248596191, 'training/sps': 40258.8235194869, 'training/walltime': 1170.5724444389343, 'training/entropy_loss': Array(-0.02183062, dtype=float32), 'training/policy_loss': Array(0.00858486, dtype=float32), 'training/total_loss': Array(116.55348, dtype=float32), 'training/v_loss': Array(116.56673, dtype=float32), 'eval/episode_goal_distance': (Array(0.28472805, dtype=float32), Array(0.06100616, dtype=float32)), 'eval/episode_reward': (Array(-13757.503, dtype=float32), Array(4790.4805, dtype=float32)), 'eval/avg_episode_length': (Array(922.3672, dtype=float32), Array(266.67798, dtype=float32)), 'eval/epoch_eval_time': 4.1149585247039795, 'eval/sps': 31106.02433330917}
I0726 23:15:46.602547 140267183036224 train.py:379] starting iteration 377 2738.8573932647705
I0726 23:15:53.780457 140267183036224 train.py:394] {'eval/walltime': 1564.4747931957245, 'training/sps': 40163.33490694177, 'training/walltime': 1173.6319513320923, 'training/entropy_loss': Array(-0.02138685, dtype=float32), 'training/policy_loss': Array(0.0066887, dtype=float32), 'training/total_loss': Array(85.28389, dtype=float32), 'training/v_loss': Array(85.29859, dtype=float32), 'eval/episode_goal_distance': (Array(0.28582597, dtype=float32), Array(0.0596605, dtype=float32)), 'eval/episode_reward': (Array(-13382.096, dtype=float32), Array(5340.0312, dtype=float32)), 'eval/avg_episode_length': (Array(883.5078, dtype=float32), Array(319.73557, dtype=float32)), 'eval/epoch_eval_time': 4.114468336105347, 'eval/sps': 31109.730235805295}
I0726 23:15:53.783007 140267183036224 train.py:379] starting iteration 378 2746.0378534793854
I0726 23:16:00.965373 140267183036224 train.py:394] {'eval/walltime': 1568.597797870636, 'training/sps': 40215.85603893586, 'training/walltime': 1176.687462568283, 'training/entropy_loss': Array(-0.01991454, dtype=float32), 'training/policy_loss': Array(0.00611866, dtype=float32), 'training/total_loss': Array(81.44614, dtype=float32), 'training/v_loss': Array(81.45993, dtype=float32), 'eval/episode_goal_distance': (Array(0.29963517, dtype=float32), Array(0.05687474, dtype=float32)), 'eval/episode_reward': (Array(-13950.352, dtype=float32), Array(5006.796, dtype=float32)), 'eval/avg_episode_length': (Array(914.5625, dtype=float32), Array(278.64145, dtype=float32)), 'eval/epoch_eval_time': 4.123004674911499, 'eval/sps': 31045.32012269609}
I0726 23:16:00.967742 140267183036224 train.py:379] starting iteration 379 2753.2225890159607
I0726 23:16:08.145447 140267183036224 train.py:394] {'eval/walltime': 1572.7191557884216, 'training/sps': 40257.72918902085, 'training/walltime': 1179.7397956848145, 'training/entropy_loss': Array(-0.01949563, dtype=float32), 'training/policy_loss': Array(0.00599959, dtype=float32), 'training/total_loss': Array(72.74594, dtype=float32), 'training/v_loss': Array(72.75943, dtype=float32), 'eval/episode_goal_distance': (Array(0.29165226, dtype=float32), Array(0.06093821, dtype=float32)), 'eval/episode_reward': (Array(-14202.41, dtype=float32), Array(4651.1, dtype=float32)), 'eval/avg_episode_length': (Array(930.1406, dtype=float32), Array(254.02588, dtype=float32)), 'eval/epoch_eval_time': 4.1213579177856445, 'eval/sps': 31057.72479687298}
I0726 23:16:08.147862 140267183036224 train.py:379] starting iteration 380 2760.402708530426
I0726 23:16:15.341957 140267183036224 train.py:394] {'eval/walltime': 1576.859058856964, 'training/sps': 40285.72563789345, 'training/walltime': 1182.7900075912476, 'training/entropy_loss': Array(-0.01920235, dtype=float32), 'training/policy_loss': Array(0.00494791, dtype=float32), 'training/total_loss': Array(73.71585, dtype=float32), 'training/v_loss': Array(73.7301, dtype=float32), 'eval/episode_goal_distance': (Array(0.2917227, dtype=float32), Array(0.06240834, dtype=float32)), 'eval/episode_reward': (Array(-14653.552, dtype=float32), Array(3559.8162, dtype=float32)), 'eval/avg_episode_length': (Array(968.9375, dtype=float32), Array(172.94899, dtype=float32)), 'eval/epoch_eval_time': 4.1399030685424805, 'eval/sps': 30918.598305506814}
I0726 23:16:15.344290 140267183036224 train.py:379] starting iteration 381 2767.599136829376
I0726 23:16:22.508893 140267183036224 train.py:394] {'eval/walltime': 1580.9708790779114, 'training/sps': 40303.383056075385, 'training/walltime': 1185.8388831615448, 'training/entropy_loss': Array(-0.01967006, dtype=float32), 'training/policy_loss': Array(0.00699886, dtype=float32), 'training/total_loss': Array(76.51305, dtype=float32), 'training/v_loss': Array(76.52572, dtype=float32), 'eval/episode_goal_distance': (Array(0.29721, dtype=float32), Array(0.06368651, dtype=float32)), 'eval/episode_reward': (Array(-14544.176, dtype=float32), Array(4551.9204, dtype=float32)), 'eval/avg_episode_length': (Array(937.9531, dtype=float32), Array(240.30687, dtype=float32)), 'eval/epoch_eval_time': 4.111820220947266, 'eval/sps': 31129.7656808818}
I0726 23:16:22.511370 140267183036224 train.py:379] starting iteration 382 2774.7662165164948
I0726 23:16:29.667528 140267183036224 train.py:394] {'eval/walltime': 1585.0774772167206, 'training/sps': 40341.21977467449, 'training/walltime': 1188.8848991394043, 'training/entropy_loss': Array(-0.01883851, dtype=float32), 'training/policy_loss': Array(0.00436751, dtype=float32), 'training/total_loss': Array(79.21861, dtype=float32), 'training/v_loss': Array(79.23308, dtype=float32), 'eval/episode_goal_distance': (Array(0.2858915, dtype=float32), Array(0.05775051, dtype=float32)), 'eval/episode_reward': (Array(-13879.883, dtype=float32), Array(4379.247, dtype=float32)), 'eval/avg_episode_length': (Array(937.83594, dtype=float32), Array(240.76122, dtype=float32)), 'eval/epoch_eval_time': 4.106598138809204, 'eval/sps': 31169.35129111911}
I0726 23:16:29.669953 140267183036224 train.py:379] starting iteration 383 2781.9247994422913
I0726 23:16:36.839819 140267183036224 train.py:394] {'eval/walltime': 1589.1975464820862, 'training/sps': 40339.57157546011, 'training/walltime': 1191.931039571762, 'training/entropy_loss': Array(-0.02244791, dtype=float32), 'training/policy_loss': Array(0.00433734, dtype=float32), 'training/total_loss': Array(466.59644, dtype=float32), 'training/v_loss': Array(466.61453, dtype=float32), 'eval/episode_goal_distance': (Array(0.28116328, dtype=float32), Array(0.05938843, dtype=float32)), 'eval/episode_reward': (Array(-13677.27, dtype=float32), Array(5139.4585, dtype=float32)), 'eval/avg_episode_length': (Array(906.8047, dtype=float32), Array(289.7565, dtype=float32)), 'eval/epoch_eval_time': 4.120069265365601, 'eval/sps': 31067.438859827453}
I0726 23:16:36.842054 140267183036224 train.py:379] starting iteration 384 2789.0969004631042
I0726 23:16:44.022602 140267183036224 train.py:394] {'eval/walltime': 1593.3274946212769, 'training/sps': 40326.024819707105, 'training/walltime': 1194.9782032966614, 'training/entropy_loss': Array(-0.0226492, dtype=float32), 'training/policy_loss': Array(0.00368193, dtype=float32), 'training/total_loss': Array(179.2277, dtype=float32), 'training/v_loss': Array(179.24667, dtype=float32), 'eval/episode_goal_distance': (Array(0.29399937, dtype=float32), Array(0.06492388, dtype=float32)), 'eval/episode_reward': (Array(-14509.461, dtype=float32), Array(4662.346, dtype=float32)), 'eval/avg_episode_length': (Array(937.8281, dtype=float32), Array(240.79094, dtype=float32)), 'eval/epoch_eval_time': 4.129948139190674, 'eval/sps': 30993.125261152443}
I0726 23:16:44.024822 140267183036224 train.py:379] starting iteration 385 2796.2796680927277
I0726 23:16:51.182526 140267183036224 train.py:394] {'eval/walltime': 1597.4346454143524, 'training/sps': 40328.47026390388, 'training/walltime': 1198.0251822471619, 'training/entropy_loss': Array(-0.0220168, dtype=float32), 'training/policy_loss': Array(0.00452433, dtype=float32), 'training/total_loss': Array(157.77751, dtype=float32), 'training/v_loss': Array(157.795, dtype=float32), 'eval/episode_goal_distance': (Array(0.2922437, dtype=float32), Array(0.06661475, dtype=float32)), 'eval/episode_reward': (Array(-13802.478, dtype=float32), Array(5119.5483, dtype=float32)), 'eval/avg_episode_length': (Array(906.75, dtype=float32), Array(289.92636, dtype=float32)), 'eval/epoch_eval_time': 4.1071507930755615, 'eval/sps': 31165.157173143292}
I0726 23:16:51.184844 140267183036224 train.py:379] starting iteration 386 2803.4396908283234
I0726 23:16:58.346018 140267183036224 train.py:394] {'eval/walltime': 1601.5443997383118, 'training/sps': 40316.816834002806, 'training/walltime': 1201.0730419158936, 'training/entropy_loss': Array(-0.02175381, dtype=float32), 'training/policy_loss': Array(0.00566771, dtype=float32), 'training/total_loss': Array(138.1337, dtype=float32), 'training/v_loss': Array(138.1498, dtype=float32), 'eval/episode_goal_distance': (Array(0.2786547, dtype=float32), Array(0.06314682, dtype=float32)), 'eval/episode_reward': (Array(-14238.236, dtype=float32), Array(4460.1265, dtype=float32)), 'eval/avg_episode_length': (Array(937.8281, dtype=float32), Array(240.79068, dtype=float32)), 'eval/epoch_eval_time': 4.109754323959351, 'eval/sps': 31145.414034550948}
I0726 23:16:58.348426 140267183036224 train.py:379] starting iteration 387 2810.603272676468
I0726 23:17:05.513931 140267183036224 train.py:394] {'eval/walltime': 1605.6579949855804, 'training/sps': 40310.762486146494, 'training/walltime': 1204.1213593482971, 'training/entropy_loss': Array(-0.02291687, dtype=float32), 'training/policy_loss': Array(0.00368574, dtype=float32), 'training/total_loss': Array(138.95496, dtype=float32), 'training/v_loss': Array(138.9742, dtype=float32), 'eval/episode_goal_distance': (Array(0.2907403, dtype=float32), Array(0.06287952, dtype=float32)), 'eval/episode_reward': (Array(-14097.77, dtype=float32), Array(4795.271, dtype=float32)), 'eval/avg_episode_length': (Array(922.27344, dtype=float32), Array(267., dtype=float32)), 'eval/epoch_eval_time': 4.113595247268677, 'eval/sps': 31116.333111525437}
I0726 23:17:05.516440 140267183036224 train.py:379] starting iteration 388 2817.7712862491608
I0726 23:17:12.687847 140267183036224 train.py:394] {'eval/walltime': 1609.777398109436, 'training/sps': 40310.25173372944, 'training/walltime': 1207.1697154045105, 'training/entropy_loss': Array(-0.02240643, dtype=float32), 'training/policy_loss': Array(0.00453734, dtype=float32), 'training/total_loss': Array(116.16945, dtype=float32), 'training/v_loss': Array(116.187325, dtype=float32), 'eval/episode_goal_distance': (Array(0.29205012, dtype=float32), Array(0.0582689, dtype=float32)), 'eval/episode_reward': (Array(-14488.261, dtype=float32), Array(4282.071, dtype=float32)), 'eval/avg_episode_length': (Array(945.6406, dtype=float32), Array(226.0057, dtype=float32)), 'eval/epoch_eval_time': 4.119403123855591, 'eval/sps': 31072.462721297667}
I0726 23:17:12.690111 140267183036224 train.py:379] starting iteration 389 2824.9449574947357
I0726 23:17:19.852781 140267183036224 train.py:394] {'eval/walltime': 1613.8904330730438, 'training/sps': 40342.624955070154, 'training/walltime': 1210.2156252861023, 'training/entropy_loss': Array(-0.02166841, dtype=float32), 'training/policy_loss': Array(0.00399547, dtype=float32), 'training/total_loss': Array(105.028275, dtype=float32), 'training/v_loss': Array(105.04595, dtype=float32), 'eval/episode_goal_distance': (Array(0.28840917, dtype=float32), Array(0.06298941, dtype=float32)), 'eval/episode_reward': (Array(-14162.959, dtype=float32), Array(4860.433, dtype=float32)), 'eval/avg_episode_length': (Array(930.0469, dtype=float32), Array(254.36667, dtype=float32)), 'eval/epoch_eval_time': 4.113034963607788, 'eval/sps': 31120.571824102262}
I0726 23:17:19.855269 140267183036224 train.py:379] starting iteration 390 2832.110115289688
I0726 23:17:27.015273 140267183036224 train.py:394] {'eval/walltime': 1618.0008432865143, 'training/sps': 40341.09347111923, 'training/walltime': 1213.261650800705, 'training/entropy_loss': Array(-0.02129108, dtype=float32), 'training/policy_loss': Array(0.00324335, dtype=float32), 'training/total_loss': Array(87.87723, dtype=float32), 'training/v_loss': Array(87.89528, dtype=float32), 'eval/episode_goal_distance': (Array(0.28747022, dtype=float32), Array(0.06127862, dtype=float32)), 'eval/episode_reward': (Array(-14324.908, dtype=float32), Array(4286.8765, dtype=float32)), 'eval/avg_episode_length': (Array(945.5703, dtype=float32), Array(226.29749, dtype=float32)), 'eval/epoch_eval_time': 4.110410213470459, 'eval/sps': 31140.44422635092}
I0726 23:17:27.017663 140267183036224 train.py:379] starting iteration 391 2839.27250957489
I0726 23:17:34.187704 140267183036224 train.py:394] {'eval/walltime': 1622.121149301529, 'training/sps': 40338.169764879276, 'training/walltime': 1216.3078970909119, 'training/entropy_loss': Array(-0.017971, dtype=float32), 'training/policy_loss': Array(0.00316938, dtype=float32), 'training/total_loss': Array(86.589676, dtype=float32), 'training/v_loss': Array(86.60448, dtype=float32), 'eval/episode_goal_distance': (Array(0.28836605, dtype=float32), Array(0.06085549, dtype=float32)), 'eval/episode_reward': (Array(-13943.823, dtype=float32), Array(4628.4033, dtype=float32)), 'eval/avg_episode_length': (Array(930.0703, dtype=float32), Array(254.28108, dtype=float32)), 'eval/epoch_eval_time': 4.120306015014648, 'eval/sps': 31065.653748425513}
I0726 23:17:34.190112 140267183036224 train.py:379] starting iteration 392 2846.4449586868286
I0726 23:17:41.351350 140267183036224 train.py:394] {'eval/walltime': 1626.2321078777313, 'training/sps': 40333.399918643925, 'training/walltime': 1219.3545036315918, 'training/entropy_loss': Array(-0.01683383, dtype=float32), 'training/policy_loss': Array(0.00316052, dtype=float32), 'training/total_loss': Array(131.1354, dtype=float32), 'training/v_loss': Array(131.14906, dtype=float32), 'eval/episode_goal_distance': (Array(0.28733796, dtype=float32), Array(0.06054305, dtype=float32)), 'eval/episode_reward': (Array(-13977.373, dtype=float32), Array(4504.066, dtype=float32)), 'eval/avg_episode_length': (Array(930.1719, dtype=float32), Array(253.91193, dtype=float32)), 'eval/epoch_eval_time': 4.110958576202393, 'eval/sps': 31136.290387592133}
I0726 23:17:41.353672 140267183036224 train.py:379] starting iteration 393 2853.6085181236267
I0726 23:17:48.517490 140267183036224 train.py:394] {'eval/walltime': 1630.345263004303, 'training/sps': 40326.30248102541, 'training/walltime': 1222.4016463756561, 'training/entropy_loss': Array(-0.0176292, dtype=float32), 'training/policy_loss': Array(0.00296881, dtype=float32), 'training/total_loss': Array(105.938416, dtype=float32), 'training/v_loss': Array(105.95307, dtype=float32), 'eval/episode_goal_distance': (Array(0.29898363, dtype=float32), Array(0.06413017, dtype=float32)), 'eval/episode_reward': (Array(-12720.066, dtype=float32), Array(6194.0894, dtype=float32)), 'eval/avg_episode_length': (Array(829.27344, dtype=float32), Array(374.7518, dtype=float32)), 'eval/epoch_eval_time': 4.113155126571655, 'eval/sps': 31119.662658259364}
I0726 23:17:48.519822 140267183036224 train.py:379] starting iteration 394 2860.7746686935425
I0726 23:17:55.695083 140267183036224 train.py:394] {'eval/walltime': 1634.468290090561, 'training/sps': 40306.37106872054, 'training/walltime': 1225.4502959251404, 'training/entropy_loss': Array(-0.01719071, dtype=float32), 'training/policy_loss': Array(0.00188542, dtype=float32), 'training/total_loss': Array(93.6689, dtype=float32), 'training/v_loss': Array(93.684204, dtype=float32), 'eval/episode_goal_distance': (Array(0.29898956, dtype=float32), Array(0.05666956, dtype=float32)), 'eval/episode_reward': (Array(-14078.002, dtype=float32), Array(5295.2725, dtype=float32)), 'eval/avg_episode_length': (Array(899.0625, dtype=float32), Array(300.2138, dtype=float32)), 'eval/epoch_eval_time': 4.123027086257935, 'eval/sps': 31045.15137109443}
I0726 23:17:55.697440 140267183036224 train.py:379] starting iteration 395 2867.95228600502
I0726 23:18:02.868099 140267183036224 train.py:394] {'eval/walltime': 1638.587986946106, 'training/sps': 40323.869919243676, 'training/walltime': 1228.4976224899292, 'training/entropy_loss': Array(-0.01758558, dtype=float32), 'training/policy_loss': Array(0.00169067, dtype=float32), 'training/total_loss': Array(104.14136, dtype=float32), 'training/v_loss': Array(104.15725, dtype=float32), 'eval/episode_goal_distance': (Array(0.29018974, dtype=float32), Array(0.05653071, dtype=float32)), 'eval/episode_reward': (Array(-14670.367, dtype=float32), Array(4049.2715, dtype=float32)), 'eval/avg_episode_length': (Array(953.4453, dtype=float32), Array(209.92705, dtype=float32)), 'eval/epoch_eval_time': 4.119696855545044, 'eval/sps': 31070.247275042606}
I0726 23:18:02.870423 140267183036224 train.py:379] starting iteration 396 2875.1252694129944
I0726 23:18:10.033357 140267183036224 train.py:394] {'eval/walltime': 1642.7008440494537, 'training/sps': 40335.072866638744, 'training/walltime': 1231.5441026687622, 'training/entropy_loss': Array(-0.01904036, dtype=float32), 'training/policy_loss': Array(0.0020618, dtype=float32), 'training/total_loss': Array(93.76926, dtype=float32), 'training/v_loss': Array(93.78624, dtype=float32), 'eval/episode_goal_distance': (Array(0.29483163, dtype=float32), Array(0.06118337, dtype=float32)), 'eval/episode_reward': (Array(-13993.435, dtype=float32), Array(5089.684, dtype=float32)), 'eval/avg_episode_length': (Array(906.78125, dtype=float32), Array(289.82947, dtype=float32)), 'eval/epoch_eval_time': 4.112857103347778, 'eval/sps': 31121.91763137375}
I0726 23:18:10.035893 140267183036224 train.py:379] starting iteration 397 2882.2907395362854
I0726 23:18:17.197439 140267183036224 train.py:394] {'eval/walltime': 1646.8140139579773, 'training/sps': 40357.91459028809, 'training/walltime': 1234.5888586044312, 'training/entropy_loss': Array(-0.01959116, dtype=float32), 'training/policy_loss': Array(0.00122878, dtype=float32), 'training/total_loss': Array(88.46238, dtype=float32), 'training/v_loss': Array(88.48074, dtype=float32), 'eval/episode_goal_distance': (Array(0.29919678, dtype=float32), Array(0.05711925, dtype=float32)), 'eval/episode_reward': (Array(-15199.202, dtype=float32), Array(4056.4678, dtype=float32)), 'eval/avg_episode_length': (Array(961.22656, dtype=float32), Array(192.3101, dtype=float32)), 'eval/epoch_eval_time': 4.11316990852356, 'eval/sps': 31119.55082009879}
I0726 23:18:17.199729 140267183036224 train.py:379] starting iteration 398 2889.4545755386353
I0726 23:18:24.356029 140267183036224 train.py:394] {'eval/walltime': 1650.921730041504, 'training/sps': 40355.174871798525, 'training/walltime': 1237.6338212490082, 'training/entropy_loss': Array(-0.02012058, dtype=float32), 'training/policy_loss': Array(0.00210284, dtype=float32), 'training/total_loss': Array(93.4521, dtype=float32), 'training/v_loss': Array(93.47012, dtype=float32), 'eval/episode_goal_distance': (Array(0.30433494, dtype=float32), Array(0.0547298, dtype=float32)), 'eval/episode_reward': (Array(-15420.866, dtype=float32), Array(4017.4956, dtype=float32)), 'eval/avg_episode_length': (Array(961.10156, dtype=float32), Array(192.93, dtype=float32)), 'eval/epoch_eval_time': 4.107716083526611, 'eval/sps': 31160.86832615455}
I0726 23:18:24.360806 140267183036224 train.py:379] starting iteration 399 2896.6156356334686
I0726 23:18:31.829978 140267183036224 train.py:394] {'eval/walltime': 1655.062961101532, 'training/sps': 36975.487914433164, 'training/walltime': 1240.9571039676666, 'training/entropy_loss': Array(-0.01991363, dtype=float32), 'training/policy_loss': Array(0.00220165, dtype=float32), 'training/total_loss': Array(87.2173, dtype=float32), 'training/v_loss': Array(87.235, dtype=float32), 'eval/episode_goal_distance': (Array(0.2839498, dtype=float32), Array(0.05850258, dtype=float32)), 'eval/episode_reward': (Array(-14393.175, dtype=float32), Array(3737.2207, dtype=float32)), 'eval/avg_episode_length': (Array(961.2031, dtype=float32), Array(192.42662, dtype=float32)), 'eval/epoch_eval_time': 4.141231060028076, 'eval/sps': 30908.68346745477}
I0726 23:18:31.832455 140267183036224 train.py:379] starting iteration 400 2904.087301492691
I0726 23:18:39.013775 140267183036224 train.py:394] {'eval/walltime': 1659.1810352802277, 'training/sps': 40161.89211486077, 'training/walltime': 1244.0167207717896, 'training/entropy_loss': Array(-0.02178895, dtype=float32), 'training/policy_loss': Array(0.00098235, dtype=float32), 'training/total_loss': Array(520.09, dtype=float32), 'training/v_loss': Array(520.11084, dtype=float32), 'eval/episode_goal_distance': (Array(0.28031448, dtype=float32), Array(0.05900721, dtype=float32)), 'eval/episode_reward': (Array(-13919.424, dtype=float32), Array(4767.7285, dtype=float32)), 'eval/avg_episode_length': (Array(922.3125, dtype=float32), Array(266.86575, dtype=float32)), 'eval/epoch_eval_time': 4.118074178695679, 'eval/sps': 31082.49012662068}
I0726 23:18:39.016406 140267183036224 train.py:379] starting iteration 401 2911.2712523937225
I0726 23:18:46.199299 140267183036224 train.py:394] {'eval/walltime': 1663.3036170005798, 'training/sps': 40200.162730000266, 'training/walltime': 1247.0734248161316, 'training/entropy_loss': Array(-0.02400267, dtype=float32), 'training/policy_loss': Array(0.00138799, dtype=float32), 'training/total_loss': Array(166.59207, dtype=float32), 'training/v_loss': Array(166.61469, dtype=float32), 'eval/episode_goal_distance': (Array(0.29292887, dtype=float32), Array(0.06231882, dtype=float32)), 'eval/episode_reward': (Array(-14770.961, dtype=float32), Array(4463.964, dtype=float32)), 'eval/avg_episode_length': (Array(945.65625, dtype=float32), Array(225.9403, dtype=float32)), 'eval/epoch_eval_time': 4.122581720352173, 'eval/sps': 31048.50520441971}
I0726 23:18:46.201823 140267183036224 train.py:379] starting iteration 402 2918.456669330597
I0726 23:18:53.379265 140267183036224 train.py:394] {'eval/walltime': 1667.4224119186401, 'training/sps': 40219.571779255144, 'training/walltime': 1250.1286537647247, 'training/entropy_loss': Array(-0.02566498, dtype=float32), 'training/policy_loss': Array(0.00072063, dtype=float32), 'training/total_loss': Array(177.94519, dtype=float32), 'training/v_loss': Array(177.97015, dtype=float32), 'eval/episode_goal_distance': (Array(0.29189318, dtype=float32), Array(0.06255707, dtype=float32)), 'eval/episode_reward': (Array(-14066.46, dtype=float32), Array(5315.0063, dtype=float32)), 'eval/avg_episode_length': (Array(914.5625, dtype=float32), Array(278.64157, dtype=float32)), 'eval/epoch_eval_time': 4.118794918060303, 'eval/sps': 31077.051066257038}
I0726 23:18:53.381622 140267183036224 train.py:379] starting iteration 403 2925.636468887329
I0726 23:19:00.570215 140267183036224 train.py:394] {'eval/walltime': 1671.5474836826324, 'training/sps': 40157.72393575183, 'training/walltime': 1253.188588142395, 'training/entropy_loss': Array(-0.02586894, dtype=float32), 'training/policy_loss': Array(0.00059607, dtype=float32), 'training/total_loss': Array(152.13797, dtype=float32), 'training/v_loss': Array(152.16324, dtype=float32), 'eval/episode_goal_distance': (Array(0.30206832, dtype=float32), Array(0.05506325, dtype=float32)), 'eval/episode_reward': (Array(-14462.391, dtype=float32), Array(4779.947, dtype=float32)), 'eval/avg_episode_length': (Array(922.27344, dtype=float32), Array(266.99966, dtype=float32)), 'eval/epoch_eval_time': 4.12507176399231, 'eval/sps': 31029.76319522732}
I0726 23:19:00.572731 140267183036224 train.py:379] starting iteration 404 2932.827577114105
I0726 23:19:07.750486 140267183036224 train.py:394] {'eval/walltime': 1675.665455341339, 'training/sps': 40206.53518909931, 'training/walltime': 1256.2448077201843, 'training/entropy_loss': Array(-0.02555983, dtype=float32), 'training/policy_loss': Array(0.000818, dtype=float32), 'training/total_loss': Array(162.57559, dtype=float32), 'training/v_loss': Array(162.60033, dtype=float32), 'eval/episode_goal_distance': (Array(0.29859388, dtype=float32), Array(0.06215644, dtype=float32)), 'eval/episode_reward': (Array(-14060.641, dtype=float32), Array(5336.4385, dtype=float32)), 'eval/avg_episode_length': (Array(899.125, dtype=float32), Array(300.02798, dtype=float32)), 'eval/epoch_eval_time': 4.117971658706665, 'eval/sps': 31083.263948494747}
I0726 23:19:07.753031 140267183036224 train.py:379] starting iteration 405 2940.007876634598
I0726 23:19:14.937352 140267183036224 train.py:394] {'eval/walltime': 1679.794596672058, 'training/sps': 40265.14538570342, 'training/walltime': 1259.2965786457062, 'training/entropy_loss': Array(-0.0243192, dtype=float32), 'training/policy_loss': Array(0.00026221, dtype=float32), 'training/total_loss': Array(109.74291, dtype=float32), 'training/v_loss': Array(109.76697, dtype=float32), 'eval/episode_goal_distance': (Array(0.29025164, dtype=float32), Array(0.05816951, dtype=float32)), 'eval/episode_reward': (Array(-14011.592, dtype=float32), Array(5084.2983, dtype=float32)), 'eval/avg_episode_length': (Array(914.6719, dtype=float32), Array(278.28494, dtype=float32)), 'eval/epoch_eval_time': 4.129141330718994, 'eval/sps': 30999.181124592742}
I0726 23:19:14.939739 140267183036224 train.py:379] starting iteration 406 2947.194585800171
I0726 23:19:22.100544 140267183036224 train.py:394] {'eval/walltime': 1683.9063923358917, 'training/sps': 40349.31116531248, 'training/walltime': 1262.341983795166, 'training/entropy_loss': Array(-0.02253826, dtype=float32), 'training/policy_loss': Array(0.00053126, dtype=float32), 'training/total_loss': Array(98.838875, dtype=float32), 'training/v_loss': Array(98.860886, dtype=float32), 'eval/episode_goal_distance': (Array(0.28763723, dtype=float32), Array(0.06122696, dtype=float32)), 'eval/episode_reward': (Array(-13654.816, dtype=float32), Array(4865.6187, dtype=float32)), 'eval/avg_episode_length': (Array(914.60156, dtype=float32), Array(278.51422, dtype=float32)), 'eval/epoch_eval_time': 4.111795663833618, 'eval/sps': 31129.951598971154}
I0726 23:19:22.102968 140267183036224 train.py:379] starting iteration 407 2954.3578145504
I0726 23:19:29.305605 140267183036224 train.py:394] {'eval/walltime': 1688.0557661056519, 'training/sps': 40294.86586961345, 'training/walltime': 1265.3915038108826, 'training/entropy_loss': Array(-0.01883774, dtype=float32), 'training/policy_loss': Array(-0.00040395, dtype=float32), 'training/total_loss': Array(100.04499, dtype=float32), 'training/v_loss': Array(100.064224, dtype=float32), 'eval/episode_goal_distance': (Array(0.28420877, dtype=float32), Array(0.06348963, dtype=float32)), 'eval/episode_reward': (Array(-13898.238, dtype=float32), Array(4512.8765, dtype=float32)), 'eval/avg_episode_length': (Array(937.89844, dtype=float32), Array(240.51903, dtype=float32)), 'eval/epoch_eval_time': 4.149373769760132, 'eval/sps': 30848.028426082103}
I0726 23:19:29.308107 140267183036224 train.py:379] starting iteration 408 2961.562953710556
I0726 23:19:36.484860 140267183036224 train.py:394] {'eval/walltime': 1692.1805129051208, 'training/sps': 40310.11616581709, 'training/walltime': 1268.4398701190948, 'training/entropy_loss': Array(-0.01843245, dtype=float32), 'training/policy_loss': Array(0.00041647, dtype=float32), 'training/total_loss': Array(83.81681, dtype=float32), 'training/v_loss': Array(83.83482, dtype=float32), 'eval/episode_goal_distance': (Array(0.29322246, dtype=float32), Array(0.05772181, dtype=float32)), 'eval/episode_reward': (Array(-13920.388, dtype=float32), Array(5288.6123, dtype=float32)), 'eval/avg_episode_length': (Array(898.96094, dtype=float32), Array(300.5159, dtype=float32)), 'eval/epoch_eval_time': 4.124746799468994, 'eval/sps': 31032.20784763765}
I0726 23:19:36.659650 140267183036224 train.py:379] starting iteration 409 2968.9144852161407
I0726 23:19:43.826704 140267183036224 train.py:394] {'eval/walltime': 1696.290771484375, 'training/sps': 40250.92869299225, 'training/walltime': 1271.4927189350128, 'training/entropy_loss': Array(-0.01877042, dtype=float32), 'training/policy_loss': Array(0.00052329, dtype=float32), 'training/total_loss': Array(114.800125, dtype=float32), 'training/v_loss': Array(114.818375, dtype=float32), 'eval/episode_goal_distance': (Array(0.2904151, dtype=float32), Array(0.06073658, dtype=float32)), 'eval/episode_reward': (Array(-14664.55, dtype=float32), Array(3852.0083, dtype=float32)), 'eval/avg_episode_length': (Array(968.9453, dtype=float32), Array(172.90533, dtype=float32)), 'eval/epoch_eval_time': 4.11025857925415, 'eval/sps': 31141.59304868526}
I0726 23:19:43.829420 140267183036224 train.py:379] starting iteration 410 2976.0842666625977
I0726 23:19:50.997969 140267183036224 train.py:394] {'eval/walltime': 1700.4087083339691, 'training/sps': 40328.318795518375, 'training/walltime': 1274.539709329605, 'training/entropy_loss': Array(-0.01890623, dtype=float32), 'training/policy_loss': Array(6.454391e-05, dtype=float32), 'training/total_loss': Array(124.45465, dtype=float32), 'training/v_loss': Array(124.47349, dtype=float32), 'eval/episode_goal_distance': (Array(0.28934044, dtype=float32), Array(0.05418763, dtype=float32)), 'eval/episode_reward': (Array(-14189.336, dtype=float32), Array(4517.208, dtype=float32)), 'eval/avg_episode_length': (Array(930.1719, dtype=float32), Array(253.9122, dtype=float32)), 'eval/epoch_eval_time': 4.117936849594116, 'eval/sps': 31083.526696776884}
I0726 23:19:51.000424 140267183036224 train.py:379] starting iteration 411 2983.255270719528
I0726 23:19:58.163746 140267183036224 train.py:394] {'eval/walltime': 1704.5277953147888, 'training/sps': 40412.95513398596, 'training/walltime': 1277.5803184509277, 'training/entropy_loss': Array(-0.01892577, dtype=float32), 'training/policy_loss': Array(0.00045303, dtype=float32), 'training/total_loss': Array(108.780365, dtype=float32), 'training/v_loss': Array(108.798836, dtype=float32), 'eval/episode_goal_distance': (Array(0.28630847, dtype=float32), Array(0.05858703, dtype=float32)), 'eval/episode_reward': (Array(-14328.859, dtype=float32), Array(4456.75, dtype=float32)), 'eval/avg_episode_length': (Array(945.6797, dtype=float32), Array(225.84283, dtype=float32)), 'eval/epoch_eval_time': 4.119086980819702, 'eval/sps': 31074.847556272744}
I0726 23:19:58.166377 140267183036224 train.py:379] starting iteration 412 2990.4212234020233
I0726 23:20:05.329909 140267183036224 train.py:394] {'eval/walltime': 1708.6464974880219, 'training/sps': 40403.97347334297, 'training/walltime': 1280.6216034889221, 'training/entropy_loss': Array(-0.01900569, dtype=float32), 'training/policy_loss': Array(0.00035592, dtype=float32), 'training/total_loss': Array(88.49793, dtype=float32), 'training/v_loss': Array(88.51659, dtype=float32), 'eval/episode_goal_distance': (Array(0.2953375, dtype=float32), Array(0.06256115, dtype=float32)), 'eval/episode_reward': (Array(-13479.662, dtype=float32), Array(5079.7583, dtype=float32)), 'eval/avg_episode_length': (Array(899.0156, dtype=float32), Array(300.35324, dtype=float32)), 'eval/epoch_eval_time': 4.118702173233032, 'eval/sps': 31077.75085847604}
I0726 23:20:05.332227 140267183036224 train.py:379] starting iteration 413 2997.587073326111
I0726 23:20:12.493818 140267183036224 train.py:394] {'eval/walltime': 1712.763929605484, 'training/sps': 40415.87701434963, 'training/walltime': 1283.6619927883148, 'training/entropy_loss': Array(-0.02107608, dtype=float32), 'training/policy_loss': Array(0.00059052, dtype=float32), 'training/total_loss': Array(76.167854, dtype=float32), 'training/v_loss': Array(76.18834, dtype=float32), 'eval/episode_goal_distance': (Array(0.28553328, dtype=float32), Array(0.05197707, dtype=float32)), 'eval/episode_reward': (Array(-14212.906, dtype=float32), Array(4414.941, dtype=float32)), 'eval/avg_episode_length': (Array(937.9375, dtype=float32), Array(240.36761, dtype=float32)), 'eval/epoch_eval_time': 4.117432117462158, 'eval/sps': 31087.33704610405}
I0726 23:20:12.496362 140267183036224 train.py:379] starting iteration 414 3004.751208305359
I0726 23:20:19.664577 140267183036224 train.py:394] {'eval/walltime': 1716.8885793685913, 'training/sps': 40421.17360328253, 'training/walltime': 1286.7019836902618, 'training/entropy_loss': Array(-0.02015579, dtype=float32), 'training/policy_loss': Array(0.00049922, dtype=float32), 'training/total_loss': Array(67.121925, dtype=float32), 'training/v_loss': Array(67.14159, dtype=float32), 'eval/episode_goal_distance': (Array(0.28852233, dtype=float32), Array(0.06198618, dtype=float32)), 'eval/episode_reward': (Array(-14188.278, dtype=float32), Array(4482.1763, dtype=float32)), 'eval/avg_episode_length': (Array(937.875, dtype=float32), Array(240.6093, dtype=float32)), 'eval/epoch_eval_time': 4.1246497631073, 'eval/sps': 31032.93791024122}
I0726 23:20:19.666978 140267183036224 train.py:379] starting iteration 415 3011.921823978424
I0726 23:20:26.830761 140267183036224 train.py:394] {'eval/walltime': 1721.0079610347748, 'training/sps': 40410.53745483182, 'training/walltime': 1289.7427747249603, 'training/entropy_loss': Array(-0.01869609, dtype=float32), 'training/policy_loss': Array(8.438085e-05, dtype=float32), 'training/total_loss': Array(72.946304, dtype=float32), 'training/v_loss': Array(72.96492, dtype=float32), 'eval/episode_goal_distance': (Array(0.28994226, dtype=float32), Array(0.05766306, dtype=float32)), 'eval/episode_reward': (Array(-13930.774, dtype=float32), Array(4787.031, dtype=float32)), 'eval/avg_episode_length': (Array(922.28125, dtype=float32), Array(266.97308, dtype=float32)), 'eval/epoch_eval_time': 4.119381666183472, 'eval/sps': 31072.62457634559}
I0726 23:20:26.833223 140267183036224 train.py:379] starting iteration 416 3019.088068962097
I0726 23:20:33.999205 140267183036224 train.py:394] {'eval/walltime': 1725.1261792182922, 'training/sps': 40367.10974384391, 'training/walltime': 1292.7868371009827, 'training/entropy_loss': Array(-0.01883435, dtype=float32), 'training/policy_loss': Array(0.00044984, dtype=float32), 'training/total_loss': Array(390.8747, dtype=float32), 'training/v_loss': Array(390.89313, dtype=float32), 'eval/episode_goal_distance': (Array(0.29776, dtype=float32), Array(0.05539759, dtype=float32)), 'eval/episode_reward': (Array(-14683.479, dtype=float32), Array(4416.342, dtype=float32)), 'eval/avg_episode_length': (Array(937.85156, dtype=float32), Array(240.70018, dtype=float32)), 'eval/epoch_eval_time': 4.118218183517456, 'eval/sps': 31081.403241892476}
I0726 23:20:34.001595 140267183036224 train.py:379] starting iteration 417 3026.2564408779144
I0726 23:20:41.172813 140267183036224 train.py:394] {'eval/walltime': 1729.246694803238, 'training/sps': 40326.77577617702, 'training/walltime': 1295.8339440822601, 'training/entropy_loss': Array(-0.02187236, dtype=float32), 'training/policy_loss': Array(0.00042838, dtype=float32), 'training/total_loss': Array(217.77548, dtype=float32), 'training/v_loss': Array(217.7969, dtype=float32), 'eval/episode_goal_distance': (Array(0.28244308, dtype=float32), Array(0.05497488, dtype=float32)), 'eval/episode_reward': (Array(-13234.061, dtype=float32), Array(5355.5684, dtype=float32)), 'eval/avg_episode_length': (Array(883.5625, dtype=float32), Array(319.586, dtype=float32)), 'eval/epoch_eval_time': 4.120515584945679, 'eval/sps': 31064.073745443056}
I0726 23:20:41.175226 140267183036224 train.py:379] starting iteration 418 3033.4300723075867
I0726 23:20:48.335761 140267183036224 train.py:394] {'eval/walltime': 1733.3612349033356, 'training/sps': 40390.43408056989, 'training/walltime': 1298.8762485980988, 'training/entropy_loss': Array(-0.02240183, dtype=float32), 'training/policy_loss': Array(0.00032073, dtype=float32), 'training/total_loss': Array(157.4339, dtype=float32), 'training/v_loss': Array(157.45596, dtype=float32), 'eval/episode_goal_distance': (Array(0.29288548, dtype=float32), Array(0.05693155, dtype=float32)), 'eval/episode_reward': (Array(-14363.066, dtype=float32), Array(4673.9336, dtype=float32)), 'eval/avg_episode_length': (Array(930.21094, dtype=float32), Array(253.76993, dtype=float32)), 'eval/epoch_eval_time': 4.114540100097656, 'eval/sps': 31109.18763362204}
I0726 23:20:48.338164 140267183036224 train.py:379] starting iteration 419 3040.593010902405
I0726 23:20:55.500474 140267183036224 train.py:394] {'eval/walltime': 1737.4798543453217, 'training/sps': 40421.262367047624, 'training/walltime': 1301.9162328243256, 'training/entropy_loss': Array(-0.02199517, dtype=float32), 'training/policy_loss': Array(0.00014965, dtype=float32), 'training/total_loss': Array(132.45557, dtype=float32), 'training/v_loss': Array(132.47742, dtype=float32), 'eval/episode_goal_distance': (Array(0.29575557, dtype=float32), Array(0.05939807, dtype=float32)), 'eval/episode_reward': (Array(-14072.187, dtype=float32), Array(4749.0127, dtype=float32)), 'eval/avg_episode_length': (Array(922.41406, dtype=float32), Array(266.5167, dtype=float32)), 'eval/epoch_eval_time': 4.118619441986084, 'eval/sps': 31078.375121318742}
I0726 23:20:55.502865 140267183036224 train.py:379] starting iteration 420 3047.7577114105225
I0726 23:21:02.665451 140267183036224 train.py:394] {'eval/walltime': 1741.5955004692078, 'training/sps': 40376.97967551201, 'training/walltime': 1304.9595510959625, 'training/entropy_loss': Array(-0.02189002, dtype=float32), 'training/policy_loss': Array(-0.00010376, dtype=float32), 'training/total_loss': Array(128.03394, dtype=float32), 'training/v_loss': Array(128.05591, dtype=float32), 'eval/episode_goal_distance': (Array(0.28845397, dtype=float32), Array(0.05993097, dtype=float32)), 'eval/episode_reward': (Array(-13858.332, dtype=float32), Array(5069.44, dtype=float32)), 'eval/avg_episode_length': (Array(906.7344, dtype=float32), Array(289.97485, dtype=float32)), 'eval/epoch_eval_time': 4.115646123886108, 'eval/sps': 31100.827463547525}
I0726 23:21:02.667842 140267183036224 train.py:379] starting iteration 421 3054.922688961029
I0726 23:21:09.837486 140267183036224 train.py:394] {'eval/walltime': 1745.7187628746033, 'training/sps': 40385.825918369235, 'training/walltime': 1308.0022027492523, 'training/entropy_loss': Array(-0.02239167, dtype=float32), 'training/policy_loss': Array(0.00020949, dtype=float32), 'training/total_loss': Array(155.54745, dtype=float32), 'training/v_loss': Array(155.56964, dtype=float32), 'eval/episode_goal_distance': (Array(0.30296057, dtype=float32), Array(0.06453305, dtype=float32)), 'eval/episode_reward': (Array(-14068.979, dtype=float32), Array(5343.7583, dtype=float32)), 'eval/avg_episode_length': (Array(899.10156, dtype=float32), Array(300.0976, dtype=float32)), 'eval/epoch_eval_time': 4.123262405395508, 'eval/sps': 31043.37959003172}
I0726 23:21:09.840065 140267183036224 train.py:379] starting iteration 422 3062.09491109848
I0726 23:21:17.005872 140267183036224 train.py:394] {'eval/walltime': 1749.837214231491, 'training/sps': 40372.56433222064, 'training/walltime': 1311.0458538532257, 'training/entropy_loss': Array(-0.02108215, dtype=float32), 'training/policy_loss': Array(5.563254e-05, dtype=float32), 'training/total_loss': Array(111.76346, dtype=float32), 'training/v_loss': Array(111.78448, dtype=float32), 'eval/episode_goal_distance': (Array(0.29338574, dtype=float32), Array(0.05626413, dtype=float32)), 'eval/episode_reward': (Array(-14361.772, dtype=float32), Array(4341.968, dtype=float32)), 'eval/avg_episode_length': (Array(937.8672, dtype=float32), Array(240.63991, dtype=float32)), 'eval/epoch_eval_time': 4.118451356887817, 'eval/sps': 31079.64351355737}
I0726 23:21:17.008466 140267183036224 train.py:379] starting iteration 423 3069.2633123397827
I0726 23:21:24.187852 140267183036224 train.py:394] {'eval/walltime': 1753.963892698288, 'training/sps': 40300.178054016724, 'training/walltime': 1314.094971895218, 'training/entropy_loss': Array(-0.0197193, dtype=float32), 'training/policy_loss': Array(-2.7682829e-05, dtype=float32), 'training/total_loss': Array(81.053764, dtype=float32), 'training/v_loss': Array(81.07351, dtype=float32), 'eval/episode_goal_distance': (Array(0.28677663, dtype=float32), Array(0.06105789, dtype=float32)), 'eval/episode_reward': (Array(-14143.302, dtype=float32), Array(4493.8423, dtype=float32)), 'eval/avg_episode_length': (Array(937.875, dtype=float32), Array(240.6094, dtype=float32)), 'eval/epoch_eval_time': 4.126678466796875, 'eval/sps': 31017.681903226523}
I0726 23:21:24.190211 140267183036224 train.py:379] starting iteration 424 3076.4450573921204
I0726 23:21:31.359710 140267183036224 train.py:394] {'eval/walltime': 1758.0790123939514, 'training/sps': 40279.07936985034, 'training/walltime': 1317.1456871032715, 'training/entropy_loss': Array(-0.01915338, dtype=float32), 'training/policy_loss': Array(6.0893664e-05, dtype=float32), 'training/total_loss': Array(79.16603, dtype=float32), 'training/v_loss': Array(79.18512, dtype=float32), 'eval/episode_goal_distance': (Array(0.28089726, dtype=float32), Array(0.06095186, dtype=float32)), 'eval/episode_reward': (Array(-13188.093, dtype=float32), Array(5030.4995, dtype=float32)), 'eval/avg_episode_length': (Array(898.9922, dtype=float32), Array(300.4227, dtype=float32)), 'eval/epoch_eval_time': 4.115119695663452, 'eval/sps': 31104.80604850631}
I0726 23:21:31.362677 140267183036224 train.py:379] starting iteration 425 3083.6175241470337
I0726 23:21:38.541095 140267183036224 train.py:394] {'eval/walltime': 1762.202453136444, 'training/sps': 40315.277848220976, 'training/walltime': 1320.1936631202698, 'training/entropy_loss': Array(-0.02044732, dtype=float32), 'training/policy_loss': Array(0.00029093, dtype=float32), 'training/total_loss': Array(77.95464, dtype=float32), 'training/v_loss': Array(77.97479, dtype=float32), 'eval/episode_goal_distance': (Array(0.2973488, dtype=float32), Array(0.05556768, dtype=float32)), 'eval/episode_reward': (Array(-13955.591, dtype=float32), Array(5189.52, dtype=float32)), 'eval/avg_episode_length': (Array(899.0703, dtype=float32), Array(300.19025, dtype=float32)), 'eval/epoch_eval_time': 4.123440742492676, 'eval/sps': 31042.036976775435}
I0726 23:21:38.543699 140267183036224 train.py:379] starting iteration 426 3090.7985458374023
I0726 23:21:45.726903 140267183036224 train.py:394] {'eval/walltime': 1766.3205769062042, 'training/sps': 40154.845518364906, 'training/walltime': 1323.2538168430328, 'training/entropy_loss': Array(-0.02220075, dtype=float32), 'training/policy_loss': Array(0.00043833, dtype=float32), 'training/total_loss': Array(91.16429, dtype=float32), 'training/v_loss': Array(91.18605, dtype=float32), 'eval/episode_goal_distance': (Array(0.28696162, dtype=float32), Array(0.05704145, dtype=float32)), 'eval/episode_reward': (Array(-13825.863, dtype=float32), Array(4661.2744, dtype=float32)), 'eval/avg_episode_length': (Array(922.3594, dtype=float32), Array(266.7047, dtype=float32)), 'eval/epoch_eval_time': 4.118123769760132, 'eval/sps': 31082.115826609945}
I0726 23:21:45.729398 140267183036224 train.py:379] starting iteration 427 3097.9842443466187
I0726 23:21:52.922875 140267183036224 train.py:394] {'eval/walltime': 1770.4438366889954, 'training/sps': 40102.89852792297, 'training/walltime': 1326.317934513092, 'training/entropy_loss': Array(-0.02226781, dtype=float32), 'training/policy_loss': Array(0.00039524, dtype=float32), 'training/total_loss': Array(92.81688, dtype=float32), 'training/v_loss': Array(92.838745, dtype=float32), 'eval/episode_goal_distance': (Array(0.29477757, dtype=float32), Array(0.05625978, dtype=float32)), 'eval/episode_reward': (Array(-14044.042, dtype=float32), Array(4514.968, dtype=float32)), 'eval/avg_episode_length': (Array(930.03125, dtype=float32), Array(254.42325, dtype=float32)), 'eval/epoch_eval_time': 4.123259782791138, 'eval/sps': 31043.39933521084}
I0726 23:21:52.926017 140267183036224 train.py:379] starting iteration 428 3105.180848121643
I0726 23:22:00.113664 140267183036224 train.py:394] {'eval/walltime': 1774.5608808994293, 'training/sps': 40092.360582888556, 'training/walltime': 1329.3828575611115, 'training/entropy_loss': Array(-0.02162551, dtype=float32), 'training/policy_loss': Array(0.00030032, dtype=float32), 'training/total_loss': Array(101.877525, dtype=float32), 'training/v_loss': Array(101.89885, dtype=float32), 'eval/episode_goal_distance': (Array(0.28411508, dtype=float32), Array(0.06196636, dtype=float32)), 'eval/episode_reward': (Array(-13503.873, dtype=float32), Array(5444.41, dtype=float32)), 'eval/avg_episode_length': (Array(891.25, dtype=float32), Array(310.32632, dtype=float32)), 'eval/epoch_eval_time': 4.11704421043396, 'eval/sps': 31090.266088376076}
I0726 23:22:00.115819 140267183036224 train.py:379] starting iteration 429 3112.3706653118134
I0726 23:22:07.301190 140267183036224 train.py:394] {'eval/walltime': 1778.6857364177704, 'training/sps': 40197.06817479353, 'training/walltime': 1332.439796924591, 'training/entropy_loss': Array(-0.02117833, dtype=float32), 'training/policy_loss': Array(-5.1526797e-05, dtype=float32), 'training/total_loss': Array(87.39317, dtype=float32), 'training/v_loss': Array(87.41441, dtype=float32), 'eval/episode_goal_distance': (Array(0.2945618, dtype=float32), Array(0.05323039, dtype=float32)), 'eval/episode_reward': (Array(-14532.645, dtype=float32), Array(4137.5254, dtype=float32)), 'eval/avg_episode_length': (Array(945.6172, dtype=float32), Array(226.10268, dtype=float32)), 'eval/epoch_eval_time': 4.1248555183410645, 'eval/sps': 31031.38993132033}
I0726 23:22:07.303400 140267183036224 train.py:379] starting iteration 430 3119.558245420456
I0726 23:22:14.497299 140267183036224 train.py:394] {'eval/walltime': 1782.8192021846771, 'training/sps': 40198.94303585447, 'training/walltime': 1335.4965937137604, 'training/entropy_loss': Array(-0.0213267, dtype=float32), 'training/policy_loss': Array(5.3226373e-05, dtype=float32), 'training/total_loss': Array(82.07617, dtype=float32), 'training/v_loss': Array(82.09745, dtype=float32), 'eval/episode_goal_distance': (Array(0.28969026, dtype=float32), Array(0.06039866, dtype=float32)), 'eval/episode_reward': (Array(-14197.842, dtype=float32), Array(4737.1753, dtype=float32)), 'eval/avg_episode_length': (Array(922.3906, dtype=float32), Array(266.5974, dtype=float32)), 'eval/epoch_eval_time': 4.133465766906738, 'eval/sps': 30966.749749034032}
I0726 23:22:14.499485 140267183036224 train.py:379] starting iteration 431 3126.754331588745
I0726 23:22:21.697370 140267183036224 train.py:394] {'eval/walltime': 1786.9529075622559, 'training/sps': 40149.293215933016, 'training/walltime': 1338.5571706295013, 'training/entropy_loss': Array(-0.0216587, dtype=float32), 'training/policy_loss': Array(0.0001921, dtype=float32), 'training/total_loss': Array(93.845505, dtype=float32), 'training/v_loss': Array(93.866974, dtype=float32), 'eval/episode_goal_distance': (Array(0.2890306, dtype=float32), Array(0.05688711, dtype=float32)), 'eval/episode_reward': (Array(-14196.201, dtype=float32), Array(4866.0903, dtype=float32)), 'eval/avg_episode_length': (Array(922.25, dtype=float32), Array(267.08026, dtype=float32)), 'eval/epoch_eval_time': 4.133705377578735, 'eval/sps': 30964.95475809027}
I0726 23:22:21.699690 140267183036224 train.py:379] starting iteration 432 3133.9545369148254
I0726 23:22:28.904661 140267183036224 train.py:394] {'eval/walltime': 1791.1013152599335, 'training/sps': 40249.34444142481, 'training/walltime': 1341.6101396083832, 'training/entropy_loss': Array(-0.02044744, dtype=float32), 'training/policy_loss': Array(0.00030443, dtype=float32), 'training/total_loss': Array(71.79178, dtype=float32), 'training/v_loss': Array(71.81192, dtype=float32), 'eval/episode_goal_distance': (Array(0.2986688, dtype=float32), Array(0.0509761, dtype=float32)), 'eval/episode_reward': (Array(-13875.785, dtype=float32), Array(4934.3584, dtype=float32)), 'eval/avg_episode_length': (Array(906.78125, dtype=float32), Array(289.8294, dtype=float32)), 'eval/epoch_eval_time': 4.148407697677612, 'eval/sps': 30855.21224725761}
I0726 23:22:28.907034 140267183036224 train.py:379] starting iteration 433 3141.161880493164
I0726 23:22:36.079623 140267183036224 train.py:394] {'eval/walltime': 1795.225084066391, 'training/sps': 40349.63969000531, 'training/walltime': 1344.6555199623108, 'training/entropy_loss': Array(-0.02117934, dtype=float32), 'training/policy_loss': Array(0.0003221, dtype=float32), 'training/total_loss': Array(497.4024, dtype=float32), 'training/v_loss': Array(497.42328, dtype=float32), 'eval/episode_goal_distance': (Array(0.2979858, dtype=float32), Array(0.06088398, dtype=float32)), 'eval/episode_reward': (Array(-13971.951, dtype=float32), Array(5348.8013, dtype=float32)), 'eval/avg_episode_length': (Array(899.10156, dtype=float32), Array(300.09756, dtype=float32)), 'eval/epoch_eval_time': 4.1237688064575195, 'eval/sps': 31039.567446060842}
I0726 23:22:36.081771 140267183036224 train.py:379] starting iteration 434 3148.336617708206
I0726 23:22:43.257766 140267183036224 train.py:394] {'eval/walltime': 1799.3510570526123, 'training/sps': 40334.91187881591, 'training/walltime': 1347.7020123004913, 'training/entropy_loss': Array(-0.02287449, dtype=float32), 'training/policy_loss': Array(0.00062528, dtype=float32), 'training/total_loss': Array(205.0972, dtype=float32), 'training/v_loss': Array(205.11945, dtype=float32), 'eval/episode_goal_distance': (Array(0.29047877, dtype=float32), Array(0.05882506, dtype=float32)), 'eval/episode_reward': (Array(-13696.676, dtype=float32), Array(5058.1294, dtype=float32)), 'eval/avg_episode_length': (Array(906.7656, dtype=float32), Array(289.87784, dtype=float32)), 'eval/epoch_eval_time': 4.1259729862213135, 'eval/sps': 31022.985469719748}
I0726 23:22:43.260025 140267183036224 train.py:379] starting iteration 435 3155.5148708820343
I0726 23:22:50.439260 140267183036224 train.py:394] {'eval/walltime': 1803.473867893219, 'training/sps': 40249.77192559374, 'training/walltime': 1350.7549488544464, 'training/entropy_loss': Array(-0.02399583, dtype=float32), 'training/policy_loss': Array(-6.347965e-05, dtype=float32), 'training/total_loss': Array(135.64105, dtype=float32), 'training/v_loss': Array(135.6651, dtype=float32), 'eval/episode_goal_distance': (Array(0.29585493, dtype=float32), Array(0.06233662, dtype=float32)), 'eval/episode_reward': (Array(-13766.248, dtype=float32), Array(5060.613, dtype=float32)), 'eval/avg_episode_length': (Array(899.0156, dtype=float32), Array(300.35315, dtype=float32)), 'eval/epoch_eval_time': 4.1228108406066895, 'eval/sps': 31046.779721080835}
I0726 23:22:50.441431 140267183036224 train.py:379] starting iteration 436 3162.696276664734
I0726 23:22:57.628221 140267183036224 train.py:394] {'eval/walltime': 1807.594290971756, 'training/sps': 40121.13932448067, 'training/walltime': 1353.817673444748, 'training/entropy_loss': Array(-0.02509189, dtype=float32), 'training/policy_loss': Array(-3.614344e-05, dtype=float32), 'training/total_loss': Array(124.4367, dtype=float32), 'training/v_loss': Array(124.46183, dtype=float32), 'eval/episode_goal_distance': (Array(0.2948969, dtype=float32), Array(0.05911762, dtype=float32)), 'eval/episode_reward': (Array(-14003.633, dtype=float32), Array(4782.0635, dtype=float32)), 'eval/avg_episode_length': (Array(922.3281, dtype=float32), Array(266.81213, dtype=float32)), 'eval/epoch_eval_time': 4.120423078536987, 'eval/sps': 31064.771155841638}
I0726 23:22:57.630514 140267183036224 train.py:379] starting iteration 437 3169.8853602409363
I0726 23:23:04.827186 140267183036224 train.py:394] {'eval/walltime': 1811.725830078125, 'training/sps': 40135.058187322196, 'training/walltime': 1356.8793358802795, 'training/entropy_loss': Array(-0.02439001, dtype=float32), 'training/policy_loss': Array(0.00026025, dtype=float32), 'training/total_loss': Array(142.65004, dtype=float32), 'training/v_loss': Array(142.67418, dtype=float32), 'eval/episode_goal_distance': (Array(0.27991766, dtype=float32), Array(0.05915912, dtype=float32)), 'eval/episode_reward': (Array(-13930.897, dtype=float32), Array(4556.0566, dtype=float32)), 'eval/avg_episode_length': (Array(930.0781, dtype=float32), Array(254.25294, dtype=float32)), 'eval/epoch_eval_time': 4.1315391063690186, 'eval/sps': 30981.190472741797}
I0726 23:23:04.829386 140267183036224 train.py:379] starting iteration 438 3177.084232568741
I0726 23:23:12.022065 140267183036224 train.py:394] {'eval/walltime': 1815.8547115325928, 'training/sps': 40153.86632478228, 'training/walltime': 1359.9395642280579, 'training/entropy_loss': Array(-0.02529837, dtype=float32), 'training/policy_loss': Array(0.0003619, dtype=float32), 'training/total_loss': Array(127.07168, dtype=float32), 'training/v_loss': Array(127.09662, dtype=float32), 'eval/episode_goal_distance': (Array(0.29723477, dtype=float32), Array(0.0550832, dtype=float32)), 'eval/episode_reward': (Array(-14724.11, dtype=float32), Array(4033.086, dtype=float32)), 'eval/avg_episode_length': (Array(953.3828, dtype=float32), Array(210.20868, dtype=float32)), 'eval/epoch_eval_time': 4.128881454467773, 'eval/sps': 31001.132246481422}
I0726 23:23:12.026775 140267183036224 train.py:379] starting iteration 439 3184.281605243683
I0726 23:23:19.219103 140267183036224 train.py:394] {'eval/walltime': 1819.9814803600311, 'training/sps': 40136.98665656978, 'training/walltime': 1363.0010795593262, 'training/entropy_loss': Array(-0.02559844, dtype=float32), 'training/policy_loss': Array(-0.00056692, dtype=float32), 'training/total_loss': Array(113.510765, dtype=float32), 'training/v_loss': Array(113.53693, dtype=float32), 'eval/episode_goal_distance': (Array(0.28977683, dtype=float32), Array(0.05851753, dtype=float32)), 'eval/episode_reward': (Array(-14376.491, dtype=float32), Array(4698.047, dtype=float32)), 'eval/avg_episode_length': (Array(930.1328, dtype=float32), Array(254.05441, dtype=float32)), 'eval/epoch_eval_time': 4.1267688274383545, 'eval/sps': 31017.00273321454}
I0726 23:23:19.221560 140267183036224 train.py:379] starting iteration 440 3191.4764065742493
I0726 23:23:26.407135 140267183036224 train.py:394] {'eval/walltime': 1824.1085686683655, 'training/sps': 40224.54705051302, 'training/walltime': 1366.0559306144714, 'training/entropy_loss': Array(-0.02338952, dtype=float32), 'training/policy_loss': Array(-0.00049605, dtype=float32), 'training/total_loss': Array(117.307236, dtype=float32), 'training/v_loss': Array(117.33111, dtype=float32), 'eval/episode_goal_distance': (Array(0.2955373, dtype=float32), Array(0.06128698, dtype=float32)), 'eval/episode_reward': (Array(-13663.891, dtype=float32), Array(4956.207, dtype=float32)), 'eval/avg_episode_length': (Array(906.75, dtype=float32), Array(289.92645, dtype=float32)), 'eval/epoch_eval_time': 4.127088308334351, 'eval/sps': 31014.60168456135}
I0726 23:23:26.409397 140267183036224 train.py:379] starting iteration 441 3198.664243221283
I0726 23:23:33.601439 140267183036224 train.py:394] {'eval/walltime': 1828.241601228714, 'training/sps': 40217.255637540045, 'training/walltime': 1369.111335515976, 'training/entropy_loss': Array(-0.02221376, dtype=float32), 'training/policy_loss': Array(-0.00012407, dtype=float32), 'training/total_loss': Array(92.73495, dtype=float32), 'training/v_loss': Array(92.757286, dtype=float32), 'eval/episode_goal_distance': (Array(0.2885518, dtype=float32), Array(0.06559671, dtype=float32)), 'eval/episode_reward': (Array(-13855.025, dtype=float32), Array(5384.8613, dtype=float32)), 'eval/avg_episode_length': (Array(899.0703, dtype=float32), Array(300.19046, dtype=float32)), 'eval/epoch_eval_time': 4.133032560348511, 'eval/sps': 30969.99554951646}
I0726 23:23:33.603623 140267183036224 train.py:379] starting iteration 442 3205.8584702014923
I0726 23:23:40.797807 140267183036224 train.py:394] {'eval/walltime': 1832.3826155662537, 'training/sps': 40292.70484782626, 'training/walltime': 1372.1610190868378, 'training/entropy_loss': Array(-0.02023122, dtype=float32), 'training/policy_loss': Array(-0.0001031, dtype=float32), 'training/total_loss': Array(96.98861, dtype=float32), 'training/v_loss': Array(97.00894, dtype=float32), 'eval/episode_goal_distance': (Array(0.2933476, dtype=float32), Array(0.06360459, dtype=float32)), 'eval/episode_reward': (Array(-14301.512, dtype=float32), Array(4752.024, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.08278, dtype=float32)), 'eval/epoch_eval_time': 4.141014337539673, 'eval/sps': 30910.301092086884}
I0726 23:23:40.800023 140267183036224 train.py:379] starting iteration 443 3213.054869890213
I0726 23:23:47.966034 140267183036224 train.py:394] {'eval/walltime': 1836.497298002243, 'training/sps': 40317.624217793804, 'training/walltime': 1375.2088177204132, 'training/entropy_loss': Array(-0.02076491, dtype=float32), 'training/policy_loss': Array(0.00054028, dtype=float32), 'training/total_loss': Array(86.80545, dtype=float32), 'training/v_loss': Array(86.825676, dtype=float32), 'eval/episode_goal_distance': (Array(0.2947744, dtype=float32), Array(0.05220089, dtype=float32)), 'eval/episode_reward': (Array(-14253.647, dtype=float32), Array(4530.369, dtype=float32)), 'eval/avg_episode_length': (Array(930.0703, dtype=float32), Array(254.28151, dtype=float32)), 'eval/epoch_eval_time': 4.11468243598938, 'eval/sps': 31108.111498578448}
I0726 23:23:47.968216 140267183036224 train.py:379] starting iteration 444 3220.2230620384216
I0726 23:23:55.136691 140267183036224 train.py:394] {'eval/walltime': 1840.6092793941498, 'training/sps': 40249.467027868166, 'training/walltime': 1378.2617774009705, 'training/entropy_loss': Array(-0.02003658, dtype=float32), 'training/policy_loss': Array(0.00030324, dtype=float32), 'training/total_loss': Array(90.02882, dtype=float32), 'training/v_loss': Array(90.048546, dtype=float32), 'eval/episode_goal_distance': (Array(0.28871536, dtype=float32), Array(0.0605486, dtype=float32)), 'eval/episode_reward': (Array(-14579.645, dtype=float32), Array(3933.559, dtype=float32)), 'eval/avg_episode_length': (Array(961.1797, dtype=float32), Array(192.54279, dtype=float32)), 'eval/epoch_eval_time': 4.111981391906738, 'eval/sps': 31128.545535719462}
I0726 23:23:55.138897 140267183036224 train.py:379] starting iteration 445 3227.393742799759
I0726 23:24:02.307111 140267183036224 train.py:394] {'eval/walltime': 1844.7276499271393, 'training/sps': 40342.58706128081, 'training/walltime': 1381.3076901435852, 'training/entropy_loss': Array(-0.02124937, dtype=float32), 'training/policy_loss': Array(0.00017559, dtype=float32), 'training/total_loss': Array(99.26411, dtype=float32), 'training/v_loss': Array(99.28519, dtype=float32), 'eval/episode_goal_distance': (Array(0.2895466, dtype=float32), Array(0.06321788, dtype=float32)), 'eval/episode_reward': (Array(-13699.717, dtype=float32), Array(5416.5615, dtype=float32)), 'eval/avg_episode_length': (Array(891.25, dtype=float32), Array(310.32626, dtype=float32)), 'eval/epoch_eval_time': 4.118370532989502, 'eval/sps': 31080.253458176703}
I0726 23:24:02.311944 140267183036224 train.py:379] starting iteration 446 3234.5667753219604
I0726 23:24:09.482764 140267183036224 train.py:394] {'eval/walltime': 1848.837777853012, 'training/sps': 40199.82095733865, 'training/walltime': 1384.3644201755524, 'training/entropy_loss': Array(-0.02111851, dtype=float32), 'training/policy_loss': Array(9.0573485e-05, dtype=float32), 'training/total_loss': Array(86.320946, dtype=float32), 'training/v_loss': Array(86.341965, dtype=float32), 'eval/episode_goal_distance': (Array(0.28841513, dtype=float32), Array(0.06129516, dtype=float32)), 'eval/episode_reward': (Array(-13872.208, dtype=float32), Array(4636.9517, dtype=float32)), 'eval/avg_episode_length': (Array(922.3828, dtype=float32), Array(266.62427, dtype=float32)), 'eval/epoch_eval_time': 4.110127925872803, 'eval/sps': 31142.582982455144}
I0726 23:24:09.485337 140267183036224 train.py:379] starting iteration 447 3241.740182876587
I0726 23:24:16.651060 140267183036224 train.py:394] {'eval/walltime': 1852.955267906189, 'training/sps': 40360.07313106732, 'training/walltime': 1387.4090132713318, 'training/entropy_loss': Array(-0.02110629, dtype=float32), 'training/policy_loss': Array(0.00015308, dtype=float32), 'training/total_loss': Array(77.02532, dtype=float32), 'training/v_loss': Array(77.04628, dtype=float32), 'eval/episode_goal_distance': (Array(0.2828905, dtype=float32), Array(0.05451386, dtype=float32)), 'eval/episode_reward': (Array(-13465.309, dtype=float32), Array(4833.5737, dtype=float32)), 'eval/avg_episode_length': (Array(906.7344, dtype=float32), Array(289.9748, dtype=float32)), 'eval/epoch_eval_time': 4.11749005317688, 'eval/sps': 31086.899627417595}
I0726 23:24:16.653610 140267183036224 train.py:379] starting iteration 448 3248.9084570407867
I0726 23:24:23.819510 140267183036224 train.py:394] {'eval/walltime': 1857.0652604103088, 'training/sps': 40257.90528415978, 'training/walltime': 1390.4613330364227, 'training/entropy_loss': Array(-0.01970281, dtype=float32), 'training/policy_loss': Array(-0.00020223, dtype=float32), 'training/total_loss': Array(69.38479, dtype=float32), 'training/v_loss': Array(69.40469, dtype=float32), 'eval/episode_goal_distance': (Array(0.29817688, dtype=float32), Array(0.05739679, dtype=float32)), 'eval/episode_reward': (Array(-14130.492, dtype=float32), Array(4889.495, dtype=float32)), 'eval/avg_episode_length': (Array(914.5547, dtype=float32), Array(278.6668, dtype=float32)), 'eval/epoch_eval_time': 4.109992504119873, 'eval/sps': 31143.60911162059}
I0726 23:24:23.822043 140267183036224 train.py:379] starting iteration 449 3256.0768892765045
I0726 23:24:31.001408 140267183036224 train.py:394] {'eval/walltime': 1861.185486793518, 'training/sps': 40242.989828612284, 'training/walltime': 1393.5147840976715, 'training/entropy_loss': Array(-0.02069286, dtype=float32), 'training/policy_loss': Array(-7.72871e-05, dtype=float32), 'training/total_loss': Array(67.26958, dtype=float32), 'training/v_loss': Array(67.290344, dtype=float32), 'eval/episode_goal_distance': (Array(0.286657, dtype=float32), Array(0.0598776, dtype=float32)), 'eval/episode_reward': (Array(-13652.642, dtype=float32), Array(4924.8857, dtype=float32)), 'eval/avg_episode_length': (Array(906.7969, dtype=float32), Array(289.781, dtype=float32)), 'eval/epoch_eval_time': 4.1202263832092285, 'eval/sps': 31066.25415574891}
I0726 23:24:31.003670 140267183036224 train.py:379] starting iteration 450 3263.2585163116455
I0726 23:24:38.179821 140267183036224 train.py:394] {'eval/walltime': 1865.2997233867645, 'training/sps': 40184.832575567205, 'training/walltime': 1396.572654247284, 'training/entropy_loss': Array(-0.02188196, dtype=float32), 'training/policy_loss': Array(0.00015904, dtype=float32), 'training/total_loss': Array(564.9498, dtype=float32), 'training/v_loss': Array(564.97156, dtype=float32), 'eval/episode_goal_distance': (Array(0.28573737, dtype=float32), Array(0.05399954, dtype=float32)), 'eval/episode_reward': (Array(-13727.698, dtype=float32), Array(4867.153, dtype=float32)), 'eval/avg_episode_length': (Array(914.5703, dtype=float32), Array(278.61618, dtype=float32)), 'eval/epoch_eval_time': 4.11423659324646, 'eval/sps': 31111.482555503164}
I0726 23:24:38.182039 140267183036224 train.py:379] starting iteration 451 3270.436885356903
I0726 23:24:45.361925 140267183036224 train.py:394] {'eval/walltime': 1869.4130141735077, 'training/sps': 40122.788463170284, 'training/walltime': 1399.6352529525757, 'training/entropy_loss': Array(-0.02210489, dtype=float32), 'training/policy_loss': Array(0.00019852, dtype=float32), 'training/total_loss': Array(128.1913, dtype=float32), 'training/v_loss': Array(128.21321, dtype=float32), 'eval/episode_goal_distance': (Array(0.29230496, dtype=float32), Array(0.05708276, dtype=float32)), 'eval/episode_reward': (Array(-13950.699, dtype=float32), Array(5055.7803, dtype=float32)), 'eval/avg_episode_length': (Array(906.78906, dtype=float32), Array(289.80548, dtype=float32)), 'eval/epoch_eval_time': 4.113290786743164, 'eval/sps': 31118.636302722545}
I0726 23:24:45.364184 140267183036224 train.py:379] starting iteration 452 3277.6190309524536
I0726 23:24:52.540592 140267183036224 train.py:394] {'eval/walltime': 1873.524540424347, 'training/sps': 40138.94657125975, 'training/walltime': 1402.696618795395, 'training/entropy_loss': Array(-0.02389094, dtype=float32), 'training/policy_loss': Array(0.00052134, dtype=float32), 'training/total_loss': Array(138.46088, dtype=float32), 'training/v_loss': Array(138.48424, dtype=float32), 'eval/episode_goal_distance': (Array(0.29769027, dtype=float32), Array(0.06145682, dtype=float32)), 'eval/episode_reward': (Array(-14231.806, dtype=float32), Array(5034.852, dtype=float32)), 'eval/avg_episode_length': (Array(914.5156, dtype=float32), Array(278.79416, dtype=float32)), 'eval/epoch_eval_time': 4.111526250839233, 'eval/sps': 31131.991428699497}
I0726 23:24:52.542803 140267183036224 train.py:379] starting iteration 453 3284.797649383545
I0726 23:24:59.714049 140267183036224 train.py:394] {'eval/walltime': 1877.6384665966034, 'training/sps': 40238.58803171612, 'training/walltime': 1405.750403881073, 'training/entropy_loss': Array(-0.02295127, dtype=float32), 'training/policy_loss': Array(-0.0002811, dtype=float32), 'training/total_loss': Array(123.82512, dtype=float32), 'training/v_loss': Array(123.84835, dtype=float32), 'eval/episode_goal_distance': (Array(0.30243927, dtype=float32), Array(0.06232646, dtype=float32)), 'eval/episode_reward': (Array(-13660.207, dtype=float32), Array(5654.8774, dtype=float32)), 'eval/avg_episode_length': (Array(875.71094, dtype=float32), Array(328.8384, dtype=float32)), 'eval/epoch_eval_time': 4.11392617225647, 'eval/sps': 31113.830107892427}
I0726 23:24:59.716233 140267183036224 train.py:379] starting iteration 454 3291.971079349518
I0726 23:25:06.881344 140267183036224 train.py:394] {'eval/walltime': 1881.7465479373932, 'training/sps': 40242.45879703802, 'training/walltime': 1408.8038952350616, 'training/entropy_loss': Array(-0.02407421, dtype=float32), 'training/policy_loss': Array(-0.0002527, dtype=float32), 'training/total_loss': Array(139.0937, dtype=float32), 'training/v_loss': Array(139.11804, dtype=float32), 'eval/episode_goal_distance': (Array(0.289409, dtype=float32), Array(0.06472702, dtype=float32)), 'eval/episode_reward': (Array(-14282.22, dtype=float32), Array(4633.5576, dtype=float32)), 'eval/avg_episode_length': (Array(930.08594, dtype=float32), Array(254.22452, dtype=float32)), 'eval/epoch_eval_time': 4.108081340789795, 'eval/sps': 31158.097754556995}
I0726 23:25:06.883533 140267183036224 train.py:379] starting iteration 455 3299.1383798122406
I0726 23:25:14.065898 140267183036224 train.py:394] {'eval/walltime': 1885.864783525467, 'training/sps': 40151.667223813594, 'training/walltime': 1411.864291191101, 'training/entropy_loss': Array(-0.02392611, dtype=float32), 'training/policy_loss': Array(-0.00023723, dtype=float32), 'training/total_loss': Array(116.81176, dtype=float32), 'training/v_loss': Array(116.83592, dtype=float32), 'eval/episode_goal_distance': (Array(0.2867164, dtype=float32), Array(0.05960865, dtype=float32)), 'eval/episode_reward': (Array(-13800.539, dtype=float32), Array(5381.73, dtype=float32)), 'eval/avg_episode_length': (Array(899.09375, dtype=float32), Array(300.1209, dtype=float32)), 'eval/epoch_eval_time': 4.1182355880737305, 'eval/sps': 31081.27188514509}
I0726 23:25:14.070629 140267183036224 train.py:379] starting iteration 456 3306.3254601955414
I0726 23:25:21.243107 140267183036224 train.py:394] {'eval/walltime': 1889.9759540557861, 'training/sps': 40190.789604527105, 'training/walltime': 1414.9217081069946, 'training/entropy_loss': Array(-0.02534743, dtype=float32), 'training/policy_loss': Array(-0.00043963, dtype=float32), 'training/total_loss': Array(81.47694, dtype=float32), 'training/v_loss': Array(81.502716, dtype=float32), 'eval/episode_goal_distance': (Array(0.28429234, dtype=float32), Array(0.05988219, dtype=float32)), 'eval/episode_reward': (Array(-14190.187, dtype=float32), Array(4456.986, dtype=float32)), 'eval/avg_episode_length': (Array(945.60156, dtype=float32), Array(226.16779, dtype=float32)), 'eval/epoch_eval_time': 4.111170530319214, 'eval/sps': 31134.685135540065}
I0726 23:25:21.245420 140267183036224 train.py:379] starting iteration 457 3313.5002665519714
I0726 23:25:28.426237 140267183036224 train.py:394] {'eval/walltime': 1894.0974795818329, 'training/sps': 40215.77758898257, 'training/walltime': 1417.97722530365, 'training/entropy_loss': Array(-0.02513636, dtype=float32), 'training/policy_loss': Array(-0.00032874, dtype=float32), 'training/total_loss': Array(107.784195, dtype=float32), 'training/v_loss': Array(107.809654, dtype=float32), 'eval/episode_goal_distance': (Array(0.29270768, dtype=float32), Array(0.06269582, dtype=float32)), 'eval/episode_reward': (Array(-14268.212, dtype=float32), Array(4670.3584, dtype=float32)), 'eval/avg_episode_length': (Array(930.0625, dtype=float32), Array(254.3094, dtype=float32)), 'eval/epoch_eval_time': 4.121525526046753, 'eval/sps': 31056.461786074116}
I0726 23:25:28.431131 140267183036224 train.py:379] starting iteration 458 3320.6859617233276
I0726 23:25:35.608368 140267183036224 train.py:394] {'eval/walltime': 1898.215437412262, 'training/sps': 40219.458790543125, 'training/walltime': 1421.032462835312, 'training/entropy_loss': Array(-0.02351609, dtype=float32), 'training/policy_loss': Array(-0.00053439, dtype=float32), 'training/total_loss': Array(81.07439, dtype=float32), 'training/v_loss': Array(81.098434, dtype=float32), 'eval/episode_goal_distance': (Array(0.29110152, dtype=float32), Array(0.05629293, dtype=float32)), 'eval/episode_reward': (Array(-14098.535, dtype=float32), Array(5050.1714, dtype=float32)), 'eval/avg_episode_length': (Array(914.6094, dtype=float32), Array(278.48874, dtype=float32)), 'eval/epoch_eval_time': 4.117957830429077, 'eval/sps': 31083.368327417484}
I0726 23:25:35.610922 140267183036224 train.py:379] starting iteration 459 3327.8657689094543
I0726 23:25:42.785620 140267183036224 train.py:394] {'eval/walltime': 1902.3326604366302, 'training/sps': 40239.10953623409, 'training/walltime': 1424.0862083435059, 'training/entropy_loss': Array(-0.02075038, dtype=float32), 'training/policy_loss': Array(0.00012626, dtype=float32), 'training/total_loss': Array(127.94583, dtype=float32), 'training/v_loss': Array(127.96646, dtype=float32), 'eval/episode_goal_distance': (Array(0.29682168, dtype=float32), Array(0.05731304, dtype=float32)), 'eval/episode_reward': (Array(-13930.518, dtype=float32), Array(5175.1494, dtype=float32)), 'eval/avg_episode_length': (Array(899.0078, dtype=float32), Array(300.3763, dtype=float32)), 'eval/epoch_eval_time': 4.117223024368286, 'eval/sps': 31088.91581593137}
I0726 23:25:42.975741 140267183036224 train.py:379] starting iteration 460 3335.230580806732
I0726 23:25:50.152592 140267183036224 train.py:394] {'eval/walltime': 1906.4444999694824, 'training/sps': 40142.50116323791, 'training/walltime': 1427.1473031044006, 'training/entropy_loss': Array(-0.01993229, dtype=float32), 'training/policy_loss': Array(0.00016623, dtype=float32), 'training/total_loss': Array(101.87291, dtype=float32), 'training/v_loss': Array(101.89268, dtype=float32), 'eval/episode_goal_distance': (Array(0.29034942, dtype=float32), Array(0.06115353, dtype=float32)), 'eval/episode_reward': (Array(-13470.062, dtype=float32), Array(5496.304, dtype=float32)), 'eval/avg_episode_length': (Array(883.5781, dtype=float32), Array(319.54276, dtype=float32)), 'eval/epoch_eval_time': 4.111839532852173, 'eval/sps': 31129.619475012183}
I0726 23:25:50.155179 140267183036224 train.py:379] starting iteration 461 3342.410026073456
I0726 23:25:57.326190 140267183036224 train.py:394] {'eval/walltime': 1910.5619072914124, 'training/sps': 40290.92832131423, 'training/walltime': 1430.197121143341, 'training/entropy_loss': Array(-0.02072782, dtype=float32), 'training/policy_loss': Array(0.00063637, dtype=float32), 'training/total_loss': Array(78.63702, dtype=float32), 'training/v_loss': Array(78.657104, dtype=float32), 'eval/episode_goal_distance': (Array(0.2901472, dtype=float32), Array(0.06180418, dtype=float32)), 'eval/episode_reward': (Array(-14219.111, dtype=float32), Array(4550.249, dtype=float32)), 'eval/avg_episode_length': (Array(937.89844, dtype=float32), Array(240.51865, dtype=float32)), 'eval/epoch_eval_time': 4.117407321929932, 'eval/sps': 31087.524257863126}
I0726 23:25:57.328939 140267183036224 train.py:379] starting iteration 462 3349.5837857723236
I0726 23:26:04.494859 140267183036224 train.py:394] {'eval/walltime': 1914.6727123260498, 'training/sps': 40269.61592307429, 'training/walltime': 1433.248553276062, 'training/entropy_loss': Array(-0.01919165, dtype=float32), 'training/policy_loss': Array(0.0001562, dtype=float32), 'training/total_loss': Array(100.14483, dtype=float32), 'training/v_loss': Array(100.163864, dtype=float32), 'eval/episode_goal_distance': (Array(0.29078838, dtype=float32), Array(0.05621967, dtype=float32)), 'eval/episode_reward': (Array(-13846.798, dtype=float32), Array(5002.5176, dtype=float32)), 'eval/avg_episode_length': (Array(906.8281, dtype=float32), Array(289.68372, dtype=float32)), 'eval/epoch_eval_time': 4.110805034637451, 'eval/sps': 31137.45335073738}
I0726 23:26:04.497324 140267183036224 train.py:379] starting iteration 463 3356.752170562744
I0726 23:26:11.666998 140267183036224 train.py:394] {'eval/walltime': 1918.7876424789429, 'training/sps': 40275.52258122898, 'training/walltime': 1436.29953789711, 'training/entropy_loss': Array(-0.01817409, dtype=float32), 'training/policy_loss': Array(1.3671306e-05, dtype=float32), 'training/total_loss': Array(79.89778, dtype=float32), 'training/v_loss': Array(79.91594, dtype=float32), 'eval/episode_goal_distance': (Array(0.2933544, dtype=float32), Array(0.05988153, dtype=float32)), 'eval/episode_reward': (Array(-13986.061, dtype=float32), Array(5142.896, dtype=float32)), 'eval/avg_episode_length': (Array(906.8672, dtype=float32), Array(289.56216, dtype=float32)), 'eval/epoch_eval_time': 4.114930152893066, 'eval/sps': 31106.238804565757}
I0726 23:26:11.669634 140267183036224 train.py:379] starting iteration 464 3363.924480676651
I0726 23:26:18.844022 140267183036224 train.py:394] {'eval/walltime': 1922.9022703170776, 'training/sps': 40209.03203780833, 'training/walltime': 1439.3555676937103, 'training/entropy_loss': Array(-0.01774806, dtype=float32), 'training/policy_loss': Array(-0.0004098, dtype=float32), 'training/total_loss': Array(73.67154, dtype=float32), 'training/v_loss': Array(73.6897, dtype=float32), 'eval/episode_goal_distance': (Array(0.2875163, dtype=float32), Array(0.0597326, dtype=float32)), 'eval/episode_reward': (Array(-13788.109, dtype=float32), Array(4652.3384, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.7585, dtype=float32)), 'eval/epoch_eval_time': 4.114627838134766, 'eval/sps': 31108.524278595432}
I0726 23:26:18.846423 140267183036224 train.py:379] starting iteration 465 3371.101269721985
I0726 23:26:26.006262 140267183036224 train.py:394] {'eval/walltime': 1927.0124821662903, 'training/sps': 40342.5554831774, 'training/walltime': 1442.4014828205109, 'training/entropy_loss': Array(-0.01850251, dtype=float32), 'training/policy_loss': Array(0.00037786, dtype=float32), 'training/total_loss': Array(65.70358, dtype=float32), 'training/v_loss': Array(65.72171, dtype=float32), 'eval/episode_goal_distance': (Array(0.29919642, dtype=float32), Array(0.05239789, dtype=float32)), 'eval/episode_reward': (Array(-14384.42, dtype=float32), Array(4924.8867, dtype=float32)), 'eval/avg_episode_length': (Array(914.58594, dtype=float32), Array(278.56525, dtype=float32)), 'eval/epoch_eval_time': 4.1102118492126465, 'eval/sps': 31141.94710535899}
I0726 23:26:26.008662 140267183036224 train.py:379] starting iteration 466 3378.263508796692
I0726 23:26:33.180217 140267183036224 train.py:394] {'eval/walltime': 1931.1284539699554, 'training/sps': 40264.20484308481, 'training/walltime': 1445.4533250331879, 'training/entropy_loss': Array(-0.01740508, dtype=float32), 'training/policy_loss': Array(0.0006362, dtype=float32), 'training/total_loss': Array(349.62518, dtype=float32), 'training/v_loss': Array(349.64197, dtype=float32), 'eval/episode_goal_distance': (Array(0.2958793, dtype=float32), Array(0.05894592, dtype=float32)), 'eval/episode_reward': (Array(-13902.381, dtype=float32), Array(5099.2563, dtype=float32)), 'eval/avg_episode_length': (Array(906.78906, dtype=float32), Array(289.80505, dtype=float32)), 'eval/epoch_eval_time': 4.115971803665161, 'eval/sps': 31098.366584051786}
I0726 23:26:33.182756 140267183036224 train.py:379] starting iteration 467 3385.437602519989
I0726 23:26:40.341351 140267183036224 train.py:394] {'eval/walltime': 1935.2349889278412, 'training/sps': 40310.683665484474, 'training/walltime': 1448.501648426056, 'training/entropy_loss': Array(-0.01829693, dtype=float32), 'training/policy_loss': Array(0.00025326, dtype=float32), 'training/total_loss': Array(224.12471, dtype=float32), 'training/v_loss': Array(224.14276, dtype=float32), 'eval/episode_goal_distance': (Array(0.29671636, dtype=float32), Array(0.05541822, dtype=float32)), 'eval/episode_reward': (Array(-14513.678, dtype=float32), Array(4843.9556, dtype=float32)), 'eval/avg_episode_length': (Array(922.4453, dtype=float32), Array(266.40945, dtype=float32)), 'eval/epoch_eval_time': 4.106534957885742, 'eval/sps': 31169.83084588206}
I0726 23:26:40.343893 140267183036224 train.py:379] starting iteration 468 3392.598739385605
I0726 23:26:47.516509 140267183036224 train.py:394] {'eval/walltime': 1939.352105140686, 'training/sps': 40263.67010600503, 'training/walltime': 1451.5535311698914, 'training/entropy_loss': Array(-0.01916809, dtype=float32), 'training/policy_loss': Array(0.00014457, dtype=float32), 'training/total_loss': Array(115.57818, dtype=float32), 'training/v_loss': Array(115.597206, dtype=float32), 'eval/episode_goal_distance': (Array(0.28652826, dtype=float32), Array(0.05459003, dtype=float32)), 'eval/episode_reward': (Array(-13358.161, dtype=float32), Array(5100.08, dtype=float32)), 'eval/avg_episode_length': (Array(891.2422, dtype=float32), Array(310.34836, dtype=float32)), 'eval/epoch_eval_time': 4.117116212844849, 'eval/sps': 31089.722364566056}
I0726 23:26:47.518892 140267183036224 train.py:379] starting iteration 469 3399.7737381458282
I0726 23:26:54.681242 140267183036224 train.py:394] {'eval/walltime': 1943.4620084762573, 'training/sps': 40306.0558566107, 'training/walltime': 1454.6022045612335, 'training/entropy_loss': Array(-0.02020894, dtype=float32), 'training/policy_loss': Array(0.00027138, dtype=float32), 'training/total_loss': Array(144.65851, dtype=float32), 'training/v_loss': Array(144.67844, dtype=float32), 'eval/episode_goal_distance': (Array(0.2845275, dtype=float32), Array(0.06288129, dtype=float32)), 'eval/episode_reward': (Array(-13575.124, dtype=float32), Array(5382.891, dtype=float32)), 'eval/avg_episode_length': (Array(891.22656, dtype=float32), Array(310.3932, dtype=float32)), 'eval/epoch_eval_time': 4.109903335571289, 'eval/sps': 31144.284804014158}
I0726 23:26:54.683810 140267183036224 train.py:379] starting iteration 470 3406.938656568527
I0726 23:27:01.852955 140267183036224 train.py:394] {'eval/walltime': 1947.5795593261719, 'training/sps': 40315.47652215745, 'training/walltime': 1457.6501655578613, 'training/entropy_loss': Array(-0.01946631, dtype=float32), 'training/policy_loss': Array(-9.7904296e-05, dtype=float32), 'training/total_loss': Array(107.91069, dtype=float32), 'training/v_loss': Array(107.93025, dtype=float32), 'eval/episode_goal_distance': (Array(0.29015535, dtype=float32), Array(0.0554697, dtype=float32)), 'eval/episode_reward': (Array(-14114.919, dtype=float32), Array(4773.237, dtype=float32)), 'eval/avg_episode_length': (Array(922.35156, dtype=float32), Array(266.7315, dtype=float32)), 'eval/epoch_eval_time': 4.117550849914551, 'eval/sps': 31086.4406210445}
I0726 23:27:01.855391 140267183036224 train.py:379] starting iteration 471 3414.1102378368378
I0726 23:27:09.020051 140267183036224 train.py:394] {'eval/walltime': 1951.686810016632, 'training/sps': 40239.810132857965, 'training/walltime': 1460.7038578987122, 'training/entropy_loss': Array(-0.01939512, dtype=float32), 'training/policy_loss': Array(0.00034811, dtype=float32), 'training/total_loss': Array(142.31982, dtype=float32), 'training/v_loss': Array(142.33885, dtype=float32), 'eval/episode_goal_distance': (Array(0.2892503, dtype=float32), Array(0.06561963, dtype=float32)), 'eval/episode_reward': (Array(-14182.867, dtype=float32), Array(4642.498, dtype=float32)), 'eval/avg_episode_length': (Array(930.09375, dtype=float32), Array(254.19626, dtype=float32)), 'eval/epoch_eval_time': 4.107250690460205, 'eval/sps': 31164.3991678672}
I0726 23:27:09.022624 140267183036224 train.py:379] starting iteration 472 3421.2774698734283
I0726 23:27:16.179600 140267183036224 train.py:394] {'eval/walltime': 1955.7956221103668, 'training/sps': 40360.79374952671, 'training/walltime': 1463.7483966350555, 'training/entropy_loss': Array(-0.02006502, dtype=float32), 'training/policy_loss': Array(0.00040504, dtype=float32), 'training/total_loss': Array(105.844185, dtype=float32), 'training/v_loss': Array(105.863846, dtype=float32), 'eval/episode_goal_distance': (Array(0.29586875, dtype=float32), Array(0.06463032, dtype=float32)), 'eval/episode_reward': (Array(-13734.776, dtype=float32), Array(5361.3115, dtype=float32)), 'eval/avg_episode_length': (Array(891.21094, dtype=float32), Array(310.4373, dtype=float32)), 'eval/epoch_eval_time': 4.108812093734741, 'eval/sps': 31152.556281456342}
I0726 23:27:16.182035 140267183036224 train.py:379] starting iteration 473 3428.4368822574615
I0726 23:27:23.341187 140267183036224 train.py:394] {'eval/walltime': 1959.9006538391113, 'training/sps': 40283.1720277276, 'training/walltime': 1466.7988018989563, 'training/entropy_loss': Array(-0.01946096, dtype=float32), 'training/policy_loss': Array(0.00045342, dtype=float32), 'training/total_loss': Array(93.29646, dtype=float32), 'training/v_loss': Array(93.31547, dtype=float32), 'eval/episode_goal_distance': (Array(0.28414005, dtype=float32), Array(0.06533755, dtype=float32)), 'eval/episode_reward': (Array(-13498.352, dtype=float32), Array(5317.6616, dtype=float32)), 'eval/avg_episode_length': (Array(899., dtype=float32), Array(300.39944, dtype=float32)), 'eval/epoch_eval_time': 4.105031728744507, 'eval/sps': 31181.244983738005}
I0726 23:27:23.343585 140267183036224 train.py:379] starting iteration 474 3435.5984320640564
I0726 23:27:30.507631 140267183036224 train.py:394] {'eval/walltime': 1964.007940530777, 'training/sps': 40246.94315501579, 'training/walltime': 1469.8519530296326, 'training/entropy_loss': Array(-0.01862615, dtype=float32), 'training/policy_loss': Array(0.00023967, dtype=float32), 'training/total_loss': Array(79.66792, dtype=float32), 'training/v_loss': Array(79.68632, dtype=float32), 'eval/episode_goal_distance': (Array(0.2894146, dtype=float32), Array(0.05626942, dtype=float32)), 'eval/episode_reward': (Array(-14248.594, dtype=float32), Array(4611.3706, dtype=float32)), 'eval/avg_episode_length': (Array(930.10156, dtype=float32), Array(254.16785, dtype=float32)), 'eval/epoch_eval_time': 4.107286691665649, 'eval/sps': 31164.126005553193}
I0726 23:27:30.510070 140267183036224 train.py:379] starting iteration 475 3442.76491689682
I0726 23:27:37.677716 140267183036224 train.py:394] {'eval/walltime': 1968.113204240799, 'training/sps': 40175.394567627925, 'training/walltime': 1472.9105415344238, 'training/entropy_loss': Array(-0.01733342, dtype=float32), 'training/policy_loss': Array(0.00020859, dtype=float32), 'training/total_loss': Array(70.73645, dtype=float32), 'training/v_loss': Array(70.75357, dtype=float32), 'eval/episode_goal_distance': (Array(0.2959911, dtype=float32), Array(0.0588196, dtype=float32)), 'eval/episode_reward': (Array(-13547.513, dtype=float32), Array(5336.0566, dtype=float32)), 'eval/avg_episode_length': (Array(891.3125, dtype=float32), Array(310.1479, dtype=float32)), 'eval/epoch_eval_time': 4.105263710021973, 'eval/sps': 31179.482986079573}
I0726 23:27:37.680195 140267183036224 train.py:379] starting iteration 476 3449.9350419044495
I0726 23:27:44.840983 140267183036224 train.py:394] {'eval/walltime': 1972.220248222351, 'training/sps': 40286.758509294166, 'training/walltime': 1475.960675239563, 'training/entropy_loss': Array(-0.01849404, dtype=float32), 'training/policy_loss': Array(0.00045935, dtype=float32), 'training/total_loss': Array(103.5242, dtype=float32), 'training/v_loss': Array(103.54224, dtype=float32), 'eval/episode_goal_distance': (Array(0.28986043, dtype=float32), Array(0.0537113, dtype=float32)), 'eval/episode_reward': (Array(-13526.321, dtype=float32), Array(5140.591, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.30682, dtype=float32)), 'eval/epoch_eval_time': 4.107043981552124, 'eval/sps': 31165.967682582876}
I0726 23:27:44.843549 140267183036224 train.py:379] starting iteration 477 3457.0983951091766
I0726 23:27:51.994475 140267183036224 train.py:394] {'eval/walltime': 1976.318172454834, 'training/sps': 40298.489093835364, 'training/walltime': 1479.0099210739136, 'training/entropy_loss': Array(-0.01998447, dtype=float32), 'training/policy_loss': Array(0.000607, dtype=float32), 'training/total_loss': Array(93.721924, dtype=float32), 'training/v_loss': Array(93.7413, dtype=float32), 'eval/episode_goal_distance': (Array(0.29697645, dtype=float32), Array(0.05882989, dtype=float32)), 'eval/episode_reward': (Array(-14626., dtype=float32), Array(4323.0933, dtype=float32)), 'eval/avg_episode_length': (Array(953.4219, dtype=float32), Array(210.03285, dtype=float32)), 'eval/epoch_eval_time': 4.09792423248291, 'eval/sps': 31235.32616474109}
I0726 23:27:51.997018 140267183036224 train.py:379] starting iteration 478 3464.251864671707
I0726 23:27:59.157596 140267183036224 train.py:394] {'eval/walltime': 1980.4190669059753, 'training/sps': 40209.719039275304, 'training/walltime': 1482.065898656845, 'training/entropy_loss': Array(-0.0207267, dtype=float32), 'training/policy_loss': Array(0.00014473, dtype=float32), 'training/total_loss': Array(71.59961, dtype=float32), 'training/v_loss': Array(71.620186, dtype=float32), 'eval/episode_goal_distance': (Array(0.29380935, dtype=float32), Array(0.06640273, dtype=float32)), 'eval/episode_reward': (Array(-13970.143, dtype=float32), Array(4906.751, dtype=float32)), 'eval/avg_episode_length': (Array(914.4375, dtype=float32), Array(279.04907, dtype=float32)), 'eval/epoch_eval_time': 4.100894451141357, 'eval/sps': 31212.702868852222}
I0726 23:27:59.160088 140267183036224 train.py:379] starting iteration 479 3471.4149351119995
I0726 23:28:06.317393 140267183036224 train.py:394] {'eval/walltime': 1984.5178768634796, 'training/sps': 40223.78106108864, 'training/walltime': 1485.1208078861237, 'training/entropy_loss': Array(-0.02142401, dtype=float32), 'training/policy_loss': Array(0.00016809, dtype=float32), 'training/total_loss': Array(94.804794, dtype=float32), 'training/v_loss': Array(94.826065, dtype=float32), 'eval/episode_goal_distance': (Array(0.29552492, dtype=float32), Array(0.05927254, dtype=float32)), 'eval/episode_reward': (Array(-14563.752, dtype=float32), Array(4460.692, dtype=float32)), 'eval/avg_episode_length': (Array(937.8906, dtype=float32), Array(240.54901, dtype=float32)), 'eval/epoch_eval_time': 4.0988099575042725, 'eval/sps': 31228.5764226888}
I0726 23:28:06.319784 140267183036224 train.py:379] starting iteration 480 3478.5746307373047
I0726 23:28:13.470286 140267183036224 train.py:394] {'eval/walltime': 1988.611180305481, 'training/sps': 40242.44622841315, 'training/walltime': 1488.1743001937866, 'training/entropy_loss': Array(-0.02072621, dtype=float32), 'training/policy_loss': Array(-0.00020002, dtype=float32), 'training/total_loss': Array(91.2199, dtype=float32), 'training/v_loss': Array(91.24083, dtype=float32), 'eval/episode_goal_distance': (Array(0.28722513, dtype=float32), Array(0.06085125, dtype=float32)), 'eval/episode_reward': (Array(-13566.0205, dtype=float32), Array(5007.3755, dtype=float32)), 'eval/avg_episode_length': (Array(906.78125, dtype=float32), Array(289.82904, dtype=float32)), 'eval/epoch_eval_time': 4.093303442001343, 'eval/sps': 31270.586657855212}
I0726 23:28:13.472872 140267183036224 train.py:379] starting iteration 481 3485.727718114853
I0726 23:28:20.624129 140267183036224 train.py:394] {'eval/walltime': 1992.7044370174408, 'training/sps': 40231.4202853299, 'training/walltime': 1491.2286293506622, 'training/entropy_loss': Array(-0.02247674, dtype=float32), 'training/policy_loss': Array(-3.2362077e-05, dtype=float32), 'training/total_loss': Array(101.106155, dtype=float32), 'training/v_loss': Array(101.12866, dtype=float32), 'eval/episode_goal_distance': (Array(0.2985888, dtype=float32), Array(0.05603831, dtype=float32)), 'eval/episode_reward': (Array(-14330.943, dtype=float32), Array(5024.679, dtype=float32)), 'eval/avg_episode_length': (Array(914.5625, dtype=float32), Array(278.64157, dtype=float32)), 'eval/epoch_eval_time': 4.093256711959839, 'eval/sps': 31270.943653742645}
I0726 23:28:20.626581 140267183036224 train.py:379] starting iteration 482 3492.8814272880554
I0726 23:28:27.773067 140267183036224 train.py:394] {'eval/walltime': 1996.7917304039001, 'training/sps': 40215.75248506216, 'training/walltime': 1494.2841484546661, 'training/entropy_loss': Array(-0.02174624, dtype=float32), 'training/policy_loss': Array(7.1782204e-05, dtype=float32), 'training/total_loss': Array(83.13139, dtype=float32), 'training/v_loss': Array(83.15306, dtype=float32), 'eval/episode_goal_distance': (Array(0.28458235, dtype=float32), Array(0.06597804, dtype=float32)), 'eval/episode_reward': (Array(-13254.268, dtype=float32), Array(5467.999, dtype=float32)), 'eval/avg_episode_length': (Array(891.22656, dtype=float32), Array(310.3928, dtype=float32)), 'eval/epoch_eval_time': 4.087293386459351, 'eval/sps': 31316.567688545838}
I0726 23:28:27.775646 140267183036224 train.py:379] starting iteration 483 3500.030492067337
I0726 23:28:34.916352 140267183036224 train.py:394] {'eval/walltime': 2000.877285003662, 'training/sps': 40269.27611479316, 'training/walltime': 1497.3356063365936, 'training/entropy_loss': Array(-0.02262039, dtype=float32), 'training/policy_loss': Array(0.00011201, dtype=float32), 'training/total_loss': Array(506.0899, dtype=float32), 'training/v_loss': Array(506.11243, dtype=float32), 'eval/episode_goal_distance': (Array(0.2867545, dtype=float32), Array(0.06172839, dtype=float32)), 'eval/episode_reward': (Array(-13053.178, dtype=float32), Array(5553.6987, dtype=float32)), 'eval/avg_episode_length': (Array(875.72656, dtype=float32), Array(328.79745, dtype=float32)), 'eval/epoch_eval_time': 4.085554599761963, 'eval/sps': 31329.895825515996}
I0726 23:28:34.918959 140267183036224 train.py:379] starting iteration 484 3507.1738052368164
I0726 23:28:42.072782 140267183036224 train.py:394] {'eval/walltime': 2004.9646844863892, 'training/sps': 40120.24297452532, 'training/walltime': 1500.3983993530273, 'training/entropy_loss': Array(-0.02375604, dtype=float32), 'training/policy_loss': Array(0.00011296, dtype=float32), 'training/total_loss': Array(161.06482, dtype=float32), 'training/v_loss': Array(161.08847, dtype=float32), 'eval/episode_goal_distance': (Array(0.28809798, dtype=float32), Array(0.06160706, dtype=float32)), 'eval/episode_reward': (Array(-12477.439, dtype=float32), Array(6142.736, dtype=float32)), 'eval/avg_episode_length': (Array(829.2578, dtype=float32), Array(374.78558, dtype=float32)), 'eval/epoch_eval_time': 4.087399482727051, 'eval/sps': 31315.754807161728}
I0726 23:28:42.075363 140267183036224 train.py:379] starting iteration 485 3514.330209493637
I0726 23:28:49.211578 140267183036224 train.py:394] {'eval/walltime': 2009.0478301048279, 'training/sps': 40295.773189831125, 'training/walltime': 1503.4478507041931, 'training/entropy_loss': Array(-0.02474818, dtype=float32), 'training/policy_loss': Array(0.00019396, dtype=float32), 'training/total_loss': Array(145.9704, dtype=float32), 'training/v_loss': Array(145.99496, dtype=float32), 'eval/episode_goal_distance': (Array(0.29729658, dtype=float32), Array(0.05525121, dtype=float32)), 'eval/episode_reward': (Array(-14476.602, dtype=float32), Array(4768.485, dtype=float32)), 'eval/avg_episode_length': (Array(930.0781, dtype=float32), Array(254.25296, dtype=float32)), 'eval/epoch_eval_time': 4.083145618438721, 'eval/sps': 31348.379891712895}
I0726 23:28:49.214003 140267183036224 train.py:379] starting iteration 486 3521.468849658966
I0726 23:28:56.343523 140267183036224 train.py:394] {'eval/walltime': 2013.130580663681, 'training/sps': 40380.342436372855, 'training/walltime': 1506.4909155368805, 'training/entropy_loss': Array(-0.02568991, dtype=float32), 'training/policy_loss': Array(0.00021517, dtype=float32), 'training/total_loss': Array(139.44806, dtype=float32), 'training/v_loss': Array(139.47354, dtype=float32), 'eval/episode_goal_distance': (Array(0.2892753, dtype=float32), Array(0.06141265, dtype=float32)), 'eval/episode_reward': (Array(-14493.756, dtype=float32), Array(4722.3315, dtype=float32)), 'eval/avg_episode_length': (Array(930.03906, dtype=float32), Array(254.39476, dtype=float32)), 'eval/epoch_eval_time': 4.082750558853149, 'eval/sps': 31351.41325801579}
I0726 23:28:56.346098 140267183036224 train.py:379] starting iteration 487 3528.6009452342987
I0726 23:29:03.500893 140267183036224 train.py:394] {'eval/walltime': 2017.2312252521515, 'training/sps': 40281.81191376875, 'training/walltime': 1509.5414237976074, 'training/entropy_loss': Array(-0.02509929, dtype=float32), 'training/policy_loss': Array(-0.00042356, dtype=float32), 'training/total_loss': Array(133.34256, dtype=float32), 'training/v_loss': Array(133.36809, dtype=float32), 'eval/episode_goal_distance': (Array(0.29287842, dtype=float32), Array(0.0582584, dtype=float32)), 'eval/episode_reward': (Array(-14740.242, dtype=float32), Array(4403.535, dtype=float32)), 'eval/avg_episode_length': (Array(945.6172, dtype=float32), Array(226.10286, dtype=float32)), 'eval/epoch_eval_time': 4.100644588470459, 'eval/sps': 31214.60473796975}
I0726 23:29:03.503406 140267183036224 train.py:379] starting iteration 488 3535.758252620697
I0726 23:29:10.668063 140267183036224 train.py:394] {'eval/walltime': 2021.3378653526306, 'training/sps': 40230.43735361325, 'training/walltime': 1512.5958275794983, 'training/entropy_loss': Array(-0.02545983, dtype=float32), 'training/policy_loss': Array(-0.00025389, dtype=float32), 'training/total_loss': Array(115.777084, dtype=float32), 'training/v_loss': Array(115.802795, dtype=float32), 'eval/episode_goal_distance': (Array(0.29421246, dtype=float32), Array(0.06758294, dtype=float32)), 'eval/episode_reward': (Array(-14376.803, dtype=float32), Array(4879.0107, dtype=float32)), 'eval/avg_episode_length': (Array(930.125, dtype=float32), Array(254.08263, dtype=float32)), 'eval/epoch_eval_time': 4.106640100479126, 'eval/sps': 31169.03280252538}
I0726 23:29:10.670672 140267183036224 train.py:379] starting iteration 489 3542.925518989563
I0726 23:29:17.829121 140267183036224 train.py:394] {'eval/walltime': 2025.4374196529388, 'training/sps': 40220.7048400532, 'training/walltime': 1515.6509704589844, 'training/entropy_loss': Array(-0.02446697, dtype=float32), 'training/policy_loss': Array(-0.00017011, dtype=float32), 'training/total_loss': Array(135.30508, dtype=float32), 'training/v_loss': Array(135.32973, dtype=float32), 'eval/episode_goal_distance': (Array(0.29558846, dtype=float32), Array(0.06092216, dtype=float32)), 'eval/episode_reward': (Array(-14237.305, dtype=float32), Array(4801.0244, dtype=float32)), 'eval/avg_episode_length': (Array(922.28906, dtype=float32), Array(266.94617, dtype=float32)), 'eval/epoch_eval_time': 4.0995543003082275, 'eval/sps': 31222.906351155354}
I0726 23:29:17.831490 140267183036224 train.py:379] starting iteration 490 3550.0863361358643
I0726 23:29:24.993885 140267183036224 train.py:394] {'eval/walltime': 2029.5423152446747, 'training/sps': 40238.364982706116, 'training/walltime': 1518.7047724723816, 'training/entropy_loss': Array(-0.02403774, dtype=float32), 'training/policy_loss': Array(-0.00014059, dtype=float32), 'training/total_loss': Array(111.071014, dtype=float32), 'training/v_loss': Array(111.09519, dtype=float32), 'eval/episode_goal_distance': (Array(0.29007763, dtype=float32), Array(0.05935648, dtype=float32)), 'eval/episode_reward': (Array(-14464.307, dtype=float32), Array(4380.601, dtype=float32)), 'eval/avg_episode_length': (Array(937.90625, dtype=float32), Array(240.48842, dtype=float32)), 'eval/epoch_eval_time': 4.10489559173584, 'eval/sps': 31182.279095647486}
I0726 23:29:24.996445 140267183036224 train.py:379] starting iteration 491 3557.25129199028
I0726 23:29:32.155656 140267183036224 train.py:394] {'eval/walltime': 2033.640459060669, 'training/sps': 40190.69558193748, 'training/walltime': 1521.7621965408325, 'training/entropy_loss': Array(-0.02429255, dtype=float32), 'training/policy_loss': Array(-0.0004148, dtype=float32), 'training/total_loss': Array(98.60683, dtype=float32), 'training/v_loss': Array(98.63153, dtype=float32), 'eval/episode_goal_distance': (Array(0.29765296, dtype=float32), Array(0.05834122, dtype=float32)), 'eval/episode_reward': (Array(-13968.339, dtype=float32), Array(5147.3296, dtype=float32)), 'eval/avg_episode_length': (Array(906.8594, dtype=float32), Array(289.58658, dtype=float32)), 'eval/epoch_eval_time': 4.098143815994263, 'eval/sps': 31233.652538117563}
I0726 23:29:32.158115 140267183036224 train.py:379] starting iteration 492 3564.4129617214203
I0726 23:29:39.316113 140267183036224 train.py:394] {'eval/walltime': 2037.7401041984558, 'training/sps': 40226.8954294125, 'training/walltime': 1524.8168692588806, 'training/entropy_loss': Array(-0.02362549, dtype=float32), 'training/policy_loss': Array(-0.00018577, dtype=float32), 'training/total_loss': Array(120.21249, dtype=float32), 'training/v_loss': Array(120.2363, dtype=float32), 'eval/episode_goal_distance': (Array(0.28386647, dtype=float32), Array(0.0582079, dtype=float32)), 'eval/episode_reward': (Array(-13684.703, dtype=float32), Array(4863.2383, dtype=float32)), 'eval/avg_episode_length': (Array(914.53125, dtype=float32), Array(278.74338, dtype=float32)), 'eval/epoch_eval_time': 4.099645137786865, 'eval/sps': 31222.214532719037}
I0726 23:29:39.318518 140267183036224 train.py:379] starting iteration 493 3571.5733647346497
I0726 23:29:46.478693 140267183036224 train.py:394] {'eval/walltime': 2041.8417019844055, 'training/sps': 40220.78017045812, 'training/walltime': 1527.8720064163208, 'training/entropy_loss': Array(-0.02447336, dtype=float32), 'training/policy_loss': Array(-0.00025415, dtype=float32), 'training/total_loss': Array(110.87973, dtype=float32), 'training/v_loss': Array(110.90445, dtype=float32), 'eval/episode_goal_distance': (Array(0.28695536, dtype=float32), Array(0.05686215, dtype=float32)), 'eval/episode_reward': (Array(-14329.07, dtype=float32), Array(4579.457, dtype=float32)), 'eval/avg_episode_length': (Array(937.875, dtype=float32), Array(240.60966, dtype=float32)), 'eval/epoch_eval_time': 4.101597785949707, 'eval/sps': 31207.350569203158}
I0726 23:29:46.481031 140267183036224 train.py:379] starting iteration 494 3578.735877275467
I0726 23:29:53.636105 140267183036224 train.py:394] {'eval/walltime': 2045.9378306865692, 'training/sps': 40216.97633766162, 'training/walltime': 1530.9274325370789, 'training/entropy_loss': Array(-0.02627644, dtype=float32), 'training/policy_loss': Array(-0.00066154, dtype=float32), 'training/total_loss': Array(110.17587, dtype=float32), 'training/v_loss': Array(110.202805, dtype=float32), 'eval/episode_goal_distance': (Array(0.28499806, dtype=float32), Array(0.05896204, dtype=float32)), 'eval/episode_reward': (Array(-14978.248, dtype=float32), Array(3927.9077, dtype=float32)), 'eval/avg_episode_length': (Array(968.9531, dtype=float32), Array(172.86194, dtype=float32)), 'eval/epoch_eval_time': 4.096128702163696, 'eval/sps': 31249.01811126848}
I0726 23:29:53.638332 140267183036224 train.py:379] starting iteration 495 3585.893178462982
I0726 23:30:00.790854 140267183036224 train.py:394] {'eval/walltime': 2050.0321428775787, 'training/sps': 40228.380575242125, 'training/walltime': 1533.981992483139, 'training/entropy_loss': Array(-0.02666025, dtype=float32), 'training/policy_loss': Array(-0.00017855, dtype=float32), 'training/total_loss': Array(105.2121, dtype=float32), 'training/v_loss': Array(105.23894, dtype=float32), 'eval/episode_goal_distance': (Array(0.3025444, dtype=float32), Array(0.06048617, dtype=float32)), 'eval/episode_reward': (Array(-14033.736, dtype=float32), Array(5700.309, dtype=float32)), 'eval/avg_episode_length': (Array(891.2656, dtype=float32), Array(310.28146, dtype=float32)), 'eval/epoch_eval_time': 4.0943121910095215, 'eval/sps': 31262.88226898483}
I0726 23:30:00.793237 140267183036224 train.py:379] starting iteration 496 3593.0480835437775
I0726 23:30:07.939369 140267183036224 train.py:394] {'eval/walltime': 2054.1179258823395, 'training/sps': 40200.244254612844, 'training/walltime': 1537.038690328598, 'training/entropy_loss': Array(-0.02612106, dtype=float32), 'training/policy_loss': Array(-0.00045261, dtype=float32), 'training/total_loss': Array(122.43329, dtype=float32), 'training/v_loss': Array(122.459854, dtype=float32), 'eval/episode_goal_distance': (Array(0.29441994, dtype=float32), Array(0.05700568, dtype=float32)), 'eval/episode_reward': (Array(-14531.689, dtype=float32), Array(4320.7026, dtype=float32)), 'eval/avg_episode_length': (Array(945.66406, dtype=float32), Array(225.90775, dtype=float32)), 'eval/epoch_eval_time': 4.085783004760742, 'eval/sps': 31328.144409738547}
I0726 23:30:07.941720 140267183036224 train.py:379] starting iteration 497 3600.196566104889
I0726 23:30:15.086509 140267183036224 train.py:394] {'eval/walltime': 2058.204134464264, 'training/sps': 40224.55646859733, 'training/walltime': 1540.0935406684875, 'training/entropy_loss': Array(-0.02467713, dtype=float32), 'training/policy_loss': Array(0.00047059, dtype=float32), 'training/total_loss': Array(97.03247, dtype=float32), 'training/v_loss': Array(97.05667, dtype=float32), 'eval/episode_goal_distance': (Array(0.29135752, dtype=float32), Array(0.06200526, dtype=float32)), 'eval/episode_reward': (Array(-13359.103, dtype=float32), Array(5322.8667, dtype=float32)), 'eval/avg_episode_length': (Array(891.2578, dtype=float32), Array(310.30405, dtype=float32)), 'eval/epoch_eval_time': 4.0862085819244385, 'eval/sps': 31324.88159469265}
I0726 23:30:15.090237 140267183036224 train.py:379] starting iteration 498 3607.3450770378113
I0726 23:30:22.245874 140267183036224 train.py:394] {'eval/walltime': 2062.293147087097, 'training/sps': 40120.8332492611, 'training/walltime': 1543.1562886238098, 'training/entropy_loss': Array(-0.02458702, dtype=float32), 'training/policy_loss': Array(-0.00014446, dtype=float32), 'training/total_loss': Array(111.18296, dtype=float32), 'training/v_loss': Array(111.207695, dtype=float32), 'eval/episode_goal_distance': (Array(0.29641414, dtype=float32), Array(0.06421746, dtype=float32)), 'eval/episode_reward': (Array(-14247.266, dtype=float32), Array(4946.3394, dtype=float32)), 'eval/avg_episode_length': (Array(922.27344, dtype=float32), Array(267.00024, dtype=float32)), 'eval/epoch_eval_time': 4.089012622833252, 'eval/sps': 31303.400553287993}
I0726 23:30:22.248286 140267183036224 train.py:379] starting iteration 499 3614.5031323432922
I0726 23:30:29.401388 140267183036224 train.py:394] {'eval/walltime': 2066.378894805908, 'training/sps': 40109.059145904, 'training/walltime': 1546.2199356555939, 'training/entropy_loss': Array(-0.02465874, dtype=float32), 'training/policy_loss': Array(-0.00017272, dtype=float32), 'training/total_loss': Array(127.71336, dtype=float32), 'training/v_loss': Array(127.73819, dtype=float32), 'eval/episode_goal_distance': (Array(0.2884028, dtype=float32), Array(0.06640932, dtype=float32)), 'eval/episode_reward': (Array(-13584.676, dtype=float32), Array(5448.337, dtype=float32)), 'eval/avg_episode_length': (Array(891.33594, dtype=float32), Array(310.08078, dtype=float32)), 'eval/epoch_eval_time': 4.085747718811035, 'eval/sps': 31328.414970576887}
I0726 23:30:29.403827 140267183036224 train.py:379] starting iteration 500 3621.6586730480194
I0726 23:30:36.551270 140267183036224 train.py:394] {'eval/walltime': 2070.4630036354065, 'training/sps': 40159.82982200972, 'training/walltime': 1549.2797095775604, 'training/entropy_loss': Array(-0.0265209, dtype=float32), 'training/policy_loss': Array(0.00028745, dtype=float32), 'training/total_loss': Array(569.8566, dtype=float32), 'training/v_loss': Array(569.8828, dtype=float32), 'eval/episode_goal_distance': (Array(0.29874295, dtype=float32), Array(0.06083801, dtype=float32)), 'eval/episode_reward': (Array(-14375.91, dtype=float32), Array(4976.7637, dtype=float32)), 'eval/avg_episode_length': (Array(922.28906, dtype=float32), Array(266.94614, dtype=float32)), 'eval/epoch_eval_time': 4.084108829498291, 'eval/sps': 31340.986575943927}
I0726 23:30:36.553613 140267183036224 train.py:379] starting iteration 501 3628.80845952034
I0726 23:30:43.687411 140267183036224 train.py:394] {'eval/walltime': 2074.5454273223877, 'training/sps': 40318.32755112171, 'training/walltime': 1552.3274550437927, 'training/entropy_loss': Array(-0.02655006, dtype=float32), 'training/policy_loss': Array(0.00014844, dtype=float32), 'training/total_loss': Array(159.27573, dtype=float32), 'training/v_loss': Array(159.30212, dtype=float32), 'eval/episode_goal_distance': (Array(0.29269278, dtype=float32), Array(0.05412493, dtype=float32)), 'eval/episode_reward': (Array(-14160.983, dtype=float32), Array(5204.255, dtype=float32)), 'eval/avg_episode_length': (Array(906.77344, dtype=float32), Array(289.8536, dtype=float32)), 'eval/epoch_eval_time': 4.082423686981201, 'eval/sps': 31353.92350583072}
I0726 23:30:43.689756 140267183036224 train.py:379] starting iteration 502 3635.944602251053
I0726 23:30:50.811753 140267183036224 train.py:394] {'eval/walltime': 2078.6155014038086, 'training/sps': 40311.50656838531, 'training/walltime': 1555.3757162094116, 'training/entropy_loss': Array(-0.02696156, dtype=float32), 'training/policy_loss': Array(-7.36793e-05, dtype=float32), 'training/total_loss': Array(159.06978, dtype=float32), 'training/v_loss': Array(159.09682, dtype=float32), 'eval/episode_goal_distance': (Array(0.28906396, dtype=float32), Array(0.06253655, dtype=float32)), 'eval/episode_reward': (Array(-14466.599, dtype=float32), Array(4427.1084, dtype=float32)), 'eval/avg_episode_length': (Array(945.5703, dtype=float32), Array(226.29745, dtype=float32)), 'eval/epoch_eval_time': 4.070074081420898, 'eval/sps': 31449.059019415705}
I0726 23:30:50.814161 140267183036224 train.py:379] starting iteration 503 3643.069007396698
I0726 23:30:57.961270 140267183036224 train.py:394] {'eval/walltime': 2082.7046132087708, 'training/sps': 40230.6383325499, 'training/walltime': 1558.4301047325134, 'training/entropy_loss': Array(-0.02742855, dtype=float32), 'training/policy_loss': Array(-8.456911e-06, dtype=float32), 'training/total_loss': Array(163.77618, dtype=float32), 'training/v_loss': Array(163.80363, dtype=float32), 'eval/episode_goal_distance': (Array(0.29935813, dtype=float32), Array(0.06251228, dtype=float32)), 'eval/episode_reward': (Array(-13737.818, dtype=float32), Array(5623.136, dtype=float32)), 'eval/avg_episode_length': (Array(883.46875, dtype=float32), Array(319.8426, dtype=float32)), 'eval/epoch_eval_time': 4.089111804962158, 'eval/sps': 31302.641283779853}
I0726 23:30:57.963622 140267183036224 train.py:379] starting iteration 504 3650.2184681892395
I0726 23:31:05.108048 140267183036224 train.py:394] {'eval/walltime': 2086.792337656021, 'training/sps': 40249.02697743983, 'training/walltime': 1561.4830977916718, 'training/entropy_loss': Array(-0.02706484, dtype=float32), 'training/policy_loss': Array(-0.00041846, dtype=float32), 'training/total_loss': Array(136.93144, dtype=float32), 'training/v_loss': Array(136.95892, dtype=float32), 'eval/episode_goal_distance': (Array(0.2941804, dtype=float32), Array(0.06528491, dtype=float32)), 'eval/episode_reward': (Array(-14284.644, dtype=float32), Array(5280.746, dtype=float32)), 'eval/avg_episode_length': (Array(914.53906, dtype=float32), Array(278.71783, dtype=float32)), 'eval/epoch_eval_time': 4.087724447250366, 'eval/sps': 31313.265277971466}
I0726 23:31:05.110509 140267183036224 train.py:379] starting iteration 505 3657.3653547763824
I0726 23:31:12.250519 140267183036224 train.py:394] {'eval/walltime': 2090.873156785965, 'training/sps': 40215.97214542843, 'training/walltime': 1564.5386002063751, 'training/entropy_loss': Array(-0.02599374, dtype=float32), 'training/policy_loss': Array(-0.00036939, dtype=float32), 'training/total_loss': Array(124.77438, dtype=float32), 'training/v_loss': Array(124.80075, dtype=float32), 'eval/episode_goal_distance': (Array(0.2926733, dtype=float32), Array(0.06143933, dtype=float32)), 'eval/episode_reward': (Array(-14233.438, dtype=float32), Array(4945.73, dtype=float32)), 'eval/avg_episode_length': (Array(922.3203, dtype=float32), Array(266.83875, dtype=float32)), 'eval/epoch_eval_time': 4.080819129943848, 'eval/sps': 31366.251706862902}
I0726 23:31:12.252993 140267183036224 train.py:379] starting iteration 506 3664.5078394412994
I0726 23:31:19.393362 140267183036224 train.py:394] {'eval/walltime': 2094.9505231380463, 'training/sps': 40165.01881707274, 'training/walltime': 1567.5979788303375, 'training/entropy_loss': Array(-0.02626319, dtype=float32), 'training/policy_loss': Array(-0.00029744, dtype=float32), 'training/total_loss': Array(128.55972, dtype=float32), 'training/v_loss': Array(128.5863, dtype=float32), 'eval/episode_goal_distance': (Array(0.29202807, dtype=float32), Array(0.06016944, dtype=float32)), 'eval/episode_reward': (Array(-14050.711, dtype=float32), Array(4472.863, dtype=float32)), 'eval/avg_episode_length': (Array(930.1172, dtype=float32), Array(254.1114, dtype=float32)), 'eval/epoch_eval_time': 4.077366352081299, 'eval/sps': 31392.813141419625}
I0726 23:31:19.395713 140267183036224 train.py:379] starting iteration 507 3671.650559902191
I0726 23:31:26.538718 140267183036224 train.py:394] {'eval/walltime': 2099.0327994823456, 'training/sps': 40194.69819939015, 'training/walltime': 1570.655098438263, 'training/entropy_loss': Array(-0.02540069, dtype=float32), 'training/policy_loss': Array(-0.00011541, dtype=float32), 'training/total_loss': Array(117.3525, dtype=float32), 'training/v_loss': Array(117.37803, dtype=float32), 'eval/episode_goal_distance': (Array(0.3020695, dtype=float32), Array(0.06447715, dtype=float32)), 'eval/episode_reward': (Array(-13065.267, dtype=float32), Array(6253.2583, dtype=float32)), 'eval/avg_episode_length': (Array(836.8203, dtype=float32), Array(368.34042, dtype=float32)), 'eval/epoch_eval_time': 4.082276344299316, 'eval/sps': 31355.055171300504}
I0726 23:31:26.541060 140267183036224 train.py:379] starting iteration 508 3678.7959055900574
I0726 23:31:33.676976 140267183036224 train.py:394] {'eval/walltime': 2103.1045315265656, 'training/sps': 40150.27218648564, 'training/walltime': 1573.7156007289886, 'training/entropy_loss': Array(-0.02464167, dtype=float32), 'training/policy_loss': Array(-0.0004697, dtype=float32), 'training/total_loss': Array(94.20459, dtype=float32), 'training/v_loss': Array(94.229706, dtype=float32), 'eval/episode_goal_distance': (Array(0.29606217, dtype=float32), Array(0.05491599, dtype=float32)), 'eval/episode_reward': (Array(-14399.098, dtype=float32), Array(4670.704, dtype=float32)), 'eval/avg_episode_length': (Array(930.10156, dtype=float32), Array(254.16772, dtype=float32)), 'eval/epoch_eval_time': 4.071732044219971, 'eval/sps': 31436.253321655207}
I0726 23:31:33.679329 140267183036224 train.py:379] starting iteration 509 3685.9341752529144
I0726 23:31:40.809729 140267183036224 train.py:394] {'eval/walltime': 2107.1816828250885, 'training/sps': 40293.80108063888, 'training/walltime': 1576.765201330185, 'training/entropy_loss': Array(-0.02418261, dtype=float32), 'training/policy_loss': Array(8.366671e-05, dtype=float32), 'training/total_loss': Array(104.79166, dtype=float32), 'training/v_loss': Array(104.81575, dtype=float32), 'eval/episode_goal_distance': (Array(0.28779596, dtype=float32), Array(0.0649737, dtype=float32)), 'eval/episode_reward': (Array(-13862.746, dtype=float32), Array(4738.4478, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.75842, dtype=float32)), 'eval/epoch_eval_time': 4.077151298522949, 'eval/sps': 31394.468987788416}
I0726 23:31:40.812398 140267183036224 train.py:379] starting iteration 510 3693.06724357605
I0726 23:31:47.932272 140267183036224 train.py:394] {'eval/walltime': 2111.2525565624237, 'training/sps': 40350.603182695675, 'training/walltime': 1579.810508966446, 'training/entropy_loss': Array(-0.02233638, dtype=float32), 'training/policy_loss': Array(-0.00070909, dtype=float32), 'training/total_loss': Array(115.85513, dtype=float32), 'training/v_loss': Array(115.87819, dtype=float32), 'eval/episode_goal_distance': (Array(0.29222405, dtype=float32), Array(0.05902259, dtype=float32)), 'eval/episode_reward': (Array(-14561.23, dtype=float32), Array(4384.7627, dtype=float32)), 'eval/avg_episode_length': (Array(945.59375, dtype=float32), Array(226.20032, dtype=float32)), 'eval/epoch_eval_time': 4.070873737335205, 'eval/sps': 31442.881371159605}
I0726 23:31:49.022502 140267183036224 train.py:410] total steps: 62791680
