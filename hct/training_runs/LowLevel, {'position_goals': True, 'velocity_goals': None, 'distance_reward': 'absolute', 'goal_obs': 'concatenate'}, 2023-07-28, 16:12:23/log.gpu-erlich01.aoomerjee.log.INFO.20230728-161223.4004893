I0728 16:12:23.060626 140524718552896 low_level_env.py:188] Initialising environment...
I0728 16:12:23.386327 140524718552896 low_level_env.py:294] Environment initialised.
I0728 16:12:23.405323 140524718552896 train.py:118] JAX is running on GPU.
I0728 16:12:23.405383 140524718552896 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 16:12:29.714895 140524718552896 train.py:367] Running initial eval
I0728 16:12:45.275541 140524718552896 train.py:373] {'eval/walltime': 15.418040752410889, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3061453 , 0.12687007], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.982643, 10.831157], dtype=float32), 'eval/episode_reward': Array([-51.685966,  61.091812], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30285937, 0.12896645], dtype=float32), 'eval/avg_episode_length': Array([640.46875, 469.411  ], dtype=float32), 'eval/epoch_eval_time': 15.418040752410889, 'eval/sps': 8301.962749708318}
I0728 16:12:45.276750 140524718552896 train.py:379] starting iteration 0, 0 steps, 21.87143635749817
I0728 16:13:36.530416 140524718552896 train.py:394] {'eval/walltime': 19.055352926254272, 'training/sps': 233997.45224821183, 'training/walltime': 47.61214232444763, 'training/entropy_loss': Array(-0.01607701, dtype=float32), 'training/policy_loss': Array(-0.01300733, dtype=float32), 'training/total_loss': Array(264.2279, dtype=float32), 'training/v_loss': Array(264.25702, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([1.7061739, 1.4754106], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([145.20203, 125.57688], dtype=float32), 'eval/episode_reward': Array([1052.5527,  953.3428], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([1.7066809, 1.4788563], dtype=float32), 'eval/avg_episode_length': Array([503.51562, 439.6144 ], dtype=float32), 'eval/epoch_eval_time': 3.637312173843384, 'eval/sps': 35190.82055163502}
I0728 16:13:36.550022 140524718552896 train.py:379] starting iteration 1, 11141120 steps, 73.14471054077148
I0728 16:14:19.450148 140524718552896 train.py:394] {'eval/walltime': 22.71712875366211, 'training/sps': 283962.9455995918, 'training/walltime': 86.84655714035034, 'training/entropy_loss': Array(0.02601684, dtype=float32), 'training/policy_loss': Array(-0.00427485, dtype=float32), 'training/total_loss': Array(620.84265, dtype=float32), 'training/v_loss': Array(620.8209, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([3.744855 , 2.5949771], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([318.66583, 220.87465], dtype=float32), 'eval/episode_reward': Array([2166.171 , 1533.1571], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([3.7488246, 2.599801 ], dtype=float32), 'eval/avg_episode_length': Array([631.28125, 424.8272 ], dtype=float32), 'eval/epoch_eval_time': 3.661775827407837, 'eval/sps': 34955.717125537674}
I0728 16:14:19.451986 140524718552896 train.py:379] starting iteration 2, 22282240 steps, 116.04667568206787
I0728 16:15:07.766674 140524718552896 train.py:394] {'eval/walltime': 26.512808084487915, 'training/sps': 250276.89937051476, 'training/walltime': 131.36173224449158, 'training/entropy_loss': Array(0.04487706, dtype=float32), 'training/policy_loss': Array(-0.00304516, dtype=float32), 'training/total_loss': Array(597.454, dtype=float32), 'training/v_loss': Array(597.4122, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([5.978697 , 3.9341018], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([508.77463, 334.8062 ], dtype=float32), 'eval/episode_reward': Array([3341.0667, 2285.4941], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([5.9855204, 3.941233 ], dtype=float32), 'eval/avg_episode_length': Array([679.8672 , 456.32654], dtype=float32), 'eval/epoch_eval_time': 3.7956793308258057, 'eval/sps': 33722.553683730635}
I0728 16:15:07.768151 140524718552896 train.py:379] starting iteration 3, 33423360 steps, 164.36284041404724
I0728 16:15:56.476387 140524718552896 train.py:394] {'eval/walltime': 30.31746530532837, 'training/sps': 248132.12866969287, 'training/walltime': 176.26168155670166, 'training/entropy_loss': Array(0.06203714, dtype=float32), 'training/policy_loss': Array(-0.00122464, dtype=float32), 'training/total_loss': Array(531.6201, dtype=float32), 'training/v_loss': Array(531.5593, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([7.5690575, 4.67104  ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([644.1172 , 397.50848], dtype=float32), 'eval/episode_reward': Array([4254.245 , 2716.0303], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([7.5782833, 4.67896  ], dtype=float32), 'eval/avg_episode_length': Array([704.5156, 440.2429], dtype=float32), 'eval/epoch_eval_time': 3.804657220840454, 'eval/sps': 33642.97821597831}
I0728 16:15:56.477883 140524718552896 train.py:379] starting iteration 4, 44564480 steps, 213.0725736618042
I0728 16:16:44.876587 140524718552896 train.py:394] {'eval/walltime': 34.16011071205139, 'training/sps': 250067.83155751353, 'training/walltime': 220.8140733242035, 'training/entropy_loss': Array(0.07044762, dtype=float32), 'training/policy_loss': Array(0.00041271, dtype=float32), 'training/total_loss': Array(607.76184, dtype=float32), 'training/v_loss': Array(607.6909, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([9.704855, 4.958181], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([825.9426 , 421.97546], dtype=float32), 'eval/episode_reward': Array([5475.5073, 2875.6917], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([9.717356 , 4.9661365], dtype=float32), 'eval/avg_episode_length': Array([776.7578, 404.1467], dtype=float32), 'eval/epoch_eval_time': 3.8426454067230225, 'eval/sps': 33310.385542223994}
I0728 16:16:44.878083 140524718552896 train.py:379] starting iteration 5, 55705600 steps, 261.472772359848
I0728 16:17:34.739275 140524718552896 train.py:394] {'eval/walltime': 38.0422568321228, 'training/sps': 242329.10287091252, 'training/walltime': 266.78923630714417, 'training/entropy_loss': Array(0.07634307, dtype=float32), 'training/policy_loss': Array(0.00106668, dtype=float32), 'training/total_loss': Array(484.7918, dtype=float32), 'training/v_loss': Array(484.7144, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([9.663864, 6.331807], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([822.40216, 538.9093 ], dtype=float32), 'eval/episode_reward': Array([5445.3467, 3625.9788], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([9.675622, 6.34242 ], dtype=float32), 'eval/avg_episode_length': Array([704.91406, 452.39755], dtype=float32), 'eval/epoch_eval_time': 3.882146120071411, 'eval/sps': 32971.45342835408}
I0728 16:17:34.740794 140524718552896 train.py:379] starting iteration 6, 66846720 steps, 311.3354835510254
I0728 16:18:14.874376 140524718552896 train.py:394] {'eval/walltime': 41.90383720397949, 'training/sps': 307186.9301915186, 'training/walltime': 303.0574460029602, 'training/entropy_loss': Array(0.08227092, dtype=float32), 'training/policy_loss': Array(0.00150976, dtype=float32), 'training/total_loss': Array(324.27795, dtype=float32), 'training/v_loss': Array(324.1942, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([10.14627 ,  6.657514], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([863.4602, 566.6407], dtype=float32), 'eval/episode_reward': Array([5733.533 , 3823.5017], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([10.158733 ,  6.6685014], dtype=float32), 'eval/avg_episode_length': Array([712.8281 , 450.41776], dtype=float32), 'eval/epoch_eval_time': 3.8615803718566895, 'eval/sps': 33147.05060468707}
I0728 16:18:14.875798 140524718552896 train.py:379] starting iteration 7, 77987840 steps, 351.47048783302307
I0728 16:18:55.205563 140524718552896 train.py:394] {'eval/walltime': 45.76011371612549, 'training/sps': 305488.901156235, 'training/walltime': 339.5272488594055, 'training/entropy_loss': Array(0.08603992, dtype=float32), 'training/policy_loss': Array(0.0016737, dtype=float32), 'training/total_loss': Array(258.616, dtype=float32), 'training/v_loss': Array(258.52826, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([11.915497 ,  6.2979093], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1014.0311,  535.9454], dtype=float32), 'eval/episode_reward': Array([6711.0454, 3606.8364], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([11.931026,  6.307664], dtype=float32), 'eval/avg_episode_length': Array([783.02344, 410.12726], dtype=float32), 'eval/epoch_eval_time': 3.856276512145996, 'eval/sps': 33192.64051652995}
I0728 16:18:55.206976 140524718552896 train.py:379] starting iteration 8, 89128960 steps, 391.8016655445099
I0728 16:19:35.855729 140524718552896 train.py:394] {'eval/walltime': 49.61726999282837, 'training/sps': 302844.68944744946, 'training/walltime': 376.3154785633087, 'training/entropy_loss': Array(0.08888213, dtype=float32), 'training/policy_loss': Array(0.00210932, dtype=float32), 'training/total_loss': Array(251.49078, dtype=float32), 'training/v_loss': Array(251.39981, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([10.849191 ,  7.2138786], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([923.3187, 613.9599], dtype=float32), 'eval/episode_reward': Array([6117.4785, 4124.2837], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([10.862769 ,  7.2253675], dtype=float32), 'eval/avg_episode_length': Array([702.60156, 451.85577], dtype=float32), 'eval/epoch_eval_time': 3.857156276702881, 'eval/sps': 33185.069729509414}
I0728 16:19:36.200377 140524718552896 train.py:410] total steps: 100270080
