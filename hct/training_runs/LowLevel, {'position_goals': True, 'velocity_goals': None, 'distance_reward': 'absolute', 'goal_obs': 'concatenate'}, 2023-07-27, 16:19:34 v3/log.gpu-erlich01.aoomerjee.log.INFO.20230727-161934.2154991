I0727 16:19:34.151479 140141833336640 low_level_env.py:187] Initialising environment...
I0727 16:20:13.352476 140141833336640 low_level_env.py:289] Environment initialised.
I0727 16:20:13.357071 140141833336640 train.py:118] JAX is running on GPU.
I0727 16:20:13.357130 140141833336640 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0727 16:20:21.238988 140141833336640 train.py:367] Running initial eval
I0727 16:20:36.849540 140141833336640 train.py:373] {'eval/walltime': 15.473397493362427, 'eval/episode_goal_distance': (Array(0.60041404, dtype=float32), Array(0.31090018, dtype=float32)), 'eval/episode_reward': (Array(-12057.706, dtype=float32), Array(5911.9575, dtype=float32)), 'eval/avg_episode_length': (Array(914.6094, dtype=float32), Array(278.4888, dtype=float32)), 'eval/epoch_eval_time': 15.473397493362427, 'eval/sps': 8272.262123099194}
I0727 16:20:36.850753 140141833336640 train.py:379] starting iteration 0 23.493703365325928
I0727 16:21:08.155979 140141833336640 train.py:394] {'eval/walltime': 19.27290630340576, 'training/sps': 8936.324708217413, 'training/walltime': 27.50123882293701, 'training/entropy_loss': Array(-0.04434503, dtype=float32), 'training/policy_loss': Array(0.02785979, dtype=float32), 'training/total_loss': Array(150.31316, dtype=float32), 'training/v_loss': Array(150.32964, dtype=float32), 'eval/episode_goal_distance': (Array(0.72297764, dtype=float32), Array(0.43451238, dtype=float32)), 'eval/episode_reward': (Array(-13969.039, dtype=float32), Array(6958.6274, dtype=float32)), 'eval/avg_episode_length': (Array(945.71094, dtype=float32), Array(225.713, dtype=float32)), 'eval/epoch_eval_time': 3.799508810043335, 'eval/sps': 33688.565127590824}
I0727 16:21:08.180208 140141833336640 train.py:379] starting iteration 1 54.823160886764526
I0727 16:21:17.424906 140141833336640 train.py:394] {'eval/walltime': 23.083804845809937, 'training/sps': 45258.4875585411, 'training/walltime': 32.9313805103302, 'training/entropy_loss': Array(-0.04536533, dtype=float32), 'training/policy_loss': Array(0.00256408, dtype=float32), 'training/total_loss': Array(151.78654, dtype=float32), 'training/v_loss': Array(151.82935, dtype=float32), 'eval/episode_goal_distance': (Array(0.6604854, dtype=float32), Array(0.3489062, dtype=float32)), 'eval/episode_reward': (Array(-13209.922, dtype=float32), Array(6668.4673, dtype=float32)), 'eval/avg_episode_length': (Array(930.0547, dtype=float32), Array(254.338, dtype=float32)), 'eval/epoch_eval_time': 3.810898542404175, 'eval/sps': 33587.87922998571}
I0727 16:21:17.427367 140141833336640 train.py:379] starting iteration 2 64.07032084465027
I0727 16:21:26.681257 140141833336640 train.py:394] {'eval/walltime': 26.9111270904541, 'training/sps': 45316.60253902399, 'training/walltime': 38.35455846786499, 'training/entropy_loss': Array(-0.04611887, dtype=float32), 'training/policy_loss': Array(0.00096232, dtype=float32), 'training/total_loss': Array(153.7045, dtype=float32), 'training/v_loss': Array(153.74966, dtype=float32), 'eval/episode_goal_distance': (Array(0.73702717, dtype=float32), Array(0.46659106, dtype=float32)), 'eval/episode_reward': (Array(-13835.662, dtype=float32), Array(7913.802, dtype=float32)), 'eval/avg_episode_length': (Array(914.6094, dtype=float32), Array(278.48856, dtype=float32)), 'eval/epoch_eval_time': 3.827322244644165, 'eval/sps': 33443.747826334504}
I0727 16:21:26.684731 140141833336640 train.py:379] starting iteration 3 73.32767748832703
I0727 16:21:35.942823 140141833336640 train.py:394] {'eval/walltime': 30.733775854110718, 'training/sps': 45245.17563452365, 'training/walltime': 43.78629779815674, 'training/entropy_loss': Array(-0.04676367, dtype=float32), 'training/policy_loss': Array(0.00069556, dtype=float32), 'training/total_loss': Array(214.73627, dtype=float32), 'training/v_loss': Array(214.78232, dtype=float32), 'eval/episode_goal_distance': (Array(0.64203453, dtype=float32), Array(0.38030115, dtype=float32)), 'eval/episode_reward': (Array(-12557.986, dtype=float32), Array(6372.849, dtype=float32)), 'eval/avg_episode_length': (Array(914.5703, dtype=float32), Array(278.6161, dtype=float32)), 'eval/epoch_eval_time': 3.822648763656616, 'eval/sps': 33484.63537036019}
I0727 16:21:35.945351 140141833336640 train.py:379] starting iteration 4 82.58830523490906
I0727 16:21:45.219854 140141833336640 train.py:394] {'eval/walltime': 34.56444215774536, 'training/sps': 45173.191559367835, 'training/walltime': 49.22669267654419, 'training/entropy_loss': Array(-0.04784319, dtype=float32), 'training/policy_loss': Array(0.00060049, dtype=float32), 'training/total_loss': Array(285.0281, dtype=float32), 'training/v_loss': Array(285.07532, dtype=float32), 'eval/episode_goal_distance': (Array(0.66920406, dtype=float32), Array(0.4234998, dtype=float32)), 'eval/episode_reward': (Array(-12460.485, dtype=float32), Array(7267.0415, dtype=float32)), 'eval/avg_episode_length': (Array(899.0625, dtype=float32), Array(300.21353, dtype=float32)), 'eval/epoch_eval_time': 3.8306663036346436, 'eval/sps': 33414.55241834822}
I0727 16:21:45.222423 140141833336640 train.py:379] starting iteration 5 91.86537671089172
I0727 16:21:54.502367 140141833336640 train.py:394] {'eval/walltime': 38.401756286621094, 'training/sps': 45184.15757380442, 'training/walltime': 54.665767192840576, 'training/entropy_loss': Array(-0.04805354, dtype=float32), 'training/policy_loss': Array(0.00129732, dtype=float32), 'training/total_loss': Array(257.68097, dtype=float32), 'training/v_loss': Array(257.72772, dtype=float32), 'eval/episode_goal_distance': (Array(0.72590816, dtype=float32), Array(0.4406948, dtype=float32)), 'eval/episode_reward': (Array(-13552.569, dtype=float32), Array(7620.0815, dtype=float32)), 'eval/avg_episode_length': (Array(906.90625, dtype=float32), Array(289.44077, dtype=float32)), 'eval/epoch_eval_time': 3.8373141288757324, 'eval/sps': 33356.6645057286}
I0727 16:21:54.504787 140141833336640 train.py:379] starting iteration 6 101.14774107933044
I0727 16:22:03.792983 140141833336640 train.py:394] {'eval/walltime': 42.23823595046997, 'training/sps': 45107.52682841785, 'training/walltime': 60.11408185958862, 'training/entropy_loss': Array(-0.04819477, dtype=float32), 'training/policy_loss': Array(0.00124176, dtype=float32), 'training/total_loss': Array(283.26746, dtype=float32), 'training/v_loss': Array(283.3144, dtype=float32), 'eval/episode_goal_distance': (Array(0.6772969, dtype=float32), Array(0.3619319, dtype=float32)), 'eval/episode_reward': (Array(-12678.676, dtype=float32), Array(6212.254, dtype=float32)), 'eval/avg_episode_length': (Array(922.3828, dtype=float32), Array(266.6243, dtype=float32)), 'eval/epoch_eval_time': 3.836479663848877, 'eval/sps': 33363.91984718261}
I0727 16:22:03.795789 140141833336640 train.py:379] starting iteration 7 110.43874311447144
I0727 16:22:13.078953 140141833336640 train.py:394] {'eval/walltime': 46.07603859901428, 'training/sps': 45159.51625500863, 'training/walltime': 65.55612421035767, 'training/entropy_loss': Array(-0.04869992, dtype=float32), 'training/policy_loss': Array(0.0009453, dtype=float32), 'training/total_loss': Array(305.8286, dtype=float32), 'training/v_loss': Array(305.87634, dtype=float32), 'eval/episode_goal_distance': (Array(0.6999559, dtype=float32), Array(0.39783502, dtype=float32)), 'eval/episode_reward': (Array(-12832.328, dtype=float32), Array(7220.2524, dtype=float32)), 'eval/avg_episode_length': (Array(891.33594, dtype=float32), Array(310.08115, dtype=float32)), 'eval/epoch_eval_time': 3.8378026485443115, 'eval/sps': 33352.41848575792}
I0727 16:22:13.081152 140141833336640 train.py:379] starting iteration 8 119.72410702705383
I0727 16:22:22.383584 140141833336640 train.py:394] {'eval/walltime': 49.91964054107666, 'training/sps': 45046.7670128771, 'training/walltime': 71.01178765296936, 'training/entropy_loss': Array(-0.04957945, dtype=float32), 'training/policy_loss': Array(0.00078522, dtype=float32), 'training/total_loss': Array(264.24243, dtype=float32), 'training/v_loss': Array(264.29126, dtype=float32), 'eval/episode_goal_distance': (Array(0.6347005, dtype=float32), Array(0.33096752, dtype=float32)), 'eval/episode_reward': (Array(-11996.479, dtype=float32), Array(5744.308, dtype=float32)), 'eval/avg_episode_length': (Array(914.7031, dtype=float32), Array(278.18286, dtype=float32)), 'eval/epoch_eval_time': 3.843601942062378, 'eval/sps': 33302.09577616107}
I0727 16:22:22.386279 140141833336640 train.py:379] starting iteration 9 129.0292329788208
I0727 16:22:31.706897 140141833336640 train.py:394] {'eval/walltime': 53.777246713638306, 'training/sps': 45014.66245624105, 'training/walltime': 76.47134208679199, 'training/entropy_loss': Array(-0.05008942, dtype=float32), 'training/policy_loss': Array(0.00042647, dtype=float32), 'training/total_loss': Array(135.77567, dtype=float32), 'training/v_loss': Array(135.82532, dtype=float32), 'eval/episode_goal_distance': (Array(0.6191946, dtype=float32), Array(0.3249845, dtype=float32)), 'eval/episode_reward': (Array(-11566.481, dtype=float32), Array(5803.1694, dtype=float32)), 'eval/avg_episode_length': (Array(899.0625, dtype=float32), Array(300.2139, dtype=float32)), 'eval/epoch_eval_time': 3.8576061725616455, 'eval/sps': 33181.1994989114}
I0727 16:22:31.709369 140141833336640 train.py:379] starting iteration 10 138.3523235321045
I0727 16:22:41.053157 140141833336640 train.py:394] {'eval/walltime': 57.641653537750244, 'training/sps': 44880.741671659256, 'training/walltime': 81.94718742370605, 'training/entropy_loss': Array(-0.05032428, dtype=float32), 'training/policy_loss': Array(0.00049881, dtype=float32), 'training/total_loss': Array(126.625854, dtype=float32), 'training/v_loss': Array(126.67569, dtype=float32), 'eval/episode_goal_distance': (Array(0.58419985, dtype=float32), Array(0.32655707, dtype=float32)), 'eval/episode_reward': (Array(-11104.641, dtype=float32), Array(5303.011, dtype=float32)), 'eval/avg_episode_length': (Array(899.0547, dtype=float32), Array(300.23688, dtype=float32)), 'eval/epoch_eval_time': 3.8644068241119385, 'eval/sps': 33122.806636543784}
I0727 16:22:41.055585 140141833336640 train.py:379] starting iteration 11 147.69853925704956
I0727 16:22:50.416656 140141833336640 train.py:394] {'eval/walltime': 61.49789547920227, 'training/sps': 44672.815121265, 'training/walltime': 87.44851970672607, 'training/entropy_loss': Array(-0.05044504, dtype=float32), 'training/policy_loss': Array(0.00076331, dtype=float32), 'training/total_loss': Array(137.84921, dtype=float32), 'training/v_loss': Array(137.8989, dtype=float32), 'eval/episode_goal_distance': (Array(0.5346205, dtype=float32), Array(0.2952001, dtype=float32)), 'eval/episode_reward': (Array(-10908.559, dtype=float32), Array(5595.7676, dtype=float32)), 'eval/avg_episode_length': (Array(899.03906, dtype=float32), Array(300.28363, dtype=float32)), 'eval/epoch_eval_time': 3.8562419414520264, 'eval/sps': 33192.938084119014}
I0727 16:22:50.419089 140141833336640 train.py:379] starting iteration 12 157.0620436668396
I0727 16:22:59.795225 140141833336640 train.py:394] {'eval/walltime': 65.35718703269958, 'training/sps': 44574.92293988817, 'training/walltime': 92.96193361282349, 'training/entropy_loss': Array(-0.05107411, dtype=float32), 'training/policy_loss': Array(0.00033565, dtype=float32), 'training/total_loss': Array(148.88147, dtype=float32), 'training/v_loss': Array(148.9322, dtype=float32), 'eval/episode_goal_distance': (Array(0.5371479, dtype=float32), Array(0.2713762, dtype=float32)), 'eval/episode_reward': (Array(-10075.566, dtype=float32), Array(6153.024, dtype=float32)), 'eval/avg_episode_length': (Array(805.8906, dtype=float32), Array(393.99976, dtype=float32)), 'eval/epoch_eval_time': 3.8592915534973145, 'eval/sps': 33166.70902565151}
I0727 16:22:59.797522 140141833336640 train.py:379] starting iteration 13 166.4404764175415
I0727 16:23:09.184784 140141833336640 train.py:394] {'eval/walltime': 69.2212426662445, 'training/sps': 44524.08543471656, 'training/walltime': 98.4816427230835, 'training/entropy_loss': Array(-0.05132597, dtype=float32), 'training/policy_loss': Array(0.0003967, dtype=float32), 'training/total_loss': Array(155.16617, dtype=float32), 'training/v_loss': Array(155.21709, dtype=float32), 'eval/episode_goal_distance': (Array(0.5111153, dtype=float32), Array(0.2621431, dtype=float32)), 'eval/episode_reward': (Array(-10138.627, dtype=float32), Array(5610.611, dtype=float32)), 'eval/avg_episode_length': (Array(844.5703, dtype=float32), Array(361.18658, dtype=float32)), 'eval/epoch_eval_time': 3.864055633544922, 'eval/sps': 33125.8170531493}
I0727 16:23:09.187142 140141833336640 train.py:379] starting iteration 14 175.83009576797485
I0727 16:23:18.605320 140141833336640 train.py:394] {'eval/walltime': 73.09421181678772, 'training/sps': 44346.260503223755, 'training/walltime': 104.0234854221344, 'training/entropy_loss': Array(-0.051009, dtype=float32), 'training/policy_loss': Array(0.00106791, dtype=float32), 'training/total_loss': Array(158.5643, dtype=float32), 'training/v_loss': Array(158.61424, dtype=float32), 'eval/episode_goal_distance': (Array(0.51136506, dtype=float32), Array(0.2868355, dtype=float32)), 'eval/episode_reward': (Array(-9873.244, dtype=float32), Array(5617.7344, dtype=float32)), 'eval/avg_episode_length': (Array(868.03906, dtype=float32), Array(337.1969, dtype=float32)), 'eval/epoch_eval_time': 3.872969150543213, 'eval/sps': 33049.579024415165}
I0727 16:23:18.607635 140141833336640 train.py:379] starting iteration 15 185.25058937072754
I0727 16:23:28.045148 140141833336640 train.py:394] {'eval/walltime': 76.98271203041077, 'training/sps': 44317.165901679546, 'training/walltime': 109.56896638870239, 'training/entropy_loss': Array(-0.0510933, dtype=float32), 'training/policy_loss': Array(0.00101716, dtype=float32), 'training/total_loss': Array(155.07065, dtype=float32), 'training/v_loss': Array(155.12073, dtype=float32), 'eval/episode_goal_distance': (Array(0.54372704, dtype=float32), Array(0.27457327, dtype=float32)), 'eval/episode_reward': (Array(-10383.407, dtype=float32), Array(5460.75, dtype=float32)), 'eval/avg_episode_length': (Array(875.71094, dtype=float32), Array(328.83838, dtype=float32)), 'eval/epoch_eval_time': 3.888500213623047, 'eval/sps': 32917.57566363564}
I0727 16:23:28.047654 140141833336640 train.py:379] starting iteration 16 194.6906077861786
I0727 16:23:37.522744 140141833336640 train.py:394] {'eval/walltime': 80.88782572746277, 'training/sps': 44165.683030402484, 'training/walltime': 115.13346767425537, 'training/entropy_loss': Array(-0.05136258, dtype=float32), 'training/policy_loss': Array(0.00098557, dtype=float32), 'training/total_loss': Array(171.30771, dtype=float32), 'training/v_loss': Array(171.3581, dtype=float32), 'eval/episode_goal_distance': (Array(0.58660483, dtype=float32), Array(0.28980792, dtype=float32)), 'eval/episode_reward': (Array(-11731.284, dtype=float32), Array(5497.4834, dtype=float32)), 'eval/avg_episode_length': (Array(906.85156, dtype=float32), Array(289.61075, dtype=float32)), 'eval/epoch_eval_time': 3.905113697052002, 'eval/sps': 32777.53477360419}
I0727 16:23:37.525152 140141833336640 train.py:379] starting iteration 17 204.16810655593872
I0727 16:23:47.030823 140141833336640 train.py:394] {'eval/walltime': 84.82581996917725, 'training/sps': 44168.73558318793, 'training/walltime': 120.69758439064026, 'training/entropy_loss': Array(-0.05148213, dtype=float32), 'training/policy_loss': Array(0.00083572, dtype=float32), 'training/total_loss': Array(98.71666, dtype=float32), 'training/v_loss': Array(98.7673, dtype=float32), 'eval/episode_goal_distance': (Array(0.5066726, dtype=float32), Array(0.24681143, dtype=float32)), 'eval/episode_reward': (Array(-9892.457, dtype=float32), Array(5264.189, dtype=float32)), 'eval/avg_episode_length': (Array(867.89844, dtype=float32), Array(337.556, dtype=float32)), 'eval/epoch_eval_time': 3.9379942417144775, 'eval/sps': 32503.856568432377}
I0727 16:23:47.033086 140141833336640 train.py:379] starting iteration 18 213.67603993415833
I0727 16:23:56.546782 140141833336640 train.py:394] {'eval/walltime': 88.77756071090698, 'training/sps': 44213.23375643478, 'training/walltime': 126.25610113143921, 'training/entropy_loss': Array(-0.05099548, dtype=float32), 'training/policy_loss': Array(0.002031, dtype=float32), 'training/total_loss': Array(82.0863, dtype=float32), 'training/v_loss': Array(82.13527, dtype=float32), 'eval/episode_goal_distance': (Array(0.47562712, dtype=float32), Array(0.24172223, dtype=float32)), 'eval/episode_reward': (Array(-9336.623, dtype=float32), Array(5485.736, dtype=float32)), 'eval/avg_episode_length': (Array(836.9297, dtype=float32), Array(368.09338, dtype=float32)), 'eval/epoch_eval_time': 3.9517407417297363, 'eval/sps': 32390.788861308873}
I0727 16:23:56.549156 140141833336640 train.py:379] starting iteration 19 223.19211053848267
I0727 16:24:06.097093 140141833336640 train.py:394] {'eval/walltime': 92.7623827457428, 'training/sps': 44204.574523870906, 'training/walltime': 131.81570672988892, 'training/entropy_loss': Array(-0.05094116, dtype=float32), 'training/policy_loss': Array(0.00276164, dtype=float32), 'training/total_loss': Array(82.242325, dtype=float32), 'training/v_loss': Array(82.2905, dtype=float32), 'eval/episode_goal_distance': (Array(0.48143047, dtype=float32), Array(0.22519693, dtype=float32)), 'eval/episode_reward': (Array(-9930.849, dtype=float32), Array(5053.1743, dtype=float32)), 'eval/avg_episode_length': (Array(891.2578, dtype=float32), Array(310.3039, dtype=float32)), 'eval/epoch_eval_time': 3.9848220348358154, 'eval/sps': 32121.886217504296}
I0727 16:24:06.099559 140141833336640 train.py:379] starting iteration 20 232.7425138950348
I0727 16:24:15.700705 140141833336640 train.py:394] {'eval/walltime': 96.80089950561523, 'training/sps': 44208.330170292145, 'training/walltime': 137.37484002113342, 'training/entropy_loss': Array(-0.05048326, dtype=float32), 'training/policy_loss': Array(0.00430656, dtype=float32), 'training/total_loss': Array(86.688324, dtype=float32), 'training/v_loss': Array(86.7345, dtype=float32), 'eval/episode_goal_distance': (Array(0.46334794, dtype=float32), Array(0.2129871, dtype=float32)), 'eval/episode_reward': (Array(-9477.189, dtype=float32), Array(5007.7505, dtype=float32)), 'eval/avg_episode_length': (Array(883.46094, dtype=float32), Array(319.8644, dtype=float32)), 'eval/epoch_eval_time': 4.0385167598724365, 'eval/sps': 31694.804704498267}
I0727 16:24:15.702975 140141833336640 train.py:379] starting iteration 21 242.34592962265015
I0727 16:24:25.302618 140141833336640 train.py:394] {'eval/walltime': 100.83377528190613, 'training/sps': 44176.14635529875, 'training/walltime': 142.93802332878113, 'training/entropy_loss': Array(-0.04901464, dtype=float32), 'training/policy_loss': Array(0.00641031, dtype=float32), 'training/total_loss': Array(89.28648, dtype=float32), 'training/v_loss': Array(89.32909, dtype=float32), 'eval/episode_goal_distance': (Array(0.49476373, dtype=float32), Array(0.19686547, dtype=float32)), 'eval/episode_reward': (Array(-10166.193, dtype=float32), Array(4549.551, dtype=float32)), 'eval/avg_episode_length': (Array(898.96875, dtype=float32), Array(300.49225, dtype=float32)), 'eval/epoch_eval_time': 4.0328757762908936, 'eval/sps': 31739.137801493067}
I0727 16:24:25.304908 140141833336640 train.py:379] starting iteration 22 251.9478623867035
I0727 16:24:34.904140 140141833336640 train.py:394] {'eval/walltime': 104.86886739730835, 'training/sps': 44195.5036966639, 'training/walltime': 148.49876999855042, 'training/entropy_loss': Array(-0.04737058, dtype=float32), 'training/policy_loss': Array(0.00681089, dtype=float32), 'training/total_loss': Array(92.19272, dtype=float32), 'training/v_loss': Array(92.23328, dtype=float32), 'eval/episode_goal_distance': (Array(0.49306124, dtype=float32), Array(0.23603255, dtype=float32)), 'eval/episode_reward': (Array(-10001.747, dtype=float32), Array(4984.4956, dtype=float32)), 'eval/avg_episode_length': (Array(891.25, dtype=float32), Array(310.32614, dtype=float32)), 'eval/epoch_eval_time': 4.035092115402222, 'eval/sps': 31721.704570613216}
I0727 16:24:34.906277 140141833336640 train.py:379] starting iteration 23 261.54923129081726
I0727 16:24:44.521162 140141833336640 train.py:394] {'eval/walltime': 108.91445326805115, 'training/sps': 44155.671045799136, 'training/walltime': 154.064532995224, 'training/entropy_loss': Array(-0.04589485, dtype=float32), 'training/policy_loss': Array(0.00716383, dtype=float32), 'training/total_loss': Array(96.033844, dtype=float32), 'training/v_loss': Array(96.072586, dtype=float32), 'eval/episode_goal_distance': (Array(0.47278988, dtype=float32), Array(0.20909157, dtype=float32)), 'eval/episode_reward': (Array(-9068.809, dtype=float32), Array(5186.033, dtype=float32)), 'eval/avg_episode_length': (Array(821.3203, dtype=float32), Array(381.77414, dtype=float32)), 'eval/epoch_eval_time': 4.045585870742798, 'eval/sps': 31639.422345643674}
I0727 16:24:44.523451 140141833336640 train.py:379] starting iteration 24 271.1664056777954
I0727 16:24:54.142107 140141833336640 train.py:394] {'eval/walltime': 112.9642882347107, 'training/sps': 44158.43656353243, 'training/walltime': 159.62994742393494, 'training/entropy_loss': Array(-0.04348522, dtype=float32), 'training/policy_loss': Array(0.00887919, dtype=float32), 'training/total_loss': Array(98.650024, dtype=float32), 'training/v_loss': Array(98.68462, dtype=float32), 'eval/episode_goal_distance': (Array(0.51924515, dtype=float32), Array(0.22362846, dtype=float32)), 'eval/episode_reward': (Array(-10155.938, dtype=float32), Array(5127.86, dtype=float32)), 'eval/avg_episode_length': (Array(875.85156, dtype=float32), Array(328.46646, dtype=float32)), 'eval/epoch_eval_time': 4.049834966659546, 'eval/sps': 31606.226192861173}
I0727 16:24:54.144258 140141833336640 train.py:379] starting iteration 25 280.7872123718262
I0727 16:25:03.784523 140141833336640 train.py:394] {'eval/walltime': 117.03588771820068, 'training/sps': 44160.457010230326, 'training/walltime': 165.1951072216034, 'training/entropy_loss': Array(-0.04677264, dtype=float32), 'training/policy_loss': Array(0.01007147, dtype=float32), 'training/total_loss': Array(115.85652, dtype=float32), 'training/v_loss': Array(115.89322, dtype=float32), 'eval/episode_goal_distance': (Array(0.52245915, dtype=float32), Array(0.24781764, dtype=float32)), 'eval/episode_reward': (Array(-10154.65, dtype=float32), Array(5474.4995, dtype=float32)), 'eval/avg_episode_length': (Array(868., dtype=float32), Array(337.29675, dtype=float32)), 'eval/epoch_eval_time': 4.07159948348999, 'eval/sps': 31437.276804614437}
I0727 16:25:03.786680 140141833336640 train.py:379] starting iteration 26 290.4296338558197
I0727 16:25:13.448200 140141833336640 train.py:394] {'eval/walltime': 121.12680625915527, 'training/sps': 44143.464914093245, 'training/walltime': 170.76240921020508, 'training/entropy_loss': Array(-0.042549, dtype=float32), 'training/policy_loss': Array(0.01253087, dtype=float32), 'training/total_loss': Array(62.928864, dtype=float32), 'training/v_loss': Array(62.958885, dtype=float32), 'eval/episode_goal_distance': (Array(0.49994907, dtype=float32), Array(0.2523698, dtype=float32)), 'eval/episode_reward': (Array(-10139.476, dtype=float32), Array(5331.756, dtype=float32)), 'eval/avg_episode_length': (Array(891.2656, dtype=float32), Array(310.2819, dtype=float32)), 'eval/epoch_eval_time': 4.09091854095459, 'eval/sps': 31288.816611374525}
I0727 16:25:13.450331 140141833336640 train.py:379] starting iteration 27 300.0932855606079
I0727 16:25:23.111239 140141833336640 train.py:394] {'eval/walltime': 125.2182240486145, 'training/sps': 44155.14332903787, 'training/walltime': 176.32823872566223, 'training/entropy_loss': Array(-0.03804555, dtype=float32), 'training/policy_loss': Array(0.01555542, dtype=float32), 'training/total_loss': Array(60.200787, dtype=float32), 'training/v_loss': Array(60.223274, dtype=float32), 'eval/episode_goal_distance': (Array(0.5289966, dtype=float32), Array(0.23569603, dtype=float32)), 'eval/episode_reward': (Array(-10099.874, dtype=float32), Array(5365.4707, dtype=float32)), 'eval/avg_episode_length': (Array(860.15625, dtype=float32), Array(345.70352, dtype=float32)), 'eval/epoch_eval_time': 4.0914177894592285, 'eval/sps': 31284.998645156214}
I0727 16:25:23.113589 140141833336640 train.py:379] starting iteration 28 309.7565429210663
I0727 16:25:32.774029 140141833336640 train.py:394] {'eval/walltime': 129.31151413917542, 'training/sps': 44172.82397754111, 'training/walltime': 181.89184045791626, 'training/entropy_loss': Array(-0.03236422, dtype=float32), 'training/policy_loss': Array(0.01772391, dtype=float32), 'training/total_loss': Array(63.98346, dtype=float32), 'training/v_loss': Array(63.9981, dtype=float32), 'eval/episode_goal_distance': (Array(0.5119225, dtype=float32), Array(0.24496359, dtype=float32)), 'eval/episode_reward': (Array(-10272.51, dtype=float32), Array(5722.021, dtype=float32)), 'eval/avg_episode_length': (Array(852.52344, dtype=float32), Array(353.23254, dtype=float32)), 'eval/epoch_eval_time': 4.093290090560913, 'eval/sps': 31270.688655848444}
I0727 16:25:32.776232 140141833336640 train.py:379] starting iteration 29 319.4191851615906
I0727 16:25:42.452476 140141833336640 train.py:394] {'eval/walltime': 133.4116334915161, 'training/sps': 44101.20126374777, 'training/walltime': 187.46447777748108, 'training/entropy_loss': Array(-0.02646346, dtype=float32), 'training/policy_loss': Array(0.01917448, dtype=float32), 'training/total_loss': Array(66.834946, dtype=float32), 'training/v_loss': Array(66.84223, dtype=float32), 'eval/episode_goal_distance': (Array(0.47321922, dtype=float32), Array(0.19616869, dtype=float32)), 'eval/episode_reward': (Array(-9207.75, dtype=float32), Array(5466.6304, dtype=float32)), 'eval/avg_episode_length': (Array(813.52344, dtype=float32), Array(388.18262, dtype=float32)), 'eval/epoch_eval_time': 4.100119352340698, 'eval/sps': 31218.603411368178}
I0727 16:25:42.454654 140141833336640 train.py:379] starting iteration 30 329.0976083278656
I0727 16:25:52.151964 140141833336640 train.py:394] {'eval/walltime': 137.50680232048035, 'training/sps': 43896.390062510676, 'training/walltime': 193.06311583518982, 'training/entropy_loss': Array(-0.02063791, dtype=float32), 'training/policy_loss': Array(0.02402825, dtype=float32), 'training/total_loss': Array(66.54263, dtype=float32), 'training/v_loss': Array(66.539246, dtype=float32), 'eval/episode_goal_distance': (Array(0.5202398, dtype=float32), Array(0.21963646, dtype=float32)), 'eval/episode_reward': (Array(-10620.529, dtype=float32), Array(5265.424, dtype=float32)), 'eval/avg_episode_length': (Array(883.47656, dtype=float32), Array(319.82144, dtype=float32)), 'eval/epoch_eval_time': 4.095168828964233, 'eval/sps': 31256.342618815615}
I0727 16:25:52.154121 140141833336640 train.py:379] starting iteration 31 338.79707527160645
I0727 16:26:01.858023 140141833336640 train.py:394] {'eval/walltime': 141.60015225410461, 'training/sps': 43830.73091872131, 'training/walltime': 198.67014074325562, 'training/entropy_loss': Array(-0.01339644, dtype=float32), 'training/policy_loss': Array(0.0202477, dtype=float32), 'training/total_loss': Array(67.996506, dtype=float32), 'training/v_loss': Array(67.989655, dtype=float32), 'eval/episode_goal_distance': (Array(0.45897722, dtype=float32), Array(0.17686467, dtype=float32)), 'eval/episode_reward': (Array(-9204.99, dtype=float32), Array(4947.875, dtype=float32)), 'eval/avg_episode_length': (Array(860.1406, dtype=float32), Array(345.7422, dtype=float32)), 'eval/epoch_eval_time': 4.093349933624268, 'eval/sps': 31270.231491464085}
I0727 16:26:01.860341 140141833336640 train.py:379] starting iteration 32 348.5032956600189
I0727 16:26:11.575823 140141833336640 train.py:394] {'eval/walltime': 145.68778252601624, 'training/sps': 43695.79319405285, 'training/walltime': 204.29448080062866, 'training/entropy_loss': Array(-0.00397884, dtype=float32), 'training/policy_loss': Array(0.02988603, dtype=float32), 'training/total_loss': Array(63.901672, dtype=float32), 'training/v_loss': Array(63.875767, dtype=float32), 'eval/episode_goal_distance': (Array(0.49993277, dtype=float32), Array(0.18925698, dtype=float32)), 'eval/episode_reward': (Array(-10104.865, dtype=float32), Array(4692.6377, dtype=float32)), 'eval/avg_episode_length': (Array(891.22656, dtype=float32), Array(310.3929, dtype=float32)), 'eval/epoch_eval_time': 4.087630271911621, 'eval/sps': 31313.98670754523}
I0727 16:26:11.578045 140141833336640 train.py:379] starting iteration 33 358.22099924087524
I0727 16:26:21.297372 140141833336640 train.py:394] {'eval/walltime': 149.78783297538757, 'training/sps': 43761.68005035094, 'training/walltime': 209.91035294532776, 'training/entropy_loss': Array(-0.01141086, dtype=float32), 'training/policy_loss': Array(0.04835663, dtype=float32), 'training/total_loss': Array(118.14198, dtype=float32), 'training/v_loss': Array(118.105034, dtype=float32), 'eval/episode_goal_distance': (Array(0.45356426, dtype=float32), Array(0.1794778, dtype=float32)), 'eval/episode_reward': (Array(-9225.102, dtype=float32), Array(5605.182, dtype=float32)), 'eval/avg_episode_length': (Array(813.6328, dtype=float32), Array(387.95474, dtype=float32)), 'eval/epoch_eval_time': 4.100050449371338, 'eval/sps': 31219.128052345375}
I0727 16:26:21.299619 140141833336640 train.py:379] starting iteration 34 367.94257378578186
I0727 16:26:31.031105 140141833336640 train.py:394] {'eval/walltime': 153.878559589386, 'training/sps': 43595.25879933734, 'training/walltime': 215.5476632118225, 'training/entropy_loss': Array(-0.0069815, dtype=float32), 'training/policy_loss': Array(0.05302073, dtype=float32), 'training/total_loss': Array(58.98298, dtype=float32), 'training/v_loss': Array(58.936935, dtype=float32), 'eval/episode_goal_distance': (Array(0.4304718, dtype=float32), Array(0.14860393, dtype=float32)), 'eval/episode_reward': (Array(-9260.152, dtype=float32), Array(4980.674, dtype=float32)), 'eval/avg_episode_length': (Array(844.8047, dtype=float32), Array(360.64218, dtype=float32)), 'eval/epoch_eval_time': 4.090726613998413, 'eval/sps': 31290.284606648038}
I0727 16:26:31.033262 140141833336640 train.py:379] starting iteration 35 377.676216840744
I0727 16:26:40.785001 140141833336640 train.py:394] {'eval/walltime': 157.97862076759338, 'training/sps': 43511.11684322333, 'training/walltime': 221.1958749294281, 'training/entropy_loss': Array(0.0057211, dtype=float32), 'training/policy_loss': Array(0.03708056, dtype=float32), 'training/total_loss': Array(48.424347, dtype=float32), 'training/v_loss': Array(48.381546, dtype=float32), 'eval/episode_goal_distance': (Array(0.43772638, dtype=float32), Array(0.15988317, dtype=float32)), 'eval/episode_reward': (Array(-9703.218, dtype=float32), Array(4450.146, dtype=float32)), 'eval/avg_episode_length': (Array(906.7578, dtype=float32), Array(289.9021, dtype=float32)), 'eval/epoch_eval_time': 4.1000611782073975, 'eval/sps': 31219.04635968465}
I0727 16:26:40.787247 140141833336640 train.py:379] starting iteration 36 387.43020129203796
I0727 16:26:50.565108 140141833336640 train.py:394] {'eval/walltime': 162.06887030601501, 'training/sps': 43235.08396239224, 'training/walltime': 226.8801474571228, 'training/entropy_loss': Array(0.01550009, dtype=float32), 'training/policy_loss': Array(0.03587937, dtype=float32), 'training/total_loss': Array(45.68118, dtype=float32), 'training/v_loss': Array(45.6298, dtype=float32), 'eval/episode_goal_distance': (Array(0.43707424, dtype=float32), Array(0.15622503, dtype=float32)), 'eval/episode_reward': (Array(-9342.904, dtype=float32), Array(4827.7627, dtype=float32)), 'eval/avg_episode_length': (Array(860.1172, dtype=float32), Array(345.79974, dtype=float32)), 'eval/epoch_eval_time': 4.090249538421631, 'eval/sps': 31293.934220305146}
I0727 16:26:50.567245 140141833336640 train.py:379] starting iteration 37 397.2101995944977
I0727 16:27:00.344093 140141833336640 train.py:394] {'eval/walltime': 166.15719270706177, 'training/sps': 43229.765817693806, 'training/walltime': 232.56511926651, 'training/entropy_loss': Array(0.03467554, dtype=float32), 'training/policy_loss': Array(0.03202407, dtype=float32), 'training/total_loss': Array(42.795837, dtype=float32), 'training/v_loss': Array(42.729137, dtype=float32), 'eval/episode_goal_distance': (Array(0.40279165, dtype=float32), Array(0.16156572, dtype=float32)), 'eval/episode_reward': (Array(-8814.129, dtype=float32), Array(4938.32, dtype=float32)), 'eval/avg_episode_length': (Array(844.75, dtype=float32), Array(360.76926, dtype=float32)), 'eval/epoch_eval_time': 4.088322401046753, 'eval/sps': 31308.685432251514}
I0727 16:27:00.346327 140141833336640 train.py:379] starting iteration 38 406.9892809391022
I0727 16:27:10.134892 140141833336640 train.py:394] {'eval/walltime': 170.27569150924683, 'training/sps': 43371.09726219336, 'training/walltime': 238.23156571388245, 'training/entropy_loss': Array(0.05654771, dtype=float32), 'training/policy_loss': Array(0.01240146, dtype=float32), 'training/total_loss': Array(36.601513, dtype=float32), 'training/v_loss': Array(36.532562, dtype=float32), 'eval/episode_goal_distance': (Array(0.40600765, dtype=float32), Array(0.1632665, dtype=float32)), 'eval/episode_reward': (Array(-9078.986, dtype=float32), Array(4657.9355, dtype=float32)), 'eval/avg_episode_length': (Array(875.7344, dtype=float32), Array(328.7769, dtype=float32)), 'eval/epoch_eval_time': 4.118498802185059, 'eval/sps': 31079.285474622437}
I0727 16:27:10.139545 140141833336640 train.py:379] starting iteration 39 416.7824852466583
I0727 16:27:19.983224 140141833336640 train.py:394] {'eval/walltime': 174.38922142982483, 'training/sps': 42918.84113749785, 'training/walltime': 243.95772218704224, 'training/entropy_loss': Array(0.07940308, dtype=float32), 'training/policy_loss': Array(0.00983995, dtype=float32), 'training/total_loss': Array(34.19545, dtype=float32), 'training/v_loss': Array(34.1062, dtype=float32), 'eval/episode_goal_distance': (Array(0.4011182, dtype=float32), Array(0.1437387, dtype=float32)), 'eval/episode_reward': (Array(-8523.361, dtype=float32), Array(5124.4663, dtype=float32)), 'eval/avg_episode_length': (Array(805.9375, dtype=float32), Array(393.90472, dtype=float32)), 'eval/epoch_eval_time': 4.113529920578003, 'eval/sps': 31116.82726790872}
I0727 16:27:19.987343 140141833336640 train.py:379] starting iteration 40 426.63028287887573
I0727 16:27:29.831145 140141833336640 train.py:394] {'eval/walltime': 178.51614618301392, 'training/sps': 43016.75759346701, 'training/walltime': 249.67084455490112, 'training/entropy_loss': Array(0.10413846, dtype=float32), 'training/policy_loss': Array(0.0107138, dtype=float32), 'training/total_loss': Array(31.926447, dtype=float32), 'training/v_loss': Array(31.811596, dtype=float32), 'eval/episode_goal_distance': (Array(0.39373267, dtype=float32), Array(0.12871826, dtype=float32)), 'eval/episode_reward': (Array(-9239.354, dtype=float32), Array(4223.7573, dtype=float32)), 'eval/avg_episode_length': (Array(891.2578, dtype=float32), Array(310.3037, dtype=float32)), 'eval/epoch_eval_time': 4.126924753189087, 'eval/sps': 31015.830831683525}
I0727 16:27:29.833413 140141833336640 train.py:379] starting iteration 41 436.4763672351837
I0727 16:27:39.659555 140141833336640 train.py:394] {'eval/walltime': 182.64377069473267, 'training/sps': 43153.289960625465, 'training/walltime': 255.36589121818542, 'training/entropy_loss': Array(0.08960611, dtype=float32), 'training/policy_loss': Array(0.01329665, dtype=float32), 'training/total_loss': Array(85.97015, dtype=float32), 'training/v_loss': Array(85.86725, dtype=float32), 'eval/episode_goal_distance': (Array(0.4167679, dtype=float32), Array(0.1663664, dtype=float32)), 'eval/episode_reward': (Array(-9521.396, dtype=float32), Array(5055.319, dtype=float32)), 'eval/avg_episode_length': (Array(875.72656, dtype=float32), Array(328.79715, dtype=float32)), 'eval/epoch_eval_time': 4.12762451171875, 'eval/sps': 31010.57270025286}
I0727 16:27:39.661844 140141833336640 train.py:379] starting iteration 42 446.30479860305786
I0727 16:27:49.471594 140141833336640 train.py:394] {'eval/walltime': 186.72303771972656, 'training/sps': 42913.328946493064, 'training/walltime': 261.09278321266174, 'training/entropy_loss': Array(-0.02907733, dtype=float32), 'training/policy_loss': Array(0.01937439, dtype=float32), 'training/total_loss': Array(75.71326, dtype=float32), 'training/v_loss': Array(75.72296, dtype=float32), 'eval/episode_goal_distance': (Array(0.49687663, dtype=float32), Array(0.23640901, dtype=float32)), 'eval/episode_reward': (Array(-9546.867, dtype=float32), Array(5551.633, dtype=float32)), 'eval/avg_episode_length': (Array(836.85156, dtype=float32), Array(368.2699, dtype=float32)), 'eval/epoch_eval_time': 4.0792670249938965, 'eval/sps': 31378.18613386593}
I0727 16:27:49.474431 140141833336640 train.py:379] starting iteration 43 456.1173851490021
I0727 16:27:59.311393 140141833336640 train.py:394] {'eval/walltime': 190.83378148078918, 'training/sps': 42945.00448013797, 'training/walltime': 266.8154511451721, 'training/entropy_loss': Array(-0.04596208, dtype=float32), 'training/policy_loss': Array(0.00614582, dtype=float32), 'training/total_loss': Array(53.739674, dtype=float32), 'training/v_loss': Array(53.779488, dtype=float32), 'eval/episode_goal_distance': (Array(0.47218746, dtype=float32), Array(0.22861923, dtype=float32)), 'eval/episode_reward': (Array(-9694.246, dtype=float32), Array(5448.0176, dtype=float32)), 'eval/avg_episode_length': (Array(860.21094, dtype=float32), Array(345.56833, dtype=float32)), 'eval/epoch_eval_time': 4.110743761062622, 'eval/sps': 31137.917476742008}
I0727 16:27:59.313686 140141833336640 train.py:379] starting iteration 44 465.95664048194885
I0727 16:28:09.147034 140141833336640 train.py:394] {'eval/walltime': 194.92117142677307, 'training/sps': 42796.95025979167, 'training/walltime': 272.5579164028168, 'training/entropy_loss': Array(-0.04596817, dtype=float32), 'training/policy_loss': Array(0.00557596, dtype=float32), 'training/total_loss': Array(53.029434, dtype=float32), 'training/v_loss': Array(53.069824, dtype=float32), 'eval/episode_goal_distance': (Array(0.47451106, dtype=float32), Array(0.21963388, dtype=float32)), 'eval/episode_reward': (Array(-9889.4375, dtype=float32), Array(5027.4326, dtype=float32)), 'eval/avg_episode_length': (Array(883.53906, dtype=float32), Array(319.65033, dtype=float32)), 'eval/epoch_eval_time': 4.087389945983887, 'eval/sps': 31315.82787342517}
I0727 16:28:09.149316 140141833336640 train.py:379] starting iteration 45 475.7922704219818
I0727 16:28:18.998537 140141833336640 train.py:394] {'eval/walltime': 199.01521801948547, 'training/sps': 42728.9955019804, 'training/walltime': 278.3095142841339, 'training/entropy_loss': Array(-0.04616015, dtype=float32), 'training/policy_loss': Array(0.00606338, dtype=float32), 'training/total_loss': Array(56.393505, dtype=float32), 'training/v_loss': Array(56.4336, dtype=float32), 'eval/episode_goal_distance': (Array(0.47043678, dtype=float32), Array(0.1832253, dtype=float32)), 'eval/episode_reward': (Array(-9180.584, dtype=float32), Array(5215.8247, dtype=float32)), 'eval/avg_episode_length': (Array(829.21094, dtype=float32), Array(374.88873, dtype=float32)), 'eval/epoch_eval_time': 4.094046592712402, 'eval/sps': 31264.91042574994}
I0727 16:28:19.000805 140141833336640 train.py:379] starting iteration 46 485.6437587738037
I0727 16:28:28.813754 140141833336640 train.py:394] {'eval/walltime': 203.10445928573608, 'training/sps': 42963.26904451638, 'training/walltime': 284.02974939346313, 'training/entropy_loss': Array(-0.04592323, dtype=float32), 'training/policy_loss': Array(0.00645467, dtype=float32), 'training/total_loss': Array(57.969826, dtype=float32), 'training/v_loss': Array(58.009293, dtype=float32), 'eval/episode_goal_distance': (Array(0.48045623, dtype=float32), Array(0.20983353, dtype=float32)), 'eval/episode_reward': (Array(-10141.212, dtype=float32), Array(4856.5034, dtype=float32)), 'eval/avg_episode_length': (Array(899.1094, dtype=float32), Array(300.07452, dtype=float32)), 'eval/epoch_eval_time': 4.08924126625061, 'eval/sps': 31301.650273465544}
I0727 16:28:28.816215 140141833336640 train.py:379] starting iteration 47 495.45916962623596
I0727 16:28:38.658452 140141833336640 train.py:394] {'eval/walltime': 207.18971180915833, 'training/sps': 42715.45167976643, 'training/walltime': 289.7831709384918, 'training/entropy_loss': Array(-0.04580679, dtype=float32), 'training/policy_loss': Array(0.00500816, dtype=float32), 'training/total_loss': Array(58.32061, dtype=float32), 'training/v_loss': Array(58.361412, dtype=float32), 'eval/episode_goal_distance': (Array(0.4751312, dtype=float32), Array(0.23650256, dtype=float32)), 'eval/episode_reward': (Array(-10456.674, dtype=float32), Array(4778.6807, dtype=float32)), 'eval/avg_episode_length': (Array(937.8828, dtype=float32), Array(240.57936, dtype=float32)), 'eval/epoch_eval_time': 4.085252523422241, 'eval/sps': 31332.212455932495}
I0727 16:28:38.660870 140141833336640 train.py:379] starting iteration 48 505.3038241863251
I0727 16:28:48.513170 140141833336640 train.py:394] {'eval/walltime': 211.27332639694214, 'training/sps': 42628.80417477628, 'training/walltime': 295.54828691482544, 'training/entropy_loss': Array(-0.04517967, dtype=float32), 'training/policy_loss': Array(0.00513997, dtype=float32), 'training/total_loss': Array(59.687927, dtype=float32), 'training/v_loss': Array(59.727966, dtype=float32), 'eval/episode_goal_distance': (Array(0.45212096, dtype=float32), Array(0.19996653, dtype=float32)), 'eval/episode_reward': (Array(-9333.27, dtype=float32), Array(4178.8813, dtype=float32)), 'eval/avg_episode_length': (Array(914.5625, dtype=float32), Array(278.6414, dtype=float32)), 'eval/epoch_eval_time': 4.0836145877838135, 'eval/sps': 31344.77978967792}
I0727 16:28:48.515572 140141833336640 train.py:379] starting iteration 49 515.1585261821747
I0727 16:28:58.379103 140141833336640 train.py:394] {'eval/walltime': 215.37175178527832, 'training/sps': 42656.13300339824, 'training/walltime': 301.3097093105316, 'training/entropy_loss': Array(-0.04504123, dtype=float32), 'training/policy_loss': Array(0.00678736, dtype=float32), 'training/total_loss': Array(58.66314, dtype=float32), 'training/v_loss': Array(58.701393, dtype=float32), 'eval/episode_goal_distance': (Array(0.46115404, dtype=float32), Array(0.20941028, dtype=float32)), 'eval/episode_reward': (Array(-9795.02, dtype=float32), Array(4263.24, dtype=float32)), 'eval/avg_episode_length': (Array(930.1406, dtype=float32), Array(254.02585, dtype=float32)), 'eval/epoch_eval_time': 4.098425388336182, 'eval/sps': 31231.506706033644}
I0727 16:28:58.381626 140141833336640 train.py:379] starting iteration 50 525.0245804786682
I0727 16:29:08.244747 140141833336640 train.py:394] {'eval/walltime': 219.467289686203, 'training/sps': 42637.088031821375, 'training/walltime': 307.0737051963806, 'training/entropy_loss': Array(-0.04424689, dtype=float32), 'training/policy_loss': Array(0.0045374, dtype=float32), 'training/total_loss': Array(84.92699, dtype=float32), 'training/v_loss': Array(84.966705, dtype=float32), 'eval/episode_goal_distance': (Array(0.47912842, dtype=float32), Array(0.21614975, dtype=float32)), 'eval/episode_reward': (Array(-9283.81, dtype=float32), Array(5254.6157, dtype=float32)), 'eval/avg_episode_length': (Array(844.78906, dtype=float32), Array(360.67856, dtype=float32)), 'eval/epoch_eval_time': 4.095537900924683, 'eval/sps': 31253.525933943965}
I0727 16:29:08.247263 140141833336640 train.py:379] starting iteration 51 534.8902177810669
I0727 16:29:18.152082 140141833336640 train.py:394] {'eval/walltime': 223.58700346946716, 'training/sps': 42507.60450010008, 'training/walltime': 312.8552589416504, 'training/entropy_loss': Array(-0.04512842, dtype=float32), 'training/policy_loss': Array(0.00387682, dtype=float32), 'training/total_loss': Array(45.507927, dtype=float32), 'training/v_loss': Array(45.54917, dtype=float32), 'eval/episode_goal_distance': (Array(0.42256308, dtype=float32), Array(0.1752602, dtype=float32)), 'eval/episode_reward': (Array(-8375.082, dtype=float32), Array(5109.314, dtype=float32)), 'eval/avg_episode_length': (Array(805.75, dtype=float32), Array(394.28506, dtype=float32)), 'eval/epoch_eval_time': 4.11971378326416, 'eval/sps': 31070.11960879043}
I0727 16:29:18.198330 140141833336640 train.py:379] starting iteration 52 544.8412754535675
I0727 16:29:28.051086 140141833336640 train.py:394] {'eval/walltime': 227.66141271591187, 'training/sps': 42558.70563738278, 'training/walltime': 318.62987065315247, 'training/entropy_loss': Array(-0.04498802, dtype=float32), 'training/policy_loss': Array(0.00623978, dtype=float32), 'training/total_loss': Array(43.156967, dtype=float32), 'training/v_loss': Array(43.195713, dtype=float32), 'eval/episode_goal_distance': (Array(0.44553325, dtype=float32), Array(0.19494246, dtype=float32)), 'eval/episode_reward': (Array(-9383.766, dtype=float32), Array(4248.95, dtype=float32)), 'eval/avg_episode_length': (Array(914.6406, dtype=float32), Array(278.38724, dtype=float32)), 'eval/epoch_eval_time': 4.074409246444702, 'eval/sps': 31415.59727012985}
I0727 16:29:28.053579 140141833336640 train.py:379] starting iteration 53 554.696533203125
I0727 16:29:37.894603 140141833336640 train.py:394] {'eval/walltime': 231.73878955841064, 'training/sps': 42664.79126025709, 'training/walltime': 324.39012384414673, 'training/entropy_loss': Array(-0.04411753, dtype=float32), 'training/policy_loss': Array(0.00445888, dtype=float32), 'training/total_loss': Array(41.37157, dtype=float32), 'training/v_loss': Array(41.411232, dtype=float32), 'eval/episode_goal_distance': (Array(0.42473292, dtype=float32), Array(0.18991818, dtype=float32)), 'eval/episode_reward': (Array(-9204.098, dtype=float32), Array(4691.5806, dtype=float32)), 'eval/avg_episode_length': (Array(883.53906, dtype=float32), Array(319.6502, dtype=float32)), 'eval/epoch_eval_time': 4.077376842498779, 'eval/sps': 31392.73237289406}
I0727 16:29:37.896954 140141833336640 train.py:379] starting iteration 54 564.5399086475372
I0727 16:29:47.808592 140141833336640 train.py:394] {'eval/walltime': 235.85114645957947, 'training/sps': 42403.57324156189, 'training/walltime': 330.185861825943, 'training/entropy_loss': Array(-0.04400075, dtype=float32), 'training/policy_loss': Array(0.00513859, dtype=float32), 'training/total_loss': Array(42.44917, dtype=float32), 'training/v_loss': Array(42.48803, dtype=float32), 'eval/episode_goal_distance': (Array(0.44720086, dtype=float32), Array(0.1988547, dtype=float32)), 'eval/episode_reward': (Array(-9580.164, dtype=float32), Array(4721.318, dtype=float32)), 'eval/avg_episode_length': (Array(906.7031, dtype=float32), Array(290.072, dtype=float32)), 'eval/epoch_eval_time': 4.112356901168823, 'eval/sps': 31125.703112883893}
I0727 16:29:47.811059 140141833336640 train.py:379] starting iteration 55 574.4540131092072
I0727 16:29:57.690984 140141833336640 train.py:394] {'eval/walltime': 239.9380190372467, 'training/sps': 42449.097286387434, 'training/walltime': 335.9753842353821, 'training/entropy_loss': Array(-0.04329527, dtype=float32), 'training/policy_loss': Array(0.00362541, dtype=float32), 'training/total_loss': Array(41.209038, dtype=float32), 'training/v_loss': Array(41.24871, dtype=float32), 'eval/episode_goal_distance': (Array(0.44924033, dtype=float32), Array(0.17809096, dtype=float32)), 'eval/episode_reward': (Array(-9758.617, dtype=float32), Array(4308.1646, dtype=float32)), 'eval/avg_episode_length': (Array(922.3672, dtype=float32), Array(266.67816, dtype=float32)), 'eval/epoch_eval_time': 4.086872577667236, 'eval/sps': 31319.792229260467}
I0727 16:29:57.693483 140141833336640 train.py:379] starting iteration 56 584.3364374637604
I0727 16:30:07.603849 140141833336640 train.py:394] {'eval/walltime': 244.02959871292114, 'training/sps': 42262.34979962131, 'training/walltime': 341.79048919677734, 'training/entropy_loss': Array(-0.04286106, dtype=float32), 'training/policy_loss': Array(0.00381446, dtype=float32), 'training/total_loss': Array(40.343258, dtype=float32), 'training/v_loss': Array(40.3823, dtype=float32), 'eval/episode_goal_distance': (Array(0.44715196, dtype=float32), Array(0.21252096, dtype=float32)), 'eval/episode_reward': (Array(-8994.9375, dtype=float32), Array(4586.6177, dtype=float32)), 'eval/avg_episode_length': (Array(898.9922, dtype=float32), Array(300.42297, dtype=float32)), 'eval/epoch_eval_time': 4.0915796756744385, 'eval/sps': 31283.760832275868}
I0727 16:30:07.606221 140141833336640 train.py:379] starting iteration 57 594.2491755485535
I0727 16:30:17.525623 140141833336640 train.py:394] {'eval/walltime': 248.14534068107605, 'training/sps': 42371.30116811349, 'training/walltime': 347.5906414985657, 'training/entropy_loss': Array(-0.04245018, dtype=float32), 'training/policy_loss': Array(0.00189018, dtype=float32), 'training/total_loss': Array(41.602615, dtype=float32), 'training/v_loss': Array(41.643173, dtype=float32), 'eval/episode_goal_distance': (Array(0.45401055, dtype=float32), Array(0.20515625, dtype=float32)), 'eval/episode_reward': (Array(-9205.06, dtype=float32), Array(5045.5903, dtype=float32)), 'eval/avg_episode_length': (Array(860.1875, dtype=float32), Array(345.62656, dtype=float32)), 'eval/epoch_eval_time': 4.115741968154907, 'eval/sps': 31100.103211130743}
I0727 16:30:17.527925 140141833336640 train.py:379] starting iteration 58 604.1708798408508
I0727 16:30:27.432824 140141833336640 train.py:394] {'eval/walltime': 252.22552108764648, 'training/sps': 42218.027506966355, 'training/walltime': 353.4118514060974, 'training/entropy_loss': Array(-0.04219941, dtype=float32), 'training/policy_loss': Array(0.00092696, dtype=float32), 'training/total_loss': Array(78.06978, dtype=float32), 'training/v_loss': Array(78.111046, dtype=float32), 'eval/episode_goal_distance': (Array(0.43110582, dtype=float32), Array(0.19033945, dtype=float32)), 'eval/episode_reward': (Array(-9287.963, dtype=float32), Array(4067.7603, dtype=float32)), 'eval/avg_episode_length': (Array(930.0625, dtype=float32), Array(254.30959, dtype=float32)), 'eval/epoch_eval_time': 4.080180406570435, 'eval/sps': 31371.161871636323}
I0727 16:30:27.435212 140141833336640 train.py:379] starting iteration 59 614.0781662464142
I0727 16:30:37.344656 140141833336640 train.py:394] {'eval/walltime': 256.3285572528839, 'training/sps': 42350.58163223047, 'training/walltime': 359.2148413658142, 'training/entropy_loss': Array(-0.04033685, dtype=float32), 'training/policy_loss': Array(0.00128804, dtype=float32), 'training/total_loss': Array(40.455437, dtype=float32), 'training/v_loss': Array(40.494488, dtype=float32), 'eval/episode_goal_distance': (Array(0.43947607, dtype=float32), Array(0.17600057, dtype=float32)), 'eval/episode_reward': (Array(-8740.203, dtype=float32), Array(4562.528, dtype=float32)), 'eval/avg_episode_length': (Array(860.21875, dtype=float32), Array(345.5492, dtype=float32)), 'eval/epoch_eval_time': 4.103036165237427, 'eval/sps': 31196.4103764104}
I0727 16:30:37.346938 140141833336640 train.py:379] starting iteration 60 623.9898924827576
I0727 16:30:47.230216 140141833336640 train.py:394] {'eval/walltime': 260.41125869750977, 'training/sps': 42394.08260961995, 'training/walltime': 365.01187682151794, 'training/entropy_loss': Array(-0.03846674, dtype=float32), 'training/policy_loss': Array(0.00106954, dtype=float32), 'training/total_loss': Array(28.740566, dtype=float32), 'training/v_loss': Array(28.777962, dtype=float32), 'eval/episode_goal_distance': (Array(0.44711238, dtype=float32), Array(0.17120653, dtype=float32)), 'eval/episode_reward': (Array(-9081.945, dtype=float32), Array(4539.433, dtype=float32)), 'eval/avg_episode_length': (Array(867.96094, dtype=float32), Array(337.39624, dtype=float32)), 'eval/epoch_eval_time': 4.0827014446258545, 'eval/sps': 31351.79041036397}
I0727 16:30:47.235067 140141833336640 train.py:379] starting iteration 61 633.878006696701
I0727 16:30:57.162521 140141833336640 train.py:394] {'eval/walltime': 264.4978346824646, 'training/sps': 42103.506509489394, 'training/walltime': 370.8489203453064, 'training/entropy_loss': Array(-0.03759819, dtype=float32), 'training/policy_loss': Array(0.00209407, dtype=float32), 'training/total_loss': Array(27.589325, dtype=float32), 'training/v_loss': Array(27.624828, dtype=float32), 'eval/episode_goal_distance': (Array(0.42286128, dtype=float32), Array(0.17031078, dtype=float32)), 'eval/episode_reward': (Array(-8854.949, dtype=float32), Array(4133.6685, dtype=float32)), 'eval/avg_episode_length': (Array(906.7969, dtype=float32), Array(289.78073, dtype=float32)), 'eval/epoch_eval_time': 4.086575984954834, 'eval/sps': 31322.065335685835}
I0727 16:30:57.164844 140141833336640 train.py:379] starting iteration 62 643.8077988624573
I0727 16:31:07.088438 140141833336640 train.py:394] {'eval/walltime': 268.6175253391266, 'training/sps': 42369.622239140605, 'training/walltime': 376.649302482605, 'training/entropy_loss': Array(-0.03677929, dtype=float32), 'training/policy_loss': Array(0.00164455, dtype=float32), 'training/total_loss': Array(27.889042, dtype=float32), 'training/v_loss': Array(27.924177, dtype=float32), 'eval/episode_goal_distance': (Array(0.42495546, dtype=float32), Array(0.15697278, dtype=float32)), 'eval/episode_reward': (Array(-8701.809, dtype=float32), Array(4193.473, dtype=float32)), 'eval/avg_episode_length': (Array(883.39844, dtype=float32), Array(320.0356, dtype=float32)), 'eval/epoch_eval_time': 4.119690656661987, 'eval/sps': 31070.294026326977}
I0727 16:31:07.091060 140141833336640 train.py:379] starting iteration 63 653.7340133190155
I0727 16:31:17.004297 140141833336640 train.py:394] {'eval/walltime': 272.7060935497284, 'training/sps': 42218.7744994039, 'training/walltime': 382.47040939331055, 'training/entropy_loss': Array(-0.03440515, dtype=float32), 'training/policy_loss': Array(0.00212971, dtype=float32), 'training/total_loss': Array(27.429436, dtype=float32), 'training/v_loss': Array(27.461712, dtype=float32), 'eval/episode_goal_distance': (Array(0.41432747, dtype=float32), Array(0.14720039, dtype=float32)), 'eval/episode_reward': (Array(-8112.954, dtype=float32), Array(4580.811, dtype=float32)), 'eval/avg_episode_length': (Array(829.08594, dtype=float32), Array(375.1631, dtype=float32)), 'eval/epoch_eval_time': 4.088568210601807, 'eval/sps': 31306.80311706463}
I0727 16:31:17.006803 140141833336640 train.py:379] starting iteration 64 663.6497573852539
I0727 16:31:26.898946 140141833336640 train.py:394] {'eval/walltime': 276.79261016845703, 'training/sps': 42356.364420530896, 'training/walltime': 388.272607088089, 'training/entropy_loss': Array(-0.03130276, dtype=float32), 'training/policy_loss': Array(0.00244704, dtype=float32), 'training/total_loss': Array(26.00327, dtype=float32), 'training/v_loss': Array(26.032125, dtype=float32), 'eval/episode_goal_distance': (Array(0.40348417, dtype=float32), Array(0.13261078, dtype=float32)), 'eval/episode_reward': (Array(-8467.535, dtype=float32), Array(4421.2207, dtype=float32)), 'eval/avg_episode_length': (Array(844.6094, dtype=float32), Array(361.09622, dtype=float32)), 'eval/epoch_eval_time': 4.086516618728638, 'eval/sps': 31322.520362054045}
I0727 16:31:26.901180 140141833336640 train.py:379] starting iteration 65 673.5441339015961
I0727 16:31:36.793922 140141833336640 train.py:394] {'eval/walltime': 280.8666398525238, 'training/sps': 42262.31514462817, 'training/walltime': 394.08771681785583, 'training/entropy_loss': Array(-0.02841184, dtype=float32), 'training/policy_loss': Array(0.00330978, dtype=float32), 'training/total_loss': Array(25.15229, dtype=float32), 'training/v_loss': Array(25.177393, dtype=float32), 'eval/episode_goal_distance': (Array(0.38351983, dtype=float32), Array(0.10552912, dtype=float32)), 'eval/episode_reward': (Array(-7655.6523, dtype=float32), Array(4344.0527, dtype=float32)), 'eval/avg_episode_length': (Array(821.34375, dtype=float32), Array(381.72433, dtype=float32)), 'eval/epoch_eval_time': 4.0740296840667725, 'eval/sps': 31418.524145908534}
I0727 16:31:36.796394 140141833336640 train.py:379] starting iteration 66 683.4393484592438
I0727 16:31:46.692420 140141833336640 train.py:394] {'eval/walltime': 284.95277881622314, 'training/sps': 42325.2781124129, 'training/walltime': 399.89417600631714, 'training/entropy_loss': Array(-0.02555455, dtype=float32), 'training/policy_loss': Array(0.00328707, dtype=float32), 'training/total_loss': Array(65.50584, dtype=float32), 'training/v_loss': Array(65.52811, dtype=float32), 'eval/episode_goal_distance': (Array(0.3854174, dtype=float32), Array(0.1187111, dtype=float32)), 'eval/episode_reward': (Array(-7516.3164, dtype=float32), Array(4426.474, dtype=float32)), 'eval/avg_episode_length': (Array(805.78906, dtype=float32), Array(394.20572, dtype=float32)), 'eval/epoch_eval_time': 4.086138963699341, 'eval/sps': 31325.415297211675}
I0727 16:31:46.694904 140141833336640 train.py:379] starting iteration 67 693.3378586769104
I0727 16:31:56.628124 140141833336640 train.py:394] {'eval/walltime': 289.064071893692, 'training/sps': 42240.0420141638, 'training/walltime': 405.7123520374298, 'training/entropy_loss': Array(-0.02444879, dtype=float32), 'training/policy_loss': Array(0.00405539, dtype=float32), 'training/total_loss': Array(32.99077, dtype=float32), 'training/v_loss': Array(33.01116, dtype=float32), 'eval/episode_goal_distance': (Array(0.39400297, dtype=float32), Array(0.12379296, dtype=float32)), 'eval/episode_reward': (Array(-8025.0137, dtype=float32), Array(4600.1035, dtype=float32)), 'eval/avg_episode_length': (Array(813.58594, dtype=float32), Array(388.0524, dtype=float32)), 'eval/epoch_eval_time': 4.111293077468872, 'eval/sps': 31133.757090069463}
I0727 16:31:56.630504 140141833336640 train.py:379] starting iteration 68 703.273458480835
I0727 16:32:06.523351 140141833336640 train.py:394] {'eval/walltime': 293.1498520374298, 'training/sps': 42346.447810933256, 'training/walltime': 411.51590847969055, 'training/entropy_loss': Array(-0.02128431, dtype=float32), 'training/policy_loss': Array(0.00443095, dtype=float32), 'training/total_loss': Array(19.977009, dtype=float32), 'training/v_loss': Array(19.993862, dtype=float32), 'eval/episode_goal_distance': (Array(0.3690296, dtype=float32), Array(0.1180585, dtype=float32)), 'eval/episode_reward': (Array(-8278.383, dtype=float32), Array(3653.225, dtype=float32)), 'eval/avg_episode_length': (Array(906.8125, dtype=float32), Array(289.73224, dtype=float32)), 'eval/epoch_eval_time': 4.085780143737793, 'eval/sps': 31328.16634692972}
I0727 16:32:06.525481 140141833336640 train.py:379] starting iteration 69 713.1684358119965
I0727 16:32:16.481957 140141833336640 train.py:394] {'eval/walltime': 297.25953221321106, 'training/sps': 42058.58816714964, 'training/walltime': 417.3591859340668, 'training/entropy_loss': Array(-0.01758488, dtype=float32), 'training/policy_loss': Array(0.00465703, dtype=float32), 'training/total_loss': Array(16.317553, dtype=float32), 'training/v_loss': Array(16.330479, dtype=float32), 'eval/episode_goal_distance': (Array(0.36722624, dtype=float32), Array(0.12190085, dtype=float32)), 'eval/episode_reward': (Array(-7765.001, dtype=float32), Array(4453.986, dtype=float32)), 'eval/avg_episode_length': (Array(821.40625, dtype=float32), Array(381.5907, dtype=float32)), 'eval/epoch_eval_time': 4.10968017578125, 'eval/sps': 31145.97597017807}
I0727 16:32:16.484491 140141833336640 train.py:379] starting iteration 70 723.1274454593658
I0727 16:32:26.400712 140141833336640 train.py:394] {'eval/walltime': 301.3479278087616, 'training/sps': 42195.48147302158, 'training/walltime': 423.18350625038147, 'training/entropy_loss': Array(-0.01412774, dtype=float32), 'training/policy_loss': Array(0.00503119, dtype=float32), 'training/total_loss': Array(16.761545, dtype=float32), 'training/v_loss': Array(16.770641, dtype=float32), 'eval/episode_goal_distance': (Array(0.3644049, dtype=float32), Array(0.11520199, dtype=float32)), 'eval/episode_reward': (Array(-8288.207, dtype=float32), Array(3850.892, dtype=float32)), 'eval/avg_episode_length': (Array(899.08594, dtype=float32), Array(300.1443, dtype=float32)), 'eval/epoch_eval_time': 4.088395595550537, 'eval/sps': 31308.124913182164}
I0727 16:32:26.403027 140141833336640 train.py:379] starting iteration 71 733.045981168747
I0727 16:32:36.360360 140141833336640 train.py:394] {'eval/walltime': 305.46397161483765, 'training/sps': 42097.73753731261, 'training/walltime': 429.0213496685028, 'training/entropy_loss': Array(-0.01174073, dtype=float32), 'training/policy_loss': Array(0.00632261, dtype=float32), 'training/total_loss': Array(15.349158, dtype=float32), 'training/v_loss': Array(15.354578, dtype=float32), 'eval/episode_goal_distance': (Array(0.34746373, dtype=float32), Array(0.09990723, dtype=float32)), 'eval/episode_reward': (Array(-7691.2974, dtype=float32), Array(3666.9314, dtype=float32)), 'eval/avg_episode_length': (Array(875.72656, dtype=float32), Array(328.79742, dtype=float32)), 'eval/epoch_eval_time': 4.11604380607605, 'eval/sps': 31097.822576875416}
I0727 16:32:36.362717 140141833336640 train.py:379] starting iteration 72 743.0056715011597
I0727 16:32:46.311833 140141833336640 train.py:394] {'eval/walltime': 309.5630371570587, 'training/sps': 42035.67378992165, 'training/walltime': 434.8678123950958, 'training/entropy_loss': Array(-0.0080875, dtype=float32), 'training/policy_loss': Array(0.00747438, dtype=float32), 'training/total_loss': Array(14.629778, dtype=float32), 'training/v_loss': Array(14.630392, dtype=float32), 'eval/episode_goal_distance': (Array(0.34684038, dtype=float32), Array(0.10862073, dtype=float32)), 'eval/episode_reward': (Array(-7069.2944, dtype=float32), Array(4193.2837, dtype=float32)), 'eval/avg_episode_length': (Array(813.53906, dtype=float32), Array(388.15033, dtype=float32)), 'eval/epoch_eval_time': 4.099065542221069, 'eval/sps': 31226.62926015169}
I0727 16:32:46.314208 140141833336640 train.py:379] starting iteration 73 752.9571626186371
I0727 16:32:56.276100 140141833336640 train.py:394] {'eval/walltime': 313.6921212673187, 'training/sps': 42159.01057921923, 'training/walltime': 440.6971712112427, 'training/entropy_loss': Array(-0.00314387, dtype=float32), 'training/policy_loss': Array(0.00679261, dtype=float32), 'training/total_loss': Array(14.809856, dtype=float32), 'training/v_loss': Array(14.806208, dtype=float32), 'eval/episode_goal_distance': (Array(0.3258209, dtype=float32), Array(0.10542802, dtype=float32)), 'eval/episode_reward': (Array(-7096.172, dtype=float32), Array(3774.4736, dtype=float32)), 'eval/avg_episode_length': (Array(852.39844, dtype=float32), Array(353.5316, dtype=float32)), 'eval/epoch_eval_time': 4.12908411026001, 'eval/sps': 30999.610708327225}
I0727 16:32:56.278514 140141833336640 train.py:379] starting iteration 74 762.9214670658112
I0727 16:33:06.240114 140141833336640 train.py:394] {'eval/walltime': 317.7910487651825, 'training/sps': 41943.83190828473, 'training/walltime': 446.556435585022, 'training/entropy_loss': Array(0.00042149, dtype=float32), 'training/policy_loss': Array(0.00585344, dtype=float32), 'training/total_loss': Array(14.367838, dtype=float32), 'training/v_loss': Array(14.361563, dtype=float32), 'eval/episode_goal_distance': (Array(0.33667633, dtype=float32), Array(0.09309011, dtype=float32)), 'eval/episode_reward': (Array(-7180.908, dtype=float32), Array(4043.7695, dtype=float32)), 'eval/avg_episode_length': (Array(821.39844, dtype=float32), Array(381.60715, dtype=float32)), 'eval/epoch_eval_time': 4.0989274978637695, 'eval/sps': 31227.68091572967}
I0727 16:33:06.242330 140141833336640 train.py:379] starting iteration 75 772.8852846622467
I0727 16:33:16.179574 140141833336640 train.py:394] {'eval/walltime': 321.87499356269836, 'training/sps': 42010.82382149225, 'training/walltime': 452.40635657310486, 'training/entropy_loss': Array(0.00221886, dtype=float32), 'training/policy_loss': Array(0.00944436, dtype=float32), 'training/total_loss': Array(78.43579, dtype=float32), 'training/v_loss': Array(78.42413, dtype=float32), 'eval/episode_goal_distance': (Array(0.33129904, dtype=float32), Array(0.09289879, dtype=float32)), 'eval/episode_reward': (Array(-7062.9478, dtype=float32), Array(3972.593, dtype=float32)), 'eval/avg_episode_length': (Array(813.5781, dtype=float32), Array(388.06915, dtype=float32)), 'eval/epoch_eval_time': 4.083944797515869, 'eval/sps': 31342.245389276133}
I0727 16:33:16.181790 140141833336640 train.py:379] starting iteration 76 782.8247444629669
I0727 16:33:26.166115 140141833336640 train.py:394] {'eval/walltime': 326.000447511673, 'training/sps': 41970.56097941827, 'training/walltime': 458.26188945770264, 'training/entropy_loss': Array(0.00562928, dtype=float32), 'training/policy_loss': Array(0.00778929, dtype=float32), 'training/total_loss': Array(16.54446, dtype=float32), 'training/v_loss': Array(16.53104, dtype=float32), 'eval/episode_goal_distance': (Array(0.35102856, dtype=float32), Array(0.10874432, dtype=float32)), 'eval/episode_reward': (Array(-7145.2812, dtype=float32), Array(4313.515, dtype=float32)), 'eval/avg_episode_length': (Array(805.71875, dtype=float32), Array(394.34854, dtype=float32)), 'eval/epoch_eval_time': 4.125453948974609, 'eval/sps': 31026.888575938334}
I0727 16:33:26.168346 140141833336640 train.py:379] starting iteration 77 792.8113005161285
I0727 16:33:36.094677 140141833336640 train.py:394] {'eval/walltime': 330.0853428840637, 'training/sps': 42096.234940925155, 'training/walltime': 464.0999412536621, 'training/entropy_loss': Array(0.00894109, dtype=float32), 'training/policy_loss': Array(0.00912076, dtype=float32), 'training/total_loss': Array(14.612389, dtype=float32), 'training/v_loss': Array(14.594326, dtype=float32), 'eval/episode_goal_distance': (Array(0.33931333, dtype=float32), Array(0.09887896, dtype=float32)), 'eval/episode_reward': (Array(-7559.4355, dtype=float32), Array(3794.1816, dtype=float32)), 'eval/avg_episode_length': (Array(852.39844, dtype=float32), Array(353.53168, dtype=float32)), 'eval/epoch_eval_time': 4.084895372390747, 'eval/sps': 31334.951897454855}
I0727 16:33:36.096968 140141833336640 train.py:379] starting iteration 78 802.7399213314056
I0727 16:33:46.030104 140141833336640 train.py:394] {'eval/walltime': 334.2063317298889, 'training/sps': 42309.29880504873, 'training/walltime': 469.908593416214, 'training/entropy_loss': Array(0.01284726, dtype=float32), 'training/policy_loss': Array(0.0085742, dtype=float32), 'training/total_loss': Array(13.745871, dtype=float32), 'training/v_loss': Array(13.724448, dtype=float32), 'eval/episode_goal_distance': (Array(0.3575707, dtype=float32), Array(0.11321863, dtype=float32)), 'eval/episode_reward': (Array(-6391.9077, dtype=float32), Array(4792.4946, dtype=float32)), 'eval/avg_episode_length': (Array(712.6328, dtype=float32), Array(450.6693, dtype=float32)), 'eval/epoch_eval_time': 4.120988845825195, 'eval/sps': 31060.50629806279}
I0727 16:33:46.032337 140141833336640 train.py:379] starting iteration 79 812.6752915382385
I0727 16:33:56.004619 140141833336640 train.py:394] {'eval/walltime': 338.31514072418213, 'training/sps': 41939.108208605445, 'training/walltime': 475.76851773262024, 'training/entropy_loss': Array(0.01827535, dtype=float32), 'training/policy_loss': Array(0.00706402, dtype=float32), 'training/total_loss': Array(11.50008, dtype=float32), 'training/v_loss': Array(11.474741, dtype=float32), 'eval/episode_goal_distance': (Array(0.34145164, dtype=float32), Array(0.10652014, dtype=float32)), 'eval/episode_reward': (Array(-7587.45, dtype=float32), Array(3967.4458, dtype=float32)), 'eval/avg_episode_length': (Array(852.3594, dtype=float32), Array(353.6251, dtype=float32)), 'eval/epoch_eval_time': 4.108808994293213, 'eval/sps': 31152.579781094995}
I0727 16:33:56.007402 140141833336640 train.py:379] starting iteration 80 822.6503565311432
I0727 16:34:05.931335 140141833336640 train.py:394] {'eval/walltime': 342.41213870048523, 'training/sps': 42201.05784462966, 'training/walltime': 481.5920684337616, 'training/entropy_loss': Array(0.02529805, dtype=float32), 'training/policy_loss': Array(0.00857742, dtype=float32), 'training/total_loss': Array(10.74761, dtype=float32), 'training/v_loss': Array(10.713736, dtype=float32), 'eval/episode_goal_distance': (Array(0.33114302, dtype=float32), Array(0.09220586, dtype=float32)), 'eval/episode_reward': (Array(-6808.0703, dtype=float32), Array(4000.9175, dtype=float32)), 'eval/avg_episode_length': (Array(798.0547, dtype=float32), Array(399.98917, dtype=float32)), 'eval/epoch_eval_time': 4.096997976303101, 'eval/sps': 31242.387899712845}
I0727 16:34:05.933527 140141833336640 train.py:379] starting iteration 81 832.5764813423157
I0727 16:34:15.870698 140141833336640 train.py:394] {'eval/walltime': 346.509211063385, 'training/sps': 42106.48360800248, 'training/walltime': 487.4286992549896, 'training/entropy_loss': Array(0.03249138, dtype=float32), 'training/policy_loss': Array(0.00973285, dtype=float32), 'training/total_loss': Array(10.522362, dtype=float32), 'training/v_loss': Array(10.480138, dtype=float32), 'eval/episode_goal_distance': (Array(0.32107508, dtype=float32), Array(0.09849664, dtype=float32)), 'eval/episode_reward': (Array(-6319.6074, dtype=float32), Array(4293.4097, dtype=float32)), 'eval/avg_episode_length': (Array(751.41406, dtype=float32), Array(430.56445, dtype=float32)), 'eval/epoch_eval_time': 4.09707236289978, 'eval/sps': 31241.820661767757}
I0727 16:34:15.874207 140141833336640 train.py:379] starting iteration 82 842.5171465873718
I0727 16:34:25.791709 140141833336640 train.py:394] {'eval/walltime': 350.5850200653076, 'training/sps': 42097.07046755303, 'training/walltime': 493.26663517951965, 'training/entropy_loss': Array(0.0397516, dtype=float32), 'training/policy_loss': Array(0.01058815, dtype=float32), 'training/total_loss': Array(10.324425, dtype=float32), 'training/v_loss': Array(10.274085, dtype=float32), 'eval/episode_goal_distance': (Array(0.33737874, dtype=float32), Array(0.09740563, dtype=float32)), 'eval/episode_reward': (Array(-7293.907, dtype=float32), Array(4225.679, dtype=float32)), 'eval/avg_episode_length': (Array(805.85156, dtype=float32), Array(394.0795, dtype=float32)), 'eval/epoch_eval_time': 4.075809001922607, 'eval/sps': 31404.808208535014}
I0727 16:34:25.793956 140141833336640 train.py:379] starting iteration 83 852.4369101524353
I0727 16:34:35.736967 140141833336640 train.py:394] {'eval/walltime': 354.6906008720398, 'training/sps': 42126.16953484802, 'training/walltime': 499.10053849220276, 'training/entropy_loss': Array(0.04628509, dtype=float32), 'training/policy_loss': Array(0.01328667, dtype=float32), 'training/total_loss': Array(77.899414, dtype=float32), 'training/v_loss': Array(77.83984, dtype=float32), 'eval/episode_goal_distance': (Array(0.3223182, dtype=float32), Array(0.09748076, dtype=float32)), 'eval/episode_reward': (Array(-6349.3076, dtype=float32), Array(4376.576, dtype=float32)), 'eval/avg_episode_length': (Array(743.7344, dtype=float32), Array(434.80692, dtype=float32)), 'eval/epoch_eval_time': 4.105580806732178, 'eval/sps': 31177.074822181163}
I0727 16:34:35.739496 140141833336640 train.py:379] starting iteration 84 862.3824498653412
I0727 16:34:45.691889 140141833336640 train.py:394] {'eval/walltime': 358.77074003219604, 'training/sps': 41876.69093621118, 'training/walltime': 504.9691970348358, 'training/entropy_loss': Array(0.05242757, dtype=float32), 'training/policy_loss': Array(0.01717176, dtype=float32), 'training/total_loss': Array(17.943325, dtype=float32), 'training/v_loss': Array(17.873726, dtype=float32), 'eval/episode_goal_distance': (Array(0.33743706, dtype=float32), Array(0.09691689, dtype=float32)), 'eval/episode_reward': (Array(-7253.823, dtype=float32), Array(4225.565, dtype=float32)), 'eval/avg_episode_length': (Array(805.84375, dtype=float32), Array(394.09534, dtype=float32)), 'eval/epoch_eval_time': 4.08013916015625, 'eval/sps': 31371.47900492154}
I0727 16:34:45.694655 140141833336640 train.py:379] starting iteration 85 872.3376092910767
I0727 16:34:55.673912 140141833336640 train.py:394] {'eval/walltime': 362.90441250801086, 'training/sps': 42067.76778186823, 'training/walltime': 510.811199426651, 'training/entropy_loss': Array(0.06339365, dtype=float32), 'training/policy_loss': Array(0.01695682, dtype=float32), 'training/total_loss': Array(12.282867, dtype=float32), 'training/v_loss': Array(12.2025175, dtype=float32), 'eval/episode_goal_distance': (Array(0.31662223, dtype=float32), Array(0.09992965, dtype=float32)), 'eval/episode_reward': (Array(-6720.5903, dtype=float32), Array(3995.3662, dtype=float32)), 'eval/avg_episode_length': (Array(813.7578, dtype=float32), Array(387.69504, dtype=float32)), 'eval/epoch_eval_time': 4.133672475814819, 'eval/sps': 30965.201222133343}
I0727 16:34:55.678740 140141833336640 train.py:379] starting iteration 86 882.3216784000397
I0727 16:35:05.619691 140141833336640 train.py:394] {'eval/walltime': 366.99227023124695, 'training/sps': 42016.551869192364, 'training/walltime': 516.6603229045868, 'training/entropy_loss': Array(0.07416128, dtype=float32), 'training/policy_loss': Array(0.01890271, dtype=float32), 'training/total_loss': Array(10.940268, dtype=float32), 'training/v_loss': Array(10.847203, dtype=float32), 'eval/episode_goal_distance': (Array(0.30816114, dtype=float32), Array(0.0989676, dtype=float32)), 'eval/episode_reward': (Array(-6776.5645, dtype=float32), Array(3900.056, dtype=float32)), 'eval/avg_episode_length': (Array(821.41406, dtype=float32), Array(381.5744, dtype=float32)), 'eval/epoch_eval_time': 4.087857723236084, 'eval/sps': 31312.244374951228}
I0727 16:35:05.622114 140141833336640 train.py:379] starting iteration 87 892.2650678157806
I0727 16:35:15.561248 140141833336640 train.py:394] {'eval/walltime': 371.07743310928345, 'training/sps': 42007.71300737904, 'training/walltime': 522.5106770992279, 'training/entropy_loss': Array(0.08331956, dtype=float32), 'training/policy_loss': Array(0.02288263, dtype=float32), 'training/total_loss': Array(10.985809, dtype=float32), 'training/v_loss': Array(10.879608, dtype=float32), 'eval/episode_goal_distance': (Array(0.33313584, dtype=float32), Array(0.10310596, dtype=float32)), 'eval/episode_reward': (Array(-6523.342, dtype=float32), Array(4514.3433, dtype=float32)), 'eval/avg_episode_length': (Array(743.7344, dtype=float32), Array(434.80704, dtype=float32)), 'eval/epoch_eval_time': 4.085162878036499, 'eval/sps': 31332.90001438625}
I0727 16:35:15.563738 140141833336640 train.py:379] starting iteration 88 902.2066917419434
I0727 16:35:25.560591 140141833336640 train.py:394] {'eval/walltime': 375.19198656082153, 'training/sps': 41804.140232788544, 'training/walltime': 528.3895206451416, 'training/entropy_loss': Array(0.09321953, dtype=float32), 'training/policy_loss': Array(0.03181076, dtype=float32), 'training/total_loss': Array(10.492138, dtype=float32), 'training/v_loss': Array(10.367107, dtype=float32), 'eval/episode_goal_distance': (Array(0.33834338, dtype=float32), Array(0.112434, dtype=float32)), 'eval/episode_reward': (Array(-6714.4077, dtype=float32), Array(4653.427, dtype=float32)), 'eval/avg_episode_length': (Array(751.4219, dtype=float32), Array(430.55072, dtype=float32)), 'eval/epoch_eval_time': 4.114553451538086, 'eval/sps': 31109.086686466922}
I0727 16:35:25.563003 140141833336640 train.py:379] starting iteration 89 912.2059571743011
I0727 16:35:35.586877 140141833336640 train.py:394] {'eval/walltime': 379.3190155029297, 'training/sps': 41702.06194720983, 'training/walltime': 534.2827544212341, 'training/entropy_loss': Array(0.106143, dtype=float32), 'training/policy_loss': Array(0.03021044, dtype=float32), 'training/total_loss': Array(10.229573, dtype=float32), 'training/v_loss': Array(10.09322, dtype=float32), 'eval/episode_goal_distance': (Array(0.32590112, dtype=float32), Array(0.09264054, dtype=float32)), 'eval/episode_reward': (Array(-6508.619, dtype=float32), Array(4221.732, dtype=float32)), 'eval/avg_episode_length': (Array(759.09375, dtype=float32), Array(426.14154, dtype=float32)), 'eval/epoch_eval_time': 4.127028942108154, 'eval/sps': 31015.04782145179}
I0727 16:35:35.589187 140141833336640 train.py:379] starting iteration 90 922.2321410179138
I0727 16:35:45.551525 140141833336640 train.py:394] {'eval/walltime': 383.4165790081024, 'training/sps': 41931.818256483035, 'training/walltime': 540.1436975002289, 'training/entropy_loss': Array(0.11979592, dtype=float32), 'training/policy_loss': Array(0.02927982, dtype=float32), 'training/total_loss': Array(10.020693, dtype=float32), 'training/v_loss': Array(9.871616, dtype=float32), 'eval/episode_goal_distance': (Array(0.30544156, dtype=float32), Array(0.08996701, dtype=float32)), 'eval/episode_reward': (Array(-6235.3115, dtype=float32), Array(3906.9248, dtype=float32)), 'eval/avg_episode_length': (Array(782.39844, dtype=float32), Array(411.22922, dtype=float32)), 'eval/epoch_eval_time': 4.0975635051727295, 'eval/sps': 31238.075953774453}
I0727 16:35:45.554041 140141833336640 train.py:379] starting iteration 91 932.1969957351685
I0727 16:35:55.518768 140141833336640 train.py:394] {'eval/walltime': 387.5099604129791, 'training/sps': 41884.744602493156, 'training/walltime': 546.011227607727, 'training/entropy_loss': Array(0.13225938, dtype=float32), 'training/policy_loss': Array(0.03733509, dtype=float32), 'training/total_loss': Array(77.01224, dtype=float32), 'training/v_loss': Array(76.84265, dtype=float32), 'eval/episode_goal_distance': (Array(0.32290256, dtype=float32), Array(0.09459262, dtype=float32)), 'eval/episode_reward': (Array(-6012.258, dtype=float32), Array(4451.962, dtype=float32)), 'eval/avg_episode_length': (Array(712.47656, dtype=float32), Array(450.9139, dtype=float32)), 'eval/epoch_eval_time': 4.093381404876709, 'eval/sps': 31269.99107571685}
I0727 16:35:55.521064 140141833336640 train.py:379] starting iteration 92 942.1640186309814
I0727 16:36:05.478815 140141833336640 train.py:394] {'eval/walltime': 391.6221504211426, 'training/sps': 42069.84525133618, 'training/walltime': 551.8529415130615, 'training/entropy_loss': Array(0.13919696, dtype=float32), 'training/policy_loss': Array(0.04271567, dtype=float32), 'training/total_loss': Array(19.995605, dtype=float32), 'training/v_loss': Array(19.813694, dtype=float32), 'eval/episode_goal_distance': (Array(0.31568664, dtype=float32), Array(0.10883723, dtype=float32)), 'eval/episode_reward': (Array(-6857.2295, dtype=float32), Array(4074.238, dtype=float32)), 'eval/avg_episode_length': (Array(813.52344, dtype=float32), Array(388.18274, dtype=float32)), 'eval/epoch_eval_time': 4.112190008163452, 'eval/sps': 31126.96634783327}
I0727 16:36:05.481140 140141833336640 train.py:379] starting iteration 93 952.124094247818
I0727 16:36:15.427068 140141833336640 train.py:394] {'eval/walltime': 395.70504570007324, 'training/sps': 41942.6065111469, 'training/walltime': 557.7123770713806, 'training/entropy_loss': Array(0.15458918, dtype=float32), 'training/policy_loss': Array(0.05141395, dtype=float32), 'training/total_loss': Array(12.842299, dtype=float32), 'training/v_loss': Array(12.636296, dtype=float32), 'eval/episode_goal_distance': (Array(0.31879473, dtype=float32), Array(0.10449012, dtype=float32)), 'eval/episode_reward': (Array(-6700.8154, dtype=float32), Array(4182.0234, dtype=float32)), 'eval/avg_episode_length': (Array(813.5078, dtype=float32), Array(388.21548, dtype=float32)), 'eval/epoch_eval_time': 4.082895278930664, 'eval/sps': 31350.3019929338}
I0727 16:36:15.429412 140141833336640 train.py:379] starting iteration 94 962.0723659992218
I0727 16:36:25.353006 140141833336640 train.py:394] {'eval/walltime': 399.8095841407776, 'training/sps': 42259.331562811836, 'training/walltime': 563.5278973579407, 'training/entropy_loss': Array(0.16779742, dtype=float32), 'training/policy_loss': Array(0.05280725, dtype=float32), 'training/total_loss': Array(11.795528, dtype=float32), 'training/v_loss': Array(11.5749235, dtype=float32), 'eval/episode_goal_distance': (Array(0.31587714, dtype=float32), Array(0.09452058, dtype=float32)), 'eval/episode_reward': (Array(-6788.3027, dtype=float32), Array(4093.1846, dtype=float32)), 'eval/avg_episode_length': (Array(798.03906, dtype=float32), Array(400.01996, dtype=float32)), 'eval/epoch_eval_time': 4.104538440704346, 'eval/sps': 31184.992380783984}
I0727 16:36:25.355257 140141833336640 train.py:379] starting iteration 95 971.9982106685638
I0727 16:36:35.361553 140141833336640 train.py:394] {'eval/walltime': 403.9256896972656, 'training/sps': 41749.48148069544, 'training/walltime': 569.4144375324249, 'training/entropy_loss': Array(0.18413839, dtype=float32), 'training/policy_loss': Array(0.06227521, dtype=float32), 'training/total_loss': Array(11.090218, dtype=float32), 'training/v_loss': Array(10.843803, dtype=float32), 'eval/episode_goal_distance': (Array(0.34217906, dtype=float32), Array(0.11872838, dtype=float32)), 'eval/episode_reward': (Array(-6483.333, dtype=float32), Array(4906.2188, dtype=float32)), 'eval/avg_episode_length': (Array(712.6719, dtype=float32), Array(450.6078, dtype=float32)), 'eval/epoch_eval_time': 4.116105556488037, 'eval/sps': 31097.356042835003}
I0727 16:36:35.363805 140141833336640 train.py:379] starting iteration 96 982.0067591667175
I0727 16:36:45.296985 140141833336640 train.py:394] {'eval/walltime': 408.0060393810272, 'training/sps': 42015.871956429684, 'training/walltime': 575.2636556625366, 'training/entropy_loss': Array(0.20027754, dtype=float32), 'training/policy_loss': Array(0.07236029, dtype=float32), 'training/total_loss': Array(10.508766, dtype=float32), 'training/v_loss': Array(10.236128, dtype=float32), 'eval/episode_goal_distance': (Array(0.34143147, dtype=float32), Array(0.10872564, dtype=float32)), 'eval/episode_reward': (Array(-7322.685, dtype=float32), Array(4250.6206, dtype=float32)), 'eval/avg_episode_length': (Array(829.03125, dtype=float32), Array(375.2835, dtype=float32)), 'eval/epoch_eval_time': 4.080349683761597, 'eval/sps': 31369.860409120436}
I0727 16:36:45.299328 140141833336640 train.py:379] starting iteration 97 991.9422821998596
I0727 16:36:55.268214 140141833336640 train.py:394] {'eval/walltime': 412.13326358795166, 'training/sps': 42096.58049421223, 'training/walltime': 581.1016595363617, 'training/entropy_loss': Array(0.20582089, dtype=float32), 'training/policy_loss': Array(0.08626078, dtype=float32), 'training/total_loss': Array(12.921725, dtype=float32), 'training/v_loss': Array(12.629644, dtype=float32), 'eval/episode_goal_distance': (Array(0.37710983, dtype=float32), Array(0.1940281, dtype=float32)), 'eval/episode_reward': (Array(-7670.1416, dtype=float32), Array(5021.8794, dtype=float32)), 'eval/avg_episode_length': (Array(798.0547, dtype=float32), Array(399.989, dtype=float32)), 'eval/epoch_eval_time': 4.1272242069244385, 'eval/sps': 31013.580455660336}
I0727 16:36:55.271143 140141833336640 train.py:379] starting iteration 98 1001.914097070694
I0727 16:37:05.250319 140141833336640 train.py:394] {'eval/walltime': 416.23517370224, 'training/sps': 41841.035450927455, 'training/walltime': 586.97531914711, 'training/entropy_loss': Array(0.20011017, dtype=float32), 'training/policy_loss': Array(0.07012492, dtype=float32), 'training/total_loss': Array(17.076576, dtype=float32), 'training/v_loss': Array(16.80634, dtype=float32), 'eval/episode_goal_distance': (Array(0.42981577, dtype=float32), Array(0.23244806, dtype=float32)), 'eval/episode_reward': (Array(-8109.2017, dtype=float32), Array(6373.7695, dtype=float32)), 'eval/avg_episode_length': (Array(712.53906, dtype=float32), Array(450.8163, dtype=float32)), 'eval/epoch_eval_time': 4.10191011428833, 'eval/sps': 31204.974373800396}
I0727 16:37:05.252703 140141833336640 train.py:379] starting iteration 99 1011.8956565856934
I0727 16:37:15.194680 140141833336640 train.py:394] {'eval/walltime': 420.33354449272156, 'training/sps': 42083.60636879891, 'training/walltime': 592.8151228427887, 'training/entropy_loss': Array(0.2142767, dtype=float32), 'training/policy_loss': Array(0.05545063, dtype=float32), 'training/total_loss': Array(21.898947, dtype=float32), 'training/v_loss': Array(21.62922, dtype=float32), 'eval/episode_goal_distance': (Array(0.41612852, dtype=float32), Array(0.22725685, dtype=float32)), 'eval/episode_reward': (Array(-8443.448, dtype=float32), Array(5387.526, dtype=float32)), 'eval/avg_episode_length': (Array(805.8906, dtype=float32), Array(394.00024, dtype=float32)), 'eval/epoch_eval_time': 4.098370790481567, 'eval/sps': 31231.92276728083}
I0727 16:37:15.197397 140141833336640 train.py:379] starting iteration 100 1021.8403468132019
I0727 16:37:25.167320 140141833336640 train.py:394] {'eval/walltime': 424.46031308174133, 'training/sps': 42085.683684646814, 'training/walltime': 598.6546382904053, 'training/entropy_loss': Array(0.1931878, dtype=float32), 'training/policy_loss': Array(0.06271666, dtype=float32), 'training/total_loss': Array(100.890945, dtype=float32), 'training/v_loss': Array(100.635056, dtype=float32), 'eval/episode_goal_distance': (Array(0.63414764, dtype=float32), Array(0.45354453, dtype=float32)), 'eval/episode_reward': (Array(-11750.02, dtype=float32), Array(8703.993, dtype=float32)), 'eval/avg_episode_length': (Array(774.85156, dtype=float32), Array(415.99615, dtype=float32)), 'eval/epoch_eval_time': 4.126768589019775, 'eval/sps': 31017.004525180713}
I0727 16:37:25.170016 140141833336640 train.py:379] starting iteration 101 1031.8129670619965
I0727 16:37:35.155592 140141833336640 train.py:394] {'eval/walltime': 428.56601572036743, 'training/sps': 41825.428107899876, 'training/walltime': 604.5304896831512, 'training/entropy_loss': Array(0.18519443, dtype=float32), 'training/policy_loss': Array(0.09643936, dtype=float32), 'training/total_loss': Array(38.64006, dtype=float32), 'training/v_loss': Array(38.35843, dtype=float32), 'eval/episode_goal_distance': (Array(0.8605893, dtype=float32), Array(0.60964274, dtype=float32)), 'eval/episode_reward': (Array(-13648.555, dtype=float32), Array(11433.989, dtype=float32)), 'eval/avg_episode_length': (Array(712.5703, dtype=float32), Array(450.7674, dtype=float32)), 'eval/epoch_eval_time': 4.105702638626099, 'eval/sps': 31176.149679177193}
I0727 16:37:35.158275 140141833336640 train.py:379] starting iteration 102 1041.8012263774872
I0727 16:37:45.052799 140141833336640 train.py:394] {'eval/walltime': 432.6377477645874, 'training/sps': 42233.28557428549, 'training/walltime': 610.3495965003967, 'training/entropy_loss': Array(0.1035258, dtype=float32), 'training/policy_loss': Array(0.13065332, dtype=float32), 'training/total_loss': Array(76.38374, dtype=float32), 'training/v_loss': Array(76.14957, dtype=float32), 'eval/episode_goal_distance': (Array(0.6100861, dtype=float32), Array(0.38597026, dtype=float32)), 'eval/episode_reward': (Array(-11915.324, dtype=float32), Array(7542.8667, dtype=float32)), 'eval/avg_episode_length': (Array(852.47656, dtype=float32), Array(353.34464, dtype=float32)), 'eval/epoch_eval_time': 4.071732044219971, 'eval/sps': 31436.253321655207}
I0727 16:37:45.114203 140141833336640 train.py:379] starting iteration 103 1051.7571530342102
I0727 16:37:55.062626 140141833336640 train.py:394] {'eval/walltime': 436.7272620201111, 'training/sps': 41973.77226147758, 'training/walltime': 616.2046813964844, 'training/entropy_loss': Array(0.04426254, dtype=float32), 'training/policy_loss': Array(0.02247296, dtype=float32), 'training/total_loss': Array(121.11034, dtype=float32), 'training/v_loss': Array(121.0436, dtype=float32), 'eval/episode_goal_distance': (Array(0.6047431, dtype=float32), Array(0.3722501, dtype=float32)), 'eval/episode_reward': (Array(-11856.535, dtype=float32), Array(7935.7324, dtype=float32)), 'eval/avg_episode_length': (Array(836.8828, dtype=float32), Array(368.19937, dtype=float32)), 'eval/epoch_eval_time': 4.089514255523682, 'eval/sps': 31299.560779647898}
I0727 16:37:55.065295 140141833336640 train.py:379] starting iteration 104 1061.7082469463348
I0727 16:38:05.029592 140141833336640 train.py:394] {'eval/walltime': 440.81702852249146, 'training/sps': 41861.672396293194, 'training/walltime': 622.0754454135895, 'training/entropy_loss': Array(0.04944973, dtype=float32), 'training/policy_loss': Array(0.01186918, dtype=float32), 'training/total_loss': Array(113.12817, dtype=float32), 'training/v_loss': Array(113.06684, dtype=float32), 'eval/episode_goal_distance': (Array(0.5611956, dtype=float32), Array(0.27032247, dtype=float32)), 'eval/episode_reward': (Array(-11989.25, dtype=float32), Array(6959.5645, dtype=float32)), 'eval/avg_episode_length': (Array(867.9922, dtype=float32), Array(337.3162, dtype=float32)), 'eval/epoch_eval_time': 4.089766502380371, 'eval/sps': 31297.630298820244}
I0727 16:38:05.032161 140141833336640 train.py:379] starting iteration 105 1071.67511510849
I0727 16:38:15.010879 140141833336640 train.py:394] {'eval/walltime': 444.90517473220825, 'training/sps': 41747.878520524595, 'training/walltime': 627.9622116088867, 'training/entropy_loss': Array(0.04799537, dtype=float32), 'training/policy_loss': Array(0.00948023, dtype=float32), 'training/total_loss': Array(118.397125, dtype=float32), 'training/v_loss': Array(118.339645, dtype=float32), 'eval/episode_goal_distance': (Array(0.6315166, dtype=float32), Array(0.35365668, dtype=float32)), 'eval/episode_reward': (Array(-12793.816, dtype=float32), Array(7286.0684, dtype=float32)), 'eval/avg_episode_length': (Array(875.7031, dtype=float32), Array(328.85934, dtype=float32)), 'eval/epoch_eval_time': 4.088146209716797, 'eval/sps': 31310.034777074936}
I0727 16:38:15.013360 140141833336640 train.py:379] starting iteration 106 1081.656314611435
I0727 16:38:24.950959 140141833336640 train.py:394] {'eval/walltime': 448.97929430007935, 'training/sps': 41941.785637514506, 'training/walltime': 633.8217618465424, 'training/entropy_loss': Array(0.0416618, dtype=float32), 'training/policy_loss': Array(0.00778213, dtype=float32), 'training/total_loss': Array(117.96289, dtype=float32), 'training/v_loss': Array(117.91344, dtype=float32), 'eval/episode_goal_distance': (Array(0.51979315, dtype=float32), Array(0.26756424, dtype=float32)), 'eval/episode_reward': (Array(-10635.377, dtype=float32), Array(6926.7, dtype=float32)), 'eval/avg_episode_length': (Array(821.46875, dtype=float32), Array(381.45715, dtype=float32)), 'eval/epoch_eval_time': 4.074119567871094, 'eval/sps': 31417.830985968245}
I0727 16:38:24.955307 140141833336640 train.py:379] starting iteration 107 1091.5982446670532
I0727 16:38:34.934182 140141833336640 train.py:394] {'eval/walltime': 453.08023500442505, 'training/sps': 41839.184303416114, 'training/walltime': 639.6956813335419, 'training/entropy_loss': Array(0.03594615, dtype=float32), 'training/policy_loss': Array(0.00620035, dtype=float32), 'training/total_loss': Array(113.994255, dtype=float32), 'training/v_loss': Array(113.9521, dtype=float32), 'eval/episode_goal_distance': (Array(0.58054686, dtype=float32), Array(0.31167814, dtype=float32)), 'eval/episode_reward': (Array(-12765.281, dtype=float32), Array(7524.598, dtype=float32)), 'eval/avg_episode_length': (Array(867.84375, dtype=float32), Array(337.69565, dtype=float32)), 'eval/epoch_eval_time': 4.100940704345703, 'eval/sps': 31212.350830715593}
I0727 16:38:34.936725 140141833336640 train.py:379] starting iteration 108 1101.5796794891357
I0727 16:38:44.874861 140141833336640 train.py:394] {'eval/walltime': 457.1619122028351, 'training/sps': 41991.207893136205, 'training/walltime': 645.5483350753784, 'training/entropy_loss': Array(0.0310425, dtype=float32), 'training/policy_loss': Array(0.00490038, dtype=float32), 'training/total_loss': Array(123.48378, dtype=float32), 'training/v_loss': Array(123.447845, dtype=float32), 'eval/episode_goal_distance': (Array(0.5518713, dtype=float32), Array(0.26371932, dtype=float32)), 'eval/episode_reward': (Array(-11759.15, dtype=float32), Array(6635.5596, dtype=float32)), 'eval/avg_episode_length': (Array(867.9219, dtype=float32), Array(337.4959, dtype=float32)), 'eval/epoch_eval_time': 4.081677198410034, 'eval/sps': 31359.65775291118}
I0727 16:38:44.877362 140141833336640 train.py:379] starting iteration 109 1111.520316362381
I0727 16:38:54.860831 140141833336640 train.py:394] {'eval/walltime': 461.28272128105164, 'training/sps': 41943.09461340444, 'training/walltime': 651.4077024459839, 'training/entropy_loss': Array(0.02681072, dtype=float32), 'training/policy_loss': Array(0.00428114, dtype=float32), 'training/total_loss': Array(65.76126, dtype=float32), 'training/v_loss': Array(65.73016, dtype=float32), 'eval/episode_goal_distance': (Array(0.6303152, dtype=float32), Array(0.37175098, dtype=float32)), 'eval/episode_reward': (Array(-13493.014, dtype=float32), Array(7467.7266, dtype=float32)), 'eval/avg_episode_length': (Array(906.8906, dtype=float32), Array(289.48935, dtype=float32)), 'eval/epoch_eval_time': 4.120809078216553, 'eval/sps': 31061.86129239387}
I0727 16:38:54.863337 140141833336640 train.py:379] starting iteration 110 1121.506290435791
I0727 16:39:04.825752 140141833336640 train.py:394] {'eval/walltime': 465.38040804862976, 'training/sps': 41931.108675009586, 'training/walltime': 657.2687447071075, 'training/entropy_loss': Array(0.01889599, dtype=float32), 'training/policy_loss': Array(0.00379634, dtype=float32), 'training/total_loss': Array(68.916534, dtype=float32), 'training/v_loss': Array(68.89384, dtype=float32), 'eval/episode_goal_distance': (Array(0.6063821, dtype=float32), Array(0.34467524, dtype=float32)), 'eval/episode_reward': (Array(-12066.398, dtype=float32), Array(6807.0903, dtype=float32)), 'eval/avg_episode_length': (Array(860.28906, dtype=float32), Array(345.3753, dtype=float32)), 'eval/epoch_eval_time': 4.097686767578125, 'eval/sps': 31237.136282052237}
I0727 16:39:04.828303 140141833336640 train.py:379] starting iteration 111 1131.471256494522
I0727 16:39:14.761856 140141833336640 train.py:394] {'eval/walltime': 469.4661056995392, 'training/sps': 42051.472775115224, 'training/walltime': 663.1130108833313, 'training/entropy_loss': Array(0.01288488, dtype=float32), 'training/policy_loss': Array(0.00332012, dtype=float32), 'training/total_loss': Array(84.93558, dtype=float32), 'training/v_loss': Array(84.91938, dtype=float32), 'eval/episode_goal_distance': (Array(0.578791, dtype=float32), Array(0.33598667, dtype=float32)), 'eval/episode_reward': (Array(-12276.487, dtype=float32), Array(7062.9873, dtype=float32)), 'eval/avg_episode_length': (Array(891.15625, dtype=float32), Array(310.59354, dtype=float32)), 'eval/epoch_eval_time': 4.085697650909424, 'eval/sps': 31328.798882489224}
I0727 16:39:14.764279 140141833336640 train.py:379] starting iteration 112 1141.4072332382202
I0727 16:39:24.761615 140141833336640 train.py:394] {'eval/walltime': 473.5799603462219, 'training/sps': 41796.61748282131, 'training/walltime': 668.992912530899, 'training/entropy_loss': Array(0.00721304, dtype=float32), 'training/policy_loss': Array(0.00298117, dtype=float32), 'training/total_loss': Array(63.23264, dtype=float32), 'training/v_loss': Array(63.22245, dtype=float32), 'eval/episode_goal_distance': (Array(0.5465377, dtype=float32), Array(0.26284403, dtype=float32)), 'eval/episode_reward': (Array(-11735.271, dtype=float32), Array(6769.2554, dtype=float32)), 'eval/avg_episode_length': (Array(867.96094, dtype=float32), Array(337.3963, dtype=float32)), 'eval/epoch_eval_time': 4.113854646682739, 'eval/sps': 31114.37106880149}
I0727 16:39:24.764218 140141833336640 train.py:379] starting iteration 113 1151.407172203064
I0727 16:39:34.722547 140141833336640 train.py:394] {'eval/walltime': 477.6842415332794, 'training/sps': 42006.687584048144, 'training/walltime': 674.843409538269, 'training/entropy_loss': Array(0.00203384, dtype=float32), 'training/policy_loss': Array(0.00209741, dtype=float32), 'training/total_loss': Array(61.749466, dtype=float32), 'training/v_loss': Array(61.745335, dtype=float32), 'eval/episode_goal_distance': (Array(0.5829836, dtype=float32), Array(0.31733856, dtype=float32)), 'eval/episode_reward': (Array(-11869.508, dtype=float32), Array(7082.4546, dtype=float32)), 'eval/avg_episode_length': (Array(852.46094, dtype=float32), Array(353.3823, dtype=float32)), 'eval/epoch_eval_time': 4.104281187057495, 'eval/sps': 31186.94703560692}
I0727 16:39:34.725054 140141833336640 train.py:379] starting iteration 114 1161.3680083751678
I0727 16:39:44.647041 140141833336640 train.py:394] {'eval/walltime': 481.76532673835754, 'training/sps': 42103.53058600939, 'training/walltime': 680.6804497241974, 'training/entropy_loss': Array(-0.0048292, dtype=float32), 'training/policy_loss': Array(0.00224495, dtype=float32), 'training/total_loss': Array(60.906654, dtype=float32), 'training/v_loss': Array(60.90924, dtype=float32), 'eval/episode_goal_distance': (Array(0.59955084, dtype=float32), Array(0.31068763, dtype=float32)), 'eval/episode_reward': (Array(-12531.096, dtype=float32), Array(6961.7197, dtype=float32)), 'eval/avg_episode_length': (Array(891.40625, dtype=float32), Array(309.88025, dtype=float32)), 'eval/epoch_eval_time': 4.081085205078125, 'eval/sps': 31364.206716568584}
I0727 16:39:44.649509 140141833336640 train.py:379] starting iteration 115 1171.2924635410309
I0727 16:39:54.612561 140141833336640 train.py:394] {'eval/walltime': 485.87183928489685, 'training/sps': 41989.839466667945, 'training/walltime': 686.5332942008972, 'training/entropy_loss': Array(-0.01123161, dtype=float32), 'training/policy_loss': Array(0.00171464, dtype=float32), 'training/total_loss': Array(59.696663, dtype=float32), 'training/v_loss': Array(59.706173, dtype=float32), 'eval/episode_goal_distance': (Array(0.5483765, dtype=float32), Array(0.26371825, dtype=float32)), 'eval/episode_reward': (Array(-12170.593, dtype=float32), Array(5635.8647, dtype=float32)), 'eval/avg_episode_length': (Array(922.35156, dtype=float32), Array(266.73154, dtype=float32)), 'eval/epoch_eval_time': 4.106512546539307, 'eval/sps': 31170.000955645395}
I0727 16:39:54.615078 140141833336640 train.py:379] starting iteration 116 1181.25803232193
I0727 16:40:04.550342 140141833336640 train.py:394] {'eval/walltime': 489.94640946388245, 'training/sps': 41959.70368440592, 'training/walltime': 692.3903422355652, 'training/entropy_loss': Array(-0.01836434, dtype=float32), 'training/policy_loss': Array(0.0012745, dtype=float32), 'training/total_loss': Array(81.47215, dtype=float32), 'training/v_loss': Array(81.48924, dtype=float32), 'eval/episode_goal_distance': (Array(0.5879929, dtype=float32), Array(0.35057455, dtype=float32)), 'eval/episode_reward': (Array(-12280.645, dtype=float32), Array(7139.806, dtype=float32)), 'eval/avg_episode_length': (Array(891.28906, dtype=float32), Array(310.21475, dtype=float32)), 'eval/epoch_eval_time': 4.074570178985596, 'eval/sps': 31414.356454124656}
I0727 16:40:04.552786 140141833336640 train.py:379] starting iteration 117 1191.195740222931
I0727 16:40:14.532697 140141833336640 train.py:394] {'eval/walltime': 494.02210783958435, 'training/sps': 41649.82193191754, 'training/walltime': 698.2909677028656, 'training/entropy_loss': Array(-0.02215606, dtype=float32), 'training/policy_loss': Array(0.00097986, dtype=float32), 'training/total_loss': Array(59.185406, dtype=float32), 'training/v_loss': Array(59.20658, dtype=float32), 'eval/episode_goal_distance': (Array(0.60099673, dtype=float32), Array(0.30934006, dtype=float32)), 'eval/episode_reward': (Array(-12852.904, dtype=float32), Array(6171.4463, dtype=float32)), 'eval/avg_episode_length': (Array(937.9219, dtype=float32), Array(240.42802, dtype=float32)), 'eval/epoch_eval_time': 4.075698375701904, 'eval/sps': 31405.660625697365}
I0727 16:40:14.535180 140141833336640 train.py:379] starting iteration 118 1201.17813372612
I0727 16:40:24.474994 140141833336640 train.py:394] {'eval/walltime': 498.1011872291565, 'training/sps': 41959.06147757591, 'training/walltime': 704.1481053829193, 'training/entropy_loss': Array(-0.02497154, dtype=float32), 'training/policy_loss': Array(0.00087234, dtype=float32), 'training/total_loss': Array(53.5181, dtype=float32), 'training/v_loss': Array(53.5422, dtype=float32), 'eval/episode_goal_distance': (Array(0.5981042, dtype=float32), Array(0.34974128, dtype=float32)), 'eval/episode_reward': (Array(-12398.33, dtype=float32), Array(7678.412, dtype=float32)), 'eval/avg_episode_length': (Array(883.5703, dtype=float32), Array(319.5641, dtype=float32)), 'eval/epoch_eval_time': 4.0790793895721436, 'eval/sps': 31379.629513272597}
I0727 16:40:24.477471 140141833336640 train.py:379] starting iteration 119 1211.1204254627228
I0727 16:40:34.402698 140141833336640 train.py:394] {'eval/walltime': 502.1778681278229, 'training/sps': 42046.364629682634, 'training/walltime': 709.9930815696716, 'training/entropy_loss': Array(-0.02928846, dtype=float32), 'training/policy_loss': Array(0.00100713, dtype=float32), 'training/total_loss': Array(69.596535, dtype=float32), 'training/v_loss': Array(69.62482, dtype=float32), 'eval/episode_goal_distance': (Array(0.59840614, dtype=float32), Array(0.351412, dtype=float32)), 'eval/episode_reward': (Array(-12263.699, dtype=float32), Array(6998.1094, dtype=float32)), 'eval/avg_episode_length': (Array(875.66406, dtype=float32), Array(328.9625, dtype=float32)), 'eval/epoch_eval_time': 4.076680898666382, 'eval/sps': 31398.09153124373}
I0727 16:40:34.405152 140141833336640 train.py:379] starting iteration 120 1221.0481057167053
I0727 16:40:44.355503 140141833336640 train.py:394] {'eval/walltime': 506.2736632823944, 'training/sps': 42002.759255122575, 'training/walltime': 715.8441257476807, 'training/entropy_loss': Array(-0.03471804, dtype=float32), 'training/policy_loss': Array(0.00063088, dtype=float32), 'training/total_loss': Array(59.292473, dtype=float32), 'training/v_loss': Array(59.326557, dtype=float32), 'eval/episode_goal_distance': (Array(0.56822664, dtype=float32), Array(0.31040084, dtype=float32)), 'eval/episode_reward': (Array(-11942.799, dtype=float32), Array(6459.637, dtype=float32)), 'eval/avg_episode_length': (Array(899.0781, dtype=float32), Array(300.1672, dtype=float32)), 'eval/epoch_eval_time': 4.095795154571533, 'eval/sps': 31251.562924755268}
I0727 16:40:44.357859 140141833336640 train.py:379] starting iteration 121 1231.0008137226105
I0727 16:40:54.276546 140141833336640 train.py:394] {'eval/walltime': 510.3616421222687, 'training/sps': 42175.44427103519, 'training/walltime': 721.6712131500244, 'training/entropy_loss': Array(-0.03642319, dtype=float32), 'training/policy_loss': Array(0.00080764, dtype=float32), 'training/total_loss': Array(49.971516, dtype=float32), 'training/v_loss': Array(50.00713, dtype=float32), 'eval/episode_goal_distance': (Array(0.5258247, dtype=float32), Array(0.26874584, dtype=float32)), 'eval/episode_reward': (Array(-10996.016, dtype=float32), Array(6174.0566, dtype=float32)), 'eval/avg_episode_length': (Array(860.14844, dtype=float32), Array(345.72284, dtype=float32)), 'eval/epoch_eval_time': 4.087978839874268, 'eval/sps': 31311.316671085522}
I0727 16:40:54.279069 140141833336640 train.py:379] starting iteration 122 1240.9220230579376
I0727 16:41:04.258881 140141833336640 train.py:394] {'eval/walltime': 514.4667685031891, 'training/sps': 41858.734913654735, 'training/walltime': 727.5423891544342, 'training/entropy_loss': Array(-0.03927293, dtype=float32), 'training/policy_loss': Array(0.00066978, dtype=float32), 'training/total_loss': Array(48.84057, dtype=float32), 'training/v_loss': Array(48.879173, dtype=float32), 'eval/episode_goal_distance': (Array(0.54829717, dtype=float32), Array(0.29306695, dtype=float32)), 'eval/episode_reward': (Array(-11325.162, dtype=float32), Array(6287.9414, dtype=float32)), 'eval/avg_episode_length': (Array(867.9297, dtype=float32), Array(337.47577, dtype=float32)), 'eval/epoch_eval_time': 4.10512638092041, 'eval/sps': 31180.526035668878}
I0727 16:41:04.261399 140141833336640 train.py:379] starting iteration 123 1250.9043536186218
I0727 16:41:14.194591 140141833336640 train.py:394] {'eval/walltime': 518.5519578456879, 'training/sps': 42051.071351396204, 'training/walltime': 733.3867111206055, 'training/entropy_loss': Array(-0.04038095, dtype=float32), 'training/policy_loss': Array(0.00068577, dtype=float32), 'training/total_loss': Array(50.00464, dtype=float32), 'training/v_loss': Array(50.04433, dtype=float32), 'eval/episode_goal_distance': (Array(0.53701, dtype=float32), Array(0.26405978, dtype=float32)), 'eval/episode_reward': (Array(-10715.754, dtype=float32), Array(6034.251, dtype=float32)), 'eval/avg_episode_length': (Array(844.65625, dtype=float32), Array(360.98715, dtype=float32)), 'eval/epoch_eval_time': 4.085189342498779, 'eval/sps': 31332.697035214165}
I0727 16:41:14.196995 140141833336640 train.py:379] starting iteration 124 1260.8399488925934
I0727 16:41:24.149189 140141833336640 train.py:394] {'eval/walltime': 522.6488704681396, 'training/sps': 41998.482576914175, 'training/walltime': 739.2383511066437, 'training/entropy_loss': Array(-0.04267455, dtype=float32), 'training/policy_loss': Array(0.00057621, dtype=float32), 'training/total_loss': Array(50.05884, dtype=float32), 'training/v_loss': Array(50.10094, dtype=float32), 'eval/episode_goal_distance': (Array(0.5416858, dtype=float32), Array(0.30098626, dtype=float32)), 'eval/episode_reward': (Array(-10943.486, dtype=float32), Array(6612.132, dtype=float32)), 'eval/avg_episode_length': (Array(860.16406, dtype=float32), Array(345.68414, dtype=float32)), 'eval/epoch_eval_time': 4.096912622451782, 'eval/sps': 31243.038794271106}
I0727 16:41:24.151737 140141833336640 train.py:379] starting iteration 125 1270.7946903705597
I0727 16:41:34.171841 140141833336640 train.py:394] {'eval/walltime': 526.7713994979858, 'training/sps': 41698.5378677427, 'training/walltime': 745.132082939148, 'training/entropy_loss': Array(-0.04510041, dtype=float32), 'training/policy_loss': Array(0.00016116, dtype=float32), 'training/total_loss': Array(80.50746, dtype=float32), 'training/v_loss': Array(80.5524, dtype=float32), 'eval/episode_goal_distance': (Array(0.5840019, dtype=float32), Array(0.27512914, dtype=float32)), 'eval/episode_reward': (Array(-12137.701, dtype=float32), Array(6227.4214, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.3064, dtype=float32)), 'eval/epoch_eval_time': 4.122529029846191, 'eval/sps': 31048.902038847642}
I0727 16:41:34.174709 140141833336640 train.py:379] starting iteration 126 1280.8176581859589
I0727 16:41:44.186324 140141833336640 train.py:394] {'eval/walltime': 530.873407125473, 'training/sps': 41614.13492262752, 'training/walltime': 751.0377686023712, 'training/entropy_loss': Array(-0.04688035, dtype=float32), 'training/policy_loss': Array(0.00055706, dtype=float32), 'training/total_loss': Array(46.182297, dtype=float32), 'training/v_loss': Array(46.228622, dtype=float32), 'eval/episode_goal_distance': (Array(0.5536555, dtype=float32), Array(0.2721895, dtype=float32)), 'eval/episode_reward': (Array(-10895.302, dtype=float32), Array(6215.749, dtype=float32)), 'eval/avg_episode_length': (Array(844.6875, dtype=float32), Array(360.91415, dtype=float32)), 'eval/epoch_eval_time': 4.102007627487183, 'eval/sps': 31204.23256706876}
I0727 16:41:44.188802 140141833336640 train.py:379] starting iteration 127 1290.8317561149597
I0727 16:41:54.118041 140141833336640 train.py:394] {'eval/walltime': 534.953296661377, 'training/sps': 42039.74029781231, 'training/walltime': 756.8836658000946, 'training/entropy_loss': Array(-0.04819304, dtype=float32), 'training/policy_loss': Array(0.00027205, dtype=float32), 'training/total_loss': Array(51.99704, dtype=float32), 'training/v_loss': Array(52.044956, dtype=float32), 'eval/episode_goal_distance': (Array(0.5580615, dtype=float32), Array(0.30021462, dtype=float32)), 'eval/episode_reward': (Array(-11575.028, dtype=float32), Array(6054.37, dtype=float32)), 'eval/avg_episode_length': (Array(891.2422, dtype=float32), Array(310.34845, dtype=float32)), 'eval/epoch_eval_time': 4.079889535903931, 'eval/sps': 31373.398439730212}
I0727 16:41:54.120460 140141833336640 train.py:379] starting iteration 128 1300.7634143829346
I0727 16:42:04.091540 140141833336640 train.py:394] {'eval/walltime': 539.0649473667145, 'training/sps': 41966.85639592098, 'training/walltime': 762.7397155761719, 'training/entropy_loss': Array(-0.04903135, dtype=float32), 'training/policy_loss': Array(0.00025993, dtype=float32), 'training/total_loss': Array(67.28549, dtype=float32), 'training/v_loss': Array(67.33426, dtype=float32), 'eval/episode_goal_distance': (Array(0.5768616, dtype=float32), Array(0.29813126, dtype=float32)), 'eval/episode_reward': (Array(-12340.85, dtype=float32), Array(5838.059, dtype=float32)), 'eval/avg_episode_length': (Array(937.8906, dtype=float32), Array(240.54921, dtype=float32)), 'eval/epoch_eval_time': 4.111650705337524, 'eval/sps': 31131.04910245349}
I0727 16:42:04.093909 140141833336640 train.py:379] starting iteration 129 1310.736862897873
I0727 16:42:14.078501 140141833336640 train.py:394] {'eval/walltime': 543.1657712459564, 'training/sps': 41797.308961061746, 'training/walltime': 768.6195199489594, 'training/entropy_loss': Array(-0.04931097, dtype=float32), 'training/policy_loss': Array(0.00074208, dtype=float32), 'training/total_loss': Array(49.192276, dtype=float32), 'training/v_loss': Array(49.240845, dtype=float32), 'eval/episode_goal_distance': (Array(0.55687034, dtype=float32), Array(0.26665214, dtype=float32)), 'eval/episode_reward': (Array(-11357.088, dtype=float32), Array(5624.765, dtype=float32)), 'eval/avg_episode_length': (Array(899.0156, dtype=float32), Array(300.35333, dtype=float32)), 'eval/epoch_eval_time': 4.100823879241943, 'eval/sps': 31213.24001450689}
I0727 16:42:14.081227 140141833336640 train.py:379] starting iteration 130 1320.724181175232
I0727 16:42:24.030368 140141833336640 train.py:394] {'eval/walltime': 547.2422573566437, 'training/sps': 41873.86531502585, 'training/walltime': 774.4885745048523, 'training/entropy_loss': Array(-0.04920067, dtype=float32), 'training/policy_loss': Array(0.00050013, dtype=float32), 'training/total_loss': Array(45.291794, dtype=float32), 'training/v_loss': Array(45.340496, dtype=float32), 'eval/episode_goal_distance': (Array(0.503898, dtype=float32), Array(0.26450872, dtype=float32)), 'eval/episode_reward': (Array(-10112.502, dtype=float32), Array(6163.016, dtype=float32)), 'eval/avg_episode_length': (Array(829.08594, dtype=float32), Array(375.16293, dtype=float32)), 'eval/epoch_eval_time': 4.076486110687256, 'eval/sps': 31399.59183582756}
I0727 16:42:24.032807 140141833336640 train.py:379] starting iteration 131 1330.6757612228394
I0727 16:42:33.999980 140141833336640 train.py:394] {'eval/walltime': 551.352775812149, 'training/sps': 41988.82346771095, 'training/walltime': 780.3415606021881, 'training/entropy_loss': Array(-0.05009712, dtype=float32), 'training/policy_loss': Array(0.00017571, dtype=float32), 'training/total_loss': Array(43.538666, dtype=float32), 'training/v_loss': Array(43.588585, dtype=float32), 'eval/episode_goal_distance': (Array(0.5303893, dtype=float32), Array(0.25222966, dtype=float32)), 'eval/episode_reward': (Array(-11444.508, dtype=float32), Array(5086.4253, dtype=float32)), 'eval/avg_episode_length': (Array(930.16406, dtype=float32), Array(253.9406, dtype=float32)), 'eval/epoch_eval_time': 4.110518455505371, 'eval/sps': 31139.624206908695}
I0727 16:42:34.002384 140141833336640 train.py:379] starting iteration 132 1340.6453382968903
I0727 16:42:44.018269 140141833336640 train.py:394] {'eval/walltime': 555.4741430282593, 'training/sps': 41719.50385629084, 'training/walltime': 786.2323305606842, 'training/entropy_loss': Array(-0.04991439, dtype=float32), 'training/policy_loss': Array(0.00044669, dtype=float32), 'training/total_loss': Array(44.116116, dtype=float32), 'training/v_loss': Array(44.16558, dtype=float32), 'eval/episode_goal_distance': (Array(0.5129062, dtype=float32), Array(0.27163842, dtype=float32)), 'eval/episode_reward': (Array(-11089.365, dtype=float32), Array(6131.1016, dtype=float32)), 'eval/avg_episode_length': (Array(899.03906, dtype=float32), Array(300.28366, dtype=float32)), 'eval/epoch_eval_time': 4.1213672161102295, 'eval/sps': 31057.654726726134}
I0727 16:42:44.020933 140141833336640 train.py:379] starting iteration 133 1350.66388463974
I0727 16:42:54.051559 140141833336640 train.py:394] {'eval/walltime': 559.6032130718231, 'training/sps': 41672.31745815363, 'training/walltime': 792.1297707557678, 'training/entropy_loss': Array(-0.0507899, dtype=float32), 'training/policy_loss': Array(0.00010435, dtype=float32), 'training/total_loss': Array(69.21817, dtype=float32), 'training/v_loss': Array(69.26886, dtype=float32), 'eval/episode_goal_distance': (Array(0.55626327, dtype=float32), Array(0.29229352, dtype=float32)), 'eval/episode_reward': (Array(-11200.073, dtype=float32), Array(6099.036, dtype=float32)), 'eval/avg_episode_length': (Array(883.5547, dtype=float32), Array(319.60773, dtype=float32)), 'eval/epoch_eval_time': 4.129070043563843, 'eval/sps': 30999.716316151877}
I0727 16:42:54.054200 140141833336640 train.py:379] starting iteration 134 1360.697154045105
I0727 16:43:04.038499 140141833336640 train.py:394] {'eval/walltime': 563.7042307853699, 'training/sps': 41798.88012534894, 'training/walltime': 798.0093541145325, 'training/entropy_loss': Array(-0.05090697, dtype=float32), 'training/policy_loss': Array(0.00038385, dtype=float32), 'training/total_loss': Array(38.598957, dtype=float32), 'training/v_loss': Array(38.649483, dtype=float32), 'eval/episode_goal_distance': (Array(0.5398984, dtype=float32), Array(0.29810983, dtype=float32)), 'eval/episode_reward': (Array(-10769.943, dtype=float32), Array(5799.7524, dtype=float32)), 'eval/avg_episode_length': (Array(891.35156, dtype=float32), Array(310.03647, dtype=float32)), 'eval/epoch_eval_time': 4.101017713546753, 'eval/sps': 31211.76472298131}
I0727 16:43:04.040918 140141833336640 train.py:379] starting iteration 135 1370.6838719844818
I0727 16:43:13.960190 140141833336640 train.py:394] {'eval/walltime': 567.7918012142181, 'training/sps': 42169.695248403, 'training/walltime': 803.8372359275818, 'training/entropy_loss': Array(-0.05085256, dtype=float32), 'training/policy_loss': Array(0.00040093, dtype=float32), 'training/total_loss': Array(42.722363, dtype=float32), 'training/v_loss': Array(42.77281, dtype=float32), 'eval/episode_goal_distance': (Array(0.5401898, dtype=float32), Array(0.26952922, dtype=float32)), 'eval/episode_reward': (Array(-11104.338, dtype=float32), Array(6254.295, dtype=float32)), 'eval/avg_episode_length': (Array(868.0469, dtype=float32), Array(337.17682, dtype=float32)), 'eval/epoch_eval_time': 4.087570428848267, 'eval/sps': 31314.445152218672}
I0727 16:43:13.965215 140141833336640 train.py:379] starting iteration 136 1380.6081550121307
I0727 16:43:23.933357 140141833336640 train.py:394] {'eval/walltime': 571.8981838226318, 'training/sps': 41957.2271971846, 'training/walltime': 809.6946296691895, 'training/entropy_loss': Array(-0.05099154, dtype=float32), 'training/policy_loss': Array(0.00054036, dtype=float32), 'training/total_loss': Array(56.62841, dtype=float32), 'training/v_loss': Array(56.678864, dtype=float32), 'eval/episode_goal_distance': (Array(0.5433187, dtype=float32), Array(0.27159587, dtype=float32)), 'eval/episode_reward': (Array(-11044.863, dtype=float32), Array(5569.2217, dtype=float32)), 'eval/avg_episode_length': (Array(891.2422, dtype=float32), Array(310.34866, dtype=float32)), 'eval/epoch_eval_time': 4.106382608413696, 'eval/sps': 31170.987266928507}
I0727 16:43:23.935812 140141833336640 train.py:379] starting iteration 137 1390.578765153885
I0727 16:43:33.887681 140141833336640 train.py:394] {'eval/walltime': 575.9792563915253, 'training/sps': 41890.73964337249, 'training/walltime': 815.561320066452, 'training/entropy_loss': Array(-0.05199477, dtype=float32), 'training/policy_loss': Array(0.0007783, dtype=float32), 'training/total_loss': Array(45.556988, dtype=float32), 'training/v_loss': Array(45.608208, dtype=float32), 'eval/episode_goal_distance': (Array(0.5007104, dtype=float32), Array(0.21962744, dtype=float32)), 'eval/episode_reward': (Array(-10025.037, dtype=float32), Array(5437.6504, dtype=float32)), 'eval/avg_episode_length': (Array(852.47656, dtype=float32), Array(353.34445, dtype=float32)), 'eval/epoch_eval_time': 4.081072568893433, 'eval/sps': 31364.3038292521}
I0727 16:43:33.890013 140141833336640 train.py:379] starting iteration 138 1400.532966852188
I0727 16:43:43.837409 140141833336640 train.py:394] {'eval/walltime': 580.0710592269897, 'training/sps': 41996.42754839954, 'training/walltime': 821.4132463932037, 'training/entropy_loss': Array(-0.0524773, dtype=float32), 'training/policy_loss': Array(0.00103259, dtype=float32), 'training/total_loss': Array(40.977684, dtype=float32), 'training/v_loss': Array(41.02913, dtype=float32), 'eval/episode_goal_distance': (Array(0.50108373, dtype=float32), Array(0.2780551, dtype=float32)), 'eval/episode_reward': (Array(-9748.819, dtype=float32), Array(6582.5396, dtype=float32)), 'eval/avg_episode_length': (Array(798.0625, dtype=float32), Array(399.97342, dtype=float32)), 'eval/epoch_eval_time': 4.0918028354644775, 'eval/sps': 31282.0546705228}
I0727 16:43:43.839871 140141833336640 train.py:379] starting iteration 139 1410.482824087143
I0727 16:43:53.858847 140141833336640 train.py:394] {'eval/walltime': 584.1927328109741, 'training/sps': 41700.88606319318, 'training/walltime': 827.3066463470459, 'training/entropy_loss': Array(-0.05210188, dtype=float32), 'training/policy_loss': Array(0.00100668, dtype=float32), 'training/total_loss': Array(39.53038, dtype=float32), 'training/v_loss': Array(39.581474, dtype=float32), 'eval/episode_goal_distance': (Array(0.4997331, dtype=float32), Array(0.23686422, dtype=float32)), 'eval/episode_reward': (Array(-9782.944, dtype=float32), Array(6043.5215, dtype=float32)), 'eval/avg_episode_length': (Array(821.41406, dtype=float32), Array(381.5745, dtype=float32)), 'eval/epoch_eval_time': 4.121673583984375, 'eval/sps': 31055.34618203896}
I0727 16:43:53.861293 140141833336640 train.py:379] starting iteration 140 1420.5042464733124
I0727 16:44:03.828895 140141833336640 train.py:394] {'eval/walltime': 588.2915525436401, 'training/sps': 41901.72990015662, 'training/walltime': 833.171797990799, 'training/entropy_loss': Array(-0.05217128, dtype=float32), 'training/policy_loss': Array(0.00075307, dtype=float32), 'training/total_loss': Array(41.064022, dtype=float32), 'training/v_loss': Array(41.11544, dtype=float32), 'eval/episode_goal_distance': (Array(0.48198396, dtype=float32), Array(0.22967994, dtype=float32)), 'eval/episode_reward': (Array(-9157.047, dtype=float32), Array(5610.064, dtype=float32)), 'eval/avg_episode_length': (Array(805.75, dtype=float32), Array(394.28513, dtype=float32)), 'eval/epoch_eval_time': 4.098819732666016, 'eval/sps': 31228.50194652116}
I0727 16:44:03.831456 140141833336640 train.py:379] starting iteration 141 1430.4744095802307
I0727 16:44:13.798144 140141833336640 train.py:394] {'eval/walltime': 592.3722960948944, 'training/sps': 41779.64446175954, 'training/walltime': 839.0540883541107, 'training/entropy_loss': Array(-0.05138095, dtype=float32), 'training/policy_loss': Array(0.00133515, dtype=float32), 'training/total_loss': Array(73.244255, dtype=float32), 'training/v_loss': Array(73.294304, dtype=float32), 'eval/episode_goal_distance': (Array(0.5040437, dtype=float32), Array(0.28013325, dtype=float32)), 'eval/episode_reward': (Array(-9713.929, dtype=float32), Array(7037.161, dtype=float32)), 'eval/avg_episode_length': (Array(766.96094, dtype=float32), Array(421.1942, dtype=float32)), 'eval/epoch_eval_time': 4.0807435512542725, 'eval/sps': 31366.832635355742}
I0727 16:44:13.800528 140141833336640 train.py:379] starting iteration 142 1440.4434814453125
I0727 16:44:23.771959 140141833336640 train.py:394] {'eval/walltime': 596.4982385635376, 'training/sps': 42068.981617208774, 'training/walltime': 844.8959221839905, 'training/entropy_loss': Array(-0.05272914, dtype=float32), 'training/policy_loss': Array(0.00044965, dtype=float32), 'training/total_loss': Array(43.53728, dtype=float32), 'training/v_loss': Array(43.58956, dtype=float32), 'eval/episode_goal_distance': (Array(0.50718915, dtype=float32), Array(0.23104809, dtype=float32)), 'eval/episode_reward': (Array(-11163.979, dtype=float32), Array(5265.111, dtype=float32)), 'eval/avg_episode_length': (Array(914.5781, dtype=float32), Array(278.59036, dtype=float32)), 'eval/epoch_eval_time': 4.1259424686431885, 'eval/sps': 31023.214931567538}
I0727 16:44:23.774630 140141833336640 train.py:379] starting iteration 143 1450.4175834655762
I0727 16:44:33.772516 140141833336640 train.py:394] {'eval/walltime': 600.6053242683411, 'training/sps': 41746.338239118915, 'training/walltime': 850.7829055786133, 'training/entropy_loss': Array(-0.05312339, dtype=float32), 'training/policy_loss': Array(-3.628745e-05, dtype=float32), 'training/total_loss': Array(34.404373, dtype=float32), 'training/v_loss': Array(34.457535, dtype=float32), 'eval/episode_goal_distance': (Array(0.52748436, dtype=float32), Array(0.2683941, dtype=float32)), 'eval/episode_reward': (Array(-10824.14, dtype=float32), Array(5526.9614, dtype=float32)), 'eval/avg_episode_length': (Array(883.5078, dtype=float32), Array(319.73566, dtype=float32)), 'eval/epoch_eval_time': 4.107085704803467, 'eval/sps': 31165.65107231554}
I0727 16:44:33.775013 140141833336640 train.py:379] starting iteration 144 1460.4179668426514
I0727 16:44:43.747614 140141833336640 train.py:394] {'eval/walltime': 604.699280500412, 'training/sps': 41830.350302633204, 'training/walltime': 856.6580655574799, 'training/entropy_loss': Array(-0.05296926, dtype=float32), 'training/policy_loss': Array(-0.00016434, dtype=float32), 'training/total_loss': Array(46.212204, dtype=float32), 'training/v_loss': Array(46.265343, dtype=float32), 'eval/episode_goal_distance': (Array(0.553998, dtype=float32), Array(0.27043703, dtype=float32)), 'eval/episode_reward': (Array(-10898.107, dtype=float32), Array(5981.08, dtype=float32)), 'eval/avg_episode_length': (Array(875.8203, dtype=float32), Array(328.5492, dtype=float32)), 'eval/epoch_eval_time': 4.093956232070923, 'eval/sps': 31265.60049599049}
I0727 16:44:43.750050 140141833336640 train.py:379] starting iteration 145 1470.3930037021637
I0727 16:44:53.675397 140141833336640 train.py:394] {'eval/walltime': 608.7791066169739, 'training/sps': 42068.03217567822, 'training/walltime': 862.5000312328339, 'training/entropy_loss': Array(-0.05242115, dtype=float32), 'training/policy_loss': Array(-0.00021935, dtype=float32), 'training/total_loss': Array(43.26731, dtype=float32), 'training/v_loss': Array(43.319954, dtype=float32), 'eval/episode_goal_distance': (Array(0.47901273, dtype=float32), Array(0.24329244, dtype=float32)), 'eval/episode_reward': (Array(-10134.692, dtype=float32), Array(5698.167, dtype=float32)), 'eval/avg_episode_length': (Array(860.1406, dtype=float32), Array(345.7421, dtype=float32)), 'eval/epoch_eval_time': 4.07982611656189, 'eval/sps': 31373.88612725164}
I0727 16:44:53.677886 140141833336640 train.py:379] starting iteration 146 1480.3208403587341
I0727 16:45:03.655512 140141833336640 train.py:394] {'eval/walltime': 612.8884799480438, 'training/sps': 41903.89491457321, 'training/walltime': 868.3648798465729, 'training/entropy_loss': Array(-0.05254534, dtype=float32), 'training/policy_loss': Array(0.00105427, dtype=float32), 'training/total_loss': Array(36.46698, dtype=float32), 'training/v_loss': Array(36.51847, dtype=float32), 'eval/episode_goal_distance': (Array(0.5096861, dtype=float32), Array(0.25606447, dtype=float32)), 'eval/episode_reward': (Array(-10192.809, dtype=float32), Array(6375.7627, dtype=float32)), 'eval/avg_episode_length': (Array(821.2344, dtype=float32), Array(381.9578, dtype=float32)), 'eval/epoch_eval_time': 4.109373331069946, 'eval/sps': 31148.301623564825}
I0727 16:45:03.657918 140141833336640 train.py:379] starting iteration 147 1490.300873041153
I0727 16:45:13.588413 140141833336640 train.py:394] {'eval/walltime': 616.9726321697235, 'training/sps': 42062.695154661356, 'training/walltime': 874.2075867652893, 'training/entropy_loss': Array(-0.05219012, dtype=float32), 'training/policy_loss': Array(0.00124225, dtype=float32), 'training/total_loss': Array(35.767784, dtype=float32), 'training/v_loss': Array(35.818733, dtype=float32), 'eval/episode_goal_distance': (Array(0.49509424, dtype=float32), Array(0.2522129, dtype=float32)), 'eval/episode_reward': (Array(-9597.965, dtype=float32), Array(6099.9004, dtype=float32)), 'eval/avg_episode_length': (Array(805.75, dtype=float32), Array(394.28543, dtype=float32)), 'eval/epoch_eval_time': 4.0841522216796875, 'eval/sps': 31340.653592817725}
I0727 16:45:13.591060 140141833336640 train.py:379] starting iteration 148 1500.2340140342712
I0727 16:45:23.557180 140141833336640 train.py:394] {'eval/walltime': 621.0514917373657, 'training/sps': 41770.595223762706, 'training/walltime': 880.0911514759064, 'training/entropy_loss': Array(-0.05228432, dtype=float32), 'training/policy_loss': Array(0.00108196, dtype=float32), 'training/total_loss': Array(36.59066, dtype=float32), 'training/v_loss': Array(36.64186, dtype=float32), 'eval/episode_goal_distance': (Array(0.5275613, dtype=float32), Array(0.26329952, dtype=float32)), 'eval/episode_reward': (Array(-10442.293, dtype=float32), Array(5989.3687, dtype=float32)), 'eval/avg_episode_length': (Array(852.5547, dtype=float32), Array(353.15762, dtype=float32)), 'eval/epoch_eval_time': 4.078859567642212, 'eval/sps': 31381.32065527093}
I0727 16:45:23.559774 140141833336640 train.py:379] starting iteration 149 1510.2027277946472
I0727 16:45:33.560481 140141833336640 train.py:394] {'eval/walltime': 625.1599478721619, 'training/sps': 41735.90079801286, 'training/walltime': 885.9796071052551, 'training/entropy_loss': Array(-0.05187831, dtype=float32), 'training/policy_loss': Array(0.00107109, dtype=float32), 'training/total_loss': Array(36.60461, dtype=float32), 'training/v_loss': Array(36.65542, dtype=float32), 'eval/episode_goal_distance': (Array(0.49356285, dtype=float32), Array(0.27244058, dtype=float32)), 'eval/episode_reward': (Array(-8964.163, dtype=float32), Array(6586.0054, dtype=float32)), 'eval/avg_episode_length': (Array(743.40625, dtype=float32), Array(435.36282, dtype=float32)), 'eval/epoch_eval_time': 4.108456134796143, 'eval/sps': 31155.255356365447}
I0727 16:45:33.563012 140141833336640 train.py:379] starting iteration 150 1520.20596575737
I0727 16:45:43.547120 140141833336640 train.py:394] {'eval/walltime': 629.2458598613739, 'training/sps': 41695.74804250361, 'training/walltime': 891.8737332820892, 'training/entropy_loss': Array(-0.051349, dtype=float32), 'training/policy_loss': Array(0.00114156, dtype=float32), 'training/total_loss': Array(69.66202, dtype=float32), 'training/v_loss': Array(69.71223, dtype=float32), 'eval/episode_goal_distance': (Array(0.49779755, dtype=float32), Array(0.26230943, dtype=float32)), 'eval/episode_reward': (Array(-9715.141, dtype=float32), Array(6075.544, dtype=float32)), 'eval/avg_episode_length': (Array(821.3281, dtype=float32), Array(381.7574, dtype=float32)), 'eval/epoch_eval_time': 4.085911989212036, 'eval/sps': 31327.155439949813}
I0727 16:45:43.549514 140141833336640 train.py:379] starting iteration 151 1530.1924681663513
I0727 16:45:53.479893 140141833336640 train.py:394] {'eval/walltime': 633.3217115402222, 'training/sps': 42004.854272445365, 'training/walltime': 897.7244856357574, 'training/entropy_loss': Array(-0.05158659, dtype=float32), 'training/policy_loss': Array(0.00059386, dtype=float32), 'training/total_loss': Array(31.977095, dtype=float32), 'training/v_loss': Array(32.028088, dtype=float32), 'eval/episode_goal_distance': (Array(0.48522025, dtype=float32), Array(0.2347623, dtype=float32)), 'eval/episode_reward': (Array(-9061.885, dtype=float32), Array(5658.418, dtype=float32)), 'eval/avg_episode_length': (Array(797.875, dtype=float32), Array(400.3447, dtype=float32)), 'eval/epoch_eval_time': 4.075851678848267, 'eval/sps': 31404.479378937947}
I0727 16:45:53.482144 140141833336640 train.py:379] starting iteration 152 1540.1250972747803
I0727 16:46:03.420943 140141833336640 train.py:394] {'eval/walltime': 637.4125950336456, 'training/sps': 42050.760853690794, 'training/walltime': 903.5688507556915, 'training/entropy_loss': Array(-0.05103447, dtype=float32), 'training/policy_loss': Array(0.00045322, dtype=float32), 'training/total_loss': Array(34.210403, dtype=float32), 'training/v_loss': Array(34.260986, dtype=float32), 'eval/episode_goal_distance': (Array(0.4891529, dtype=float32), Array(0.25144234, dtype=float32)), 'eval/episode_reward': (Array(-9107.484, dtype=float32), Array(6410.578, dtype=float32)), 'eval/avg_episode_length': (Array(766.8594, dtype=float32), Array(421.37708, dtype=float32)), 'eval/epoch_eval_time': 4.090883493423462, 'eval/sps': 31289.0846697966}
I0727 16:46:03.423416 140141833336640 train.py:379] starting iteration 153 1550.0663704872131
I0727 16:46:13.355942 140141833336640 train.py:394] {'eval/walltime': 641.4940407276154, 'training/sps': 42028.146312230405, 'training/walltime': 909.416360616684, 'training/entropy_loss': Array(-0.04756933, dtype=float32), 'training/policy_loss': Array(0.00052407, dtype=float32), 'training/total_loss': Array(41.039524, dtype=float32), 'training/v_loss': Array(41.08657, dtype=float32), 'eval/episode_goal_distance': (Array(0.5062857, dtype=float32), Array(0.2356286, dtype=float32)), 'eval/episode_reward': (Array(-10766.934, dtype=float32), Array(5460.1987, dtype=float32)), 'eval/avg_episode_length': (Array(891.25, dtype=float32), Array(310.3265, dtype=float32)), 'eval/epoch_eval_time': 4.081445693969727, 'eval/sps': 31361.4365098911}
I0727 16:46:13.437755 140141833336640 train.py:379] starting iteration 154 1560.0807058811188
I0727 16:46:23.423648 140141833336640 train.py:394] {'eval/walltime': 645.5780470371246, 'training/sps': 41665.98897775024, 'training/walltime': 915.3146965503693, 'training/entropy_loss': Array(-0.04475772, dtype=float32), 'training/policy_loss': Array(0.00062159, dtype=float32), 'training/total_loss': Array(35.41487, dtype=float32), 'training/v_loss': Array(35.45901, dtype=float32), 'eval/episode_goal_distance': (Array(0.43608278, dtype=float32), Array(0.20543791, dtype=float32)), 'eval/episode_reward': (Array(-8385.125, dtype=float32), Array(5240.942, dtype=float32)), 'eval/avg_episode_length': (Array(798.0156, dtype=float32), Array(400.06628, dtype=float32)), 'eval/epoch_eval_time': 4.084006309509277, 'eval/sps': 31341.773322426652}
I0727 16:46:23.426053 140141833336640 train.py:379] starting iteration 155 1570.0690066814423
I0727 16:46:33.397416 140141833336640 train.py:394] {'eval/walltime': 649.6676044464111, 'training/sps': 41808.61312822351, 'training/walltime': 921.1929111480713, 'training/entropy_loss': Array(-0.04296019, dtype=float32), 'training/policy_loss': Array(0.00071016, dtype=float32), 'training/total_loss': Array(31.632565, dtype=float32), 'training/v_loss': Array(31.674814, dtype=float32), 'eval/episode_goal_distance': (Array(0.45217103, dtype=float32), Array(0.19855191, dtype=float32)), 'eval/episode_reward': (Array(-9036.689, dtype=float32), Array(5760.356, dtype=float32)), 'eval/avg_episode_length': (Array(798.0547, dtype=float32), Array(399.9892, dtype=float32)), 'eval/epoch_eval_time': 4.089557409286499, 'eval/sps': 31299.230500919177}
I0727 16:46:33.399674 140141833336640 train.py:379] starting iteration 156 1580.0426280498505
I0727 16:46:43.377611 140141833336640 train.py:394] {'eval/walltime': 653.7671535015106, 'training/sps': 41831.92225436657, 'training/walltime': 927.0678503513336, 'training/entropy_loss': Array(-0.04193133, dtype=float32), 'training/policy_loss': Array(0.00129725, dtype=float32), 'training/total_loss': Array(31.307526, dtype=float32), 'training/v_loss': Array(31.34816, dtype=float32), 'eval/episode_goal_distance': (Array(0.47896376, dtype=float32), Array(0.2493754, dtype=float32)), 'eval/episode_reward': (Array(-8528.596, dtype=float32), Array(6145.974, dtype=float32)), 'eval/avg_episode_length': (Array(751.46094, dtype=float32), Array(430.483, dtype=float32)), 'eval/epoch_eval_time': 4.099549055099487, 'eval/sps': 31222.946299612875}
I0727 16:46:43.379889 140141833336640 train.py:379] starting iteration 157 1590.0228424072266
I0727 16:46:53.324162 140141833336640 train.py:394] {'eval/walltime': 657.8474681377411, 'training/sps': 41933.96591020872, 'training/walltime': 932.9284932613373, 'training/entropy_loss': Array(-0.04060105, dtype=float32), 'training/policy_loss': Array(0.0015242, dtype=float32), 'training/total_loss': Array(31.441797, dtype=float32), 'training/v_loss': Array(31.480873, dtype=float32), 'eval/episode_goal_distance': (Array(0.47049496, dtype=float32), Array(0.23645662, dtype=float32)), 'eval/episode_reward': (Array(-9468.866, dtype=float32), Array(6232.5806, dtype=float32)), 'eval/avg_episode_length': (Array(813.39844, dtype=float32), Array(388.44266, dtype=float32)), 'eval/epoch_eval_time': 4.080314636230469, 'eval/sps': 31370.129857988275}
I0727 16:46:53.326434 140141833336640 train.py:379] starting iteration 158 1599.9693882465363
I0727 16:47:03.292205 140141833336640 train.py:394] {'eval/walltime': 661.9238123893738, 'training/sps': 41755.08266236615, 'training/walltime': 938.8142437934875, 'training/entropy_loss': Array(-0.04579026, dtype=float32), 'training/policy_loss': Array(0.00113929, dtype=float32), 'training/total_loss': Array(72.37738, dtype=float32), 'training/v_loss': Array(72.422035, dtype=float32), 'eval/episode_goal_distance': (Array(0.48966393, dtype=float32), Array(0.21345162, dtype=float32)), 'eval/episode_reward': (Array(-9820.844, dtype=float32), Array(5434.474, dtype=float32)), 'eval/avg_episode_length': (Array(852.5703, dtype=float32), Array(353.11993, dtype=float32)), 'eval/epoch_eval_time': 4.07634425163269, 'eval/sps': 31400.684559144484}
I0727 16:47:03.294539 140141833336640 train.py:379] starting iteration 159 1609.9374930858612
I0727 16:47:13.288041 140141833336640 train.py:394] {'eval/walltime': 666.037858247757, 'training/sps': 41825.36192076119, 'training/walltime': 944.6901044845581, 'training/entropy_loss': Array(-0.04686723, dtype=float32), 'training/policy_loss': Array(0.00025267, dtype=float32), 'training/total_loss': Array(27.117012, dtype=float32), 'training/v_loss': Array(27.163628, dtype=float32), 'eval/episode_goal_distance': (Array(0.49983978, dtype=float32), Array(0.22116178, dtype=float32)), 'eval/episode_reward': (Array(-10104.752, dtype=float32), Array(5152.459, dtype=float32)), 'eval/avg_episode_length': (Array(891.3672, dtype=float32), Array(309.9918, dtype=float32)), 'eval/epoch_eval_time': 4.114045858383179, 'eval/sps': 31112.924942043315}
I0727 16:47:13.290268 140141833336640 train.py:379] starting iteration 160 1619.9332222938538
I0727 16:47:23.229554 140141833336640 train.py:394] {'eval/walltime': 670.1143038272858, 'training/sps': 41944.72284084965, 'training/walltime': 950.5492444038391, 'training/entropy_loss': Array(-0.04559004, dtype=float32), 'training/policy_loss': Array(0.00039698, dtype=float32), 'training/total_loss': Array(24.166061, dtype=float32), 'training/v_loss': Array(24.211254, dtype=float32), 'eval/episode_goal_distance': (Array(0.43437445, dtype=float32), Array(0.20310022, dtype=float32)), 'eval/episode_reward': (Array(-8814.588, dtype=float32), Array(4955.327, dtype=float32)), 'eval/avg_episode_length': (Array(836.9922, dtype=float32), Array(367.95273, dtype=float32)), 'eval/epoch_eval_time': 4.076445579528809, 'eval/sps': 31399.90403472904}
I0727 16:47:23.231842 140141833336640 train.py:379] starting iteration 161 1629.8747961521149
I0727 16:47:33.228898 140141833336640 train.py:394] {'eval/walltime': 674.2421054840088, 'training/sps': 41897.635551446874, 'training/walltime': 956.4149692058563, 'training/entropy_loss': Array(-0.04569928, dtype=float32), 'training/policy_loss': Array(0.00055952, dtype=float32), 'training/total_loss': Array(28.227057, dtype=float32), 'training/v_loss': Array(28.272196, dtype=float32), 'eval/episode_goal_distance': (Array(0.46562475, dtype=float32), Array(0.21138372, dtype=float32)), 'eval/episode_reward': (Array(-9212.049, dtype=float32), Array(5175.9424, dtype=float32)), 'eval/avg_episode_length': (Array(844.65625, dtype=float32), Array(360.98685, dtype=float32)), 'eval/epoch_eval_time': 4.1278016567230225, 'eval/sps': 31009.241878549125}
I0727 16:47:33.231147 140141833336640 train.py:379] starting iteration 162 1639.874099969864
I0727 16:47:43.237127 140141833336640 train.py:394] {'eval/walltime': 678.3619182109833, 'training/sps': 41777.526131518745, 'training/walltime': 962.2975578308105, 'training/entropy_loss': Array(-0.04673065, dtype=float32), 'training/policy_loss': Array(0.00074112, dtype=float32), 'training/total_loss': Array(25.841644, dtype=float32), 'training/v_loss': Array(25.887634, dtype=float32), 'eval/episode_goal_distance': (Array(0.4571338, dtype=float32), Array(0.18586649, dtype=float32)), 'eval/episode_reward': (Array(-9628.426, dtype=float32), Array(4445.4067, dtype=float32)), 'eval/avg_episode_length': (Array(906.8125, dtype=float32), Array(289.73245, dtype=float32)), 'eval/epoch_eval_time': 4.119812726974487, 'eval/sps': 31069.373411543584}
I0727 16:47:43.239604 140141833336640 train.py:379] starting iteration 163 1649.8825573921204
I0727 16:47:53.205711 140141833336640 train.py:394] {'eval/walltime': 682.4589354991913, 'training/sps': 41899.883597269785, 'training/walltime': 968.1629679203033, 'training/entropy_loss': Array(-0.04520078, dtype=float32), 'training/policy_loss': Array(0.00088705, dtype=float32), 'training/total_loss': Array(25.809948, dtype=float32), 'training/v_loss': Array(25.85426, dtype=float32), 'eval/episode_goal_distance': (Array(0.42698675, dtype=float32), Array(0.17638776, dtype=float32)), 'eval/episode_reward': (Array(-9327.265, dtype=float32), Array(4419.189, dtype=float32)), 'eval/avg_episode_length': (Array(898.9844, dtype=float32), Array(300.4458, dtype=float32)), 'eval/epoch_eval_time': 4.097017288208008, 'eval/sps': 31242.240634035952}
I0727 16:47:53.208316 140141833336640 train.py:379] starting iteration 164 1659.8512694835663
I0727 16:48:03.160163 140141833336640 train.py:394] {'eval/walltime': 686.5411570072174, 'training/sps': 41897.15020958083, 'training/walltime': 974.0287606716156, 'training/entropy_loss': Array(-0.04411826, dtype=float32), 'training/policy_loss': Array(0.0008507, dtype=float32), 'training/total_loss': Array(25.22909, dtype=float32), 'training/v_loss': Array(25.272356, dtype=float32), 'eval/episode_goal_distance': (Array(0.45140162, dtype=float32), Array(0.18715344, dtype=float32)), 'eval/episode_reward': (Array(-9348.08, dtype=float32), Array(4776.7344, dtype=float32)), 'eval/avg_episode_length': (Array(875.83594, dtype=float32), Array(328.50806, dtype=float32)), 'eval/epoch_eval_time': 4.082221508026123, 'eval/sps': 31355.47636215651}
I0727 16:48:03.162621 140141833336640 train.py:379] starting iteration 165 1669.8055746555328
I0727 16:48:13.136465 140141833336640 train.py:394] {'eval/walltime': 690.6452522277832, 'training/sps': 41893.97277537572, 'training/walltime': 979.8949983119965, 'training/entropy_loss': Array(-0.0436393, dtype=float32), 'training/policy_loss': Array(0.00071403, dtype=float32), 'training/total_loss': Array(25.499264, dtype=float32), 'training/v_loss': Array(25.54219, dtype=float32), 'eval/episode_goal_distance': (Array(0.46358618, dtype=float32), Array(0.22224721, dtype=float32)), 'eval/episode_reward': (Array(-9710.238, dtype=float32), Array(5341.658, dtype=float32)), 'eval/avg_episode_length': (Array(868.0547, dtype=float32), Array(337.15674, dtype=float32)), 'eval/epoch_eval_time': 4.104095220565796, 'eval/sps': 31188.36019169013}
I0727 16:48:13.138799 140141833336640 train.py:379] starting iteration 166 1679.781753540039
I0727 16:48:23.099727 140141833336640 train.py:394] {'eval/walltime': 694.7332854270935, 'training/sps': 41871.40405213314, 'training/walltime': 985.7643978595734, 'training/entropy_loss': Array(-0.04407784, dtype=float32), 'training/policy_loss': Array(0.00085593, dtype=float32), 'training/total_loss': Array(71.47596, dtype=float32), 'training/v_loss': Array(71.51919, dtype=float32), 'eval/episode_goal_distance': (Array(0.43759626, dtype=float32), Array(0.18965869, dtype=float32)), 'eval/episode_reward': (Array(-9059.742, dtype=float32), Array(4888.5728, dtype=float32)), 'eval/avg_episode_length': (Array(860.1719, dtype=float32), Array(345.66498, dtype=float32)), 'eval/epoch_eval_time': 4.088033199310303, 'eval/sps': 31310.900317931628}
I0727 16:48:23.101883 140141833336640 train.py:379] starting iteration 167 1689.7448375225067
I0727 16:48:33.038276 140141833336640 train.py:394] {'eval/walltime': 698.8115859031677, 'training/sps': 41977.78235404791, 'training/walltime': 991.6189234256744, 'training/entropy_loss': Array(-0.04461297, dtype=float32), 'training/policy_loss': Array(0.00064315, dtype=float32), 'training/total_loss': Array(38.814007, dtype=float32), 'training/v_loss': Array(38.85798, dtype=float32), 'eval/episode_goal_distance': (Array(0.42691898, dtype=float32), Array(0.19751577, dtype=float32)), 'eval/episode_reward': (Array(-8223.756, dtype=float32), Array(5461.2085, dtype=float32)), 'eval/avg_episode_length': (Array(774.8203, dtype=float32), Array(416.05356, dtype=float32)), 'eval/epoch_eval_time': 4.078300476074219, 'eval/sps': 31385.62270017267}
I0727 16:48:33.042021 140141833336640 train.py:379] starting iteration 168 1699.6849761009216
I0727 16:48:43.018296 140141833336640 train.py:394] {'eval/walltime': 702.9217171669006, 'training/sps': 41920.43544228186, 'training/walltime': 997.4814579486847, 'training/entropy_loss': Array(-0.04526614, dtype=float32), 'training/policy_loss': Array(0.00226883, dtype=float32), 'training/total_loss': Array(24.620522, dtype=float32), 'training/v_loss': Array(24.66352, dtype=float32), 'eval/episode_goal_distance': (Array(0.4457716, dtype=float32), Array(0.20882572, dtype=float32)), 'eval/episode_reward': (Array(-8752.149, dtype=float32), Array(5535.4585, dtype=float32)), 'eval/avg_episode_length': (Array(798.1406, dtype=float32), Array(399.819, dtype=float32)), 'eval/epoch_eval_time': 4.11013126373291, 'eval/sps': 31142.557691392958}
I0727 16:48:43.020614 140141833336640 train.py:379] starting iteration 169 1709.663568496704
I0727 16:48:53.011087 140141833336640 train.py:394] {'eval/walltime': 707.0284712314606, 'training/sps': 41795.54302526699, 'training/walltime': 1003.3615107536316, 'training/entropy_loss': Array(-0.04355333, dtype=float32), 'training/policy_loss': Array(0.00356647, dtype=float32), 'training/total_loss': Array(29.412159, dtype=float32), 'training/v_loss': Array(29.452147, dtype=float32), 'eval/episode_goal_distance': (Array(0.43521497, dtype=float32), Array(0.19532634, dtype=float32)), 'eval/episode_reward': (Array(-9367.812, dtype=float32), Array(4493.508, dtype=float32)), 'eval/avg_episode_length': (Array(906.85156, dtype=float32), Array(289.61096, dtype=float32)), 'eval/epoch_eval_time': 4.1067540645599365, 'eval/sps': 31168.16784930022}
I0727 16:48:53.013370 140141833336640 train.py:379] starting iteration 170 1719.6563239097595
I0727 16:49:02.962944 140141833336640 train.py:394] {'eval/walltime': 711.1033992767334, 'training/sps': 41859.98431335093, 'training/walltime': 1009.2325115203857, 'training/entropy_loss': Array(-0.04478768, dtype=float32), 'training/policy_loss': Array(5.668596e-05, dtype=float32), 'training/total_loss': Array(26.71664, dtype=float32), 'training/v_loss': Array(26.761372, dtype=float32), 'eval/episode_goal_distance': (Array(0.47381324, dtype=float32), Array(0.21901941, dtype=float32)), 'eval/episode_reward': (Array(-9798.34, dtype=float32), Array(4463.8813, dtype=float32)), 'eval/avg_episode_length': (Array(922.33594, dtype=float32), Array(266.78522, dtype=float32)), 'eval/epoch_eval_time': 4.074928045272827, 'eval/sps': 31411.59759826632}
I0727 16:49:02.965151 140141833336640 train.py:379] starting iteration 171 1729.6081051826477
I0727 16:49:12.932241 140141833336640 train.py:394] {'eval/walltime': 715.1909232139587, 'training/sps': 41824.209619276284, 'training/walltime': 1015.1085340976715, 'training/entropy_loss': Array(-0.04378597, dtype=float32), 'training/policy_loss': Array(0.0006576, dtype=float32), 'training/total_loss': Array(27.297016, dtype=float32), 'training/v_loss': Array(27.340145, dtype=float32), 'eval/episode_goal_distance': (Array(0.48120636, dtype=float32), Array(0.20884322, dtype=float32)), 'eval/episode_reward': (Array(-10351.32, dtype=float32), Array(4513.1523, dtype=float32)), 'eval/avg_episode_length': (Array(945.65625, dtype=float32), Array(225.94061, dtype=float32)), 'eval/epoch_eval_time': 4.087523937225342, 'eval/sps': 31314.801323680535}
I0727 16:49:12.934393 140141833336640 train.py:379] starting iteration 172 1739.577347278595
I0727 16:49:22.916481 140141833336640 train.py:394] {'eval/walltime': 719.2912759780884, 'training/sps': 41809.17103561229, 'training/walltime': 1020.986670255661, 'training/entropy_loss': Array(-0.04438375, dtype=float32), 'training/policy_loss': Array(0.00039213, dtype=float32), 'training/total_loss': Array(25.847868, dtype=float32), 'training/v_loss': Array(25.89186, dtype=float32), 'eval/episode_goal_distance': (Array(0.46679083, dtype=float32), Array(0.21182965, dtype=float32)), 'eval/episode_reward': (Array(-10020.916, dtype=float32), Array(4322.2593, dtype=float32)), 'eval/avg_episode_length': (Array(937.84375, dtype=float32), Array(240.73056, dtype=float32)), 'eval/epoch_eval_time': 4.100352764129639, 'eval/sps': 31216.826298400185}
I0727 16:49:22.918842 140141833336640 train.py:379] starting iteration 173 1749.5617957115173
I0727 16:49:32.875377 140141833336640 train.py:394] {'eval/walltime': 723.38920545578, 'training/sps': 41974.447393926064, 'training/walltime': 1026.84166097641, 'training/entropy_loss': Array(-0.04506242, dtype=float32), 'training/policy_loss': Array(0.00073373, dtype=float32), 'training/total_loss': Array(27.273863, dtype=float32), 'training/v_loss': Array(27.318193, dtype=float32), 'eval/episode_goal_distance': (Array(0.44879672, dtype=float32), Array(0.20819749, dtype=float32)), 'eval/episode_reward': (Array(-9810.658, dtype=float32), Array(4542.358, dtype=float32)), 'eval/avg_episode_length': (Array(930.0781, dtype=float32), Array(254.25357, dtype=float32)), 'eval/epoch_eval_time': 4.09792947769165, 'eval/sps': 31235.286184598266}
I0727 16:49:32.877617 140141833336640 train.py:379] starting iteration 174 1759.5205707550049
I0727 16:49:42.842477 140141833336640 train.py:394] {'eval/walltime': 727.4891731739044, 'training/sps': 41929.51561562967, 'training/walltime': 1032.7029259204865, 'training/entropy_loss': Array(-0.04516189, dtype=float32), 'training/policy_loss': Array(0.00114179, dtype=float32), 'training/total_loss': Array(26.24532, dtype=float32), 'training/v_loss': Array(26.289341, dtype=float32), 'eval/episode_goal_distance': (Array(0.4893197, dtype=float32), Array(0.1921191, dtype=float32)), 'eval/episode_reward': (Array(-10358.806, dtype=float32), Array(4603.1953, dtype=float32)), 'eval/avg_episode_length': (Array(922.3203, dtype=float32), Array(266.83893, dtype=float32)), 'eval/epoch_eval_time': 4.09996771812439, 'eval/sps': 31219.758007888926}
I0727 16:49:42.845067 140141833336640 train.py:379] starting iteration 175 1769.488020658493
I0727 16:49:52.852751 140141833336640 train.py:394] {'eval/walltime': 731.6057381629944, 'training/sps': 41743.41861034421, 'training/walltime': 1038.5903210639954, 'training/entropy_loss': Array(-0.04410804, dtype=float32), 'training/policy_loss': Array(0.00110383, dtype=float32), 'training/total_loss': Array(61.921585, dtype=float32), 'training/v_loss': Array(61.96459, dtype=float32), 'eval/episode_goal_distance': (Array(0.4857134, dtype=float32), Array(0.23858517, dtype=float32)), 'eval/episode_reward': (Array(-9934.825, dtype=float32), Array(4655.1313, dtype=float32)), 'eval/avg_episode_length': (Array(922.3281, dtype=float32), Array(266.812, dtype=float32)), 'eval/epoch_eval_time': 4.116564989089966, 'eval/sps': 31093.88539698398}
I0727 16:49:52.855330 140141833336640 train.py:379] starting iteration 176 1779.4982838630676
I0727 16:50:02.819841 140141833336640 train.py:394] {'eval/walltime': 735.6881446838379, 'training/sps': 41806.997146168076, 'training/walltime': 1044.4687628746033, 'training/entropy_loss': Array(-0.04300901, dtype=float32), 'training/policy_loss': Array(0.00304829, dtype=float32), 'training/total_loss': Array(24.547012, dtype=float32), 'training/v_loss': Array(24.586971, dtype=float32), 'eval/episode_goal_distance': (Array(0.42869854, dtype=float32), Array(0.18303941, dtype=float32)), 'eval/episode_reward': (Array(-9014.154, dtype=float32), Array(4358.2544, dtype=float32)), 'eval/avg_episode_length': (Array(906.7969, dtype=float32), Array(289.78094, dtype=float32)), 'eval/epoch_eval_time': 4.082406520843506, 'eval/sps': 31354.055346147317}
I0727 16:50:02.822168 140141833336640 train.py:379] starting iteration 177 1789.4651219844818
I0727 16:50:12.787878 140141833336640 train.py:394] {'eval/walltime': 739.7722969055176, 'training/sps': 41810.42934948999, 'training/walltime': 1050.346722126007, 'training/entropy_loss': Array(-0.04119493, dtype=float32), 'training/policy_loss': Array(0.00375567, dtype=float32), 'training/total_loss': Array(22.21465, dtype=float32), 'training/v_loss': Array(22.252089, dtype=float32), 'eval/episode_goal_distance': (Array(0.43064666, dtype=float32), Array(0.17144287, dtype=float32)), 'eval/episode_reward': (Array(-9585.89, dtype=float32), Array(3939.0518, dtype=float32)), 'eval/avg_episode_length': (Array(930.1328, dtype=float32), Array(254.05392, dtype=float32)), 'eval/epoch_eval_time': 4.0841522216796875, 'eval/sps': 31340.653592817725}
I0727 16:50:12.790380 140141833336640 train.py:379] starting iteration 178 1799.4333341121674
I0727 16:50:22.760008 140141833336640 train.py:394] {'eval/walltime': 743.8627727031708, 'training/sps': 41827.598822715365, 'training/walltime': 1056.2222685813904, 'training/entropy_loss': Array(-0.04014817, dtype=float32), 'training/policy_loss': Array(0.00364137, dtype=float32), 'training/total_loss': Array(27.177177, dtype=float32), 'training/v_loss': Array(27.213684, dtype=float32), 'eval/episode_goal_distance': (Array(0.41727537, dtype=float32), Array(0.16237694, dtype=float32)), 'eval/episode_reward': (Array(-9252.449, dtype=float32), Array(4165.9814, dtype=float32)), 'eval/avg_episode_length': (Array(922.3047, dtype=float32), Array(266.89243, dtype=float32)), 'eval/epoch_eval_time': 4.090475797653198, 'eval/sps': 31292.20323793056}
I0727 16:50:22.762282 140141833336640 train.py:379] starting iteration 179 1809.4052367210388
I0727 16:50:32.706987 140141833336640 train.py:394] {'eval/walltime': 747.9538116455078, 'training/sps': 42010.91285542499, 'training/walltime': 1062.0721771717072, 'training/entropy_loss': Array(-0.0383974, dtype=float32), 'training/policy_loss': Array(0.00316869, dtype=float32), 'training/total_loss': Array(22.937717, dtype=float32), 'training/v_loss': Array(22.972946, dtype=float32), 'eval/episode_goal_distance': (Array(0.40000042, dtype=float32), Array(0.1717654, dtype=float32)), 'eval/episode_reward': (Array(-8847.55, dtype=float32), Array(3858.3992, dtype=float32)), 'eval/avg_episode_length': (Array(922.2969, dtype=float32), Array(266.91943, dtype=float32)), 'eval/epoch_eval_time': 4.091038942337036, 'eval/sps': 31287.895765391826}
I0727 16:50:32.709284 140141833336640 train.py:379] starting iteration 180 1819.3522381782532
I0727 16:50:42.678782 140141833336640 train.py:394] {'eval/walltime': 752.0355877876282, 'training/sps': 41766.8513911713, 'training/walltime': 1067.9562692642212, 'training/entropy_loss': Array(-0.03857825, dtype=float32), 'training/policy_loss': Array(0.00322803, dtype=float32), 'training/total_loss': Array(20.987057, dtype=float32), 'training/v_loss': Array(21.022406, dtype=float32), 'eval/episode_goal_distance': (Array(0.4275757, dtype=float32), Array(0.19899035, dtype=float32)), 'eval/episode_reward': (Array(-9447.424, dtype=float32), Array(3967.6965, dtype=float32)), 'eval/avg_episode_length': (Array(953.41406, dtype=float32), Array(210.06783, dtype=float32)), 'eval/epoch_eval_time': 4.081776142120361, 'eval/sps': 31358.89758361609}
I0727 16:50:42.681131 140141833336640 train.py:379] starting iteration 181 1829.3240852355957
I0727 16:50:52.683508 140141833336640 train.py:394] {'eval/walltime': 756.1549665927887, 'training/sps': 41799.03775676465, 'training/walltime': 1073.835830450058, 'training/entropy_loss': Array(-0.038344, dtype=float32), 'training/policy_loss': Array(0.00364215, dtype=float32), 'training/total_loss': Array(21.293812, dtype=float32), 'training/v_loss': Array(21.328516, dtype=float32), 'eval/episode_goal_distance': (Array(0.41031832, dtype=float32), Array(0.1819376, dtype=float32)), 'eval/episode_reward': (Array(-8797.911, dtype=float32), Array(4302.951, dtype=float32)), 'eval/avg_episode_length': (Array(899.02344, dtype=float32), Array(300.32983, dtype=float32)), 'eval/epoch_eval_time': 4.1193788051605225, 'eval/sps': 31072.64615714605}
I0727 16:50:52.685774 140141833336640 train.py:379] starting iteration 182 1839.3287279605865
I0727 16:51:02.675637 140141833336640 train.py:394] {'eval/walltime': 760.2661943435669, 'training/sps': 41836.15998272642, 'training/walltime': 1079.7101745605469, 'training/entropy_loss': Array(-0.03905677, dtype=float32), 'training/policy_loss': Array(0.0038339, dtype=float32), 'training/total_loss': Array(22.140585, dtype=float32), 'training/v_loss': Array(22.175808, dtype=float32), 'eval/episode_goal_distance': (Array(0.4082457, dtype=float32), Array(0.17411233, dtype=float32)), 'eval/episode_reward': (Array(-8946.594, dtype=float32), Array(4460.5493, dtype=float32)), 'eval/avg_episode_length': (Array(891.3203, dtype=float32), Array(310.126, dtype=float32)), 'eval/epoch_eval_time': 4.111227750778198, 'eval/sps': 31134.251800030142}
I0727 16:51:02.677988 140141833336640 train.py:379] starting iteration 183 1849.3209431171417
I0727 16:51:12.626227 140141833336640 train.py:394] {'eval/walltime': 764.3510293960571, 'training/sps': 41941.42726201324, 'training/walltime': 1085.5697748661041, 'training/entropy_loss': Array(-0.0387133, dtype=float32), 'training/policy_loss': Array(0.0045704, dtype=float32), 'training/total_loss': Array(51.8719, dtype=float32), 'training/v_loss': Array(51.906036, dtype=float32), 'eval/episode_goal_distance': (Array(0.3951128, dtype=float32), Array(0.17652221, dtype=float32)), 'eval/episode_reward': (Array(-8808.2, dtype=float32), Array(4351.2163, dtype=float32)), 'eval/avg_episode_length': (Array(906.85156, dtype=float32), Array(289.61087, dtype=float32)), 'eval/epoch_eval_time': 4.084835052490234, 'eval/sps': 31335.41461410234}
I0727 16:51:12.628596 140141833336640 train.py:379] starting iteration 184 1859.2715497016907
I0727 16:51:22.589869 140141833336640 train.py:394] {'eval/walltime': 768.428008556366, 'training/sps': 41792.318289605944, 'training/walltime': 1091.450281381607, 'training/entropy_loss': Array(-0.03846492, dtype=float32), 'training/policy_loss': Array(0.00525615, dtype=float32), 'training/total_loss': Array(23.57151, dtype=float32), 'training/v_loss': Array(23.60472, dtype=float32), 'eval/episode_goal_distance': (Array(0.4230042, dtype=float32), Array(0.18849254, dtype=float32)), 'eval/episode_reward': (Array(-9011.193, dtype=float32), Array(4439.623, dtype=float32)), 'eval/avg_episode_length': (Array(906.8047, dtype=float32), Array(289.75638, dtype=float32)), 'eval/epoch_eval_time': 4.076979160308838, 'eval/sps': 31395.794525058056}
I0727 16:51:22.592183 140141833336640 train.py:379] starting iteration 185 1869.235137462616
I0727 16:51:32.604636 140141833336640 train.py:394] {'eval/walltime': 772.5514996051788, 'training/sps': 41759.38942349119, 'training/walltime': 1097.335424900055, 'training/entropy_loss': Array(-0.03490933, dtype=float32), 'training/policy_loss': Array(0.00615935, dtype=float32), 'training/total_loss': Array(18.460745, dtype=float32), 'training/v_loss': Array(18.489496, dtype=float32), 'eval/episode_goal_distance': (Array(0.3973158, dtype=float32), Array(0.17299151, dtype=float32)), 'eval/episode_reward': (Array(-8672.341, dtype=float32), Array(4621.902, dtype=float32)), 'eval/avg_episode_length': (Array(891.22656, dtype=float32), Array(310.3932, dtype=float32)), 'eval/epoch_eval_time': 4.123491048812866, 'eval/sps': 31041.65826596146}
I0727 16:51:32.606932 140141833336640 train.py:379] starting iteration 186 1879.2498862743378
I0727 16:51:42.651075 140141833336640 train.py:394] {'eval/walltime': 776.6698212623596, 'training/sps': 41497.300301477684, 'training/walltime': 1103.2577378749847, 'training/entropy_loss': Array(-0.03400563, dtype=float32), 'training/policy_loss': Array(0.00564818, dtype=float32), 'training/total_loss': Array(22.809246, dtype=float32), 'training/v_loss': Array(22.837605, dtype=float32), 'eval/episode_goal_distance': (Array(0.38330746, dtype=float32), Array(0.17701721, dtype=float32)), 'eval/episode_reward': (Array(-8086., dtype=float32), Array(4638.543, dtype=float32)), 'eval/avg_episode_length': (Array(867.8906, dtype=float32), Array(337.5757, dtype=float32)), 'eval/epoch_eval_time': 4.118321657180786, 'eval/sps': 31080.622315359145}
I0727 16:51:42.653363 140141833336640 train.py:379] starting iteration 187 1889.2963168621063
I0727 16:51:52.632549 140141833336640 train.py:394] {'eval/walltime': 780.7744770050049, 'training/sps': 41861.32048861646, 'training/walltime': 1109.1285512447357, 'training/entropy_loss': Array(-0.03322644, dtype=float32), 'training/policy_loss': Array(0.00745487, dtype=float32), 'training/total_loss': Array(18.827929, dtype=float32), 'training/v_loss': Array(18.853699, dtype=float32), 'eval/episode_goal_distance': (Array(0.37434632, dtype=float32), Array(0.16452287, dtype=float32)), 'eval/episode_reward': (Array(-8242.188, dtype=float32), Array(4149.829, dtype=float32)), 'eval/avg_episode_length': (Array(883.5156, dtype=float32), Array(319.7144, dtype=float32)), 'eval/epoch_eval_time': 4.104655742645264, 'eval/sps': 31184.101182992224}
I0727 16:51:52.634863 140141833336640 train.py:379] starting iteration 188 1899.277816772461
I0727 16:52:02.583520 140141833336640 train.py:394] {'eval/walltime': 784.8533089160919, 'training/sps': 41895.4899145432, 'training/walltime': 1114.9945764541626, 'training/entropy_loss': Array(-0.03261706, dtype=float32), 'training/policy_loss': Array(0.00654384, dtype=float32), 'training/total_loss': Array(17.1166, dtype=float32), 'training/v_loss': Array(17.142675, dtype=float32), 'eval/episode_goal_distance': (Array(0.37285775, dtype=float32), Array(0.17166375, dtype=float32)), 'eval/episode_reward': (Array(-8858.8, dtype=float32), Array(3913.2227, dtype=float32)), 'eval/avg_episode_length': (Array(945.6172, dtype=float32), Array(226.10269, dtype=float32)), 'eval/epoch_eval_time': 4.078831911087036, 'eval/sps': 31381.53343658801}
I0727 16:52:02.585974 140141833336640 train.py:379] starting iteration 189 1909.2289290428162
I0727 16:52:12.565618 140141833336640 train.py:394] {'eval/walltime': 788.9350459575653, 'training/sps': 41694.88620704217, 'training/walltime': 1120.8888244628906, 'training/entropy_loss': Array(-0.03164843, dtype=float32), 'training/policy_loss': Array(0.00631717, dtype=float32), 'training/total_loss': Array(16.792751, dtype=float32), 'training/v_loss': Array(16.818083, dtype=float32), 'eval/episode_goal_distance': (Array(0.37162638, dtype=float32), Array(0.165016, dtype=float32)), 'eval/episode_reward': (Array(-8687.162, dtype=float32), Array(4103.3213, dtype=float32)), 'eval/avg_episode_length': (Array(937.91406, dtype=float32), Array(240.45828, dtype=float32)), 'eval/epoch_eval_time': 4.081737041473389, 'eval/sps': 31359.197983463364}
I0727 16:52:12.567923 140141833336640 train.py:379] starting iteration 190 1919.2108771800995
I0727 16:52:22.530172 140141833336640 train.py:394] {'eval/walltime': 793.0325155258179, 'training/sps': 41930.1535071029, 'training/walltime': 1126.7500002384186, 'training/entropy_loss': Array(-0.03228552, dtype=float32), 'training/policy_loss': Array(0.00727425, dtype=float32), 'training/total_loss': Array(17.201199, dtype=float32), 'training/v_loss': Array(17.22621, dtype=float32), 'eval/episode_goal_distance': (Array(0.37951553, dtype=float32), Array(0.18381894, dtype=float32)), 'eval/episode_reward': (Array(-8866.129, dtype=float32), Array(3884.8896, dtype=float32)), 'eval/avg_episode_length': (Array(961.14844, dtype=float32), Array(192.69759, dtype=float32)), 'eval/epoch_eval_time': 4.0974695682525635, 'eval/sps': 31238.792105193792}
I0727 16:52:22.532477 140141833336640 train.py:379] starting iteration 191 1929.1754310131073
I0727 16:52:32.475277 140141833336640 train.py:394] {'eval/walltime': 797.113995552063, 'training/sps': 41956.17008236098, 'training/walltime': 1132.6075415611267, 'training/entropy_loss': Array(-0.03212864, dtype=float32), 'training/policy_loss': Array(0.00646209, dtype=float32), 'training/total_loss': Array(44.846504, dtype=float32), 'training/v_loss': Array(44.87217, dtype=float32), 'eval/episode_goal_distance': (Array(0.37062606, dtype=float32), Array(0.14643249, dtype=float32)), 'eval/episode_reward': (Array(-8542.265, dtype=float32), Array(4039.8984, dtype=float32)), 'eval/avg_episode_length': (Array(906.84375, dtype=float32), Array(289.63504, dtype=float32)), 'eval/epoch_eval_time': 4.081480026245117, 'eval/sps': 31361.172706205187}
I0727 16:52:32.477699 140141833336640 train.py:379] starting iteration 192 1939.1206529140472
I0727 16:52:42.444973 140141833336640 train.py:394] {'eval/walltime': 801.1827936172485, 'training/sps': 41699.786157110924, 'training/walltime': 1138.5010969638824, 'training/entropy_loss': Array(-0.03196109, dtype=float32), 'training/policy_loss': Array(0.00655444, dtype=float32), 'training/total_loss': Array(30.462412, dtype=float32), 'training/v_loss': Array(30.487818, dtype=float32), 'eval/episode_goal_distance': (Array(0.38389963, dtype=float32), Array(0.17831208, dtype=float32)), 'eval/episode_reward': (Array(-8603.061, dtype=float32), Array(4633.867, dtype=float32)), 'eval/avg_episode_length': (Array(899.0781, dtype=float32), Array(300.16733, dtype=float32)), 'eval/epoch_eval_time': 4.068798065185547, 'eval/sps': 31458.92176247948}
I0727 16:52:42.447126 140141833336640 train.py:379] starting iteration 193 1949.0900807380676
I0727 16:52:52.434522 140141833336640 train.py:394] {'eval/walltime': 805.2834694385529, 'training/sps': 41773.44755386397, 'training/walltime': 1144.3842599391937, 'training/entropy_loss': Array(-0.031702, dtype=float32), 'training/policy_loss': Array(0.00637533, dtype=float32), 'training/total_loss': Array(17.299772, dtype=float32), 'training/v_loss': Array(17.3251, dtype=float32), 'eval/episode_goal_distance': (Array(0.38325346, dtype=float32), Array(0.1603962, dtype=float32)), 'eval/episode_reward': (Array(-8894.875, dtype=float32), Array(4233.61, dtype=float32)), 'eval/avg_episode_length': (Array(914.5, dtype=float32), Array(278.84537, dtype=float32)), 'eval/epoch_eval_time': 4.100675821304321, 'eval/sps': 31214.366991655155}
I0727 16:52:52.437774 140141833336640 train.py:379] starting iteration 194 1959.0807285308838
I0727 16:53:02.432553 140141833336640 train.py:394] {'eval/walltime': 809.3720874786377, 'training/sps': 41637.65988752899, 'training/walltime': 1150.2866089344025, 'training/entropy_loss': Array(-0.03150003, dtype=float32), 'training/policy_loss': Array(0.00631598, dtype=float32), 'training/total_loss': Array(21.817673, dtype=float32), 'training/v_loss': Array(21.842857, dtype=float32), 'eval/episode_goal_distance': (Array(0.38801014, dtype=float32), Array(0.16847357, dtype=float32)), 'eval/episode_reward': (Array(-8666.627, dtype=float32), Array(4346.9917, dtype=float32)), 'eval/avg_episode_length': (Array(906.91406, dtype=float32), Array(289.41666, dtype=float32)), 'eval/epoch_eval_time': 4.088618040084839, 'eval/sps': 31306.421569607905}
I0727 16:53:02.434914 140141833336640 train.py:379] starting iteration 195 1969.0778687000275
I0727 16:53:12.387167 140141833336640 train.py:394] {'eval/walltime': 813.4575445652008, 'training/sps': 41915.769848465694, 'training/walltime': 1156.1497960090637, 'training/entropy_loss': Array(-0.03246417, dtype=float32), 'training/policy_loss': Array(0.00693287, dtype=float32), 'training/total_loss': Array(22.637741, dtype=float32), 'training/v_loss': Array(22.663273, dtype=float32), 'eval/episode_goal_distance': (Array(0.39683568, dtype=float32), Array(0.15756781, dtype=float32)), 'eval/episode_reward': (Array(-8954.336, dtype=float32), Array(3951.173, dtype=float32)), 'eval/avg_episode_length': (Array(922.3281, dtype=float32), Array(266.81198, dtype=float32)), 'eval/epoch_eval_time': 4.08545708656311, 'eval/sps': 31330.643619042385}
I0727 16:53:12.389460 140141833336640 train.py:379] starting iteration 196 1979.0324144363403
I0727 16:53:22.380998 140141833336640 train.py:394] {'eval/walltime': 817.5710594654083, 'training/sps': 41835.58437548348, 'training/walltime': 1162.024220943451, 'training/entropy_loss': Array(-0.03317777, dtype=float32), 'training/policy_loss': Array(0.00817193, dtype=float32), 'training/total_loss': Array(20.79696, dtype=float32), 'training/v_loss': Array(20.821966, dtype=float32), 'eval/episode_goal_distance': (Array(0.40053058, dtype=float32), Array(0.1611009, dtype=float32)), 'eval/episode_reward': (Array(-9091.903, dtype=float32), Array(4438.3584, dtype=float32)), 'eval/avg_episode_length': (Array(906.78125, dtype=float32), Array(289.82935, dtype=float32)), 'eval/epoch_eval_time': 4.1135149002075195, 'eval/sps': 31116.940890026344}
I0727 16:53:22.383236 140141833336640 train.py:379] starting iteration 197 1989.0261907577515
I0727 16:53:32.420844 140141833336640 train.py:394] {'eval/walltime': 821.6939420700073, 'training/sps': 41574.923622032664, 'training/walltime': 1167.9354765415192, 'training/entropy_loss': Array(-0.03268295, dtype=float32), 'training/policy_loss': Array(0.04181015, dtype=float32), 'training/total_loss': Array(19.496174, dtype=float32), 'training/v_loss': Array(19.487045, dtype=float32), 'eval/episode_goal_distance': (Array(0.39709175, dtype=float32), Array(0.18856534, dtype=float32)), 'eval/episode_reward': (Array(-8777.088, dtype=float32), Array(4397.3604, dtype=float32)), 'eval/avg_episode_length': (Array(899.02344, dtype=float32), Array(300.32983, dtype=float32)), 'eval/epoch_eval_time': 4.122882604598999, 'eval/sps': 31046.239312566984}
I0727 16:53:32.423019 140141833336640 train.py:379] starting iteration 198 1999.0659725666046
I0727 16:53:42.440550 140141833336640 train.py:394] {'eval/walltime': 825.7910177707672, 'training/sps': 41535.79953577709, 'training/walltime': 1173.8523001670837, 'training/entropy_loss': Array(-0.03139209, dtype=float32), 'training/policy_loss': Array(0.00699648, dtype=float32), 'training/total_loss': Array(18.823265, dtype=float32), 'training/v_loss': Array(18.84766, dtype=float32), 'eval/episode_goal_distance': (Array(0.4028938, dtype=float32), Array(0.18041402, dtype=float32)), 'eval/episode_reward': (Array(-9242.193, dtype=float32), Array(4289.4326, dtype=float32)), 'eval/avg_episode_length': (Array(937.8594, dtype=float32), Array(240.6702, dtype=float32)), 'eval/epoch_eval_time': 4.097075700759888, 'eval/sps': 31241.795209265903}
I0727 16:53:42.442870 140141833336640 train.py:379] starting iteration 199 2009.085824251175
I0727 16:53:52.466542 140141833336640 train.py:394] {'eval/walltime': 829.9098381996155, 'training/sps': 41645.44856545932, 'training/walltime': 1179.7535452842712, 'training/entropy_loss': Array(-0.02874843, dtype=float32), 'training/policy_loss': Array(0.01244557, dtype=float32), 'training/total_loss': Array(18.898876, dtype=float32), 'training/v_loss': Array(18.915178, dtype=float32), 'eval/episode_goal_distance': (Array(0.3946216, dtype=float32), Array(0.17857614, dtype=float32)), 'eval/episode_reward': (Array(-9011.673, dtype=float32), Array(4156.7275, dtype=float32)), 'eval/avg_episode_length': (Array(930.0625, dtype=float32), Array(254.30965, dtype=float32)), 'eval/epoch_eval_time': 4.118820428848267, 'eval/sps': 31076.858583949546}
I0727 16:53:52.468784 140141833336640 train.py:379] starting iteration 200 2019.111737728119
I0727 16:54:02.479228 140141833336640 train.py:394] {'eval/walltime': 834.011923789978, 'training/sps': 41622.269417816795, 'training/walltime': 1185.658076763153, 'training/entropy_loss': Array(-0.02914298, dtype=float32), 'training/policy_loss': Array(0.00717422, dtype=float32), 'training/total_loss': Array(59.45135, dtype=float32), 'training/v_loss': Array(59.47332, dtype=float32), 'eval/episode_goal_distance': (Array(0.3927848, dtype=float32), Array(0.17482123, dtype=float32)), 'eval/episode_reward': (Array(-8166.7036, dtype=float32), Array(4860.6074, dtype=float32)), 'eval/avg_episode_length': (Array(836.875, dtype=float32), Array(368.21655, dtype=float32)), 'eval/epoch_eval_time': 4.102085590362549, 'eval/sps': 31203.63950979559}
I0727 16:54:02.481469 140141833336640 train.py:379] starting iteration 201 2029.1244230270386
I0727 16:54:12.425765 140141833336640 train.py:394] {'eval/walltime': 838.099687576294, 'training/sps': 41991.115521542546, 'training/walltime': 1191.510743379593, 'training/entropy_loss': Array(-0.02853497, dtype=float32), 'training/policy_loss': Array(0.00564464, dtype=float32), 'training/total_loss': Array(21.248898, dtype=float32), 'training/v_loss': Array(21.271788, dtype=float32), 'eval/episode_goal_distance': (Array(0.39072764, dtype=float32), Array(0.16880159, dtype=float32)), 'eval/episode_reward': (Array(-9325.272, dtype=float32), Array(4258.551, dtype=float32)), 'eval/avg_episode_length': (Array(945.64844, dtype=float32), Array(225.97295, dtype=float32)), 'eval/epoch_eval_time': 4.087763786315918, 'eval/sps': 31312.963931156974}
I0727 16:54:12.428617 140141833336640 train.py:379] starting iteration 202 2039.0715682506561
I0727 16:54:22.414255 140141833336640 train.py:394] {'eval/walltime': 842.177307844162, 'training/sps': 41624.254374426266, 'training/walltime': 1197.4149932861328, 'training/entropy_loss': Array(-0.02860113, dtype=float32), 'training/policy_loss': Array(0.00550144, dtype=float32), 'training/total_loss': Array(19.920807, dtype=float32), 'training/v_loss': Array(19.943907, dtype=float32), 'eval/episode_goal_distance': (Array(0.3906615, dtype=float32), Array(0.15344882, dtype=float32)), 'eval/episode_reward': (Array(-8343.624, dtype=float32), Array(4582.825, dtype=float32)), 'eval/avg_episode_length': (Array(867.9375, dtype=float32), Array(337.45596, dtype=float32)), 'eval/epoch_eval_time': 4.077620267868042, 'eval/sps': 31390.858292678633}
I0727 16:54:22.416809 140141833336640 train.py:379] starting iteration 203 2049.0597631931305
I0727 16:54:32.422805 140141833336640 train.py:394] {'eval/walltime': 846.2955780029297, 'training/sps': 41767.49788213104, 'training/walltime': 1203.2989943027496, 'training/entropy_loss': Array(-0.02906271, dtype=float32), 'training/policy_loss': Array(0.00624438, dtype=float32), 'training/total_loss': Array(27.272781, dtype=float32), 'training/v_loss': Array(27.2956, dtype=float32), 'eval/episode_goal_distance': (Array(0.37537032, dtype=float32), Array(0.15865342, dtype=float32)), 'eval/episode_reward': (Array(-8512.436, dtype=float32), Array(4135.149, dtype=float32)), 'eval/avg_episode_length': (Array(898.9922, dtype=float32), Array(300.42303, dtype=float32)), 'eval/epoch_eval_time': 4.1182701587677, 'eval/sps': 31081.010974350727}
I0727 16:54:32.425372 140141833336640 train.py:379] starting iteration 204 2059.0683217048645
I0727 16:54:42.397261 140141833336640 train.py:394] {'eval/walltime': 850.3882162570953, 'training/sps': 41827.303497216075, 'training/walltime': 1209.1745822429657, 'training/entropy_loss': Array(-0.02936978, dtype=float32), 'training/policy_loss': Array(0.00494336, dtype=float32), 'training/total_loss': Array(21.650963, dtype=float32), 'training/v_loss': Array(21.67539, dtype=float32), 'eval/episode_goal_distance': (Array(0.38648015, dtype=float32), Array(0.15474181, dtype=float32)), 'eval/episode_reward': (Array(-8609.303, dtype=float32), Array(4108.897, dtype=float32)), 'eval/avg_episode_length': (Array(906.8594, dtype=float32), Array(289.58688, dtype=float32)), 'eval/epoch_eval_time': 4.092638254165649, 'eval/sps': 31275.669152952018}
I0727 16:54:42.720447 140141833336640 train.py:379] starting iteration 205 2069.3633852005005
I0727 16:54:52.688784 140141833336640 train.py:394] {'eval/walltime': 854.4734709262848, 'training/sps': 41803.88423172084, 'training/walltime': 1215.0534617900848, 'training/entropy_loss': Array(-0.02839641, dtype=float32), 'training/policy_loss': Array(0.00635291, dtype=float32), 'training/total_loss': Array(19.818485, dtype=float32), 'training/v_loss': Array(19.840528, dtype=float32), 'eval/episode_goal_distance': (Array(0.38463053, dtype=float32), Array(0.1741418, dtype=float32)), 'eval/episode_reward': (Array(-8997.925, dtype=float32), Array(4604.634, dtype=float32)), 'eval/avg_episode_length': (Array(906.85156, dtype=float32), Array(289.6109, dtype=float32)), 'eval/epoch_eval_time': 4.085254669189453, 'eval/sps': 31332.195998786097}
I0727 16:54:52.691621 140141833336640 train.py:379] starting iteration 206 2079.3345725536346
I0727 16:55:02.733443 140141833336640 train.py:394] {'eval/walltime': 858.6003851890564, 'training/sps': 41576.2315989591, 'training/walltime': 1220.9645314216614, 'training/entropy_loss': Array(-0.02656318, dtype=float32), 'training/policy_loss': Array(0.00795817, dtype=float32), 'training/total_loss': Array(18.68343, dtype=float32), 'training/v_loss': Array(18.702034, dtype=float32), 'eval/episode_goal_distance': (Array(0.36661997, dtype=float32), Array(0.16062364, dtype=float32)), 'eval/episode_reward': (Array(-8217.998, dtype=float32), Array(4271.2803, dtype=float32)), 'eval/avg_episode_length': (Array(875.8125, dtype=float32), Array(328.56985, dtype=float32)), 'eval/epoch_eval_time': 4.1269142627716064, 'eval/sps': 31015.909672433103}
I0727 16:55:02.735827 140141833336640 train.py:379] starting iteration 207 2089.3787813186646
I0727 16:55:12.739786 140141833336640 train.py:394] {'eval/walltime': 862.7115671634674, 'training/sps': 41730.45680745405, 'training/walltime': 1226.853755235672, 'training/entropy_loss': Array(-0.02492654, dtype=float32), 'training/policy_loss': Array(0.00964765, dtype=float32), 'training/total_loss': Array(18.771343, dtype=float32), 'training/v_loss': Array(18.786621, dtype=float32), 'eval/episode_goal_distance': (Array(0.37695813, dtype=float32), Array(0.15329549, dtype=float32)), 'eval/episode_reward': (Array(-8260.852, dtype=float32), Array(4693.9517, dtype=float32)), 'eval/avg_episode_length': (Array(844.6406, dtype=float32), Array(361.02304, dtype=float32)), 'eval/epoch_eval_time': 4.111181974411011, 'eval/sps': 31134.59846747308}
I0727 16:55:12.742184 140141833336640 train.py:379] starting iteration 208 2099.3851380348206
I0727 16:55:22.688656 140141833336640 train.py:394] {'eval/walltime': 866.7878341674805, 'training/sps': 41891.580652331, 'training/walltime': 1232.7203278541565, 'training/entropy_loss': Array(-0.02283578, dtype=float32), 'training/policy_loss': Array(0.00507689, dtype=float32), 'training/total_loss': Array(51.775066, dtype=float32), 'training/v_loss': Array(51.792824, dtype=float32), 'eval/episode_goal_distance': (Array(0.36345407, dtype=float32), Array(0.15767452, dtype=float32)), 'eval/episode_reward': (Array(-8515.43, dtype=float32), Array(4088.5725, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.75827, dtype=float32)), 'eval/epoch_eval_time': 4.0762670040130615, 'eval/sps': 31401.279620295907}
I0727 16:55:22.691044 140141833336640 train.py:379] starting iteration 209 2109.3339977264404
I0727 16:55:32.655858 140141833336640 train.py:394] {'eval/walltime': 870.859760761261, 'training/sps': 41729.642527509364, 'training/walltime': 1238.6096665859222, 'training/entropy_loss': Array(-0.02182842, dtype=float32), 'training/policy_loss': Array(0.0147001, dtype=float32), 'training/total_loss': Array(26.544476, dtype=float32), 'training/v_loss': Array(26.551603, dtype=float32), 'eval/episode_goal_distance': (Array(0.37412345, dtype=float32), Array(0.17018324, dtype=float32)), 'eval/episode_reward': (Array(-9180.949, dtype=float32), Array(4363.1797, dtype=float32)), 'eval/avg_episode_length': (Array(937.8125, dtype=float32), Array(240.85135, dtype=float32)), 'eval/epoch_eval_time': 4.071926593780518, 'eval/sps': 31434.751352224248}
I0727 16:55:32.658274 140141833336640 train.py:379] starting iteration 210 2119.3012285232544
I0727 16:55:42.639433 140141833336640 train.py:394] {'eval/walltime': 874.974595785141, 'training/sps': 41918.03859172862, 'training/walltime': 1244.4725363254547, 'training/entropy_loss': Array(-0.02193331, dtype=float32), 'training/policy_loss': Array(0.00783658, dtype=float32), 'training/total_loss': Array(18.816341, dtype=float32), 'training/v_loss': Array(18.830437, dtype=float32), 'eval/episode_goal_distance': (Array(0.36378688, dtype=float32), Array(0.14003374, dtype=float32)), 'eval/episode_reward': (Array(-8760.03, dtype=float32), Array(3478.3066, dtype=float32)), 'eval/avg_episode_length': (Array(945.7422, dtype=float32), Array(225.58315, dtype=float32)), 'eval/epoch_eval_time': 4.114835023880005, 'eval/sps': 31106.957935656155}
I0727 16:55:42.641851 140141833336640 train.py:379] starting iteration 211 2129.284804582596
I0727 16:55:52.669391 140141833336640 train.py:394] {'eval/walltime': 879.0826959609985, 'training/sps': 41542.40160354569, 'training/walltime': 1250.3884196281433, 'training/entropy_loss': Array(-0.02118904, dtype=float32), 'training/policy_loss': Array(0.00441845, dtype=float32), 'training/total_loss': Array(23.90337, dtype=float32), 'training/v_loss': Array(23.92014, dtype=float32), 'eval/episode_goal_distance': (Array(0.35028267, dtype=float32), Array(0.15218149, dtype=float32)), 'eval/episode_reward': (Array(-7965.2656, dtype=float32), Array(4011.324, dtype=float32)), 'eval/avg_episode_length': (Array(906.89844, dtype=float32), Array(289.4648, dtype=float32)), 'eval/epoch_eval_time': 4.108100175857544, 'eval/sps': 31157.954899013796}
I0727 16:55:52.671849 140141833336640 train.py:379] starting iteration 212 2139.3148024082184
I0727 16:56:02.675980 140141833336640 train.py:394] {'eval/walltime': 883.1808576583862, 'training/sps': 41638.467217697886, 'training/walltime': 1256.290654182434, 'training/entropy_loss': Array(-0.01857776, dtype=float32), 'training/policy_loss': Array(0.01356942, dtype=float32), 'training/total_loss': Array(20.422682, dtype=float32), 'training/v_loss': Array(20.42769, dtype=float32), 'eval/episode_goal_distance': (Array(0.38103974, dtype=float32), Array(0.15566584, dtype=float32)), 'eval/episode_reward': (Array(-8826.109, dtype=float32), Array(3967.398, dtype=float32)), 'eval/avg_episode_length': (Array(930.2031, dtype=float32), Array(253.79826, dtype=float32)), 'eval/epoch_eval_time': 4.098161697387695, 'eval/sps': 31233.516257201725}
I0727 16:56:02.678478 140141833336640 train.py:379] starting iteration 213 2149.3214313983917
I0727 16:56:12.671503 140141833336640 train.py:394] {'eval/walltime': 887.2684187889099, 'training/sps': 41642.05012306071, 'training/walltime': 1262.1923809051514, 'training/entropy_loss': Array(-0.0172657, dtype=float32), 'training/policy_loss': Array(0.00705761, dtype=float32), 'training/total_loss': Array(19.190483, dtype=float32), 'training/v_loss': Array(19.200691, dtype=float32), 'eval/episode_goal_distance': (Array(0.3765605, dtype=float32), Array(0.18133695, dtype=float32)), 'eval/episode_reward': (Array(-8634.681, dtype=float32), Array(4515.592, dtype=float32)), 'eval/avg_episode_length': (Array(914.6875, dtype=float32), Array(278.23395, dtype=float32)), 'eval/epoch_eval_time': 4.087561130523682, 'eval/sps': 31314.516385862873}
I0727 16:56:12.673830 140141833336640 train.py:379] starting iteration 214 2159.3167848587036
I0727 16:56:22.626056 140141833336640 train.py:394] {'eval/walltime': 891.3455018997192, 'training/sps': 41856.16325489777, 'training/walltime': 1268.0639176368713, 'training/entropy_loss': Array(-0.01588221, dtype=float32), 'training/policy_loss': Array(0.08438174, dtype=float32), 'training/total_loss': Array(17.101578, dtype=float32), 'training/v_loss': Array(17.033077, dtype=float32), 'eval/episode_goal_distance': (Array(0.38540927, dtype=float32), Array(0.15807666, dtype=float32)), 'eval/episode_reward': (Array(-9188.539, dtype=float32), Array(3874.0757, dtype=float32)), 'eval/avg_episode_length': (Array(953.41406, dtype=float32), Array(210.0679, dtype=float32)), 'eval/epoch_eval_time': 4.077083110809326, 'eval/sps': 31394.994048721073}
I0727 16:56:22.628564 140141833336640 train.py:379] starting iteration 215 2169.271518468857
I0727 16:56:32.632471 140141833336640 train.py:394] {'eval/walltime': 895.4196441173553, 'training/sps': 41471.16877047549, 'training/walltime': 1273.9899623394012, 'training/entropy_loss': Array(-0.01562967, dtype=float32), 'training/policy_loss': Array(0.0242531, dtype=float32), 'training/total_loss': Array(15.870458, dtype=float32), 'training/v_loss': Array(15.861833, dtype=float32), 'eval/episode_goal_distance': (Array(0.36615005, dtype=float32), Array(0.19122642, dtype=float32)), 'eval/episode_reward': (Array(-8054.3823, dtype=float32), Array(4823.045, dtype=float32)), 'eval/avg_episode_length': (Array(875.7031, dtype=float32), Array(328.85907, dtype=float32)), 'eval/epoch_eval_time': 4.074142217636108, 'eval/sps': 31417.656321842376}
I0727 16:56:32.634977 140141833336640 train.py:379] starting iteration 216 2179.2779302597046
I0727 16:56:42.673330 140141833336640 train.py:394] {'eval/walltime': 899.5331521034241, 'training/sps': 41504.23271666518, 'training/walltime': 1279.9112861156464, 'training/entropy_loss': Array(-0.01229305, dtype=float32), 'training/policy_loss': Array(0.0184197, dtype=float32), 'training/total_loss': Array(44.780266, dtype=float32), 'training/v_loss': Array(44.77414, dtype=float32), 'eval/episode_goal_distance': (Array(0.36170888, dtype=float32), Array(0.15274975, dtype=float32)), 'eval/episode_reward': (Array(-8037.007, dtype=float32), Array(4280.093, dtype=float32)), 'eval/avg_episode_length': (Array(891.2969, dtype=float32), Array(310.1928, dtype=float32)), 'eval/epoch_eval_time': 4.113507986068726, 'eval/sps': 31116.993192549857}
I0727 16:56:42.675847 140141833336640 train.py:379] starting iteration 217 2189.3188004493713
I0727 16:56:52.676449 140141833336640 train.py:394] {'eval/walltime': 903.6413781642914, 'training/sps': 41734.34450303726, 'training/walltime': 1285.7999613285065, 'training/entropy_loss': Array(-0.01314583, dtype=float32), 'training/policy_loss': Array(0.00840235, dtype=float32), 'training/total_loss': Array(32.568497, dtype=float32), 'training/v_loss': Array(32.57324, dtype=float32), 'eval/episode_goal_distance': (Array(0.3516712, dtype=float32), Array(0.1327323, dtype=float32)), 'eval/episode_reward': (Array(-7886.5728, dtype=float32), Array(4225.8037, dtype=float32)), 'eval/avg_episode_length': (Array(867.89844, dtype=float32), Array(337.55573, dtype=float32)), 'eval/epoch_eval_time': 4.10822606086731, 'eval/sps': 31157.00015129577}
I0727 16:56:52.678998 140141833336640 train.py:379] starting iteration 218 2199.321952342987
I0727 16:57:02.686222 140141833336640 train.py:394] {'eval/walltime': 907.7391576766968, 'training/sps': 41613.93668295239, 'training/walltime': 1291.705675125122, 'training/entropy_loss': Array(-0.01531055, dtype=float32), 'training/policy_loss': Array(0.00553925, dtype=float32), 'training/total_loss': Array(18.651993, dtype=float32), 'training/v_loss': Array(18.661762, dtype=float32), 'eval/episode_goal_distance': (Array(0.36218566, dtype=float32), Array(0.14895459, dtype=float32)), 'eval/episode_reward': (Array(-8507.562, dtype=float32), Array(4419.168, dtype=float32)), 'eval/avg_episode_length': (Array(899.1172, dtype=float32), Array(300.0514, dtype=float32)), 'eval/epoch_eval_time': 4.0977795124053955, 'eval/sps': 31236.429293596626}
I0727 16:57:02.688783 140141833336640 train.py:379] starting iteration 219 2209.331737279892
I0727 16:57:12.630810 140141833336640 train.py:394] {'eval/walltime': 911.8167321681976, 'training/sps': 41932.858791975435, 'training/walltime': 1297.5664727687836, 'training/entropy_loss': Array(-0.01406798, dtype=float32), 'training/policy_loss': Array(0.00539396, dtype=float32), 'training/total_loss': Array(19.015388, dtype=float32), 'training/v_loss': Array(19.024063, dtype=float32), 'eval/episode_goal_distance': (Array(0.33987463, dtype=float32), Array(0.16079223, dtype=float32)), 'eval/episode_reward': (Array(-7721.757, dtype=float32), Array(4426.106, dtype=float32)), 'eval/avg_episode_length': (Array(883.4844, dtype=float32), Array(319.80026, dtype=float32)), 'eval/epoch_eval_time': 4.0775744915008545, 'eval/sps': 31391.210698124207}
I0727 16:57:12.633236 140141833336640 train.py:379] starting iteration 220 2219.2761902809143
I0727 16:57:22.597158 140141833336640 train.py:394] {'eval/walltime': 915.8921446800232, 'training/sps': 41761.380712739476, 'training/walltime': 1303.4513356685638, 'training/entropy_loss': Array(-0.01344778, dtype=float32), 'training/policy_loss': Array(0.07203535, dtype=float32), 'training/total_loss': Array(18.435837, dtype=float32), 'training/v_loss': Array(18.37725, dtype=float32), 'eval/episode_goal_distance': (Array(0.34916523, dtype=float32), Array(0.14442784, dtype=float32)), 'eval/episode_reward': (Array(-8398.959, dtype=float32), Array(4158.29, dtype=float32)), 'eval/avg_episode_length': (Array(922.3672, dtype=float32), Array(266.67816, dtype=float32)), 'eval/epoch_eval_time': 4.0754125118255615, 'eval/sps': 31407.86353003147}
I0727 16:57:22.599433 140141833336640 train.py:379] starting iteration 221 2229.2423870563507
I0727 16:57:32.664177 140141833336640 train.py:394] {'eval/walltime': 920.020378112793, 'training/sps': 41422.80589727331, 'training/walltime': 1309.3842992782593, 'training/entropy_loss': Array(-0.01370222, dtype=float32), 'training/policy_loss': Array(0.00499179, dtype=float32), 'training/total_loss': Array(15.35706, dtype=float32), 'training/v_loss': Array(15.36577, dtype=float32), 'eval/episode_goal_distance': (Array(0.3483212, dtype=float32), Array(0.175292, dtype=float32)), 'eval/episode_reward': (Array(-8410.392, dtype=float32), Array(4146.7173, dtype=float32)), 'eval/avg_episode_length': (Array(922.3906, dtype=float32), Array(266.59744, dtype=float32)), 'eval/epoch_eval_time': 4.128233432769775, 'eval/sps': 31005.998591053594}
I0727 16:57:32.666726 140141833336640 train.py:379] starting iteration 222 2239.3096799850464
I0727 16:57:42.649545 140141833336640 train.py:394] {'eval/walltime': 924.12580037117, 'training/sps': 41838.88202149093, 'training/walltime': 1315.2582612037659, 'training/entropy_loss': Array(-0.01263968, dtype=float32), 'training/policy_loss': Array(0.00509109, dtype=float32), 'training/total_loss': Array(14.715313, dtype=float32), 'training/v_loss': Array(14.72286, dtype=float32), 'eval/episode_goal_distance': (Array(0.3548991, dtype=float32), Array(0.15827763, dtype=float32)), 'eval/episode_reward': (Array(-8153.98, dtype=float32), Array(4322.955, dtype=float32)), 'eval/avg_episode_length': (Array(899.1094, dtype=float32), Array(300.0746, dtype=float32)), 'eval/epoch_eval_time': 4.105422258377075, 'eval/sps': 31178.27885762962}
I0727 16:57:42.651846 140141833336640 train.py:379] starting iteration 223 2249.294799566269
I0727 16:57:52.605377 140141833336640 train.py:394] {'eval/walltime': 928.2098398208618, 'training/sps': 41896.62400952526, 'training/walltime': 1321.124127626419, 'training/entropy_loss': Array(-0.01071124, dtype=float32), 'training/policy_loss': Array(0.00654135, dtype=float32), 'training/total_loss': Array(13.952234, dtype=float32), 'training/v_loss': Array(13.956404, dtype=float32), 'eval/episode_goal_distance': (Array(0.35459423, dtype=float32), Array(0.16755246, dtype=float32)), 'eval/episode_reward': (Array(-7769.411, dtype=float32), Array(4209.361, dtype=float32)), 'eval/avg_episode_length': (Array(891.2578, dtype=float32), Array(310.30402, dtype=float32)), 'eval/epoch_eval_time': 4.0840394496917725, 'eval/sps': 31341.5189977316}
I0727 16:57:52.607797 140141833336640 train.py:379] starting iteration 224 2259.2507507801056
I0727 16:58:02.613978 140141833336640 train.py:394] {'eval/walltime': 932.2980773448944, 'training/sps': 41552.496244102214, 'training/walltime': 1327.0385737419128, 'training/entropy_loss': Array(-0.01039195, dtype=float32), 'training/policy_loss': Array(0.00510168, dtype=float32), 'training/total_loss': Array(15.444841, dtype=float32), 'training/v_loss': Array(15.450131, dtype=float32), 'eval/episode_goal_distance': (Array(0.36928546, dtype=float32), Array(0.16488044, dtype=float32)), 'eval/episode_reward': (Array(-8289.41, dtype=float32), Array(4940.193, dtype=float32)), 'eval/avg_episode_length': (Array(868.02344, dtype=float32), Array(337.23666, dtype=float32)), 'eval/epoch_eval_time': 4.088237524032593, 'eval/sps': 31309.33544040812}
I0727 16:58:02.616441 140141833336640 train.py:379] starting iteration 225 2269.259395837784
I0727 16:58:12.617773 140141833336640 train.py:394] {'eval/walltime': 936.3858640193939, 'training/sps': 41584.11809592183, 'training/walltime': 1332.9485223293304, 'training/entropy_loss': Array(-0.00886006, dtype=float32), 'training/policy_loss': Array(0.00640381, dtype=float32), 'training/total_loss': Array(57.97901, dtype=float32), 'training/v_loss': Array(57.98146, dtype=float32), 'eval/episode_goal_distance': (Array(0.33545512, dtype=float32), Array(0.14735922, dtype=float32)), 'eval/episode_reward': (Array(-8110.648, dtype=float32), Array(4225.5107, dtype=float32)), 'eval/avg_episode_length': (Array(899.10156, dtype=float32), Array(300.0977, dtype=float32)), 'eval/epoch_eval_time': 4.087786674499512, 'eval/sps': 31312.788604770252}
I0727 16:58:12.620098 140141833336640 train.py:379] starting iteration 226 2279.2630519866943
I0727 16:58:22.607077 140141833336640 train.py:394] {'eval/walltime': 940.4634852409363, 'training/sps': 41613.50493019632, 'training/walltime': 1338.8542973995209, 'training/entropy_loss': Array(-0.00978687, dtype=float32), 'training/policy_loss': Array(0.00784265, dtype=float32), 'training/total_loss': Array(20.449453, dtype=float32), 'training/v_loss': Array(20.451397, dtype=float32), 'eval/episode_goal_distance': (Array(0.37145084, dtype=float32), Array(0.1560093, dtype=float32)), 'eval/episode_reward': (Array(-9065.295, dtype=float32), Array(3826.8096, dtype=float32)), 'eval/avg_episode_length': (Array(976.6953, dtype=float32), Array(150.43134, dtype=float32)), 'eval/epoch_eval_time': 4.077621221542358, 'eval/sps': 31390.850950982654}
I0727 16:58:22.609441 140141833336640 train.py:379] starting iteration 227 2289.2523958683014
I0727 16:58:32.663101 140141833336640 train.py:394] {'eval/walltime': 944.5859777927399, 'training/sps': 41459.76616562096, 'training/walltime': 1344.7819719314575, 'training/entropy_loss': Array(-0.01066382, dtype=float32), 'training/policy_loss': Array(0.0072771, dtype=float32), 'training/total_loss': Array(18.041578, dtype=float32), 'training/v_loss': Array(18.044964, dtype=float32), 'eval/episode_goal_distance': (Array(0.37553963, dtype=float32), Array(0.16181841, dtype=float32)), 'eval/episode_reward': (Array(-8915.97, dtype=float32), Array(4730.241, dtype=float32)), 'eval/avg_episode_length': (Array(883.5625, dtype=float32), Array(319.5853, dtype=float32)), 'eval/epoch_eval_time': 4.122492551803589, 'eval/sps': 31049.17677631705}
I0727 16:58:32.665437 140141833336640 train.py:379] starting iteration 228 2299.308390378952
I0727 16:58:42.667988 140141833336640 train.py:394] {'eval/walltime': 948.6972894668579, 'training/sps': 41742.63087080578, 'training/walltime': 1350.6694781780243, 'training/entropy_loss': Array(-0.0109528, dtype=float32), 'training/policy_loss': Array(0.00506728, dtype=float32), 'training/total_loss': Array(23.21498, dtype=float32), 'training/v_loss': Array(23.220865, dtype=float32), 'eval/episode_goal_distance': (Array(0.3561446, dtype=float32), Array(0.16019262, dtype=float32)), 'eval/episode_reward': (Array(-7804.339, dtype=float32), Array(4138.3457, dtype=float32)), 'eval/avg_episode_length': (Array(875.71094, dtype=float32), Array(328.83878, dtype=float32)), 'eval/epoch_eval_time': 4.111311674118042, 'eval/sps': 31133.616263101372}
I0727 16:58:42.670581 140141833336640 train.py:379] starting iteration 229 2309.313534975052
I0727 16:58:52.675316 140141833336640 train.py:394] {'eval/walltime': 952.7968428134918, 'training/sps': 41642.13423622743, 'training/walltime': 1356.5711929798126, 'training/entropy_loss': Array(-0.01288301, dtype=float32), 'training/policy_loss': Array(0.00388926, dtype=float32), 'training/total_loss': Array(17.741196, dtype=float32), 'training/v_loss': Array(17.750187, dtype=float32), 'eval/episode_goal_distance': (Array(0.35424918, dtype=float32), Array(0.1731944, dtype=float32)), 'eval/episode_reward': (Array(-8282.504, dtype=float32), Array(3815.1108, dtype=float32)), 'eval/avg_episode_length': (Array(945.60156, dtype=float32), Array(226.16768, dtype=float32)), 'eval/epoch_eval_time': 4.099553346633911, 'eval/sps': 31222.913614503665}
I0727 16:58:52.677651 140141833336640 train.py:379] starting iteration 230 2319.3206055164337
I0727 16:59:02.654089 140141833336640 train.py:394] {'eval/walltime': 956.8814568519592, 'training/sps': 41737.590717696454, 'training/walltime': 1362.4594101905823, 'training/entropy_loss': Array(-0.01436825, dtype=float32), 'training/policy_loss': Array(0.00547022, dtype=float32), 'training/total_loss': Array(16.132145, dtype=float32), 'training/v_loss': Array(16.141043, dtype=float32), 'eval/episode_goal_distance': (Array(0.37260923, dtype=float32), Array(0.17790538, dtype=float32)), 'eval/episode_reward': (Array(-8886.118, dtype=float32), Array(4458.0596, dtype=float32)), 'eval/avg_episode_length': (Array(922.3203, dtype=float32), Array(266.8391, dtype=float32)), 'eval/epoch_eval_time': 4.084614038467407, 'eval/sps': 31337.11013930389}
I0727 16:59:02.656615 140141833336640 train.py:379] starting iteration 231 2329.2995693683624
I0727 16:59:12.609719 140141833336640 train.py:394] {'eval/walltime': 960.9620862007141, 'training/sps': 41876.01384112734, 'training/walltime': 1368.3281636238098, 'training/entropy_loss': Array(-0.01481211, dtype=float32), 'training/policy_loss': Array(0.00596362, dtype=float32), 'training/total_loss': Array(15.170613, dtype=float32), 'training/v_loss': Array(15.1794615, dtype=float32), 'eval/episode_goal_distance': (Array(0.37345517, dtype=float32), Array(0.16355258, dtype=float32)), 'eval/episode_reward': (Array(-8425.082, dtype=float32), Array(4377.8887, dtype=float32)), 'eval/avg_episode_length': (Array(899.0703, dtype=float32), Array(300.19077, dtype=float32)), 'eval/epoch_eval_time': 4.080629348754883, 'eval/sps': 31367.71048295687}
I0727 16:59:12.612169 140141833336640 train.py:379] starting iteration 232 2339.2551231384277
I0727 16:59:22.606549 140141833336640 train.py:394] {'eval/walltime': 965.0446128845215, 'training/sps': 41595.81245617637, 'training/walltime': 1374.2364506721497, 'training/entropy_loss': Array(-0.01581077, dtype=float32), 'training/policy_loss': Array(0.00591582, dtype=float32), 'training/total_loss': Array(16.228157, dtype=float32), 'training/v_loss': Array(16.23805, dtype=float32), 'eval/episode_goal_distance': (Array(0.37288594, dtype=float32), Array(0.1947958, dtype=float32)), 'eval/episode_reward': (Array(-8479.979, dtype=float32), Array(4555.5537, dtype=float32)), 'eval/avg_episode_length': (Array(914.60156, dtype=float32), Array(278.51392, dtype=float32)), 'eval/epoch_eval_time': 4.082526683807373, 'eval/sps': 31353.132487214247}
I0727 16:59:22.608821 140141833336640 train.py:379] starting iteration 233 2349.251775741577
I0727 16:59:32.612185 140141833336640 train.py:394] {'eval/walltime': 969.15731549263, 'training/sps': 41744.93838929889, 'training/walltime': 1380.123631477356, 'training/entropy_loss': Array(-0.01387859, dtype=float32), 'training/policy_loss': Array(0.00843188, dtype=float32), 'training/total_loss': Array(50.469933, dtype=float32), 'training/v_loss': Array(50.47538, dtype=float32), 'eval/episode_goal_distance': (Array(0.37592363, dtype=float32), Array(0.16974847, dtype=float32)), 'eval/episode_reward': (Array(-8800.297, dtype=float32), Array(4472.3716, dtype=float32)), 'eval/avg_episode_length': (Array(914.58594, dtype=float32), Array(278.56488, dtype=float32)), 'eval/epoch_eval_time': 4.1127026081085205, 'eval/sps': 31123.086738058282}
I0727 16:59:32.614542 140141833336640 train.py:379] starting iteration 234 2359.25749540329
I0727 16:59:42.656303 140141833336640 train.py:394] {'eval/walltime': 973.2667717933655, 'training/sps': 41453.18866805644, 'training/walltime': 1386.0522465705872, 'training/entropy_loss': Array(-0.01557564, dtype=float32), 'training/policy_loss': Array(0.00924415, dtype=float32), 'training/total_loss': Array(23.087826, dtype=float32), 'training/v_loss': Array(23.094158, dtype=float32), 'eval/episode_goal_distance': (Array(0.38432673, dtype=float32), Array(0.16135159, dtype=float32)), 'eval/episode_reward': (Array(-8737.982, dtype=float32), Array(4692.2935, dtype=float32)), 'eval/avg_episode_length': (Array(891.3125, dtype=float32), Array(310.14822, dtype=float32)), 'eval/epoch_eval_time': 4.109456300735474, 'eval/sps': 31147.672741304417}
I0727 16:59:42.658823 140141833336640 train.py:379] starting iteration 235 2369.301777124405
I0727 16:59:52.694136 140141833336640 train.py:394] {'eval/walltime': 977.3825173377991, 'training/sps': 41540.36083163888, 'training/walltime': 1391.9684205055237, 'training/entropy_loss': Array(-0.01595667, dtype=float32), 'training/policy_loss': Array(0.00766746, dtype=float32), 'training/total_loss': Array(16.058193, dtype=float32), 'training/v_loss': Array(16.066483, dtype=float32), 'eval/episode_goal_distance': (Array(0.37656146, dtype=float32), Array(0.15942848, dtype=float32)), 'eval/episode_reward': (Array(-8708.241, dtype=float32), Array(4244.5537, dtype=float32)), 'eval/avg_episode_length': (Array(906.78906, dtype=float32), Array(289.80527, dtype=float32)), 'eval/epoch_eval_time': 4.115745544433594, 'eval/sps': 31100.076187439638}
I0727 16:59:52.696455 140141833336640 train.py:379] starting iteration 236 2379.3394095897675
I0727 17:00:02.708321 140141833336640 train.py:394] {'eval/walltime': 981.4906802177429, 'training/sps': 41653.5852093248, 'training/walltime': 1397.8685128688812, 'training/entropy_loss': Array(-0.01574862, dtype=float32), 'training/policy_loss': Array(0.00530568, dtype=float32), 'training/total_loss': Array(20.08141, dtype=float32), 'training/v_loss': Array(20.091852, dtype=float32), 'eval/episode_goal_distance': (Array(0.34042785, dtype=float32), Array(0.14506103, dtype=float32)), 'eval/episode_reward': (Array(-8314.692, dtype=float32), Array(3517.3242, dtype=float32)), 'eval/avg_episode_length': (Array(961.1719, dtype=float32), Array(192.582, dtype=float32)), 'eval/epoch_eval_time': 4.108162879943848, 'eval/sps': 31157.479326075205}
I0727 17:00:02.710840 140141833336640 train.py:379] starting iteration 237 2389.353793859482
I0727 17:00:12.733689 140141833336640 train.py:394] {'eval/walltime': 985.6016321182251, 'training/sps': 41596.52919920624, 'training/walltime': 1403.7766981124878, 'training/entropy_loss': Array(-0.01662301, dtype=float32), 'training/policy_loss': Array(0.00554477, dtype=float32), 'training/total_loss': Array(17.515968, dtype=float32), 'training/v_loss': Array(17.527048, dtype=float32), 'eval/episode_goal_distance': (Array(0.34613115, dtype=float32), Array(0.14189984, dtype=float32)), 'eval/episode_reward': (Array(-8093.9043, dtype=float32), Array(4061.465, dtype=float32)), 'eval/avg_episode_length': (Array(906.85156, dtype=float32), Array(289.61072, dtype=float32)), 'eval/epoch_eval_time': 4.110951900482178, 'eval/sps': 31136.340949400732}
I0727 17:00:12.736794 140141833336640 train.py:379] starting iteration 238 2399.3797419071198
I0727 17:00:22.714777 140141833336640 train.py:394] {'eval/walltime': 989.6867077350616, 'training/sps': 41730.02939098955, 'training/walltime': 1409.665982246399, 'training/entropy_loss': Array(-0.01796988, dtype=float32), 'training/policy_loss': Array(0.00552631, dtype=float32), 'training/total_loss': Array(16.567122, dtype=float32), 'training/v_loss': Array(16.579567, dtype=float32), 'eval/episode_goal_distance': (Array(0.36370325, dtype=float32), Array(0.12688488, dtype=float32)), 'eval/episode_reward': (Array(-8601.756, dtype=float32), Array(3468.907, dtype=float32)), 'eval/avg_episode_length': (Array(937.8281, dtype=float32), Array(240.79088, dtype=float32)), 'eval/epoch_eval_time': 4.085075616836548, 'eval/sps': 31333.56931569415}
I0727 17:00:22.717135 140141833336640 train.py:379] starting iteration 239 2409.3600895404816
I0727 17:00:32.680967 140141833336640 train.py:394] {'eval/walltime': 993.764475107193, 'training/sps': 41780.804469818104, 'training/walltime': 1415.548109292984, 'training/entropy_loss': Array(-0.01839024, dtype=float32), 'training/policy_loss': Array(0.00672302, dtype=float32), 'training/total_loss': Array(14.866634, dtype=float32), 'training/v_loss': Array(14.878303, dtype=float32), 'eval/episode_goal_distance': (Array(0.33663166, dtype=float32), Array(0.15394826, dtype=float32)), 'eval/episode_reward': (Array(-7840.7456, dtype=float32), Array(3928.132, dtype=float32)), 'eval/avg_episode_length': (Array(906.8594, dtype=float32), Array(289.58633, dtype=float32)), 'eval/epoch_eval_time': 4.077767372131348, 'eval/sps': 31389.725876662156}
I0727 17:00:32.683383 140141833336640 train.py:379] starting iteration 240 2419.3263375759125
I0727 17:00:42.709626 140141833336640 train.py:394] {'eval/walltime': 997.872531414032, 'training/sps': 41553.24667203649, 'training/walltime': 1421.4624485969543, 'training/entropy_loss': Array(-0.01881192, dtype=float32), 'training/policy_loss': Array(0.0054877, dtype=float32), 'training/total_loss': Array(15.987156, dtype=float32), 'training/v_loss': Array(16.00048, dtype=float32), 'eval/episode_goal_distance': (Array(0.35569876, dtype=float32), Array(0.14203332, dtype=float32)), 'eval/episode_reward': (Array(-7941.9683, dtype=float32), Array(3601.0583, dtype=float32)), 'eval/avg_episode_length': (Array(914.60156, dtype=float32), Array(278.51385, dtype=float32)), 'eval/epoch_eval_time': 4.108056306838989, 'eval/sps': 31158.287627876183}
I0727 17:00:42.712042 140141833336640 train.py:379] starting iteration 241 2429.354996442795
I0727 17:00:52.712832 140141833336640 train.py:394] {'eval/walltime': 1001.9799916744232, 'training/sps': 41728.93639345165, 'training/walltime': 1427.3518869876862, 'training/entropy_loss': Array(-0.02024347, dtype=float32), 'training/policy_loss': Array(0.00667202, dtype=float32), 'training/total_loss': Array(40.3267, dtype=float32), 'training/v_loss': Array(40.340267, dtype=float32), 'eval/episode_goal_distance': (Array(0.37544507, dtype=float32), Array(0.17630638, dtype=float32)), 'eval/episode_reward': (Array(-8326.994, dtype=float32), Array(4339.5947, dtype=float32)), 'eval/avg_episode_length': (Array(891.2344, dtype=float32), Array(310.37128, dtype=float32)), 'eval/epoch_eval_time': 4.107460260391235, 'eval/sps': 31162.80910476977}
I0727 17:00:52.715303 140141833336640 train.py:379] starting iteration 242 2439.358257293701
I0727 17:01:02.756149 140141833336640 train.py:394] {'eval/walltime': 1006.106657743454, 'training/sps': 41581.57503637508, 'training/walltime': 1433.2621970176697, 'training/entropy_loss': Array(-0.02380063, dtype=float32), 'training/policy_loss': Array(0.0082691, dtype=float32), 'training/total_loss': Array(27.151882, dtype=float32), 'training/v_loss': Array(27.167412, dtype=float32), 'eval/episode_goal_distance': (Array(0.37768355, dtype=float32), Array(0.14956985, dtype=float32)), 'eval/episode_reward': (Array(-8660.715, dtype=float32), Array(3621.5032, dtype=float32)), 'eval/avg_episode_length': (Array(930.0547, dtype=float32), Array(254.33803, dtype=float32)), 'eval/epoch_eval_time': 4.126666069030762, 'eval/sps': 31017.77508982296}
I0727 17:01:02.758731 140141833336640 train.py:379] starting iteration 243 2449.401684999466
I0727 17:01:12.788047 140141833336640 train.py:394] {'eval/walltime': 1010.2247724533081, 'training/sps': 41603.45786939756, 'training/walltime': 1439.1693983078003, 'training/entropy_loss': Array(-0.02553122, dtype=float32), 'training/policy_loss': Array(0.00802484, dtype=float32), 'training/total_loss': Array(17.783257, dtype=float32), 'training/v_loss': Array(17.800762, dtype=float32), 'eval/episode_goal_distance': (Array(0.4111446, dtype=float32), Array(0.19430453, dtype=float32)), 'eval/episode_reward': (Array(-8807.027, dtype=float32), Array(4792.2095, dtype=float32)), 'eval/avg_episode_length': (Array(891.3281, dtype=float32), Array(310.1033, dtype=float32)), 'eval/epoch_eval_time': 4.118114709854126, 'eval/sps': 31082.184207669652}
I0727 17:01:12.790451 140141833336640 train.py:379] starting iteration 244 2459.433405160904
I0727 17:01:22.831692 140141833336640 train.py:394] {'eval/walltime': 1014.3112406730652, 'training/sps': 41297.84188046515, 'training/walltime': 1445.1203145980835, 'training/entropy_loss': Array(-0.02539109, dtype=float32), 'training/policy_loss': Array(0.00632147, dtype=float32), 'training/total_loss': Array(19.268085, dtype=float32), 'training/v_loss': Array(19.287153, dtype=float32), 'eval/episode_goal_distance': (Array(0.38835478, dtype=float32), Array(0.20157637, dtype=float32)), 'eval/episode_reward': (Array(-8371.474, dtype=float32), Array(4720.9487, dtype=float32)), 'eval/avg_episode_length': (Array(868.0469, dtype=float32), Array(337.17657, dtype=float32)), 'eval/epoch_eval_time': 4.08646821975708, 'eval/sps': 31322.891337108933}
I0727 17:01:22.834014 140141833336640 train.py:379] starting iteration 245 2469.47696852684
I0727 17:01:32.855963 140141833336640 train.py:394] {'eval/walltime': 1018.4257423877716, 'training/sps': 41629.64715132189, 'training/walltime': 1451.0237996578217, 'training/entropy_loss': Array(-0.0246914, dtype=float32), 'training/policy_loss': Array(0.00827046, dtype=float32), 'training/total_loss': Array(20.010014, dtype=float32), 'training/v_loss': Array(20.026436, dtype=float32), 'eval/episode_goal_distance': (Array(0.38530284, dtype=float32), Array(0.17012419, dtype=float32)), 'eval/episode_reward': (Array(-9014.854, dtype=float32), Array(3491.7568, dtype=float32)), 'eval/avg_episode_length': (Array(961.1875, dtype=float32), Array(192.50397, dtype=float32)), 'eval/epoch_eval_time': 4.114501714706421, 'eval/sps': 31109.47786034234}
I0727 17:01:32.858465 140141833336640 train.py:379] starting iteration 246 2479.5014193058014
I0727 17:01:42.896482 140141833336640 train.py:394] {'eval/walltime': 1022.5415334701538, 'training/sps': 41525.79999470651, 'training/walltime': 1456.942048072815, 'training/entropy_loss': Array(-0.02573137, dtype=float32), 'training/policy_loss': Array(0.00865075, dtype=float32), 'training/total_loss': Array(18.536865, dtype=float32), 'training/v_loss': Array(18.553946, dtype=float32), 'eval/episode_goal_distance': (Array(0.3786441, dtype=float32), Array(0.1473631, dtype=float32)), 'eval/episode_reward': (Array(-8654.627, dtype=float32), Array(4169.705, dtype=float32)), 'eval/avg_episode_length': (Array(906.84375, dtype=float32), Array(289.63498, dtype=float32)), 'eval/epoch_eval_time': 4.115791082382202, 'eval/sps': 31099.732089879097}
I0727 17:01:42.899004 140141833336640 train.py:379] starting iteration 247 2489.541958093643
I0727 17:01:52.925827 140141833336640 train.py:394] {'eval/walltime': 1026.661215543747, 'training/sps': 41633.52113016241, 'training/walltime': 1462.8449838161469, 'training/entropy_loss': Array(-0.02572593, dtype=float32), 'training/policy_loss': Array(0.01024866, dtype=float32), 'training/total_loss': Array(16.25761, dtype=float32), 'training/v_loss': Array(16.273087, dtype=float32), 'eval/episode_goal_distance': (Array(0.43089893, dtype=float32), Array(0.18274507, dtype=float32)), 'eval/episode_reward': (Array(-9576.965, dtype=float32), Array(3961.8826, dtype=float32)), 'eval/avg_episode_length': (Array(953.375, dtype=float32), Array(210.24385, dtype=float32)), 'eval/epoch_eval_time': 4.11968207359314, 'eval/sps': 31070.35875910683}
I0727 17:01:52.928549 140141833336640 train.py:379] starting iteration 248 2499.571501016617
I0727 17:02:02.983086 140141833336640 train.py:394] {'eval/walltime': 1030.7862031459808, 'training/sps': 41475.57403005747, 'training/walltime': 1468.770399093628, 'training/entropy_loss': Array(-0.0250455, dtype=float32), 'training/policy_loss': Array(0.00755476, dtype=float32), 'training/total_loss': Array(16.40861, dtype=float32), 'training/v_loss': Array(16.4261, dtype=float32), 'eval/episode_goal_distance': (Array(0.3769607, dtype=float32), Array(0.15283789, dtype=float32)), 'eval/episode_reward': (Array(-8319.575, dtype=float32), Array(3615.6387, dtype=float32)), 'eval/avg_episode_length': (Array(914.5156, dtype=float32), Array(278.7942, dtype=float32)), 'eval/epoch_eval_time': 4.124987602233887, 'eval/sps': 31030.396292750458}
I0727 17:02:02.985521 140141833336640 train.py:379] starting iteration 249 2509.6284744739532
I0727 17:02:12.999967 140141833336640 train.py:394] {'eval/walltime': 1034.901311635971, 'training/sps': 41685.765753087115, 'training/walltime': 1474.6659367084503, 'training/entropy_loss': Array(-0.02600575, dtype=float32), 'training/policy_loss': Array(0.0062878, dtype=float32), 'training/total_loss': Array(16.183292, dtype=float32), 'training/v_loss': Array(16.20301, dtype=float32), 'eval/episode_goal_distance': (Array(0.37675261, dtype=float32), Array(0.16889575, dtype=float32)), 'eval/episode_reward': (Array(-8378.639, dtype=float32), Array(4087.0996, dtype=float32)), 'eval/avg_episode_length': (Array(899.0703, dtype=float32), Array(300.19043, dtype=float32)), 'eval/epoch_eval_time': 4.115108489990234, 'eval/sps': 31104.89074865284}
I0727 17:02:13.002516 140141833336640 train.py:379] starting iteration 250 2519.6454708576202
I0727 17:02:23.039670 140141833336640 train.py:394] {'eval/walltime': 1039.0186569690704, 'training/sps': 41540.76428253722, 'training/walltime': 1480.5820531845093, 'training/entropy_loss': Array(-0.02693145, dtype=float32), 'training/policy_loss': Array(0.00544945, dtype=float32), 'training/total_loss': Array(50.782776, dtype=float32), 'training/v_loss': Array(50.804256, dtype=float32), 'eval/episode_goal_distance': (Array(0.37403843, dtype=float32), Array(0.17093891, dtype=float32)), 'eval/episode_reward': (Array(-8863.436, dtype=float32), Array(4463.7134, dtype=float32)), 'eval/avg_episode_length': (Array(922.39844, dtype=float32), Array(266.57062, dtype=float32)), 'eval/epoch_eval_time': 4.117345333099365, 'eval/sps': 31087.992297125817}
I0727 17:02:23.042133 140141833336640 train.py:379] starting iteration 251 2529.6850876808167
I0727 17:02:33.075305 140141833336640 train.py:394] {'eval/walltime': 1043.14124584198, 'training/sps': 41605.62575632262, 'training/walltime': 1486.4889466762543, 'training/entropy_loss': Array(-0.0267886, dtype=float32), 'training/policy_loss': Array(0.00563041, dtype=float32), 'training/total_loss': Array(20.281883, dtype=float32), 'training/v_loss': Array(20.303041, dtype=float32), 'eval/episode_goal_distance': (Array(0.3853793, dtype=float32), Array(0.15474524, dtype=float32)), 'eval/episode_reward': (Array(-8930.059, dtype=float32), Array(3716.5078, dtype=float32)), 'eval/avg_episode_length': (Array(945.60156, dtype=float32), Array(226.16757, dtype=float32)), 'eval/epoch_eval_time': 4.122588872909546, 'eval/sps': 31048.451336274797}
I0727 17:02:33.077672 140141833336640 train.py:379] starting iteration 252 2539.7206258773804
I0727 17:02:43.060876 140141833336640 train.py:394] {'eval/walltime': 1047.237310886383, 'training/sps': 41770.31593641268, 'training/walltime': 1492.372550725937, 'training/entropy_loss': Array(-0.02800456, dtype=float32), 'training/policy_loss': Array(0.00434095, dtype=float32), 'training/total_loss': Array(16.012657, dtype=float32), 'training/v_loss': Array(16.03632, dtype=float32), 'eval/episode_goal_distance': (Array(0.37320387, dtype=float32), Array(0.14187045, dtype=float32)), 'eval/episode_reward': (Array(-8691.086, dtype=float32), Array(3170.2297, dtype=float32)), 'eval/avg_episode_length': (Array(961.1875, dtype=float32), Array(192.50381, dtype=float32)), 'eval/epoch_eval_time': 4.096065044403076, 'eval/sps': 31249.503758467188}
I0727 17:02:43.063254 140141833336640 train.py:379] starting iteration 253 2549.706207752228
I0727 17:02:53.057676 140141833336640 train.py:394] {'eval/walltime': 1051.3212094306946, 'training/sps': 41605.18577958795, 'training/walltime': 1498.2795066833496, 'training/entropy_loss': Array(-0.02909491, dtype=float32), 'training/policy_loss': Array(0.00461638, dtype=float32), 'training/total_loss': Array(19.539799, dtype=float32), 'training/v_loss': Array(19.564278, dtype=float32), 'eval/episode_goal_distance': (Array(0.40696722, dtype=float32), Array(0.16852164, dtype=float32)), 'eval/episode_reward': (Array(-8348.185, dtype=float32), Array(4382.244, dtype=float32)), 'eval/avg_episode_length': (Array(883.46875, dtype=float32), Array(319.8429, dtype=float32)), 'eval/epoch_eval_time': 4.083898544311523, 'eval/sps': 31342.60036363823}
I0727 17:02:53.060378 140141833336640 train.py:379] starting iteration 254 2559.7033319473267
I0727 17:03:03.072959 140141833336640 train.py:394] {'eval/walltime': 1055.4102115631104, 'training/sps': 41513.19866544601, 'training/walltime': 1504.1995515823364, 'training/entropy_loss': Array(-0.02923678, dtype=float32), 'training/policy_loss': Array(0.00332571, dtype=float32), 'training/total_loss': Array(16.317028, dtype=float32), 'training/v_loss': Array(16.342937, dtype=float32), 'eval/episode_goal_distance': (Array(0.37404874, dtype=float32), Array(0.15756261, dtype=float32)), 'eval/episode_reward': (Array(-8435.801, dtype=float32), Array(3558.89, dtype=float32)), 'eval/avg_episode_length': (Array(937.8672, dtype=float32), Array(240.63988, dtype=float32)), 'eval/epoch_eval_time': 4.0890021324157715, 'eval/sps': 31303.480862793764}
I0727 17:03:03.075335 140141833336640 train.py:379] starting iteration 255 2569.718288421631
I0727 17:03:13.067376 140141833336640 train.py:394] {'eval/walltime': 1059.488070011139, 'training/sps': 41578.88134289999, 'training/walltime': 1510.110244512558, 'training/entropy_loss': Array(-0.02949927, dtype=float32), 'training/policy_loss': Array(0.0220326, dtype=float32), 'training/total_loss': Array(15.796864, dtype=float32), 'training/v_loss': Array(15.804329, dtype=float32), 'eval/episode_goal_distance': (Array(0.37635124, dtype=float32), Array(0.13390668, dtype=float32)), 'eval/episode_reward': (Array(-8190.028, dtype=float32), Array(4278.791, dtype=float32)), 'eval/avg_episode_length': (Array(860.1875, dtype=float32), Array(345.6263, dtype=float32)), 'eval/epoch_eval_time': 4.0778584480285645, 'eval/sps': 31389.024810775725}
I0727 17:03:13.192239 140141833336640 train.py:379] starting iteration 256 2579.835181951523
I0727 17:03:23.174376 140141833336640 train.py:394] {'eval/walltime': 1063.5865931510925, 'training/sps': 41797.9631747733, 'training/walltime': 1515.989956855774, 'training/entropy_loss': Array(-0.02807607, dtype=float32), 'training/policy_loss': Array(0.00609319, dtype=float32), 'training/total_loss': Array(15.732244, dtype=float32), 'training/v_loss': Array(15.754227, dtype=float32), 'eval/episode_goal_distance': (Array(0.4048134, dtype=float32), Array(0.16690332, dtype=float32)), 'eval/episode_reward': (Array(-9309.643, dtype=float32), Array(3677.474, dtype=float32)), 'eval/avg_episode_length': (Array(961.21875, dtype=float32), Array(192.34897, dtype=float32)), 'eval/epoch_eval_time': 4.098523139953613, 'eval/sps': 31230.76182057342}
I0727 17:03:23.177070 140141833336640 train.py:379] starting iteration 257 2589.820024728775
I0727 17:03:33.158115 140141833336640 train.py:394] {'eval/walltime': 1067.6638765335083, 'training/sps': 41654.893090617785, 'training/walltime': 1521.8898639678955, 'training/entropy_loss': Array(-0.02944547, dtype=float32), 'training/policy_loss': Array(0.00430423, dtype=float32), 'training/total_loss': Array(16.141714, dtype=float32), 'training/v_loss': Array(16.166855, dtype=float32), 'eval/episode_goal_distance': (Array(0.40290803, dtype=float32), Array(0.17029694, dtype=float32)), 'eval/episode_reward': (Array(-9016.079, dtype=float32), Array(4025.9053, dtype=float32)), 'eval/avg_episode_length': (Array(922.33594, dtype=float32), Array(266.78516, dtype=float32)), 'eval/epoch_eval_time': 4.0772833824157715, 'eval/sps': 31393.451961673705}
I0727 17:03:33.160361 140141833336640 train.py:379] starting iteration 258 2599.80331492424
I0727 17:03:43.164853 140141833336640 train.py:394] {'eval/walltime': 1071.7543280124664, 'training/sps': 41581.63709942879, 'training/walltime': 1527.8001651763916, 'training/entropy_loss': Array(-0.03065263, dtype=float32), 'training/policy_loss': Array(0.00245149, dtype=float32), 'training/total_loss': Array(49.252213, dtype=float32), 'training/v_loss': Array(49.28042, dtype=float32), 'eval/episode_goal_distance': (Array(0.3656, dtype=float32), Array(0.15277915, dtype=float32)), 'eval/episode_reward': (Array(-8181.005, dtype=float32), Array(3372.0403, dtype=float32)), 'eval/avg_episode_length': (Array(945.5625, dtype=float32), Array(226.32999, dtype=float32)), 'eval/epoch_eval_time': 4.09045147895813, 'eval/sps': 31292.389277430717}
I0727 17:03:43.167290 140141833336640 train.py:379] starting iteration 259 2609.810244321823
I0727 17:03:53.145790 140141833336640 train.py:394] {'eval/walltime': 1075.8319191932678, 'training/sps': 41673.81690072116, 'training/walltime': 1533.6973931789398, 'training/entropy_loss': Array(-0.02977486, dtype=float32), 'training/policy_loss': Array(0.00284505, dtype=float32), 'training/total_loss': Array(20.869305, dtype=float32), 'training/v_loss': Array(20.896233, dtype=float32), 'eval/episode_goal_distance': (Array(0.37587616, dtype=float32), Array(0.13612968, dtype=float32)), 'eval/episode_reward': (Array(-8294.742, dtype=float32), Array(3869.7695, dtype=float32)), 'eval/avg_episode_length': (Array(883.5, dtype=float32), Array(319.75717, dtype=float32)), 'eval/epoch_eval_time': 4.077591180801392, 'eval/sps': 31391.082216055667}
I0727 17:03:53.148388 140141833336640 train.py:379] starting iteration 260 2619.7913422584534
I0727 17:04:03.122539 140141833336640 train.py:394] {'eval/walltime': 1079.9251334667206, 'training/sps': 41815.81110695303, 'training/walltime': 1539.5745959281921, 'training/entropy_loss': Array(-0.03000736, dtype=float32), 'training/policy_loss': Array(0.0016017, dtype=float32), 'training/total_loss': Array(16.738941, dtype=float32), 'training/v_loss': Array(16.767345, dtype=float32), 'eval/episode_goal_distance': (Array(0.38754857, dtype=float32), Array(0.14331707, dtype=float32)), 'eval/episode_reward': (Array(-8801.418, dtype=float32), Array(3799.7676, dtype=float32)), 'eval/avg_episode_length': (Array(922.3047, dtype=float32), Array(266.89267, dtype=float32)), 'eval/epoch_eval_time': 4.093214273452759, 'eval/sps': 31271.26787135623}
I0727 17:04:03.125178 140141833336640 train.py:379] starting iteration 261 2629.768131494522
I0727 17:04:13.140174 140141833336640 train.py:394] {'eval/walltime': 1084.012622833252, 'training/sps': 41486.28404789924, 'training/walltime': 1545.4984815120697, 'training/entropy_loss': Array(-0.03100989, dtype=float32), 'training/policy_loss': Array(0.0030608, dtype=float32), 'training/total_loss': Array(18.512989, dtype=float32), 'training/v_loss': Array(18.54094, dtype=float32), 'eval/episode_goal_distance': (Array(0.3949229, dtype=float32), Array(0.14531349, dtype=float32)), 'eval/episode_reward': (Array(-8909.886, dtype=float32), Array(3930.7786, dtype=float32)), 'eval/avg_episode_length': (Array(922.33594, dtype=float32), Array(266.78534, dtype=float32)), 'eval/epoch_eval_time': 4.087489366531372, 'eval/sps': 31315.0661743789}
I0727 17:04:13.142505 140141833336640 train.py:379] starting iteration 262 2639.785459756851
I0727 17:04:23.159526 140141833336640 train.py:394] {'eval/walltime': 1088.0997486114502, 'training/sps': 41470.08929091562, 'training/walltime': 1551.4246804714203, 'training/entropy_loss': Array(-0.03189397, dtype=float32), 'training/policy_loss': Array(0.00439931, dtype=float32), 'training/total_loss': Array(17.87472, dtype=float32), 'training/v_loss': Array(17.902214, dtype=float32), 'eval/episode_goal_distance': (Array(0.38631892, dtype=float32), Array(0.19819142, dtype=float32)), 'eval/episode_reward': (Array(-8679.572, dtype=float32), Array(4269.4727, dtype=float32)), 'eval/avg_episode_length': (Array(930.1094, dtype=float32), Array(254.13933, dtype=float32)), 'eval/epoch_eval_time': 4.087125778198242, 'eval/sps': 31317.851944460388}
I0727 17:04:23.162423 140141833336640 train.py:379] starting iteration 263 2649.805375814438
I0727 17:04:33.121572 140141833336640 train.py:394] {'eval/walltime': 1092.1790571212769, 'training/sps': 41822.61618766667, 'training/walltime': 1557.3009269237518, 'training/entropy_loss': Array(-0.03067682, dtype=float32), 'training/policy_loss': Array(0.00720956, dtype=float32), 'training/total_loss': Array(16.463676, dtype=float32), 'training/v_loss': Array(16.487143, dtype=float32), 'eval/episode_goal_distance': (Array(0.40472734, dtype=float32), Array(0.18137269, dtype=float32)), 'eval/episode_reward': (Array(-8773.058, dtype=float32), Array(4123.7725, dtype=float32)), 'eval/avg_episode_length': (Array(914.6094, dtype=float32), Array(278.48834, dtype=float32)), 'eval/epoch_eval_time': 4.07930850982666, 'eval/sps': 31377.867031056947}
I0727 17:04:33.123907 140141833336640 train.py:379] starting iteration 264 2659.7668612003326
I0727 17:04:43.141078 140141833336640 train.py:394] {'eval/walltime': 1096.2601342201233, 'training/sps': 41425.81236946078, 'training/walltime': 1563.2334599494934, 'training/entropy_loss': Array(-0.03204171, dtype=float32), 'training/policy_loss': Array(0.00488233, dtype=float32), 'training/total_loss': Array(16.39574, dtype=float32), 'training/v_loss': Array(16.4229, dtype=float32), 'eval/episode_goal_distance': (Array(0.43051082, dtype=float32), Array(0.18074176, dtype=float32)), 'eval/episode_reward': (Array(-9372.989, dtype=float32), Array(4476.396, dtype=float32)), 'eval/avg_episode_length': (Array(899.125, dtype=float32), Array(300.02798, dtype=float32)), 'eval/epoch_eval_time': 4.0810770988464355, 'eval/sps': 31364.269015202066}
I0727 17:04:43.143431 140141833336640 train.py:379] starting iteration 265 2669.7863852977753
I0727 17:04:53.176675 140141833336640 train.py:394] {'eval/walltime': 1100.3841681480408, 'training/sps': 41613.77708458634, 'training/walltime': 1569.139196395874, 'training/entropy_loss': Array(-0.0321915, dtype=float32), 'training/policy_loss': Array(0.00536478, dtype=float32), 'training/total_loss': Array(16.249994, dtype=float32), 'training/v_loss': Array(16.276821, dtype=float32), 'eval/episode_goal_distance': (Array(0.41022813, dtype=float32), Array(0.16552216, dtype=float32)), 'eval/episode_reward': (Array(-8733.061, dtype=float32), Array(4061.1963, dtype=float32)), 'eval/avg_episode_length': (Array(906.8125, dtype=float32), Array(289.7321, dtype=float32)), 'eval/epoch_eval_time': 4.1240339279174805, 'eval/sps': 31037.572007715356}
I0727 17:04:53.179011 140141833336640 train.py:379] starting iteration 266 2679.8219652175903
I0727 17:05:03.181874 140141833336640 train.py:394] {'eval/walltime': 1104.497820854187, 'training/sps': 41756.736921925774, 'training/walltime': 1575.024713754654, 'training/entropy_loss': Array(-0.03456915, dtype=float32), 'training/policy_loss': Array(0.0025839, dtype=float32), 'training/total_loss': Array(40.750668, dtype=float32), 'training/v_loss': Array(40.782654, dtype=float32), 'eval/episode_goal_distance': (Array(0.43666792, dtype=float32), Array(0.19566567, dtype=float32)), 'eval/episode_reward': (Array(-9321.702, dtype=float32), Array(3968.0198, dtype=float32)), 'eval/avg_episode_length': (Array(945.65625, dtype=float32), Array(225.94023, dtype=float32)), 'eval/epoch_eval_time': 4.11365270614624, 'eval/sps': 31115.89848330031}
I0727 17:05:03.184175 140141833336640 train.py:379] starting iteration 267 2689.8271300792694
I0727 17:05:13.229270 140141833336640 train.py:394] {'eval/walltime': 1108.6089856624603, 'training/sps': 41442.060861675775, 'training/walltime': 1580.9549207687378, 'training/entropy_loss': Array(-0.03698207, dtype=float32), 'training/policy_loss': Array(0.00078564, dtype=float32), 'training/total_loss': Array(27.541239, dtype=float32), 'training/v_loss': Array(27.577435, dtype=float32), 'eval/episode_goal_distance': (Array(0.42707634, dtype=float32), Array(0.16605566, dtype=float32)), 'eval/episode_reward': (Array(-9431.732, dtype=float32), Array(4251.0796, dtype=float32)), 'eval/avg_episode_length': (Array(914.625, dtype=float32), Array(278.43793, dtype=float32)), 'eval/epoch_eval_time': 4.111164808273315, 'eval/sps': 31134.7284697545}
I0727 17:05:13.231667 140141833336640 train.py:379] starting iteration 268 2699.874621152878
I0727 17:05:23.243732 140141833336640 train.py:394] {'eval/walltime': 1112.7237362861633, 'training/sps': 41699.18224562486, 'training/walltime': 1586.8485615253448, 'training/entropy_loss': Array(-0.03831465, dtype=float32), 'training/policy_loss': Array(0.00106097, dtype=float32), 'training/total_loss': Array(20.580091, dtype=float32), 'training/v_loss': Array(20.617346, dtype=float32), 'eval/episode_goal_distance': (Array(0.44669694, dtype=float32), Array(0.17833136, dtype=float32)), 'eval/episode_reward': (Array(-9343.816, dtype=float32), Array(4325.559, dtype=float32)), 'eval/avg_episode_length': (Array(906.78125, dtype=float32), Array(289.82913, dtype=float32)), 'eval/epoch_eval_time': 4.114750623703003, 'eval/sps': 31107.59598957384}
I0727 17:05:23.246100 140141833336640 train.py:379] starting iteration 269 2709.8890538215637
I0727 17:05:33.277464 140141833336640 train.py:394] {'eval/walltime': 1116.835865020752, 'training/sps': 41544.82601319183, 'training/walltime': 1592.764099597931, 'training/entropy_loss': Array(-0.03939728, dtype=float32), 'training/policy_loss': Array(0.00048886, dtype=float32), 'training/total_loss': Array(20.225498, dtype=float32), 'training/v_loss': Array(20.264408, dtype=float32), 'eval/episode_goal_distance': (Array(0.41465765, dtype=float32), Array(0.2026744, dtype=float32)), 'eval/episode_reward': (Array(-9540.096, dtype=float32), Array(4271.5254, dtype=float32)), 'eval/avg_episode_length': (Array(945.59375, dtype=float32), Array(226.20023, dtype=float32)), 'eval/epoch_eval_time': 4.112128734588623, 'eval/sps': 31127.430161255666}
I0727 17:05:33.279906 140141833336640 train.py:379] starting iteration 270 2719.922860145569
I0727 17:05:43.262078 140141833336640 train.py:394] {'eval/walltime': 1120.9205076694489, 'training/sps': 41700.37659027891, 'training/walltime': 1598.657571554184, 'training/entropy_loss': Array(-0.03878867, dtype=float32), 'training/policy_loss': Array(-0.00010544, dtype=float32), 'training/total_loss': Array(20.86447, dtype=float32), 'training/v_loss': Array(20.903364, dtype=float32), 'eval/episode_goal_distance': (Array(0.40577638, dtype=float32), Array(0.16423003, dtype=float32)), 'eval/episode_reward': (Array(-9113.875, dtype=float32), Array(4076.471, dtype=float32)), 'eval/avg_episode_length': (Array(922.2656, dtype=float32), Array(267.02673, dtype=float32)), 'eval/epoch_eval_time': 4.084642648696899, 'eval/sps': 31336.89064350222}
I0727 17:05:43.264589 140141833336640 train.py:379] starting iteration 271 2729.907542705536
I0727 17:05:53.239472 140141833336640 train.py:394] {'eval/walltime': 1125.0013546943665, 'training/sps': 41721.72607187363, 'training/walltime': 1604.54802775383, 'training/entropy_loss': Array(-0.03888378, dtype=float32), 'training/policy_loss': Array(0.00094321, dtype=float32), 'training/total_loss': Array(19.673876, dtype=float32), 'training/v_loss': Array(19.711817, dtype=float32), 'eval/episode_goal_distance': (Array(0.41924495, dtype=float32), Array(0.18336284, dtype=float32)), 'eval/episode_reward': (Array(-8371.549, dtype=float32), Array(4581.0786, dtype=float32)), 'eval/avg_episode_length': (Array(860.2031, dtype=float32), Array(345.5876, dtype=float32)), 'eval/epoch_eval_time': 4.0808470249176025, 'eval/sps': 31366.03730020595}
I0727 17:05:53.241999 140141833336640 train.py:379] starting iteration 272 2739.884950876236
I0727 17:06:03.242148 140141833336640 train.py:394] {'eval/walltime': 1129.089527606964, 'training/sps': 41597.57834068466, 'training/walltime': 1610.4560639858246, 'training/entropy_loss': Array(-0.03926598, dtype=float32), 'training/policy_loss': Array(0.00096696, dtype=float32), 'training/total_loss': Array(18.530088, dtype=float32), 'training/v_loss': Array(18.568388, dtype=float32), 'eval/episode_goal_distance': (Array(0.45881012, dtype=float32), Array(0.1859505, dtype=float32)), 'eval/episode_reward': (Array(-9992.736, dtype=float32), Array(4063.4788, dtype=float32)), 'eval/avg_episode_length': (Array(945.64844, dtype=float32), Array(225.97292, dtype=float32)), 'eval/epoch_eval_time': 4.088172912597656, 'eval/sps': 31309.830268081256}
I0727 17:06:03.244581 140141833336640 train.py:379] starting iteration 273 2749.887533903122
I0727 17:06:13.225461 140141833336640 train.py:394] {'eval/walltime': 1133.2058408260345, 'training/sps': 41935.11061934212, 'training/walltime': 1616.3165469169617, 'training/entropy_loss': Array(-0.03940562, dtype=float32), 'training/policy_loss': Array(0.00105559, dtype=float32), 'training/total_loss': Array(17.69307, dtype=float32), 'training/v_loss': Array(17.73142, dtype=float32), 'eval/episode_goal_distance': (Array(0.4305877, dtype=float32), Array(0.17996708, dtype=float32)), 'eval/episode_reward': (Array(-9197.571, dtype=float32), Array(4251.372, dtype=float32)), 'eval/avg_episode_length': (Array(914.65625, dtype=float32), Array(278.3361, dtype=float32)), 'eval/epoch_eval_time': 4.116313219070435, 'eval/sps': 31095.78722216517}
I0727 17:06:13.227920 140141833336640 train.py:379] starting iteration 274 2759.87087392807
I0727 17:06:23.266328 140141833336640 train.py:394] {'eval/walltime': 1137.3301718235016, 'training/sps': 41582.552970355384, 'training/walltime': 1622.2267179489136, 'training/entropy_loss': Array(-0.03829227, dtype=float32), 'training/policy_loss': Array(0.00071446, dtype=float32), 'training/total_loss': Array(17.711758, dtype=float32), 'training/v_loss': Array(17.749332, dtype=float32), 'eval/episode_goal_distance': (Array(0.38641644, dtype=float32), Array(0.15615633, dtype=float32)), 'eval/episode_reward': (Array(-8356.538, dtype=float32), Array(4074.2974, dtype=float32)), 'eval/avg_episode_length': (Array(899.02344, dtype=float32), Array(300.32974, dtype=float32)), 'eval/epoch_eval_time': 4.124330997467041, 'eval/sps': 31035.336416648235}
I0727 17:06:23.268669 140141833336640 train.py:379] starting iteration 275 2769.91162276268
I0727 17:06:33.310580 140141833336640 train.py:394] {'eval/walltime': 1141.4467778205872, 'training/sps': 41504.26112611968, 'training/walltime': 1628.1480376720428, 'training/entropy_loss': Array(-0.03568127, dtype=float32), 'training/policy_loss': Array(0.0002321, dtype=float32), 'training/total_loss': Array(48.622845, dtype=float32), 'training/v_loss': Array(48.65829, dtype=float32), 'eval/episode_goal_distance': (Array(0.4031169, dtype=float32), Array(0.1644271, dtype=float32)), 'eval/episode_reward': (Array(-8647.99, dtype=float32), Array(3909.8306, dtype=float32)), 'eval/avg_episode_length': (Array(914.4922, dtype=float32), Array(278.87067, dtype=float32)), 'eval/epoch_eval_time': 4.116605997085571, 'eval/sps': 31093.575652034713}
I0727 17:06:33.313043 140141833336640 train.py:379] starting iteration 276 2779.9559977054596
I0727 17:06:43.338897 140141833336640 train.py:394] {'eval/walltime': 1145.5609414577484, 'training/sps': 41599.86649198464, 'training/walltime': 1634.0557489395142, 'training/entropy_loss': Array(-0.03643968, dtype=float32), 'training/policy_loss': Array(0.0001097, dtype=float32), 'training/total_loss': Array(20.74807, dtype=float32), 'training/v_loss': Array(20.784397, dtype=float32), 'eval/episode_goal_distance': (Array(0.41324776, dtype=float32), Array(0.17317846, dtype=float32)), 'eval/episode_reward': (Array(-9302.482, dtype=float32), Array(4035.2744, dtype=float32)), 'eval/avg_episode_length': (Array(930.1094, dtype=float32), Array(254.13948, dtype=float32)), 'eval/epoch_eval_time': 4.114163637161255, 'eval/sps': 31112.034252560537}
I0727 17:06:43.341611 140141833336640 train.py:379] starting iteration 277 2789.9845654964447
I0727 17:06:53.353952 140141833336640 train.py:394] {'eval/walltime': 1149.6731185913086, 'training/sps': 41681.78597380238, 'training/walltime': 1639.9518494606018, 'training/entropy_loss': Array(-0.03788966, dtype=float32), 'training/policy_loss': Array(-1.6933627e-06, dtype=float32), 'training/total_loss': Array(18.058334, dtype=float32), 'training/v_loss': Array(18.096226, dtype=float32), 'eval/episode_goal_distance': (Array(0.44007418, dtype=float32), Array(0.16710047, dtype=float32)), 'eval/episode_reward': (Array(-9825.943, dtype=float32), Array(4145.864, dtype=float32)), 'eval/avg_episode_length': (Array(930.0703, dtype=float32), Array(254.2814, dtype=float32)), 'eval/epoch_eval_time': 4.112177133560181, 'eval/sps': 31127.063801646607}
I0727 17:06:53.356366 140141833336640 train.py:379] starting iteration 278 2799.999319791794
I0727 17:07:03.337304 140141833336640 train.py:394] {'eval/walltime': 1153.765278339386, 'training/sps': 41762.74105687928, 'training/walltime': 1645.8365206718445, 'training/entropy_loss': Array(-0.03863581, dtype=float32), 'training/policy_loss': Array(0.00030793, dtype=float32), 'training/total_loss': Array(21.288164, dtype=float32), 'training/v_loss': Array(21.326492, dtype=float32), 'eval/episode_goal_distance': (Array(0.4247592, dtype=float32), Array(0.19077393, dtype=float32)), 'eval/episode_reward': (Array(-9424.189, dtype=float32), Array(4258.4526, dtype=float32)), 'eval/avg_episode_length': (Array(922.40625, dtype=float32), Array(266.54373, dtype=float32)), 'eval/epoch_eval_time': 4.092159748077393, 'eval/sps': 31279.32629221962}
I0727 17:07:03.339739 140141833336640 train.py:379] starting iteration 279 2809.9826934337616
I0727 17:07:13.341196 140141833336640 train.py:394] {'eval/walltime': 1157.8484053611755, 'training/sps': 41553.08083853046, 'training/walltime': 1651.7508835792542, 'training/entropy_loss': Array(-0.03867787, dtype=float32), 'training/policy_loss': Array(0.00041002, dtype=float32), 'training/total_loss': Array(18.755531, dtype=float32), 'training/v_loss': Array(18.793798, dtype=float32), 'eval/episode_goal_distance': (Array(0.4119745, dtype=float32), Array(0.17049886, dtype=float32)), 'eval/episode_reward': (Array(-8894.924, dtype=float32), Array(4407.2544, dtype=float32)), 'eval/avg_episode_length': (Array(891.2578, dtype=float32), Array(310.3038, dtype=float32)), 'eval/epoch_eval_time': 4.083127021789551, 'eval/sps': 31348.5226682711}
I0727 17:07:13.343847 140141833336640 train.py:379] starting iteration 280 2819.9868013858795
I0727 17:07:23.347381 140141833336640 train.py:394] {'eval/walltime': 1161.934402704239, 'training/sps': 41557.93243438288, 'training/walltime': 1657.6645560264587, 'training/entropy_loss': Array(-0.04002963, dtype=float32), 'training/policy_loss': Array(0.00053329, dtype=float32), 'training/total_loss': Array(19.348866, dtype=float32), 'training/v_loss': Array(19.388361, dtype=float32), 'eval/episode_goal_distance': (Array(0.4138064, dtype=float32), Array(0.17442343, dtype=float32)), 'eval/episode_reward': (Array(-9420.817, dtype=float32), Array(4131.997, dtype=float32)), 'eval/avg_episode_length': (Array(930.1172, dtype=float32), Array(254.11104, dtype=float32)), 'eval/epoch_eval_time': 4.0859973430633545, 'eval/sps': 31326.50103586113}
I0727 17:07:23.349797 140141833336640 train.py:379] starting iteration 281 2829.992751121521
I0727 17:07:33.342279 140141833336640 train.py:394] {'eval/walltime': 1166.010959148407, 'training/sps': 41570.785590991414, 'training/walltime': 1663.5764000415802, 'training/entropy_loss': Array(-0.03975223, dtype=float32), 'training/policy_loss': Array(6.4445885e-06, dtype=float32), 'training/total_loss': Array(17.47127, dtype=float32), 'training/v_loss': Array(17.511015, dtype=float32), 'eval/episode_goal_distance': (Array(0.39953458, dtype=float32), Array(0.16354753, dtype=float32)), 'eval/episode_reward': (Array(-8519.304, dtype=float32), Array(4234.427, dtype=float32)), 'eval/avg_episode_length': (Array(891.3125, dtype=float32), Array(310.14764, dtype=float32)), 'eval/epoch_eval_time': 4.076556444168091, 'eval/sps': 31399.05009364372}
I0727 17:07:33.344876 140141833336640 train.py:379] starting iteration 282 2839.9878299236298
I0727 17:07:43.360856 140141833336640 train.py:394] {'eval/walltime': 1170.1390552520752, 'training/sps': 41768.978794180206, 'training/walltime': 1669.4601924419403, 'training/entropy_loss': Array(-0.03975531, dtype=float32), 'training/policy_loss': Array(0.00010494, dtype=float32), 'training/total_loss': Array(17.925283, dtype=float32), 'training/v_loss': Array(17.964933, dtype=float32), 'eval/episode_goal_distance': (Array(0.4259791, dtype=float32), Array(0.17372449, dtype=float32)), 'eval/episode_reward': (Array(-8846.928, dtype=float32), Array(4462.0806, dtype=float32)), 'eval/avg_episode_length': (Array(883.4375, dtype=float32), Array(319.92865, dtype=float32)), 'eval/epoch_eval_time': 4.128096103668213, 'eval/sps': 31007.030065569357}
I0727 17:07:43.363427 140141833336640 train.py:379] starting iteration 283 2850.0063815116882
I0727 17:07:53.377600 140141833336640 train.py:394] {'eval/walltime': 1174.211309671402, 'training/sps': 41387.637972491386, 'training/walltime': 1675.3981974124908, 'training/entropy_loss': Array(-0.03936791, dtype=float32), 'training/policy_loss': Array(0.00014409, dtype=float32), 'training/total_loss': Array(48.089314, dtype=float32), 'training/v_loss': Array(48.12854, dtype=float32), 'eval/episode_goal_distance': (Array(0.4156084, dtype=float32), Array(0.17702083, dtype=float32)), 'eval/episode_reward': (Array(-9142.004, dtype=float32), Array(4154.2725, dtype=float32)), 'eval/avg_episode_length': (Array(922.3672, dtype=float32), Array(266.67838, dtype=float32)), 'eval/epoch_eval_time': 4.072254419326782, 'eval/sps': 31432.220784761463}
I0727 17:07:53.380087 140141833336640 train.py:379] starting iteration 284 2860.0230417251587
I0727 17:08:03.420493 140141833336640 train.py:394] {'eval/walltime': 1178.3262553215027, 'training/sps': 41502.43130244463, 'training/walltime': 1681.3197782039642, 'training/entropy_loss': Array(-0.04004493, dtype=float32), 'training/policy_loss': Array(3.5701647e-05, dtype=float32), 'training/total_loss': Array(24.418907, dtype=float32), 'training/v_loss': Array(24.458918, dtype=float32), 'eval/episode_goal_distance': (Array(0.4318633, dtype=float32), Array(0.20125648, dtype=float32)), 'eval/episode_reward': (Array(-9507.646, dtype=float32), Array(4671.3384, dtype=float32)), 'eval/avg_episode_length': (Array(906.84375, dtype=float32), Array(289.6352, dtype=float32)), 'eval/epoch_eval_time': 4.114945650100708, 'eval/sps': 31106.12165603387}
I0727 17:08:03.422896 140141833336640 train.py:379] starting iteration 285 2870.065850496292
I0727 17:08:13.436116 140141833336640 train.py:394] {'eval/walltime': 1182.4378697872162, 'training/sps': 41670.742317292534, 'training/walltime': 1687.2174413204193, 'training/entropy_loss': Array(-0.04117941, dtype=float32), 'training/policy_loss': Array(3.632323e-05, dtype=float32), 'training/total_loss': Array(20.464706, dtype=float32), 'training/v_loss': Array(20.50585, dtype=float32), 'eval/episode_goal_distance': (Array(0.45516387, dtype=float32), Array(0.1925338, dtype=float32)), 'eval/episode_reward': (Array(-9795.311, dtype=float32), Array(4300.7773, dtype=float32)), 'eval/avg_episode_length': (Array(922.34375, dtype=float32), Array(266.75836, dtype=float32)), 'eval/epoch_eval_time': 4.111614465713501, 'eval/sps': 31131.32349041577}
I0727 17:08:13.438595 140141833336640 train.py:379] starting iteration 286 2880.0815494060516
I0727 17:08:23.467461 140141833336640 train.py:394] {'eval/walltime': 1186.5458343029022, 'training/sps': 41534.73509860525, 'training/walltime': 1693.1344165802002, 'training/entropy_loss': Array(-0.04175806, dtype=float32), 'training/policy_loss': Array(-4.226601e-05, dtype=float32), 'training/total_loss': Array(22.124807, dtype=float32), 'training/v_loss': Array(22.166607, dtype=float32), 'eval/episode_goal_distance': (Array(0.42979032, dtype=float32), Array(0.18500945, dtype=float32)), 'eval/episode_reward': (Array(-9432.067, dtype=float32), Array(3889.7883, dtype=float32)), 'eval/avg_episode_length': (Array(945.5547, dtype=float32), Array(226.36244, dtype=float32)), 'eval/epoch_eval_time': 4.107964515686035, 'eval/sps': 31158.98384984561}
I0727 17:08:23.469810 140141833336640 train.py:379] starting iteration 287 2890.112764120102
I0727 17:08:33.468977 140141833336640 train.py:394] {'eval/walltime': 1190.6444799900055, 'training/sps': 41675.998866637085, 'training/walltime': 1699.0313358306885, 'training/entropy_loss': Array(-0.04360046, dtype=float32), 'training/policy_loss': Array(-6.590051e-05, dtype=float32), 'training/total_loss': Array(21.223152, dtype=float32), 'training/v_loss': Array(21.266823, dtype=float32), 'eval/episode_goal_distance': (Array(0.43879688, dtype=float32), Array(0.1857531, dtype=float32)), 'eval/episode_reward': (Array(-9928.029, dtype=float32), Array(3770.3757, dtype=float32)), 'eval/avg_episode_length': (Array(953.3906, dtype=float32), Array(210.1735, dtype=float32)), 'eval/epoch_eval_time': 4.0986456871032715, 'eval/sps': 31229.82803875012}
I0727 17:08:33.471761 140141833336640 train.py:379] starting iteration 288 2900.1147150993347
I0727 17:08:43.461501 140141833336640 train.py:394] {'eval/walltime': 1194.7366364002228, 'training/sps': 41699.081033120805, 'training/walltime': 1704.9249908924103, 'training/entropy_loss': Array(-0.04317917, dtype=float32), 'training/policy_loss': Array(-0.00016094, dtype=float32), 'training/total_loss': Array(20.400274, dtype=float32), 'training/v_loss': Array(20.443615, dtype=float32), 'eval/episode_goal_distance': (Array(0.41393137, dtype=float32), Array(0.16837153, dtype=float32)), 'eval/episode_reward': (Array(-9401.934, dtype=float32), Array(4246.007, dtype=float32)), 'eval/avg_episode_length': (Array(922.35156, dtype=float32), Array(266.7316, dtype=float32)), 'eval/epoch_eval_time': 4.092156410217285, 'eval/sps': 31279.351805910923}
I0727 17:08:43.463848 140141833336640 train.py:379] starting iteration 289 2910.106802225113
I0727 17:08:53.430045 140141833336640 train.py:394] {'eval/walltime': 1198.816372871399, 'training/sps': 41776.49329060275, 'training/walltime': 1710.8077249526978, 'training/entropy_loss': Array(-0.04244139, dtype=float32), 'training/policy_loss': Array(8.043502e-05, dtype=float32), 'training/total_loss': Array(20.288853, dtype=float32), 'training/v_loss': Array(20.331211, dtype=float32), 'eval/episode_goal_distance': (Array(0.44698036, dtype=float32), Array(0.217111, dtype=float32)), 'eval/episode_reward': (Array(-9232.844, dtype=float32), Array(4370.388, dtype=float32)), 'eval/avg_episode_length': (Array(899.0156, dtype=float32), Array(300.35303, dtype=float32)), 'eval/epoch_eval_time': 4.0797364711761475, 'eval/sps': 31374.575515927594}
I0727 17:08:53.432421 140141833336640 train.py:379] starting iteration 290 2920.075375318527
I0727 17:09:03.456531 140141833336640 train.py:394] {'eval/walltime': 1202.8990614414215, 'training/sps': 41389.63218920458, 'training/walltime': 1716.7454438209534, 'training/entropy_loss': Array(-0.04360155, dtype=float32), 'training/policy_loss': Array(-2.4552573e-05, dtype=float32), 'training/total_loss': Array(20.109673, dtype=float32), 'training/v_loss': Array(20.153297, dtype=float32), 'eval/episode_goal_distance': (Array(0.44946444, dtype=float32), Array(0.19151449, dtype=float32)), 'eval/episode_reward': (Array(-9010.35, dtype=float32), Array(4327.8125, dtype=float32)), 'eval/avg_episode_length': (Array(898.9844, dtype=float32), Array(300.4461, dtype=float32)), 'eval/epoch_eval_time': 4.082688570022583, 'eval/sps': 31351.889277043727}
I0727 17:09:03.458962 140141833336640 train.py:379] starting iteration 291 2930.1019167900085
I0727 17:09:13.453068 140141833336640 train.py:394] {'eval/walltime': 1206.9741909503937, 'training/sps': 41546.51891449099, 'training/walltime': 1722.660740852356, 'training/entropy_loss': Array(-0.04338818, dtype=float32), 'training/policy_loss': Array(-6.6047505e-07, dtype=float32), 'training/total_loss': Array(43.314453, dtype=float32), 'training/v_loss': Array(43.357845, dtype=float32), 'eval/episode_goal_distance': (Array(0.42113176, dtype=float32), Array(0.18621023, dtype=float32)), 'eval/episode_reward': (Array(-8633.873, dtype=float32), Array(4153.1357, dtype=float32)), 'eval/avg_episode_length': (Array(899.0469, dtype=float32), Array(300.26038, dtype=float32)), 'eval/epoch_eval_time': 4.075129508972168, 'eval/sps': 31410.04469138559}
I0727 17:09:13.455409 140141833336640 train.py:379] starting iteration 292 2940.098363637924
I0727 17:09:23.498777 140141833336640 train.py:394] {'eval/walltime': 1211.0802116394043, 'training/sps': 41418.43014596773, 'training/walltime': 1728.5943312644958, 'training/entropy_loss': Array(-0.04339536, dtype=float32), 'training/policy_loss': Array(-0.00032822, dtype=float32), 'training/total_loss': Array(30.891874, dtype=float32), 'training/v_loss': Array(30.935595, dtype=float32), 'eval/episode_goal_distance': (Array(0.46812993, dtype=float32), Array(0.21705864, dtype=float32)), 'eval/episode_reward': (Array(-10067.733, dtype=float32), Array(4488.479, dtype=float32)), 'eval/avg_episode_length': (Array(930.0547, dtype=float32), Array(254.338, dtype=float32)), 'eval/epoch_eval_time': 4.10602068901062, 'eval/sps': 31173.734789641956}
I0727 17:09:23.501218 140141833336640 train.py:379] starting iteration 293 2950.14417219162
I0727 17:09:33.510014 140141833336640 train.py:394] {'eval/walltime': 1215.1638703346252, 'training/sps': 41503.676233428145, 'training/walltime': 1734.5157344341278, 'training/entropy_loss': Array(-0.04384546, dtype=float32), 'training/policy_loss': Array(0.00013029, dtype=float32), 'training/total_loss': Array(22.471252, dtype=float32), 'training/v_loss': Array(22.514969, dtype=float32), 'eval/episode_goal_distance': (Array(0.44237247, dtype=float32), Array(0.17068686, dtype=float32)), 'eval/episode_reward': (Array(-8627.927, dtype=float32), Array(4775.5776, dtype=float32)), 'eval/avg_episode_length': (Array(844.60156, dtype=float32), Array(361.11395, dtype=float32)), 'eval/epoch_eval_time': 4.083658695220947, 'eval/sps': 31344.441235942842}
I0727 17:09:33.512364 140141833336640 train.py:379] starting iteration 294 2960.155318260193
I0727 17:09:43.544759 140141833336640 train.py:394] {'eval/walltime': 1219.2879247665405, 'training/sps': 41621.08458475808, 'training/walltime': 1740.420433998108, 'training/entropy_loss': Array(-0.04436328, dtype=float32), 'training/policy_loss': Array(-0.0001926, dtype=float32), 'training/total_loss': Array(22.09324, dtype=float32), 'training/v_loss': Array(22.137794, dtype=float32), 'eval/episode_goal_distance': (Array(0.462684, dtype=float32), Array(0.2037123, dtype=float32)), 'eval/episode_reward': (Array(-9701.209, dtype=float32), Array(4801.416, dtype=float32)), 'eval/avg_episode_length': (Array(899., dtype=float32), Array(300.3995, dtype=float32)), 'eval/epoch_eval_time': 4.124054431915283, 'eval/sps': 31037.41769493439}
I0727 17:09:43.547035 140141833336640 train.py:379] starting iteration 295 2970.18999004364
I0727 17:09:53.564221 140141833336640 train.py:394] {'eval/walltime': 1223.3908751010895, 'training/sps': 41580.4227100864, 'training/walltime': 1746.3309078216553, 'training/entropy_loss': Array(-0.04499727, dtype=float32), 'training/policy_loss': Array(0.00010952, dtype=float32), 'training/total_loss': Array(22.148441, dtype=float32), 'training/v_loss': Array(22.193329, dtype=float32), 'eval/episode_goal_distance': (Array(0.45065925, dtype=float32), Array(0.1982294, dtype=float32)), 'eval/episode_reward': (Array(-9224.487, dtype=float32), Array(4672.126, dtype=float32)), 'eval/avg_episode_length': (Array(875.77344, dtype=float32), Array(328.67307, dtype=float32)), 'eval/epoch_eval_time': 4.10295033454895, 'eval/sps': 31197.06298225797}
I0727 17:09:53.567620 140141833336640 train.py:379] starting iteration 296 2980.2105674743652
I0727 17:10:03.579592 140141833336640 train.py:394] {'eval/walltime': 1227.5004625320435, 'training/sps': 41666.366240770214, 'training/walltime': 1752.2291903495789, 'training/entropy_loss': Array(-0.04479945, dtype=float32), 'training/policy_loss': Array(-0.00025213, dtype=float32), 'training/total_loss': Array(20.709114, dtype=float32), 'training/v_loss': Array(20.754166, dtype=float32), 'eval/episode_goal_distance': (Array(0.4722093, dtype=float32), Array(0.18774804, dtype=float32)), 'eval/episode_reward': (Array(-10334.689, dtype=float32), Array(4657.072, dtype=float32)), 'eval/avg_episode_length': (Array(906.7422, dtype=float32), Array(289.95056, dtype=float32)), 'eval/epoch_eval_time': 4.1095874309539795, 'eval/sps': 31146.678869972773}
I0727 17:10:03.581918 140141833336640 train.py:379] starting iteration 297 2990.2248725891113
I0727 17:10:13.609837 140141833336640 train.py:394] {'eval/walltime': 1231.606464624405, 'training/sps': 41526.02248900205, 'training/walltime': 1758.1474070549011, 'training/entropy_loss': Array(-0.0451827, dtype=float32), 'training/policy_loss': Array(-0.00031538, dtype=float32), 'training/total_loss': Array(22.189667, dtype=float32), 'training/v_loss': Array(22.23516, dtype=float32), 'eval/episode_goal_distance': (Array(0.4636926, dtype=float32), Array(0.22427508, dtype=float32)), 'eval/episode_reward': (Array(-9787.504, dtype=float32), Array(4807.392, dtype=float32)), 'eval/avg_episode_length': (Array(906.72656, dtype=float32), Array(289.99945, dtype=float32)), 'eval/epoch_eval_time': 4.10600209236145, 'eval/sps': 31173.875979781697}
I0727 17:10:13.612236 140141833336640 train.py:379] starting iteration 298 3000.255190372467
I0727 17:10:23.631261 140141833336640 train.py:394] {'eval/walltime': 1235.70343542099, 'training/sps': 41524.64406647474, 'training/walltime': 1764.0658202171326, 'training/entropy_loss': Array(-0.04560637, dtype=float32), 'training/policy_loss': Array(-0.00010959, dtype=float32), 'training/total_loss': Array(22.076818, dtype=float32), 'training/v_loss': Array(22.122536, dtype=float32), 'eval/episode_goal_distance': (Array(0.4635072, dtype=float32), Array(0.2175918, dtype=float32)), 'eval/episode_reward': (Array(-9405.195, dtype=float32), Array(5218.9795, dtype=float32)), 'eval/avg_episode_length': (Array(852.4297, dtype=float32), Array(353.4565, dtype=float32)), 'eval/epoch_eval_time': 4.096970796585083, 'eval/sps': 31242.59516486934}
I0727 17:10:23.633634 140141833336640 train.py:379] starting iteration 299 3010.2765879631042
I0727 17:10:33.581846 140141833336640 train.py:394] {'eval/walltime': 1239.7954359054565, 'training/sps': 41992.28730243452, 'training/walltime': 1769.9183235168457, 'training/entropy_loss': Array(-0.04562079, dtype=float32), 'training/policy_loss': Array(0.00023386, dtype=float32), 'training/total_loss': Array(22.544552, dtype=float32), 'training/v_loss': Array(22.58994, dtype=float32), 'eval/episode_goal_distance': (Array(0.44426146, dtype=float32), Array(0.20684172, dtype=float32)), 'eval/episode_reward': (Array(-9189.172, dtype=float32), Array(4603.8823, dtype=float32)), 'eval/avg_episode_length': (Array(875.71875, dtype=float32), Array(328.81793, dtype=float32)), 'eval/epoch_eval_time': 4.092000484466553, 'eval/sps': 31280.543706163935}
I0727 17:10:33.584469 140141833336640 train.py:379] starting iteration 300 3020.227416753769
I0727 17:10:43.573348 140141833336640 train.py:394] {'eval/walltime': 1243.8787310123444, 'training/sps': 41641.666571328016, 'training/walltime': 1775.820104598999, 'training/entropy_loss': Array(-0.04501885, dtype=float32), 'training/policy_loss': Array(-0.00011059, dtype=float32), 'training/total_loss': Array(52.530243, dtype=float32), 'training/v_loss': Array(52.575375, dtype=float32), 'eval/episode_goal_distance': (Array(0.40788126, dtype=float32), Array(0.18681014, dtype=float32)), 'eval/episode_reward': (Array(-9164.804, dtype=float32), Array(4319.2803, dtype=float32)), 'eval/avg_episode_length': (Array(914.64844, dtype=float32), Array(278.36133, dtype=float32)), 'eval/epoch_eval_time': 4.083295106887817, 'eval/sps': 31347.232235085332}
I0727 17:10:43.575754 140141833336640 train.py:379] starting iteration 301 3030.2187082767487
I0727 17:10:53.605714 140141833336640 train.py:394] {'eval/walltime': 1247.9865579605103, 'training/sps': 41523.17039312566, 'training/walltime': 1781.7387278079987, 'training/entropy_loss': Array(-0.04487243, dtype=float32), 'training/policy_loss': Array(-0.00013462, dtype=float32), 'training/total_loss': Array(24.903101, dtype=float32), 'training/v_loss': Array(24.948109, dtype=float32), 'eval/episode_goal_distance': (Array(0.4508825, dtype=float32), Array(0.18504973, dtype=float32)), 'eval/episode_reward': (Array(-9678.328, dtype=float32), Array(4483.0786, dtype=float32)), 'eval/avg_episode_length': (Array(906.7422, dtype=float32), Array(289.9506, dtype=float32)), 'eval/epoch_eval_time': 4.1078269481658936, 'eval/sps': 31160.027336874744}
I0727 17:10:53.608180 140141833336640 train.py:379] starting iteration 302 3040.251134157181
I0727 17:11:03.637442 140141833336640 train.py:394] {'eval/walltime': 1252.1068029403687, 'training/sps': 41616.35768829304, 'training/walltime': 1787.6440980434418, 'training/entropy_loss': Array(-0.04414039, dtype=float32), 'training/policy_loss': Array(-0.00015074, dtype=float32), 'training/total_loss': Array(21.706589, dtype=float32), 'training/v_loss': Array(21.750881, dtype=float32), 'eval/episode_goal_distance': (Array(0.4355502, dtype=float32), Array(0.19270228, dtype=float32)), 'eval/episode_reward': (Array(-9582.379, dtype=float32), Array(4309.1616, dtype=float32)), 'eval/avg_episode_length': (Array(922.35156, dtype=float32), Array(266.73145, dtype=float32)), 'eval/epoch_eval_time': 4.120244979858398, 'eval/sps': 31066.113938787934}
I0727 17:11:03.639762 140141833336640 train.py:379] starting iteration 303 3050.2827167510986
I0727 17:11:13.643713 140141833336640 train.py:394] {'eval/walltime': 1256.217788696289, 'training/sps': 41729.787811643386, 'training/walltime': 1793.5334162712097, 'training/entropy_loss': Array(-0.04384065, dtype=float32), 'training/policy_loss': Array(-5.6291415e-06, dtype=float32), 'training/total_loss': Array(23.916485, dtype=float32), 'training/v_loss': Array(23.960331, dtype=float32), 'eval/episode_goal_distance': (Array(0.45067966, dtype=float32), Array(0.20489684, dtype=float32)), 'eval/episode_reward': (Array(-9494.752, dtype=float32), Array(4534.777, dtype=float32)), 'eval/avg_episode_length': (Array(899.0078, dtype=float32), Array(300.37662, dtype=float32)), 'eval/epoch_eval_time': 4.11098575592041, 'eval/sps': 31136.084530495296}
I0727 17:11:13.646168 140141833336640 train.py:379] starting iteration 304 3060.2891228199005
I0727 17:11:23.646080 140141833336640 train.py:394] {'eval/walltime': 1260.3237018585205, 'training/sps': 41721.82908304816, 'training/walltime': 1799.4238579273224, 'training/entropy_loss': Array(-0.04417624, dtype=float32), 'training/policy_loss': Array(-5.5887678e-05, dtype=float32), 'training/total_loss': Array(21.799992, dtype=float32), 'training/v_loss': Array(21.844223, dtype=float32), 'eval/episode_goal_distance': (Array(0.43438503, dtype=float32), Array(0.17709942, dtype=float32)), 'eval/episode_reward': (Array(-9637.115, dtype=float32), Array(4325.7314, dtype=float32)), 'eval/avg_episode_length': (Array(914.6797, dtype=float32), Array(278.2591, dtype=float32)), 'eval/epoch_eval_time': 4.105913162231445, 'eval/sps': 31174.55117595222}
I0727 17:11:23.648667 140141833336640 train.py:379] starting iteration 305 3070.291621208191
I0727 17:11:33.611585 140141833336640 train.py:394] {'eval/walltime': 1264.4185681343079, 'training/sps': 41907.291934077715, 'training/walltime': 1805.2882311344147, 'training/entropy_loss': Array(-0.04380224, dtype=float32), 'training/policy_loss': Array(-0.0001913, dtype=float32), 'training/total_loss': Array(21.338223, dtype=float32), 'training/v_loss': Array(21.382215, dtype=float32), 'eval/episode_goal_distance': (Array(0.46200478, dtype=float32), Array(0.21337278, dtype=float32)), 'eval/episode_reward': (Array(-9196.728, dtype=float32), Array(4971.9756, dtype=float32)), 'eval/avg_episode_length': (Array(867.9922, dtype=float32), Array(337.31644, dtype=float32)), 'eval/epoch_eval_time': 4.0948662757873535, 'eval/sps': 31258.652024085546}
I0727 17:11:33.614220 140141833336640 train.py:379] starting iteration 306 3080.2571742534637
I0727 17:11:43.575447 140141833336640 train.py:394] {'eval/walltime': 1268.4929749965668, 'training/sps': 41771.879993678245, 'training/walltime': 1811.1716148853302, 'training/entropy_loss': Array(-0.04327473, dtype=float32), 'training/policy_loss': Array(1.2544322e-05, dtype=float32), 'training/total_loss': Array(20.90386, dtype=float32), 'training/v_loss': Array(20.947124, dtype=float32), 'eval/episode_goal_distance': (Array(0.42903432, dtype=float32), Array(0.16727541, dtype=float32)), 'eval/episode_reward': (Array(-9161.338, dtype=float32), Array(4269.2446, dtype=float32)), 'eval/avg_episode_length': (Array(899.0703, dtype=float32), Array(300.19073, dtype=float32)), 'eval/epoch_eval_time': 4.074406862258911, 'eval/sps': 31415.61565332602}
I0727 17:11:43.756943 140141833336640 train.py:379] starting iteration 307 3090.3998823165894
I0727 17:11:53.762074 140141833336640 train.py:394] {'eval/walltime': 1272.5950150489807, 'training/sps': 41660.63897835113, 'training/walltime': 1817.0707082748413, 'training/entropy_loss': Array(-0.04350786, dtype=float32), 'training/policy_loss': Array(4.351133e-05, dtype=float32), 'training/total_loss': Array(21.524933, dtype=float32), 'training/v_loss': Array(21.568398, dtype=float32), 'eval/episode_goal_distance': (Array(0.4318747, dtype=float32), Array(0.18657276, dtype=float32)), 'eval/episode_reward': (Array(-9114.963, dtype=float32), Array(4519.337, dtype=float32)), 'eval/avg_episode_length': (Array(883.46875, dtype=float32), Array(319.84317, dtype=float32)), 'eval/epoch_eval_time': 4.10204005241394, 'eval/sps': 31203.98591054113}
I0727 17:11:53.764692 140141833336640 train.py:379] starting iteration 308 3100.407646417618
I0727 17:12:03.695842 140141833336640 train.py:394] {'eval/walltime': 1276.675508260727, 'training/sps': 42030.16331582838, 'training/walltime': 1822.9179375171661, 'training/entropy_loss': Array(-0.04318092, dtype=float32), 'training/policy_loss': Array(-0.00029352, dtype=float32), 'training/total_loss': Array(48.556564, dtype=float32), 'training/v_loss': Array(48.600044, dtype=float32), 'eval/episode_goal_distance': (Array(0.41389582, dtype=float32), Array(0.16365317, dtype=float32)), 'eval/episode_reward': (Array(-9054.722, dtype=float32), Array(4859.396, dtype=float32)), 'eval/avg_episode_length': (Array(867.9844, dtype=float32), Array(337.3362, dtype=float32)), 'eval/epoch_eval_time': 4.080493211746216, 'eval/sps': 31368.757000142974}
I0727 17:12:03.698135 140141833336640 train.py:379] starting iteration 309 3110.341089248657
I0727 17:12:13.692876 140141833336640 train.py:394] {'eval/walltime': 1280.8007757663727, 'training/sps': 41897.09571575766, 'training/walltime': 1828.783737897873, 'training/entropy_loss': Array(-0.04301626, dtype=float32), 'training/policy_loss': Array(-0.00043656, dtype=float32), 'training/total_loss': Array(24.426626, dtype=float32), 'training/v_loss': Array(24.470078, dtype=float32), 'eval/episode_goal_distance': (Array(0.3995582, dtype=float32), Array(0.16431391, dtype=float32)), 'eval/episode_reward': (Array(-9003.098, dtype=float32), Array(4406.9634, dtype=float32)), 'eval/avg_episode_length': (Array(891.3203, dtype=float32), Array(310.1256, dtype=float32)), 'eval/epoch_eval_time': 4.125267505645752, 'eval/sps': 31028.290850186557}
I0727 17:12:13.695440 140141833336640 train.py:379] starting iteration 310 3120.338394641876
I0727 17:12:23.703552 140141833336640 train.py:394] {'eval/walltime': 1284.8898944854736, 'training/sps': 41547.86194718915, 'training/walltime': 1834.698843717575, 'training/entropy_loss': Array(-0.04350531, dtype=float32), 'training/policy_loss': Array(-0.00040012, dtype=float32), 'training/total_loss': Array(21.224934, dtype=float32), 'training/v_loss': Array(21.268839, dtype=float32), 'eval/episode_goal_distance': (Array(0.41535407, dtype=float32), Array(0.1827357, dtype=float32)), 'eval/episode_reward': (Array(-9137., dtype=float32), Array(4408.044, dtype=float32)), 'eval/avg_episode_length': (Array(906.8281, dtype=float32), Array(289.6836, dtype=float32)), 'eval/epoch_eval_time': 4.089118719100952, 'eval/sps': 31302.58835530765}
I0727 17:12:23.706042 140141833336640 train.py:379] starting iteration 311 3130.3489952087402
I0727 17:12:33.694319 140141833336640 train.py:394] {'eval/walltime': 1288.9902477264404, 'training/sps': 41768.39149420832, 'training/walltime': 1840.5827188491821, 'training/entropy_loss': Array(-0.04425205, dtype=float32), 'training/policy_loss': Array(-0.00012693, dtype=float32), 'training/total_loss': Array(23.086155, dtype=float32), 'training/v_loss': Array(23.130533, dtype=float32), 'eval/episode_goal_distance': (Array(0.41375148, dtype=float32), Array(0.17227536, dtype=float32)), 'eval/episode_reward': (Array(-9100.178, dtype=float32), Array(4402.0776, dtype=float32)), 'eval/avg_episode_length': (Array(899.0469, dtype=float32), Array(300.26013, dtype=float32)), 'eval/epoch_eval_time': 4.100353240966797, 'eval/sps': 31216.822668141554}
I0727 17:12:33.696859 140141833336640 train.py:379] starting iteration 312 3140.339813709259
I0727 17:12:43.654939 140141833336640 train.py:394] {'eval/walltime': 1293.0764198303223, 'training/sps': 41880.338779904465, 'training/walltime': 1846.4508662223816, 'training/entropy_loss': Array(-0.04357983, dtype=float32), 'training/policy_loss': Array(-0.00010855, dtype=float32), 'training/total_loss': Array(19.829712, dtype=float32), 'training/v_loss': Array(19.8734, dtype=float32), 'eval/episode_goal_distance': (Array(0.42296293, dtype=float32), Array(0.19227344, dtype=float32)), 'eval/episode_reward': (Array(-8821.947, dtype=float32), Array(4822.909, dtype=float32)), 'eval/avg_episode_length': (Array(868.0547, dtype=float32), Array(337.157, dtype=float32)), 'eval/epoch_eval_time': 4.086172103881836, 'eval/sps': 31325.161237922617}
I0727 17:12:43.657285 140141833336640 train.py:379] starting iteration 313 3150.300239086151
I0727 17:12:53.668794 140141833336640 train.py:394] {'eval/walltime': 1297.1627836227417, 'training/sps': 41504.08398544994, 'training/walltime': 1852.3722112178802, 'training/entropy_loss': Array(-0.04468284, dtype=float32), 'training/policy_loss': Array(0.00016392, dtype=float32), 'training/total_loss': Array(20.594713, dtype=float32), 'training/v_loss': Array(20.639233, dtype=float32), 'eval/episode_goal_distance': (Array(0.4228849, dtype=float32), Array(0.18415783, dtype=float32)), 'eval/episode_reward': (Array(-9534.957, dtype=float32), Array(4226.6987, dtype=float32)), 'eval/avg_episode_length': (Array(937.9531, dtype=float32), Array(240.30685, dtype=float32)), 'eval/epoch_eval_time': 4.086363792419434, 'eval/sps': 31323.69179598041}
I0727 17:12:53.671304 140141833336640 train.py:379] starting iteration 314 3160.3142580986023
I0727 17:13:03.643057 140141833336640 train.py:394] {'eval/walltime': 1301.2402379512787, 'training/sps': 41721.10294954821, 'training/walltime': 1858.262755393982, 'training/entropy_loss': Array(-0.04654169, dtype=float32), 'training/policy_loss': Array(-5.4839657e-05, dtype=float32), 'training/total_loss': Array(22.552847, dtype=float32), 'training/v_loss': Array(22.599443, dtype=float32), 'eval/episode_goal_distance': (Array(0.44791436, dtype=float32), Array(0.2111846, dtype=float32)), 'eval/episode_reward': (Array(-9572.855, dtype=float32), Array(4922.0605, dtype=float32)), 'eval/avg_episode_length': (Array(875.8047, dtype=float32), Array(328.59088, dtype=float32)), 'eval/epoch_eval_time': 4.077454328536987, 'eval/sps': 31392.135800065993}
I0727 17:13:03.645437 140141833336640 train.py:379] starting iteration 315 3170.288390636444
I0727 17:13:13.663088 140141833336640 train.py:394] {'eval/walltime': 1305.3531365394592, 'training/sps': 41646.29489752643, 'training/walltime': 1864.1638805866241, 'training/entropy_loss': Array(-0.04782311, dtype=float32), 'training/policy_loss': Array(-0.00011485, dtype=float32), 'training/total_loss': Array(22.750116, dtype=float32), 'training/v_loss': Array(22.798054, dtype=float32), 'eval/episode_goal_distance': (Array(0.47478566, dtype=float32), Array(0.22242121, dtype=float32)), 'eval/episode_reward': (Array(-10184.621, dtype=float32), Array(4654.389, dtype=float32)), 'eval/avg_episode_length': (Array(922.2969, dtype=float32), Array(266.91934, dtype=float32)), 'eval/epoch_eval_time': 4.112898588180542, 'eval/sps': 31121.603719537477}
I0727 17:13:13.665669 140141833336640 train.py:379] starting iteration 316 3180.3086228370667
I0727 17:13:23.635784 140141833336640 train.py:394] {'eval/walltime': 1309.4402465820312, 'training/sps': 41801.590534244046, 'training/walltime': 1870.0430827140808, 'training/entropy_loss': Array(-0.04765914, dtype=float32), 'training/policy_loss': Array(-0.00011178, dtype=float32), 'training/total_loss': Array(45.52908, dtype=float32), 'training/v_loss': Array(45.576847, dtype=float32), 'eval/episode_goal_distance': (Array(0.5041909, dtype=float32), Array(0.2550163, dtype=float32)), 'eval/episode_reward': (Array(-10334.211, dtype=float32), Array(4975.3296, dtype=float32)), 'eval/avg_episode_length': (Array(922.3047, dtype=float32), Array(266.89255, dtype=float32)), 'eval/epoch_eval_time': 4.0871100425720215, 'eval/sps': 31317.972520125615}
I0727 17:13:23.638390 140141833336640 train.py:379] starting iteration 317 3190.28134393692
I0727 17:13:33.589407 140141833336640 train.py:394] {'eval/walltime': 1313.5199921131134, 'training/sps': 41885.534310986855, 'training/walltime': 1875.9105021953583, 'training/entropy_loss': Array(-0.04859837, dtype=float32), 'training/policy_loss': Array(-0.00013437, dtype=float32), 'training/total_loss': Array(33.372116, dtype=float32), 'training/v_loss': Array(33.420853, dtype=float32), 'eval/episode_goal_distance': (Array(0.49147493, dtype=float32), Array(0.20604713, dtype=float32)), 'eval/episode_reward': (Array(-10591.711, dtype=float32), Array(4168.5386, dtype=float32)), 'eval/avg_episode_length': (Array(953.3828, dtype=float32), Array(210.20886, dtype=float32)), 'eval/epoch_eval_time': 4.079745531082153, 'eval/sps': 31374.505842291583}
I0727 17:13:33.591965 140141833336640 train.py:379] starting iteration 318 3200.234918117523
I0727 17:13:43.587545 140141833336640 train.py:394] {'eval/walltime': 1317.6092722415924, 'training/sps': 41636.42540568357, 'training/walltime': 1881.813026189804, 'training/entropy_loss': Array(-0.04875367, dtype=float32), 'training/policy_loss': Array(-0.00012782, dtype=float32), 'training/total_loss': Array(24.415201, dtype=float32), 'training/v_loss': Array(24.46408, dtype=float32), 'eval/episode_goal_distance': (Array(0.44908467, dtype=float32), Array(0.19648023, dtype=float32)), 'eval/episode_reward': (Array(-9584.148, dtype=float32), Array(4063.3718, dtype=float32)), 'eval/avg_episode_length': (Array(937.8672, dtype=float32), Array(240.63991, dtype=float32)), 'eval/epoch_eval_time': 4.089280128479004, 'eval/sps': 31301.35280010989}
I0727 17:13:43.590238 140141833336640 train.py:379] starting iteration 319 3210.2331907749176
I0727 17:13:53.564245 140141833336640 train.py:394] {'eval/walltime': 1321.7075119018555, 'training/sps': 41854.492610908244, 'training/walltime': 1887.6847972869873, 'training/entropy_loss': Array(-0.04880407, dtype=float32), 'training/policy_loss': Array(-6.8351976e-05, dtype=float32), 'training/total_loss': Array(26.033342, dtype=float32), 'training/v_loss': Array(26.082214, dtype=float32), 'eval/episode_goal_distance': (Array(0.4781613, dtype=float32), Array(0.22598769, dtype=float32)), 'eval/episode_reward': (Array(-10159.687, dtype=float32), Array(4338.624, dtype=float32)), 'eval/avg_episode_length': (Array(945.6797, dtype=float32), Array(225.84303, dtype=float32)), 'eval/epoch_eval_time': 4.0982396602630615, 'eval/sps': 31232.92208630469}
I0727 17:13:53.566913 140141833336640 train.py:379] starting iteration 320 3220.209864616394
I0727 17:14:03.592972 140141833336640 train.py:394] {'eval/walltime': 1325.8088879585266, 'training/sps': 41508.182003891605, 'training/walltime': 1893.60555768013, 'training/entropy_loss': Array(-0.04923053, dtype=float32), 'training/policy_loss': Array(-0.00025305, dtype=float32), 'training/total_loss': Array(24.955175, dtype=float32), 'training/v_loss': Array(25.004658, dtype=float32), 'eval/episode_goal_distance': (Array(0.51991606, dtype=float32), Array(0.23524177, dtype=float32)), 'eval/episode_reward': (Array(-10948.187, dtype=float32), Array(4651.519, dtype=float32)), 'eval/avg_episode_length': (Array(937.8594, dtype=float32), Array(240.67001, dtype=float32)), 'eval/epoch_eval_time': 4.101376056671143, 'eval/sps': 31209.037706210835}
I0727 17:14:03.597820 140141833336640 train.py:379] starting iteration 321 3230.2407586574554
I0727 17:14:13.520252 140141833336640 train.py:394] {'eval/walltime': 1329.8830342292786, 'training/sps': 42052.94129641295, 'training/walltime': 1899.44961977005, 'training/entropy_loss': Array(-0.04941629, dtype=float32), 'training/policy_loss': Array(-0.00011164, dtype=float32), 'training/total_loss': Array(24.942165, dtype=float32), 'training/v_loss': Array(24.991695, dtype=float32), 'eval/episode_goal_distance': (Array(0.467727, dtype=float32), Array(0.20428039, dtype=float32)), 'eval/episode_reward': (Array(-9735.025, dtype=float32), Array(4310.105, dtype=float32)), 'eval/avg_episode_length': (Array(922.3281, dtype=float32), Array(266.812, dtype=float32)), 'eval/epoch_eval_time': 4.074146270751953, 'eval/sps': 31417.62506636155}
I0727 17:14:13.522789 140141833336640 train.py:379] starting iteration 322 3240.1657423973083
I0727 17:14:23.524642 140141833336640 train.py:394] {'eval/walltime': 1333.9993793964386, 'training/sps': 41784.98272606687, 'training/walltime': 1905.3311586380005, 'training/entropy_loss': Array(-0.04939304, dtype=float32), 'training/policy_loss': Array(9.561674e-05, dtype=float32), 'training/total_loss': Array(25.083685, dtype=float32), 'training/v_loss': Array(25.132982, dtype=float32), 'eval/episode_goal_distance': (Array(0.45244053, dtype=float32), Array(0.2265478, dtype=float32)), 'eval/episode_reward': (Array(-9555.031, dtype=float32), Array(5030.401, dtype=float32)), 'eval/avg_episode_length': (Array(883.60156, dtype=float32), Array(319.47858, dtype=float32)), 'eval/epoch_eval_time': 4.116345167160034, 'eval/sps': 31095.545879188332}
I0727 17:14:23.527155 140141833336640 train.py:379] starting iteration 323 3250.1701090335846
I0727 17:14:33.521439 140141833336640 train.py:394] {'eval/walltime': 1338.1108367443085, 'training/sps': 41802.3381197402, 'training/walltime': 1911.2102556228638, 'training/entropy_loss': Array(-0.04897286, dtype=float32), 'training/policy_loss': Array(3.0717558e-05, dtype=float32), 'training/total_loss': Array(24.730328, dtype=float32), 'training/v_loss': Array(24.77927, dtype=float32), 'eval/episode_goal_distance': (Array(0.4521935, dtype=float32), Array(0.21661142, dtype=float32)), 'eval/episode_reward': (Array(-9191.982, dtype=float32), Array(4832.9634, dtype=float32)), 'eval/avg_episode_length': (Array(875.71875, dtype=float32), Array(328.8178, dtype=float32)), 'eval/epoch_eval_time': 4.111457347869873, 'eval/sps': 31132.513162593357}
I0727 17:14:33.523995 140141833336640 train.py:379] starting iteration 324 3260.1669483184814
I0727 17:14:43.482050 140141833336640 train.py:394] {'eval/walltime': 1342.202134847641, 'training/sps': 41917.18458816846, 'training/walltime': 1917.0732448101044, 'training/entropy_loss': Array(-0.04860158, dtype=float32), 'training/policy_loss': Array(0.00012272, dtype=float32), 'training/total_loss': Array(25.10493, dtype=float32), 'training/v_loss': Array(25.153408, dtype=float32), 'eval/episode_goal_distance': (Array(0.4972365, dtype=float32), Array(0.22741525, dtype=float32)), 'eval/episode_reward': (Array(-10258.974, dtype=float32), Array(4862.197, dtype=float32)), 'eval/avg_episode_length': (Array(914.5781, dtype=float32), Array(278.59067, dtype=float32)), 'eval/epoch_eval_time': 4.0912981033325195, 'eval/sps': 31285.91385109266}
I0727 17:14:43.484377 140141833336640 train.py:379] starting iteration 325 3270.1273312568665
I0727 17:14:53.450546 140141833336640 train.py:394] {'eval/walltime': 1346.2823917865753, 'training/sps': 41780.626654183725, 'training/walltime': 1922.9553968906403, 'training/entropy_loss': Array(-0.04915899, dtype=float32), 'training/policy_loss': Array(1.9072988e-05, dtype=float32), 'training/total_loss': Array(54.305225, dtype=float32), 'training/v_loss': Array(54.354362, dtype=float32), 'eval/episode_goal_distance': (Array(0.49161395, dtype=float32), Array(0.2351074, dtype=float32)), 'eval/episode_reward': (Array(-9489.525, dtype=float32), Array(5076.573, dtype=float32)), 'eval/avg_episode_length': (Array(868.03906, dtype=float32), Array(337.19666, dtype=float32)), 'eval/epoch_eval_time': 4.080256938934326, 'eval/sps': 31370.573450560885}
I0727 17:14:53.452827 140141833336640 train.py:379] starting iteration 326 3280.095781326294
I0727 17:15:03.447935 140141833336640 train.py:394] {'eval/walltime': 1350.3726162910461, 'training/sps': 41645.47885110287, 'training/walltime': 1928.8566377162933, 'training/entropy_loss': Array(-0.04908898, dtype=float32), 'training/policy_loss': Array(-0.00017774, dtype=float32), 'training/total_loss': Array(28.347809, dtype=float32), 'training/v_loss': Array(28.397076, dtype=float32), 'eval/episode_goal_distance': (Array(0.448577, dtype=float32), Array(0.20966028, dtype=float32)), 'eval/episode_reward': (Array(-9593.24, dtype=float32), Array(4694.6235, dtype=float32)), 'eval/avg_episode_length': (Array(914.5625, dtype=float32), Array(278.64188, dtype=float32)), 'eval/epoch_eval_time': 4.090224504470825, 'eval/sps': 31294.125752777003}
I0727 17:15:03.450410 140141833336640 train.py:379] starting iteration 327 3290.093364238739
I0727 17:15:13.433734 140141833336640 train.py:394] {'eval/walltime': 1354.4733662605286, 'training/sps': 41805.09984085309, 'training/walltime': 1934.7353463172913, 'training/entropy_loss': Array(-0.04908656, dtype=float32), 'training/policy_loss': Array(-9.757469e-05, dtype=float32), 'training/total_loss': Array(23.961666, dtype=float32), 'training/v_loss': Array(24.010849, dtype=float32), 'eval/episode_goal_distance': (Array(0.49127436, dtype=float32), Array(0.22351998, dtype=float32)), 'eval/episode_reward': (Array(-9870.223, dtype=float32), Array(5114.04, dtype=float32)), 'eval/avg_episode_length': (Array(875.78125, dtype=float32), Array(328.6528, dtype=float32)), 'eval/epoch_eval_time': 4.100749969482422, 'eval/sps': 31213.80258551964}
I0727 17:15:13.436260 140141833336640 train.py:379] starting iteration 328 3300.079212665558
I0727 17:15:23.410708 140141833336640 train.py:394] {'eval/walltime': 1358.5558052062988, 'training/sps': 41738.37826701566, 'training/walltime': 1940.623452425003, 'training/entropy_loss': Array(-0.0491748, dtype=float32), 'training/policy_loss': Array(0.00028869, dtype=float32), 'training/total_loss': Array(28.007153, dtype=float32), 'training/v_loss': Array(28.056038, dtype=float32), 'eval/episode_goal_distance': (Array(0.45319456, dtype=float32), Array(0.17093813, dtype=float32)), 'eval/episode_reward': (Array(-9115.304, dtype=float32), Array(4777.6895, dtype=float32)), 'eval/avg_episode_length': (Array(852.46875, dtype=float32), Array(353.36356, dtype=float32)), 'eval/epoch_eval_time': 4.082438945770264, 'eval/sps': 31353.806315368987}
I0727 17:15:23.413419 140141833336640 train.py:379] starting iteration 329 3310.0563724040985
I0727 17:15:33.378578 140141833336640 train.py:394] {'eval/walltime': 1362.6365807056427, 'training/sps': 41792.39792763592, 'training/walltime': 1946.5039477348328, 'training/entropy_loss': Array(-0.04861811, dtype=float32), 'training/policy_loss': Array(8.978969e-05, dtype=float32), 'training/total_loss': Array(23.728367, dtype=float32), 'training/v_loss': Array(23.776897, dtype=float32), 'eval/episode_goal_distance': (Array(0.46002585, dtype=float32), Array(0.23915741, dtype=float32)), 'eval/episode_reward': (Array(-9726.321, dtype=float32), Array(5206.0186, dtype=float32)), 'eval/avg_episode_length': (Array(875.8203, dtype=float32), Array(328.54892, dtype=float32)), 'eval/epoch_eval_time': 4.080775499343872, 'eval/sps': 31366.587066742704}
I0727 17:15:33.381170 140141833336640 train.py:379] starting iteration 330 3320.024122238159
I0727 17:15:43.400141 140141833336640 train.py:394] {'eval/walltime': 1366.7297358512878, 'training/sps': 41499.091245915835, 'training/walltime': 1952.4260051250458, 'training/entropy_loss': Array(-0.04802517, dtype=float32), 'training/policy_loss': Array(-3.209129e-05, dtype=float32), 'training/total_loss': Array(23.856564, dtype=float32), 'training/v_loss': Array(23.904621, dtype=float32), 'eval/episode_goal_distance': (Array(0.44963536, dtype=float32), Array(0.21433395, dtype=float32)), 'eval/episode_reward': (Array(-9580.161, dtype=float32), Array(3991.5044, dtype=float32)), 'eval/avg_episode_length': (Array(945.66406, dtype=float32), Array(225.90837, dtype=float32)), 'eval/epoch_eval_time': 4.093155145645142, 'eval/sps': 31271.719601487355}
I0727 17:15:43.402975 140141833336640 train.py:379] starting iteration 331 3330.045923948288
I0727 17:15:53.376724 140141833336640 train.py:394] {'eval/walltime': 1370.8362593650818, 'training/sps': 41913.66155216888, 'training/walltime': 1958.2894871234894, 'training/entropy_loss': Array(-0.04863, dtype=float32), 'training/policy_loss': Array(9.35067e-05, dtype=float32), 'training/total_loss': Array(24.101742, dtype=float32), 'training/v_loss': Array(24.15028, dtype=float32), 'eval/episode_goal_distance': (Array(0.47574082, dtype=float32), Array(0.22216253, dtype=float32)), 'eval/episode_reward': (Array(-10260.321, dtype=float32), Array(4875.995, dtype=float32)), 'eval/avg_episode_length': (Array(906.8828, dtype=float32), Array(289.51334, dtype=float32)), 'eval/epoch_eval_time': 4.106523513793945, 'eval/sps': 31169.917710210073}
I0727 17:15:53.379151 140141833336640 train.py:379] starting iteration 332 3340.022104024887
I0727 17:16:03.327700 140141833336640 train.py:394] {'eval/walltime': 1374.9220564365387, 'training/sps': 41946.35290165129, 'training/walltime': 1964.1483993530273, 'training/entropy_loss': Array(-0.04903507, dtype=float32), 'training/policy_loss': Array(-0.00022218, dtype=float32), 'training/total_loss': Array(25.323515, dtype=float32), 'training/v_loss': Array(25.372772, dtype=float32), 'eval/episode_goal_distance': (Array(0.43873113, dtype=float32), Array(0.19707891, dtype=float32)), 'eval/episode_reward': (Array(-9309.197, dtype=float32), Array(4517.29, dtype=float32)), 'eval/avg_episode_length': (Array(883.4922, dtype=float32), Array(319.7785, dtype=float32)), 'eval/epoch_eval_time': 4.085797071456909, 'eval/sps': 31328.036552328795}
I0727 17:16:03.330247 140141833336640 train.py:379] starting iteration 333 3349.973201274872
I0727 17:16:13.283822 140141833336640 train.py:394] {'eval/walltime': 1378.9998247623444, 'training/sps': 41851.46777601239, 'training/walltime': 1970.0205948352814, 'training/entropy_loss': Array(-0.04970425, dtype=float32), 'training/policy_loss': Array(-0.00012619, dtype=float32), 'training/total_loss': Array(50.11571, dtype=float32), 'training/v_loss': Array(50.16554, dtype=float32), 'eval/episode_goal_distance': (Array(0.48059487, dtype=float32), Array(0.23253597, dtype=float32)), 'eval/episode_reward': (Array(-9100.774, dtype=float32), Array(5367.655, dtype=float32)), 'eval/avg_episode_length': (Array(829.21094, dtype=float32), Array(374.88876, dtype=float32)), 'eval/epoch_eval_time': 4.077768325805664, 'eval/sps': 31389.718535495867}
I0727 17:16:13.286178 140141833336640 train.py:379] starting iteration 334 3359.9291326999664
I0727 17:16:23.288165 140141833336640 train.py:394] {'eval/walltime': 1383.1113011837006, 'training/sps': 41747.12273707042, 'training/walltime': 1975.9074676036835, 'training/entropy_loss': Array(-0.04948445, dtype=float32), 'training/policy_loss': Array(-0.00021462, dtype=float32), 'training/total_loss': Array(29.748795, dtype=float32), 'training/v_loss': Array(29.798492, dtype=float32), 'eval/episode_goal_distance': (Array(0.47982216, dtype=float32), Array(0.22495404, dtype=float32)), 'eval/episode_reward': (Array(-10251.376, dtype=float32), Array(4758.0557, dtype=float32)), 'eval/avg_episode_length': (Array(922.3906, dtype=float32), Array(266.59735, dtype=float32)), 'eval/epoch_eval_time': 4.111476421356201, 'eval/sps': 31132.36873623569}
I0727 17:16:23.290524 140141833336640 train.py:379] starting iteration 335 3369.9334778785706
I0727 17:16:33.265228 140141833336640 train.py:394] {'eval/walltime': 1387.195825099945, 'training/sps': 41750.45041820839, 'training/walltime': 1981.793871164322, 'training/entropy_loss': Array(-0.0485513, dtype=float32), 'training/policy_loss': Array(9.383169e-05, dtype=float32), 'training/total_loss': Array(24.349495, dtype=float32), 'training/v_loss': Array(24.397953, dtype=float32), 'eval/episode_goal_distance': (Array(0.40507165, dtype=float32), Array(0.17463967, dtype=float32)), 'eval/episode_reward': (Array(-9030.415, dtype=float32), Array(4259.237, dtype=float32)), 'eval/avg_episode_length': (Array(914.5625, dtype=float32), Array(278.642, dtype=float32)), 'eval/epoch_eval_time': 4.084523916244507, 'eval/sps': 31337.8015711777}
I0727 17:16:33.267833 140141833336640 train.py:379] starting iteration 336 3379.9107875823975
I0727 17:16:43.286265 140141833336640 train.py:394] {'eval/walltime': 1391.289789199829, 'training/sps': 41508.163617825616, 'training/walltime': 1987.714634180069, 'training/entropy_loss': Array(-0.04736372, dtype=float32), 'training/policy_loss': Array(0.0001946, dtype=float32), 'training/total_loss': Array(25.963795, dtype=float32), 'training/v_loss': Array(26.010963, dtype=float32), 'eval/episode_goal_distance': (Array(0.42554802, dtype=float32), Array(0.18764392, dtype=float32)), 'eval/episode_reward': (Array(-8939.0205, dtype=float32), Array(4881.284, dtype=float32)), 'eval/avg_episode_length': (Array(860.21875, dtype=float32), Array(345.54913, dtype=float32)), 'eval/epoch_eval_time': 4.093964099884033, 'eval/sps': 31265.540409508176}
I0727 17:16:43.288701 140141833336640 train.py:379] starting iteration 337 3389.9316551685333
I0727 17:16:53.273749 140141833336640 train.py:394] {'eval/walltime': 1395.3815224170685, 'training/sps': 41727.48027857556, 'training/walltime': 1993.604278087616, 'training/entropy_loss': Array(-0.04730302, dtype=float32), 'training/policy_loss': Array(-0.00012304, dtype=float32), 'training/total_loss': Array(22.857382, dtype=float32), 'training/v_loss': Array(22.904808, dtype=float32), 'eval/episode_goal_distance': (Array(0.41150242, dtype=float32), Array(0.17137238, dtype=float32)), 'eval/episode_reward': (Array(-8950.677, dtype=float32), Array(3942.6953, dtype=float32)), 'eval/avg_episode_length': (Array(914.6406, dtype=float32), Array(278.3869, dtype=float32)), 'eval/epoch_eval_time': 4.09173321723938, 'eval/sps': 31282.586914686326}
I0727 17:16:53.276123 140141833336640 train.py:379] starting iteration 338 3399.9190769195557
I0727 17:17:03.308094 140141833336640 train.py:394] {'eval/walltime': 1399.4637553691864, 'training/sps': 41332.16453358113, 'training/walltime': 1999.5502526760101, 'training/entropy_loss': Array(-0.04701698, dtype=float32), 'training/policy_loss': Array(0.00013974, dtype=float32), 'training/total_loss': Array(21.777462, dtype=float32), 'training/v_loss': Array(21.824339, dtype=float32), 'eval/episode_goal_distance': (Array(0.45828757, dtype=float32), Array(0.21100667, dtype=float32)), 'eval/episode_reward': (Array(-9546.295, dtype=float32), Array(4900.7397, dtype=float32)), 'eval/avg_episode_length': (Array(891.27344, dtype=float32), Array(310.25946, dtype=float32)), 'eval/epoch_eval_time': 4.08223295211792, 'eval/sps': 31355.388460521783}
I0727 17:17:03.310609 140141833336640 train.py:379] starting iteration 339 3409.9535627365112
I0727 17:17:13.296099 140141833336640 train.py:394] {'eval/walltime': 1403.5517535209656, 'training/sps': 41697.34531514137, 'training/walltime': 2005.4441530704498, 'training/entropy_loss': Array(-0.04655781, dtype=float32), 'training/policy_loss': Array(0.00062065, dtype=float32), 'training/total_loss': Array(21.702837, dtype=float32), 'training/v_loss': Array(21.748774, dtype=float32), 'eval/episode_goal_distance': (Array(0.44235143, dtype=float32), Array(0.18665802, dtype=float32)), 'eval/episode_reward': (Array(-9192.021, dtype=float32), Array(4453.913, dtype=float32)), 'eval/avg_episode_length': (Array(891.28906, dtype=float32), Array(310.21487, dtype=float32)), 'eval/epoch_eval_time': 4.087998151779175, 'eval/sps': 31311.168754881153}
I0727 17:17:13.298541 140141833336640 train.py:379] starting iteration 340 3419.9414942264557
I0727 17:17:23.328293 140141833336640 train.py:394] {'eval/walltime': 1407.6747691631317, 'training/sps': 41633.05365870208, 'training/walltime': 2011.3471550941467, 'training/entropy_loss': Array(-0.04626817, dtype=float32), 'training/policy_loss': Array(0.00162497, dtype=float32), 'training/total_loss': Array(21.859259, dtype=float32), 'training/v_loss': Array(21.9039, dtype=float32), 'eval/episode_goal_distance': (Array(0.45002297, dtype=float32), Array(0.19405732, dtype=float32)), 'eval/episode_reward': (Array(-9338.746, dtype=float32), Array(4380.8438, dtype=float32)), 'eval/avg_episode_length': (Array(906.8828, dtype=float32), Array(289.51367, dtype=float32)), 'eval/epoch_eval_time': 4.123015642166138, 'eval/sps': 31045.237541895847}
I0727 17:17:23.330708 140141833336640 train.py:379] starting iteration 341 3429.9736626148224
I0727 17:17:33.354801 140141833336640 train.py:394] {'eval/walltime': 1411.7918782234192, 'training/sps': 41631.047686329235, 'training/walltime': 2017.2504415512085, 'training/entropy_loss': Array(-0.04699198, dtype=float32), 'training/policy_loss': Array(0.00352998, dtype=float32), 'training/total_loss': Array(47.692352, dtype=float32), 'training/v_loss': Array(47.73581, dtype=float32), 'eval/episode_goal_distance': (Array(0.4479153, dtype=float32), Array(0.20618041, dtype=float32)), 'eval/episode_reward': (Array(-9963.758, dtype=float32), Array(4943.9243, dtype=float32)), 'eval/avg_episode_length': (Array(906.7656, dtype=float32), Array(289.8778, dtype=float32)), 'eval/epoch_eval_time': 4.117109060287476, 'eval/sps': 31089.776376014303}
I0727 17:17:33.357280 140141833336640 train.py:379] starting iteration 342 3440.0002341270447
I0727 17:17:43.390200 140141833336640 train.py:394] {'eval/walltime': 1415.9105064868927, 'training/sps': 41579.08092589912, 'training/walltime': 2023.1611061096191, 'training/entropy_loss': Array(-0.04693652, dtype=float32), 'training/policy_loss': Array(0.00166673, dtype=float32), 'training/total_loss': Array(33.72132, dtype=float32), 'training/v_loss': Array(33.766586, dtype=float32), 'eval/episode_goal_distance': (Array(0.42933637, dtype=float32), Array(0.21040533, dtype=float32)), 'eval/episode_reward': (Array(-8537.066, dtype=float32), Array(5481.4805, dtype=float32)), 'eval/avg_episode_length': (Array(790.27344, dtype=float32), Array(405.63263, dtype=float32)), 'eval/epoch_eval_time': 4.118628263473511, 'eval/sps': 31078.308556074728}
I0727 17:17:43.392564 140141833336640 train.py:379] starting iteration 343 3450.0355179309845
I0727 17:17:53.364600 140141833336640 train.py:394] {'eval/walltime': 1420.0054125785828, 'training/sps': 41841.76746362148, 'training/walltime': 2029.0346629619598, 'training/entropy_loss': Array(-0.04577129, dtype=float32), 'training/policy_loss': Array(0.00183354, dtype=float32), 'training/total_loss': Array(24.622759, dtype=float32), 'training/v_loss': Array(24.666698, dtype=float32), 'eval/episode_goal_distance': (Array(0.4358017, dtype=float32), Array(0.21879235, dtype=float32)), 'eval/episode_reward': (Array(-8608.102, dtype=float32), Array(4851.045, dtype=float32)), 'eval/avg_episode_length': (Array(852.4453, dtype=float32), Array(353.41953, dtype=float32)), 'eval/epoch_eval_time': 4.0949060916900635, 'eval/sps': 31258.348087580052}
I0727 17:17:53.367093 140141833336640 train.py:379] starting iteration 344 3460.0100479125977
I0727 17:18:03.318319 140141833336640 train.py:394] {'eval/walltime': 1424.0895025730133, 'training/sps': 41913.18436013149, 'training/walltime': 2034.8982117176056, 'training/entropy_loss': Array(-0.0453244, dtype=float32), 'training/policy_loss': Array(0.00168602, dtype=float32), 'training/total_loss': Array(25.626429, dtype=float32), 'training/v_loss': Array(25.670067, dtype=float32), 'eval/episode_goal_distance': (Array(0.43372115, dtype=float32), Array(0.20218773, dtype=float32)), 'eval/episode_reward': (Array(-9364.075, dtype=float32), Array(4705.647, dtype=float32)), 'eval/avg_episode_length': (Array(898.9297, dtype=float32), Array(300.6086, dtype=float32)), 'eval/epoch_eval_time': 4.084089994430542, 'eval/sps': 31341.13111477786}
I0727 17:18:03.320693 140141833336640 train.py:379] starting iteration 345 3469.963648080826
I0727 17:18:13.327934 140141833336640 train.py:394] {'eval/walltime': 1428.2111876010895, 'training/sps': 41781.608892789474, 'training/walltime': 2040.7802255153656, 'training/entropy_loss': Array(-0.04516557, dtype=float32), 'training/policy_loss': Array(0.00277082, dtype=float32), 'training/total_loss': Array(25.854149, dtype=float32), 'training/v_loss': Array(25.896544, dtype=float32), 'eval/episode_goal_distance': (Array(0.438281, dtype=float32), Array(0.19067685, dtype=float32)), 'eval/episode_reward': (Array(-8904.714, dtype=float32), Array(5102.559, dtype=float32)), 'eval/avg_episode_length': (Array(836.96094, dtype=float32), Array(368.0232, dtype=float32)), 'eval/epoch_eval_time': 4.121685028076172, 'eval/sps': 31055.259955112335}
I0727 17:18:13.330264 140141833336640 train.py:379] starting iteration 346 3479.9732191562653
I0727 17:18:23.308036 140141833336640 train.py:394] {'eval/walltime': 1432.3111097812653, 'training/sps': 41838.03463645787, 'training/walltime': 2046.6543064117432, 'training/entropy_loss': Array(-0.04487059, dtype=float32), 'training/policy_loss': Array(0.00178826, dtype=float32), 'training/total_loss': Array(22.825466, dtype=float32), 'training/v_loss': Array(22.868551, dtype=float32), 'eval/episode_goal_distance': (Array(0.4224955, dtype=float32), Array(0.1896657, dtype=float32)), 'eval/episode_reward': (Array(-8996.853, dtype=float32), Array(5139.438, dtype=float32)), 'eval/avg_episode_length': (Array(844.6094, dtype=float32), Array(361.0957, dtype=float32)), 'eval/epoch_eval_time': 4.099922180175781, 'eval/sps': 31220.104766601227}
I0727 17:18:23.310494 140141833336640 train.py:379] starting iteration 347 3489.9534482955933
I0727 17:18:33.339078 140141833336640 train.py:394] {'eval/walltime': 1436.4219100475311, 'training/sps': 41554.50302947453, 'training/walltime': 2052.568466901779, 'training/entropy_loss': Array(-0.04454345, dtype=float32), 'training/policy_loss': Array(0.00137346, dtype=float32), 'training/total_loss': Array(21.64795, dtype=float32), 'training/v_loss': Array(21.691116, dtype=float32), 'eval/episode_goal_distance': (Array(0.44373325, dtype=float32), Array(0.2084944, dtype=float32)), 'eval/episode_reward': (Array(-9436.934, dtype=float32), Array(5049.921, dtype=float32)), 'eval/avg_episode_length': (Array(867.91406, dtype=float32), Array(337.51617, dtype=float32)), 'eval/epoch_eval_time': 4.110800266265869, 'eval/sps': 31137.489468996133}
I0727 17:18:33.341381 140141833336640 train.py:379] starting iteration 348 3499.984335422516
I0727 17:18:43.286989 140141833336640 train.py:394] {'eval/walltime': 1440.5029876232147, 'training/sps': 41931.45322839868, 'training/walltime': 2058.42946100235, 'training/entropy_loss': Array(-0.04590395, dtype=float32), 'training/policy_loss': Array(0.00783426, dtype=float32), 'training/total_loss': Array(29.180002, dtype=float32), 'training/v_loss': Array(29.218073, dtype=float32), 'eval/episode_goal_distance': (Array(0.4065658, dtype=float32), Array(0.17896931, dtype=float32)), 'eval/episode_reward': (Array(-8589.111, dtype=float32), Array(4515.408, dtype=float32)), 'eval/avg_episode_length': (Array(860.10156, dtype=float32), Array(345.83844, dtype=float32)), 'eval/epoch_eval_time': 4.081077575683594, 'eval/sps': 31364.265350569716}
I0727 17:18:43.289346 140141833336640 train.py:379] starting iteration 349 3509.93230009079
I0727 17:18:53.231228 140141833336640 train.py:394] {'eval/walltime': 1444.581253528595, 'training/sps': 41937.70905098164, 'training/walltime': 2064.289580821991, 'training/entropy_loss': Array(-0.04767948, dtype=float32), 'training/policy_loss': Array(0.00012795, dtype=float32), 'training/total_loss': Array(23.103687, dtype=float32), 'training/v_loss': Array(23.151237, dtype=float32), 'eval/episode_goal_distance': (Array(0.44859976, dtype=float32), Array(0.20611812, dtype=float32)), 'eval/episode_reward': (Array(-9516.531, dtype=float32), Array(4783.486, dtype=float32)), 'eval/avg_episode_length': (Array(891.28906, dtype=float32), Array(310.21478, dtype=float32)), 'eval/epoch_eval_time': 4.078265905380249, 'eval/sps': 31385.888750200447}
I0727 17:18:53.233406 140141833336640 train.py:379] starting iteration 350 3519.876360177994
I0727 17:19:03.253382 140141833336640 train.py:394] {'eval/walltime': 1448.6940865516663, 'training/sps': 41630.025437803146, 'training/walltime': 2070.193012237549, 'training/entropy_loss': Array(-0.0477256, dtype=float32), 'training/policy_loss': Array(-0.00028188, dtype=float32), 'training/total_loss': Array(51.5485, dtype=float32), 'training/v_loss': Array(51.596504, dtype=float32), 'eval/episode_goal_distance': (Array(0.50123703, dtype=float32), Array(0.24058174, dtype=float32)), 'eval/episode_reward': (Array(-10450.478, dtype=float32), Array(4681.8965, dtype=float32)), 'eval/avg_episode_length': (Array(937.78906, dtype=float32), Array(240.94211, dtype=float32)), 'eval/epoch_eval_time': 4.112833023071289, 'eval/sps': 31122.09984747084}
I0727 17:19:03.255688 140141833336640 train.py:379] starting iteration 351 3529.8986411094666
I0727 17:19:13.244965 140141833336640 train.py:394] {'eval/walltime': 1452.7933638095856, 'training/sps': 41750.61783081693, 'training/walltime': 2076.079392194748, 'training/entropy_loss': Array(-0.04780712, dtype=float32), 'training/policy_loss': Array(-0.0001382, dtype=float32), 'training/total_loss': Array(23.597, dtype=float32), 'training/v_loss': Array(23.644947, dtype=float32), 'eval/episode_goal_distance': (Array(0.50105655, dtype=float32), Array(0.21027279, dtype=float32)), 'eval/episode_reward': (Array(-10396.425, dtype=float32), Array(4544.491, dtype=float32)), 'eval/avg_episode_length': (Array(922.3594, dtype=float32), Array(266.70477, dtype=float32)), 'eval/epoch_eval_time': 4.0992772579193115, 'eval/sps': 31225.01649594922}
I0727 17:19:13.247107 140141833336640 train.py:379] starting iteration 352 3539.890060901642
I0727 17:19:23.213978 140141833336640 train.py:394] {'eval/walltime': 1456.8710358142853, 'training/sps': 41756.97543028079, 'training/walltime': 2081.964875936508, 'training/entropy_loss': Array(-0.04792291, dtype=float32), 'training/policy_loss': Array(-0.0004965, dtype=float32), 'training/total_loss': Array(21.58318, dtype=float32), 'training/v_loss': Array(21.631598, dtype=float32), 'eval/episode_goal_distance': (Array(0.4933201, dtype=float32), Array(0.21791899, dtype=float32)), 'eval/episode_reward': (Array(-10030.967, dtype=float32), Array(4945.5522, dtype=float32)), 'eval/avg_episode_length': (Array(906.7969, dtype=float32), Array(289.78098, dtype=float32)), 'eval/epoch_eval_time': 4.077672004699707, 'eval/sps': 31390.460010632054}
I0727 17:19:23.216354 140141833336640 train.py:379] starting iteration 353 3549.8593077659607
I0727 17:19:33.177132 140141833336640 train.py:394] {'eval/walltime': 1460.9515550136566, 'training/sps': 41819.99976144405, 'training/walltime': 2087.8414900302887, 'training/entropy_loss': Array(-0.04836242, dtype=float32), 'training/policy_loss': Array(0.00037478, dtype=float32), 'training/total_loss': Array(25.623093, dtype=float32), 'training/v_loss': Array(25.671082, dtype=float32), 'eval/episode_goal_distance': (Array(0.50472844, dtype=float32), Array(0.22783734, dtype=float32)), 'eval/episode_reward': (Array(-10360.773, dtype=float32), Array(4937.1665, dtype=float32)), 'eval/avg_episode_length': (Array(914.52344, dtype=float32), Array(278.76874, dtype=float32)), 'eval/epoch_eval_time': 4.080519199371338, 'eval/sps': 31368.557221767325}
I0727 17:19:33.181256 140141833336640 train.py:379] starting iteration 354 3559.8241951465607
I0727 17:19:43.200735 140141833336640 train.py:394] {'eval/walltime': 1465.0632400512695, 'training/sps': 41627.505338865136, 'training/walltime': 2093.7452788352966, 'training/entropy_loss': Array(-0.04876472, dtype=float32), 'training/policy_loss': Array(0.00053099, dtype=float32), 'training/total_loss': Array(23.340214, dtype=float32), 'training/v_loss': Array(23.388447, dtype=float32), 'eval/episode_goal_distance': (Array(0.48229277, dtype=float32), Array(0.2232807, dtype=float32)), 'eval/episode_reward': (Array(-10765.813, dtype=float32), Array(4829.245, dtype=float32)), 'eval/avg_episode_length': (Array(914.46875, dtype=float32), Array(278.94708, dtype=float32)), 'eval/epoch_eval_time': 4.111685037612915, 'eval/sps': 31130.789160424563}
I0727 17:19:43.202927 140141833336640 train.py:379] starting iteration 355 3569.845881462097
I0727 17:19:53.208645 140141833336640 train.py:394] {'eval/walltime': 1469.1840534210205, 'training/sps': 41788.25376430589, 'training/walltime': 2099.626357316971, 'training/entropy_loss': Array(-0.04834297, dtype=float32), 'training/policy_loss': Array(0.00082381, dtype=float32), 'training/total_loss': Array(22.119007, dtype=float32), 'training/v_loss': Array(22.166523, dtype=float32), 'eval/episode_goal_distance': (Array(0.47228196, dtype=float32), Array(0.23289315, dtype=float32)), 'eval/episode_reward': (Array(-9767.857, dtype=float32), Array(4953.846, dtype=float32)), 'eval/avg_episode_length': (Array(891.2344, dtype=float32), Array(310.3706, dtype=float32)), 'eval/epoch_eval_time': 4.120813369750977, 'eval/sps': 31061.82894367165}
I0727 17:19:53.210970 140141833336640 train.py:379] starting iteration 356 3579.8539242744446
I0727 17:20:03.155529 140141833336640 train.py:394] {'eval/walltime': 1473.2678291797638, 'training/sps': 41958.66864798192, 'training/walltime': 2105.4835498332977, 'training/entropy_loss': Array(-0.04931331, dtype=float32), 'training/policy_loss': Array(0.00058034, dtype=float32), 'training/total_loss': Array(24.856205, dtype=float32), 'training/v_loss': Array(24.904938, dtype=float32), 'eval/episode_goal_distance': (Array(0.46639636, dtype=float32), Array(0.21888563, dtype=float32)), 'eval/episode_reward': (Array(-9540.621, dtype=float32), Array(5322.842, dtype=float32)), 'eval/avg_episode_length': (Array(860.16406, dtype=float32), Array(345.68423, dtype=float32)), 'eval/epoch_eval_time': 4.083775758743286, 'eval/sps': 31343.542731491667}
I0727 17:20:03.157800 140141833336640 train.py:379] starting iteration 357 3589.8007543087006
I0727 17:20:13.135290 140141833336640 train.py:394] {'eval/walltime': 1477.3586611747742, 'training/sps': 41774.06716192899, 'training/walltime': 2111.366625547409, 'training/entropy_loss': Array(-0.04872365, dtype=float32), 'training/policy_loss': Array(0.00084904, dtype=float32), 'training/total_loss': Array(26.351515, dtype=float32), 'training/v_loss': Array(26.399391, dtype=float32), 'eval/episode_goal_distance': (Array(0.44981742, dtype=float32), Array(0.20312384, dtype=float32)), 'eval/episode_reward': (Array(-8780., dtype=float32), Array(5366.536, dtype=float32)), 'eval/avg_episode_length': (Array(813.625, dtype=float32), Array(387.97125, dtype=float32)), 'eval/epoch_eval_time': 4.090831995010376, 'eval/sps': 31289.478559892643}
I0727 17:20:14.935121 140141833336640 train.py:379] starting iteration 358 3601.5780172348022
I0727 17:20:24.870942 140141833336640 train.py:394] {'eval/walltime': 1481.4473152160645, 'training/sps': 42063.93444505442, 'training/walltime': 2117.2091603279114, 'training/entropy_loss': Array(-0.04838052, dtype=float32), 'training/policy_loss': Array(0.00191366, dtype=float32), 'training/total_loss': Array(65.061844, dtype=float32), 'training/v_loss': Array(65.10831, dtype=float32), 'eval/episode_goal_distance': (Array(0.48003113, dtype=float32), Array(0.235055, dtype=float32)), 'eval/episode_reward': (Array(-9254.514, dtype=float32), Array(5770.413, dtype=float32)), 'eval/avg_episode_length': (Array(797.9922, dtype=float32), Array(400.113, dtype=float32)), 'eval/epoch_eval_time': 4.088654041290283, 'eval/sps': 31306.145911921227}
I0727 17:20:24.873397 140141833336640 train.py:379] starting iteration 359 3611.516349554062
I0727 17:20:34.843863 140141833336640 train.py:394] {'eval/walltime': 1485.5793697834015, 'training/sps': 42120.27042988624, 'training/walltime': 2123.043880701065, 'training/entropy_loss': Array(-0.0475552, dtype=float32), 'training/policy_loss': Array(0.00049557, dtype=float32), 'training/total_loss': Array(26.733353, dtype=float32), 'training/v_loss': Array(26.780415, dtype=float32), 'eval/episode_goal_distance': (Array(0.4444583, dtype=float32), Array(0.19749399, dtype=float32)), 'eval/episode_reward': (Array(-9016.453, dtype=float32), Array(5348.6445, dtype=float32)), 'eval/avg_episode_length': (Array(829.10156, dtype=float32), Array(375.1289, dtype=float32)), 'eval/epoch_eval_time': 4.132054567337036, 'eval/sps': 30977.32566549611}
I0727 17:20:34.846194 140141833336640 train.py:379] starting iteration 360 3621.489146709442
I0727 17:20:44.809239 140141833336640 train.py:394] {'eval/walltime': 1489.6829755306244, 'training/sps': 41970.1183771561, 'training/walltime': 2128.899475336075, 'training/entropy_loss': Array(-0.04660976, dtype=float32), 'training/policy_loss': Array(0.00112086, dtype=float32), 'training/total_loss': Array(22.874763, dtype=float32), 'training/v_loss': Array(22.920252, dtype=float32), 'eval/episode_goal_distance': (Array(0.46942654, dtype=float32), Array(0.2179333, dtype=float32)), 'eval/episode_reward': (Array(-10296.135, dtype=float32), Array(4835.5757, dtype=float32)), 'eval/avg_episode_length': (Array(930.0469, dtype=float32), Array(254.36649, dtype=float32)), 'eval/epoch_eval_time': 4.1036057472229, 'eval/sps': 31192.080303187875}
I0727 17:20:44.811686 140141833336640 train.py:379] starting iteration 361 3631.4546387195587
I0727 17:20:54.726200 140141833336640 train.py:394] {'eval/walltime': 1493.772388458252, 'training/sps': 42217.7404752358, 'training/walltime': 2134.7207248210907, 'training/entropy_loss': Array(-0.04647366, dtype=float32), 'training/policy_loss': Array(0.00172183, dtype=float32), 'training/total_loss': Array(28.924185, dtype=float32), 'training/v_loss': Array(28.968937, dtype=float32), 'eval/episode_goal_distance': (Array(0.4155882, dtype=float32), Array(0.19470207, dtype=float32)), 'eval/episode_reward': (Array(-8438.992, dtype=float32), Array(4905.7314, dtype=float32)), 'eval/avg_episode_length': (Array(836.8125, dtype=float32), Array(368.3577, dtype=float32)), 'eval/epoch_eval_time': 4.0894129276275635, 'eval/sps': 31300.336323399373}
I0727 17:20:54.731022 140141833336640 train.py:379] starting iteration 362 3641.373961210251
I0727 17:21:04.683970 140141833336640 train.py:394] {'eval/walltime': 1497.8787422180176, 'training/sps': 42066.23814006066, 'training/walltime': 2140.56293964386, 'training/entropy_loss': Array(-0.04627959, dtype=float32), 'training/policy_loss': Array(0.00329678, dtype=float32), 'training/total_loss': Array(23.111006, dtype=float32), 'training/v_loss': Array(23.15399, dtype=float32), 'eval/episode_goal_distance': (Array(0.43468204, dtype=float32), Array(0.16174792, dtype=float32)), 'eval/episode_reward': (Array(-8873.983, dtype=float32), Array(4879.8003, dtype=float32)), 'eval/avg_episode_length': (Array(836.8906, dtype=float32), Array(368.1815, dtype=float32)), 'eval/epoch_eval_time': 4.106353759765625, 'eval/sps': 31171.206254598423}
I0727 17:21:04.688750 140141833336640 train.py:379] starting iteration 363 3651.3316898345947
I0727 17:21:14.632913 140141833336640 train.py:394] {'eval/walltime': 1501.9678168296814, 'training/sps': 42003.81187250776, 'training/walltime': 2146.4138371944427, 'training/entropy_loss': Array(-0.04685958, dtype=float32), 'training/policy_loss': Array(0.00108007, dtype=float32), 'training/total_loss': Array(28.738478, dtype=float32), 'training/v_loss': Array(28.784256, dtype=float32), 'eval/episode_goal_distance': (Array(0.41949952, dtype=float32), Array(0.18887874, dtype=float32)), 'eval/episode_reward': (Array(-8977.568, dtype=float32), Array(4665.7095, dtype=float32)), 'eval/avg_episode_length': (Array(867.9453, dtype=float32), Array(337.43594, dtype=float32)), 'eval/epoch_eval_time': 4.089074611663818, 'eval/sps': 31302.926005529065}
I0727 17:21:14.635256 140141833336640 train.py:379] starting iteration 364 3661.278209924698
I0727 17:21:24.629895 140141833336640 train.py:394] {'eval/walltime': 1506.05650472641, 'training/sps': 41638.231743164644, 'training/walltime': 2152.3161051273346, 'training/entropy_loss': Array(-0.04564541, dtype=float32), 'training/policy_loss': Array(0.00026041, dtype=float32), 'training/total_loss': Array(19.265806, dtype=float32), 'training/v_loss': Array(19.311192, dtype=float32), 'eval/episode_goal_distance': (Array(0.43723923, dtype=float32), Array(0.16102052, dtype=float32)), 'eval/episode_reward': (Array(-9328.698, dtype=float32), Array(3899.7327, dtype=float32)), 'eval/avg_episode_length': (Array(930.02344, dtype=float32), Array(254.45148, dtype=float32)), 'eval/epoch_eval_time': 4.088687896728516, 'eval/sps': 31305.886688591887}
I0727 17:21:24.632117 140141833336640 train.py:379] starting iteration 365 3671.275068998337
I0727 17:21:34.597806 140141833336640 train.py:394] {'eval/walltime': 1510.1701254844666, 'training/sps': 42022.4699233133, 'training/walltime': 2158.1644048690796, 'training/entropy_loss': Array(-0.04562119, dtype=float32), 'training/policy_loss': Array(7.943828e-05, dtype=float32), 'training/total_loss': Array(22.350967, dtype=float32), 'training/v_loss': Array(22.39651, dtype=float32), 'eval/episode_goal_distance': (Array(0.42811424, dtype=float32), Array(0.20810738, dtype=float32)), 'eval/episode_reward': (Array(-9357.3545, dtype=float32), Array(4322.349, dtype=float32)), 'eval/avg_episode_length': (Array(922.3672, dtype=float32), Array(266.67795, dtype=float32)), 'eval/epoch_eval_time': 4.113620758056641, 'eval/sps': 31116.14014230856}
I0727 17:21:34.600643 140141833336640 train.py:379] starting iteration 366 3681.243595600128
I0727 17:21:44.583682 140141833336640 train.py:394] {'eval/walltime': 1514.268921136856, 'training/sps': 41792.5724545928, 'training/walltime': 2164.0448756217957, 'training/entropy_loss': Array(-0.04625554, dtype=float32), 'training/policy_loss': Array(-0.00023341, dtype=float32), 'training/total_loss': Array(39.264416, dtype=float32), 'training/v_loss': Array(39.310905, dtype=float32), 'eval/episode_goal_distance': (Array(0.4685309, dtype=float32), Array(0.19643702, dtype=float32)), 'eval/episode_reward': (Array(-9793.627, dtype=float32), Array(4321.792, dtype=float32)), 'eval/avg_episode_length': (Array(914.5547, dtype=float32), Array(278.6669, dtype=float32)), 'eval/epoch_eval_time': 4.098795652389526, 'eval/sps': 31228.68541284273}
I0727 17:21:44.586453 140141833336640 train.py:379] starting iteration 367 3691.2294075489044
I0727 17:21:54.565037 140141833336640 train.py:394] {'eval/walltime': 1518.3598048686981, 'training/sps': 41768.36610697387, 'training/walltime': 2169.9287543296814, 'training/entropy_loss': Array(-0.04653925, dtype=float32), 'training/policy_loss': Array(-0.00012289, dtype=float32), 'training/total_loss': Array(26.696028, dtype=float32), 'training/v_loss': Array(26.74269, dtype=float32), 'eval/episode_goal_distance': (Array(0.48835886, dtype=float32), Array(0.22346301, dtype=float32)), 'eval/episode_reward': (Array(-10226.404, dtype=float32), Array(5249.5474, dtype=float32)), 'eval/avg_episode_length': (Array(899.0078, dtype=float32), Array(300.37613, dtype=float32)), 'eval/epoch_eval_time': 4.090883731842041, 'eval/sps': 31289.0828462544}
I0727 17:21:54.567543 140141833336640 train.py:379] starting iteration 368 3701.210496902466
I0727 17:22:04.557887 140141833336640 train.py:394] {'eval/walltime': 1522.4489607810974, 'training/sps': 41672.736954247936, 'training/walltime': 2175.826135158539, 'training/entropy_loss': Array(-0.04760135, dtype=float32), 'training/policy_loss': Array(-4.4191405e-05, dtype=float32), 'training/total_loss': Array(20.177128, dtype=float32), 'training/v_loss': Array(20.224775, dtype=float32), 'eval/episode_goal_distance': (Array(0.43985906, dtype=float32), Array(0.22104804, dtype=float32)), 'eval/episode_reward': (Array(-9210.195, dtype=float32), Array(4752.3647, dtype=float32)), 'eval/avg_episode_length': (Array(891.2969, dtype=float32), Array(310.19232, dtype=float32)), 'eval/epoch_eval_time': 4.089155912399292, 'eval/sps': 31302.3036397007}
I0727 17:22:04.560385 140141833336640 train.py:379] starting iteration 369 3711.2033376693726
I0727 17:22:14.525082 140141833336640 train.py:394] {'eval/walltime': 1526.5598883628845, 'training/sps': 42011.38713869545, 'training/walltime': 2181.675977706909, 'training/entropy_loss': Array(-0.04723857, dtype=float32), 'training/policy_loss': Array(-2.5371293e-05, dtype=float32), 'training/total_loss': Array(21.936188, dtype=float32), 'training/v_loss': Array(21.983452, dtype=float32), 'eval/episode_goal_distance': (Array(0.4634174, dtype=float32), Array(0.18844168, dtype=float32)), 'eval/episode_reward': (Array(-9870.861, dtype=float32), Array(4973.472, dtype=float32)), 'eval/avg_episode_length': (Array(875.71094, dtype=float32), Array(328.83832, dtype=float32)), 'eval/epoch_eval_time': 4.110927581787109, 'eval/sps': 31136.525140235048}
I0727 17:22:14.527689 140141833336640 train.py:379] starting iteration 370 3721.1706414222717
I0727 17:22:24.468884 140141833336640 train.py:394] {'eval/walltime': 1530.6430552005768, 'training/sps': 41980.29888180379, 'training/walltime': 2187.530152320862, 'training/entropy_loss': Array(-0.04800361, dtype=float32), 'training/policy_loss': Array(-7.877597e-05, dtype=float32), 'training/total_loss': Array(26.398857, dtype=float32), 'training/v_loss': Array(26.446941, dtype=float32), 'eval/episode_goal_distance': (Array(0.4311115, dtype=float32), Array(0.16305953, dtype=float32)), 'eval/episode_reward': (Array(-8713.5, dtype=float32), Array(5151.6733, dtype=float32)), 'eval/avg_episode_length': (Array(821.4219, dtype=float32), Array(381.55743, dtype=float32)), 'eval/epoch_eval_time': 4.083166837692261, 'eval/sps': 31348.216981587633}
I0727 17:22:24.471531 140141833336640 train.py:379] starting iteration 371 3731.11448431015
I0727 17:22:34.456487 140141833336640 train.py:394] {'eval/walltime': 1534.7374477386475, 'training/sps': 41747.61644486288, 'training/walltime': 2193.416955471039, 'training/entropy_loss': Array(-0.04849578, dtype=float32), 'training/policy_loss': Array(0.0009348, dtype=float32), 'training/total_loss': Array(29.7407, dtype=float32), 'training/v_loss': Array(29.788261, dtype=float32), 'eval/episode_goal_distance': (Array(0.46672374, dtype=float32), Array(0.19425344, dtype=float32)), 'eval/episode_reward': (Array(-9473.346, dtype=float32), Array(5200.145, dtype=float32)), 'eval/avg_episode_length': (Array(836.8672, dtype=float32), Array(368.23416, dtype=float32)), 'eval/epoch_eval_time': 4.094392538070679, 'eval/sps': 31262.268776094184}
I0727 17:22:34.458903 140141833336640 train.py:379] starting iteration 372 3741.101857662201
I0727 17:22:44.470019 140141833336640 train.py:394] {'eval/walltime': 1538.8530223369598, 'training/sps': 41711.31105985776, 'training/walltime': 2199.3088824748993, 'training/entropy_loss': Array(-0.04806252, dtype=float32), 'training/policy_loss': Array(0.0020173, dtype=float32), 'training/total_loss': Array(22.404732, dtype=float32), 'training/v_loss': Array(22.450777, dtype=float32), 'eval/episode_goal_distance': (Array(0.4595768, dtype=float32), Array(0.19705655, dtype=float32)), 'eval/episode_reward': (Array(-8975.387, dtype=float32), Array(5416.166, dtype=float32)), 'eval/avg_episode_length': (Array(821.3594, dtype=float32), Array(381.69095, dtype=float32)), 'eval/epoch_eval_time': 4.115574598312378, 'eval/sps': 31101.367972405933}
I0727 17:22:44.472452 140141833336640 train.py:379] starting iteration 373 3751.115406036377
I0727 17:22:54.471588 140141833336640 train.py:394] {'eval/walltime': 1542.9637262821198, 'training/sps': 41762.38911855407, 'training/walltime': 2205.1936032772064, 'training/entropy_loss': Array(-0.04855334, dtype=float32), 'training/policy_loss': Array(0.00077296, dtype=float32), 'training/total_loss': Array(23.464844, dtype=float32), 'training/v_loss': Array(23.512625, dtype=float32), 'eval/episode_goal_distance': (Array(0.46377218, dtype=float32), Array(0.23146208, dtype=float32)), 'eval/episode_reward': (Array(-8843.415, dtype=float32), Array(5641.5947, dtype=float32)), 'eval/avg_episode_length': (Array(790.34375, dtype=float32), Array(405.4967, dtype=float32)), 'eval/epoch_eval_time': 4.110703945159912, 'eval/sps': 31138.219075764802}
I0727 17:22:54.473960 140141833336640 train.py:379] starting iteration 374 3761.1169142723083
I0727 17:23:04.445001 140141833336640 train.py:394] {'eval/walltime': 1547.0520622730255, 'training/sps': 41819.582385204616, 'training/walltime': 2211.0702760219574, 'training/entropy_loss': Array(-0.04915028, dtype=float32), 'training/policy_loss': Array(0.00157392, dtype=float32), 'training/total_loss': Array(25.831732, dtype=float32), 'training/v_loss': Array(25.879307, dtype=float32), 'eval/episode_goal_distance': (Array(0.45343137, dtype=float32), Array(0.22091079, dtype=float32)), 'eval/episode_reward': (Array(-9861.582, dtype=float32), Array(4393.2397, dtype=float32)), 'eval/avg_episode_length': (Array(937.91406, dtype=float32), Array(240.45831, dtype=float32)), 'eval/epoch_eval_time': 4.088335990905762, 'eval/sps': 31308.581360418444}
I0727 17:23:04.447415 140141833336640 train.py:379] starting iteration 375 3771.090368747711
I0727 17:23:14.431965 140141833336640 train.py:394] {'eval/walltime': 1551.1418180465698, 'training/sps': 41718.958470257596, 'training/walltime': 2216.9611229896545, 'training/entropy_loss': Array(-0.04856315, dtype=float32), 'training/policy_loss': Array(-1.1757231e-05, dtype=float32), 'training/total_loss': Array(54.505936, dtype=float32), 'training/v_loss': Array(54.554512, dtype=float32), 'eval/episode_goal_distance': (Array(0.45469728, dtype=float32), Array(0.20423172, dtype=float32)), 'eval/episode_reward': (Array(-9444.646, dtype=float32), Array(4013.319, dtype=float32)), 'eval/avg_episode_length': (Array(930.1328, dtype=float32), Array(254.05417, dtype=float32)), 'eval/epoch_eval_time': 4.0897557735443115, 'eval/sps': 31297.712403269292}
I0727 17:23:14.434603 140141833336640 train.py:379] starting iteration 376 3781.077555179596
I0727 17:23:24.384605 140141833336640 train.py:394] {'eval/walltime': 1555.2238681316376, 'training/sps': 41910.41004452344, 'training/walltime': 2222.825059890747, 'training/entropy_loss': Array(-0.04785587, dtype=float32), 'training/policy_loss': Array(-0.00026599, dtype=float32), 'training/total_loss': Array(21.714756, dtype=float32), 'training/v_loss': Array(21.762878, dtype=float32), 'eval/episode_goal_distance': (Array(0.46156856, dtype=float32), Array(0.21015477, dtype=float32)), 'eval/episode_reward': (Array(-9441.46, dtype=float32), Array(4467.259, dtype=float32)), 'eval/avg_episode_length': (Array(914.6172, dtype=float32), Array(278.46317, dtype=float32)), 'eval/epoch_eval_time': 4.082050085067749, 'eval/sps': 31356.79311437836}
I0727 17:23:24.387220 140141833336640 train.py:379] starting iteration 377 3791.030172586441
I0727 17:23:34.436103 140141833336640 train.py:394] {'eval/walltime': 1559.350263118744, 'training/sps': 41523.6303820904, 'training/walltime': 2228.7436175346375, 'training/entropy_loss': Array(-0.04648277, dtype=float32), 'training/policy_loss': Array(-0.00030839, dtype=float32), 'training/total_loss': Array(20.534966, dtype=float32), 'training/v_loss': Array(20.581755, dtype=float32), 'eval/episode_goal_distance': (Array(0.4462986, dtype=float32), Array(0.19649215, dtype=float32)), 'eval/episode_reward': (Array(-9115.953, dtype=float32), Array(5018.383, dtype=float32)), 'eval/avg_episode_length': (Array(852.41406, dtype=float32), Array(353.4939, dtype=float32)), 'eval/epoch_eval_time': 4.126394987106323, 'eval/sps': 31019.81279057372}
I0727 17:23:34.438783 140141833336640 train.py:379] starting iteration 378 3801.081735610962
I0727 17:23:44.448959 140141833336640 train.py:394] {'eval/walltime': 1563.4576785564423, 'training/sps': 41661.339435195165, 'training/walltime': 2234.6426117420197, 'training/entropy_loss': Array(-0.0453918, dtype=float32), 'training/policy_loss': Array(0.00086886, dtype=float32), 'training/total_loss': Array(24.798729, dtype=float32), 'training/v_loss': Array(24.84325, dtype=float32), 'eval/episode_goal_distance': (Array(0.4261837, dtype=float32), Array(0.17989369, dtype=float32)), 'eval/episode_reward': (Array(-9097.418, dtype=float32), Array(4622.4385, dtype=float32)), 'eval/avg_episode_length': (Array(875.72656, dtype=float32), Array(328.79718, dtype=float32)), 'eval/epoch_eval_time': 4.107415437698364, 'eval/sps': 31163.149172883815}
I0727 17:23:44.451292 140141833336640 train.py:379] starting iteration 379 3811.094245672226
I0727 17:23:54.442109 140141833336640 train.py:394] {'eval/walltime': 1567.5594828128815, 'training/sps': 41758.496196557135, 'training/walltime': 2240.5278811454773, 'training/entropy_loss': Array(-0.04365903, dtype=float32), 'training/policy_loss': Array(0.00048907, dtype=float32), 'training/total_loss': Array(23.098106, dtype=float32), 'training/v_loss': Array(23.141277, dtype=float32), 'eval/episode_goal_distance': (Array(0.41338366, dtype=float32), Array(0.16273548, dtype=float32)), 'eval/episode_reward': (Array(-8997.095, dtype=float32), Array(4362.0156, dtype=float32)), 'eval/avg_episode_length': (Array(883.46875, dtype=float32), Array(319.84293, dtype=float32)), 'eval/epoch_eval_time': 4.101804256439209, 'eval/sps': 31205.779700252508}
I0727 17:23:54.444646 140141833336640 train.py:379] starting iteration 380 3821.087598800659
I0727 17:24:04.402936 140141833336640 train.py:394] {'eval/walltime': 1571.6367619037628, 'training/sps': 41815.453184712365, 'training/walltime': 2246.40513420105, 'training/entropy_loss': Array(-0.04358463, dtype=float32), 'training/policy_loss': Array(0.00020875, dtype=float32), 'training/total_loss': Array(19.828634, dtype=float32), 'training/v_loss': Array(19.87201, dtype=float32), 'eval/episode_goal_distance': (Array(0.42342535, dtype=float32), Array(0.17502686, dtype=float32)), 'eval/episode_reward': (Array(-9431.464, dtype=float32), Array(3711.1414, dtype=float32)), 'eval/avg_episode_length': (Array(953.3594, dtype=float32), Array(210.31432, dtype=float32)), 'eval/epoch_eval_time': 4.077279090881348, 'eval/sps': 31393.485004807808}
I0727 17:24:04.405400 140141833336640 train.py:379] starting iteration 381 3831.048353433609
I0727 17:24:14.379431 140141833336640 train.py:394] {'eval/walltime': 1575.727293729782, 'training/sps': 41797.310655889465, 'training/walltime': 2252.2849383354187, 'training/entropy_loss': Array(-0.04260113, dtype=float32), 'training/policy_loss': Array(0.00069395, dtype=float32), 'training/total_loss': Array(17.502775, dtype=float32), 'training/v_loss': Array(17.544682, dtype=float32), 'eval/episode_goal_distance': (Array(0.38875276, dtype=float32), Array(0.17627521, dtype=float32)), 'eval/episode_reward': (Array(-8177.3125, dtype=float32), Array(4177.3022, dtype=float32)), 'eval/avg_episode_length': (Array(883.53906, dtype=float32), Array(319.65018, dtype=float32)), 'eval/epoch_eval_time': 4.090531826019287, 'eval/sps': 31291.774625932583}
I0727 17:24:14.381754 140141833336640 train.py:379] starting iteration 382 3841.0247082710266
I0727 17:24:24.409077 140141833336640 train.py:394] {'eval/walltime': 1579.8460912704468, 'training/sps': 41620.63587835958, 'training/walltime': 2258.1897015571594, 'training/entropy_loss': Array(-0.04249923, dtype=float32), 'training/policy_loss': Array(0.00099073, dtype=float32), 'training/total_loss': Array(17.86832, dtype=float32), 'training/v_loss': Array(17.909828, dtype=float32), 'eval/episode_goal_distance': (Array(0.41075706, dtype=float32), Array(0.16679531, dtype=float32)), 'eval/episode_reward': (Array(-9009.488, dtype=float32), Array(4595.195, dtype=float32)), 'eval/avg_episode_length': (Array(875.71875, dtype=float32), Array(328.81787, dtype=float32)), 'eval/epoch_eval_time': 4.118797540664673, 'eval/sps': 31077.031278246304}
I0727 17:24:24.414011 140141833336640 train.py:379] starting iteration 383 3851.0569496154785
I0727 17:24:34.434570 140141833336640 train.py:394] {'eval/walltime': 1583.9762499332428, 'training/sps': 41751.56652762603, 'training/walltime': 2264.0759477615356, 'training/entropy_loss': Array(-0.04427937, dtype=float32), 'training/policy_loss': Array(0.00360504, dtype=float32), 'training/total_loss': Array(53.363083, dtype=float32), 'training/v_loss': Array(53.40376, dtype=float32), 'eval/episode_goal_distance': (Array(0.44173226, dtype=float32), Array(0.16910763, dtype=float32)), 'eval/episode_reward': (Array(-8943.822, dtype=float32), Array(5040.7524, dtype=float32)), 'eval/avg_episode_length': (Array(821.34375, dtype=float32), Array(381.72424, dtype=float32)), 'eval/epoch_eval_time': 4.1301586627960205, 'eval/sps': 30991.545470881985}
I0727 17:24:34.437283 140141833336640 train.py:379] starting iteration 384 3861.080236196518
I0727 17:24:44.460206 140141833336640 train.py:394] {'eval/walltime': 1588.0722241401672, 'training/sps': 41490.88290638683, 'training/walltime': 2269.9991767406464, 'training/entropy_loss': Array(-0.0436412, dtype=float32), 'training/policy_loss': Array(0.00099418, dtype=float32), 'training/total_loss': Array(22.349672, dtype=float32), 'training/v_loss': Array(22.39232, dtype=float32), 'eval/episode_goal_distance': (Array(0.44231683, dtype=float32), Array(0.1920182, dtype=float32)), 'eval/episode_reward': (Array(-8943.49, dtype=float32), Array(5062.3726, dtype=float32)), 'eval/avg_episode_length': (Array(844.64844, dtype=float32), Array(361.00513, dtype=float32)), 'eval/epoch_eval_time': 4.0959742069244385, 'eval/sps': 31250.196786788827}
I0727 17:24:44.462609 140141833336640 train.py:379] starting iteration 385 3871.1055624485016
I0727 17:24:54.473741 140141833336640 train.py:394] {'eval/walltime': 1592.1889204978943, 'training/sps': 41720.06276513114, 'training/walltime': 2275.889867782593, 'training/entropy_loss': Array(-0.04288879, dtype=float32), 'training/policy_loss': Array(0.00118368, dtype=float32), 'training/total_loss': Array(17.662525, dtype=float32), 'training/v_loss': Array(17.704233, dtype=float32), 'eval/episode_goal_distance': (Array(0.41489115, dtype=float32), Array(0.1757766, dtype=float32)), 'eval/episode_reward': (Array(-8075.0645, dtype=float32), Array(4904.8384, dtype=float32)), 'eval/avg_episode_length': (Array(798.0625, dtype=float32), Array(399.9735, dtype=float32)), 'eval/epoch_eval_time': 4.116696357727051, 'eval/sps': 31092.89315442069}
I0727 17:24:54.476094 140141833336640 train.py:379] starting iteration 386 3881.119047641754
I0727 17:25:04.516293 140141833336640 train.py:394] {'eval/walltime': 1596.31028008461, 'training/sps': 41548.838296034824, 'training/walltime': 2281.8048346042633, 'training/entropy_loss': Array(-0.04253624, dtype=float32), 'training/policy_loss': Array(0.00201891, dtype=float32), 'training/total_loss': Array(20.286846, dtype=float32), 'training/v_loss': Array(20.327366, dtype=float32), 'eval/episode_goal_distance': (Array(0.44964567, dtype=float32), Array(0.19895001, dtype=float32)), 'eval/episode_reward': (Array(-9149.986, dtype=float32), Array(5071.529, dtype=float32)), 'eval/avg_episode_length': (Array(844.58594, dtype=float32), Array(361.1503, dtype=float32)), 'eval/epoch_eval_time': 4.121359586715698, 'eval/sps': 31057.712220156674}
I0727 17:25:04.521288 140141833336640 train.py:379] starting iteration 387 3891.164227247238
I0727 17:25:14.507490 140141833336640 train.py:394] {'eval/walltime': 1600.4171278476715, 'training/sps': 41829.81389689684, 'training/walltime': 2287.680069923401, 'training/entropy_loss': Array(-0.041939, dtype=float32), 'training/policy_loss': Array(0.00253988, dtype=float32), 'training/total_loss': Array(19.587017, dtype=float32), 'training/v_loss': Array(19.626417, dtype=float32), 'eval/episode_goal_distance': (Array(0.38701853, dtype=float32), Array(0.16600905, dtype=float32)), 'eval/episode_reward': (Array(-7689.8374, dtype=float32), Array(4714.721, dtype=float32)), 'eval/avg_episode_length': (Array(805.8047, dtype=float32), Array(394.17432, dtype=float32)), 'eval/epoch_eval_time': 4.106847763061523, 'eval/sps': 31167.456741707927}
I0727 17:25:14.509829 140141833336640 train.py:379] starting iteration 388 3901.1527831554413
I0727 17:25:24.450880 140141833336640 train.py:394] {'eval/walltime': 1604.4957993030548, 'training/sps': 41948.71372739379, 'training/walltime': 2293.538652420044, 'training/entropy_loss': Array(-0.04104133, dtype=float32), 'training/policy_loss': Array(0.00213156, dtype=float32), 'training/total_loss': Array(18.960606, dtype=float32), 'training/v_loss': Array(18.999516, dtype=float32), 'eval/episode_goal_distance': (Array(0.44542044, dtype=float32), Array(0.20495224, dtype=float32)), 'eval/episode_reward': (Array(-9217.45, dtype=float32), Array(5122.912, dtype=float32)), 'eval/avg_episode_length': (Array(844.5703, dtype=float32), Array(361.1869, dtype=float32)), 'eval/epoch_eval_time': 4.078671455383301, 'eval/sps': 31382.76799202768}
I0727 17:25:24.453304 140141833336640 train.py:379] starting iteration 389 3911.0962579250336
I0727 17:25:34.448534 140141833336640 train.py:394] {'eval/walltime': 1608.5782446861267, 'training/sps': 41591.09297608728, 'training/walltime': 2299.447609901428, 'training/entropy_loss': Array(-0.04040369, dtype=float32), 'training/policy_loss': Array(0.00189333, dtype=float32), 'training/total_loss': Array(17.971394, dtype=float32), 'training/v_loss': Array(18.009903, dtype=float32), 'eval/episode_goal_distance': (Array(0.4010315, dtype=float32), Array(0.16265973, dtype=float32)), 'eval/episode_reward': (Array(-8128.423, dtype=float32), Array(5055.976, dtype=float32)), 'eval/avg_episode_length': (Array(813.5469, dtype=float32), Array(388.1342, dtype=float32)), 'eval/epoch_eval_time': 4.082445383071899, 'eval/sps': 31353.75687590569}
I0727 17:25:34.451013 140141833336640 train.py:379] starting iteration 390 3921.09396648407
I0727 17:25:44.469119 140141833336640 train.py:394] {'eval/walltime': 1612.6952741146088, 'training/sps': 41673.11433947692, 'training/walltime': 2305.344937324524, 'training/entropy_loss': Array(-0.04090873, dtype=float32), 'training/policy_loss': Array(0.00492937, dtype=float32), 'training/total_loss': Array(24.73346, dtype=float32), 'training/v_loss': Array(24.76944, dtype=float32), 'eval/episode_goal_distance': (Array(0.4247555, dtype=float32), Array(0.1692219, dtype=float32)), 'eval/episode_reward': (Array(-8700.619, dtype=float32), Array(4946.229, dtype=float32)), 'eval/avg_episode_length': (Array(836.7344, dtype=float32), Array(368.53403, dtype=float32)), 'eval/epoch_eval_time': 4.117029428482056, 'eval/sps': 31090.377716147017}
I0727 17:25:44.471535 140141833336640 train.py:379] starting iteration 391 3931.114487886429
I0727 17:25:54.516552 140141833336640 train.py:394] {'eval/walltime': 1616.8221232891083, 'training/sps': 41552.3387916812, 'training/walltime': 2311.259405851364, 'training/entropy_loss': Array(-0.04069769, dtype=float32), 'training/policy_loss': Array(0.00716953, dtype=float32), 'training/total_loss': Array(59.834606, dtype=float32), 'training/v_loss': Array(59.868134, dtype=float32), 'eval/episode_goal_distance': (Array(0.3981548, dtype=float32), Array(0.1773533, dtype=float32)), 'eval/episode_reward': (Array(-8427.882, dtype=float32), Array(4332.837, dtype=float32)), 'eval/avg_episode_length': (Array(867.9375, dtype=float32), Array(337.456, dtype=float32)), 'eval/epoch_eval_time': 4.126849174499512, 'eval/sps': 31016.398852406168}
I0727 17:25:54.519021 140141833336640 train.py:379] starting iteration 392 3941.16197514534
I0727 17:26:04.508848 140141833336640 train.py:394] {'eval/walltime': 1620.9339447021484, 'training/sps': 41836.54882372165, 'training/walltime': 2317.1336953639984, 'training/entropy_loss': Array(-0.04119555, dtype=float32), 'training/policy_loss': Array(0.00195504, dtype=float32), 'training/total_loss': Array(28.901379, dtype=float32), 'training/v_loss': Array(28.940619, dtype=float32), 'eval/episode_goal_distance': (Array(0.42818773, dtype=float32), Array(0.19086309, dtype=float32)), 'eval/episode_reward': (Array(-8772.946, dtype=float32), Array(4804.4233, dtype=float32)), 'eval/avg_episode_length': (Array(860.1406, dtype=float32), Array(345.74194, dtype=float32)), 'eval/epoch_eval_time': 4.111821413040161, 'eval/sps': 31129.756655788347}
I0727 17:26:04.511339 140141833336640 train.py:379] starting iteration 393 3951.154292345047
I0727 17:26:14.515075 140141833336640 train.py:394] {'eval/walltime': 1625.0357208251953, 'training/sps': 41666.66435075644, 'training/walltime': 2323.0319356918335, 'training/entropy_loss': Array(-0.03887551, dtype=float32), 'training/policy_loss': Array(0.00311728, dtype=float32), 'training/total_loss': Array(19.381554, dtype=float32), 'training/v_loss': Array(19.41731, dtype=float32), 'eval/episode_goal_distance': (Array(0.42255977, dtype=float32), Array(0.17366265, dtype=float32)), 'eval/episode_reward': (Array(-7276.4146, dtype=float32), Array(5663.62, dtype=float32)), 'eval/avg_episode_length': (Array(704.7344, dtype=float32), Array(454.40497, dtype=float32)), 'eval/epoch_eval_time': 4.101776123046875, 'eval/sps': 31205.993735445325}
I0727 17:26:14.517595 140141833336640 train.py:379] starting iteration 394 3961.1605489254
I0727 17:26:24.505529 140141833336640 train.py:394] {'eval/walltime': 1629.124845981598, 'training/sps': 41689.84577349388, 'training/walltime': 2328.9268963336945, 'training/entropy_loss': Array(-0.03754349, dtype=float32), 'training/policy_loss': Array(0.00308842, dtype=float32), 'training/total_loss': Array(16.513966, dtype=float32), 'training/v_loss': Array(16.54842, dtype=float32), 'eval/episode_goal_distance': (Array(0.43006548, dtype=float32), Array(0.1642861, dtype=float32)), 'eval/episode_reward': (Array(-8059.038, dtype=float32), Array(5185.6045, dtype=float32)), 'eval/avg_episode_length': (Array(782.3594, dtype=float32), Array(411.3029, dtype=float32)), 'eval/epoch_eval_time': 4.089125156402588, 'eval/sps': 31302.53907723581}
I0727 17:26:24.508280 140141833336640 train.py:379] starting iteration 395 3971.1512339115143
I0727 17:26:34.501704 140141833336640 train.py:394] {'eval/walltime': 1633.2210335731506, 'training/sps': 41700.337789871744, 'training/walltime': 2334.820373773575, 'training/entropy_loss': Array(-0.0352706, dtype=float32), 'training/policy_loss': Array(0.00317289, dtype=float32), 'training/total_loss': Array(16.946892, dtype=float32), 'training/v_loss': Array(16.978989, dtype=float32), 'eval/episode_goal_distance': (Array(0.42098033, dtype=float32), Array(0.16689235, dtype=float32)), 'eval/episode_reward': (Array(-8810.353, dtype=float32), Array(4787.119, dtype=float32)), 'eval/avg_episode_length': (Array(836.8281, dtype=float32), Array(368.32236, dtype=float32)), 'eval/epoch_eval_time': 4.096187591552734, 'eval/sps': 31248.56885557804}
I0727 17:26:34.504071 140141833336640 train.py:379] starting iteration 396 3981.147025346756
I0727 17:26:44.475802 140141833336640 train.py:394] {'eval/walltime': 1637.3168427944183, 'training/sps': 41852.17126600818, 'training/walltime': 2340.692470550537, 'training/entropy_loss': Array(-0.03368261, dtype=float32), 'training/policy_loss': Array(0.00347653, dtype=float32), 'training/total_loss': Array(19.347366, dtype=float32), 'training/v_loss': Array(19.377573, dtype=float32), 'eval/episode_goal_distance': (Array(0.42308003, dtype=float32), Array(0.17652795, dtype=float32)), 'eval/episode_reward': (Array(-7452.364, dtype=float32), Array(5494.6416, dtype=float32)), 'eval/avg_episode_length': (Array(720.2578, dtype=float32), Array(447.1997, dtype=float32)), 'eval/epoch_eval_time': 4.0958092212677, 'eval/sps': 31251.455594013856}
I0727 17:26:44.478285 140141833336640 train.py:379] starting iteration 397 3991.1212396621704
I0727 17:26:54.516208 140141833336640 train.py:394] {'eval/walltime': 1641.4327747821808, 'training/sps': 41525.93215268478, 'training/walltime': 2346.6107001304626, 'training/entropy_loss': Array(-0.03561787, dtype=float32), 'training/policy_loss': Array(0.00648991, dtype=float32), 'training/total_loss': Array(24.948978, dtype=float32), 'training/v_loss': Array(24.978107, dtype=float32), 'eval/episode_goal_distance': (Array(0.42139706, dtype=float32), Array(0.17647897, dtype=float32)), 'eval/episode_reward': (Array(-8308.657, dtype=float32), Array(4960.4653, dtype=float32)), 'eval/avg_episode_length': (Array(798.0703, dtype=float32), Array(399.95825, dtype=float32)), 'eval/epoch_eval_time': 4.115931987762451, 'eval/sps': 31098.6674173848}
I0727 17:26:54.518728 140141833336640 train.py:379] starting iteration 398 4001.161681652069
I0727 17:27:04.485358 140141833336640 train.py:394] {'eval/walltime': 1645.5302150249481, 'training/sps': 41898.7322951815, 'training/walltime': 2352.476271390915, 'training/entropy_loss': Array(-0.03444377, dtype=float32), 'training/policy_loss': Array(0.00348252, dtype=float32), 'training/total_loss': Array(20.083683, dtype=float32), 'training/v_loss': Array(20.114645, dtype=float32), 'eval/episode_goal_distance': (Array(0.42641994, dtype=float32), Array(0.17547967, dtype=float32)), 'eval/episode_reward': (Array(-8435.35, dtype=float32), Array(5262.8994, dtype=float32)), 'eval/avg_episode_length': (Array(782.5, dtype=float32), Array(411.0374, dtype=float32)), 'eval/epoch_eval_time': 4.097440242767334, 'eval/sps': 31239.01568203255}
I0727 17:27:04.487765 140141833336640 train.py:379] starting iteration 399 4011.1307187080383
I0727 17:27:14.426501 140141833336640 train.py:394] {'eval/walltime': 1649.6199300289154, 'training/sps': 42044.13856642998, 'training/walltime': 2358.321557044983, 'training/entropy_loss': Array(-0.03369274, dtype=float32), 'training/policy_loss': Array(0.0081543, dtype=float32), 'training/total_loss': Array(19.67616, dtype=float32), 'training/v_loss': Array(19.701698, dtype=float32), 'eval/episode_goal_distance': (Array(0.3991804, dtype=float32), Array(0.16475427, dtype=float32)), 'eval/episode_reward': (Array(-8704.1875, dtype=float32), Array(3969.476, dtype=float32)), 'eval/avg_episode_length': (Array(914.6328, dtype=float32), Array(278.41254, dtype=float32)), 'eval/epoch_eval_time': 4.089715003967285, 'eval/sps': 31298.024404104395}
I0727 17:27:14.429028 140141833336640 train.py:379] starting iteration 400 4021.0719814300537
I0727 17:27:24.397022 140141833336640 train.py:394] {'eval/walltime': 1653.6957926750183, 'training/sps': 41737.15977517801, 'training/walltime': 2364.2098350524902, 'training/entropy_loss': Array(-0.04358671, dtype=float32), 'training/policy_loss': Array(0.01190193, dtype=float32), 'training/total_loss': Array(78.83749, dtype=float32), 'training/v_loss': Array(78.86917, dtype=float32), 'eval/episode_goal_distance': (Array(0.43497375, dtype=float32), Array(0.20528395, dtype=float32)), 'eval/episode_reward': (Array(-9509.84, dtype=float32), Array(4430.4277, dtype=float32)), 'eval/avg_episode_length': (Array(930.1094, dtype=float32), Array(254.13925, dtype=float32)), 'eval/epoch_eval_time': 4.075862646102905, 'eval/sps': 31404.3948763548}
I0727 17:27:24.399644 140141833336640 train.py:379] starting iteration 401 4031.042597055435
I0727 17:27:34.387555 140141833336640 train.py:394] {'eval/walltime': 1657.7946908473969, 'training/sps': 41759.218557050015, 'training/walltime': 2370.0950026512146, 'training/entropy_loss': Array(-0.04314677, dtype=float32), 'training/policy_loss': Array(4.511827e-06, dtype=float32), 'training/total_loss': Array(19.165688, dtype=float32), 'training/v_loss': Array(19.208832, dtype=float32), 'eval/episode_goal_distance': (Array(0.4431271, dtype=float32), Array(0.1711473, dtype=float32)), 'eval/episode_reward': (Array(-9727.117, dtype=float32), Array(3847.7136, dtype=float32)), 'eval/avg_episode_length': (Array(945.66406, dtype=float32), Array(225.90793, dtype=float32)), 'eval/epoch_eval_time': 4.09889817237854, 'eval/sps': 31227.904333550003}
I0727 17:27:34.389995 140141833336640 train.py:379] starting iteration 402 4041.0329492092133
I0727 17:27:44.340365 140141833336640 train.py:394] {'eval/walltime': 1661.8726732730865, 'training/sps': 41875.510287121615, 'training/walltime': 2375.9638266563416, 'training/entropy_loss': Array(-0.04322569, dtype=float32), 'training/policy_loss': Array(0.0002004, dtype=float32), 'training/total_loss': Array(15.32723, dtype=float32), 'training/v_loss': Array(15.370256, dtype=float32), 'eval/episode_goal_distance': (Array(0.43334034, dtype=float32), Array(0.18488032, dtype=float32)), 'eval/episode_reward': (Array(-9106.107, dtype=float32), Array(4298.0166, dtype=float32)), 'eval/avg_episode_length': (Array(914.5781, dtype=float32), Array(278.59055, dtype=float32)), 'eval/epoch_eval_time': 4.077982425689697, 'eval/sps': 31388.070530576584}
I0727 17:27:44.342801 140141833336640 train.py:379] starting iteration 403 4050.9857547283173
I0727 17:27:54.314584 140141833336640 train.py:394] {'eval/walltime': 1665.9566287994385, 'training/sps': 41767.56219384454, 'training/walltime': 2381.8478186130524, 'training/entropy_loss': Array(-0.04391236, dtype=float32), 'training/policy_loss': Array(0.00014761, dtype=float32), 'training/total_loss': Array(15.024401, dtype=float32), 'training/v_loss': Array(15.068167, dtype=float32), 'eval/episode_goal_distance': (Array(0.4141883, dtype=float32), Array(0.1849162, dtype=float32)), 'eval/episode_reward': (Array(-9056.445, dtype=float32), Array(3936.3484, dtype=float32)), 'eval/avg_episode_length': (Array(922.3203, dtype=float32), Array(266.8391, dtype=float32)), 'eval/epoch_eval_time': 4.083955526351929, 'eval/sps': 31342.1630510111}
I0727 17:27:54.317018 140141833336640 train.py:379] starting iteration 404 4060.959972143173
I0727 17:28:04.313212 140141833336640 train.py:394] {'eval/walltime': 1670.0551884174347, 'training/sps': 41698.181950277816, 'training/walltime': 2387.741600751877, 'training/entropy_loss': Array(-0.04539225, dtype=float32), 'training/policy_loss': Array(1.1585871e-07, dtype=float32), 'training/total_loss': Array(18.019363, dtype=float32), 'training/v_loss': Array(18.064754, dtype=float32), 'eval/episode_goal_distance': (Array(0.43136844, dtype=float32), Array(0.18067552, dtype=float32)), 'eval/episode_reward': (Array(-9059.239, dtype=float32), Array(4269.307, dtype=float32)), 'eval/avg_episode_length': (Array(899.10156, dtype=float32), Array(300.09763, dtype=float32)), 'eval/epoch_eval_time': 4.098559617996216, 'eval/sps': 31230.483860224816}
I0727 17:28:04.315995 140141833336640 train.py:379] starting iteration 405 4070.9589490890503
I0727 17:28:14.327605 140141833336640 train.py:394] {'eval/walltime': 1674.1611125469208, 'training/sps': 41640.57315122405, 'training/walltime': 2393.6435368061066, 'training/entropy_loss': Array(-0.04605116, dtype=float32), 'training/policy_loss': Array(3.035074e-05, dtype=float32), 'training/total_loss': Array(17.786629, dtype=float32), 'training/v_loss': Array(17.832647, dtype=float32), 'eval/episode_goal_distance': (Array(0.43134752, dtype=float32), Array(0.17105052, dtype=float32)), 'eval/episode_reward': (Array(-9663.691, dtype=float32), Array(4132.3115, dtype=float32)), 'eval/avg_episode_length': (Array(922.3828, dtype=float32), Array(266.6241, dtype=float32)), 'eval/epoch_eval_time': 4.105924129486084, 'eval/sps': 31174.46790621069}
I0727 17:28:14.330076 140141833336640 train.py:379] starting iteration 406 4080.9730293750763
I0727 17:28:24.276380 140141833336640 train.py:394] {'eval/walltime': 1678.240826368332, 'training/sps': 41918.31644869288, 'training/walltime': 2399.5063676834106, 'training/entropy_loss': Array(-0.0469318, dtype=float32), 'training/policy_loss': Array(0.00010535, dtype=float32), 'training/total_loss': Array(24.740705, dtype=float32), 'training/v_loss': Array(24.78753, dtype=float32), 'eval/episode_goal_distance': (Array(0.45301157, dtype=float32), Array(0.20625995, dtype=float32)), 'eval/episode_reward': (Array(-9233.331, dtype=float32), Array(4246.1323, dtype=float32)), 'eval/avg_episode_length': (Array(906.7422, dtype=float32), Array(289.95062, dtype=float32)), 'eval/epoch_eval_time': 4.079713821411133, 'eval/sps': 31374.74970137147}
I0727 17:28:24.278819 140141833336640 train.py:379] starting iteration 407 4090.9217731952667
I0727 17:28:34.261168 140141833336640 train.py:394] {'eval/walltime': 1682.3247294425964, 'training/sps': 41690.37185077662, 'training/walltime': 2405.401253938675, 'training/entropy_loss': Array(-0.04707414, dtype=float32), 'training/policy_loss': Array(0.00016495, dtype=float32), 'training/total_loss': Array(19.799097, dtype=float32), 'training/v_loss': Array(19.846004, dtype=float32), 'eval/episode_goal_distance': (Array(0.41521204, dtype=float32), Array(0.1711599, dtype=float32)), 'eval/episode_reward': (Array(-9314.014, dtype=float32), Array(3885.951, dtype=float32)), 'eval/avg_episode_length': (Array(937.89844, dtype=float32), Array(240.51894, dtype=float32)), 'eval/epoch_eval_time': 4.083903074264526, 'eval/sps': 31342.565597752742}
I0727 17:28:34.263857 140141833336640 train.py:379] starting iteration 408 4100.906809806824
I0727 17:28:44.279549 140141833336640 train.py:394] {'eval/walltime': 1686.437933921814, 'training/sps': 41663.34327594475, 'training/walltime': 2411.299964427948, 'training/entropy_loss': Array(-0.04678137, dtype=float32), 'training/policy_loss': Array(-0.00020419, dtype=float32), 'training/total_loss': Array(54.498573, dtype=float32), 'training/v_loss': Array(54.545563, dtype=float32), 'eval/episode_goal_distance': (Array(0.43553147, dtype=float32), Array(0.20302221, dtype=float32)), 'eval/episode_reward': (Array(-9250.576, dtype=float32), Array(4826.3647, dtype=float32)), 'eval/avg_episode_length': (Array(883.4453, dtype=float32), Array(319.90738, dtype=float32)), 'eval/epoch_eval_time': 4.113204479217529, 'eval/sps': 31119.289266248667}
I0727 17:28:44.468331 140141833336640 train.py:379] starting iteration 409 4111.11127781868
I0727 17:28:54.493776 140141833336640 train.py:394] {'eval/walltime': 1690.5498840808868, 'training/sps': 41586.580932891586, 'training/walltime': 2417.2095630168915, 'training/entropy_loss': Array(-0.04625294, dtype=float32), 'training/policy_loss': Array(-0.00011474, dtype=float32), 'training/total_loss': Array(22.39393, dtype=float32), 'training/v_loss': Array(22.4403, dtype=float32), 'eval/episode_goal_distance': (Array(0.4499314, dtype=float32), Array(0.21903019, dtype=float32)), 'eval/episode_reward': (Array(-9754.094, dtype=float32), Array(4320.757, dtype=float32)), 'eval/avg_episode_length': (Array(953.4375, dtype=float32), Array(209.96214, dtype=float32)), 'eval/epoch_eval_time': 4.111950159072876, 'eval/sps': 31128.781976496583}
I0727 17:28:54.496296 140141833336640 train.py:379] starting iteration 410 4121.13925075531
I0727 17:29:04.452174 140141833336640 train.py:394] {'eval/walltime': 1694.6382920742035, 'training/sps': 41911.46315056576, 'training/walltime': 2423.073352575302, 'training/entropy_loss': Array(-0.04580671, dtype=float32), 'training/policy_loss': Array(-0.00023575, dtype=float32), 'training/total_loss': Array(18.701067, dtype=float32), 'training/v_loss': Array(18.74711, dtype=float32), 'eval/episode_goal_distance': (Array(0.44433686, dtype=float32), Array(0.17812712, dtype=float32)), 'eval/episode_reward': (Array(-9354.344, dtype=float32), Array(4161.304, dtype=float32)), 'eval/avg_episode_length': (Array(914.65625, dtype=float32), Array(278.33627, dtype=float32)), 'eval/epoch_eval_time': 4.08840799331665, 'eval/sps': 31308.029973829056}
I0727 17:29:04.454654 140141833336640 train.py:379] starting iteration 411 4131.097608327866
I0727 17:29:14.422872 140141833336640 train.py:394] {'eval/walltime': 1698.7171185016632, 'training/sps': 41756.06370027571, 'training/walltime': 2428.9589648246765, 'training/entropy_loss': Array(-0.04550133, dtype=float32), 'training/policy_loss': Array(-0.00010576, dtype=float32), 'training/total_loss': Array(17.55376, dtype=float32), 'training/v_loss': Array(17.599365, dtype=float32), 'eval/episode_goal_distance': (Array(0.43435547, dtype=float32), Array(0.1768265, dtype=float32)), 'eval/episode_reward': (Array(-9383.23, dtype=float32), Array(4158.3184, dtype=float32)), 'eval/avg_episode_length': (Array(937.875, dtype=float32), Array(240.60934, dtype=float32)), 'eval/epoch_eval_time': 4.078826427459717, 'eval/sps': 31381.57562632987}
I0727 17:29:14.425281 140141833336640 train.py:379] starting iteration 412 4141.068235397339
I0727 17:29:24.389061 140141833336640 train.py:394] {'eval/walltime': 1702.8244762420654, 'training/sps': 41990.92735844218, 'training/walltime': 2434.81165766716, 'training/entropy_loss': Array(-0.04586332, dtype=float32), 'training/policy_loss': Array(8.348519e-06, dtype=float32), 'training/total_loss': Array(19.457865, dtype=float32), 'training/v_loss': Array(19.50372, dtype=float32), 'eval/episode_goal_distance': (Array(0.46519768, dtype=float32), Array(0.20034386, dtype=float32)), 'eval/episode_reward': (Array(-9350.836, dtype=float32), Array(4476.363, dtype=float32)), 'eval/avg_episode_length': (Array(899.0078, dtype=float32), Array(300.37653, dtype=float32)), 'eval/epoch_eval_time': 4.107357740402222, 'eval/sps': 31163.58693106321}
I0727 17:29:24.391552 140141833336640 train.py:379] starting iteration 413 4151.034506320953
I0727 17:29:34.373380 140141833336640 train.py:394] {'eval/walltime': 1706.911542892456, 'training/sps': 41717.975798290354, 'training/walltime': 2440.70264339447, 'training/entropy_loss': Array(-0.04681313, dtype=float32), 'training/policy_loss': Array(-0.00016079, dtype=float32), 'training/total_loss': Array(18.924816, dtype=float32), 'training/v_loss': Array(18.971792, dtype=float32), 'eval/episode_goal_distance': (Array(0.46404588, dtype=float32), Array(0.20030144, dtype=float32)), 'eval/episode_reward': (Array(-10150.99, dtype=float32), Array(4597.763, dtype=float32)), 'eval/avg_episode_length': (Array(922.40625, dtype=float32), Array(266.5438, dtype=float32)), 'eval/epoch_eval_time': 4.087066650390625, 'eval/sps': 31318.30502146724}
I0727 17:29:34.375927 140141833336640 train.py:379] starting iteration 414 4161.018881320953
I0727 17:29:44.320919 140141833336640 train.py:394] {'eval/walltime': 1710.99542593956, 'training/sps': 41957.29551015258, 'training/walltime': 2446.5600275993347, 'training/entropy_loss': Array(-0.04638809, dtype=float32), 'training/policy_loss': Array(-0.00038351, dtype=float32), 'training/total_loss': Array(24.059006, dtype=float32), 'training/v_loss': Array(24.105778, dtype=float32), 'eval/episode_goal_distance': (Array(0.46423954, dtype=float32), Array(0.21282952, dtype=float32)), 'eval/episode_reward': (Array(-9960.303, dtype=float32), Array(4406.848, dtype=float32)), 'eval/avg_episode_length': (Array(937.85156, dtype=float32), Array(240.70003, dtype=float32)), 'eval/epoch_eval_time': 4.083883047103882, 'eval/sps': 31342.719300145538}
I0727 17:29:44.323439 140141833336640 train.py:379] starting iteration 415 4170.966393470764
I0727 17:29:54.311686 140141833336640 train.py:394] {'eval/walltime': 1715.0861639976501, 'training/sps': 41698.53449408892, 'training/walltime': 2452.453759908676, 'training/entropy_loss': Array(-0.04593479, dtype=float32), 'training/policy_loss': Array(0.00018387, dtype=float32), 'training/total_loss': Array(20.5246, dtype=float32), 'training/v_loss': Array(20.57035, dtype=float32), 'eval/episode_goal_distance': (Array(0.45560518, dtype=float32), Array(0.21192147, dtype=float32)), 'eval/episode_reward': (Array(-9022.326, dtype=float32), Array(4611.362, dtype=float32)), 'eval/avg_episode_length': (Array(875.7578, dtype=float32), Array(328.7148, dtype=float32)), 'eval/epoch_eval_time': 4.09073805809021, 'eval/sps': 31290.197070148683}
I0727 17:29:54.314218 140141833336640 train.py:379] starting iteration 416 4180.95717215538
I0727 17:30:04.320430 140141833336640 train.py:394] {'eval/walltime': 1719.1935505867004, 'training/sps': 41689.319709487776, 'training/walltime': 2458.348794937134, 'training/entropy_loss': Array(-0.0472264, dtype=float32), 'training/policy_loss': Array(-0.00049104, dtype=float32), 'training/total_loss': Array(45.065712, dtype=float32), 'training/v_loss': Array(45.113434, dtype=float32), 'eval/episode_goal_distance': (Array(0.4551143, dtype=float32), Array(0.18318208, dtype=float32)), 'eval/episode_reward': (Array(-9485.855, dtype=float32), Array(4328.436, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.30652, dtype=float32)), 'eval/epoch_eval_time': 4.107386589050293, 'eval/sps': 31163.368050436195}
I0727 17:30:04.322827 140141833336640 train.py:379] starting iteration 417 4190.965779781342
I0727 17:30:14.316947 140141833336640 train.py:394] {'eval/walltime': 1723.2922503948212, 'training/sps': 41713.1914213425, 'training/walltime': 2464.240456342697, 'training/entropy_loss': Array(-0.04752715, dtype=float32), 'training/policy_loss': Array(-0.00035547, dtype=float32), 'training/total_loss': Array(34.21486, dtype=float32), 'training/v_loss': Array(34.26274, dtype=float32), 'eval/episode_goal_distance': (Array(0.4728723, dtype=float32), Array(0.19019313, dtype=float32)), 'eval/episode_reward': (Array(-9768.155, dtype=float32), Array(4813.73, dtype=float32)), 'eval/avg_episode_length': (Array(883.53125, dtype=float32), Array(319.67178, dtype=float32)), 'eval/epoch_eval_time': 4.0986998081207275, 'eval/sps': 31229.41566649854}
I0727 17:30:14.319311 140141833336640 train.py:379] starting iteration 418 4200.962264299393
I0727 17:30:24.219349 140141833336640 train.py:394] {'eval/walltime': 1727.3635623455048, 'training/sps': 42190.68020260522, 'training/walltime': 2470.0654394626617, 'training/entropy_loss': Array(-0.04714151, dtype=float32), 'training/policy_loss': Array(-0.00017844, dtype=float32), 'training/total_loss': Array(19.913465, dtype=float32), 'training/v_loss': Array(19.960785, dtype=float32), 'eval/episode_goal_distance': (Array(0.45049167, dtype=float32), Array(0.18183926, dtype=float32)), 'eval/episode_reward': (Array(-9445.42, dtype=float32), Array(4437.224, dtype=float32)), 'eval/avg_episode_length': (Array(914.6797, dtype=float32), Array(278.25946, dtype=float32)), 'eval/epoch_eval_time': 4.071311950683594, 'eval/sps': 31439.49703448987}
I0727 17:30:24.221709 140141833336640 train.py:379] starting iteration 419 4210.864664316177
I0727 17:30:34.253110 140141833336640 train.py:394] {'eval/walltime': 1731.4845983982086, 'training/sps': 41607.73172803013, 'training/walltime': 2475.9720339775085, 'training/entropy_loss': Array(-0.04623023, dtype=float32), 'training/policy_loss': Array(-0.00024475, dtype=float32), 'training/total_loss': Array(17.285702, dtype=float32), 'training/v_loss': Array(17.332176, dtype=float32), 'eval/episode_goal_distance': (Array(0.43785417, dtype=float32), Array(0.21145947, dtype=float32)), 'eval/episode_reward': (Array(-8859.279, dtype=float32), Array(4919.3525, dtype=float32)), 'eval/avg_episode_length': (Array(868.0625, dtype=float32), Array(337.13663, dtype=float32)), 'eval/epoch_eval_time': 4.121036052703857, 'eval/sps': 31060.150496867842}
I0727 17:30:34.255465 140141833336640 train.py:379] starting iteration 420 4220.898419380188
I0727 17:30:44.236308 140141833336640 train.py:394] {'eval/walltime': 1735.590839624405, 'training/sps': 41862.39493059919, 'training/walltime': 2481.8426966667175, 'training/entropy_loss': Array(-0.04685779, dtype=float32), 'training/policy_loss': Array(9.870564e-07, dtype=float32), 'training/total_loss': Array(18.52721, dtype=float32), 'training/v_loss': Array(18.574066, dtype=float32), 'eval/episode_goal_distance': (Array(0.48758578, dtype=float32), Array(0.24718043, dtype=float32)), 'eval/episode_reward': (Array(-10138.018, dtype=float32), Array(5076.136, dtype=float32)), 'eval/avg_episode_length': (Array(898.91406, dtype=float32), Array(300.6549, dtype=float32)), 'eval/epoch_eval_time': 4.106241226196289, 'eval/sps': 31172.060516904778}
I0727 17:30:44.238764 140141833336640 train.py:379] starting iteration 421 4230.881718158722
I0727 17:30:54.206007 140141833336640 train.py:394] {'eval/walltime': 1739.6766810417175, 'training/sps': 41812.629036934, 'training/walltime': 2487.7203466892242, 'training/entropy_loss': Array(-0.04762691, dtype=float32), 'training/policy_loss': Array(0.0005849, dtype=float32), 'training/total_loss': Array(20.470253, dtype=float32), 'training/v_loss': Array(20.517292, dtype=float32), 'eval/episode_goal_distance': (Array(0.41652387, dtype=float32), Array(0.17730615, dtype=float32)), 'eval/episode_reward': (Array(-8856.445, dtype=float32), Array(4674.726, dtype=float32)), 'eval/avg_episode_length': (Array(860.22656, dtype=float32), Array(345.52972, dtype=float32)), 'eval/epoch_eval_time': 4.085841417312622, 'eval/sps': 31327.696532135444}
I0727 17:30:54.208380 140141833336640 train.py:379] starting iteration 422 4240.85133433342
I0727 17:31:04.142117 140141833336640 train.py:394] {'eval/walltime': 1743.7646944522858, 'training/sps': 42068.36524793548, 'training/walltime': 2493.562266111374, 'training/entropy_loss': Array(-0.04687871, dtype=float32), 'training/policy_loss': Array(0.00088933, dtype=float32), 'training/total_loss': Array(22.865797, dtype=float32), 'training/v_loss': Array(22.911787, dtype=float32), 'eval/episode_goal_distance': (Array(0.4399082, dtype=float32), Array(0.18889993, dtype=float32)), 'eval/episode_reward': (Array(-9102.943, dtype=float32), Array(4436.4062, dtype=float32)), 'eval/avg_episode_length': (Array(883.46875, dtype=float32), Array(319.84317, dtype=float32)), 'eval/epoch_eval_time': 4.088013410568237, 'eval/sps': 31311.051883806784}
I0727 17:31:04.144694 140141833336640 train.py:379] starting iteration 423 4250.787648677826
I0727 17:31:14.103798 140141833336640 train.py:394] {'eval/walltime': 1747.8618857860565, 'training/sps': 41951.366772750545, 'training/walltime': 2499.420478105545, 'training/entropy_loss': Array(-0.0464779, dtype=float32), 'training/policy_loss': Array(0.00100133, dtype=float32), 'training/total_loss': Array(21.666565, dtype=float32), 'training/v_loss': Array(21.712044, dtype=float32), 'eval/episode_goal_distance': (Array(0.47166312, dtype=float32), Array(0.21907884, dtype=float32)), 'eval/episode_reward': (Array(-9187.715, dtype=float32), Array(5144.087, dtype=float32)), 'eval/avg_episode_length': (Array(844.78906, dtype=float32), Array(360.67844, dtype=float32)), 'eval/epoch_eval_time': 4.097191333770752, 'eval/sps': 31240.913487483696}
I0727 17:31:14.106186 140141833336640 train.py:379] starting iteration 424 4260.749140262604
I0727 17:31:24.072721 140141833336640 train.py:394] {'eval/walltime': 1751.9544613361359, 'training/sps': 41865.8124307968, 'training/walltime': 2505.29066157341, 'training/entropy_loss': Array(-0.04631122, dtype=float32), 'training/policy_loss': Array(0.00052815, dtype=float32), 'training/total_loss': Array(20.731617, dtype=float32), 'training/v_loss': Array(20.777401, dtype=float32), 'eval/episode_goal_distance': (Array(0.43960255, dtype=float32), Array(0.20586926, dtype=float32)), 'eval/episode_reward': (Array(-8114.623, dtype=float32), Array(5341.774, dtype=float32)), 'eval/avg_episode_length': (Array(782.4219, dtype=float32), Array(411.18463, dtype=float32)), 'eval/epoch_eval_time': 4.092575550079346, 'eval/sps': 31276.14834074801}
I0727 17:31:24.075086 140141833336640 train.py:379] starting iteration 425 4270.718039989471
I0727 17:31:34.053135 140141833336640 train.py:394] {'eval/walltime': 1756.0754771232605, 'training/sps': 41987.1490582025, 'training/walltime': 2511.143881082535, 'training/entropy_loss': Array(-0.04572, dtype=float32), 'training/policy_loss': Array(0.0031617, dtype=float32), 'training/total_loss': Array(74.70335, dtype=float32), 'training/v_loss': Array(74.7459, dtype=float32), 'eval/episode_goal_distance': (Array(0.41351053, dtype=float32), Array(0.17472748, dtype=float32)), 'eval/episode_reward': (Array(-8221.963, dtype=float32), Array(4868.157, dtype=float32)), 'eval/avg_episode_length': (Array(813.5469, dtype=float32), Array(388.13382, dtype=float32)), 'eval/epoch_eval_time': 4.121015787124634, 'eval/sps': 31060.303238806504}
I0727 17:31:34.055514 140141833336640 train.py:379] starting iteration 426 4280.69846868515
I0727 17:31:44.050458 140141833336640 train.py:394] {'eval/walltime': 1760.1875095367432, 'training/sps': 41801.11080525579, 'training/walltime': 2517.0231506824493, 'training/entropy_loss': Array(-0.04568106, dtype=float32), 'training/policy_loss': Array(0.00102537, dtype=float32), 'training/total_loss': Array(20.697298, dtype=float32), 'training/v_loss': Array(20.741955, dtype=float32), 'eval/episode_goal_distance': (Array(0.4386369, dtype=float32), Array(0.19524905, dtype=float32)), 'eval/episode_reward': (Array(-8821.1, dtype=float32), Array(5393.688, dtype=float32)), 'eval/avg_episode_length': (Array(821.3672, dtype=float32), Array(381.67407, dtype=float32)), 'eval/epoch_eval_time': 4.112032413482666, 'eval/sps': 31128.159296680013}
I0727 17:31:44.052865 140141833336640 train.py:379] starting iteration 427 4290.695819616318
I0727 17:31:54.023811 140141833336640 train.py:394] {'eval/walltime': 1764.2871716022491, 'training/sps': 41885.83386337792, 'training/walltime': 2522.890528202057, 'training/entropy_loss': Array(-0.04512508, dtype=float32), 'training/policy_loss': Array(0.00073242, dtype=float32), 'training/total_loss': Array(18.942131, dtype=float32), 'training/v_loss': Array(18.986523, dtype=float32), 'eval/episode_goal_distance': (Array(0.43840486, dtype=float32), Array(0.18252064, dtype=float32)), 'eval/episode_reward': (Array(-8394.283, dtype=float32), Array(5007.356, dtype=float32)), 'eval/avg_episode_length': (Array(813.5547, dtype=float32), Array(388.11798, dtype=float32)), 'eval/epoch_eval_time': 4.0996620655059814, 'eval/sps': 31222.08561456204}
I0727 17:31:54.026429 140141833336640 train.py:379] starting iteration 428 4300.6693823337555
I0727 17:32:04.009597 140141833336640 train.py:394] {'eval/walltime': 1768.3799493312836, 'training/sps': 41750.181546222535, 'training/walltime': 2528.7769696712494, 'training/entropy_loss': Array(-0.04381555, dtype=float32), 'training/policy_loss': Array(0.00094302, dtype=float32), 'training/total_loss': Array(16.925001, dtype=float32), 'training/v_loss': Array(16.967875, dtype=float32), 'eval/episode_goal_distance': (Array(0.40933162, dtype=float32), Array(0.15911634, dtype=float32)), 'eval/episode_reward': (Array(-7516.118, dtype=float32), Array(5598.3467, dtype=float32)), 'eval/avg_episode_length': (Array(712.6328, dtype=float32), Array(450.66983, dtype=float32)), 'eval/epoch_eval_time': 4.092777729034424, 'eval/sps': 31274.603331610197}
I0727 17:32:04.012061 140141833336640 train.py:379] starting iteration 429 4310.655015945435
I0727 17:32:14.002526 140141833336640 train.py:394] {'eval/walltime': 1772.5042970180511, 'training/sps': 41922.13010942052, 'training/walltime': 2534.639267206192, 'training/entropy_loss': Array(-0.04481328, dtype=float32), 'training/policy_loss': Array(0.00075531, dtype=float32), 'training/total_loss': Array(19.96765, dtype=float32), 'training/v_loss': Array(20.011707, dtype=float32), 'eval/episode_goal_distance': (Array(0.43459353, dtype=float32), Array(0.20409817, dtype=float32)), 'eval/episode_reward': (Array(-8760.467, dtype=float32), Array(5378.4663, dtype=float32)), 'eval/avg_episode_length': (Array(813.3828, dtype=float32), Array(388.4749, dtype=float32)), 'eval/epoch_eval_time': 4.124347686767578, 'eval/sps': 31035.210831199078}
I0727 17:32:14.005047 140141833336640 train.py:379] starting iteration 430 4320.648001194
I0727 17:32:23.976492 140141833336640 train.py:394] {'eval/walltime': 1776.6158998012543, 'training/sps': 41967.885001582355, 'training/walltime': 2540.4951734542847, 'training/entropy_loss': Array(-0.04439723, dtype=float32), 'training/policy_loss': Array(0.00076257, dtype=float32), 'training/total_loss': Array(19.885052, dtype=float32), 'training/v_loss': Array(19.928688, dtype=float32), 'eval/episode_goal_distance': (Array(0.47836262, dtype=float32), Array(0.22656623, dtype=float32)), 'eval/episode_reward': (Array(-9448.867, dtype=float32), Array(5587.083, dtype=float32)), 'eval/avg_episode_length': (Array(836.9375, dtype=float32), Array(368.07602, dtype=float32)), 'eval/epoch_eval_time': 4.111602783203125, 'eval/sps': 31131.411945460888}
I0727 17:32:23.978971 140141833336640 train.py:379] starting iteration 431 4330.62192440033
I0727 17:32:33.923328 140141833336640 train.py:394] {'eval/walltime': 1780.704810142517, 'training/sps': 41997.42167113609, 'training/walltime': 2546.346961259842, 'training/entropy_loss': Array(-0.04426657, dtype=float32), 'training/policy_loss': Array(0.00079407, dtype=float32), 'training/total_loss': Array(24.446949, dtype=float32), 'training/v_loss': Array(24.490421, dtype=float32), 'eval/episode_goal_distance': (Array(0.47618574, dtype=float32), Array(0.22280163, dtype=float32)), 'eval/episode_reward': (Array(-8805.598, dtype=float32), Array(6179.235, dtype=float32)), 'eval/avg_episode_length': (Array(743.6953, dtype=float32), Array(434.8729, dtype=float32)), 'eval/epoch_eval_time': 4.088910341262817, 'eval/sps': 31304.183588547097}
I0727 17:32:33.925644 140141833336640 train.py:379] starting iteration 432 4340.568598747253
I0727 17:32:43.905480 140141833336640 train.py:394] {'eval/walltime': 1784.8235414028168, 'training/sps': 41957.59779770594, 'training/walltime': 2552.204303264618, 'training/entropy_loss': Array(-0.04463093, dtype=float32), 'training/policy_loss': Array(0.00106002, dtype=float32), 'training/total_loss': Array(23.660997, dtype=float32), 'training/v_loss': Array(23.704569, dtype=float32), 'eval/episode_goal_distance': (Array(0.4463379, dtype=float32), Array(0.20561004, dtype=float32)), 'eval/episode_reward': (Array(-8037.5464, dtype=float32), Array(5783.399, dtype=float32)), 'eval/avg_episode_length': (Array(743.53906, dtype=float32), Array(435.1375, dtype=float32)), 'eval/epoch_eval_time': 4.118731260299683, 'eval/sps': 31077.531382974135}
I0727 17:32:43.907927 140141833336640 train.py:379] starting iteration 433 4350.550880908966
I0727 17:32:53.875541 140141833336640 train.py:394] {'eval/walltime': 1788.9184665679932, 'training/sps': 41874.673325499985, 'training/walltime': 2558.073244571686, 'training/entropy_loss': Array(-0.04174755, dtype=float32), 'training/policy_loss': Array(0.00253293, dtype=float32), 'training/total_loss': Array(83.00659, dtype=float32), 'training/v_loss': Array(83.04581, dtype=float32), 'eval/episode_goal_distance': (Array(0.46364132, dtype=float32), Array(0.21876615, dtype=float32)), 'eval/episode_reward': (Array(-8113.9756, dtype=float32), Array(5659.4946, dtype=float32)), 'eval/avg_episode_length': (Array(743.6094, dtype=float32), Array(435.01837, dtype=float32)), 'eval/epoch_eval_time': 4.094925165176392, 'eval/sps': 31258.202491347925}
I0727 17:32:53.878092 140141833336640 train.py:379] starting iteration 434 4360.521045923233
I0727 17:33:03.871554 140141833336640 train.py:394] {'eval/walltime': 1793.0073840618134, 'training/sps': 41648.86270789084, 'training/walltime': 2563.9740059375763, 'training/entropy_loss': Array(-0.04031469, dtype=float32), 'training/policy_loss': Array(0.00083851, dtype=float32), 'training/total_loss': Array(24.560692, dtype=float32), 'training/v_loss': Array(24.60017, dtype=float32), 'eval/episode_goal_distance': (Array(0.46043855, dtype=float32), Array(0.21237592, dtype=float32)), 'eval/episode_reward': (Array(-8812.582, dtype=float32), Array(5588.773, dtype=float32)), 'eval/avg_episode_length': (Array(790.1406, dtype=float32), Array(405.88947, dtype=float32)), 'eval/epoch_eval_time': 4.08891749382019, 'eval/sps': 31304.128829562727}
I0727 17:33:03.873974 140141833336640 train.py:379] starting iteration 435 4370.516927242279
I0727 17:33:13.855357 140141833336640 train.py:394] {'eval/walltime': 1797.1006546020508, 'training/sps': 41763.8138285627, 'training/walltime': 2569.85852599144, 'training/entropy_loss': Array(-0.03921447, dtype=float32), 'training/policy_loss': Array(0.00140344, dtype=float32), 'training/total_loss': Array(22.18415, dtype=float32), 'training/v_loss': Array(22.221962, dtype=float32), 'eval/episode_goal_distance': (Array(0.4747727, dtype=float32), Array(0.22247946, dtype=float32)), 'eval/episode_reward': (Array(-8479.959, dtype=float32), Array(5598.923, dtype=float32)), 'eval/avg_episode_length': (Array(766.8828, dtype=float32), Array(421.33496, dtype=float32)), 'eval/epoch_eval_time': 4.093270540237427, 'eval/sps': 31270.838011253334}
I0727 17:33:13.857766 140141833336640 train.py:379] starting iteration 436 4380.500719547272
I0727 17:33:23.837211 140141833336640 train.py:394] {'eval/walltime': 1801.2202653884888, 'training/sps': 41966.71970793416, 'training/walltime': 2575.7145948410034, 'training/entropy_loss': Array(-0.03777463, dtype=float32), 'training/policy_loss': Array(0.00124275, dtype=float32), 'training/total_loss': Array(21.229057, dtype=float32), 'training/v_loss': Array(21.265589, dtype=float32), 'eval/episode_goal_distance': (Array(0.4604854, dtype=float32), Array(0.19667087, dtype=float32)), 'eval/episode_reward': (Array(-8572.035, dtype=float32), Array(5134.6313, dtype=float32)), 'eval/avg_episode_length': (Array(813.5703, dtype=float32), Array(388.0851, dtype=float32)), 'eval/epoch_eval_time': 4.119610786437988, 'eval/sps': 31070.89641122988}
I0727 17:33:23.839709 140141833336640 train.py:379] starting iteration 437 4390.482664108276
I0727 17:33:33.774026 140141833336640 train.py:394] {'eval/walltime': 1805.304184436798, 'training/sps': 42034.4258795061, 'training/walltime': 2581.561231136322, 'training/entropy_loss': Array(-0.03635134, dtype=float32), 'training/policy_loss': Array(0.0009848, dtype=float32), 'training/total_loss': Array(21.152084, dtype=float32), 'training/v_loss': Array(21.18745, dtype=float32), 'eval/episode_goal_distance': (Array(0.46031213, dtype=float32), Array(0.21193397, dtype=float32)), 'eval/episode_reward': (Array(-8606.035, dtype=float32), Array(5807.591, dtype=float32)), 'eval/avg_episode_length': (Array(767., dtype=float32), Array(421.12308, dtype=float32)), 'eval/epoch_eval_time': 4.083919048309326, 'eval/sps': 31342.44300287731}
I0727 17:33:33.776356 140141833336640 train.py:379] starting iteration 438 4400.419310808182
I0727 17:33:43.724892 140141833336640 train.py:394] {'eval/walltime': 1809.3790290355682, 'training/sps': 41867.24420616777, 'training/walltime': 2587.4312138557434, 'training/entropy_loss': Array(-0.03468152, dtype=float32), 'training/policy_loss': Array(0.00155865, dtype=float32), 'training/total_loss': Array(20.705948, dtype=float32), 'training/v_loss': Array(20.73907, dtype=float32), 'eval/episode_goal_distance': (Array(0.4209471, dtype=float32), Array(0.20184116, dtype=float32)), 'eval/episode_reward': (Array(-7734.335, dtype=float32), Array(5482.369, dtype=float32)), 'eval/avg_episode_length': (Array(743.72656, dtype=float32), Array(434.81952, dtype=float32)), 'eval/epoch_eval_time': 4.074844598770142, 'eval/sps': 31412.240859107267}
I0727 17:33:43.727207 140141833336640 train.py:379] starting iteration 439 4410.370161056519
I0727 17:33:53.648309 140141833336640 train.py:394] {'eval/walltime': 1813.469111442566, 'training/sps': 42173.62035670857, 'training/walltime': 2593.2585532665253, 'training/entropy_loss': Array(-0.0377027, dtype=float32), 'training/policy_loss': Array(0.00609599, dtype=float32), 'training/total_loss': Array(21.395023, dtype=float32), 'training/v_loss': Array(21.42663, dtype=float32), 'eval/episode_goal_distance': (Array(0.4483463, dtype=float32), Array(0.20762557, dtype=float32)), 'eval/episode_reward': (Array(-9847.593, dtype=float32), Array(4471.4688, dtype=float32)), 'eval/avg_episode_length': (Array(930.10156, dtype=float32), Array(254.16763, dtype=float32)), 'eval/epoch_eval_time': 4.090082406997681, 'eval/sps': 31295.212971994424}
I0727 17:33:53.650659 140141833336640 train.py:379] starting iteration 440 4420.293612957001
I0727 17:34:03.655903 140141833336640 train.py:394] {'eval/walltime': 1817.5602660179138, 'training/sps': 41580.28181867761, 'training/walltime': 2599.1690471172333, 'training/entropy_loss': Array(-0.04730774, dtype=float32), 'training/policy_loss': Array(1.2279896e-05, dtype=float32), 'training/total_loss': Array(46.516678, dtype=float32), 'training/v_loss': Array(46.56397, dtype=float32), 'eval/episode_goal_distance': (Array(0.48279992, dtype=float32), Array(0.22721143, dtype=float32)), 'eval/episode_reward': (Array(-9913.878, dtype=float32), Array(4546.5977, dtype=float32)), 'eval/avg_episode_length': (Array(922.2578, dtype=float32), Array(267.05344, dtype=float32)), 'eval/epoch_eval_time': 4.0911545753479, 'eval/sps': 31287.011439580045}
I0727 17:34:03.658244 140141833336640 train.py:379] starting iteration 441 4430.301198005676
I0727 17:34:13.579277 140141833336640 train.py:394] {'eval/walltime': 1821.6434581279755, 'training/sps': 42125.3707265186, 'training/walltime': 2605.003061056137, 'training/entropy_loss': Array(-0.04798964, dtype=float32), 'training/policy_loss': Array(-5.2428004e-05, dtype=float32), 'training/total_loss': Array(42.137276, dtype=float32), 'training/v_loss': Array(42.18532, dtype=float32), 'eval/episode_goal_distance': (Array(0.44997633, dtype=float32), Array(0.20081943, dtype=float32)), 'eval/episode_reward': (Array(-9709.099, dtype=float32), Array(4857.14, dtype=float32)), 'eval/avg_episode_length': (Array(883.5469, dtype=float32), Array(319.62866, dtype=float32)), 'eval/epoch_eval_time': 4.0831921100616455, 'eval/sps': 31348.022956007215}
I0727 17:34:13.581595 140141833336640 train.py:379] starting iteration 442 4440.224549770355
I0727 17:34:23.550662 140141833336640 train.py:394] {'eval/walltime': 1825.7324419021606, 'training/sps': 41821.81357853621, 'training/walltime': 2610.8794202804565, 'training/entropy_loss': Array(-0.04384608, dtype=float32), 'training/policy_loss': Array(-0.00037812, dtype=float32), 'training/total_loss': Array(23.258194, dtype=float32), 'training/v_loss': Array(23.30242, dtype=float32), 'eval/episode_goal_distance': (Array(0.4628241, dtype=float32), Array(0.2128847, dtype=float32)), 'eval/episode_reward': (Array(-10168.407, dtype=float32), Array(4663.6465, dtype=float32)), 'eval/avg_episode_length': (Array(930.1172, dtype=float32), Array(254.11118, dtype=float32)), 'eval/epoch_eval_time': 4.088983774185181, 'eval/sps': 31303.62140542042}
I0727 17:34:23.553071 140141833336640 train.py:379] starting iteration 443 4450.196024894714
I0727 17:34:33.503881 140141833336640 train.py:394] {'eval/walltime': 1829.8117978572845, 'training/sps': 41883.7711235204, 'training/walltime': 2616.747086763382, 'training/entropy_loss': Array(-0.04232481, dtype=float32), 'training/policy_loss': Array(-5.63414e-05, dtype=float32), 'training/total_loss': Array(15.743638, dtype=float32), 'training/v_loss': Array(15.78602, dtype=float32), 'eval/episode_goal_distance': (Array(0.40930212, dtype=float32), Array(0.16734798, dtype=float32)), 'eval/episode_reward': (Array(-9236.996, dtype=float32), Array(4187.646, dtype=float32)), 'eval/avg_episode_length': (Array(930.03906, dtype=float32), Array(254.39485, dtype=float32)), 'eval/epoch_eval_time': 4.079355955123901, 'eval/sps': 31377.50208809917}
I0727 17:34:33.506331 140141833336640 train.py:379] starting iteration 444 4460.149285078049
I0727 17:34:43.489225 140141833336640 train.py:394] {'eval/walltime': 1833.9228184223175, 'training/sps': 41883.50904075916, 'training/walltime': 2622.6147899627686, 'training/entropy_loss': Array(-0.0394062, dtype=float32), 'training/policy_loss': Array(-1.794931e-05, dtype=float32), 'training/total_loss': Array(13.088446, dtype=float32), 'training/v_loss': Array(13.12787, dtype=float32), 'eval/episode_goal_distance': (Array(0.4040674, dtype=float32), Array(0.15908171, dtype=float32)), 'eval/episode_reward': (Array(-9120.709, dtype=float32), Array(3525.3499, dtype=float32)), 'eval/avg_episode_length': (Array(961.1797, dtype=float32), Array(192.54279, dtype=float32)), 'eval/epoch_eval_time': 4.111020565032959, 'eval/sps': 31135.820892925596}
I0727 17:34:43.492544 140141833336640 train.py:379] starting iteration 445 4470.135482311249
I0727 17:34:53.459341 140141833336640 train.py:394] {'eval/walltime': 1838.0169444084167, 'training/sps': 41880.731845765105, 'training/walltime': 2628.4828822612762, 'training/entropy_loss': Array(-0.03888597, dtype=float32), 'training/policy_loss': Array(0.00020743, dtype=float32), 'training/total_loss': Array(12.319582, dtype=float32), 'training/v_loss': Array(12.358261, dtype=float32), 'eval/episode_goal_distance': (Array(0.39692414, dtype=float32), Array(0.14669973, dtype=float32)), 'eval/episode_reward': (Array(-9043.419, dtype=float32), Array(4228.0933, dtype=float32)), 'eval/avg_episode_length': (Array(899.03125, dtype=float32), Array(300.30673, dtype=float32)), 'eval/epoch_eval_time': 4.094125986099243, 'eval/sps': 31264.304135876006}
I0727 17:34:53.461744 140141833336640 train.py:379] starting iteration 446 4480.104697227478
I0727 17:35:03.389587 140141833336640 train.py:394] {'eval/walltime': 1842.1012988090515, 'training/sps': 42087.589359843216, 'training/walltime': 2634.3221333026886, 'training/entropy_loss': Array(-0.03817271, dtype=float32), 'training/policy_loss': Array(0.00026746, dtype=float32), 'training/total_loss': Array(11.958797, dtype=float32), 'training/v_loss': Array(11.996702, dtype=float32), 'eval/episode_goal_distance': (Array(0.39852947, dtype=float32), Array(0.1493242, dtype=float32)), 'eval/episode_reward': (Array(-9247.296, dtype=float32), Array(3866.6672, dtype=float32)), 'eval/avg_episode_length': (Array(922.3828, dtype=float32), Array(266.62427, dtype=float32)), 'eval/epoch_eval_time': 4.084354400634766, 'eval/sps': 31339.102204281542}
I0727 17:35:03.392139 140141833336640 train.py:379] starting iteration 447 4490.035089254379
I0727 17:35:13.390488 140141833336640 train.py:394] {'eval/walltime': 1846.2258577346802, 'training/sps': 41869.443073269766, 'training/walltime': 2640.191807746887, 'training/entropy_loss': Array(-0.0389505, dtype=float32), 'training/policy_loss': Array(0.00018946, dtype=float32), 'training/total_loss': Array(12.912719, dtype=float32), 'training/v_loss': Array(12.951479, dtype=float32), 'eval/episode_goal_distance': (Array(0.38292187, dtype=float32), Array(0.15699504, dtype=float32)), 'eval/episode_reward': (Array(-8451.75, dtype=float32), Array(3941.642, dtype=float32)), 'eval/avg_episode_length': (Array(898.96875, dtype=float32), Array(300.49222, dtype=float32)), 'eval/epoch_eval_time': 4.124558925628662, 'eval/sps': 31033.62136606894}
I0727 17:35:13.392653 140141833336640 train.py:379] starting iteration 448 4500.035607814789
I0727 17:35:23.367721 140141833336640 train.py:394] {'eval/walltime': 1850.3306863307953, 'training/sps': 41909.634733284896, 'training/walltime': 2646.0558531284332, 'training/entropy_loss': Array(-0.04111222, dtype=float32), 'training/policy_loss': Array(-0.00028389, dtype=float32), 'training/total_loss': Array(36.592087, dtype=float32), 'training/v_loss': Array(36.63348, dtype=float32), 'eval/episode_goal_distance': (Array(0.40081626, dtype=float32), Array(0.14928833, dtype=float32)), 'eval/episode_reward': (Array(-8801.805, dtype=float32), Array(4132.239, dtype=float32)), 'eval/avg_episode_length': (Array(906.8047, dtype=float32), Array(289.7564, dtype=float32)), 'eval/epoch_eval_time': 4.104828596115112, 'eval/sps': 31182.7880270425}
I0727 17:35:23.370006 140141833336640 train.py:379] starting iteration 449 4510.012960195541
I0727 17:35:33.330257 140141833336640 train.py:394] {'eval/walltime': 1854.4190738201141, 'training/sps': 41883.568604735075, 'training/walltime': 2651.9235479831696, 'training/entropy_loss': Array(-0.03973477, dtype=float32), 'training/policy_loss': Array(0.00013456, dtype=float32), 'training/total_loss': Array(15.965044, dtype=float32), 'training/v_loss': Array(16.004644, dtype=float32), 'eval/episode_goal_distance': (Array(0.38073358, dtype=float32), Array(0.1460158, dtype=float32)), 'eval/episode_reward': (Array(-8369.652, dtype=float32), Array(3911.335, dtype=float32)), 'eval/avg_episode_length': (Array(906.7969, dtype=float32), Array(289.78094, dtype=float32)), 'eval/epoch_eval_time': 4.088387489318848, 'eval/sps': 31308.186989224363}
I0727 17:35:33.332851 140141833336640 train.py:379] starting iteration 450 4519.975805282593
I0727 17:35:43.304953 140141833336640 train.py:394] {'eval/walltime': 1858.5380642414093, 'training/sps': 42014.96943620031, 'training/walltime': 2657.7728917598724, 'training/entropy_loss': Array(-0.03811726, dtype=float32), 'training/policy_loss': Array(0.00051196, dtype=float32), 'training/total_loss': Array(39.19664, dtype=float32), 'training/v_loss': Array(39.234245, dtype=float32), 'eval/episode_goal_distance': (Array(0.41105342, dtype=float32), Array(0.16944174, dtype=float32)), 'eval/episode_reward': (Array(-9229.729, dtype=float32), Array(4315.519, dtype=float32)), 'eval/avg_episode_length': (Array(922.3594, dtype=float32), Array(266.7048, dtype=float32)), 'eval/epoch_eval_time': 4.118990421295166, 'eval/sps': 31075.576029077525}
I0727 17:35:43.307298 140141833336640 train.py:379] starting iteration 451 4529.95025229454
I0727 17:35:53.240540 140141833336640 train.py:394] {'eval/walltime': 1862.6301259994507, 'training/sps': 42100.35100393438, 'training/walltime': 2663.6103727817535, 'training/entropy_loss': Array(-0.03813094, dtype=float32), 'training/policy_loss': Array(0.00051326, dtype=float32), 'training/total_loss': Array(13.236808, dtype=float32), 'training/v_loss': Array(13.274426, dtype=float32), 'eval/episode_goal_distance': (Array(0.4037786, dtype=float32), Array(0.16342942, dtype=float32)), 'eval/episode_reward': (Array(-8987.33, dtype=float32), Array(4059.7495, dtype=float32)), 'eval/avg_episode_length': (Array(922.28125, dtype=float32), Array(266.9732, dtype=float32)), 'eval/epoch_eval_time': 4.092061758041382, 'eval/sps': 31280.075318625135}
I0727 17:35:53.242887 140141833336640 train.py:379] starting iteration 452 4539.885840654373
I0727 17:36:03.210065 140141833336640 train.py:394] {'eval/walltime': 1866.7190237045288, 'training/sps': 41834.28889263981, 'training/walltime': 2669.4849796295166, 'training/entropy_loss': Array(-0.03789638, dtype=float32), 'training/policy_loss': Array(0.00032588, dtype=float32), 'training/total_loss': Array(11.990045, dtype=float32), 'training/v_loss': Array(12.027614, dtype=float32), 'eval/episode_goal_distance': (Array(0.38176993, dtype=float32), Array(0.13460343, dtype=float32)), 'eval/episode_reward': (Array(-8444.333, dtype=float32), Array(3919.1064, dtype=float32)), 'eval/avg_episode_length': (Array(891.28125, dtype=float32), Array(310.23712, dtype=float32)), 'eval/epoch_eval_time': 4.088897705078125, 'eval/sps': 31304.280329887675}
I0727 17:36:03.212336 140141833336640 train.py:379] starting iteration 453 4549.85529088974
I0727 17:36:13.177957 140141833336640 train.py:394] {'eval/walltime': 1870.8433306217194, 'training/sps': 42099.928012985896, 'training/walltime': 2675.322519302368, 'training/entropy_loss': Array(-0.03761513, dtype=float32), 'training/policy_loss': Array(0.00066122, dtype=float32), 'training/total_loss': Array(11.092949, dtype=float32), 'training/v_loss': Array(11.129902, dtype=float32), 'eval/episode_goal_distance': (Array(0.38447335, dtype=float32), Array(0.16956331, dtype=float32)), 'eval/episode_reward': (Array(-8513.396, dtype=float32), Array(4404.1978, dtype=float32)), 'eval/avg_episode_length': (Array(867.9219, dtype=float32), Array(337.49594, dtype=float32)), 'eval/epoch_eval_time': 4.124306917190552, 'eval/sps': 31035.51762030181}
I0727 17:36:13.180451 140141833336640 train.py:379] starting iteration 454 4559.823405504227
I0727 17:36:23.133978 140141833336640 train.py:394] {'eval/walltime': 1874.9323065280914, 'training/sps': 41932.14746950499, 'training/walltime': 2681.183416366577, 'training/entropy_loss': Array(-0.03899022, dtype=float32), 'training/policy_loss': Array(0.00071227, dtype=float32), 'training/total_loss': Array(11.817505, dtype=float32), 'training/v_loss': Array(11.855783, dtype=float32), 'eval/episode_goal_distance': (Array(0.4217455, dtype=float32), Array(0.15481383, dtype=float32)), 'eval/episode_reward': (Array(-9357.845, dtype=float32), Array(4144.9053, dtype=float32)), 'eval/avg_episode_length': (Array(914.5781, dtype=float32), Array(278.59067, dtype=float32)), 'eval/epoch_eval_time': 4.08897590637207, 'eval/sps': 31303.681638361024}
I0727 17:36:23.136322 140141833336640 train.py:379] starting iteration 455 4569.7792756557465
I0727 17:36:33.079384 140141833336640 train.py:394] {'eval/walltime': 1879.0152654647827, 'training/sps': 41964.486694046114, 'training/walltime': 2687.0397968292236, 'training/entropy_loss': Array(-0.03954562, dtype=float32), 'training/policy_loss': Array(0.00068455, dtype=float32), 'training/total_loss': Array(12.046563, dtype=float32), 'training/v_loss': Array(12.085425, dtype=float32), 'eval/episode_goal_distance': (Array(0.42124388, dtype=float32), Array(0.1641337, dtype=float32)), 'eval/episode_reward': (Array(-9179.215, dtype=float32), Array(4236.0923, dtype=float32)), 'eval/avg_episode_length': (Array(898.96875, dtype=float32), Array(300.49277, dtype=float32)), 'eval/epoch_eval_time': 4.082958936691284, 'eval/sps': 31349.813207704614}
I0727 17:36:33.081832 140141833336640 train.py:379] starting iteration 456 4579.724786758423
I0727 17:36:43.039285 140141833336640 train.py:394] {'eval/walltime': 1883.1166067123413, 'training/sps': 41993.496785749776, 'training/walltime': 2692.8921315670013, 'training/entropy_loss': Array(-0.03985549, dtype=float32), 'training/policy_loss': Array(0.00163175, dtype=float32), 'training/total_loss': Array(32.213806, dtype=float32), 'training/v_loss': Array(32.25203, dtype=float32), 'eval/episode_goal_distance': (Array(0.41895288, dtype=float32), Array(0.1737342, dtype=float32)), 'eval/episode_reward': (Array(-8745.264, dtype=float32), Array(4471.3228, dtype=float32)), 'eval/avg_episode_length': (Array(891.1875, dtype=float32), Array(310.5046, dtype=float32)), 'eval/epoch_eval_time': 4.101341247558594, 'eval/sps': 31209.302585146892}
I0727 17:36:43.042047 140141833336640 train.py:379] starting iteration 457 4589.684998989105
I0727 17:36:53.040166 140141833336640 train.py:394] {'eval/walltime': 1887.212512254715, 'training/sps': 41666.2449769116, 'training/walltime': 2698.7904312610626, 'training/entropy_loss': Array(-0.0416882, dtype=float32), 'training/policy_loss': Array(0.00082189, dtype=float32), 'training/total_loss': Array(20.886497, dtype=float32), 'training/v_loss': Array(20.927362, dtype=float32), 'eval/episode_goal_distance': (Array(0.40270025, dtype=float32), Array(0.15655902, dtype=float32)), 'eval/episode_reward': (Array(-8509.584, dtype=float32), Array(4151.67, dtype=float32)), 'eval/avg_episode_length': (Array(875.72656, dtype=float32), Array(328.7971, dtype=float32)), 'eval/epoch_eval_time': 4.095905542373657, 'eval/sps': 31250.72067111721}
I0727 17:36:53.042670 140141833336640 train.py:379] starting iteration 458 4599.685623407364
I0727 17:37:03.014072 140141833336640 train.py:394] {'eval/walltime': 1891.3017599582672, 'training/sps': 41807.21079455152, 'training/walltime': 2704.6688430309296, 'training/entropy_loss': Array(-0.04087102, dtype=float32), 'training/policy_loss': Array(0.00176984, dtype=float32), 'training/total_loss': Array(50.38997, dtype=float32), 'training/v_loss': Array(50.429066, dtype=float32), 'eval/episode_goal_distance': (Array(0.3993604, dtype=float32), Array(0.15160796, dtype=float32)), 'eval/episode_reward': (Array(-8189.907, dtype=float32), Array(4674.733, dtype=float32)), 'eval/avg_episode_length': (Array(829.1172, dtype=float32), Array(375.09488, dtype=float32)), 'eval/epoch_eval_time': 4.089247703552246, 'eval/sps': 31301.600998347203}
I0727 17:37:03.016451 140141833336640 train.py:379] starting iteration 459 4609.659404754639
I0727 17:37:13.000802 140141833336640 train.py:394] {'eval/walltime': 1895.4126217365265, 'training/sps': 41868.95668369992, 'training/walltime': 2710.538585662842, 'training/entropy_loss': Array(-0.03827531, dtype=float32), 'training/policy_loss': Array(0.00115491, dtype=float32), 'training/total_loss': Array(15.283556, dtype=float32), 'training/v_loss': Array(15.320677, dtype=float32), 'eval/episode_goal_distance': (Array(0.41032058, dtype=float32), Array(0.16321181, dtype=float32)), 'eval/episode_reward': (Array(-8133.6914, dtype=float32), Array(5171.28, dtype=float32)), 'eval/avg_episode_length': (Array(797.96875, dtype=float32), Array(400.15903, dtype=float32)), 'eval/epoch_eval_time': 4.110861778259277, 'eval/sps': 31137.02354988956}
I0727 17:37:13.428789 140141833336640 train.py:379] starting iteration 460 4620.071734189987
I0727 17:37:23.340873 140141833336640 train.py:394] {'eval/walltime': 1899.4935326576233, 'training/sps': 42174.53488248873, 'training/walltime': 2716.3657987117767, 'training/entropy_loss': Array(-0.03852114, dtype=float32), 'training/policy_loss': Array(0.00185901, dtype=float32), 'training/total_loss': Array(13.711727, dtype=float32), 'training/v_loss': Array(13.748389, dtype=float32), 'eval/episode_goal_distance': (Array(0.3796726, dtype=float32), Array(0.13333642, dtype=float32)), 'eval/episode_reward': (Array(-7618.4795, dtype=float32), Array(4710.149, dtype=float32)), 'eval/avg_episode_length': (Array(782.4922, dtype=float32), Array(411.0522, dtype=float32)), 'eval/epoch_eval_time': 4.080910921096802, 'eval/sps': 31365.546191730697}
I0727 17:37:23.343454 140141833336640 train.py:379] starting iteration 461 4629.986406803131
I0727 17:37:33.327661 140141833336640 train.py:394] {'eval/walltime': 1903.5853197574615, 'training/sps': 41734.05556202229, 'training/walltime': 2722.254514694214, 'training/entropy_loss': Array(-0.03784231, dtype=float32), 'training/policy_loss': Array(0.00073108, dtype=float32), 'training/total_loss': Array(11.932238, dtype=float32), 'training/v_loss': Array(11.969349, dtype=float32), 'eval/episode_goal_distance': (Array(0.37025827, dtype=float32), Array(0.13845044, dtype=float32)), 'eval/episode_reward': (Array(-7346.1235, dtype=float32), Array(4498.4795, dtype=float32)), 'eval/avg_episode_length': (Array(813.5469, dtype=float32), Array(388.13354, dtype=float32)), 'eval/epoch_eval_time': 4.091787099838257, 'eval/sps': 31282.1749707016}
I0727 17:37:33.330078 140141833336640 train.py:379] starting iteration 462 4639.973031997681
I0727 17:37:43.303144 140141833336640 train.py:394] {'eval/walltime': 1907.6718504428864, 'training/sps': 41777.590474118064, 'training/walltime': 2728.137094259262, 'training/entropy_loss': Array(-0.03809895, dtype=float32), 'training/policy_loss': Array(0.00080602, dtype=float32), 'training/total_loss': Array(10.921504, dtype=float32), 'training/v_loss': Array(10.9587965, dtype=float32), 'eval/episode_goal_distance': (Array(0.40285975, dtype=float32), Array(0.15443666, dtype=float32)), 'eval/episode_reward': (Array(-8566.664, dtype=float32), Array(4737.9556, dtype=float32)), 'eval/avg_episode_length': (Array(844.5469, dtype=float32), Array(361.24075, dtype=float32)), 'eval/epoch_eval_time': 4.086530685424805, 'eval/sps': 31322.41254336601}
I0727 17:37:43.305540 140141833336640 train.py:379] starting iteration 463 4649.948494195938
I0727 17:37:53.246310 140141833336640 train.py:394] {'eval/walltime': 1911.7559168338776, 'training/sps': 41989.23225599255, 'training/walltime': 2733.9900233745575, 'training/entropy_loss': Array(-0.04030678, dtype=float32), 'training/policy_loss': Array(0.00092008, dtype=float32), 'training/total_loss': Array(11.922134, dtype=float32), 'training/v_loss': Array(11.961522, dtype=float32), 'eval/episode_goal_distance': (Array(0.40635878, dtype=float32), Array(0.15404284, dtype=float32)), 'eval/episode_reward': (Array(-8598.914, dtype=float32), Array(4270.8857, dtype=float32)), 'eval/avg_episode_length': (Array(860.125, dtype=float32), Array(345.78088, dtype=float32)), 'eval/epoch_eval_time': 4.084066390991211, 'eval/sps': 31341.312247603826}
I0727 17:37:53.248578 140141833336640 train.py:379] starting iteration 464 4659.891532659531
I0727 17:38:03.180433 140141833336640 train.py:394] {'eval/walltime': 1915.8357598781586, 'training/sps': 42023.386471497506, 'training/walltime': 2739.8381955623627, 'training/entropy_loss': Array(-0.04043052, dtype=float32), 'training/policy_loss': Array(0.00114446, dtype=float32), 'training/total_loss': Array(13.367401, dtype=float32), 'training/v_loss': Array(13.406688, dtype=float32), 'eval/episode_goal_distance': (Array(0.38338414, dtype=float32), Array(0.15350287, dtype=float32)), 'eval/episode_reward': (Array(-7933.5586, dtype=float32), Array(4246.0093, dtype=float32)), 'eval/avg_episode_length': (Array(860.1719, dtype=float32), Array(345.6655, dtype=float32)), 'eval/epoch_eval_time': 4.079843044281006, 'eval/sps': 31373.755953535107}
I0727 17:38:03.182889 140141833336640 train.py:379] starting iteration 465 4669.825842857361
I0727 17:38:13.144998 140141833336640 train.py:394] {'eval/walltime': 1919.93776345253, 'training/sps': 41964.51744549273, 'training/walltime': 2745.6945717334747, 'training/entropy_loss': Array(-0.03943603, dtype=float32), 'training/policy_loss': Array(0.00165223, dtype=float32), 'training/total_loss': Array(28.927612, dtype=float32), 'training/v_loss': Array(28.965397, dtype=float32), 'eval/episode_goal_distance': (Array(0.4033447, dtype=float32), Array(0.16923763, dtype=float32)), 'eval/episode_reward': (Array(-8218.198, dtype=float32), Array(4648.653, dtype=float32)), 'eval/avg_episode_length': (Array(828.96094, dtype=float32), Array(375.4373, dtype=float32)), 'eval/epoch_eval_time': 4.102003574371338, 'eval/sps': 31204.263399408894}
I0727 17:38:13.147455 140141833336640 train.py:379] starting iteration 466 4679.790409326553
I0727 17:38:23.076857 140141833336640 train.py:394] {'eval/walltime': 1924.0109043121338, 'training/sps': 41993.17687320205, 'training/walltime': 2751.5469510555267, 'training/entropy_loss': Array(-0.03787985, dtype=float32), 'training/policy_loss': Array(0.00290104, dtype=float32), 'training/total_loss': Array(73.281235, dtype=float32), 'training/v_loss': Array(73.31621, dtype=float32), 'eval/episode_goal_distance': (Array(0.405237, dtype=float32), Array(0.13043343, dtype=float32)), 'eval/episode_reward': (Array(-7547.1064, dtype=float32), Array(4807.52, dtype=float32)), 'eval/avg_episode_length': (Array(759.2344, dtype=float32), Array(425.89285, dtype=float32)), 'eval/epoch_eval_time': 4.073140859603882, 'eval/sps': 31425.38017024242}
I0727 17:38:23.079211 140141833336640 train.py:379] starting iteration 467 4689.722165107727
I0727 17:38:33.019486 140141833336640 train.py:394] {'eval/walltime': 1928.108288526535, 'training/sps': 42087.82307039458, 'training/walltime': 2757.3861696720123, 'training/entropy_loss': Array(-0.03778097, dtype=float32), 'training/policy_loss': Array(0.00162276, dtype=float32), 'training/total_loss': Array(25.273071, dtype=float32), 'training/v_loss': Array(25.309229, dtype=float32), 'eval/episode_goal_distance': (Array(0.4072439, dtype=float32), Array(0.14556834, dtype=float32)), 'eval/episode_reward': (Array(-7727.1924, dtype=float32), Array(5001.249, dtype=float32)), 'eval/avg_episode_length': (Array(774.71875, dtype=float32), Array(416.24072, dtype=float32)), 'eval/epoch_eval_time': 4.097384214401245, 'eval/sps': 31239.4428499317}
I0727 17:38:33.021861 140141833336640 train.py:379] starting iteration 468 4699.664814949036
I0727 17:38:42.978127 140141833336640 train.py:394] {'eval/walltime': 1932.1957445144653, 'training/sps': 41901.71116379352, 'training/walltime': 2763.2513239383698, 'training/entropy_loss': Array(-0.0357086, dtype=float32), 'training/policy_loss': Array(0.00137535, dtype=float32), 'training/total_loss': Array(14.009134, dtype=float32), 'training/v_loss': Array(14.043467, dtype=float32), 'eval/episode_goal_distance': (Array(0.36623043, dtype=float32), Array(0.13156594, dtype=float32)), 'eval/episode_reward': (Array(-6735.19, dtype=float32), Array(4797.433, dtype=float32)), 'eval/avg_episode_length': (Array(727.9844, dtype=float32), Array(443.40646, dtype=float32)), 'eval/epoch_eval_time': 4.087455987930298, 'eval/sps': 31315.321896545578}
I0727 17:38:42.980582 140141833336640 train.py:379] starting iteration 469 4709.6235365867615
I0727 17:38:52.960313 140141833336640 train.py:394] {'eval/walltime': 1936.3210446834564, 'training/sps': 42005.43625879165, 'training/walltime': 2769.101995229721, 'training/entropy_loss': Array(-0.03525832, dtype=float32), 'training/policy_loss': Array(0.00153544, dtype=float32), 'training/total_loss': Array(14.025499, dtype=float32), 'training/v_loss': Array(14.059222, dtype=float32), 'eval/episode_goal_distance': (Array(0.40301782, dtype=float32), Array(0.14376587, dtype=float32)), 'eval/episode_reward': (Array(-6991.963, dtype=float32), Array(5218.921, dtype=float32)), 'eval/avg_episode_length': (Array(704.6328, dtype=float32), Array(454.56116, dtype=float32)), 'eval/epoch_eval_time': 4.125300168991089, 'eval/sps': 31028.045174056882}
I0727 17:38:52.962743 140141833336640 train.py:379] starting iteration 470 4719.605697154999
I0727 17:39:02.975640 140141833336640 train.py:394] {'eval/walltime': 1940.4369122982025, 'training/sps': 41702.05182453935, 'training/walltime': 2774.995230436325, 'training/entropy_loss': Array(-0.03628494, dtype=float32), 'training/policy_loss': Array(0.00149948, dtype=float32), 'training/total_loss': Array(13.325813, dtype=float32), 'training/v_loss': Array(13.360598, dtype=float32), 'eval/episode_goal_distance': (Array(0.37977386, dtype=float32), Array(0.12929676, dtype=float32)), 'eval/episode_reward': (Array(-7676.3184, dtype=float32), Array(4493.9927, dtype=float32)), 'eval/avg_episode_length': (Array(798.0703, dtype=float32), Array(399.9581, dtype=float32)), 'eval/epoch_eval_time': 4.115867614746094, 'eval/sps': 31099.153806941933}
I0727 17:39:02.978474 140141833336640 train.py:379] starting iteration 471 4729.621428728104
I0727 17:39:13.021720 140141833336640 train.py:394] {'eval/walltime': 1944.572278022766, 'training/sps': 41622.79883311525, 'training/walltime': 2780.8996868133545, 'training/entropy_loss': Array(-0.03714347, dtype=float32), 'training/policy_loss': Array(0.00357384, dtype=float32), 'training/total_loss': Array(17.348576, dtype=float32), 'training/v_loss': Array(17.382145, dtype=float32), 'eval/episode_goal_distance': (Array(0.3899296, dtype=float32), Array(0.1567598, dtype=float32)), 'eval/episode_reward': (Array(-7408.7114, dtype=float32), Array(5321.124, dtype=float32)), 'eval/avg_episode_length': (Array(735.875, dtype=float32), Array(439.17233, dtype=float32)), 'eval/epoch_eval_time': 4.135365724563599, 'eval/sps': 30952.522346377893}
I0727 17:39:13.024201 140141833336640 train.py:379] starting iteration 472 4739.667154788971
I0727 17:39:23.051225 140141833336640 train.py:394] {'eval/walltime': 1948.6989843845367, 'training/sps': 41678.764141644024, 'training/walltime': 2786.7962148189545, 'training/entropy_loss': Array(-0.0370876, dtype=float32), 'training/policy_loss': Array(0.00240366, dtype=float32), 'training/total_loss': Array(14.952152, dtype=float32), 'training/v_loss': Array(14.986837, dtype=float32), 'eval/episode_goal_distance': (Array(0.39347172, dtype=float32), Array(0.15915342, dtype=float32)), 'eval/episode_reward': (Array(-7180.5703, dtype=float32), Array(4781.0757, dtype=float32)), 'eval/avg_episode_length': (Array(751.3906, dtype=float32), Array(430.6048, dtype=float32)), 'eval/epoch_eval_time': 4.12670636177063, 'eval/sps': 31017.47223543173}
I0727 17:39:23.053620 140141833336640 train.py:379] starting iteration 473 4749.696574211121
I0727 17:39:33.031437 140141833336640 train.py:394] {'eval/walltime': 1952.8015422821045, 'training/sps': 41857.56717484122, 'training/walltime': 2792.667554616928, 'training/entropy_loss': Array(-0.03478109, dtype=float32), 'training/policy_loss': Array(0.00171152, dtype=float32), 'training/total_loss': Array(15.200988, dtype=float32), 'training/v_loss': Array(15.234058, dtype=float32), 'eval/episode_goal_distance': (Array(0.40848023, dtype=float32), Array(0.15604605, dtype=float32)), 'eval/episode_reward': (Array(-7971.9937, dtype=float32), Array(4972.881, dtype=float32)), 'eval/avg_episode_length': (Array(790.0703, dtype=float32), Array(406.02557, dtype=float32)), 'eval/epoch_eval_time': 4.102557897567749, 'eval/sps': 31200.047189068642}
I0727 17:39:33.033957 140141833336640 train.py:379] starting iteration 474 4759.676911354065
I0727 17:39:43.027164 140141833336640 train.py:394] {'eval/walltime': 1956.891946554184, 'training/sps': 41662.657907040964, 'training/walltime': 2798.566362142563, 'training/entropy_loss': Array(-0.03371859, dtype=float32), 'training/policy_loss': Array(0.00153913, dtype=float32), 'training/total_loss': Array(16.016895, dtype=float32), 'training/v_loss': Array(16.049076, dtype=float32), 'eval/episode_goal_distance': (Array(0.4374898, dtype=float32), Array(0.19020307, dtype=float32)), 'eval/episode_reward': (Array(-8568.823, dtype=float32), Array(5176.058, dtype=float32)), 'eval/avg_episode_length': (Array(805.8203, dtype=float32), Array(394.1427, dtype=float32)), 'eval/epoch_eval_time': 4.090404272079468, 'eval/sps': 31292.750419245902}
I0727 17:39:43.029815 140141833336640 train.py:379] starting iteration 475 4769.672768831253
I0727 17:39:52.997281 140141833336640 train.py:394] {'eval/walltime': 1960.9761817455292, 'training/sps': 41801.117585800814, 'training/walltime': 2804.445630788803, 'training/entropy_loss': Array(-0.03093193, dtype=float32), 'training/policy_loss': Array(0.00246124, dtype=float32), 'training/total_loss': Array(85.720085, dtype=float32), 'training/v_loss': Array(85.74855, dtype=float32), 'eval/episode_goal_distance': (Array(0.4162543, dtype=float32), Array(0.18073855, dtype=float32)), 'eval/episode_reward': (Array(-7421.4414, dtype=float32), Array(5599.5776, dtype=float32)), 'eval/avg_episode_length': (Array(728.0703, dtype=float32), Array(443.26657, dtype=float32)), 'eval/epoch_eval_time': 4.084235191345215, 'eval/sps': 31340.01691950579}
I0727 17:39:52.999870 140141833336640 train.py:379] starting iteration 476 4779.642823219299
I0727 17:40:02.987088 140141833336640 train.py:394] {'eval/walltime': 1965.0868945121765, 'training/sps': 41848.667644469235, 'training/walltime': 2810.3182191848755, 'training/entropy_loss': Array(-0.03076056, dtype=float32), 'training/policy_loss': Array(0.00159523, dtype=float32), 'training/total_loss': Array(16.37802, dtype=float32), 'training/v_loss': Array(16.407185, dtype=float32), 'eval/episode_goal_distance': (Array(0.38285464, dtype=float32), Array(0.1449926, dtype=float32)), 'eval/episode_reward': (Array(-6871.879, dtype=float32), Array(4995.385, dtype=float32)), 'eval/avg_episode_length': (Array(727.91406, dtype=float32), Array(443.52097, dtype=float32)), 'eval/epoch_eval_time': 4.110712766647339, 'eval/sps': 31138.152253920594}
I0727 17:40:02.989537 140141833336640 train.py:379] starting iteration 477 4789.632491111755
I0727 17:40:12.986767 140141833336640 train.py:394] {'eval/walltime': 1969.1991746425629, 'training/sps': 41791.26099643969, 'training/walltime': 2816.198874473572, 'training/entropy_loss': Array(-0.02911196, dtype=float32), 'training/policy_loss': Array(0.00140375, dtype=float32), 'training/total_loss': Array(15.0945425, dtype=float32), 'training/v_loss': Array(15.1222515, dtype=float32), 'eval/episode_goal_distance': (Array(0.39895552, dtype=float32), Array(0.16159564, dtype=float32)), 'eval/episode_reward': (Array(-8439.957, dtype=float32), Array(4723.1143, dtype=float32)), 'eval/avg_episode_length': (Array(844.7578, dtype=float32), Array(360.7512, dtype=float32)), 'eval/epoch_eval_time': 4.1122801303863525, 'eval/sps': 31126.284188225836}
I0727 17:40:12.992106 140141833336640 train.py:379] starting iteration 478 4799.635045528412
I0727 17:40:22.992194 140141833336640 train.py:394] {'eval/walltime': 1973.307996749878, 'training/sps': 41748.87612895058, 'training/walltime': 2822.0855000019073, 'training/entropy_loss': Array(-0.02796805, dtype=float32), 'training/policy_loss': Array(0.00166235, dtype=float32), 'training/total_loss': Array(16.2728, dtype=float32), 'training/v_loss': Array(16.299107, dtype=float32), 'eval/episode_goal_distance': (Array(0.41023445, dtype=float32), Array(0.1540803, dtype=float32)), 'eval/episode_reward': (Array(-7878.9883, dtype=float32), Array(5074.9214, dtype=float32)), 'eval/avg_episode_length': (Array(774.6406, dtype=float32), Array(416.38535, dtype=float32)), 'eval/epoch_eval_time': 4.1088221073150635, 'eval/sps': 31152.480359789155}
I0727 17:40:22.994635 140141833336640 train.py:379] starting iteration 479 4809.637588977814
I0727 17:40:32.926718 140141833336640 train.py:394] {'eval/walltime': 1977.3960177898407, 'training/sps': 42079.941926951884, 'training/walltime': 2827.9258122444153, 'training/entropy_loss': Array(-0.03580503, dtype=float32), 'training/policy_loss': Array(0.00435765, dtype=float32), 'training/total_loss': Array(33.560158, dtype=float32), 'training/v_loss': Array(33.591606, dtype=float32), 'eval/episode_goal_distance': (Array(0.41872633, dtype=float32), Array(0.17290546, dtype=float32)), 'eval/episode_reward': (Array(-9500.289, dtype=float32), Array(3824.4268, dtype=float32)), 'eval/avg_episode_length': (Array(937.9219, dtype=float32), Array(240.42792, dtype=float32)), 'eval/epoch_eval_time': 4.0880210399627686, 'eval/sps': 31310.99344859677}
I0727 17:40:32.929073 140141833336640 train.py:379] starting iteration 480 4819.572028160095
I0727 17:40:42.918966 140141833336640 train.py:394] {'eval/walltime': 1981.4892778396606, 'training/sps': 41705.73005165918, 'training/walltime': 2833.818527698517, 'training/entropy_loss': Array(-0.04193893, dtype=float32), 'training/policy_loss': Array(0.00091055, dtype=float32), 'training/total_loss': Array(20.113886, dtype=float32), 'training/v_loss': Array(20.154915, dtype=float32), 'eval/episode_goal_distance': (Array(0.42165858, dtype=float32), Array(0.18479507, dtype=float32)), 'eval/episode_reward': (Array(-8895.795, dtype=float32), Array(4314.1978, dtype=float32)), 'eval/avg_episode_length': (Array(906.78906, dtype=float32), Array(289.80484, dtype=float32)), 'eval/epoch_eval_time': 4.093260049819946, 'eval/sps': 31270.918153766077}
I0727 17:40:42.921415 140141833336640 train.py:379] starting iteration 481 4829.564369678497
I0727 17:40:52.891833 140141833336640 train.py:394] {'eval/walltime': 1985.5839223861694, 'training/sps': 41851.33183864313, 'training/walltime': 2839.690742254257, 'training/entropy_loss': Array(-0.04091122, dtype=float32), 'training/policy_loss': Array(0.00057004, dtype=float32), 'training/total_loss': Array(15.578541, dtype=float32), 'training/v_loss': Array(15.618882, dtype=float32), 'eval/episode_goal_distance': (Array(0.43395805, dtype=float32), Array(0.1869501, dtype=float32)), 'eval/episode_reward': (Array(-9364.197, dtype=float32), Array(4699.304, dtype=float32)), 'eval/avg_episode_length': (Array(899.02344, dtype=float32), Array(300.33002, dtype=float32)), 'eval/epoch_eval_time': 4.094644546508789, 'eval/sps': 31260.34471273861}
I0727 17:40:52.894112 140141833336640 train.py:379] starting iteration 482 4839.537065982819
I0727 17:41:02.832903 140141833336640 train.py:394] {'eval/walltime': 1989.6630635261536, 'training/sps': 41968.05757989687, 'training/walltime': 2845.5466244220734, 'training/entropy_loss': Array(-0.04187477, dtype=float32), 'training/policy_loss': Array(0.00016371, dtype=float32), 'training/total_loss': Array(14.765997, dtype=float32), 'training/v_loss': Array(14.807709, dtype=float32), 'eval/episode_goal_distance': (Array(0.4099335, dtype=float32), Array(0.15533733, dtype=float32)), 'eval/episode_reward': (Array(-8851.896, dtype=float32), Array(4367.5933, dtype=float32)), 'eval/avg_episode_length': (Array(891.2344, dtype=float32), Array(310.3705, dtype=float32)), 'eval/epoch_eval_time': 4.079141139984131, 'eval/sps': 31379.154485568488}
I0727 17:41:02.835469 140141833336640 train.py:379] starting iteration 483 4849.478422880173
I0727 17:41:12.804662 140141833336640 train.py:394] {'eval/walltime': 1993.7419991493225, 'training/sps': 41750.77509842978, 'training/walltime': 2851.4329822063446, 'training/entropy_loss': Array(-0.04174694, dtype=float32), 'training/policy_loss': Array(-6.8480906e-05, dtype=float32), 'training/total_loss': Array(37.59941, dtype=float32), 'training/v_loss': Array(37.641228, dtype=float32), 'eval/episode_goal_distance': (Array(0.39295423, dtype=float32), Array(0.13585962, dtype=float32)), 'eval/episode_reward': (Array(-8802.628, dtype=float32), Array(3934.735, dtype=float32)), 'eval/avg_episode_length': (Array(899.0156, dtype=float32), Array(300.35327, dtype=float32)), 'eval/epoch_eval_time': 4.078935623168945, 'eval/sps': 31380.7355215271}
I0727 17:41:12.807164 140141833336640 train.py:379] starting iteration 484 4859.4501185417175
I0727 17:41:22.827207 140141833336640 train.py:394] {'eval/walltime': 1997.8600697517395, 'training/sps': 41667.68671514244, 'training/walltime': 2857.331077814102, 'training/entropy_loss': Array(-0.04026739, dtype=float32), 'training/policy_loss': Array(7.4190815e-05, dtype=float32), 'training/total_loss': Array(15.185911, dtype=float32), 'training/v_loss': Array(15.226104, dtype=float32), 'eval/episode_goal_distance': (Array(0.39946353, dtype=float32), Array(0.17888786, dtype=float32)), 'eval/episode_reward': (Array(-8318.4, dtype=float32), Array(4469.078, dtype=float32)), 'eval/avg_episode_length': (Array(868.08594, dtype=float32), Array(337.07657, dtype=float32)), 'eval/epoch_eval_time': 4.118070602416992, 'eval/sps': 31082.517119758413}
I0727 17:41:22.829645 140141833336640 train.py:379] starting iteration 485 4869.47259926796
I0727 17:41:32.871909 140141833336640 train.py:394] {'eval/walltime': 2001.9781184196472, 'training/sps': 41509.051181959905, 'training/walltime': 2863.2517142295837, 'training/entropy_loss': Array(-0.0402007, dtype=float32), 'training/policy_loss': Array(3.2854558e-05, dtype=float32), 'training/total_loss': Array(12.804178, dtype=float32), 'training/v_loss': Array(12.844345, dtype=float32), 'eval/episode_goal_distance': (Array(0.40212655, dtype=float32), Array(0.16059974, dtype=float32)), 'eval/episode_reward': (Array(-9016.286, dtype=float32), Array(3712.709, dtype=float32)), 'eval/avg_episode_length': (Array(953.4219, dtype=float32), Array(210.03279, dtype=float32)), 'eval/epoch_eval_time': 4.118048667907715, 'eval/sps': 31082.68267869545}
I0727 17:41:32.874276 140141833336640 train.py:379] starting iteration 486 4879.5172300338745
I0727 17:41:42.863078 140141833336640 train.py:394] {'eval/walltime': 2006.0711748600006, 'training/sps': 41713.2032374354, 'training/walltime': 2869.143373966217, 'training/entropy_loss': Array(-0.04058302, dtype=float32), 'training/policy_loss': Array(-6.199015e-06, dtype=float32), 'training/total_loss': Array(13.382208, dtype=float32), 'training/v_loss': Array(13.422796, dtype=float32), 'eval/episode_goal_distance': (Array(0.40340072, dtype=float32), Array(0.16975017, dtype=float32)), 'eval/episode_reward': (Array(-9374.702, dtype=float32), Array(4010.6453, dtype=float32)), 'eval/avg_episode_length': (Array(945.60156, dtype=float32), Array(226.16798, dtype=float32)), 'eval/epoch_eval_time': 4.0930564403533936, 'eval/sps': 31272.47372844644}
I0727 17:41:42.865593 140141833336640 train.py:379] starting iteration 487 4889.508547782898
I0727 17:41:52.830057 140141833336640 train.py:394] {'eval/walltime': 2010.1562101840973, 'training/sps': 41827.75497351538, 'training/walltime': 2875.018898487091, 'training/entropy_loss': Array(-0.04022595, dtype=float32), 'training/policy_loss': Array(0.00018846, dtype=float32), 'training/total_loss': Array(17.321358, dtype=float32), 'training/v_loss': Array(17.361397, dtype=float32), 'eval/episode_goal_distance': (Array(0.3903221, dtype=float32), Array(0.1687596, dtype=float32)), 'eval/episode_reward': (Array(-8869.492, dtype=float32), Array(3884.7915, dtype=float32)), 'eval/avg_episode_length': (Array(937.84375, dtype=float32), Array(240.7303, dtype=float32)), 'eval/epoch_eval_time': 4.08503532409668, 'eval/sps': 31333.87837430868}
I0727 17:41:52.832529 140141833336640 train.py:379] starting iteration 488 4899.475481510162
I0727 17:42:02.825498 140141833336640 train.py:394] {'eval/walltime': 2014.2488896846771, 'training/sps': 41678.46417329057, 'training/walltime': 2880.915468931198, 'training/entropy_loss': Array(-0.0418938, dtype=float32), 'training/policy_loss': Array(0.00020021, dtype=float32), 'training/total_loss': Array(34.195194, dtype=float32), 'training/v_loss': Array(34.236885, dtype=float32), 'eval/episode_goal_distance': (Array(0.3866573, dtype=float32), Array(0.17416395, dtype=float32)), 'eval/episode_reward': (Array(-9020.205, dtype=float32), Array(3921.538, dtype=float32)), 'eval/avg_episode_length': (Array(945.6406, dtype=float32), Array(226.00542, dtype=float32)), 'eval/epoch_eval_time': 4.092679500579834, 'eval/sps': 31275.353953776612}
I0727 17:42:02.827949 140141833336640 train.py:379] starting iteration 489 4909.470903396606
I0727 17:42:12.833867 140141833336640 train.py:394] {'eval/walltime': 2018.3396933078766, 'training/sps': 41575.15167362434, 'training/walltime': 2886.8266921043396, 'training/entropy_loss': Array(-0.04145004, dtype=float32), 'training/policy_loss': Array(0.00030849, dtype=float32), 'training/total_loss': Array(16.019852, dtype=float32), 'training/v_loss': Array(16.060993, dtype=float32), 'eval/episode_goal_distance': (Array(0.39573604, dtype=float32), Array(0.17308514, dtype=float32)), 'eval/episode_reward': (Array(-9033.41, dtype=float32), Array(3867.988, dtype=float32)), 'eval/avg_episode_length': (Array(930.0781, dtype=float32), Array(254.25272, dtype=float32)), 'eval/epoch_eval_time': 4.090803623199463, 'eval/sps': 31289.695568395382}
I0727 17:42:12.836290 140141833336640 train.py:379] starting iteration 490 4919.479243755341
I0727 17:42:22.867580 140141833336640 train.py:394] {'eval/walltime': 2022.4562714099884, 'training/sps': 41578.86960278324, 'training/walltime': 2892.737386703491, 'training/entropy_loss': Array(-0.04088428, dtype=float32), 'training/policy_loss': Array(6.418713e-05, dtype=float32), 'training/total_loss': Array(15.675779, dtype=float32), 'training/v_loss': Array(15.716599, dtype=float32), 'eval/episode_goal_distance': (Array(0.38771337, dtype=float32), Array(0.17184834, dtype=float32)), 'eval/episode_reward': (Array(-9018.056, dtype=float32), Array(3749.7234, dtype=float32)), 'eval/avg_episode_length': (Array(953.4297, dtype=float32), Array(209.99734, dtype=float32)), 'eval/epoch_eval_time': 4.116578102111816, 'eval/sps': 31093.78634996276}
I0727 17:42:22.869966 140141833336640 train.py:379] starting iteration 491 4929.512919425964
I0727 17:42:32.898131 140141833336640 train.py:394] {'eval/walltime': 2026.5956242084503, 'training/sps': 41761.7512454581, 'training/walltime': 2898.6221973896027, 'training/entropy_loss': Array(-0.04097595, dtype=float32), 'training/policy_loss': Array(7.924753e-05, dtype=float32), 'training/total_loss': Array(35.13082, dtype=float32), 'training/v_loss': Array(35.17172, dtype=float32), 'eval/episode_goal_distance': (Array(0.39935493, dtype=float32), Array(0.15525769, dtype=float32)), 'eval/episode_reward': (Array(-8726.549, dtype=float32), Array(3971.3442, dtype=float32)), 'eval/avg_episode_length': (Array(906.8203, dtype=float32), Array(289.7077, dtype=float32)), 'eval/epoch_eval_time': 4.139352798461914, 'eval/sps': 30922.708508335356}
I0727 17:42:32.903035 140141833336640 train.py:379] starting iteration 492 4939.545973300934
I0727 17:42:42.909179 140141833336640 train.py:394] {'eval/walltime': 2030.7157609462738, 'training/sps': 41782.989185427774, 'training/walltime': 2904.5040168762207, 'training/entropy_loss': Array(-0.03923912, dtype=float32), 'training/policy_loss': Array(0.00016959, dtype=float32), 'training/total_loss': Array(16.330334, dtype=float32), 'training/v_loss': Array(16.369404, dtype=float32), 'eval/episode_goal_distance': (Array(0.40738052, dtype=float32), Array(0.14934286, dtype=float32)), 'eval/episode_reward': (Array(-8904.532, dtype=float32), Array(3998.5945, dtype=float32)), 'eval/avg_episode_length': (Array(922.3047, dtype=float32), Array(266.89233, dtype=float32)), 'eval/epoch_eval_time': 4.120136737823486, 'eval/sps': 31066.930091164304}
I0727 17:42:42.911565 140141833336640 train.py:379] starting iteration 493 4949.554518222809
I0727 17:42:52.953920 140141833336640 train.py:394] {'eval/walltime': 2034.8406946659088, 'training/sps': 41558.64284598076, 'training/walltime': 2910.4175882339478, 'training/entropy_loss': Array(-0.04082752, dtype=float32), 'training/policy_loss': Array(0.00021776, dtype=float32), 'training/total_loss': Array(13.577364, dtype=float32), 'training/v_loss': Array(13.617975, dtype=float32), 'eval/episode_goal_distance': (Array(0.39613765, dtype=float32), Array(0.14247051, dtype=float32)), 'eval/episode_reward': (Array(-9141.196, dtype=float32), Array(3513.534, dtype=float32)), 'eval/avg_episode_length': (Array(945.6328, dtype=float32), Array(226.03798, dtype=float32)), 'eval/epoch_eval_time': 4.12493371963501, 'eval/sps': 31030.80163220803}
I0727 17:42:52.956274 140141833336640 train.py:379] starting iteration 494 4959.599227905273
I0727 17:43:02.980415 140141833336640 train.py:394] {'eval/walltime': 2038.962924003601, 'training/sps': 41668.53563572206, 'training/walltime': 2916.3155636787415, 'training/entropy_loss': Array(-0.04188962, dtype=float32), 'training/policy_loss': Array(0.00025714, dtype=float32), 'training/total_loss': Array(13.572592, dtype=float32), 'training/v_loss': Array(13.614225, dtype=float32), 'eval/episode_goal_distance': (Array(0.39924854, dtype=float32), Array(0.16235836, dtype=float32)), 'eval/episode_reward': (Array(-9449.357, dtype=float32), Array(3524.191, dtype=float32)), 'eval/avg_episode_length': (Array(976.7578, dtype=float32), Array(150.02817, dtype=float32)), 'eval/epoch_eval_time': 4.122229337692261, 'eval/sps': 31051.159339828962}
I0727 17:43:02.982859 140141833336640 train.py:379] starting iteration 495 4969.6258137226105
I0727 17:43:12.985765 140141833336640 train.py:394] {'eval/walltime': 2043.0725536346436, 'training/sps': 41729.97195229015, 'training/walltime': 2922.2048559188843, 'training/entropy_loss': Array(-0.04156649, dtype=float32), 'training/policy_loss': Array(0.00019624, dtype=float32), 'training/total_loss': Array(16.130564, dtype=float32), 'training/v_loss': Array(16.171936, dtype=float32), 'eval/episode_goal_distance': (Array(0.42600167, dtype=float32), Array(0.17198019, dtype=float32)), 'eval/episode_reward': (Array(-9074.018, dtype=float32), Array(4516.8706, dtype=float32)), 'eval/avg_episode_length': (Array(899., dtype=float32), Array(300.39935, dtype=float32)), 'eval/epoch_eval_time': 4.1096296310424805, 'eval/sps': 31146.35903759788}
I0727 17:43:12.988170 140141833336640 train.py:379] starting iteration 496 4979.631123781204
I0727 17:43:23.010649 140141833336640 train.py:394] {'eval/walltime': 2047.1740062236786, 'training/sps': 41532.17632114464, 'training/walltime': 2928.1221957206726, 'training/entropy_loss': Array(-0.04400817, dtype=float32), 'training/policy_loss': Array(-8.196194e-06, dtype=float32), 'training/total_loss': Array(35.456654, dtype=float32), 'training/v_loss': Array(35.50067, dtype=float32), 'eval/episode_goal_distance': (Array(0.43405396, dtype=float32), Array(0.17775068, dtype=float32)), 'eval/episode_reward': (Array(-9740.886, dtype=float32), Array(3727.233, dtype=float32)), 'eval/avg_episode_length': (Array(968.9453, dtype=float32), Array(172.90524, dtype=float32)), 'eval/epoch_eval_time': 4.101452589035034, 'eval/sps': 31208.455351208897}
I0727 17:43:23.013182 140141833336640 train.py:379] starting iteration 497 4989.656136035919
I0727 17:43:32.965131 140141833336640 train.py:394] {'eval/walltime': 2051.2561225891113, 'training/sps': 41897.66279904288, 'training/walltime': 2933.9879167079926, 'training/entropy_loss': Array(-0.0446141, dtype=float32), 'training/policy_loss': Array(0.00013491, dtype=float32), 'training/total_loss': Array(20.73447, dtype=float32), 'training/v_loss': Array(20.77895, dtype=float32), 'eval/episode_goal_distance': (Array(0.4359308, dtype=float32), Array(0.19914506, dtype=float32)), 'eval/episode_reward': (Array(-9416.295, dtype=float32), Array(4584.3604, dtype=float32)), 'eval/avg_episode_length': (Array(914.5469, dtype=float32), Array(278.69235, dtype=float32)), 'eval/epoch_eval_time': 4.082116365432739, 'eval/sps': 31356.28398149079}
I0727 17:43:32.967597 140141833336640 train.py:379] starting iteration 498 4999.610549449921
I0727 17:43:42.933048 140141833336640 train.py:394] {'eval/walltime': 2055.3353345394135, 'training/sps': 41782.11188457608, 'training/walltime': 2939.8698596954346, 'training/entropy_loss': Array(-0.04474731, dtype=float32), 'training/policy_loss': Array(0.00013601, dtype=float32), 'training/total_loss': Array(17.870327, dtype=float32), 'training/v_loss': Array(17.91494, dtype=float32), 'eval/episode_goal_distance': (Array(0.43758708, dtype=float32), Array(0.19545886, dtype=float32)), 'eval/episode_reward': (Array(-9440.383, dtype=float32), Array(4140.8564, dtype=float32)), 'eval/avg_episode_length': (Array(937.78906, dtype=float32), Array(240.94229, dtype=float32)), 'eval/epoch_eval_time': 4.079211950302124, 'eval/sps': 31378.609780381666}
I0727 17:43:42.935669 140141833336640 train.py:379] starting iteration 499 5009.5786209106445
I0727 17:43:52.938742 140141833336640 train.py:394] {'eval/walltime': 2059.439550638199, 'training/sps': 41690.66018722759, 'training/walltime': 2945.764705181122, 'training/entropy_loss': Array(-0.04547397, dtype=float32), 'training/policy_loss': Array(-0.00020572, dtype=float32), 'training/total_loss': Array(17.632648, dtype=float32), 'training/v_loss': Array(17.678328, dtype=float32), 'eval/episode_goal_distance': (Array(0.44617635, dtype=float32), Array(0.19613726, dtype=float32)), 'eval/episode_reward': (Array(-9528.198, dtype=float32), Array(4883.8384, dtype=float32)), 'eval/avg_episode_length': (Array(883.6094, dtype=float32), Array(319.45685, dtype=float32)), 'eval/epoch_eval_time': 4.1042160987854, 'eval/sps': 31187.441625668846}
I0727 17:43:52.941315 140141833336640 train.py:379] starting iteration 500 5019.584269285202
I0727 17:44:02.982254 140141833336640 train.py:394] {'eval/walltime': 2063.5492532253265, 'training/sps': 41461.65560209636, 'training/walltime': 2951.6921095848083, 'training/entropy_loss': Array(-0.04495025, dtype=float32), 'training/policy_loss': Array(-0.00035815, dtype=float32), 'training/total_loss': Array(41.29338, dtype=float32), 'training/v_loss': Array(41.33869, dtype=float32), 'eval/episode_goal_distance': (Array(0.4037597, dtype=float32), Array(0.16767204, dtype=float32)), 'eval/episode_reward': (Array(-9163.597, dtype=float32), Array(4158.0103, dtype=float32)), 'eval/avg_episode_length': (Array(922.3047, dtype=float32), Array(266.89236, dtype=float32)), 'eval/epoch_eval_time': 4.1097025871276855, 'eval/sps': 31145.80612254488}
I0727 17:44:02.984835 140141833336640 train.py:379] starting iteration 501 5029.627788305283
I0727 17:44:12.981178 140141833336640 train.py:394] {'eval/walltime': 2067.650381088257, 'training/sps': 41714.540187174585, 'training/walltime': 2957.583580493927, 'training/entropy_loss': Array(-0.04408488, dtype=float32), 'training/policy_loss': Array(-8.006718e-05, dtype=float32), 'training/total_loss': Array(16.471981, dtype=float32), 'training/v_loss': Array(16.516148, dtype=float32), 'eval/episode_goal_distance': (Array(0.42693958, dtype=float32), Array(0.18138184, dtype=float32)), 'eval/episode_reward': (Array(-9123.133, dtype=float32), Array(4340.2456, dtype=float32)), 'eval/avg_episode_length': (Array(906.8047, dtype=float32), Array(289.7566, dtype=float32)), 'eval/epoch_eval_time': 4.101127862930298, 'eval/sps': 31210.926427576116}
I0727 17:44:12.983718 140141833336640 train.py:379] starting iteration 502 5039.6266713142395
I0727 17:44:22.936951 140141833336640 train.py:394] {'eval/walltime': 2071.74284863472, 'training/sps': 41961.12993211388, 'training/walltime': 2963.4404294490814, 'training/entropy_loss': Array(-0.04254195, dtype=float32), 'training/policy_loss': Array(-3.454004e-06, dtype=float32), 'training/total_loss': Array(14.0920515, dtype=float32), 'training/v_loss': Array(14.134597, dtype=float32), 'eval/episode_goal_distance': (Array(0.4696116, dtype=float32), Array(0.19148536, dtype=float32)), 'eval/episode_reward': (Array(-10072.48, dtype=float32), Array(3969.2935, dtype=float32)), 'eval/avg_episode_length': (Array(953.3594, dtype=float32), Array(210.31438, dtype=float32)), 'eval/epoch_eval_time': 4.092467546463013, 'eval/sps': 31276.973744269824}
I0727 17:44:22.939547 140141833336640 train.py:379] starting iteration 503 5049.582500219345
I0727 17:44:32.901486 140141833336640 train.py:394] {'eval/walltime': 2075.8388566970825, 'training/sps': 41924.65872567671, 'training/walltime': 2969.3023734092712, 'training/entropy_loss': Array(-0.04268203, dtype=float32), 'training/policy_loss': Array(6.0723687e-05, dtype=float32), 'training/total_loss': Array(16.31338, dtype=float32), 'training/v_loss': Array(16.355999, dtype=float32), 'eval/episode_goal_distance': (Array(0.3963632, dtype=float32), Array(0.15835106, dtype=float32)), 'eval/episode_reward': (Array(-8470.846, dtype=float32), Array(3965.2937, dtype=float32)), 'eval/avg_episode_length': (Array(906.7578, dtype=float32), Array(289.90195, dtype=float32)), 'eval/epoch_eval_time': 4.096008062362671, 'eval/sps': 31249.938489175405}
I0727 17:44:32.904127 140141833336640 train.py:379] starting iteration 504 5059.5470814704895
I0727 17:44:42.949517 140141833336640 train.py:394] {'eval/walltime': 2079.9634249210358, 'training/sps': 41533.53348823778, 'training/walltime': 2975.219519853592, 'training/entropy_loss': Array(-0.04304519, dtype=float32), 'training/policy_loss': Array(3.0895604e-05, dtype=float32), 'training/total_loss': Array(28.688778, dtype=float32), 'training/v_loss': Array(28.73179, dtype=float32), 'eval/episode_goal_distance': (Array(0.42557284, dtype=float32), Array(0.17343909, dtype=float32)), 'eval/episode_reward': (Array(-9173.395, dtype=float32), Array(4670.955, dtype=float32)), 'eval/avg_episode_length': (Array(899.0703, dtype=float32), Array(300.1906, dtype=float32)), 'eval/epoch_eval_time': 4.124568223953247, 'eval/sps': 31033.551404640533}
I0727 17:44:42.951981 140141833336640 train.py:379] starting iteration 505 5069.594934463501
I0727 17:44:52.982817 140141833336640 train.py:394] {'eval/walltime': 2084.081310272217, 'training/sps': 41590.19183158055, 'training/walltime': 2981.128605365753, 'training/entropy_loss': Array(-0.04418415, dtype=float32), 'training/policy_loss': Array(0.00012038, dtype=float32), 'training/total_loss': Array(26.025908, dtype=float32), 'training/v_loss': Array(26.06997, dtype=float32), 'eval/episode_goal_distance': (Array(0.39103505, dtype=float32), Array(0.17355813, dtype=float32)), 'eval/episode_reward': (Array(-8838.093, dtype=float32), Array(3700.1584, dtype=float32)), 'eval/avg_episode_length': (Array(937.8594, dtype=float32), Array(240.66992, dtype=float32)), 'eval/epoch_eval_time': 4.11788535118103, 'eval/sps': 31083.915428410106}
I0727 17:44:52.985301 140141833336640 train.py:379] starting iteration 506 5079.628254652023
I0727 17:45:03.014650 140141833336640 train.py:394] {'eval/walltime': 2088.1991312503815, 'training/sps': 41597.41215314696, 'training/walltime': 2987.036665201187, 'training/entropy_loss': Array(-0.04150607, dtype=float32), 'training/policy_loss': Array(6.411276e-05, dtype=float32), 'training/total_loss': Array(17.992264, dtype=float32), 'training/v_loss': Array(18.033707, dtype=float32), 'eval/episode_goal_distance': (Array(0.3868516, dtype=float32), Array(0.15973456, dtype=float32)), 'eval/episode_reward': (Array(-8794.775, dtype=float32), Array(3793.24, dtype=float32)), 'eval/avg_episode_length': (Array(937.89844, dtype=float32), Array(240.51913, dtype=float32)), 'eval/epoch_eval_time': 4.117820978164673, 'eval/sps': 31084.401356624796}
I0727 17:45:03.017188 140141833336640 train.py:379] starting iteration 507 5089.660141944885
I0727 17:45:12.999093 140141833336640 train.py:394] {'eval/walltime': 2092.302206993103, 'training/sps': 41832.019019834406, 'training/walltime': 2992.9115908145905, 'training/entropy_loss': Array(-0.04167703, dtype=float32), 'training/policy_loss': Array(0.00030509, dtype=float32), 'training/total_loss': Array(16.052034, dtype=float32), 'training/v_loss': Array(16.093407, dtype=float32), 'eval/episode_goal_distance': (Array(0.43664694, dtype=float32), Array(0.17105725, dtype=float32)), 'eval/episode_reward': (Array(-8399.566, dtype=float32), Array(4740.928, dtype=float32)), 'eval/avg_episode_length': (Array(836.96875, dtype=float32), Array(368.0056, dtype=float32)), 'eval/epoch_eval_time': 4.103075742721558, 'eval/sps': 31196.109461800475}
I0727 17:45:13.001619 140141833336640 train.py:379] starting iteration 508 5099.644573688507
I0727 17:45:22.983651 140141833336640 train.py:394] {'eval/walltime': 2096.3984293937683, 'training/sps': 41780.414970878126, 'training/walltime': 2998.7937726974487, 'training/entropy_loss': Array(-0.04328216, dtype=float32), 'training/policy_loss': Array(-0.00017092, dtype=float32), 'training/total_loss': Array(40.11191, dtype=float32), 'training/v_loss': Array(40.15536, dtype=float32), 'eval/episode_goal_distance': (Array(0.43763924, dtype=float32), Array(0.18356924, dtype=float32)), 'eval/episode_reward': (Array(-9518.353, dtype=float32), Array(4099.3564, dtype=float32)), 'eval/avg_episode_length': (Array(937.875, dtype=float32), Array(240.60924, dtype=float32)), 'eval/epoch_eval_time': 4.096222400665283, 'eval/sps': 31248.3033097058}
I0727 17:45:22.987909 140141833336640 train.py:379] starting iteration 509 5109.630848169327
I0727 17:45:33.005841 140141833336640 train.py:394] {'eval/walltime': 2100.493689060211, 'training/sps': 41524.50689836562, 'training/walltime': 3004.7122054100037, 'training/entropy_loss': Array(-0.04152625, dtype=float32), 'training/policy_loss': Array(-0.00018609, dtype=float32), 'training/total_loss': Array(15.960216, dtype=float32), 'training/v_loss': Array(16.001928, dtype=float32), 'eval/episode_goal_distance': (Array(0.39824492, dtype=float32), Array(0.1507792, dtype=float32)), 'eval/episode_reward': (Array(-8828.623, dtype=float32), Array(3691.5916, dtype=float32)), 'eval/avg_episode_length': (Array(930.10156, dtype=float32), Array(254.16776, dtype=float32)), 'eval/epoch_eval_time': 4.095259666442871, 'eval/sps': 31255.64931788083}
I0727 17:45:33.008305 140141833336640 train.py:379] starting iteration 510 5119.651259660721
I0727 17:45:43.002221 140141833336640 train.py:394] {'eval/walltime': 2104.57692861557, 'training/sps': 41605.81887882039, 'training/walltime': 3010.619071483612, 'training/entropy_loss': Array(-0.04116778, dtype=float32), 'training/policy_loss': Array(0.00019061, dtype=float32), 'training/total_loss': Array(12.521917, dtype=float32), 'training/v_loss': Array(12.562895, dtype=float32), 'eval/episode_goal_distance': (Array(0.42558685, dtype=float32), Array(0.15960996, dtype=float32)), 'eval/episode_reward': (Array(-9613.407, dtype=float32), Array(3815.07, dtype=float32)), 'eval/avg_episode_length': (Array(961.1328, dtype=float32), Array(192.77527, dtype=float32)), 'eval/epoch_eval_time': 4.083239555358887, 'eval/sps': 31347.65870692339}
I0727 17:45:44.131852 140141833336640 train.py:410] total steps: 125583360
