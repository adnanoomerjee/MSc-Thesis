I0728 20:08:58.821979 140034207004480 low_level_env.py:190] Initialising environment...
I0728 20:08:59.178472 140034207004480 low_level_env.py:298] Environment initialised.
I0728 20:08:59.195962 140034207004480 train.py:118] JAX is running on GPU.
I0728 20:08:59.196076 140034207004480 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 20:09:05.584884 140034207004480 train.py:367] Running initial eval
I0728 20:09:21.049031 140034207004480 train.py:373] {'eval/walltime': 15.320311069488525, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3123188, 0.1401814], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.410564, 12.041116], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-25178.385,  10206.926], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30802828, 0.14279291], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.320311069488525, 'eval/sps': 8354.921738822979}
I0728 20:09:21.052532 140034207004480 train.py:379] starting iteration 0, 0 steps, 21.856593132019043
I0728 20:10:12.298430 140034207004480 train.py:394] {'eval/walltime': 18.96977424621582, 'training/sps': 234099.36939272718, 'training/walltime': 47.59141397476196, 'training/entropy_loss': Array(-0.03485895, dtype=float32), 'training/policy_loss': Array(-0.00540158, dtype=float32), 'training/total_loss': Array(20396.027, dtype=float32), 'training/v_loss': Array(20396.066, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11698672, 0.10991793], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([9.469385, 9.407211], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-12954.066,   8338.653], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10124315, 0.11620225], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.649463176727295, 'eval/sps': 35073.65160340807}
I0728 20:10:12.355075 140034207004480 train.py:379] starting iteration 1, 11141120 steps, 73.15913581848145
I0728 20:10:51.926582 140034207004480 train.py:394] {'eval/walltime': 22.653793573379517, 'training/sps': 310484.9842633127, 'training/walltime': 83.47437310218811, 'training/entropy_loss': Array(-0.01451137, dtype=float32), 'training/policy_loss': Array(-0.00510666, dtype=float32), 'training/total_loss': Array(1307.0316, dtype=float32), 'training/v_loss': Array(1307.0513, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.094521  , 0.10355324], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([7.226673, 8.955122], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-9642.672,  8419.175], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.07819512, 0.10935497], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6840193271636963, 'eval/sps': 34744.660283459045}
I0728 20:10:51.929542 140034207004480 train.py:379] starting iteration 2, 22282240 steps, 112.73362755775452
I0728 20:11:31.984982 140034207004480 train.py:394] {'eval/walltime': 26.470667839050293, 'training/sps': 307478.8978943711, 'training/walltime': 119.70814418792725, 'training/entropy_loss': Array(-0.00391642, dtype=float32), 'training/policy_loss': Array(-0.00373576, dtype=float32), 'training/total_loss': Array(1666.1785, dtype=float32), 'training/v_loss': Array(1666.1862, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11273909, 0.12927377], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 8.511398, 11.229236], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-10468.648,  10603.554], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09888425, 0.13448432], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.8168742656707764, 'eval/sps': 33535.293827004105}
I0728 20:11:31.986697 140034207004480 train.py:379] starting iteration 3, 33423360 steps, 152.79078269004822
I0728 20:12:12.039934 140034207004480 train.py:394] {'eval/walltime': 30.35790205001831, 'training/sps': 308090.13190057565, 'training/walltime': 155.8700294494629, 'training/entropy_loss': Array(0.00040729, dtype=float32), 'training/policy_loss': Array(-0.00280396, dtype=float32), 'training/total_loss': Array(1085.1421, dtype=float32), 'training/v_loss': Array(1085.1444, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.10624919, 0.12949602], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 7.6800027, 11.371058 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-9566.307, 11035.11 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09003507, 0.13572755], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.8872342109680176, 'eval/sps': 32928.29632926204}
I0728 20:12:12.041636 140034207004480 train.py:379] starting iteration 4, 44564480 steps, 192.84572172164917
I0728 20:12:52.360768 140034207004480 train.py:394] {'eval/walltime': 34.251402139663696, 'training/sps': 305896.45128637727, 'training/walltime': 192.29124307632446, 'training/entropy_loss': Array(0.00496366, dtype=float32), 'training/policy_loss': Array(-0.00087356, dtype=float32), 'training/total_loss': Array(1173.9962, dtype=float32), 'training/v_loss': Array(1173.9921, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.08937775, 0.11172906], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([6.3080673, 9.752153 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-8543.227,  9918.436], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.07445872, 0.1164792 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.8935000896453857, 'eval/sps': 32875.30423857215}
I0728 20:12:52.362482 140034207004480 train.py:379] starting iteration 5, 55705600 steps, 233.1665678024292
I0728 20:13:32.581147 140034207004480 train.py:394] {'eval/walltime': 38.13234758377075, 'training/sps': 306633.4636714836, 'training/walltime': 228.62491607666016, 'training/entropy_loss': Array(0.00599067, dtype=float32), 'training/policy_loss': Array(0.00040502, dtype=float32), 'training/total_loss': Array(1014.3131, dtype=float32), 'training/v_loss': Array(1014.30676, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.07274837, 0.08027755], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([4.8703165, 7.0158763], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-7528.633,  8058.893], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.05570094, 0.08411616], dtype=float32), 'eval/avg_episode_length': Array([997.6875 ,  26.06055], dtype=float32), 'eval/epoch_eval_time': 3.8809454441070557, 'eval/sps': 32981.65404369676}
I0728 20:13:32.582888 140034207004480 train.py:379] starting iteration 6, 66846720 steps, 273.38697385787964
I0728 20:14:13.096047 140034207004480 train.py:394] {'eval/walltime': 42.00420260429382, 'training/sps': 304099.52825402335, 'training/walltime': 265.2613425254822, 'training/entropy_loss': Array(0.00651814, dtype=float32), 'training/policy_loss': Array(-0.00023402, dtype=float32), 'training/total_loss': Array(1284.1133, dtype=float32), 'training/v_loss': Array(1284.1069, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.06777766, 0.07489049], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([4.461691 , 6.5322204], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-7824.139,  8726.471], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.05067535, 0.07856016], dtype=float32), 'eval/avg_episode_length': Array([996.40625 ,  40.499508], dtype=float32), 'eval/epoch_eval_time': 3.8718550205230713, 'eval/sps': 33059.089072686336}
I0728 20:14:13.097760 140034207004480 train.py:379] starting iteration 7, 77987840 steps, 313.90184593200684
I0728 20:14:53.866939 140034207004480 train.py:394] {'eval/walltime': 45.881030321121216, 'training/sps': 302023.79692891834, 'training/walltime': 302.14956164360046, 'training/entropy_loss': Array(0.00413411, dtype=float32), 'training/policy_loss': Array(-0.00140544, dtype=float32), 'training/total_loss': Array(1969.2004, dtype=float32), 'training/v_loss': Array(1969.1978, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04268527, 0.03346049], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.3910913, 2.8157065], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5782.615,  5422.13 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.02421605, 0.03282738], dtype=float32), 'eval/avg_episode_length': Array([964.77344, 141.71974], dtype=float32), 'eval/epoch_eval_time': 3.8768277168273926, 'eval/sps': 33016.68512232702}
I0728 20:14:53.868636 140034207004480 train.py:379] starting iteration 8, 89128960 steps, 354.6727223396301
I0728 20:15:34.792091 140034207004480 train.py:394] {'eval/walltime': 49.739604473114014, 'training/sps': 300617.73846834735, 'training/walltime': 339.2103154659271, 'training/entropy_loss': Array(3.868159e-05, dtype=float32), 'training/policy_loss': Array(-0.00153935, dtype=float32), 'training/total_loss': Array(6124.943, dtype=float32), 'training/v_loss': Array(6124.9443, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.0392234 , 0.03713088], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.5615294, 2.9251513], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6604.3994,  6426.182 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.02401922, 0.03354779], dtype=float32), 'eval/avg_episode_length': Array([787.375 , 309.7454], dtype=float32), 'eval/epoch_eval_time': 3.858574151992798, 'eval/sps': 33172.87551254993}
I0728 20:15:35.133387 140034207004480 train.py:410] total steps: 100270080
