I0728 17:32:50.020871 140644997207872 low_level_env.py:188] Initialising environment...
I0728 17:32:50.369621 140644997207872 low_level_env.py:294] Environment initialised.
I0728 17:32:50.373847 140644997207872 train.py:118] JAX is running on GPU.
I0728 17:32:50.373902 140644997207872 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 17:32:56.594851 140644997207872 train.py:367] Running initial eval
I0728 17:33:11.929801 140644997207872 train.py:373] {'eval/walltime': 15.198944330215454, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31136602, 0.1325948 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.306282, 11.368146], dtype=float32), 'eval/episode_reward': Array([-26280.174 ,  10047.2705], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.306863  , 0.13567431], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.198944330215454, 'eval/sps': 8421.637530807742}
I0728 17:33:11.930968 140644997207872 train.py:379] starting iteration 0, 0 steps, 21.557132244110107
I0728 17:34:02.697269 140644997207872 train.py:394] {'eval/walltime': 18.81417942047119, 'training/sps': 236315.02454532555, 'training/walltime': 47.145203828811646, 'training/entropy_loss': Array(-0.03034966, dtype=float32), 'training/policy_loss': Array(-0.00472216, dtype=float32), 'training/total_loss': Array(22059.666, dtype=float32), 'training/v_loss': Array(22059.703, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.13083723, 0.10506427], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([10.860366,  8.9556  ], dtype=float32), 'eval/episode_reward': Array([-15034.334 ,   7496.3193], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.11719402, 0.11088872], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6152350902557373, 'eval/sps': 35405.71962941017}
I0728 17:34:02.731352 140644997207872 train.py:379] starting iteration 1, 11141120 steps, 72.35751557350159
I0728 17:34:41.493790 140644997207872 train.py:394] {'eval/walltime': 22.44580912590027, 'training/sps': 317167.18453704263, 'training/walltime': 82.27216696739197, 'training/entropy_loss': Array(-0.00170176, dtype=float32), 'training/policy_loss': Array(-0.00470972, dtype=float32), 'training/total_loss': Array(2308.724, dtype=float32), 'training/v_loss': Array(2308.7305, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.10216361, 0.11035216], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([8.10313 , 9.441208], dtype=float32), 'eval/episode_reward': Array([-11089.182,   8640.189], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08466729, 0.11712722], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.631629705429077, 'eval/sps': 35245.884184901166}
I0728 17:34:41.495699 140644997207872 train.py:379] starting iteration 2, 22282240 steps, 111.1218638420105
I0728 17:35:20.350886 140644997207872 train.py:394] {'eval/walltime': 26.09999656677246, 'training/sps': 316532.8047934395, 'training/walltime': 117.46952986717224, 'training/entropy_loss': Array(0.01743784, dtype=float32), 'training/policy_loss': Array(-0.00562988, dtype=float32), 'training/total_loss': Array(2027.338, dtype=float32), 'training/v_loss': Array(2027.3262, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11851754, 0.13108814], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 9.005377, 11.396389], dtype=float32), 'eval/episode_reward': Array([-10945.257,  10575.93 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10537438, 0.13659407], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6541874408721924, 'eval/sps': 35028.307132884394}
I0728 17:35:20.352455 140644997207872 train.py:379] starting iteration 3, 33423360 steps, 149.9786195755005
I0728 17:35:59.377307 140644997207872 train.py:394] {'eval/walltime': 29.7507381439209, 'training/sps': 314987.17075854744, 'training/walltime': 152.8396053314209, 'training/entropy_loss': Array(0.04119174, dtype=float32), 'training/policy_loss': Array(-0.00559134, dtype=float32), 'training/total_loss': Array(1340.6572, dtype=float32), 'training/v_loss': Array(1340.6218, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.10900579, 0.13446483], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 8.04227 , 11.710207], dtype=float32), 'eval/episode_reward': Array([-9819.391, 11031.626], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09471136, 0.1400774 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6507415771484375, 'eval/sps': 35061.36966834549}
I0728 17:35:59.378878 140644997207872 train.py:379] starting iteration 4, 44564480 steps, 189.00504279136658
I0728 17:36:39.181838 140644997207872 train.py:394] {'eval/walltime': 33.486780881881714, 'training/sps': 308945.91041478864, 'training/walltime': 188.90132236480713, 'training/entropy_loss': Array(0.0551226, dtype=float32), 'training/policy_loss': Array(0.00277464, dtype=float32), 'training/total_loss': Array(1282.0901, dtype=float32), 'training/v_loss': Array(1282.0322, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09597103, 0.12615423], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 7.371897, 10.740501], dtype=float32), 'eval/episode_reward': Array([-9712.725,  9954.984], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08712642, 0.12834667], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.7360427379608154, 'eval/sps': 34260.85004312991}
I0728 17:36:39.183423 140644997207872 train.py:379] starting iteration 5, 55705600 steps, 228.809588432312
I0728 17:37:19.229708 140644997207872 train.py:394] {'eval/walltime': 37.30015015602112, 'training/sps': 307517.2304433136, 'training/walltime': 225.13057684898376, 'training/entropy_loss': Array(0.05608481, dtype=float32), 'training/policy_loss': Array(-0.00718582, dtype=float32), 'training/total_loss': Array(1104.3066, dtype=float32), 'training/v_loss': Array(1104.2577, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09397419, 0.11579702], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([6.981453, 9.876192], dtype=float32), 'eval/episode_reward': Array([-8469.057,  9372.921], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08277808, 0.11893713], dtype=float32), 'eval/avg_episode_length': Array([986.96094, 103.4998 ], dtype=float32), 'eval/epoch_eval_time': 3.8133692741394043, 'eval/sps': 33566.11720455183}
I0728 17:37:19.231307 140644997207872 train.py:379] starting iteration 6, 66846720 steps, 268.85747170448303
I0728 17:37:59.309709 140644997207872 train.py:394] {'eval/walltime': 41.12992525100708, 'training/sps': 307383.52281949436, 'training/walltime': 261.37559056282043, 'training/entropy_loss': Array(0.07036167, dtype=float32), 'training/policy_loss': Array(-0.00313269, dtype=float32), 'training/total_loss': Array(1109.2499, dtype=float32), 'training/v_loss': Array(1109.1826, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.10163116, 0.13076255], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 7.6652493, 11.125551 ], dtype=float32), 'eval/episode_reward': Array([-9212.256, 10546.965], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09083195, 0.13389394], dtype=float32), 'eval/avg_episode_length': Array([989.0078 ,  74.12874], dtype=float32), 'eval/epoch_eval_time': 3.829775094985962, 'eval/sps': 33422.328159055825}
I0728 17:37:59.311280 140644997207872 train.py:379] starting iteration 7, 77987840 steps, 308.9374449253082
I0728 17:38:39.447319 140644997207872 train.py:394] {'eval/walltime': 44.98089957237244, 'training/sps': 307078.14467730897, 'training/walltime': 297.65664863586426, 'training/entropy_loss': Array(0.0733779, dtype=float32), 'training/policy_loss': Array(-0.00137526, dtype=float32), 'training/total_loss': Array(1063.5095, dtype=float32), 'training/v_loss': Array(1063.4375, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09994528, 0.12103241], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 7.6905913, 10.196445 ], dtype=float32), 'eval/episode_reward': Array([-9062.368,  9643.71 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09090096, 0.1229554 ], dtype=float32), 'eval/avg_episode_length': Array([926.90625, 214.52155], dtype=float32), 'eval/epoch_eval_time': 3.8509743213653564, 'eval/sps': 33238.34160353939}
I0728 17:38:39.448919 140644997207872 train.py:379] starting iteration 8, 89128960 steps, 349.07508420944214
I0728 17:39:19.566253 140644997207872 train.py:394] {'eval/walltime': 48.87812352180481, 'training/sps': 307629.00663953525, 'training/walltime': 333.87273931503296, 'training/entropy_loss': Array(0.06551811, dtype=float32), 'training/policy_loss': Array(0.0014073, dtype=float32), 'training/total_loss': Array(1295.7787, dtype=float32), 'training/v_loss': Array(1295.7118, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09292135, 0.12798275], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 7.329451, 10.701894], dtype=float32), 'eval/episode_reward': Array([-8683.041, 10229.295], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08637153, 0.12858982], dtype=float32), 'eval/avg_episode_length': Array([758.1172 , 335.97922], dtype=float32), 'eval/epoch_eval_time': 3.897223949432373, 'eval/sps': 32843.89135980832}
I0728 17:39:19.924687 140644997207872 train.py:410] total steps: 100270080
