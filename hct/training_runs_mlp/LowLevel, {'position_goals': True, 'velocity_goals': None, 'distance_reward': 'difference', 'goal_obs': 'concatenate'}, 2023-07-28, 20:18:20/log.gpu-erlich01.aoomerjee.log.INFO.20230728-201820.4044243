I0728 20:18:20.373388 139653538875200 low_level_env.py:190] Initialising environment...
I0728 20:18:20.726998 139653538875200 low_level_env.py:298] Environment initialised.
I0728 20:18:20.732214 139653538875200 train.py:118] JAX is running on GPU.
I0728 20:18:20.732265 139653538875200 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 20:18:26.985874 139653538875200 train.py:367] Running initial eval
I0728 20:18:42.536152 139653538875200 train.py:373] {'eval/walltime': 15.416944026947021, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31400037, 0.13993669], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.557377, 12.053771], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-2.9973423,  7.6953855], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30913103, 0.1435872 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.416944026947021, 'eval/sps': 8302.553331987903}
I0728 20:18:42.537468 139653538875200 train.py:379] starting iteration 0, 0 steps, 21.805268049240112
I0728 20:19:33.859288 139653538875200 train.py:394] {'eval/walltime': 19.08045768737793, 'training/sps': 233793.96232100952, 'training/walltime': 47.65358304977417, 'training/entropy_loss': Array(-0.04338607, dtype=float32), 'training/policy_loss': Array(-0.00580862, dtype=float32), 'training/total_loss': Array(4.803473, dtype=float32), 'training/v_loss': Array(4.8526683, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.12317249, 0.10481641], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([9.807236, 8.891715], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([13.709723, 10.222179], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10602046, 0.11257808], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.663513660430908, 'eval/sps': 34939.1354487114}
I0728 20:19:33.874321 139653538875200 train.py:379] starting iteration 1, 11141120 steps, 73.14212274551392
I0728 20:20:13.249884 139653538875200 train.py:394] {'eval/walltime': 22.765095949172974, 'training/sps': 312193.51439571765, 'training/walltime': 83.34016680717468, 'training/entropy_loss': Array(-0.02460489, dtype=float32), 'training/policy_loss': Array(-0.00602168, dtype=float32), 'training/total_loss': Array(23.723846, dtype=float32), 'training/v_loss': Array(23.754475, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.07284458, 0.06417885], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([5.3871346, 5.418348 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([18.371975,  8.613414], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.04801335, 0.07056805], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.684638261795044, 'eval/sps': 34738.82397824374}
I0728 20:20:13.251846 139653538875200 train.py:379] starting iteration 2, 22282240 steps, 112.51964712142944
I0728 20:20:53.256975 139653538875200 train.py:394] {'eval/walltime': 26.50292944908142, 'training/sps': 307231.57695617026, 'training/walltime': 119.6031060218811, 'training/entropy_loss': Array(-0.01696471, dtype=float32), 'training/policy_loss': Array(-0.00423674, dtype=float32), 'training/total_loss': Array(129.3633, dtype=float32), 'training/v_loss': Array(129.38449, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.07634701, 0.06786963], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([5.579854 , 5.7192616], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([22.206444, 45.438675], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.05344395, 0.07378361], dtype=float32), 'eval/avg_episode_length': Array([996.9453  ,  34.424576], dtype=float32), 'eval/epoch_eval_time': 3.7378334999084473, 'eval/sps': 34244.43598227025}
I0728 20:20:53.258608 139653538875200 train.py:379] starting iteration 3, 33423360 steps, 152.52640962600708
I0728 20:21:33.794409 139653538875200 train.py:394] {'eval/walltime': 30.35612463951111, 'training/sps': 303768.8730709544, 'training/walltime': 156.27941155433655, 'training/entropy_loss': Array(-0.02063452, dtype=float32), 'training/policy_loss': Array(-0.00393247, dtype=float32), 'training/total_loss': Array(346.6535, dtype=float32), 'training/v_loss': Array(346.6781, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.07445994, 0.05822507], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([5.427548, 4.877391], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([21.742256, 44.59264 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.05243503, 0.06304405], dtype=float32), 'eval/avg_episode_length': Array([998.0781  ,  21.658432], dtype=float32), 'eval/epoch_eval_time': 3.8531951904296875, 'eval/sps': 33219.18399512124}
I0728 20:21:33.797809 139653538875200 train.py:379] starting iteration 4, 44564480 steps, 193.0655961036682
I0728 20:22:14.413299 139653538875200 train.py:394] {'eval/walltime': 34.22003197669983, 'training/sps': 303183.7421260731, 'training/walltime': 193.0265007019043, 'training/entropy_loss': Array(-0.02863536, dtype=float32), 'training/policy_loss': Array(-0.0026585, dtype=float32), 'training/total_loss': Array(816.73987, dtype=float32), 'training/v_loss': Array(816.7711, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.10986045, 0.1157742 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([9.05006 , 9.727798], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([35.524174, 99.744514], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09618474, 0.12028854], dtype=float32), 'eval/avg_episode_length': Array([987.96094,  64.7072 ], dtype=float32), 'eval/epoch_eval_time': 3.8639073371887207, 'eval/sps': 33127.0884185151}
I0728 20:22:14.415004 139653538875200 train.py:379] starting iteration 5, 55705600 steps, 233.68280625343323
I0728 20:22:55.023575 139653538875200 train.py:394] {'eval/walltime': 38.14652109146118, 'training/sps': 303755.5938961884, 'training/walltime': 229.7044095993042, 'training/entropy_loss': Array(-0.04140381, dtype=float32), 'training/policy_loss': Array(-0.00193179, dtype=float32), 'training/total_loss': Array(1036.6189, dtype=float32), 'training/v_loss': Array(1036.6622, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.14432497, 0.13320471], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([11.936609, 11.374554], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([ 32.15589, 108.35124], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.13370459, 0.13754596], dtype=float32), 'eval/avg_episode_length': Array([982.4297 ,  94.25262], dtype=float32), 'eval/epoch_eval_time': 3.9264891147613525, 'eval/sps': 32599.097121851995}
I0728 20:22:55.025283 139653538875200 train.py:379] starting iteration 6, 66846720 steps, 274.2930850982666
I0728 20:23:35.622621 139653538875200 train.py:394] {'eval/walltime': 42.07832860946655, 'training/sps': 303892.18490777415, 'training/walltime': 266.36583280563354, 'training/entropy_loss': Array(-0.04820166, dtype=float32), 'training/policy_loss': Array(-0.00153752, dtype=float32), 'training/total_loss': Array(924.32983, dtype=float32), 'training/v_loss': Array(924.3795, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.21654812, 0.1600732 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.107773, 13.662669], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([ 40.177826, 131.36975 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.20862009, 0.16433375], dtype=float32), 'eval/avg_episode_length': Array([967.1094, 128.7638], dtype=float32), 'eval/epoch_eval_time': 3.931807518005371, 'eval/sps': 32555.001589939264}
I0728 20:23:35.624328 139653538875200 train.py:379] starting iteration 7, 77987840 steps, 314.892128944397
I0728 20:24:16.087623 139653538875200 train.py:394] {'eval/walltime': 45.994423389434814, 'training/sps': 304876.9963583149, 'training/walltime': 302.9088325500488, 'training/entropy_loss': Array(-0.04943229, dtype=float32), 'training/policy_loss': Array(-0.0013847, dtype=float32), 'training/total_loss': Array(1255.4299, dtype=float32), 'training/v_loss': Array(1255.4807, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.23741673, 0.1587792 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([19.920841, 13.560637], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([ 41.92545, 138.30003], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.23123792, 0.16240631], dtype=float32), 'eval/avg_episode_length': Array([955.39844, 162.25821], dtype=float32), 'eval/epoch_eval_time': 3.9160947799682617, 'eval/sps': 32685.623610222578}
I0728 20:24:16.089313 139653538875200 train.py:379] starting iteration 8, 89128960 steps, 355.35711455345154
I0728 20:24:56.714494 139653538875200 train.py:394] {'eval/walltime': 49.900141954422, 'training/sps': 303447.7854799724, 'training/walltime': 339.62394642829895, 'training/entropy_loss': Array(-0.04956635, dtype=float32), 'training/policy_loss': Array(-0.00143232, dtype=float32), 'training/total_loss': Array(1718.9961, dtype=float32), 'training/v_loss': Array(1719.047, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.24339037, 0.16968791], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.496815, 14.463717], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([100.42255, 205.7351 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.23889574, 0.17116793], dtype=float32), 'eval/avg_episode_length': Array([902.8594 , 240.72705], dtype=float32), 'eval/epoch_eval_time': 3.9057185649871826, 'eval/sps': 32772.45860658167}
I0728 20:24:57.060704 139653538875200 train.py:410] total steps: 100270080
