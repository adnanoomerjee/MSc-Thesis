I0728 20:40:50.786568 140583248410432 low_level_env.py:190] Initialising environment...
I0728 20:40:51.107692 140583248410432 low_level_env.py:298] Environment initialised.
I0728 20:40:51.111863 140583248410432 train.py:118] JAX is running on GPU.
I0728 20:40:51.111912 140583248410432 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 20:40:57.335572 140583248410432 train.py:367] Running initial eval
I0728 20:41:12.872617 140583248410432 train.py:373] {'eval/walltime': 15.3915536403656, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32374308, 0.13015793], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.375633, 11.15229 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-25567.057,   9790.785], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31965357, 0.13279371], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.3915536403656, 'eval/sps': 8316.249482723408}
I0728 20:41:12.873844 140583248410432 train.py:379] starting iteration 0, 0 steps, 21.761993408203125
I0728 20:41:32.850435 140583248410432 train.py:394] {'eval/walltime': 18.98575234413147, 'training/sps': 80028.69588159437, 'training/walltime': 16.378125190734863, 'training/entropy_loss': Array(-0.04852688, dtype=float32), 'training/policy_loss': Array(-0.00160611, dtype=float32), 'training/total_loss': Array(115200.914, dtype=float32), 'training/v_loss': Array(115200.96, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2947163 , 0.10878543], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.947086,  9.307743], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-24018.25  ,   8123.4556], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.29091552, 0.1109992 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.594198703765869, 'eval/sps': 35612.9447895817}
I0728 20:41:32.864837 140583248410432 train.py:379] starting iteration 1, 1310720 steps, 41.752986669540405
I0728 20:41:40.606001 140583248410432 train.py:394] {'eval/walltime': 22.57722234725952, 'training/sps': 316162.5375641405, 'training/walltime': 20.52384090423584, 'training/entropy_loss': Array(-0.04356462, dtype=float32), 'training/policy_loss': Array(-0.00987455, dtype=float32), 'training/total_loss': Array(33252.9, dtype=float32), 'training/v_loss': Array(33252.953, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2878804 , 0.11409691], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.355227,  9.774628], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-24153.014,   8595.527], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2837116 , 0.11659206], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.5914700031280518, 'eval/sps': 35640.002530583915}
I0728 20:41:40.607642 140583248410432 train.py:379] starting iteration 2, 2621440 steps, 49.49579215049744
I0728 20:41:48.381422 140583248410432 train.py:394] {'eval/walltime': 26.20005702972412, 'training/sps': 316052.5723607575, 'training/walltime': 24.67099905014038, 'training/entropy_loss': Array(-0.0395219, dtype=float32), 'training/policy_loss': Array(-0.00690902, dtype=float32), 'training/total_loss': Array(9363.569, dtype=float32), 'training/v_loss': Array(9363.615, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.28929198, 0.10541549], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([24.443163,  9.10519 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-24305.023,   8387.   ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2850988 , 0.10790429], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6228346824645996, 'eval/sps': 35331.44932600737}
I0728 20:41:48.383043 140583248410432 train.py:379] starting iteration 3, 3932160 steps, 57.27119207382202
I0728 20:41:56.176673 140583248410432 train.py:394] {'eval/walltime': 29.8198082447052, 'training/sps': 314320.92383702914, 'training/walltime': 28.841004610061646, 'training/entropy_loss': Array(-0.03583126, dtype=float32), 'training/policy_loss': Array(-0.00515573, dtype=float32), 'training/total_loss': Array(4982.3574, dtype=float32), 'training/v_loss': Array(4982.3984, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.24700093, 0.10387564], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([20.876339,  8.926622], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-21825.828,   8579.182], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.24166387, 0.10740627], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.619751214981079, 'eval/sps': 35361.54624943446}
I0728 20:41:56.178314 140583248410432 train.py:379] starting iteration 4, 5242880 steps, 65.0664644241333
I0728 20:42:03.967023 140583248410432 train.py:394] {'eval/walltime': 33.43313765525818, 'training/sps': 314220.9819137516, 'training/walltime': 33.01233649253845, 'training/entropy_loss': Array(-0.03210228, dtype=float32), 'training/policy_loss': Array(-0.005213, dtype=float32), 'training/total_loss': Array(2882.6548, dtype=float32), 'training/v_loss': Array(2882.6921, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2172541 , 0.08715413], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.334919 ,  7.4652815], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-21401.281,   7241.551], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.2118075 , 0.08990148], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6133294105529785, 'eval/sps': 35424.39270169145}
I0728 20:42:03.968711 140583248410432 train.py:379] starting iteration 5, 6553600 steps, 72.85686039924622
I0728 20:42:11.767582 140583248410432 train.py:394] {'eval/walltime': 37.053040742874146, 'training/sps': 313941.4178021261, 'training/walltime': 37.18738293647766, 'training/entropy_loss': Array(-0.03031713, dtype=float32), 'training/policy_loss': Array(-0.00580776, dtype=float32), 'training/total_loss': Array(1697.5725, dtype=float32), 'training/v_loss': Array(1697.6085, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16078997, 0.09997055], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.432354,  8.587839], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-16352.264 ,   7905.4966], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15117934, 0.10402936], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.619903087615967, 'eval/sps': 35360.06265966075}
I0728 20:42:11.769266 140583248410432 train.py:379] starting iteration 6, 7864320 steps, 80.65741610527039
I0728 20:42:19.565654 140583248410432 train.py:394] {'eval/walltime': 40.67384600639343, 'training/sps': 314221.77214518463, 'training/walltime': 41.35870432853699, 'training/entropy_loss': Array(-0.02806782, dtype=float32), 'training/policy_loss': Array(-0.00436277, dtype=float32), 'training/total_loss': Array(2064.3337, dtype=float32), 'training/v_loss': Array(2064.3662, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.16018781, 0.12215059], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([13.350159, 10.378879], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-16742.04 ,   9168.074], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.15010247, 0.1265158 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.620805263519287, 'eval/sps': 35351.25218957199}
I0728 20:42:19.567357 140583248410432 train.py:379] starting iteration 7, 9175040 steps, 88.45550680160522
I0728 20:42:27.389080 140583248410432 train.py:394] {'eval/walltime': 44.31381440162659, 'training/sps': 313734.21944293834, 'training/walltime': 45.536508083343506, 'training/entropy_loss': Array(-0.02669593, dtype=float32), 'training/policy_loss': Array(-0.00441242, dtype=float32), 'training/total_loss': Array(1854.0428, dtype=float32), 'training/v_loss': Array(1854.074, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1223969 , 0.10866894], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([10.051083,  9.291237], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-13725.606 ,   7781.4556], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10860713, 0.11441869], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6399683952331543, 'eval/sps': 35165.14049067756}
I0728 20:42:27.390701 140583248410432 train.py:379] starting iteration 8, 10485760 steps, 96.27885150909424
I0728 20:42:35.218767 140583248410432 train.py:394] {'eval/walltime': 47.9504599571228, 'training/sps': 312994.4313986089, 'training/walltime': 49.724186420440674, 'training/entropy_loss': Array(-0.02580503, dtype=float32), 'training/policy_loss': Array(0.00035816, dtype=float32), 'training/total_loss': Array(2121.0718, dtype=float32), 'training/v_loss': Array(2121.0972, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.13629223, 0.11716234], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([11.239462,  9.999894], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-14174.374,   8647.214], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.12162288, 0.12387553], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.636645555496216, 'eval/sps': 35197.27123435172}
I0728 20:42:35.220355 140583248410432 train.py:379] starting iteration 9, 11796480 steps, 104.10850429534912
I0728 20:42:43.040252 140583248410432 train.py:394] {'eval/walltime': 51.58934044837952, 'training/sps': 313813.178751462, 'training/walltime': 53.900938987731934, 'training/entropy_loss': Array(-0.02229017, dtype=float32), 'training/policy_loss': Array(0.00097204, dtype=float32), 'training/total_loss': Array(2228.0195, dtype=float32), 'training/v_loss': Array(2228.041, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.13127744, 0.12076582], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([10.810658, 10.278611], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-13362.769,   9004.752], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.11628786, 0.12735367], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.638880491256714, 'eval/sps': 35175.653695566754}
I0728 20:42:43.041738 140583248410432 train.py:379] starting iteration 10, 13107200 steps, 111.92988848686218
I0728 20:42:50.866466 140583248410432 train.py:394] {'eval/walltime': 55.228715896606445, 'training/sps': 313441.83079337433, 'training/walltime': 58.082639932632446, 'training/entropy_loss': Array(-0.01985369, dtype=float32), 'training/policy_loss': Array(-0.00730753, dtype=float32), 'training/total_loss': Array(963.619, dtype=float32), 'training/v_loss': Array(963.6461, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.13292065, 0.12090458], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([10.832108, 10.34089 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-13303.408,   9197.58 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.11857948, 0.12745717], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6393754482269287, 'eval/sps': 35170.86978821063}
I0728 20:42:50.868039 140583248410432 train.py:379] starting iteration 11, 14417920 steps, 119.7561891078949
I0728 20:42:58.697625 140583248410432 train.py:394] {'eval/walltime': 58.86310935020447, 'training/sps': 312776.699988388, 'training/walltime': 62.27323341369629, 'training/entropy_loss': Array(-0.01833523, dtype=float32), 'training/policy_loss': Array(-0.0035624, dtype=float32), 'training/total_loss': Array(1380.5767, dtype=float32), 'training/v_loss': Array(1380.5986, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.12012874, 0.11519985], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([9.697042, 9.882347], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-12350.041,   8651.293], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10514098, 0.12160855], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6343934535980225, 'eval/sps': 35219.08170764532}
I0728 20:42:58.699149 140583248410432 train.py:379] starting iteration 12, 15728640 steps, 127.58729910850525
I0728 20:43:06.531991 140583248410432 train.py:394] {'eval/walltime': 62.5025999546051, 'training/sps': 312864.0802416884, 'training/walltime': 66.46265649795532, 'training/entropy_loss': Array(-0.01401302, dtype=float32), 'training/policy_loss': Array(-0.00179116, dtype=float32), 'training/total_loss': Array(2521.9258, dtype=float32), 'training/v_loss': Array(2521.9414, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11077075, 0.11198599], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([8.773132, 9.599466], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-11101.979,   8806.609], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.094998  , 0.11846393], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6394906044006348, 'eval/sps': 35169.756955885736}
I0728 20:43:06.533493 140583248410432 train.py:379] starting iteration 13, 17039360 steps, 135.42164373397827
I0728 20:43:14.358252 140583248410432 train.py:394] {'eval/walltime': 66.14196300506592, 'training/sps': 313463.63469029294, 'training/walltime': 70.64406657218933, 'training/entropy_loss': Array(-0.01558379, dtype=float32), 'training/policy_loss': Array(-0.00843102, dtype=float32), 'training/total_loss': Array(466.49612, dtype=float32), 'training/v_loss': Array(466.5201, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11795467, 0.11223055], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([9.28042 , 9.772138], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-12003.697,   8708.899], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10304189, 0.11882637], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6393630504608154, 'eval/sps': 35170.98960044469}
I0728 20:43:14.359753 140583248410432 train.py:379] starting iteration 14, 18350080 steps, 143.24790287017822
I0728 20:43:22.178677 140583248410432 train.py:394] {'eval/walltime': 69.7730827331543, 'training/sps': 313288.55641693086, 'training/walltime': 74.82781338691711, 'training/entropy_loss': Array(-0.0112359, dtype=float32), 'training/policy_loss': Array(-0.0062244, dtype=float32), 'training/total_loss': Array(1372.6404, dtype=float32), 'training/v_loss': Array(1372.658, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1265166 , 0.12296363], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([10.109695, 10.589935], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-12084.315,   9842.004], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.11234346, 0.1294121 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.631119728088379, 'eval/sps': 35250.834339022535}
I0728 20:43:22.180170 140583248410432 train.py:379] starting iteration 15, 19660800 steps, 151.0683205127716
I0728 20:43:29.999066 140583248410432 train.py:394] {'eval/walltime': 73.40845465660095, 'training/sps': 313602.07081165403, 'training/walltime': 79.00737762451172, 'training/entropy_loss': Array(-0.00877153, dtype=float32), 'training/policy_loss': Array(-0.00489201, dtype=float32), 'training/total_loss': Array(1386.5084, dtype=float32), 'training/v_loss': Array(1386.522, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.12659147, 0.1240477 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 9.9957695, 10.724491 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-11963.127,   9989.044], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.11257357, 0.13025017], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6353719234466553, 'eval/sps': 35209.60239981296}
I0728 20:43:30.000564 140583248410432 train.py:379] starting iteration 16, 20971520 steps, 158.88871455192566
I0728 20:43:37.836927 140583248410432 train.py:394] {'eval/walltime': 77.05405712127686, 'training/sps': 313040.85892378516, 'training/walltime': 83.19443488121033, 'training/entropy_loss': Array(-0.01133954, dtype=float32), 'training/policy_loss': Array(-0.00553575, dtype=float32), 'training/total_loss': Array(394.28125, dtype=float32), 'training/v_loss': Array(394.2981, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1236795 , 0.12148894], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 9.677913, 10.521207], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-11632.145,   9698.483], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10895182, 0.12804735], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6456024646759033, 'eval/sps': 35110.794783648824}
I0728 20:43:37.838474 140583248410432 train.py:379] starting iteration 17, 22282240 steps, 166.7266240119934
I0728 20:43:45.676928 140583248410432 train.py:394] {'eval/walltime': 80.70183825492859, 'training/sps': 313061.0739040117, 'training/walltime': 87.38122177124023, 'training/entropy_loss': Array(-0.00765714, dtype=float32), 'training/policy_loss': Array(-0.0035131, dtype=float32), 'training/total_loss': Array(1500.0989, dtype=float32), 'training/v_loss': Array(1500.1101, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11300281, 0.10835917], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([8.783107, 9.403656], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-10756.47 ,   8879.879], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09771102, 0.11448816], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6477811336517334, 'eval/sps': 35089.82455640405}
I0728 20:43:45.678419 140583248410432 train.py:379] starting iteration 18, 23592960 steps, 174.5665693283081
I0728 20:43:53.533265 140583248410432 train.py:394] {'eval/walltime': 84.35109376907349, 'training/sps': 311960.0701506937, 'training/walltime': 91.58278512954712, 'training/entropy_loss': Array(-0.00674501, dtype=float32), 'training/policy_loss': Array(-0.00437697, dtype=float32), 'training/total_loss': Array(1728.9722, dtype=float32), 'training/v_loss': Array(1728.9834, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.12559325, 0.13007884], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 9.691011, 11.29395 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-11644.908,  10637.241], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10964842, 0.13719182], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6492555141448975, 'eval/sps': 35075.64748586624}
I0728 20:43:53.534758 140583248410432 train.py:379] starting iteration 19, 24903680 steps, 182.42290711402893
I0728 20:44:01.371592 140583248410432 train.py:394] {'eval/walltime': 88.00018572807312, 'training/sps': 313294.162462861, 'training/walltime': 95.76645708084106, 'training/entropy_loss': Array(-0.00945987, dtype=float32), 'training/policy_loss': Array(-0.00546281, dtype=float32), 'training/total_loss': Array(538.1063, dtype=float32), 'training/v_loss': Array(538.1213, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.0969782 , 0.10230395], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([7.2176924, 8.9577465], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-9537.546,  8373.868], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.07988068, 0.10823088], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.649091958999634, 'eval/sps': 35077.21960372028}
I0728 20:44:01.373085 140583248410432 train.py:379] starting iteration 20, 26214400 steps, 190.26123571395874
I0728 20:44:09.226174 140583248410432 train.py:394] {'eval/walltime': 91.6544349193573, 'training/sps': 312459.16768123646, 'training/walltime': 99.96130919456482, 'training/entropy_loss': Array(-0.00503432, dtype=float32), 'training/policy_loss': Array(-0.00340884, dtype=float32), 'training/total_loss': Array(1633.5009, dtype=float32), 'training/v_loss': Array(1633.5093, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.12484039, 0.12787932], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 9.603717, 11.116172], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-11479.786,  10232.138], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10923034, 0.13479331], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6542491912841797, 'eval/sps': 35027.715215835655}
I0728 20:44:09.227672 140583248410432 train.py:379] starting iteration 21, 27525120 steps, 198.1158218383789
I0728 20:44:17.068528 140583248410432 train.py:394] {'eval/walltime': 95.30358910560608, 'training/sps': 312994.34229951823, 'training/walltime': 104.14898872375488, 'training/entropy_loss': Array(-0.00596575, dtype=float32), 'training/policy_loss': Array(-0.00318955, dtype=float32), 'training/total_loss': Array(1242.7938, dtype=float32), 'training/v_loss': Array(1242.803, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.12612316, 0.13460639], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 9.626192, 11.752646], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-11465.379,  11125.649], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10993915, 0.14192244], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6491541862487793, 'eval/sps': 35076.62144897751}
I0728 20:44:17.070013 140583248410432 train.py:379] starting iteration 22, 28835840 steps, 205.95816326141357
I0728 20:44:24.930032 140583248410432 train.py:394] {'eval/walltime': 98.9607162475586, 'training/sps': 312141.19834607514, 'training/walltime': 108.34811401367188, 'training/entropy_loss': Array(-0.00711061, dtype=float32), 'training/policy_loss': Array(-0.00539436, dtype=float32), 'training/total_loss': Array(696.9978, dtype=float32), 'training/v_loss': Array(697.0103, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11404294, 0.12809715], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 8.470602, 11.224462], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-10685.158,  10502.709], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09716885, 0.13504827], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6571271419525146, 'eval/sps': 35000.15039992886}
I0728 20:44:24.931597 140583248410432 train.py:379] starting iteration 23, 30146560 steps, 213.81974720954895
I0728 20:44:32.772696 140583248410432 train.py:394] {'eval/walltime': 102.61351752281189, 'training/sps': 313221.67427079374, 'training/walltime': 112.53275418281555, 'training/entropy_loss': Array(-0.0034138, dtype=float32), 'training/policy_loss': Array(0.00145073, dtype=float32), 'training/total_loss': Array(1581.3542, dtype=float32), 'training/v_loss': Array(1581.3562, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.10237499, 0.10933557], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([7.4892473, 9.596468 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-9455.788,  9094.848], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.0851039 , 0.11573287], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.652801275253296, 'eval/sps': 35041.59968054219}
I0728 20:44:32.774196 140583248410432 train.py:379] starting iteration 24, 31457280 steps, 221.66234636306763
I0728 20:44:40.624907 140583248410432 train.py:394] {'eval/walltime': 106.26525282859802, 'training/sps': 312434.12960125075, 'training/walltime': 116.72794246673584, 'training/entropy_loss': Array(-0.0070258, dtype=float32), 'training/policy_loss': Array(-0.00624744, dtype=float32), 'training/total_loss': Array(204.94458, dtype=float32), 'training/v_loss': Array(204.95786, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09521497, 0.11083165], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([6.741185, 9.769926], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-9133.916,  9136.315], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.077463 , 0.1172533], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.651735305786133, 'eval/sps': 35051.82859151524}
I0728 20:44:40.626412 140583248410432 train.py:379] starting iteration 25, 32768000 steps, 229.51456212997437
I0728 20:44:48.471193 140583248410432 train.py:394] {'eval/walltime': 109.91316986083984, 'training/sps': 312609.96042874566, 'training/walltime': 120.92077112197876, 'training/entropy_loss': Array(-0.00046446, dtype=float32), 'training/policy_loss': Array(-0.00207982, dtype=float32), 'training/total_loss': Array(2223.1365, dtype=float32), 'training/v_loss': Array(2223.139, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11430465, 0.12455545], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 8.401853, 10.993901], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-10166.246 ,  10449.3125], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09805819, 0.13156253], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6479170322418213, 'eval/sps': 35088.517328843365}
I0728 20:44:48.472752 140583248410432 train.py:379] starting iteration 26, 34078720 steps, 237.36090207099915
I0728 20:44:56.334809 140583248410432 train.py:394] {'eval/walltime': 113.5662350654602, 'training/sps': 311695.18652924063, 'training/walltime': 125.12590503692627, 'training/entropy_loss': Array(0.00063856, dtype=float32), 'training/policy_loss': Array(-0.00251507, dtype=float32), 'training/total_loss': Array(1443.6134, dtype=float32), 'training/v_loss': Array(1443.6154, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11329792, 0.1154419 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 8.359898, 10.189268], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-10393.431 ,   9781.3545], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09651142, 0.12243657], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6530652046203613, 'eval/sps': 35039.067969032374}
I0728 20:44:56.336305 140583248410432 train.py:379] starting iteration 27, 35389440 steps, 245.22445464134216
I0728 20:45:04.185978 140583248410432 train.py:394] {'eval/walltime': 117.2196295261383, 'training/sps': 312668.2234723511, 'training/walltime': 129.31795239448547, 'training/entropy_loss': Array(-0.00457056, dtype=float32), 'training/policy_loss': Array(-0.00636961, dtype=float32), 'training/total_loss': Array(182.54582, dtype=float32), 'training/v_loss': Array(182.55676, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11922199, 0.13639173], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 8.730999, 12.024294], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-10840.232,  11402.433], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10267781, 0.1434681 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6533944606781006, 'eval/sps': 35035.91013170862}
I0728 20:45:04.187550 140583248410432 train.py:379] starting iteration 28, 36700160 steps, 253.07570004463196
I0728 20:45:12.053959 140583248410432 train.py:394] {'eval/walltime': 120.86935377120972, 'training/sps': 311289.13213394524, 'training/walltime': 133.52857160568237, 'training/entropy_loss': Array(0.00218011, dtype=float32), 'training/policy_loss': Array(-0.00131166, dtype=float32), 'training/total_loss': Array(1479.341, dtype=float32), 'training/v_loss': Array(1479.34, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11583166, 0.11180749], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([8.5371895, 9.905175 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-10139.644,   9565.353], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10016239, 0.11862453], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.649724245071411, 'eval/sps': 35071.142750812265}
I0728 20:45:12.057298 140583248410432 train.py:379] starting iteration 29, 38010880 steps, 260.9454321861267
I0728 20:45:19.928193 140583248410432 train.py:394] {'eval/walltime': 124.51906204223633, 'training/sps': 310854.1275749942, 'training/walltime': 137.7450830936432, 'training/entropy_loss': Array(0.00289853, dtype=float32), 'training/policy_loss': Array(0.02087872, dtype=float32), 'training/total_loss': Array(1624.4586, dtype=float32), 'training/v_loss': Array(1624.4348, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.12039312, 0.11151604], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([9.441877, 9.463107], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-11679.177,   8958.672], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10881543, 0.11509991], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6497082710266113, 'eval/sps': 35071.29625020561}
I0728 20:45:19.929807 140583248410432 train.py:379] starting iteration 30, 39321600 steps, 268.81795716285706
I0728 20:45:27.816709 140583248410432 train.py:394] {'eval/walltime': 128.17098665237427, 'training/sps': 309782.2502525002, 'training/walltime': 141.97618412971497, 'training/entropy_loss': Array(-0.00374835, dtype=float32), 'training/policy_loss': Array(-0.0060531, dtype=float32), 'training/total_loss': Array(682.0727, dtype=float32), 'training/v_loss': Array(682.0825, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11376243, 0.1152214 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([8.726847, 9.902868], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-10714.072,   9518.77 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10028544, 0.11993646], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6519246101379395, 'eval/sps': 35050.0116143321}
I0728 20:45:27.818305 140583248410432 train.py:379] starting iteration 31, 40632320 steps, 276.7064542770386
I0728 20:45:35.705888 140583248410432 train.py:394] {'eval/walltime': 131.82613515853882, 'training/sps': 310054.6975753669, 'training/walltime': 146.20356726646423, 'training/entropy_loss': Array(0.00228262, dtype=float32), 'training/policy_loss': Array(-0.00493316, dtype=float32), 'training/total_loss': Array(2378.925, dtype=float32), 'training/v_loss': Array(2378.9275, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11496564, 0.1166655 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 8.812521, 10.070915], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-10990.23 ,   9718.267], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10113914, 0.12182912], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.655148506164551, 'eval/sps': 35019.09697625774}
I0728 20:45:35.707472 140583248410432 train.py:379] starting iteration 32, 41943040 steps, 284.5956220626831
I0728 20:45:43.603078 140583248410432 train.py:394] {'eval/walltime': 135.4732642173767, 'training/sps': 308811.9557512957, 'training/walltime': 150.4479625225067, 'training/entropy_loss': Array(0.00058715, dtype=float32), 'training/policy_loss': Array(-0.00335537, dtype=float32), 'training/total_loss': Array(1727.064, dtype=float32), 'training/v_loss': Array(1727.0668, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09188423, 0.10322496], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([6.7254953, 8.9606495], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-8953.871,  8660.732], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.07677799, 0.1079872 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6471290588378906, 'eval/sps': 35096.09831048466}
I0728 20:45:43.604578 140583248410432 train.py:379] starting iteration 33, 43253760 steps, 292.49272775650024
I0728 20:45:51.526100 140583248410432 train.py:394] {'eval/walltime': 139.15141654014587, 'training/sps': 309316.7201982892, 'training/walltime': 154.68543148040771, 'training/entropy_loss': Array(-0.004363, dtype=float32), 'training/policy_loss': Array(-0.00658766, dtype=float32), 'training/total_loss': Array(546.67346, dtype=float32), 'training/v_loss': Array(546.6844, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11920775, 0.13142163], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 9.079514, 11.343939], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-11249.094,  10936.291], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1050181 , 0.13676615], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.678152322769165, 'eval/sps': 34800.08133639034}
I0728 20:45:51.528588 140583248410432 train.py:379] starting iteration 34, 44564480 steps, 300.41673851013184
I0728 20:45:59.460928 140583248410432 train.py:394] {'eval/walltime': 142.8182671070099, 'training/sps': 307569.0352381712, 'training/walltime': 158.94697880744934, 'training/entropy_loss': Array(0.00186459, dtype=float32), 'training/policy_loss': Array(-0.00353598, dtype=float32), 'training/total_loss': Array(1475.123, dtype=float32), 'training/v_loss': Array(1475.1248, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09636773, 0.1095778 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([7.0202665, 9.556021 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-9175.248,  9397.401], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08072069, 0.11484292], dtype=float32), 'eval/avg_episode_length': Array([997.16406,  31.95939], dtype=float32), 'eval/epoch_eval_time': 3.6668505668640137, 'eval/sps': 34907.34014543411}
I0728 20:45:59.462418 140583248410432 train.py:379] starting iteration 35, 45875200 steps, 308.3505685329437
I0728 20:46:07.394412 140583248410432 train.py:394] {'eval/walltime': 146.48743343353271, 'training/sps': 307766.5481414568, 'training/walltime': 163.2057912349701, 'training/entropy_loss': Array(-0.00169902, dtype=float32), 'training/policy_loss': Array(-0.00421861, dtype=float32), 'training/total_loss': Array(762.9988, dtype=float32), 'training/v_loss': Array(763.00476, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11046027, 0.11879782], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 8.238176, 10.333576], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-10051.644,  10122.67 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09689184, 0.12401558], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.669166326522827, 'eval/sps': 34885.30870752383}
I0728 20:46:07.395910 140583248410432 train.py:379] starting iteration 36, 47185920 steps, 316.2840609550476
I0728 20:46:15.348292 140583248410432 train.py:394] {'eval/walltime': 150.15524649620056, 'training/sps': 306209.3553574485, 'training/walltime': 167.48626136779785, 'training/entropy_loss': Array(-8.080429e-05, dtype=float32), 'training/policy_loss': Array(-0.00485505, dtype=float32), 'training/total_loss': Array(893.5585, dtype=float32), 'training/v_loss': Array(893.5635, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09542064, 0.10052712], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([6.9044933, 8.7683935], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-9055.058,  8520.947], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08081514, 0.10513135], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6678130626678467, 'eval/sps': 34898.17987258516}
I0728 20:46:15.351308 140583248410432 train.py:379] starting iteration 37, 48496640 steps, 324.23944640159607
I0728 20:46:23.293097 140583248410432 train.py:394] {'eval/walltime': 153.82809948921204, 'training/sps': 307352.2371948125, 'training/walltime': 171.7508146762848, 'training/entropy_loss': Array(0.00465804, dtype=float32), 'training/policy_loss': Array(-0.00116666, dtype=float32), 'training/total_loss': Array(2173.0483, dtype=float32), 'training/v_loss': Array(2173.045, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.10052504, 0.11077035], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([7.3428288, 9.655753 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-9670.741,  9418.018], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08591898, 0.11587071], dtype=float32), 'eval/avg_episode_length': Array([999.7656  ,   2.641272], dtype=float32), 'eval/epoch_eval_time': 3.6728529930114746, 'eval/sps': 34850.29219616253}
I0728 20:46:23.294616 140583248410432 train.py:379] starting iteration 38, 49807360 steps, 332.18276596069336
I0728 20:46:31.251867 140583248410432 train.py:394] {'eval/walltime': 157.49937868118286, 'training/sps': 306084.20134943986, 'training/walltime': 176.03303503990173, 'training/entropy_loss': Array(-0.00190604, dtype=float32), 'training/policy_loss': Array(-0.0068149, dtype=float32), 'training/total_loss': Array(138.2729, dtype=float32), 'training/v_loss': Array(138.28162, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1116778 , 0.12531452], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 8.256048, 10.969654], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-10674.4375,  10885.195 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09664063, 0.13135716], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.671279191970825, 'eval/sps': 34865.23179167061}
I0728 20:46:31.253360 140583248410432 train.py:379] starting iteration 39, 51118080 steps, 340.1415104866028
I0728 20:46:39.205285 140583248410432 train.py:394] {'eval/walltime': 161.18569254875183, 'training/sps': 307554.4096391204, 'training/walltime': 180.2947850227356, 'training/entropy_loss': Array(0.00642366, dtype=float32), 'training/policy_loss': Array(-0.00229045, dtype=float32), 'training/total_loss': Array(1525.3838, dtype=float32), 'training/v_loss': Array(1525.3796, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09004489, 0.09330574], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([6.296735, 8.216271], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-8793.682,  8371.635], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.07333204, 0.09893498], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6863138675689697, 'eval/sps': 34723.03352302791}
I0728 20:46:39.206849 140583248410432 train.py:379] starting iteration 40, 52428800 steps, 348.09499883651733
I0728 20:46:47.183436 140583248410432 train.py:394] {'eval/walltime': 164.87468934059143, 'training/sps': 305993.92420659354, 'training/walltime': 184.5782687664032, 'training/entropy_loss': Array(0.00466673, dtype=float32), 'training/policy_loss': Array(0.0348818, dtype=float32), 'training/total_loss': Array(1490.2173, dtype=float32), 'training/v_loss': Array(1490.1777, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09934677, 0.10824997], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([7.335248, 9.316121], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-9958.227,  9142.177], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08655131, 0.11206711], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6889967918395996, 'eval/sps': 34697.780242896326}
I0728 20:46:47.184930 140583248410432 train.py:379] starting iteration 41, 53739520 steps, 356.07307982444763
I0728 20:46:55.139711 140583248410432 train.py:394] {'eval/walltime': 168.5592179298401, 'training/sps': 307237.90865381254, 'training/walltime': 188.84440898895264, 'training/entropy_loss': Array(-0.00475855, dtype=float32), 'training/policy_loss': Array(-0.00856134, dtype=float32), 'training/total_loss': Array(260.7133, dtype=float32), 'training/v_loss': Array(260.7266, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.08470807, 0.10156744], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([5.9767904, 8.818129 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-8987.551,  9074.04 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.06968784, 0.10602457], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6845285892486572, 'eval/sps': 34739.85800340921}
I0728 20:46:55.141208 140583248410432 train.py:379] starting iteration 42, 55050240 steps, 364.029358625412
I0728 20:47:03.112553 140583248410432 train.py:394] {'eval/walltime': 172.24885535240173, 'training/sps': 306389.7563565379, 'training/walltime': 193.1223587989807, 'training/entropy_loss': Array(0.00186871, dtype=float32), 'training/policy_loss': Array(-0.0043874, dtype=float32), 'training/total_loss': Array(1465.7834, dtype=float32), 'training/v_loss': Array(1465.7859, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.08923817, 0.1012802 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([6.3403263, 8.831923 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-8868.13 ,  9231.131], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.07388194, 0.10608955], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6896374225616455, 'eval/sps': 34691.75567693912}
I0728 20:47:03.114058 140583248410432 train.py:379] starting iteration 43, 56360960 steps, 372.00220799446106
I0728 20:47:11.075131 140583248410432 train.py:394] {'eval/walltime': 175.93875122070312, 'training/sps': 307185.6680880966, 'training/walltime': 197.38922452926636, 'training/entropy_loss': Array(0.00330963, dtype=float32), 'training/policy_loss': Array(-0.0059213, dtype=float32), 'training/total_loss': Array(1333.6835, dtype=float32), 'training/v_loss': Array(1333.686, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.0846381 , 0.09611303], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([5.846558, 8.429159], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-8589.284,  8962.171], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.06768365, 0.10118102], dtype=float32), 'eval/avg_episode_length': Array([999.59375 ,   4.578205], dtype=float32), 'eval/epoch_eval_time': 3.6898958683013916, 'eval/sps': 34689.32581529017}
I0728 20:47:11.076627 140583248410432 train.py:379] starting iteration 44, 57671680 steps, 379.964777469635
I0728 20:47:19.056360 140583248410432 train.py:394] {'eval/walltime': 179.62895369529724, 'training/sps': 305891.666524132, 'training/walltime': 201.67414021492004, 'training/entropy_loss': Array(-0.00049265, dtype=float32), 'training/policy_loss': Array(-0.00577166, dtype=float32), 'training/total_loss': Array(276.64154, dtype=float32), 'training/v_loss': Array(276.6478, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.07151203, 0.0785479 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([4.6523046, 6.8975873], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-7628.9043,  7631.178 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.05355065, 0.08304984], dtype=float32), 'eval/avg_episode_length': Array([993.625 ,  71.8426], dtype=float32), 'eval/epoch_eval_time': 3.690202474594116, 'eval/sps': 34686.443597943406}
I0728 20:47:19.057971 140583248410432 train.py:379] starting iteration 45, 58982400 steps, 387.9461214542389
I0728 20:47:27.033400 140583248410432 train.py:394] {'eval/walltime': 183.33119821548462, 'training/sps': 307041.7594091456, 'training/walltime': 205.9430058002472, 'training/entropy_loss': Array(0.00697222, dtype=float32), 'training/policy_loss': Array(-0.00391958, dtype=float32), 'training/total_loss': Array(1518.6675, dtype=float32), 'training/v_loss': Array(1518.6646, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.0763987 , 0.08780687], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([5.087601 , 7.6997623], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-8147.0977,  8691.603 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.0590288 , 0.09273998], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.702244520187378, 'eval/sps': 34573.62129974108}
I0728 20:47:27.035027 140583248410432 train.py:379] starting iteration 46, 60293120 steps, 395.92317724227905
I0728 20:47:35.030878 140583248410432 train.py:394] {'eval/walltime': 187.03798627853394, 'training/sps': 305880.5867548291, 'training/walltime': 210.22807669639587, 'training/entropy_loss': Array(0.00593574, dtype=float32), 'training/policy_loss': Array(-0.00342507, dtype=float32), 'training/total_loss': Array(1084.8699, dtype=float32), 'training/v_loss': Array(1084.8674, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.07949486, 0.0893273 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([5.390543, 7.800365], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-8882.957,  8775.396], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.06182589, 0.09420877], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.7067880630493164, 'eval/sps': 34531.24317409809}
I0728 20:47:35.032372 140583248410432 train.py:379] starting iteration 47, 61603840 steps, 403.92052268981934
I0728 20:47:43.040493 140583248410432 train.py:394] {'eval/walltime': 190.7675142288208, 'training/sps': 306655.8217848446, 'training/walltime': 214.5023148059845, 'training/entropy_loss': Array(0.00474088, dtype=float32), 'training/policy_loss': Array(-0.00523926, dtype=float32), 'training/total_loss': Array(543.93445, dtype=float32), 'training/v_loss': Array(543.93494, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.0698429 , 0.07464886], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([4.5123563, 6.5719085], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-7629.911 ,  7870.3223], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.05142672, 0.07916959], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.7295279502868652, 'eval/sps': 34320.69733923152}
I0728 20:47:43.042173 140583248410432 train.py:379] starting iteration 48, 62914560 steps, 411.93032336235046
I0728 20:47:51.052126 140583248410432 train.py:394] {'eval/walltime': 194.49002766609192, 'training/sps': 306045.3342500215, 'training/walltime': 218.78507900238037, 'training/entropy_loss': Array(0.00922219, dtype=float32), 'training/policy_loss': Array(0.00058428, dtype=float32), 'training/total_loss': Array(1617.1058, dtype=float32), 'training/v_loss': Array(1617.0961, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.06909726, 0.05815139], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([4.488831 , 5.1624703], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-8124.95  ,  7288.5054], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.04989293, 0.06273889], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.722513437271118, 'eval/sps': 34385.3694975064}
I0728 20:47:51.053796 140583248410432 train.py:379] starting iteration 49, 64225280 steps, 419.9419457912445
I0728 20:47:59.059468 140583248410432 train.py:394] {'eval/walltime': 198.22599530220032, 'training/sps': 307257.99930573563, 'training/walltime': 223.05094027519226, 'training/entropy_loss': Array(0.00226007, dtype=float32), 'training/policy_loss': Array(-0.00607761, dtype=float32), 'training/total_loss': Array(131.00677, dtype=float32), 'training/v_loss': Array(131.01059, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.06280416, 0.06480812], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([3.8870075, 5.7611732], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-7546.4023,  7399.8057], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.04300355, 0.06909592], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.7359676361083984, 'eval/sps': 34261.53876786049}
I0728 20:47:59.061074 140583248410432 train.py:379] starting iteration 50, 65536000 steps, 427.94922399520874
I0728 20:48:07.091796 140583248410432 train.py:394] {'eval/walltime': 201.96338653564453, 'training/sps': 305565.36269293976, 'training/walltime': 227.34043169021606, 'training/entropy_loss': Array(0.01145691, dtype=float32), 'training/policy_loss': Array(-0.00269658, dtype=float32), 'training/total_loss': Array(1607.2871, dtype=float32), 'training/v_loss': Array(1607.2783, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.06793553, 0.07404505], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([4.2698655, 6.5879965], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-8272.8545,  8391.732 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.04718362, 0.07893911], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.737391233444214, 'eval/sps': 34248.488318425494}
I0728 20:48:07.093411 140583248410432 train.py:379] starting iteration 51, 66846720 steps, 435.98156094551086
I0728 20:48:15.100930 140583248410432 train.py:394] {'eval/walltime': 205.70101761817932, 'training/sps': 307261.0045436701, 'training/walltime': 231.6062512397766, 'training/entropy_loss': Array(0.0121457, dtype=float32), 'training/policy_loss': Array(-0.00223338, dtype=float32), 'training/total_loss': Array(1510.3455, dtype=float32), 'training/v_loss': Array(1510.3354, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.05524015, 0.05281366], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([3.1522145, 4.7143884], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6877.409,  6421.71 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.03344694, 0.05624568], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.73763108253479, 'eval/sps': 34246.29054432864}
I0728 20:48:15.102443 140583248410432 train.py:379] starting iteration 52, 68157440 steps, 443.9905936717987
I0728 20:48:23.144457 140583248410432 train.py:394] {'eval/walltime': 209.44861459732056, 'training/sps': 305477.02080733544, 'training/walltime': 235.89698314666748, 'training/entropy_loss': Array(0.00535917, dtype=float32), 'training/policy_loss': Array(-0.00501853, dtype=float32), 'training/total_loss': Array(152.49608, dtype=float32), 'training/v_loss': Array(152.49573, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.06466661, 0.07185563], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([4.0037556, 6.3810754], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-7631.682 ,  7900.3047], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.0436841 , 0.07651843], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.7475969791412354, 'eval/sps': 34155.220188412924}
I0728 20:48:23.145954 140583248410432 train.py:379] starting iteration 53, 69468160 steps, 452.03410506248474
I0728 20:48:31.164830 140583248410432 train.py:394] {'eval/walltime': 213.18937230110168, 'training/sps': 306653.5467868598, 'training/walltime': 240.17125296592712, 'training/entropy_loss': Array(0.01359394, dtype=float32), 'training/policy_loss': Array(-0.00101879, dtype=float32), 'training/total_loss': Array(1545.8313, dtype=float32), 'training/v_loss': Array(1545.8186, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.05455548, 0.05529673], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([3.104865, 4.916726], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6739.588,  6959.347], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.03233635, 0.05877156], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.740757703781128, 'eval/sps': 34217.6666162095}
I0728 20:48:31.166338 140583248410432 train.py:379] starting iteration 54, 70778880 steps, 460.05448865890503
I0728 20:48:39.203387 140583248410432 train.py:394] {'eval/walltime': 216.93297052383423, 'training/sps': 305579.2052301044, 'training/walltime': 244.46055006980896, 'training/entropy_loss': Array(0.01306495, dtype=float32), 'training/policy_loss': Array(0.00442816, dtype=float32), 'training/total_loss': Array(1515.2081, dtype=float32), 'training/v_loss': Array(1515.1907, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.05754802, 0.04910177], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([3.4270942, 4.3541775], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-7321.2256,  7275.085 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.03598301, 0.05282129], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.743598222732544, 'eval/sps': 34191.70337851311}
I0728 20:48:39.204997 140583248410432 train.py:379] starting iteration 55, 72089600 steps, 468.0931475162506
I0728 20:48:47.218890 140583248410432 train.py:394] {'eval/walltime': 220.67587757110596, 'training/sps': 307191.1608378354, 'training/walltime': 248.7273395061493, 'training/entropy_loss': Array(0.00598124, dtype=float32), 'training/policy_loss': Array(-0.00508101, dtype=float32), 'training/total_loss': Array(206.72504, dtype=float32), 'training/v_loss': Array(206.72412, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04754777, 0.03824816], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.5065792, 3.3766377], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6051.584,  5577.213], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.02345739, 0.03999314], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.7429070472717285, 'eval/sps': 34198.017312052}
I0728 20:48:47.220577 140583248410432 train.py:379] starting iteration 56, 73400320 steps, 476.10872745513916
I0728 20:48:55.252187 140583248410432 train.py:394] {'eval/walltime': 224.4179904460907, 'training/sps': 305846.5014996195, 'training/walltime': 253.0128879547119, 'training/entropy_loss': Array(0.01549374, dtype=float32), 'training/policy_loss': Array(-0.00192817, dtype=float32), 'training/total_loss': Array(1593.1019, dtype=float32), 'training/v_loss': Array(1593.0884, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.05232404, 0.04275343], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.8417554, 3.8186033], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6882.094,  6523.915], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.02708359, 0.0456345 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.742112874984741, 'eval/sps': 34205.27500804527}
I0728 20:48:55.253881 140583248410432 train.py:379] starting iteration 57, 74711040 steps, 484.14203214645386
I0728 20:49:03.287564 140583248410432 train.py:394] {'eval/walltime': 228.17663478851318, 'training/sps': 306897.36873695947, 'training/walltime': 257.2837619781494, 'training/entropy_loss': Array(0.01466523, dtype=float32), 'training/policy_loss': Array(-0.00090172, dtype=float32), 'training/total_loss': Array(1374.0972, dtype=float32), 'training/v_loss': Array(1374.0834, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04898416, 0.05028365], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.5913537, 4.4088554], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6260.068,  6959.002], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.0249196 , 0.05241823], dtype=float32), 'eval/avg_episode_length': Array([998.6953  ,  14.703082], dtype=float32), 'eval/epoch_eval_time': 3.7586443424224854, 'eval/sps': 34054.83156661283}
I0728 20:49:03.289221 140583248410432 train.py:379] starting iteration 58, 76021760 steps, 492.1773715019226
I0728 20:49:11.322129 140583248410432 train.py:394] {'eval/walltime': 231.9163932800293, 'training/sps': 305606.9280444076, 'training/walltime': 261.57266998291016, 'training/entropy_loss': Array(0.00958119, dtype=float32), 'training/policy_loss': Array(-0.0044096, dtype=float32), 'training/total_loss': Array(364.78705, dtype=float32), 'training/v_loss': Array(364.7819, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04719619, 0.03046473], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.4362464, 2.723719 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6672.8584,  6046.935 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.02108751, 0.03153854], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.7397584915161133, 'eval/sps': 34226.80910822888}
I0728 20:49:11.323803 140583248410432 train.py:379] starting iteration 59, 77332480 steps, 500.211953163147
I0728 20:49:19.364341 140583248410432 train.py:394] {'eval/walltime': 235.68321084976196, 'training/sps': 306999.3914177671, 'training/walltime': 265.84212470054626, 'training/entropy_loss': Array(0.0166594, dtype=float32), 'training/policy_loss': Array(0.00191635, dtype=float32), 'training/total_loss': Array(1647.8782, dtype=float32), 'training/v_loss': Array(1647.8599, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.0447551 , 0.03425153], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.190175 , 3.0418098], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5953.7705,  5692.236 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01991581, 0.03535325], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.766817569732666, 'eval/sps': 33980.939514701335}
I0728 20:49:19.366024 140583248410432 train.py:379] starting iteration 60, 78643200 steps, 508.25417470932007
I0728 20:49:27.420783 140583248410432 train.py:394] {'eval/walltime': 239.4440975189209, 'training/sps': 305555.3764633085, 'training/walltime': 270.1317563056946, 'training/entropy_loss': Array(0.01041852, dtype=float32), 'training/policy_loss': Array(0.01318618, dtype=float32), 'training/total_loss': Array(759.19006, dtype=float32), 'training/v_loss': Array(759.1665, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04519012, 0.02560523], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.230071 , 2.2802916], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6181.6084,  5231.6934], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01830605, 0.02553295], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.7608866691589355, 'eval/sps': 34034.52729635834}
I0728 20:49:27.422450 140583248410432 train.py:379] starting iteration 61, 79953920 steps, 516.3106002807617
I0728 20:49:35.480187 140583248410432 train.py:394] {'eval/walltime': 243.2214334011078, 'training/sps': 306518.33913031116, 'training/walltime': 274.40791153907776, 'training/entropy_loss': Array(0.01227067, dtype=float32), 'training/policy_loss': Array(-0.00333956, dtype=float32), 'training/total_loss': Array(917.7721, dtype=float32), 'training/v_loss': Array(917.7632, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04115117, 0.01634573], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.9083797, 1.3830377], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6121.3164,  4727.6255], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01480491, 0.01149458], dtype=float32), 'eval/avg_episode_length': Array([999.875    ,   1.4086784], dtype=float32), 'eval/epoch_eval_time': 3.7773358821868896, 'eval/sps': 33886.31670369074}
I0728 20:49:35.481846 140583248410432 train.py:379] starting iteration 62, 81264640 steps, 524.3699963092804
I0728 20:49:43.552745 140583248410432 train.py:394] {'eval/walltime': 246.99995684623718, 'training/sps': 305647.0432987893, 'training/walltime': 278.69625663757324, 'training/entropy_loss': Array(0.01602017, dtype=float32), 'training/policy_loss': Array(0.0005185, dtype=float32), 'training/total_loss': Array(1612.0012, dtype=float32), 'training/v_loss': Array(1611.9846, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04479736, 0.02232449], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.2421057, 2.0446587], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6630.663,  5953.02 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01746825, 0.02162544], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.7785234451293945, 'eval/sps': 33875.666476277926}
I0728 20:49:43.554430 140583248410432 train.py:379] starting iteration 63, 82575360 steps, 532.4425804615021
I0728 20:49:51.622162 140583248410432 train.py:394] {'eval/walltime': 250.7930862903595, 'training/sps': 306910.7154045219, 'training/walltime': 282.9669449329376, 'training/entropy_loss': Array(0.00734424, dtype=float32), 'training/policy_loss': Array(-0.00538296, dtype=float32), 'training/total_loss': Array(131.79814, dtype=float32), 'training/v_loss': Array(131.79617, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04405664, 0.02708561], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.045596 , 2.4255826], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6075.2803,  5640.9224], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01584343, 0.02706525], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.7931294441223145, 'eval/sps': 33745.223274239645}
I0728 20:49:51.623842 140583248410432 train.py:379] starting iteration 64, 83886080 steps, 540.5119917392731
I0728 20:49:59.701500 140583248410432 train.py:394] {'eval/walltime': 254.5759425163269, 'training/sps': 305494.6749120924, 'training/walltime': 287.2574288845062, 'training/entropy_loss': Array(0.01840665, dtype=float32), 'training/policy_loss': Array(0.00117597, dtype=float32), 'training/total_loss': Array(1702.4133, dtype=float32), 'training/v_loss': Array(1702.3937, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04638909, 0.02839228], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.4248927, 2.4990585], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6880.5664,  5695.7666], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.0190367 , 0.02877783], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.7828562259674072, 'eval/sps': 33836.86620742927}
I0728 20:49:59.703180 140583248410432 train.py:379] starting iteration 65, 85196800 steps, 548.5913307666779
I0728 20:50:07.777981 140583248410432 train.py:394] {'eval/walltime': 258.38085770606995, 'training/sps': 307275.80834509234, 'training/walltime': 291.5230429172516, 'training/entropy_loss': Array(0.01674805, dtype=float32), 'training/policy_loss': Array(0.0009443, dtype=float32), 'training/total_loss': Array(1548.3656, dtype=float32), 'training/v_loss': Array(1548.3479, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04344452, 0.01981354], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.1089349, 1.7737348], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5882.7773,  5083.1104], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.0162713 , 0.01821158], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.804915189743042, 'eval/sps': 33640.6972604938}
I0728 20:50:07.779661 140583248410432 train.py:379] starting iteration 66, 86507520 steps, 556.667810678482
I0728 20:50:15.869299 140583248410432 train.py:394] {'eval/walltime': 262.17096996307373, 'training/sps': 305135.1377131497, 'training/walltime': 295.81858229637146, 'training/entropy_loss': Array(0.00772874, dtype=float32), 'training/policy_loss': Array(-0.00140226, dtype=float32), 'training/total_loss': Array(166.47845, dtype=float32), 'training/v_loss': Array(166.47214, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04516963, 0.02004967], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.2539432, 1.8406018], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6761.827 ,  5642.2417], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01759243, 0.01979494], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.790112257003784, 'eval/sps': 33772.086766946704}
I0728 20:50:15.870972 140583248410432 train.py:379] starting iteration 67, 87818240 steps, 564.7591226100922
I0728 20:50:23.955775 140583248410432 train.py:394] {'eval/walltime': 265.9795255661011, 'training/sps': 306816.40556508565, 'training/walltime': 300.0905833244324, 'training/entropy_loss': Array(0.01733579, dtype=float32), 'training/policy_loss': Array(0.00575834, dtype=float32), 'training/total_loss': Array(1755.7676, dtype=float32), 'training/v_loss': Array(1755.7445, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04428202, 0.02007125], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.21542, 1.81646], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6668.4966,  5918.3037], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01707581, 0.01859267], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.8085556030273438, 'eval/sps': 33608.54175222108}
I0728 20:50:23.957426 140583248410432 train.py:379] starting iteration 68, 89128960 steps, 572.8455765247345
I0728 20:50:32.047065 140583248410432 train.py:394] {'eval/walltime': 269.770290851593, 'training/sps': 305202.4737532812, 'training/walltime': 304.3851749897003, 'training/entropy_loss': Array(0.01486106, dtype=float32), 'training/policy_loss': Array(-0.00314764, dtype=float32), 'training/total_loss': Array(1395.8171, dtype=float32), 'training/v_loss': Array(1395.8054, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04291881, 0.01587508], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.0216784, 1.4669122], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6634.876,  5656.976], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01494726, 0.01318206], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.7907652854919434, 'eval/sps': 33766.26890878286}
I0728 20:50:32.048758 140583248410432 train.py:379] starting iteration 69, 90439680 steps, 580.9369080066681
I0728 20:50:40.142590 140583248410432 train.py:394] {'eval/walltime': 273.58874225616455, 'training/sps': 306882.89259871043, 'training/walltime': 308.65625047683716, 'training/entropy_loss': Array(0.00977987, dtype=float32), 'training/policy_loss': Array(-0.00426577, dtype=float32), 'training/total_loss': Array(259.81885, dtype=float32), 'training/v_loss': Array(259.8133, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04177494, 0.0159438 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.9612283, 1.4350549], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5767.12  ,  4394.0845], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01459475, 0.01318882], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.818451404571533, 'eval/sps': 33521.44271019283}
I0728 20:50:40.144258 140583248410432 train.py:379] starting iteration 70, 91750400 steps, 589.0324077606201
I0728 20:50:48.238200 140583248410432 train.py:394] {'eval/walltime': 277.38216066360474, 'training/sps': 305082.6108291389, 'training/walltime': 312.9525294303894, 'training/entropy_loss': Array(0.01934903, dtype=float32), 'training/policy_loss': Array(0.00020234, dtype=float32), 'training/total_loss': Array(1557.9636, dtype=float32), 'training/v_loss': Array(1557.944, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04303686, 0.02062383], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.0826986, 1.8607701], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6201.946,  5555.06 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01527637, 0.01914294], dtype=float32), 'eval/avg_episode_length': Array([994.27344,  64.53508], dtype=float32), 'eval/epoch_eval_time': 3.7934184074401855, 'eval/sps': 33742.65273478623}
I0728 20:50:48.239886 140583248410432 train.py:379] starting iteration 71, 93061120 steps, 597.1280357837677
I0728 20:50:56.332367 140583248410432 train.py:394] {'eval/walltime': 281.20213627815247, 'training/sps': 307086.6949431079, 'training/walltime': 317.2207703590393, 'training/entropy_loss': Array(0.01341599, dtype=float32), 'training/policy_loss': Array(0.00069483, dtype=float32), 'training/total_loss': Array(1122.2465, dtype=float32), 'training/v_loss': Array(1122.2324, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04270862, 0.02064867], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.0655394, 1.8632737], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6021.8936,  4908.158 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01490542, 0.01905785], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.8199756145477295, 'eval/sps': 33508.06730611937}
I0728 20:50:56.334042 140583248410432 train.py:379] starting iteration 72, 94371840 steps, 605.2221918106079
I0728 20:51:04.432500 140583248410432 train.py:394] {'eval/walltime': 285.00575947761536, 'training/sps': 305493.6563505833, 'training/walltime': 321.51126861572266, 'training/entropy_loss': Array(0.01286547, dtype=float32), 'training/policy_loss': Array(-0.00348788, dtype=float32), 'training/total_loss': Array(595.367, dtype=float32), 'training/v_loss': Array(595.3576, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04136159, 0.02356832], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.8359851, 2.1744242], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5184.871 ,  4828.9824], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01363313, 0.02337961], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.8036231994628906, 'eval/sps': 33652.12411630965}
I0728 20:51:04.434089 140583248410432 train.py:379] starting iteration 73, 95682560 steps, 613.3222393989563
I0728 20:51:12.522089 140583248410432 train.py:394] {'eval/walltime': 288.82022738456726, 'training/sps': 307011.11817376583, 'training/walltime': 325.78056025505066, 'training/entropy_loss': Array(0.01368307, dtype=float32), 'training/policy_loss': Array(0.02862195, dtype=float32), 'training/total_loss': Array(1691.5361, dtype=float32), 'training/v_loss': Array(1691.4937, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04557081, 0.0134856 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.6658082, 1.0244918], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6833.0225,  4442.005 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01902705, 0.00930989], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.8144679069519043, 'eval/sps': 33556.44958153109}
I0728 20:51:12.523743 140583248410432 train.py:379] starting iteration 74, 96993280 steps, 621.4118940830231
I0728 20:51:20.628473 140583248410432 train.py:394] {'eval/walltime': 292.6293659210205, 'training/sps': 305439.6483683723, 'training/walltime': 330.0718171596527, 'training/entropy_loss': Array(0.00035195, dtype=float32), 'training/policy_loss': Array(-0.00998083, dtype=float32), 'training/total_loss': Array(250.03055, dtype=float32), 'training/v_loss': Array(250.04018, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04392944, 0.01761427], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.4660149, 1.3137182], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6525.091 ,  5176.5986], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01921341, 0.01433191], dtype=float32), 'eval/avg_episode_length': Array([996.91406,  34.77675], dtype=float32), 'eval/epoch_eval_time': 3.809138536453247, 'eval/sps': 33603.39845218204}
I0728 20:51:20.630063 140583248410432 train.py:379] starting iteration 75, 98304000 steps, 629.5182127952576
I0728 20:51:28.721044 140583248410432 train.py:394] {'eval/walltime': 296.4449644088745, 'training/sps': 306860.5386421385, 'training/walltime': 334.3432037830353, 'training/entropy_loss': Array(0.01175895, dtype=float32), 'training/policy_loss': Array(-0.00576713, dtype=float32), 'training/total_loss': Array(1610.8391, dtype=float32), 'training/v_loss': Array(1610.8331, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03692862, 0.01546709], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.9163457, 1.208751 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5186.4463,  3701.5464], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01514103, 0.01113694], dtype=float32), 'eval/avg_episode_length': Array([991.7344,  70.4332], dtype=float32), 'eval/epoch_eval_time': 3.815598487854004, 'eval/sps': 33546.506637806815}
I0728 20:51:28.722646 140583248410432 train.py:379] starting iteration 76, 99614720 steps, 637.6107964515686
I0728 20:51:36.837562 140583248410432 train.py:394] {'eval/walltime': 300.26256704330444, 'training/sps': 305302.7282018656, 'training/walltime': 338.63638520240784, 'training/entropy_loss': Array(0.00967134, dtype=float32), 'training/policy_loss': Array(-0.00543376, dtype=float32), 'training/total_loss': Array(1496.6118, dtype=float32), 'training/v_loss': Array(1496.6074, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03679132, 0.0153032 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.8011678, 1.176416 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5360.5425,  4365.1045], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01293262, 0.0096241 ], dtype=float32), 'eval/avg_episode_length': Array([994.71875 ,  59.516666], dtype=float32), 'eval/epoch_eval_time': 3.8176026344299316, 'eval/sps': 33528.89555492298}
I0728 20:51:36.839169 140583248410432 train.py:379] starting iteration 77, 100925440 steps, 645.7273192405701
I0728 20:51:44.936816 140583248410432 train.py:394] {'eval/walltime': 304.0809245109558, 'training/sps': 306583.7053825045, 'training/walltime': 342.91162872314453, 'training/entropy_loss': Array(0.00205817, dtype=float32), 'training/policy_loss': Array(-0.00717544, dtype=float32), 'training/total_loss': Array(171.23401, dtype=float32), 'training/v_loss': Array(171.23914, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03935569, 0.01970246], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.9645784, 1.682485 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5580.26  ,  5019.3823], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01571831, 0.01722161], dtype=float32), 'eval/avg_episode_length': Array([993.72656,  63.36473], dtype=float32), 'eval/epoch_eval_time': 3.818357467651367, 'eval/sps': 33522.267384444625}
I0728 20:51:44.938414 140583248410432 train.py:379] starting iteration 78, 102236160 steps, 653.8265643119812
I0728 20:51:53.051192 140583248410432 train.py:394] {'eval/walltime': 307.8996846675873, 'training/sps': 305595.98780363885, 'training/walltime': 347.2006902694702, 'training/entropy_loss': Array(0.01340931, dtype=float32), 'training/policy_loss': Array(-0.00295086, dtype=float32), 'training/total_loss': Array(1535.739, dtype=float32), 'training/v_loss': Array(1535.7285, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04042892, 0.01551109], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.0455015, 1.2839679], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6009.0938,  4898.918 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01456412, 0.01027008], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.8187601566314697, 'eval/sps': 33518.73245501463}
I0728 20:51:53.052783 140583248410432 train.py:379] starting iteration 79, 103546880 steps, 661.9409325122833
I0728 20:52:01.147156 140583248410432 train.py:394] {'eval/walltime': 311.71373867988586, 'training/sps': 306533.960223548, 'training/walltime': 351.4766275882721, 'training/entropy_loss': Array(0.0110107, dtype=float32), 'training/policy_loss': Array(-0.00075905, dtype=float32), 'training/total_loss': Array(1586.5219, dtype=float32), 'training/v_loss': Array(1586.5115, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03809223, 0.01563073], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.8446901, 1.2854156], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5578.707,  4835.12 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01343766, 0.01032342], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.814054012298584, 'eval/sps': 33560.09107035674}
I0728 20:52:01.148746 140583248410432 train.py:379] starting iteration 80, 104857600 steps, 670.0368962287903
I0728 20:52:09.262381 140583248410432 train.py:394] {'eval/walltime': 315.532906293869, 'training/sps': 305481.3662438075, 'training/walltime': 355.7672984600067, 'training/entropy_loss': Array(0.00522623, dtype=float32), 'training/policy_loss': Array(-0.00528838, dtype=float32), 'training/total_loss': Array(216.10965, dtype=float32), 'training/v_loss': Array(216.10968, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.0363384 , 0.01557028], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.690594 , 1.2114853], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5121.5127,  4310.428 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01190481, 0.00803519], dtype=float32), 'eval/avg_episode_length': Array([994.02344,  67.35244], dtype=float32), 'eval/epoch_eval_time': 3.8191676139831543, 'eval/sps': 33515.15642606321}
I0728 20:52:09.263976 140583248410432 train.py:379] starting iteration 81, 106168320 steps, 678.1521260738373
I0728 20:52:17.359486 140583248410432 train.py:394] {'eval/walltime': 319.35026383399963, 'training/sps': 306700.0968866378, 'training/walltime': 360.0409195423126, 'training/entropy_loss': Array(0.01564109, dtype=float32), 'training/policy_loss': Array(0.00082347, dtype=float32), 'training/total_loss': Array(1455.0382, dtype=float32), 'training/v_loss': Array(1455.0217, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.0404432 , 0.01603086], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.9714947, 1.3344868], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6023.6084,  4510.222 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01279854, 0.01000076], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.8173575401306152, 'eval/sps': 33531.04828520211}
I0728 20:52:17.360990 140583248410432 train.py:379] starting iteration 82, 107479040 steps, 686.2491400241852
I0728 20:52:25.477810 140583248410432 train.py:394] {'eval/walltime': 323.17234778404236, 'training/sps': 305463.0008278946, 'training/walltime': 364.33184838294983, 'training/entropy_loss': Array(0.01210156, dtype=float32), 'training/policy_loss': Array(-0.00278733, dtype=float32), 'training/total_loss': Array(1216.1985, dtype=float32), 'training/v_loss': Array(1216.1892, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.038969, 0.018922], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.8726015, 1.6132474], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5761.927,  5444.706], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01267516, 0.01489763], dtype=float32), 'eval/avg_episode_length': Array([993.3125  ,  75.364296], dtype=float32), 'eval/epoch_eval_time': 3.8220839500427246, 'eval/sps': 33489.58360754194}
I0728 20:52:25.479326 140583248410432 train.py:379] starting iteration 83, 108789760 steps, 694.367476940155
I0728 20:52:33.577336 140583248410432 train.py:394] {'eval/walltime': 326.9939069747925, 'training/sps': 306758.3514603887, 'training/walltime': 368.6046578884125, 'training/entropy_loss': Array(0.00995526, dtype=float32), 'training/policy_loss': Array(-0.00418095, dtype=float32), 'training/total_loss': Array(359.13754, dtype=float32), 'training/v_loss': Array(359.13177, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04038897, 0.01757591], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.9329026, 1.4654771], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6035.76 ,  5541.612], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01409109, 0.01391195], dtype=float32), 'eval/avg_episode_length': Array([990.1797 ,  78.59314], dtype=float32), 'eval/epoch_eval_time': 3.821559190750122, 'eval/sps': 33494.18224629808}
I0728 20:52:33.578841 140583248410432 train.py:379] starting iteration 84, 110100480 steps, 702.4669919013977
I0728 20:52:41.689449 140583248410432 train.py:394] {'eval/walltime': 330.8062708377838, 'training/sps': 305223.3497240749, 'training/walltime': 372.89895582199097, 'training/entropy_loss': Array(0.01678208, dtype=float32), 'training/policy_loss': Array(0.00106972, dtype=float32), 'training/total_loss': Array(1458.9072, dtype=float32), 'training/v_loss': Array(1458.8894, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03778021, 0.01597025], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.7382381, 1.3108957], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5452.252 ,  4480.8325], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01200883, 0.00946504], dtype=float32), 'eval/avg_episode_length': Array([980.2422  , 116.697845], dtype=float32), 'eval/epoch_eval_time': 3.812363862991333, 'eval/sps': 33574.96938908819}
I0728 20:52:41.690968 140583248410432 train.py:379] starting iteration 85, 111411200 steps, 710.5791184902191
I0728 20:52:49.788990 140583248410432 train.py:394] {'eval/walltime': 334.6261351108551, 'training/sps': 306644.4642365696, 'training/walltime': 377.1733522415161, 'training/entropy_loss': Array(0.00749543, dtype=float32), 'training/policy_loss': Array(0.00141213, dtype=float32), 'training/total_loss': Array(731.84326, dtype=float32), 'training/v_loss': Array(731.83435, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03834958, 0.01519948], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.7234108, 1.2952554], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5093.1904,  4617.657 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01243986, 0.01004554], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.819864273071289, 'eval/sps': 33509.04399990213}
I0728 20:52:49.790491 140583248410432 train.py:379] starting iteration 86, 112721920 steps, 718.6786413192749
I0728 20:52:57.904841 140583248410432 train.py:394] {'eval/walltime': 338.4445426464081, 'training/sps': 305388.390702137, 'training/walltime': 381.46532940864563, 'training/entropy_loss': Array(0.01286316, dtype=float32), 'training/policy_loss': Array(-0.00339763, dtype=float32), 'training/total_loss': Array(917.9297, dtype=float32), 'training/v_loss': Array(917.9202, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04037986, 0.01620427], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.9439604, 1.2944145], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5363.5967,  3737.9036], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01410559, 0.01094858], dtype=float32), 'eval/avg_episode_length': Array([993.02344 ,  57.692272], dtype=float32), 'eval/epoch_eval_time': 3.8184075355529785, 'eval/sps': 33521.827832204704}
I0728 20:52:57.906535 140583248410432 train.py:379] starting iteration 87, 114032640 steps, 726.794685125351
I0728 20:53:06.004138 140583248410432 train.py:394] {'eval/walltime': 342.26525926589966, 'training/sps': 306761.3640494326, 'training/walltime': 385.73809695243835, 'training/entropy_loss': Array(0.0152576, dtype=float32), 'training/policy_loss': Array(0.02860745, dtype=float32), 'training/total_loss': Array(1414.543, dtype=float32), 'training/v_loss': Array(1414.4993, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03873304, 0.01698355], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.8875353, 1.4302166], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5262.663 ,  4544.1753], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01463616, 0.01171612], dtype=float32), 'eval/avg_episode_length': Array([997.35156 ,  23.359818], dtype=float32), 'eval/epoch_eval_time': 3.820716619491577, 'eval/sps': 33501.56861856794}
I0728 20:53:06.005811 140583248410432 train.py:379] starting iteration 88, 115343360 steps, 734.8939611911774
I0728 20:53:14.116912 140583248410432 train.py:394] {'eval/walltime': 346.08025908470154, 'training/sps': 305404.3549161609, 'training/walltime': 390.02984976768494, 'training/entropy_loss': Array(0.00525912, dtype=float32), 'training/policy_loss': Array(-0.00661776, dtype=float32), 'training/total_loss': Array(140.74477, dtype=float32), 'training/v_loss': Array(140.74612, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04000857, 0.01692482], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.891949 , 1.3260901], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5378.284,  4115.851], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01320224, 0.00999319], dtype=float32), 'eval/avg_episode_length': Array([983.96875,  96.99718], dtype=float32), 'eval/epoch_eval_time': 3.81499981880188, 'eval/sps': 33551.7709251685}
I0728 20:53:14.118588 140583248410432 train.py:379] starting iteration 89, 116654080 steps, 743.0067384243011
I0728 20:53:22.219897 140583248410432 train.py:394] {'eval/walltime': 349.90282940864563, 'training/sps': 306629.55017990415, 'training/walltime': 394.30445408821106, 'training/entropy_loss': Array(0.0182935, dtype=float32), 'training/policy_loss': Array(-0.00224072, dtype=float32), 'training/total_loss': Array(1484.425, dtype=float32), 'training/v_loss': Array(1484.4089, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03709882, 0.01536055], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.6815233, 1.2482966], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-4906.264 ,  3992.7244], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01177862, 0.00858894], dtype=float32), 'eval/avg_episode_length': Array([985.75   ,  94.14077], dtype=float32), 'eval/epoch_eval_time': 3.822570323944092, 'eval/sps': 33485.322480066454}
I0728 20:53:22.221570 140583248410432 train.py:379] starting iteration 90, 117964800 steps, 751.1097204685211
I0728 20:53:30.333497 140583248410432 train.py:394] {'eval/walltime': 353.7195682525635, 'training/sps': 305489.106858765, 'training/walltime': 398.5950162410736, 'training/entropy_loss': Array(0.0158125, dtype=float32), 'training/policy_loss': Array(-0.00048848, dtype=float32), 'training/total_loss': Array(1386.093, dtype=float32), 'training/v_loss': Array(1386.0778, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03579161, 0.01683585], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.7103264, 1.2644227], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5113.2515,  4725.0454], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01142402, 0.00801581], dtype=float32), 'eval/avg_episode_length': Array([955.75   , 162.57176], dtype=float32), 'eval/epoch_eval_time': 3.8167388439178467, 'eval/sps': 33536.48369313348}
I0728 20:53:30.335177 140583248410432 train.py:379] starting iteration 91, 119275520 steps, 759.2233271598816
I0728 20:53:38.432545 140583248410432 train.py:394] {'eval/walltime': 357.5334463119507, 'training/sps': 306304.2650853603, 'training/walltime': 402.8741600513458, 'training/entropy_loss': Array(0.00647375, dtype=float32), 'training/policy_loss': Array(-0.00422926, dtype=float32), 'training/total_loss': Array(206.1445, dtype=float32), 'training/v_loss': Array(206.14226, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03902023, 0.01743908], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.9308631, 1.3506509], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5291.4824,  4423.9746], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01317289, 0.01057401], dtype=float32), 'eval/avg_episode_length': Array([975.03906, 126.0441 ], dtype=float32), 'eval/epoch_eval_time': 3.813878059387207, 'eval/sps': 33561.639362053}
I0728 20:53:38.434219 140583248410432 train.py:379] starting iteration 92, 120586240 steps, 767.3223698139191
I0728 20:53:46.545656 140583248410432 train.py:394] {'eval/walltime': 361.35115694999695, 'training/sps': 305582.7382502943, 'training/walltime': 407.1634075641632, 'training/entropy_loss': Array(0.01803148, dtype=float32), 'training/policy_loss': Array(0.00495463, dtype=float32), 'training/total_loss': Array(1624.437, dtype=float32), 'training/v_loss': Array(1624.4141, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03869756, 0.01646177], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.8837719, 1.2982124], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5239.1514,  3843.6667], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01301687, 0.0098201 ], dtype=float32), 'eval/avg_episode_length': Array([991.8203  ,  71.546585], dtype=float32), 'eval/epoch_eval_time': 3.8177106380462646, 'eval/sps': 33527.94701735298}
I0728 20:53:46.547311 140583248410432 train.py:379] starting iteration 93, 121896960 steps, 775.435460805893
I0728 20:53:54.642426 140583248410432 train.py:394] {'eval/walltime': 365.17105197906494, 'training/sps': 306892.0406739998, 'training/walltime': 411.4343557357788, 'training/entropy_loss': Array(0.01279914, dtype=float32), 'training/policy_loss': Array(-4.8666043e-05, dtype=float32), 'training/total_loss': Array(1363.4993, dtype=float32), 'training/v_loss': Array(1363.4865, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03870375, 0.01854751], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.9245002, 1.3218433], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5829.329,  4790.538], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01334274, 0.00988377], dtype=float32), 'eval/avg_episode_length': Array([950.4219 , 156.70543], dtype=float32), 'eval/epoch_eval_time': 3.819895029067993, 'eval/sps': 33508.77420085295}
I0728 20:53:54.644121 140583248410432 train.py:379] starting iteration 94, 123207680 steps, 783.5322711467743
I0728 20:54:02.760264 140583248410432 train.py:394] {'eval/walltime': 368.9904909133911, 'training/sps': 305385.69340678287, 'training/walltime': 415.7263708114624, 'training/entropy_loss': Array(0.00800505, dtype=float32), 'training/policy_loss': Array(-0.00332328, dtype=float32), 'training/total_loss': Array(411.66245, dtype=float32), 'training/v_loss': Array(411.65778, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03371667, 0.01736666], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.5800725, 1.2221289], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-4384.5635,  3332.368 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01110594, 0.00844521], dtype=float32), 'eval/avg_episode_length': Array([923.64844, 220.92947], dtype=float32), 'eval/epoch_eval_time': 3.819438934326172, 'eval/sps': 33512.77561990446}
I0728 20:54:02.761922 140583248410432 train.py:379] starting iteration 95, 124518400 steps, 791.6500728130341
I0728 20:54:10.851572 140583248410432 train.py:394] {'eval/walltime': 372.8092269897461, 'training/sps': 307190.59438893467, 'training/walltime': 419.99316811561584, 'training/entropy_loss': Array(0.01719923, dtype=float32), 'training/policy_loss': Array(0.00076159, dtype=float32), 'training/total_loss': Array(1575.627, dtype=float32), 'training/v_loss': Array(1575.609, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03635729, 0.01941701], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.8113452, 1.299469 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5233.076,  4368.056], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01244336, 0.00928851], dtype=float32), 'eval/avg_episode_length': Array([914.1328 , 228.74089], dtype=float32), 'eval/epoch_eval_time': 3.8187360763549805, 'eval/sps': 33518.943818232445}
I0728 20:54:10.853238 140583248410432 train.py:379] starting iteration 96, 125829120 steps, 799.7413868904114
I0728 20:54:18.975754 140583248410432 train.py:394] {'eval/walltime': 376.6265835762024, 'training/sps': 304752.94478478294, 'training/walltime': 424.2940945625305, 'training/entropy_loss': Array(0.0091288, dtype=float32), 'training/policy_loss': Array(0.01327036, dtype=float32), 'training/total_loss': Array(1264.802, dtype=float32), 'training/v_loss': Array(1264.7795, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03661862, 0.02030398], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.9035776, 1.3342265], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5091.6577,  4149.065 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01295481, 0.01005309], dtype=float32), 'eval/avg_episode_length': Array([908.0156 , 232.74406], dtype=float32), 'eval/epoch_eval_time': 3.817356586456299, 'eval/sps': 33531.056662124414}
I0728 20:54:18.977427 140583248410432 train.py:379] starting iteration 97, 127139840 steps, 807.8655769824982
I0728 20:54:27.070050 140583248410432 train.py:394] {'eval/walltime': 380.4477572441101, 'training/sps': 307171.26771747647, 'training/walltime': 428.561160326004, 'training/entropy_loss': Array(0.01006162, dtype=float32), 'training/policy_loss': Array(-0.00321339, dtype=float32), 'training/total_loss': Array(884.3583, dtype=float32), 'training/v_loss': Array(884.35144, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03465077, 0.02040718], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.8041779, 1.3333075], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-4992.834,  4390.397], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01281975, 0.01043993], dtype=float32), 'eval/avg_episode_length': Array([853.08594, 290.6464 ], dtype=float32), 'eval/epoch_eval_time': 3.821173667907715, 'eval/sps': 33497.56151493802}
I0728 20:54:27.071737 140583248410432 train.py:379] starting iteration 98, 128450560 steps, 815.9598877429962
I0728 20:54:35.184413 140583248410432 train.py:394] {'eval/walltime': 384.2610282897949, 'training/sps': 305163.22042903915, 'training/walltime': 432.85630440711975, 'training/entropy_loss': Array(0.0141132, dtype=float32), 'training/policy_loss': Array(-0.00050097, dtype=float32), 'training/total_loss': Array(1630.7732, dtype=float32), 'training/v_loss': Array(1630.7595, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03296462, 0.02023458], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.7386073, 1.2856088], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5444.86 ,  4762.809], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.0121929 , 0.00931206], dtype=float32), 'eval/avg_episode_length': Array([823.6406, 305.9525], dtype=float32), 'eval/epoch_eval_time': 3.8132710456848145, 'eval/sps': 33566.98185534117}
I0728 20:54:35.557769 140583248410432 train.py:410] total steps: 129761280
