I0728 19:15:07.607064 139931924436800 low_level_env.py:190] Initialising environment...
I0728 19:15:07.922089 139931924436800 low_level_env.py:298] Environment initialised.
I0728 19:15:07.926456 139931924436800 train.py:118] JAX is running on GPU.
I0728 19:15:07.926507 139931924436800 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 19:15:14.190808 139931924436800 train.py:367] Running initial eval
I0728 19:15:29.637975 139931924436800 train.py:373] {'eval/walltime': 15.308596849441528, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.30722523, 0.119355  ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([25.959553, 10.25181 ], dtype=float32), 'eval/episode_reward': Array([-25022.172 ,   9682.9795], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30309358, 0.12174172], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.308596849441528, 'eval/sps': 8361.314969547295}
I0728 19:15:29.639147 139931924436800 train.py:379] starting iteration 0, 0 steps, 21.712696313858032
I0728 19:16:20.484549 139931924436800 train.py:394] {'eval/walltime': 18.926694869995117, 'training/sps': 235925.07308686207, 'training/walltime': 47.22312831878662, 'training/entropy_loss': Array(-0.03412234, dtype=float32), 'training/policy_loss': Array(-0.00532302, dtype=float32), 'training/total_loss': Array(22779.604, dtype=float32), 'training/v_loss': Array(22779.643, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.12508906, 0.10933679], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([10.202175,  9.36121 ], dtype=float32), 'eval/episode_reward': Array([-13260.449,   8140.311], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10993488, 0.11559266], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.618098020553589, 'eval/sps': 35377.70377498377}
I0728 19:16:20.501040 139931924436800 train.py:379] starting iteration 1, 11141120 steps, 72.57459616661072
I0728 19:16:59.470428 139931924436800 train.py:394] {'eval/walltime': 22.56203293800354, 'training/sps': 315343.93087257707, 'training/walltime': 82.55318832397461, 'training/entropy_loss': Array(-0.0181656, dtype=float32), 'training/policy_loss': Array(-0.00276103, dtype=float32), 'training/total_loss': Array(2948.6143, dtype=float32), 'training/v_loss': Array(2948.6353, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.10987806, 0.11174134], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([8.747063, 9.671781], dtype=float32), 'eval/episode_reward': Array([-10841.943,   8769.882], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09637655, 0.11691257], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.635338068008423, 'eval/sps': 35209.93030233452}
I0728 19:16:59.472335 139931924436800 train.py:379] starting iteration 2, 22282240 steps, 111.54589128494263
I0728 19:17:38.369606 139931924436800 train.py:394] {'eval/walltime': 26.20887517929077, 'training/sps': 316091.0926387877, 'training/walltime': 117.79973673820496, 'training/entropy_loss': Array(-0.01185857, dtype=float32), 'training/policy_loss': Array(-0.00324516, dtype=float32), 'training/total_loss': Array(2993.7764, dtype=float32), 'training/v_loss': Array(2993.7915, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1174015 , 0.12478451], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 9.01366 , 10.884041], dtype=float32), 'eval/episode_reward': Array([-11184.697,  10461.785], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10454257, 0.12955055], dtype=float32), 'eval/avg_episode_length': Array([999.14844 ,   9.596622], dtype=float32), 'eval/epoch_eval_time': 3.6468422412872314, 'eval/sps': 35098.858555181054}
I0728 19:17:38.371198 139931924436800 train.py:379] starting iteration 3, 33423360 steps, 150.44475436210632
I0728 19:18:17.452751 139931924436800 train.py:394] {'eval/walltime': 29.855322122573853, 'training/sps': 314447.8886786136, 'training/walltime': 153.2304723262787, 'training/entropy_loss': Array(-0.00503196, dtype=float32), 'training/policy_loss': Array(-0.00325227, dtype=float32), 'training/total_loss': Array(2334.0425, dtype=float32), 'training/v_loss': Array(2334.0508, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09692401, 0.1205707 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 7.10353 , 10.485929], dtype=float32), 'eval/episode_reward': Array([-9148.713, 10354.965], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08344159, 0.12519674], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.646446943283081, 'eval/sps': 35102.66349433158}
I0728 19:18:17.454355 139931924436800 train.py:379] starting iteration 4, 44564480 steps, 189.52790117263794
I0728 19:18:57.135657 139931924436800 train.py:394] {'eval/walltime': 33.5377733707428, 'training/sps': 309519.3550357491, 'training/walltime': 189.22537803649902, 'training/entropy_loss': Array(0.0001413, dtype=float32), 'training/policy_loss': Array(-0.00207009, dtype=float32), 'training/total_loss': Array(2815.9683, dtype=float32), 'training/v_loss': Array(2815.9702, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.08915437, 0.10206235], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([6.59616 , 8.789319], dtype=float32), 'eval/episode_reward': Array([-9221.615,  9153.683], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.07723011, 0.10502706], dtype=float32), 'eval/avg_episode_length': Array([991.9922  ,  47.360237], dtype=float32), 'eval/epoch_eval_time': 3.6824512481689453, 'eval/sps': 34759.45542080061}
I0728 19:18:57.137247 139931924436800 train.py:379] starting iteration 5, 55705600 steps, 229.21080327033997
I0728 19:19:37.338717 139931924436800 train.py:394] {'eval/walltime': 37.3156943321228, 'training/sps': 305907.5171856294, 'training/walltime': 225.64527416229248, 'training/entropy_loss': Array(-0.00421567, dtype=float32), 'training/policy_loss': Array(-0.00121091, dtype=float32), 'training/total_loss': Array(5816.8716, dtype=float32), 'training/v_loss': Array(5816.877, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.07445331, 0.09530734], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([5.6570935, 8.0063925], dtype=float32), 'eval/episode_reward': Array([-7625.84 ,  8837.922], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.06461128, 0.09609971], dtype=float32), 'eval/avg_episode_length': Array([834.9375, 294.6805], dtype=float32), 'eval/epoch_eval_time': 3.777920961380005, 'eval/sps': 33881.0687964324}
I0728 19:19:37.340329 139931924436800 train.py:379] starting iteration 6, 66846720 steps, 269.4138855934143
I0728 19:20:17.654759 139931924436800 train.py:394] {'eval/walltime': 41.13844680786133, 'training/sps': 305336.2444517733, 'training/walltime': 262.13331055641174, 'training/entropy_loss': Array(-0.0072029, dtype=float32), 'training/policy_loss': Array(-0.00177806, dtype=float32), 'training/total_loss': Array(14351.008, dtype=float32), 'training/v_loss': Array(14351.016, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.08871405, 0.11405073], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([7.099534, 9.553328], dtype=float32), 'eval/episode_reward': Array([-8613.405, 10239.335], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08112541, 0.11390931], dtype=float32), 'eval/avg_episode_length': Array([611.4922, 386.812 ], dtype=float32), 'eval/epoch_eval_time': 3.8227524757385254, 'eval/sps': 33483.72692513172}
I0728 19:20:17.656368 139931924436800 train.py:379] starting iteration 7, 77987840 steps, 309.72992420196533
I0728 19:20:57.977662 139931924436800 train.py:394] {'eval/walltime': 44.96981930732727, 'training/sps': 305352.16430541046, 'training/walltime': 298.61944460868835, 'training/entropy_loss': Array(-0.0101547, dtype=float32), 'training/policy_loss': Array(-0.00098744, dtype=float32), 'training/total_loss': Array(12953.278, dtype=float32), 'training/v_loss': Array(12953.289, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.0797481 , 0.10623497], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([6.459831, 8.831857], dtype=float32), 'eval/episode_reward': Array([-7735.986 ,  9502.5625], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.07401734, 0.10538771], dtype=float32), 'eval/avg_episode_length': Array([528.5078 , 413.30984], dtype=float32), 'eval/epoch_eval_time': 3.8313724994659424, 'eval/sps': 33408.39347201088}
I0728 19:20:57.979291 139931924436800 train.py:379] starting iteration 8, 89128960 steps, 350.05284786224365
I0728 19:21:38.363959 139931924436800 train.py:394] {'eval/walltime': 48.845842361450195, 'training/sps': 305195.8429608298, 'training/walltime': 335.12426686286926, 'training/entropy_loss': Array(-0.0106491, dtype=float32), 'training/policy_loss': Array(-0.00047582, dtype=float32), 'training/total_loss': Array(10374.408, dtype=float32), 'training/v_loss': Array(10374.418, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.08981712, 0.11186577], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([7.276911, 9.372732], dtype=float32), 'eval/episode_reward': Array([-8542.614, 10074.716], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.082784  , 0.11154069], dtype=float32), 'eval/avg_episode_length': Array([550.08594, 425.53827], dtype=float32), 'eval/epoch_eval_time': 3.876023054122925, 'eval/sps': 33023.53938887088}
I0728 19:21:38.727436 139931924436800 train.py:410] total steps: 100270080
