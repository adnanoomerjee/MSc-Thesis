I0728 19:25:48.972149 139923278997312 low_level_env.py:190] Initialising environment...
I0728 19:25:49.276283 139923278997312 low_level_env.py:298] Environment initialised.
I0728 19:25:49.281543 139923278997312 train.py:118] JAX is running on GPU.
I0728 19:25:49.281625 139923278997312 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 19:25:55.523383 139923278997312 train.py:367] Running initial eval
I0728 19:26:11.001895 139923278997312 train.py:373] {'eval/walltime': 15.344581842422485, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31338343, 0.13913155], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.499846, 11.933443], dtype=float32), 'eval/episode_reward': Array([-25329.668,  10075.487], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30885553, 0.14221905], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.344581842422485, 'eval/sps': 8341.706624166458}
I0728 19:26:11.005501 139923278997312 train.py:379] starting iteration 0, 0 steps, 21.72397255897522
I0728 19:27:01.969134 139923278997312 train.py:394] {'eval/walltime': 18.959495067596436, 'training/sps': 235319.88060623873, 'training/walltime': 47.34457612037659, 'training/entropy_loss': Array(-0.0345707, dtype=float32), 'training/policy_loss': Array(-0.00439878, dtype=float32), 'training/total_loss': Array(20701.314, dtype=float32), 'training/v_loss': Array(20701.354, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.12429049, 0.10570297], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([10.134421,  9.101447], dtype=float32), 'eval/episode_reward': Array([-13423.242,   7723.281], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.11117684, 0.11085764], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.61491322517395, 'eval/sps': 35408.87208816489}
I0728 19:27:01.986860 139923278997312 train.py:379] starting iteration 1, 11141120 steps, 72.70534253120422
I0728 19:27:40.960790 139923278997312 train.py:394] {'eval/walltime': 22.594018936157227, 'training/sps': 315295.0147757879, 'training/walltime': 82.68011736869812, 'training/entropy_loss': Array(-0.0160584, dtype=float32), 'training/policy_loss': Array(-0.00375254, dtype=float32), 'training/total_loss': Array(1382.9995, dtype=float32), 'training/v_loss': Array(1383.0193, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.10112502, 0.10645185], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([7.986026, 9.156673], dtype=float32), 'eval/episode_reward': Array([-10145.042,   8540.965], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08654924, 0.11157649], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.634523868560791, 'eval/sps': 35217.81796708514}
I0728 19:27:40.962572 139923278997312 train.py:379] starting iteration 2, 22282240 steps, 111.68105435371399
I0728 19:28:20.038784 139923278997312 train.py:394] {'eval/walltime': 26.248520851135254, 'training/sps': 314561.71380247903, 'training/walltime': 118.09803223609924, 'training/entropy_loss': Array(-0.00717652, dtype=float32), 'training/policy_loss': Array(-0.0051375, dtype=float32), 'training/total_loss': Array(1588.8672, dtype=float32), 'training/v_loss': Array(1588.8794, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11472666, 0.12917934], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 8.739666, 11.222526], dtype=float32), 'eval/episode_reward': Array([-10565.578,  10532.299], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10271773, 0.13355988], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6545019149780273, 'eval/sps': 35025.29290664487}
I0728 19:28:20.040197 139923278997312 train.py:379] starting iteration 3, 33423360 steps, 150.75867986679077
I0728 19:28:59.700800 139923278997312 train.py:394] {'eval/walltime': 29.92780089378357, 'training/sps': 309669.1503741203, 'training/walltime': 154.0755262374878, 'training/entropy_loss': Array(-0.00131291, dtype=float32), 'training/policy_loss': Array(-0.00359056, dtype=float32), 'training/total_loss': Array(1375.4006, dtype=float32), 'training/v_loss': Array(1375.4055, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.10113773, 0.12282605], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 7.5387745, 10.670454 ], dtype=float32), 'eval/episode_reward': Array([-9202.462, 10641.613], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08847859, 0.126967  ], dtype=float32), 'eval/avg_episode_length': Array([996.0078  ,  30.865253], dtype=float32), 'eval/epoch_eval_time': 3.6792800426483154, 'eval/sps': 34789.414917127826}
I0728 19:28:59.702218 139923278997312 train.py:379] starting iteration 4, 44564480 steps, 190.42070174217224
I0728 19:29:39.927658 139923278997312 train.py:394] {'eval/walltime': 33.71813225746155, 'training/sps': 305811.0169024738, 'training/walltime': 190.50691485404968, 'training/entropy_loss': Array(-0.00231399, dtype=float32), 'training/policy_loss': Array(-0.00191699, dtype=float32), 'training/total_loss': Array(3355.0813, dtype=float32), 'training/v_loss': Array(3355.0854, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.08163868, 0.11694078], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([6.139718, 9.997855], dtype=float32), 'eval/episode_reward': Array([-8139.8022, 10094.536 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.07143553, 0.11941522], dtype=float32), 'eval/avg_episode_length': Array([915.0547 , 222.48877], dtype=float32), 'eval/epoch_eval_time': 3.7903313636779785, 'eval/sps': 33770.13451293456}
I0728 19:29:39.929068 139923278997312 train.py:379] starting iteration 5, 55705600 steps, 230.64755129814148
I0728 19:30:20.161861 139923278997312 train.py:394] {'eval/walltime': 37.53603792190552, 'training/sps': 305982.80296763725, 'training/walltime': 226.9178500175476, 'training/entropy_loss': Array(-0.00535718, dtype=float32), 'training/policy_loss': Array(-0.00222372, dtype=float32), 'training/total_loss': Array(12038.477, dtype=float32), 'training/v_loss': Array(12038.484, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.0665815 , 0.09767645], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([5.3054075, 8.110115 ], dtype=float32), 'eval/episode_reward': Array([-6505.9546,  8733.144 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.06034333, 0.09681567], dtype=float32), 'eval/avg_episode_length': Array([573.7422, 389.5452], dtype=float32), 'eval/epoch_eval_time': 3.8179056644439697, 'eval/sps': 33526.234341529125}
I0728 19:30:20.163399 139923278997312 train.py:379] starting iteration 6, 66846720 steps, 270.88188195228577
I0728 19:31:00.509684 139923278997312 train.py:394] {'eval/walltime': 41.39229106903076, 'training/sps': 305352.45362754614, 'training/walltime': 263.40394949913025, 'training/entropy_loss': Array(-0.00659096, dtype=float32), 'training/policy_loss': Array(-0.00079048, dtype=float32), 'training/total_loss': Array(13457.137, dtype=float32), 'training/v_loss': Array(13457.143, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09287792, 0.1278725 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 7.571462 , 10.6678915], dtype=float32), 'eval/episode_reward': Array([-8569.016 , 11179.1045], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08694588, 0.12769113], dtype=float32), 'eval/avg_episode_length': Array([529.4219 , 414.76312], dtype=float32), 'eval/epoch_eval_time': 3.856253147125244, 'eval/sps': 33192.841630591945}
I0728 19:31:00.511213 139923278997312 train.py:379] starting iteration 7, 77987840 steps, 311.22969603538513
I0728 19:31:40.870030 139923278997312 train.py:394] {'eval/walltime': 45.314799547195435, 'training/sps': 305800.40624850447, 'training/walltime': 299.83660221099854, 'training/entropy_loss': Array(-0.0056206, dtype=float32), 'training/policy_loss': Array(-6.4111715e-05, dtype=float32), 'training/total_loss': Array(10952.735, dtype=float32), 'training/v_loss': Array(10952.74, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.0763858 , 0.10868283], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([6.228952, 9.041958], dtype=float32), 'eval/episode_reward': Array([-7192.754 ,  9920.1045], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.07181871, 0.10773827], dtype=float32), 'eval/avg_episode_length': Array([465.125 , 419.0248], dtype=float32), 'eval/epoch_eval_time': 3.922508478164673, 'eval/sps': 32632.17930886174}
I0728 19:31:40.871496 139923278997312 train.py:379] starting iteration 8, 89128960 steps, 351.58997917175293
I0728 19:32:21.178825 139923278997312 train.py:394] {'eval/walltime': 49.204506397247314, 'training/sps': 305958.14296889876, 'training/walltime': 336.2504720687866, 'training/entropy_loss': Array(-0.00413246, dtype=float32), 'training/policy_loss': Array(0.00017677, dtype=float32), 'training/total_loss': Array(10105.985, dtype=float32), 'training/v_loss': Array(10105.989, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.07795529, 0.11396821], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([6.3614664, 9.521071 ], dtype=float32), 'eval/episode_reward': Array([-7482.57 , 10679.203], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.07252211, 0.11332831], dtype=float32), 'eval/avg_episode_length': Array([455.54688, 424.90308], dtype=float32), 'eval/epoch_eval_time': 3.88970685005188, 'eval/sps': 32907.36421391056}
I0728 19:32:21.522011 139923278997312 train.py:410] total steps: 100270080
