I0728 18:16:22.374871 140290830563136 low_level_env.py:190] Initialising environment...
I0728 18:16:22.685734 140290830563136 low_level_env.py:298] Environment initialised.
I0728 18:16:22.689942 140290830563136 train.py:118] JAX is running on GPU.
I0728 18:16:22.689997 140290830563136 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 18:16:28.989841 140290830563136 train.py:367] Running initial eval
I0728 18:16:44.467981 140290830563136 train.py:373] {'eval/walltime': 15.345545053482056, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32111508, 0.12389068], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.161306, 10.643019], dtype=float32), 'eval/episode_reward': Array([-25869.807,   9721.692], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3170408 , 0.12654008], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.345545053482056, 'eval/sps': 8341.183030898961}
I0728 18:16:44.469164 140290830563136 train.py:379] starting iteration 0, 0 steps, 21.779248237609863
I0728 18:17:35.356279 140290830563136 train.py:394] {'eval/walltime': 18.979639530181885, 'training/sps': 235795.7824140429, 'training/walltime': 47.24902153015137, 'training/entropy_loss': Array(-0.03475451, dtype=float32), 'training/policy_loss': Array(-0.00514334, dtype=float32), 'training/total_loss': Array(20517.418, dtype=float32), 'training/v_loss': Array(20517.457, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.1292556 , 0.10780127], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([10.627043,  9.160035], dtype=float32), 'eval/episode_reward': Array([-13746.311,   7917.101], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.11498234, 0.11399926], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.634094476699829, 'eval/sps': 35221.97918097015}
I0728 18:17:35.375105 140290830563136 train.py:379] starting iteration 1, 11141120 steps, 72.68518924713135
I0728 18:18:14.233518 140290830563136 train.py:394] {'eval/walltime': 22.634760856628418, 'training/sps': 316531.4733017234, 'training/walltime': 82.44653248786926, 'training/entropy_loss': Array(-0.01720568, dtype=float32), 'training/policy_loss': Array(-0.00458279, dtype=float32), 'training/total_loss': Array(1288.0663, dtype=float32), 'training/v_loss': Array(1288.0881, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09952068, 0.10285737], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([7.7529182, 8.899312 ], dtype=float32), 'eval/episode_reward': Array([-10304.125,   8240.095], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08200064, 0.10935052], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.655121326446533, 'eval/sps': 35019.357380522335}
I0728 18:18:14.236582 140290830563136 train.py:379] starting iteration 2, 22282240 steps, 111.54666709899902
I0728 18:18:53.328817 140290830563136 train.py:394] {'eval/walltime': 26.30157709121704, 'training/sps': 314533.74837795086, 'training/walltime': 117.86759638786316, 'training/entropy_loss': Array(-0.00871401, dtype=float32), 'training/policy_loss': Array(-0.00366615, dtype=float32), 'training/total_loss': Array(1425.3567, dtype=float32), 'training/v_loss': Array(1425.3691, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.12030281, 0.12323801], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 9.146965, 10.746619], dtype=float32), 'eval/episode_reward': Array([-11264.547,  10008.983], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1064117 , 0.12895957], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.666816234588623, 'eval/sps': 34907.66698166978}
I0728 18:18:53.330992 140290830563136 train.py:379] starting iteration 3, 33423360 steps, 150.6410756111145
I0728 18:19:33.183417 140290830563136 train.py:394] {'eval/walltime': 30.052797555923462, 'training/sps': 308641.620896558, 'training/walltime': 153.9648666381836, 'training/entropy_loss': Array(-0.00516063, dtype=float32), 'training/policy_loss': Array(-0.00348961, dtype=float32), 'training/total_loss': Array(1020.4608, dtype=float32), 'training/v_loss': Array(1020.4695, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.12231516, 0.1308485 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 9.048773, 11.547026], dtype=float32), 'eval/episode_reward': Array([-10852.128,  11313.74 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10635577, 0.13777465], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.751220464706421, 'eval/sps': 34122.228006670244}
I0728 18:19:33.185010 140290830563136 train.py:379] starting iteration 4, 44564480 steps, 190.49509620666504
I0728 18:20:13.283274 140290830563136 train.py:394] {'eval/walltime': 33.884299755096436, 'training/sps': 307233.11819399917, 'training/walltime': 190.22762393951416, 'training/entropy_loss': Array(-0.00052133, dtype=float32), 'training/policy_loss': Array(-0.00161616, dtype=float32), 'training/total_loss': Array(1204.2246, dtype=float32), 'training/v_loss': Array(1204.2267, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09371129, 0.10745849], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([6.6689234, 9.432743 ], dtype=float32), 'eval/episode_reward': Array([-9063.104,  9514.595], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.07813275, 0.11290887], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.8315021991729736, 'eval/sps': 33407.2625686157}
I0728 18:20:13.284826 140290830563136 train.py:379] starting iteration 5, 55705600 steps, 230.59491157531738
I0728 18:20:53.434274 140290830563136 train.py:394] {'eval/walltime': 37.74508619308472, 'training/sps': 307047.68280237867, 'training/walltime': 226.51228141784668, 'training/entropy_loss': Array(0.00254542, dtype=float32), 'training/policy_loss': Array(-0.00126478, dtype=float32), 'training/total_loss': Array(1077.3872, dtype=float32), 'training/v_loss': Array(1077.3857, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.07037087, 0.07139115], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([4.619522, 6.25557 ], dtype=float32), 'eval/episode_reward': Array([-7687.8677,  7958.7246], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.05163453, 0.07553238], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.8607864379882812, 'eval/sps': 33153.866979157814}
I0728 18:20:53.435842 140290830563136 train.py:379] starting iteration 6, 66846720 steps, 270.7459282875061
I0728 18:21:33.636562 140290830563136 train.py:394] {'eval/walltime': 41.67310094833374, 'training/sps': 307181.0074824403, 'training/walltime': 262.7811903953552, 'training/entropy_loss': Array(0.00740787, dtype=float32), 'training/policy_loss': Array(0.00148766, dtype=float32), 'training/total_loss': Array(1215.9507, dtype=float32), 'training/v_loss': Array(1215.9417, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.05716727, 0.05534426], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([3.4100814, 4.846212 ], dtype=float32), 'eval/episode_reward': Array([-8187.6846,  8013.5825], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.03331793, 0.05860117], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9280147552490234, 'eval/sps': 32586.435636208604}
I0728 18:21:33.638134 140290830563136 train.py:379] starting iteration 7, 77987840 steps, 310.9482204914093
I0728 18:22:13.733377 140290830563136 train.py:394] {'eval/walltime': 45.55939292907715, 'training/sps': 307724.09439792426, 'training/walltime': 298.98609018325806, 'training/entropy_loss': Array(0.00969779, dtype=float32), 'training/policy_loss': Array(0.00208688, dtype=float32), 'training/total_loss': Array(1106.2686, dtype=float32), 'training/v_loss': Array(1106.2567, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04268735, 0.02869594], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.1065373, 2.488168 ], dtype=float32), 'eval/episode_reward': Array([-5650.458,  5348.789], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01781052, 0.02870802], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.886291980743408, 'eval/sps': 32936.279783979306}
I0728 18:22:13.734967 140290830563136 train.py:379] starting iteration 8, 89128960 steps, 351.0450530052185
I0728 18:22:53.838290 140290830563136 train.py:394] {'eval/walltime': 49.43535304069519, 'training/sps': 307568.13741911185, 'training/walltime': 335.2093482017517, 'training/entropy_loss': Array(0.0126217, dtype=float32), 'training/policy_loss': Array(-0.00184026, dtype=float32), 'training/total_loss': Array(1021.7955, dtype=float32), 'training/v_loss': Array(1021.78467, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04325939, 0.01888222], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.2318897, 1.6599102], dtype=float32), 'eval/episode_reward': Array([-6371.1875,  5377.293 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01495054, 0.01584631], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.875960111618042, 'eval/sps': 33024.07566484621}
I0728 18:22:54.183744 140290830563136 train.py:410] total steps: 100270080
