I0728 19:41:32.403403 139709070006080 low_level_env.py:190] Initialising environment...
I0728 19:41:32.749968 139709070006080 low_level_env.py:298] Environment initialised.
I0728 19:41:32.755788 139709070006080 train.py:118] JAX is running on GPU.
I0728 19:41:32.755841 139709070006080 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 19:41:39.064252 139709070006080 train.py:367] Running initial eval
I0728 19:41:54.429781 139709070006080 train.py:373] {'eval/walltime': 15.234264373779297, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31626773, 0.1303053 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.715168, 11.212859], dtype=float32), 'eval/episode_reward': Array([-25343.86 ,   9843.345], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31192005, 0.1331396 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.234264373779297, 'eval/sps': 8402.112294986116}
I0728 19:41:54.430998 139709070006080 train.py:379] starting iteration 0, 0 steps, 21.67521905899048
I0728 19:42:45.302504 139709070006080 train.py:394] {'eval/walltime': 18.85019326210022, 'training/sps': 235784.684243225, 'training/walltime': 47.25124549865723, 'training/entropy_loss': Array(-0.0353891, dtype=float32), 'training/policy_loss': Array(-0.00535562, dtype=float32), 'training/total_loss': Array(20408.53, dtype=float32), 'training/v_loss': Array(20408.57, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.12891296, 0.10963131], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([10.556162,  9.378677], dtype=float32), 'eval/episode_reward': Array([-13249.426,   8280.763], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.11531255, 0.11522059], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.615928888320923, 'eval/sps': 35398.92623813117}
I0728 19:42:45.320482 139709070006080 train.py:379] starting iteration 1, 11141120 steps, 72.564706325531
I0728 19:43:24.281530 139709070006080 train.py:394] {'eval/walltime': 22.491121292114258, 'training/sps': 315466.72200248146, 'training/walltime': 82.56755375862122, 'training/entropy_loss': Array(-0.01891216, dtype=float32), 'training/policy_loss': Array(-0.00253795, dtype=float32), 'training/total_loss': Array(1950.3422, dtype=float32), 'training/v_loss': Array(1950.3638, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.0942871 , 0.10160705], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([7.389101, 8.709369], dtype=float32), 'eval/episode_reward': Array([-9494.486,  8170.536], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.07656066, 0.10760462], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.640928030014038, 'eval/sps': 35155.872059219604}
I0728 19:43:24.283679 139709070006080 train.py:379] starting iteration 2, 22282240 steps, 111.52790379524231
I0728 19:44:03.231606 139709070006080 train.py:394] {'eval/walltime': 26.151846885681152, 'training/sps': 315761.7096730417, 'training/walltime': 117.85086917877197, 'training/entropy_loss': Array(-0.01014517, dtype=float32), 'training/policy_loss': Array(-0.00392903, dtype=float32), 'training/total_loss': Array(1582.0105, dtype=float32), 'training/v_loss': Array(1582.0245, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09960602, 0.12086324], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 7.4618006, 10.503281 ], dtype=float32), 'eval/episode_reward': Array([-9283.365,  9879.793], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08620085, 0.12532437], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6607255935668945, 'eval/sps': 34965.745650244404}
I0728 19:44:03.233186 139709070006080 train.py:379] starting iteration 3, 33423360 steps, 150.47741103172302
I0728 19:44:42.403229 139709070006080 train.py:394] {'eval/walltime': 29.80881953239441, 'training/sps': 313754.8727244154, 'training/walltime': 153.35986351966858, 'training/entropy_loss': Array(-0.00355704, dtype=float32), 'training/policy_loss': Array(-0.0034373, dtype=float32), 'training/total_loss': Array(1179.4524, dtype=float32), 'training/v_loss': Array(1179.4595, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.10473816, 0.13498577], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 7.8159223, 11.704288 ], dtype=float32), 'eval/episode_reward': Array([-9542.99 , 11561.931], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09190518, 0.1393475 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.656972646713257, 'eval/sps': 35001.62904282081}
I0728 19:44:42.404814 139709070006080 train.py:379] starting iteration 4, 44564480 steps, 189.6490397453308
I0728 19:45:22.085641 139709070006080 train.py:394] {'eval/walltime': 33.522539138793945, 'training/sps': 309789.00892902206, 'training/walltime': 189.32343769073486, 'training/entropy_loss': Array(0.00290685, dtype=float32), 'training/policy_loss': Array(-0.00098097, dtype=float32), 'training/total_loss': Array(1246.2627, dtype=float32), 'training/v_loss': Array(1246.2607, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.08186398, 0.10428961], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([5.88968  , 9.0103655], dtype=float32), 'eval/episode_reward': Array([-8366.594,  9655.075], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.0687791 , 0.10754982], dtype=float32), 'eval/avg_episode_length': Array([994.34375 ,  63.742702], dtype=float32), 'eval/epoch_eval_time': 3.713719606399536, 'eval/sps': 34466.791671462895}
I0728 19:45:22.087183 139709070006080 train.py:379] starting iteration 5, 55705600 steps, 229.33140778541565
I0728 19:46:02.280155 139709070006080 train.py:394] {'eval/walltime': 37.340184926986694, 'training/sps': 306315.8062717383, 'training/walltime': 225.69478964805603, 'training/entropy_loss': Array(0.00692045, dtype=float32), 'training/policy_loss': Array(-0.00162489, dtype=float32), 'training/total_loss': Array(1122.2444, dtype=float32), 'training/v_loss': Array(1122.239, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.05608008, 0.06520452], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([3.6716323, 5.59278  ], dtype=float32), 'eval/episode_reward': Array([-6472.5  ,  7762.613], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.04084647, 0.06660911], dtype=float32), 'eval/avg_episode_length': Array([954.8203, 177.8882], dtype=float32), 'eval/epoch_eval_time': 3.817645788192749, 'eval/sps': 33528.51655223741}
I0728 19:46:02.281688 139709070006080 train.py:379] starting iteration 6, 66846720 steps, 269.52591252326965
I0728 19:46:42.539719 139709070006080 train.py:394] {'eval/walltime': 41.163426637649536, 'training/sps': 305818.41397649277, 'training/walltime': 262.12529706954956, 'training/entropy_loss': Array(0.00947513, dtype=float32), 'training/policy_loss': Array(0.00039094, dtype=float32), 'training/total_loss': Array(1303.3638, dtype=float32), 'training/v_loss': Array(1303.3539, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04976947, 0.0473836 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([3.090587, 4.039412], dtype=float32), 'eval/episode_reward': Array([-7252.01  ,  7675.1694], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.03087968, 0.04765126], dtype=float32), 'eval/avg_episode_length': Array([945.8906 , 182.59981], dtype=float32), 'eval/epoch_eval_time': 3.823241710662842, 'eval/sps': 33479.44223432539}
I0728 19:46:42.541261 139709070006080 train.py:379] starting iteration 7, 77987840 steps, 309.7854859828949
I0728 19:47:22.833651 139709070006080 train.py:394] {'eval/walltime': 45.029099464416504, 'training/sps': 305881.13737047545, 'training/walltime': 298.5483341217041, 'training/entropy_loss': Array(0.00798178, dtype=float32), 'training/policy_loss': Array(-0.00071633, dtype=float32), 'training/total_loss': Array(1189.9866, dtype=float32), 'training/v_loss': Array(1189.9792, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04006857, 0.02857477], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.2729454, 2.2991035], dtype=float32), 'eval/episode_reward': Array([-6037.461,  5890.153], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01832624, 0.02542855], dtype=float32), 'eval/avg_episode_length': Array([926.0078 , 209.21776], dtype=float32), 'eval/epoch_eval_time': 3.8656728267669678, 'eval/sps': 33111.9589618897}
I0728 19:47:22.835187 139709070006080 train.py:379] starting iteration 8, 89128960 steps, 350.0794117450714
I0728 19:48:03.167658 139709070006080 train.py:394] {'eval/walltime': 48.93777775764465, 'training/sps': 305904.32309161796, 'training/walltime': 334.9686105251312, 'training/entropy_loss': Array(0.00584832, dtype=float32), 'training/policy_loss': Array(-0.00085939, dtype=float32), 'training/total_loss': Array(1202.3763, dtype=float32), 'training/v_loss': Array(1202.3713, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03557664, 0.01909782], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.8860624, 1.3692629], dtype=float32), 'eval/episode_reward': Array([-5512.5137,  4993.7676], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01337412, 0.00947876], dtype=float32), 'eval/avg_episode_length': Array([927.5781 , 200.34691], dtype=float32), 'eval/epoch_eval_time': 3.9086782932281494, 'eval/sps': 32747.642655002368}
I0728 19:48:03.515183 139709070006080 train.py:410] total steps: 100270080
