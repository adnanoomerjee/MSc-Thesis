I0729 01:49:04.172480 139831723206464 low_level_env.py:192] Initialising environment...
I0729 01:49:04.487168 139831723206464 low_level_env.py:300] Environment initialised.
I0729 01:49:04.491505 139831723206464 train.py:118] JAX is running on GPU.
I0729 01:49:04.491556 139831723206464 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0729 01:49:15.157222 139831723206464 train.py:367] Running initial eval
I0729 01:49:30.964170 139831723206464 train.py:373] {'eval/walltime': 15.670511960983276, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.33038273, 0.14411595], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.204517, 12.251559], dtype=float32), 'eval/episode_is_unhealthy': Array([0.1953125 , 0.39644107], dtype=float32), 'eval/episode_reward': Array([-25841.719,   9445.934], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32892084, 0.14519987], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.670511960983276, 'eval/sps': 8168.207925733168}
I0729 01:49:30.965770 139831723206464 train.py:379] starting iteration 0, 0 steps, 26.47427725791931
I0729 01:59:29.443187 139831723206464 train.py:394] {'eval/walltime': 19.655715703964233, 'training/sps': 93703.79800754422, 'training/walltime': 594.4860420227051, 'training/entropy_loss': Array(-0.04373581, dtype=float32), 'training/policy_loss': Array(-0.00116096, dtype=float32), 'training/total_loss': Array(47077.906, dtype=float32), 'training/v_loss': Array(47077.953, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.12259617, 0.11538038], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([10.857561,  9.525229], dtype=float32), 'eval/episode_is_unhealthy': Array([0.203125 , 0.4023248], dtype=float32), 'eval/episode_reward': Array([-13643.408,   8788.796], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.11199533, 0.1214619 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.985203742980957, 'eval/sps': 32118.809540275903}
I0729 01:59:29.477564 139831723206464 train.py:379] starting iteration 1, 55705600 steps, 624.9860756397247
I0729 02:09:22.635314 139831723206464 train.py:394] {'eval/walltime': 23.62916874885559, 'training/sps': 94548.05427646259, 'training/walltime': 1183.663687467575, 'training/entropy_loss': Array(-0.01332818, dtype=float32), 'training/policy_loss': Array(0.00049948, dtype=float32), 'training/total_loss': Array(1098.2415, dtype=float32), 'training/v_loss': Array(1098.2544, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.06977034, 0.07733012], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([5.4649134, 6.7768965], dtype=float32), 'eval/episode_is_unhealthy': Array([0.125    , 0.3307189], dtype=float32), 'eval/episode_reward': Array([-9181.542 ,  6847.9673], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.05908875, 0.08189418], dtype=float32), 'eval/avg_episode_length': Array([998.0781  ,  21.658432], dtype=float32), 'eval/epoch_eval_time': 3.9734530448913574, 'eval/sps': 32213.79453937898}
I0729 02:09:22.638026 139831723206464 train.py:379] starting iteration 2, 111411200 steps, 1218.1465413570404
I0729 02:19:14.816810 139831723206464 train.py:394] {'eval/walltime': 27.627671480178833, 'training/sps': 94709.22935431633, 'training/walltime': 1771.8386771678925, 'training/entropy_loss': Array(-0.00707504, dtype=float32), 'training/policy_loss': Array(0.00182669, dtype=float32), 'training/total_loss': Array(1277.5425, dtype=float32), 'training/v_loss': Array(1277.5479, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.0950021 , 0.10942961], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([7.593742, 9.415384], dtype=float32), 'eval/episode_is_unhealthy': Array([0.2265625 , 0.41860715], dtype=float32), 'eval/episode_reward': Array([-10543.857,   9727.508], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08580711, 0.11395486], dtype=float32), 'eval/avg_episode_length': Array([995.4297  ,  51.504803], dtype=float32), 'eval/epoch_eval_time': 3.998502731323242, 'eval/sps': 32011.98263471997}
I0729 02:19:14.819686 139831723206464 train.py:379] starting iteration 3, 167116800 steps, 1810.328201532364
I0729 02:29:07.610720 139831723206464 train.py:394] {'eval/walltime': 31.614731311798096, 'training/sps': 94608.93325423653, 'training/walltime': 2360.637198448181, 'training/entropy_loss': Array(-0.00583249, dtype=float32), 'training/policy_loss': Array(0.00247841, dtype=float32), 'training/total_loss': Array(1320.9567, dtype=float32), 'training/v_loss': Array(1320.96, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.07441688, 0.08657671], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([5.58908  , 7.5974274], dtype=float32), 'eval/episode_is_unhealthy': Array([0.1328125 , 0.33937198], dtype=float32), 'eval/episode_reward': Array([-8822.83 ,  8675.071], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.06137074, 0.09189942], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9870598316192627, 'eval/sps': 32103.857329879953}
I0729 02:29:07.613179 139831723206464 train.py:379] starting iteration 4, 222822400 steps, 2403.1216945648193
I0729 02:39:01.958428 139831723206464 train.py:394] {'eval/walltime': 35.596863746643066, 'training/sps': 94359.03898885626, 'training/walltime': 2950.995054960251, 'training/entropy_loss': Array(-0.00912811, dtype=float32), 'training/policy_loss': Array(0.0030514, dtype=float32), 'training/total_loss': Array(1640.6033, dtype=float32), 'training/v_loss': Array(1640.6094, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09836525, 0.09512655], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([8.068691, 8.136966], dtype=float32), 'eval/episode_is_unhealthy': Array([0.1953125 , 0.39644107], dtype=float32), 'eval/episode_reward': Array([-12683.802,   8279.835], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08822413, 0.1002047 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9821324348449707, 'eval/sps': 32143.581885915653}
I0729 02:39:01.960957 139831723206464 train.py:379] starting iteration 5, 278528000 steps, 2997.469471693039
