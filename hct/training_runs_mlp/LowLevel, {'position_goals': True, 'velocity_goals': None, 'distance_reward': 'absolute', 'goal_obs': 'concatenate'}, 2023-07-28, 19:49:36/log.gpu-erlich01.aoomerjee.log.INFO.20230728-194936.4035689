I0728 19:49:36.281013 139924877399872 low_level_env.py:190] Initialising environment...
I0728 19:49:36.598462 139924877399872 low_level_env.py:298] Environment initialised.
I0728 19:49:36.603001 139924877399872 train.py:118] JAX is running on GPU.
I0728 19:49:36.603039 139924877399872 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 19:49:42.758696 139924877399872 train.py:367] Running initial eval
I0728 19:49:58.141263 139924877399872 train.py:373] {'eval/walltime': 15.245935201644897, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3122307, 0.125826 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.410095, 10.832719], dtype=float32), 'eval/episode_reward': Array([-25113.64 ,   9349.084], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30779392, 0.12849705], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.245935201644897, 'eval/sps': 8395.680442495253}
I0728 19:49:58.142729 139924877399872 train.py:379] starting iteration 0, 0 steps, 21.539737701416016
I0728 19:50:49.208822 139924877399872 train.py:394] {'eval/walltime': 18.88105010986328, 'training/sps': 234912.92306440635, 'training/walltime': 47.426594734191895, 'training/entropy_loss': Array(-0.03547734, dtype=float32), 'training/policy_loss': Array(-0.00415606, dtype=float32), 'training/total_loss': Array(20856.264, dtype=float32), 'training/v_loss': Array(20856.3, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.12795204, 0.10881514], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([10.470005,  9.312751], dtype=float32), 'eval/episode_reward': Array([-13650.583,   8019.014], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1141195 , 0.11457096], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.635114908218384, 'eval/sps': 35212.09184078707}
I0728 19:50:49.226704 139924877399872 train.py:379] starting iteration 1, 11141120 steps, 72.62371325492859
I0728 19:51:28.521121 139924877399872 train.py:394] {'eval/walltime': 22.540252685546875, 'training/sps': 312678.8170892678, 'training/walltime': 83.05779004096985, 'training/entropy_loss': Array(-0.01711632, dtype=float32), 'training/policy_loss': Array(-0.00465633, dtype=float32), 'training/total_loss': Array(2132.238, dtype=float32), 'training/v_loss': Array(2132.2598, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.10989449, 0.11519759], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([8.617655, 9.965758], dtype=float32), 'eval/episode_reward': Array([-11004.871,   9061.589], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09311798, 0.12225565], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6592025756835938, 'eval/sps': 34980.29894562142}
I0728 19:51:28.523112 139924877399872 train.py:379] starting iteration 2, 22282240 steps, 111.92012214660645
I0728 19:52:08.582126 139924877399872 train.py:394] {'eval/walltime': 26.309470891952515, 'training/sps': 307040.9261981195, 'training/walltime': 119.34324598312378, 'training/entropy_loss': Array(-0.00856625, dtype=float32), 'training/policy_loss': Array(-0.00348269, dtype=float32), 'training/total_loss': Array(1964.3278, dtype=float32), 'training/v_loss': Array(1964.34, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11397769, 0.11919712], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 8.581897, 10.449262], dtype=float32), 'eval/episode_reward': Array([-10378.621,   9909.556], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09944674, 0.12512952], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.7692182064056396, 'eval/sps': 33959.2968596164}
I0728 19:52:08.583633 139924877399872 train.py:379] starting iteration 3, 33423360 steps, 151.9806432723999
I0728 19:52:48.954212 139924877399872 train.py:394] {'eval/walltime': 30.137646913528442, 'training/sps': 304911.71240998147, 'training/walltime': 155.88208508491516, 'training/entropy_loss': Array(-0.00894162, dtype=float32), 'training/policy_loss': Array(-0.0013979, dtype=float32), 'training/total_loss': Array(1281.3779, dtype=float32), 'training/v_loss': Array(1281.3882, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11117905, 0.12733692], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 8.411599, 11.054949], dtype=float32), 'eval/episode_reward': Array([-10405.273,  11007.448], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09644672, 0.13303226], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.8281760215759277, 'eval/sps': 33436.28905217029}
I0728 19:52:48.955656 139924877399872 train.py:379] starting iteration 4, 44564480 steps, 192.35266661643982
I0728 19:53:29.333476 139924877399872 train.py:394] {'eval/walltime': 33.99830985069275, 'training/sps': 305135.79125057126, 'training/walltime': 192.39409160614014, 'training/entropy_loss': Array(-0.00650285, dtype=float32), 'training/policy_loss': Array(-0.00383533, dtype=float32), 'training/total_loss': Array(1359.4131, dtype=float32), 'training/v_loss': Array(1359.4236, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.08259358, 0.08983213], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([5.7861834, 7.856446 ], dtype=float32), 'eval/episode_reward': Array([-8514.708,  9096.984], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.06759171, 0.09406288], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.8606629371643066, 'eval/sps': 33154.92755604746}
I0728 19:53:29.335059 139924877399872 train.py:379] starting iteration 5, 55705600 steps, 232.7320694923401
I0728 19:54:09.691123 139924877399872 train.py:394] {'eval/walltime': 37.91989803314209, 'training/sps': 305817.43528463325, 'training/walltime': 228.82471561431885, 'training/entropy_loss': Array(0.00050745, dtype=float32), 'training/policy_loss': Array(-0.00060285, dtype=float32), 'training/total_loss': Array(1229.3232, dtype=float32), 'training/v_loss': Array(1229.3234, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.05059601, 0.03342033], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.8915417, 2.919353 ], dtype=float32), 'eval/episode_reward': Array([-6607.4043,  6358.6665], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.02852174, 0.03412944], dtype=float32), 'eval/avg_episode_length': Array([996.1953  ,  42.876648], dtype=float32), 'eval/epoch_eval_time': 3.921588182449341, 'eval/sps': 32639.83724064925}
I0728 19:54:09.692696 139924877399872 train.py:379] starting iteration 6, 66846720 steps, 273.0897059440613
I0728 19:54:50.098385 139924877399872 train.py:394] {'eval/walltime': 41.81986141204834, 'training/sps': 305217.8264442363, 'training/walltime': 265.3269085884094, 'training/entropy_loss': Array(0.00583019, dtype=float32), 'training/policy_loss': Array(-0.0008847, dtype=float32), 'training/total_loss': Array(1390.7278, dtype=float32), 'training/v_loss': Array(1390.7229, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04097348, 0.01963063], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.0533085, 1.5481548], dtype=float32), 'eval/episode_reward': Array([-6397.995,  6078.961], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.0157239 , 0.01413983], dtype=float32), 'eval/avg_episode_length': Array([992.03906,  50.01615], dtype=float32), 'eval/epoch_eval_time': 3.89996337890625, 'eval/sps': 32820.82100881106}
I0728 19:54:50.099922 139924877399872 train.py:379] starting iteration 7, 77987840 steps, 313.496933221817
I0728 19:55:30.342262 139924877399872 train.py:394] {'eval/walltime': 45.69003987312317, 'training/sps': 306340.0821445238, 'training/walltime': 301.69537830352783, 'training/entropy_loss': Array(0.0057751, dtype=float32), 'training/policy_loss': Array(-4.0347004e-05, dtype=float32), 'training/total_loss': Array(1297.8877, dtype=float32), 'training/v_loss': Array(1297.8818, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03615601, 0.02073069], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.8510771, 1.5672147], dtype=float32), 'eval/episode_reward': Array([-4989.1084,  4754.324 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01432189, 0.01507104], dtype=float32), 'eval/avg_episode_length': Array([905.3203, 245.9542], dtype=float32), 'eval/epoch_eval_time': 3.870178461074829, 'eval/sps': 33073.410254175134}
I0728 19:55:30.343813 139924877399872 train.py:379] starting iteration 8, 89128960 steps, 353.7408242225647
I0728 19:56:10.928845 139924877399872 train.py:394] {'eval/walltime': 49.57221460342407, 'training/sps': 303580.95990032144, 'training/walltime': 338.3943860530853, 'training/entropy_loss': Array(0.00446673, dtype=float32), 'training/policy_loss': Array(-0.00154691, dtype=float32), 'training/total_loss': Array(1431.2599, dtype=float32), 'training/v_loss': Array(1431.257, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03341177, 0.02089683], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.8479476, 1.3906252], dtype=float32), 'eval/episode_reward': Array([-5277.7974,  5127.264 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01264472, 0.00989528], dtype=float32), 'eval/avg_episode_length': Array([802.91406, 328.15833], dtype=float32), 'eval/epoch_eval_time': 3.8821747303009033, 'eval/sps': 32971.21044061787}
I0728 19:56:11.271976 139924877399872 train.py:410] total steps: 100270080
