I0729 00:12:33.323844 140612055705408 low_level_env.py:192] Initialising environment...
I0729 00:12:33.673419 140612055705408 low_level_env.py:300] Environment initialised.
I0729 00:12:33.679713 140612055705408 train.py:118] JAX is running on GPU.
I0729 00:12:33.679753 140612055705408 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0729 00:12:42.186371 140612055705408 train.py:367] Running initial eval
I0729 00:12:57.973755 140612055705408 train.py:373] {'eval/walltime': 15.641863584518433, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32945865, 0.13466775], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([28.11353 , 11.478465], dtype=float32), 'eval/episode_is_unhealthy': Array([0.2109375, 0.4079741], dtype=float32), 'eval/episode_reward': Array([-25859.717,   9123.682], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.32774666, 0.13605049], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.641863584518433, 'eval/sps': 8183.1681569380435}
I0729 00:12:57.975178 140612055705408 train.py:379] starting iteration 0, 0 steps, 24.295479774475098
I0729 00:20:06.687637 140612055705408 train.py:394] {'eval/walltime': 19.64639973640442, 'training/sps': 131163.84755158928, 'training/walltime': 424.7023935317993, 'training/entropy_loss': Array(-0.04810971, dtype=float32), 'training/policy_loss': Array(-0.00093863, dtype=float32), 'training/total_loss': Array(68811.7, dtype=float32), 'training/v_loss': Array(68811.75, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.2139316 , 0.12372244], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([18.46474 , 10.492338], dtype=float32), 'eval/episode_is_unhealthy': Array([0.2265625 , 0.41860715], dtype=float32), 'eval/episode_reward': Array([-20381.805,   9042.057], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.21080276, 0.12584512], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.004536151885986, 'eval/sps': 31963.75189164338}
I0729 00:20:06.780106 140612055705408 train.py:379] starting iteration 1, 55705600 steps, 453.1003906726837
I0729 00:27:14.279419 140612055705408 train.py:394] {'eval/walltime': 23.615628004074097, 'training/sps': 131528.63591876693, 'training/walltime': 848.2268953323364, 'training/entropy_loss': Array(-0.00715831, dtype=float32), 'training/policy_loss': Array(-0.00056843, dtype=float32), 'training/total_loss': Array(1159.2656, dtype=float32), 'training/v_loss': Array(1159.2734, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11940808, 0.12026291], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([10.226714, 10.351635], dtype=float32), 'eval/episode_is_unhealthy': Array([0.203125 , 0.4023248], dtype=float32), 'eval/episode_reward': Array([-13392.979,   8910.96 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.11058448, 0.12511909], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9692282676696777, 'eval/sps': 32248.082339479162}
I0729 00:27:14.282427 140612055705408 train.py:379] starting iteration 2, 111411200 steps, 880.6027314662933
I0729 00:34:20.728079 140612055705408 train.py:394] {'eval/walltime': 27.59300994873047, 'training/sps': 131859.44961827694, 'training/walltime': 1270.6888437271118, 'training/entropy_loss': Array(-0.00107235, dtype=float32), 'training/policy_loss': Array(0.00092788, dtype=float32), 'training/total_loss': Array(809.88336, dtype=float32), 'training/v_loss': Array(809.88354, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.12432677, 0.12430539], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([10.4727545, 10.632012 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0.2578125 , 0.43743023], dtype=float32), 'eval/episode_reward': Array([-13552.461,   9277.375], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.11670375, 0.12873004], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.977381944656372, 'eval/sps': 32181.97341393589}
I0729 00:34:20.730552 140612055705408 train.py:379] starting iteration 3, 167116800 steps, 1307.0508573055267
I0729 00:41:27.330178 140612055705408 train.py:394] {'eval/walltime': 31.565946578979492, 'training/sps': 131809.8064213258, 'training/walltime': 1693.3099029064178, 'training/entropy_loss': Array(0.00209597, dtype=float32), 'training/policy_loss': Array(0.00158699, dtype=float32), 'training/total_loss': Array(746.3795, dtype=float32), 'training/v_loss': Array(746.37573, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09790125, 0.11573614], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([8.100475, 9.926133], dtype=float32), 'eval/episode_is_unhealthy': Array([0.21875   , 0.41339865], dtype=float32), 'eval/episode_reward': Array([-11943.601,   9276.338], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08959623, 0.11971205], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9729366302490234, 'eval/sps': 32217.981788442714}
I0729 00:41:27.332665 140612055705408 train.py:379] starting iteration 4, 222822400 steps, 1733.652969121933
I0729 00:48:34.659453 140612055705408 train.py:394] {'eval/walltime': 35.54404878616333, 'training/sps': 131584.9491555699, 'training/walltime': 2116.653152704239, 'training/entropy_loss': Array(0.00413427, dtype=float32), 'training/policy_loss': Array(0.00198268, dtype=float32), 'training/total_loss': Array(704.02795, dtype=float32), 'training/v_loss': Array(704.02185, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09813284, 0.12063151], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 8.096137, 10.300727], dtype=float32), 'eval/episode_is_unhealthy': Array([0.2421875, 0.4284072], dtype=float32), 'eval/episode_reward': Array([-11912.42 ,   9143.338], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09079056, 0.12403521], dtype=float32), 'eval/avg_episode_length': Array([989.65625 ,  62.540916], dtype=float32), 'eval/epoch_eval_time': 3.978102207183838, 'eval/sps': 32176.146648231355}
I0729 00:48:34.661869 140612055705408 train.py:379] starting iteration 5, 278528000 steps, 2160.9821746349335
I0729 00:55:42.815568 140612055705408 train.py:394] {'eval/walltime': 39.52865195274353, 'training/sps': 131330.2809634867, 'training/walltime': 2540.817325592041, 'training/entropy_loss': Array(0.00341818, dtype=float32), 'training/policy_loss': Array(0.00259934, dtype=float32), 'training/total_loss': Array(896.6548, dtype=float32), 'training/v_loss': Array(896.6488, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.0860673 , 0.11236029], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([7.243073 , 9.4979725], dtype=float32), 'eval/episode_is_unhealthy': Array([0.234375  , 0.42360756], dtype=float32), 'eval/episode_reward': Array([-9619.725,  9323.358], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08160074, 0.11375333], dtype=float32), 'eval/avg_episode_length': Array([807.96875, 286.5594 ], dtype=float32), 'eval/epoch_eval_time': 3.9846031665802, 'eval/sps': 32123.650624374837}
I0729 00:55:42.818038 140612055705408 train.py:379] starting iteration 6, 334233600 steps, 2589.138343334198
I0729 01:02:51.581649 140612055705408 train.py:394] {'eval/walltime': 43.503721475601196, 'training/sps': 131138.69472853598, 'training/walltime': 2965.601178407669, 'training/entropy_loss': Array(0.00126246, dtype=float32), 'training/policy_loss': Array(0.002697, dtype=float32), 'training/total_loss': Array(1281.8413, dtype=float32), 'training/v_loss': Array(1281.8374, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.08125751, 0.11763754], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([6.89798 , 9.897555], dtype=float32), 'eval/episode_is_unhealthy': Array([0.1875    , 0.39031237], dtype=float32), 'eval/episode_reward': Array([-9365.805,  9677.349], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.07798205, 0.11844626], dtype=float32), 'eval/avg_episode_length': Array([702.7656 , 315.29733], dtype=float32), 'eval/epoch_eval_time': 3.975069522857666, 'eval/sps': 32200.69467061325}
I0729 01:02:51.584184 140612055705408 train.py:379] starting iteration 7, 389939200 steps, 3017.904489517212
