I0728 20:29:00.279041 140244282623808 low_level_env.py:190] Initialising environment...
I0728 20:29:00.599673 140244282623808 low_level_env.py:298] Environment initialised.
I0728 20:29:00.604628 140244282623808 train.py:118] JAX is running on GPU.
I0728 20:29:00.604668 140244282623808 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 20:29:06.889307 140244282623808 train.py:367] Running initial eval
I0728 20:29:22.277186 140244282623808 train.py:373] {'eval/walltime': 15.260882139205933, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.311153  , 0.12763171], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.327017, 10.983034], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-25240.465,   9691.533], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30682778, 0.13043036], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.260882139205933, 'eval/sps': 8387.457476731433}
I0728 20:29:22.278573 140244282623808 train.py:379] starting iteration 0, 0 steps, 21.673951148986816
I0728 20:30:13.292856 140244282623808 train.py:394] {'eval/walltime': 18.89620280265808, 'training/sps': 235172.11163150988, 'training/walltime': 47.374324798583984, 'training/entropy_loss': Array(-0.03543367, dtype=float32), 'training/policy_loss': Array(-0.00451194, dtype=float32), 'training/total_loss': Array(20712.594, dtype=float32), 'training/v_loss': Array(20712.633, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.13519832, 0.11210379], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([11.150543,  9.538354], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-13836.006,   8189.788], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.12263401, 0.11743281], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6353206634521484, 'eval/sps': 35210.0988743176}
I0728 20:30:13.310315 140244282623808 train.py:379] starting iteration 1, 11141120 steps, 72.70569682121277
I0728 20:30:52.299036 140244282623808 train.py:394] {'eval/walltime': 22.54192590713501, 'training/sps': 315267.44844462315, 'training/walltime': 82.7129557132721, 'training/entropy_loss': Array(-0.01796051, dtype=float32), 'training/policy_loss': Array(-0.00418171, dtype=float32), 'training/total_loss': Array(1754.9375, dtype=float32), 'training/v_loss': Array(1754.9597, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09869915, 0.10907178], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([7.8046007, 9.305063 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-10028.566,   8448.542], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08007989, 0.1161423 ], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6457231044769287, 'eval/sps': 35109.63294025722}
I0728 20:30:52.301035 140244282623808 train.py:379] starting iteration 2, 22282240 steps, 111.69641757011414
I0728 20:31:31.372092 140244282623808 train.py:394] {'eval/walltime': 26.19759178161621, 'training/sps': 314619.3859568453, 'training/walltime': 118.1243782043457, 'training/entropy_loss': Array(-0.00870483, dtype=float32), 'training/policy_loss': Array(-0.00402341, dtype=float32), 'training/total_loss': Array(1717.2422, dtype=float32), 'training/v_loss': Array(1717.2548, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.10993459, 0.12042969], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 8.280529, 10.488347], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-10131.568,   9951.907], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09674531, 0.1253961 ], dtype=float32), 'eval/avg_episode_length': Array([994.75    ,  59.164494], dtype=float32), 'eval/epoch_eval_time': 3.655665874481201, 'eval/sps': 35014.14089660623}
I0728 20:31:31.373813 140244282623808 train.py:379] starting iteration 3, 33423360 steps, 150.76919555664062
I0728 20:32:11.269065 140244282623808 train.py:394] {'eval/walltime': 29.929503917694092, 'training/sps': 308111.75218273146, 'training/walltime': 154.28372597694397, 'training/entropy_loss': Array(-0.00398262, dtype=float32), 'training/policy_loss': Array(-0.00380261, dtype=float32), 'training/total_loss': Array(1105.9125, dtype=float32), 'training/v_loss': Array(1105.9202, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.10814272, 0.12518474], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 7.8656693, 10.99868  ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-9868.708, 10915.269], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.092216  , 0.13131097], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.731912136077881, 'eval/sps': 34298.771067671456}
I0728 20:32:11.270797 140244282623808 train.py:379] starting iteration 4, 44564480 steps, 190.66617965698242
I0728 20:32:51.492800 140244282623808 train.py:394] {'eval/walltime': 33.754820346832275, 'training/sps': 306137.2301589239, 'training/walltime': 190.67629408836365, 'training/entropy_loss': Array(0.00264644, dtype=float32), 'training/policy_loss': Array(-0.00122616, dtype=float32), 'training/total_loss': Array(1296.467, dtype=float32), 'training/v_loss': Array(1296.4657, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.07922819, 0.08168597], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([5.43322  , 7.1575956], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-8882.138,  8737.664], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.06320861, 0.08566141], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.8253164291381836, 'eval/sps': 33461.28415024675}
I0728 20:32:51.494431 140244282623808 train.py:379] starting iteration 5, 55705600 steps, 230.88981366157532
I0728 20:33:31.732646 140244282623808 train.py:394] {'eval/walltime': 37.59686350822449, 'training/sps': 306150.6943196649, 'training/walltime': 227.06726169586182, 'training/entropy_loss': Array(0.00543995, dtype=float32), 'training/policy_loss': Array(-0.00089775, dtype=float32), 'training/total_loss': Array(1178.7483, dtype=float32), 'training/v_loss': Array(1178.7438, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04753637, 0.03683201], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.7007449, 3.1416898], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6131.7637,  5894.178 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.02640201, 0.03683124], dtype=float32), 'eval/avg_episode_length': Array([976.21875, 128.79053], dtype=float32), 'eval/epoch_eval_time': 3.842043161392212, 'eval/sps': 33315.60698907339}
I0728 20:33:31.734327 140244282623808 train.py:379] starting iteration 6, 66846720 steps, 271.12970876693726
I0728 20:34:12.119704 140244282623808 train.py:394] {'eval/walltime': 41.49478363990784, 'training/sps': 305377.6786393557, 'training/walltime': 263.55034732818604, 'training/entropy_loss': Array(0.0074943, dtype=float32), 'training/policy_loss': Array(-0.00052446, dtype=float32), 'training/total_loss': Array(1290.7944, dtype=float32), 'training/v_loss': Array(1290.7874, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04643227, 0.04052694], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.6958559, 3.421753 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-7052.626 ,  7369.6943], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.02613603, 0.04029217], dtype=float32), 'eval/avg_episode_length': Array([973.3281 , 121.83359], dtype=float32), 'eval/epoch_eval_time': 3.8979201316833496, 'eval/sps': 32838.02532524496}
I0728 20:34:12.121398 140244282623808 train.py:379] starting iteration 7, 77987840 steps, 311.5167803764343
I0728 20:34:52.350289 140244282623808 train.py:394] {'eval/walltime': 45.37949275970459, 'training/sps': 306584.45263824094, 'training/walltime': 299.8898286819458, 'training/entropy_loss': Array(0.00810114, dtype=float32), 'training/policy_loss': Array(-0.0009855, dtype=float32), 'training/total_loss': Array(1321.9412, dtype=float32), 'training/v_loss': Array(1321.934, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03643613, 0.02009234], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.0043862, 1.3373917], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5661.8022,  5007.616 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.0149346 , 0.01018503], dtype=float32), 'eval/avg_episode_length': Array([883.3594 , 265.25104], dtype=float32), 'eval/epoch_eval_time': 3.884709119796753, 'eval/sps': 32949.69997823078}
I0728 20:34:52.351979 140244282623808 train.py:379] starting iteration 8, 89128960 steps, 351.7473621368408
I0728 20:35:32.556293 140244282623808 train.py:394] {'eval/walltime': 49.2587149143219, 'training/sps': 306739.0710834954, 'training/walltime': 336.2109923362732, 'training/entropy_loss': Array(0.00627168, dtype=float32), 'training/policy_loss': Array(0.00036218, dtype=float32), 'training/total_loss': Array(1564.0818, dtype=float32), 'training/v_loss': Array(1564.0752, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03136183, 0.02804301], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.8769925, 2.0640483], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5185.659,  5413.506], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01484906, 0.02171017], dtype=float32), 'eval/avg_episode_length': Array([744.59375, 336.9894 ], dtype=float32), 'eval/epoch_eval_time': 3.8792221546173096, 'eval/sps': 32996.305676292825}
I0728 20:35:32.910811 140244282623808 train.py:410] total steps: 100270080
