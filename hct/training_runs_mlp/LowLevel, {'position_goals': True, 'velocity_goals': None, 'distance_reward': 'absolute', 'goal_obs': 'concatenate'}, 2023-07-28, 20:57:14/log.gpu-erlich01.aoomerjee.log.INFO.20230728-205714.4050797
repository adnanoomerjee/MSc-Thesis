I0728 20:57:14.295271 140088568579904 low_level_env.py:190] Initialising environment...
I0728 20:57:14.597849 140088568579904 low_level_env.py:298] Environment initialised.
I0728 20:57:14.602004 140088568579904 train.py:118] JAX is running on GPU.
I0728 20:57:14.602073 140088568579904 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 20:57:20.892035 140088568579904 train.py:367] Running initial eval
I0728 20:57:36.386190 140088568579904 train.py:373] {'eval/walltime': 15.358238458633423, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32227567, 0.13481641], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.243622, 11.614142], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-25424.105,  10088.303], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31787223, 0.13789417], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.358238458633423, 'eval/sps': 8334.289140304796}
I0728 20:57:36.387471 140088568579904 train.py:379] starting iteration 0, 0 steps, 21.785515546798706
I0728 21:00:51.487975 140088568579904 train.py:394] {'eval/walltime': 19.239702701568604, 'training/sps': 291324.93260132626, 'training/walltime': 191.21466708183289, 'training/entropy_loss': Array(-0.01217967, dtype=float32), 'training/policy_loss': Array(-0.00302361, dtype=float32), 'training/total_loss': Array(5227.754, dtype=float32), 'training/v_loss': Array(5227.7695, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.06153581, 0.04843934], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([3.8067143, 4.3248196], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-7640.3564,  6957.401 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.04211404, 0.05153806], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.8814642429351807, 'eval/sps': 32977.245696125705}
I0728 21:00:51.506283 140088568579904 train.py:379] starting iteration 1, 55705600 steps, 216.90432906150818
I0728 21:03:59.896485 140088568579904 train.py:394] {'eval/walltime': 23.180336236953735, 'training/sps': 302016.99297548557, 'training/walltime': 375.6599178314209, 'training/entropy_loss': Array(0.00817654, dtype=float32), 'training/policy_loss': Array(0.00267372, dtype=float32), 'training/total_loss': Array(1159.1482, dtype=float32), 'training/v_loss': Array(1159.1373, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03740904, 0.01612459], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.8278548, 1.2628549], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5028.179,  4108.022], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.0129461 , 0.00871999], dtype=float32), 'eval/avg_episode_length': Array([984.8203 ,  97.52624], dtype=float32), 'eval/epoch_eval_time': 3.940633535385132, 'eval/sps': 32482.08666211082}
I0728 21:03:59.898462 140088568579904 train.py:379] starting iteration 2, 111411200 steps, 405.2965075969696
I0728 21:07:11.154877 140088568579904 train.py:394] {'eval/walltime': 27.044329404830933, 'training/sps': 297274.1909053922, 'training/walltime': 563.0478637218475, 'training/entropy_loss': Array(0.00501689, dtype=float32), 'training/policy_loss': Array(-0.00090852, dtype=float32), 'training/total_loss': Array(1784.0396, dtype=float32), 'training/v_loss': Array(1784.0354, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.02974787, 0.02329913], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.9527535, 1.408867 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-4252.787 ,  3750.7925], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01462503, 0.01193962], dtype=float32), 'eval/avg_episode_length': Array([556.96875, 418.99133], dtype=float32), 'eval/epoch_eval_time': 3.8639931678771973, 'eval/sps': 33126.35256814409}
I0728 21:07:11.160412 140088568579904 train.py:379] starting iteration 3, 167116800 steps, 596.5584423542023
I0728 21:10:23.405153 140088568579904 train.py:394] {'eval/walltime': 30.94383692741394, 'training/sps': 295770.24318230303, 'training/walltime': 751.3886494636536, 'training/entropy_loss': Array(0.00350362, dtype=float32), 'training/policy_loss': Array(-0.00210168, dtype=float32), 'training/total_loss': Array(2356.4717, dtype=float32), 'training/v_loss': Array(2356.4702, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.02521705, 0.02277776], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.6207027, 1.3399903], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-3743.8218,  4675.257 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01139884, 0.00981274], dtype=float32), 'eval/avg_episode_length': Array([424.97656, 418.02435], dtype=float32), 'eval/epoch_eval_time': 3.899507522583008, 'eval/sps': 32824.657795560204}
I0728 21:10:23.407122 140088568579904 train.py:379] starting iteration 4, 222822400 steps, 788.805168390274
I0728 21:13:35.511168 140088568579904 train.py:394] {'eval/walltime': 34.809632539749146, 'training/sps': 295937.7017698109, 'training/walltime': 939.6228611469269, 'training/entropy_loss': Array(0.00799774, dtype=float32), 'training/policy_loss': Array(-0.00177053, dtype=float32), 'training/total_loss': Array(2329.0933, dtype=float32), 'training/v_loss': Array(2329.087, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.02427384, 0.02317425], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.5773363, 1.3677161], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-3480.8818,  4280.294 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01271843, 0.01219523], dtype=float32), 'eval/avg_episode_length': Array([392.8828 , 417.86868], dtype=float32), 'eval/epoch_eval_time': 3.865795612335205, 'eval/sps': 33110.90725841019}
I0728 21:13:35.513011 140088568579904 train.py:379] starting iteration 5, 278528000 steps, 980.9110565185547
I0728 21:16:48.138993 140088568579904 train.py:394] {'eval/walltime': 38.6800012588501, 'training/sps': 295126.4727861069, 'training/walltime': 1128.374481678009, 'training/entropy_loss': Array(0.01165655, dtype=float32), 'training/policy_loss': Array(-0.00131161, dtype=float32), 'training/total_loss': Array(2147.4932, dtype=float32), 'training/v_loss': Array(2147.483, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.0234466 , 0.02180784], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.5471911, 1.2657181], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-2786.9717,  3545.5342], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01111798, 0.0091625 ], dtype=float32), 'eval/avg_episode_length': Array([376.1172 , 421.79053], dtype=float32), 'eval/epoch_eval_time': 3.870368719100952, 'eval/sps': 33071.78444479913}
I0728 21:16:48.140890 140088568579904 train.py:379] starting iteration 6, 334233600 steps, 1173.538935661316
I0728 21:20:01.395912 140088568579904 train.py:394] {'eval/walltime': 42.548035621643066, 'training/sps': 294142.56524015777, 'training/walltime': 1317.7574768066406, 'training/entropy_loss': Array(0.01331583, dtype=float32), 'training/policy_loss': Array(-0.00103247, dtype=float32), 'training/total_loss': Array(1971.1628, dtype=float32), 'training/v_loss': Array(1971.1509, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.01946158, 0.01953085], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.3095897, 1.1130689], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-2550.9102,  3388.6326], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.00989981, 0.00902326], dtype=float32), 'eval/avg_episode_length': Array([307.6328 , 384.46487], dtype=float32), 'eval/epoch_eval_time': 3.8680343627929688, 'eval/sps': 33091.74324593533}
I0728 21:20:01.397765 140088568579904 train.py:379] starting iteration 7, 389939200 steps, 1366.7958114147186
I0728 21:23:14.429920 140088568579904 train.py:394] {'eval/walltime': 46.434244871139526, 'training/sps': 294517.2354436479, 'training/walltime': 1506.89954829216, 'training/entropy_loss': Array(0.01557274, dtype=float32), 'training/policy_loss': Array(-0.00079149, dtype=float32), 'training/total_loss': Array(1854.6414, dtype=float32), 'training/v_loss': Array(1854.6266, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.01978613, 0.01983153], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.3425739, 1.1607949], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-2303.2583,  2863.2932], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01006039, 0.00964258], dtype=float32), 'eval/avg_episode_length': Array([302.23438, 387.1993 ], dtype=float32), 'eval/epoch_eval_time': 3.88620924949646, 'eval/sps': 32936.98094527079}
I0728 21:23:14.431759 140088568579904 train.py:379] starting iteration 8, 445644800 steps, 1559.8298058509827
I0728 21:26:28.155904 140088568579904 train.py:394] {'eval/walltime': 50.31971454620361, 'training/sps': 293443.6094240691, 'training/walltime': 1696.733636379242, 'training/entropy_loss': Array(0.01463919, dtype=float32), 'training/policy_loss': Array(-0.000748, dtype=float32), 'training/total_loss': Array(1775.0635, dtype=float32), 'training/v_loss': Array(1775.0498, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.02116863, 0.02049953], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.4286704, 1.1859142], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-2779.3416,  3759.4617], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01024185, 0.00915753], dtype=float32), 'eval/avg_episode_length': Array([330.625  , 396.94562], dtype=float32), 'eval/epoch_eval_time': 3.885469675064087, 'eval/sps': 32943.25029004087}
I0728 21:26:28.509711 140088568579904 train.py:410] total steps: 501350400
