I0728 20:00:56.260429 140127528027968 low_level_env.py:190] Initialising environment...
I0728 20:00:56.556262 140127528027968 low_level_env.py:298] Environment initialised.
I0728 20:00:56.578338 140127528027968 train.py:118] JAX is running on GPU.
I0728 20:00:56.578451 140127528027968 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 20:01:02.842061 140127528027968 train.py:367] Running initial eval
I0728 20:01:18.284326 140127528027968 train.py:373] {'eval/walltime': 15.298651695251465, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.32362437, 0.13769361], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.397074 , 11.8055725], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-25865.348,   9997.226], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31939322, 0.14067674], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.298651695251465, 'eval/sps': 8366.750387534465}
I0728 20:01:18.285599 140127528027968 train.py:379] starting iteration 0, 0 steps, 21.707294702529907
I0728 20:02:09.330127 140127528027968 train.py:394] {'eval/walltime': 18.938379764556885, 'training/sps': 235041.69044838648, 'training/walltime': 47.400612115859985, 'training/entropy_loss': Array(-0.03548718, dtype=float32), 'training/policy_loss': Array(-0.00516116, dtype=float32), 'training/total_loss': Array(20570.932, dtype=float32), 'training/v_loss': Array(20570.973, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.12565118, 0.11374749], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([10.351513,  9.707926], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-13239.299,   8194.939], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.1111944 , 0.11977276], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.63972806930542, 'eval/sps': 35167.46239353717}
I0728 20:02:09.346089 140127528027968 train.py:379] starting iteration 1, 11141120 steps, 72.76778793334961
I0728 20:02:48.303056 140127528027968 train.py:394] {'eval/walltime': 22.583423614501953, 'training/sps': 315542.7492993646, 'training/walltime': 82.70841121673584, 'training/entropy_loss': Array(-0.01875837, dtype=float32), 'training/policy_loss': Array(-0.00395434, dtype=float32), 'training/total_loss': Array(1672.2432, dtype=float32), 'training/v_loss': Array(1672.2659, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11322789, 0.11626332], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 8.912956, 10.01303 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-10947.416,   9294.418], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09622929, 0.12354927], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.6450438499450684, 'eval/sps': 35116.17562623533}
I0728 20:02:48.305044 140127528027968 train.py:379] starting iteration 2, 22282240 steps, 111.72674345970154
I0728 20:03:27.423986 140127528027968 train.py:394] {'eval/walltime': 26.236619472503662, 'training/sps': 314173.61543357634, 'training/walltime': 118.17007780075073, 'training/entropy_loss': Array(-0.00865012, dtype=float32), 'training/policy_loss': Array(-0.00363969, dtype=float32), 'training/total_loss': Array(1839.357, dtype=float32), 'training/v_loss': Array(1839.3693, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.12138138, 0.13441093], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 9.199579, 11.729771], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-10793.315,  11200.309], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.10748229, 0.14000233], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.653195858001709, 'eval/sps': 35037.81482715678}
I0728 20:03:27.425658 140127528027968 train.py:379] starting iteration 3, 33423360 steps, 150.84735751152039
I0728 20:04:07.433171 140127528027968 train.py:394] {'eval/walltime': 29.978377103805542, 'training/sps': 307248.37384856265, 'training/walltime': 154.43103456497192, 'training/entropy_loss': Array(-0.00294296, dtype=float32), 'training/policy_loss': Array(-0.0023115, dtype=float32), 'training/total_loss': Array(1291.7268, dtype=float32), 'training/v_loss': Array(1291.7322, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.11001599, 0.12837094], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 7.997498, 11.274597], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-9868.823, 11081.063], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.09409105, 0.13471057], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.74175763130188, 'eval/sps': 34208.52246794633}
I0728 20:04:07.434857 140127528027968 train.py:379] starting iteration 4, 44564480 steps, 190.85655665397644
I0728 20:04:47.690237 140127528027968 train.py:394] {'eval/walltime': 33.81666827201843, 'training/sps': 305967.8830832565, 'training/walltime': 190.84374523162842, 'training/entropy_loss': Array(0.00186381, dtype=float32), 'training/policy_loss': Array(0.00121725, dtype=float32), 'training/total_loss': Array(1400.5237, dtype=float32), 'training/v_loss': Array(1400.5205, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09927587, 0.11804402], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 7.2231455, 10.295273 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-9296.419, 10679.75 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08503816, 0.12305005], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.8382911682128906, 'eval/sps': 33348.17354661419}
I0728 20:04:47.691982 140127528027968 train.py:379] starting iteration 5, 55705600 steps, 231.11368131637573
I0728 20:05:27.973047 140127528027968 train.py:394] {'eval/walltime': 37.664849519729614, 'training/sps': 305833.20919812215, 'training/walltime': 227.27249026298523, 'training/entropy_loss': Array(0.00470401, dtype=float32), 'training/policy_loss': Array(-0.0001456, dtype=float32), 'training/total_loss': Array(1201.2668, dtype=float32), 'training/v_loss': Array(1201.2622, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.05485205, 0.05937593], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([3.293826 , 5.2097244], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6555.649,  7720.746], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.03543757, 0.06204648], dtype=float32), 'eval/avg_episode_length': Array([992.7031  ,  62.456276], dtype=float32), 'eval/epoch_eval_time': 3.8481812477111816, 'eval/sps': 33262.466542118236}
I0728 20:05:27.974672 140127528027968 train.py:379] starting iteration 6, 66846720 steps, 271.39637088775635
I0728 20:06:08.322867 140127528027968 train.py:394] {'eval/walltime': 41.573344469070435, 'training/sps': 305776.51796157897, 'training/walltime': 263.70798921585083, 'training/entropy_loss': Array(0.00912225, dtype=float32), 'training/policy_loss': Array(8.3853956e-05, dtype=float32), 'training/total_loss': Array(1326.3723, dtype=float32), 'training/v_loss': Array(1326.3633, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04736321, 0.03040661], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.687999 , 2.5678453], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-6961.0015,  6649.4614], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.02338586, 0.03090843], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.9084949493408203, 'eval/sps': 32749.178816666397}
I0728 20:06:08.324518 140127528027968 train.py:379] starting iteration 7, 77987840 steps, 311.74621844291687
I0728 20:06:48.647258 140127528027968 train.py:394] {'eval/walltime': 45.47530961036682, 'training/sps': 305934.157926932, 'training/walltime': 300.1247138977051, 'training/entropy_loss': Array(0.00893648, dtype=float32), 'training/policy_loss': Array(0.00102855, dtype=float32), 'training/total_loss': Array(1243.4392, dtype=float32), 'training/v_loss': Array(1243.4292, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04092424, 0.01823309], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.048831 , 1.5184447], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5614.338 ,  4800.4575], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01560795, 0.01409068], dtype=float32), 'eval/avg_episode_length': Array([990.84375,  63.95011], dtype=float32), 'eval/epoch_eval_time': 3.9019651412963867, 'eval/sps': 32803.9834711269}
I0728 20:06:48.648871 140127528027968 train.py:379] starting iteration 8, 89128960 steps, 352.0705714225769
I0728 20:07:28.834246 140127528027968 train.py:394] {'eval/walltime': 49.35616970062256, 'training/sps': 306914.58668944356, 'training/walltime': 336.42510652542114, 'training/entropy_loss': Array(0.00751013, dtype=float32), 'training/policy_loss': Array(-0.00080795, dtype=float32), 'training/total_loss': Array(1361.3403, dtype=float32), 'training/v_loss': Array(1361.3336, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03617585, 0.01917464], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.8088363, 1.3869214], dtype=float32), 'eval/episode_is_unhealthy': Array([0., 0.], dtype=float32), 'eval/episode_reward': Array([-5706.4854,  5293.065 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01343968, 0.01088533], dtype=float32), 'eval/avg_episode_length': Array([925.4531 , 203.57213], dtype=float32), 'eval/epoch_eval_time': 3.8808600902557373, 'eval/sps': 32982.379427021595}
I0728 20:07:29.188329 140127528027968 train.py:410] total steps: 100270080
