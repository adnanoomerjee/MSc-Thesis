I0729 02:49:38.042292 140526158489408 low_level_env.py:192] Initialising environment...
I0729 02:49:38.371592 140526158489408 low_level_env.py:300] Environment initialised.
I0729 02:49:38.387121 140526158489408 train.py:118] JAX is running on GPU.
I0729 02:49:38.387181 140526158489408 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0729 02:49:49.023009 140526158489408 train.py:367] Running initial eval
I0729 02:50:05.089249 140526158489408 train.py:373] {'eval/walltime': 15.917235374450684, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31745335, 0.13872562], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([27.116007 , 11.7849045], dtype=float32), 'eval/episode_is_unhealthy': Array([0.203125 , 0.4023248], dtype=float32), 'eval/episode_reward': Array([-25625.168,   9597.641], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.31582704, 0.14004207], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.917235374450684, 'eval/sps': 8041.597487806037}
I0729 02:50:05.090802 140526158489408 train.py:379] starting iteration 0, 0 steps, 26.703698873519897
I0729 03:03:47.909464 140526158489408 train.py:394] {'eval/walltime': 20.00044274330139, 'training/sps': 68039.15843383755, 'training/walltime': 818.728527545929, 'training/entropy_loss': Array(-0.04277477, dtype=float32), 'training/policy_loss': Array(-0.00157229, dtype=float32), 'training/total_loss': Array(54151.656, dtype=float32), 'training/v_loss': Array(54151.703, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.13350308, 0.11654878], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([11.604587,  9.702962], dtype=float32), 'eval/episode_is_unhealthy': Array([0.1953125 , 0.39644107], dtype=float32), 'eval/episode_reward': Array([-14821.879,   8599.98 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.12491834, 0.12159057], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 4.083207368850708, 'eval/sps': 31347.905809649805}
I0729 03:03:47.952094 140526158489408 train.py:379] starting iteration 1, 55705600 steps, 849.5649890899658
I0729 03:17:18.659804 140526158489408 train.py:394] {'eval/walltime': 24.08438539505005, 'training/sps': 69060.74701878827, 'training/walltime': 1625.34592461586, 'training/entropy_loss': Array(-0.00970131, dtype=float32), 'training/policy_loss': Array(2.1989747e-05, dtype=float32), 'training/total_loss': Array(845.52795, dtype=float32), 'training/v_loss': Array(845.5376, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.08095432, 0.09916829], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([6.507424, 8.560899], dtype=float32), 'eval/episode_is_unhealthy': Array([0.1796875 , 0.38392696], dtype=float32), 'eval/episode_reward': Array([-10393.5205,   7821.4966], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.0719214, 0.1029043], dtype=float32), 'eval/avg_episode_length': Array([990.2422  ,  53.315506], dtype=float32), 'eval/epoch_eval_time': 4.083942651748657, 'eval/sps': 31342.261856981055}
I0729 03:17:18.662948 140526158489408 train.py:379] starting iteration 2, 111411200 steps, 1660.2758452892303
I0729 03:30:50.629642 140526158489408 train.py:394] {'eval/walltime': 28.157891750335693, 'training/sps': 68952.2024046308, 'training/walltime': 2433.233099460602, 'training/entropy_loss': Array(-0.00723496, dtype=float32), 'training/policy_loss': Array(0.0020487, dtype=float32), 'training/total_loss': Array(1279.7571, dtype=float32), 'training/v_loss': Array(1279.7625, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.08770949, 0.12952055], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 7.4119654, 10.8954935], dtype=float32), 'eval/episode_is_unhealthy': Array([0.1640625 , 0.37033227], dtype=float32), 'eval/episode_reward': Array([-9707.504, 10798.765], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08428904, 0.1305511 ], dtype=float32), 'eval/avg_episode_length': Array([637.91406, 364.9006 ], dtype=float32), 'eval/epoch_eval_time': 4.0735063552856445, 'eval/sps': 31422.560525506855}
I0729 03:30:50.632586 140526158489408 train.py:379] starting iteration 3, 167116800 steps, 2472.2454826831818
I0729 03:44:23.307456 140526158489408 train.py:394] {'eval/walltime': 32.2386040687561, 'training/sps': 68892.42582173122, 'training/walltime': 3241.821261882782, 'training/entropy_loss': Array(-0.00921019, dtype=float32), 'training/policy_loss': Array(0.00338322, dtype=float32), 'training/total_loss': Array(2556.8335, dtype=float32), 'training/v_loss': Array(2556.8394, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.08124066, 0.1186998 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 6.880869, 10.017838], dtype=float32), 'eval/episode_is_unhealthy': Array([0.2109375, 0.4079741], dtype=float32), 'eval/episode_reward': Array([-8583.525, 10086.741], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.07823659, 0.11928508], dtype=float32), 'eval/avg_episode_length': Array([575.0078 , 376.95773], dtype=float32), 'eval/epoch_eval_time': 4.08071231842041, 'eval/sps': 31367.072709880984}
I0729 03:44:23.310457 140526158489408 train.py:379] starting iteration 4, 222822400 steps, 3284.9233541488647
I0729 03:57:55.709875 140526158489408 train.py:394] {'eval/walltime': 36.3478946685791, 'training/sps': 68918.33347243027, 'training/walltime': 4050.105461359024, 'training/entropy_loss': Array(-0.00946515, dtype=float32), 'training/policy_loss': Array(0.00376339, dtype=float32), 'training/total_loss': Array(2818.4167, dtype=float32), 'training/v_loss': Array(2818.4224, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.07271877, 0.10696279], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([6.1644354, 8.998489 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0.1875    , 0.39031237], dtype=float32), 'eval/episode_reward': Array([-7870.679,  9462.547], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.06947663, 0.1075456 ], dtype=float32), 'eval/avg_episode_length': Array([528.6094 , 392.21484], dtype=float32), 'eval/epoch_eval_time': 4.109290599822998, 'eval/sps': 31148.928723978155}
I0729 03:57:55.712726 140526158489408 train.py:379] starting iteration 5, 278528000 steps, 4097.325623512268
I0729 04:11:26.848322 140526158489408 train.py:394] {'eval/walltime': 40.44293189048767, 'training/sps': 69025.07445274552, 'training/walltime': 4857.139723062515, 'training/entropy_loss': Array(-0.01116663, dtype=float32), 'training/policy_loss': Array(0.00375473, dtype=float32), 'training/total_loss': Array(2944.7231, dtype=float32), 'training/v_loss': Array(2944.731, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.05734836, 0.08110809], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([4.864469, 6.784903], dtype=float32), 'eval/episode_is_unhealthy': Array([0.1953125 , 0.39644107], dtype=float32), 'eval/episode_reward': Array([-6234.8237,  7948.3877], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.05345876, 0.08132634], dtype=float32), 'eval/avg_episode_length': Array([518.5547 , 416.40268], dtype=float32), 'eval/epoch_eval_time': 4.095037221908569, 'eval/sps': 31257.347140874874}
I0729 04:11:26.851701 140526158489408 train.py:379] starting iteration 6, 334233600 steps, 4908.464597702026
I0729 04:24:59.000047 140526158489408 train.py:394] {'eval/walltime': 44.513914346694946, 'training/sps': 68936.47989406588, 'training/walltime': 5665.211154699326, 'training/entropy_loss': Array(-0.01011269, dtype=float32), 'training/policy_loss': Array(0.00373348, dtype=float32), 'training/total_loss': Array(2937.225, dtype=float32), 'training/v_loss': Array(2937.2314, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.04818719, 0.081755  ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([4.1717205, 6.910792 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0.15625   , 0.36309218], dtype=float32), 'eval/episode_reward': Array([-6129.327,  8825.592], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.04489257, 0.081784  ], dtype=float32), 'eval/avg_episode_length': Array([458.92188, 398.23   ], dtype=float32), 'eval/epoch_eval_time': 4.070982456207275, 'eval/sps': 31442.041663635908}
I0729 04:24:59.006800 140526158489408 train.py:379] starting iteration 7, 389939200 steps, 5720.619697332382
I0729 04:38:31.351278 140526158489408 train.py:394] {'eval/walltime': 48.58686637878418, 'training/sps': 68919.9924200528, 'training/walltime': 6473.475898265839, 'training/entropy_loss': Array(-0.00998482, dtype=float32), 'training/policy_loss': Array(0.0041219, dtype=float32), 'training/total_loss': Array(2908.663, dtype=float32), 'training/v_loss': Array(2908.669, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.0319792 , 0.03601354], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.8191972, 3.071186 ], dtype=float32), 'eval/episode_is_unhealthy': Array([0.1640625 , 0.37033227], dtype=float32), 'eval/episode_reward': Array([-5244.546 ,  6162.0054], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.02740382, 0.0344661 ], dtype=float32), 'eval/avg_episode_length': Array([495.0547, 417.3934], dtype=float32), 'eval/epoch_eval_time': 4.072952032089233, 'eval/sps': 31426.83709298241}
I0729 04:38:31.354099 140526158489408 train.py:379] starting iteration 8, 445644800 steps, 6532.966996192932
