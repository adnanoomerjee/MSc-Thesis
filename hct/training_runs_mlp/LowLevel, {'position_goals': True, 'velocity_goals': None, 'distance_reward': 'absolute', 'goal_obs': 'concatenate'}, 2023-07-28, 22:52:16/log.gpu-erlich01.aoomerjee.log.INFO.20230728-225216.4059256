I0728 22:52:16.473329 140049511307072 low_level_env.py:191] Initialising environment...
I0728 22:52:16.826738 140049511307072 low_level_env.py:299] Environment initialised.
I0728 22:52:16.831381 140049511307072 train.py:118] JAX is running on GPU.
I0728 22:52:16.831461 140049511307072 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 22:52:23.141455 140049511307072 train.py:367] Running initial eval
I0728 22:52:38.590306 140049511307072 train.py:373] {'eval/walltime': 15.30277681350708, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.3065918 , 0.12877326], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.2056   , 10.8997345], dtype=float32), 'eval/episode_is_unhealthy': Array([0.2109375, 0.4079741], dtype=float32), 'eval/episode_reward': Array([-25327.36,   9517.28], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.3049535 , 0.12993073], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.30277681350708, 'eval/sps': 8364.494990675163}
I0728 22:52:38.591670 140049511307072 train.py:379] starting iteration 0, 0 steps, 21.76031184196472
I0728 22:55:50.772965 140049511307072 train.py:394] {'eval/walltime': 18.970358848571777, 'training/sps': 295506.2083587913, 'training/walltime': 188.50906825065613, 'training/entropy_loss': Array(-0.011555, dtype=float32), 'training/policy_loss': Array(-0.00343078, dtype=float32), 'training/total_loss': Array(5248.3516, dtype=float32), 'training/v_loss': Array(5248.366, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.0941468 , 0.10554836], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([7.187765, 9.185337], dtype=float32), 'eval/episode_is_unhealthy': Array([0.296875, 0.456881], dtype=float32), 'eval/episode_reward': Array([-9212.955,  9507.625], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08427486, 0.11017285], dtype=float32), 'eval/avg_episode_length': Array([986.21875,  87.54498], dtype=float32), 'eval/epoch_eval_time': 3.6675820350646973, 'eval/sps': 34900.378171838776}
I0728 22:55:50.788606 140049511307072 train.py:379] starting iteration 1, 55705600 steps, 213.95725107192993
I0728 22:58:56.040480 140049511307072 train.py:394] {'eval/walltime': 22.85914945602417, 'training/sps': 307157.6290461323, 'training/walltime': 369.8674156665802, 'training/entropy_loss': Array(0.01070561, dtype=float32), 'training/policy_loss': Array(-0.00056682, dtype=float32), 'training/total_loss': Array(1940.7811, dtype=float32), 'training/v_loss': Array(1940.771, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.02678072, 0.03061857], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.333899, 2.705452], dtype=float32), 'eval/episode_is_unhealthy': Array([0.1015625, 0.3020721], dtype=float32), 'eval/episode_reward': Array([-4763.3965,  5715.768 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.02300041, 0.02953278], dtype=float32), 'eval/avg_episode_length': Array([444.3672 , 394.91867], dtype=float32), 'eval/epoch_eval_time': 3.8887906074523926, 'eval/sps': 32915.117557294965}
I0728 22:58:56.042443 140049511307072 train.py:379] starting iteration 2, 111411200 steps, 399.2110891342163
I0728 23:02:03.551810 140049511307072 train.py:394] {'eval/walltime': 26.737871646881104, 'training/sps': 303363.298458965, 'training/walltime': 553.4941110610962, 'training/entropy_loss': Array(0.00866481, dtype=float32), 'training/policy_loss': Array(-0.00166038, dtype=float32), 'training/total_loss': Array(2621.967, dtype=float32), 'training/v_loss': Array(2621.96, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.03112846, 0.0246146 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([2.768518 , 2.2152534], dtype=float32), 'eval/episode_is_unhealthy': Array([0.1484375 , 0.35553312], dtype=float32), 'eval/episode_reward': Array([-5333.9194,  5246.559 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.02685469, 0.02250285], dtype=float32), 'eval/avg_episode_length': Array([557.47656, 436.23987], dtype=float32), 'eval/epoch_eval_time': 3.8787221908569336, 'eval/sps': 33000.55887006455}
I0728 23:02:03.553707 140049511307072 train.py:379] starting iteration 3, 167116800 steps, 586.7223520278931
I0728 23:05:13.319202 140049511307072 train.py:394] {'eval/walltime': 30.606983423233032, 'training/sps': 299666.3981237802, 'training/walltime': 739.3861575126648, 'training/entropy_loss': Array(0.00913556, dtype=float32), 'training/policy_loss': Array(-0.00155969, dtype=float32), 'training/total_loss': Array(2428.4692, dtype=float32), 'training/v_loss': Array(2428.4617, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.02227058, 0.02174529], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.9799395, 1.9719975], dtype=float32), 'eval/episode_is_unhealthy': Array([0.078125  , 0.26836818], dtype=float32), 'eval/episode_reward': Array([-3793.1992,  5308.545 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01854343, 0.01921998], dtype=float32), 'eval/avg_episode_length': Array([347.29688, 401.52435], dtype=float32), 'eval/epoch_eval_time': 3.8691117763519287, 'eval/sps': 33082.52834212183}
I0728 23:05:13.321202 140049511307072 train.py:379] starting iteration 4, 222822400 steps, 776.4898483753204
I0728 23:08:24.243062 140049511307072 train.py:394] {'eval/walltime': 34.47870087623596, 'training/sps': 297817.3727697041, 'training/walltime': 926.4323310852051, 'training/entropy_loss': Array(0.00919174, dtype=float32), 'training/policy_loss': Array(-0.00149459, dtype=float32), 'training/total_loss': Array(2282.342, dtype=float32), 'training/v_loss': Array(2282.3345, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.02225406, 0.02186222], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.9329858, 1.9362509], dtype=float32), 'eval/episode_is_unhealthy': Array([0.0625    , 0.24206145], dtype=float32), 'eval/episode_reward': Array([-3607.011 ,  4727.7476], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01837652, 0.01942821], dtype=float32), 'eval/avg_episode_length': Array([357.82812, 405.23987], dtype=float32), 'eval/epoch_eval_time': 3.8717174530029297, 'eval/sps': 33060.26370822136}
I0728 23:08:24.245070 140049511307072 train.py:379] starting iteration 5, 278528000 steps, 967.4137165546417
I0728 23:11:36.043560 140049511307072 train.py:394] {'eval/walltime': 38.35174083709717, 'training/sps': 296430.6867889042, 'training/walltime': 1114.3534960746765, 'training/entropy_loss': Array(0.01136022, dtype=float32), 'training/policy_loss': Array(-0.00098864, dtype=float32), 'training/total_loss': Array(2146.6787, dtype=float32), 'training/v_loss': Array(2146.6685, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.02110049, 0.02066056], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.8635396, 1.8375491], dtype=float32), 'eval/episode_is_unhealthy': Array([0.0859375 , 0.28027174], dtype=float32), 'eval/episode_reward': Array([-2878.0015,  3790.081 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01746747, 0.01850541], dtype=float32), 'eval/avg_episode_length': Array([340.75  , 400.8272], dtype=float32), 'eval/epoch_eval_time': 3.873039960861206, 'eval/sps': 33048.97478298624}
I0728 23:11:36.045559 140049511307072 train.py:379] starting iteration 6, 334233600 steps, 1159.2142052650452
I0728 23:14:48.541493 140049511307072 train.py:394] {'eval/walltime': 42.25215482711792, 'training/sps': 295377.1798299086, 'training/walltime': 1302.9449100494385, 'training/entropy_loss': Array(0.01331189, dtype=float32), 'training/policy_loss': Array(-0.00060086, dtype=float32), 'training/total_loss': Array(2016.9485, dtype=float32), 'training/v_loss': Array(2016.9358, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.02248662, 0.02122512], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.9986804, 1.9367101], dtype=float32), 'eval/episode_is_unhealthy': Array([0.0546875 , 0.22736925], dtype=float32), 'eval/episode_reward': Array([-3515.505,  4584.659], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01883042, 0.01913045], dtype=float32), 'eval/avg_episode_length': Array([367.9375 , 419.71204], dtype=float32), 'eval/epoch_eval_time': 3.900413990020752, 'eval/sps': 32817.02925061013}
I0728 23:14:48.543523 140049511307072 train.py:379] starting iteration 7, 389939200 steps, 1351.7121696472168
I0728 23:18:00.995082 140049511307072 train.py:394] {'eval/walltime': 46.13024854660034, 'training/sps': 295411.7683508542, 'training/walltime': 1491.5142426490784, 'training/entropy_loss': Array(0.01522943, dtype=float32), 'training/policy_loss': Array(-0.00086901, dtype=float32), 'training/total_loss': Array(1914.1067, dtype=float32), 'training/v_loss': Array(1914.0924, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.02141384, 0.02077933], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.8896203, 1.8669116], dtype=float32), 'eval/episode_is_unhealthy': Array([0.0703125 , 0.25567293], dtype=float32), 'eval/episode_reward': Array([-2963.401 ,  4048.2632], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01783163, 0.0185936 ], dtype=float32), 'eval/avg_episode_length': Array([335.35938, 408.4551 ], dtype=float32), 'eval/epoch_eval_time': 3.878093719482422, 'eval/sps': 33005.90683432043}
I0728 23:18:00.997090 140049511307072 train.py:379] starting iteration 8, 445644800 steps, 1544.1657357215881
I0728 23:21:13.845958 140049511307072 train.py:394] {'eval/walltime': 49.99989891052246, 'training/sps': 294777.6348307206, 'training/walltime': 1680.489230632782, 'training/entropy_loss': Array(0.01791332, dtype=float32), 'training/policy_loss': Array(-0.00010309, dtype=float32), 'training/total_loss': Array(1807.582, dtype=float32), 'training/v_loss': Array(1807.5642, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.02039817, 0.01999687], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([1.8206241, 1.8445565], dtype=float32), 'eval/episode_is_unhealthy': Array([0.0546875 , 0.22736925], dtype=float32), 'eval/episode_reward': Array([-3199.7666,  4719.9336], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.01681207, 0.01783216], dtype=float32), 'eval/avg_episode_length': Array([311.53906, 393.24478], dtype=float32), 'eval/epoch_eval_time': 3.869650363922119, 'eval/sps': 33077.92383347638}
I0728 23:21:14.196609 140049511307072 train.py:410] total steps: 501350400
