I0728 17:43:10.865430 139783752922944 low_level_env.py:188] Initialising environment...
I0728 17:43:11.226292 139783752922944 low_level_env.py:294] Environment initialised.
I0728 17:43:11.230983 139783752922944 train.py:118] JAX is running on GPU.
I0728 17:43:11.231065 139783752922944 train.py:121] Device count: 2, process count: 1 (id 0), local device count: 2, devices to be used count: 2
I0728 17:43:17.513222 139783752922944 train.py:367] Running initial eval
I0728 17:43:33.072974 139783752922944 train.py:373] {'eval/walltime': 15.41161322593689, 'eval/episode_goal_distance_relative_frame_normalised': Array([0.31229496, 0.12412707], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([26.407421, 10.67413 ], dtype=float32), 'eval/episode_reward': Array([-26326.354,   9718.747], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.30795324, 0.12689844], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 15.41161322593689, 'eval/sps': 8305.425144240131}
I0728 17:43:33.074270 139783752922944 train.py:379] starting iteration 0, 0 steps, 21.843326091766357
I0728 17:46:46.784850 139783752922944 train.py:394] {'eval/walltime': 19.229912042617798, 'training/sps': 293361.84580669063, 'training/walltime': 189.8869972229004, 'training/entropy_loss': Array(0.01774583, dtype=float32), 'training/policy_loss': Array(-0.00466471, dtype=float32), 'training/total_loss': Array(5614.957, dtype=float32), 'training/v_loss': Array(5614.9443, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.09610543, 0.11594315], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([7.0809646, 9.989824 ], dtype=float32), 'eval/episode_reward': Array([-8846.502,  9351.941], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.08406661, 0.11978001], dtype=float32), 'eval/avg_episode_length': Array([1000.,    0.], dtype=float32), 'eval/epoch_eval_time': 3.818298816680908, 'eval/sps': 33522.782303158034}
I0728 17:46:46.799458 139783752922944 train.py:379] starting iteration 1, 55705600 steps, 215.56851649284363
I0728 17:49:51.884480 139783752922944 train.py:394] {'eval/walltime': 23.12262535095215, 'training/sps': 307445.55165679165, 'training/walltime': 371.07550263404846, 'training/entropy_loss': Array(0.02349473, dtype=float32), 'training/policy_loss': Array(-0.00111888, dtype=float32), 'training/total_loss': Array(27400.113, dtype=float32), 'training/v_loss': Array(27400.09, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.07130266, 0.11376261], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([5.874043 , 9.4935665], dtype=float32), 'eval/episode_reward': Array([-6336.698, 10159.283], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.06736282, 0.11300199], dtype=float32), 'eval/avg_episode_length': Array([406.28906, 409.6627 ], dtype=float32), 'eval/epoch_eval_time': 3.8927133083343506, 'eval/sps': 32881.94887765053}
I0728 17:49:51.886386 139783752922944 train.py:379] starting iteration 2, 111411200 steps, 400.6554455757141
I0728 17:53:00.387754 139783752922944 train.py:394] {'eval/walltime': 26.989861011505127, 'training/sps': 301714.2290339284, 'training/walltime': 555.7058403491974, 'training/entropy_loss': Array(0.00964591, dtype=float32), 'training/policy_loss': Array(0.00073803, dtype=float32), 'training/total_loss': Array(21967.057, dtype=float32), 'training/v_loss': Array(21967.047, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.07922176, 0.12543164], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 6.5001564, 10.4431925], dtype=float32), 'eval/episode_reward': Array([-6906.1284, 11381.709 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.07506964, 0.12485556], dtype=float32), 'eval/avg_episode_length': Array([432.15625, 434.38455], dtype=float32), 'eval/epoch_eval_time': 3.8672356605529785, 'eval/sps': 33098.57770128681}
I0728 17:53:00.389589 139783752922944 train.py:379] starting iteration 3, 167116800 steps, 589.1586487293243
I0728 17:56:10.665586 139783752922944 train.py:394] {'eval/walltime': 30.843048572540283, 'training/sps': 298819.2359252335, 'training/walltime': 742.1248967647552, 'training/entropy_loss': Array(0.01397311, dtype=float32), 'training/policy_loss': Array(0.00081103, dtype=float32), 'training/total_loss': Array(21448.262, dtype=float32), 'training/v_loss': Array(21448.248, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.07848711, 0.12718658], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 6.461154, 10.657586], dtype=float32), 'eval/episode_reward': Array([-6559.961, 11478.622], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.07399913, 0.12672615], dtype=float32), 'eval/avg_episode_length': Array([375.25   , 423.29636], dtype=float32), 'eval/epoch_eval_time': 3.8531875610351562, 'eval/sps': 33219.24976982249}
I0728 17:56:10.667536 139783752922944 train.py:379] starting iteration 4, 222822400 steps, 779.4365966320038
I0728 17:59:21.212238 139783752922944 train.py:394] {'eval/walltime': 34.73516368865967, 'training/sps': 298451.47483979585, 'training/walltime': 928.7736644744873, 'training/entropy_loss': Array(0.01496808, dtype=float32), 'training/policy_loss': Array(0.00100676, dtype=float32), 'training/total_loss': Array(20774.8, dtype=float32), 'training/v_loss': Array(20774.787, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.0629278 , 0.11221099], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([5.191955, 9.350524], dtype=float32), 'eval/episode_reward': Array([-5406.3667, 10251.209 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.05898947, 0.11167332], dtype=float32), 'eval/avg_episode_length': Array([346.6797, 410.7587], dtype=float32), 'eval/epoch_eval_time': 3.8921151161193848, 'eval/sps': 32887.00261456342}
I0728 17:59:21.214112 139783752922944 train.py:379] starting iteration 5, 278528000 steps, 969.9831721782684
I0728 18:02:32.381996 139783752922944 train.py:394] {'eval/walltime': 38.58809566497803, 'training/sps': 297396.6690110236, 'training/walltime': 1116.0844376087189, 'training/entropy_loss': Array(0.01816348, dtype=float32), 'training/policy_loss': Array(0.00165472, dtype=float32), 'training/total_loss': Array(20296.777, dtype=float32), 'training/v_loss': Array(20296.76, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.05497947, 0.09314863], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([4.472851 , 7.7413774], dtype=float32), 'eval/episode_reward': Array([-4407.328,  8616.81 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.05004571, 0.09211663], dtype=float32), 'eval/avg_episode_length': Array([338.92188, 414.6348 ], dtype=float32), 'eval/epoch_eval_time': 3.8529319763183594, 'eval/sps': 33221.453372843986}
I0728 18:02:32.383836 139783752922944 train.py:379] starting iteration 6, 334233600 steps, 1161.152896642685
I0728 18:05:44.447580 139783752922944 train.py:394] {'eval/walltime': 42.44536995887756, 'training/sps': 295987.7284028972, 'training/walltime': 1304.2868347167969, 'training/entropy_loss': Array(0.02045223, dtype=float32), 'training/policy_loss': Array(0.00165853, dtype=float32), 'training/total_loss': Array(19694.84, dtype=float32), 'training/v_loss': Array(19694.816, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.07764319, 0.1262069 ], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([ 6.3466687, 10.526623 ], dtype=float32), 'eval/episode_reward': Array([-6451.6294, 11469.261 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.07303397, 0.1257095 ], dtype=float32), 'eval/avg_episode_length': Array([365.58594, 428.36023], dtype=float32), 'eval/epoch_eval_time': 3.857274293899536, 'eval/sps': 33184.05439883757}
I0728 18:05:44.449537 139783752922944 train.py:379] starting iteration 7, 389939200 steps, 1353.2185971736908
I0728 18:08:55.902564 139783752922944 train.py:394] {'eval/walltime': 46.34411358833313, 'training/sps': 297016.33010169934, 'training/walltime': 1491.8374652862549, 'training/entropy_loss': Array(0.02158657, dtype=float32), 'training/policy_loss': Array(0.00147208, dtype=float32), 'training/total_loss': Array(19414.156, dtype=float32), 'training/v_loss': Array(19414.133, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.06206141, 0.09945507], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([5.100796, 8.236865], dtype=float32), 'eval/episode_reward': Array([-5053.506,  9237.189], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.05774543, 0.09855314], dtype=float32), 'eval/avg_episode_length': Array([347.02344, 419.22507], dtype=float32), 'eval/epoch_eval_time': 3.8987436294555664, 'eval/sps': 32831.08923421937}
I0728 18:08:55.908027 139783752922944 train.py:379] starting iteration 8, 445644800 steps, 1544.6770718097687
I0728 18:12:07.573335 139783752922944 train.py:394] {'eval/walltime': 50.19967007637024, 'training/sps': 296612.36201960687, 'training/walltime': 1679.6435284614563, 'training/entropy_loss': Array(0.02434036, dtype=float32), 'training/policy_loss': Array(0.00161958, dtype=float32), 'training/total_loss': Array(18523.248, dtype=float32), 'training/v_loss': Array(18523.223, dtype=float32), 'eval/episode_goal_distance_relative_frame_normalised': Array([0.07418749, 0.11916231], dtype=float32), 'eval/episode_goal_distance_world_frame': Array([6.100851, 9.936367], dtype=float32), 'eval/episode_reward': Array([-6155.6045, 10927.076 ], dtype=float32), 'eval/episode_root_goal_distance_normalised': Array([0.06957571, 0.11852336], dtype=float32), 'eval/avg_episode_length': Array([357.14844, 426.31677], dtype=float32), 'eval/epoch_eval_time': 3.8555564880371094, 'eval/sps': 33198.839232975595}
I0728 18:12:07.931037 139783752922944 train.py:410] total steps: 501350400
